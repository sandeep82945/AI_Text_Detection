{
    "abstractText": "A particularly successful class of approaches for few-shot learning combines language models with prompts \u2013 hand-crafted task descriptions that complement data samples. However, designing prompts by hand for each task commonly requires domain knowledge and substantial guesswork. We observe, in the context of classification tasks, that instruction finetuned language models are remarkably robust towards some dimensions of a prompt\u2019s design. We subsequently propose a simple method to eliminate the need for handcrafted prompts, named AuTFew. This approach consists of (i) a prompt retrieval module that selects suitable task instructions from the instruction-tuning knowledge base, and (ii) the generation of two distinct, semantically meaningful, class descriptions and a selection mechanism via cross-validation. Over 12 datasets, spanning 8 classification tasks, we show that AuT-Few outperforms current stateof-the-art few-shot learning methods. Moreover, AuT-Few is the best ranking method across datasets on the RAFT few-shot benchmark. Notably, these results are achieved without task-specific handcrafted prompts on unseen tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Rami Aly"
        },
        {
            "affiliations": [],
            "name": "Xingjian Shi"
        },
        {
            "affiliations": [],
            "name": "Kaixiang Lin"
        },
        {
            "affiliations": [],
            "name": "Aston Zhang"
        },
        {
            "affiliations": [],
            "name": "Andrew Gordon Wilson"
        }
    ],
    "id": "SP:a4e2736fd2dddb14c8f463439e68a1714138466f",
    "references": [
        {
            "authors": [
                "Neel Alex",
                "Eli Lifland",
                "Lewis Tunstall",
                "Abhishek Thakur",
                "Pegah Maham",
                "C. Jess Riedel",
                "Emmie Hine",
                "Carolyn Ashurst",
                "Paul Sedille",
                "Alexis Carlier",
                "Michael Noetel",
                "Andreas Stuhlm\u00fcller"
            ],
            "title": "RAFT: A real-world few-shot text classification",
            "year": 2021
        },
        {
            "authors": [
                "Akari Asai",
                "Mohammadreza Salehi",
                "Matthew E Peters",
                "Hannaneh Hajishirzi."
            ],
            "title": "Parameter-efficient multi-task tuning via attentional mixtures of soft prompts",
            "venue": "EMNLP, Abu Dhabi, United Arab Emirates.",
            "year": 2022
        },
        {
            "authors": [
                "Carolyn Ashurst",
                "Emmie Hine",
                "Paul Sedille",
                "Alexis Carlier."
            ],
            "title": "Ai ethics statements: analysis and lessons learnt from neurips broader impact statements",
            "venue": "2022 ACM Conference on Fairness, Accountability, and Transparency, pages 2047\u20132056.",
            "year": 2022
        },
        {
            "authors": [
                "jan Chhablani",
                "Han Wang",
                "Jason Fries",
                "Maged Alshaibani",
                "Shanya Sharma",
                "Urmish Thakker",
                "Khalid Almubarak",
                "Xiangru Tang",
                "Dragomir Radev",
                "Mike Tian-jian Jiang",
                "Alexander Rush"
            ],
            "title": "PromptSource: An integrated development environment",
            "year": 2022
        },
        {
            "authors": [
                "M Saiful Bari",
                "Aston Zhang",
                "Shuai Zheng",
                "Xingjian Shi",
                "Yi Zhu",
                "Shafiq Joty",
                "Mu Li."
            ],
            "title": "Spt: Semiparametric prompt tuning for multitask prompted learning",
            "venue": "arXiv preprint arXiv:2212.10929.",
            "year": 2022
        },
        {
            "authors": [
                "Valerio Basile",
                "Cristina Bosco",
                "Elisabetta Fersini",
                "Debora Nozza",
                "Viviana Patti",
                "Francisco Manuel Rangel Pardo",
                "Paolo Rosso",
                "Manuela Sanguinetti."
            ],
            "title": "Semeval-2019 task 5: Multilingual detection of hate speech against immigrants and women in twitter",
            "venue": "In",
            "year": 2019
        },
        {
            "authors": [
                "Steven Bird",
                "Edward Loper."
            ],
            "title": "NLTK: The natural language toolkit",
            "venue": "Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 214\u2013217, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "I\u00f1igo Casanueva",
                "Tadas Tem\u010dinas",
                "Daniela Gerz",
                "Matthew Henderson",
                "Ivan Vuli\u0107."
            ],
            "title": "Efficient intent detection with dual sentence encoders",
            "venue": "Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pages 38\u201345, On-",
            "year": 2020
        },
        {
            "authors": [
                "Jiaao Chen",
                "Aston Zhang",
                "Xingjian Shi",
                "Mu Li",
                "Alex Smola",
                "Diyi Yang"
            ],
            "title": "Parameter-efficient fine-tuning design spaces",
            "year": 2023
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Alexis Conneau",
                "Douwe Kiela."
            ],
            "title": "SentEval: An evaluation toolkit for universal sentence representations",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language",
            "year": 2018
        },
        {
            "authors": [
                "Ganqu Cui",
                "Shengding Hu",
                "Ning Ding",
                "Longtao Huang",
                "Zhiyuan Liu."
            ],
            "title": "Prototypical verbalizer for prompt-based few-shot tuning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Ido Dagan",
                "Oren Glickman",
                "Bernardo Magnini."
            ],
            "title": "The pascal recognising textual entailment challenge",
            "venue": "Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.",
            "year": 2005
        },
        {
            "authors": [
                "Marie-Catherine de Marneffe",
                "Mandy Simons",
                "Judith Tonhauser."
            ],
            "title": "The commitmentbank: Investigating projection in naturally occurring discourse",
            "venue": "Proceedings of Sinn und Bedeutung, 23(2):107\u2013124.",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen."
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "H. Gurulingappa",
                "A.M. Rajput",
                "A. Roberts",
                "J. Fluck",
                "M. Hofmann-Apitius",
                "L. Toldo."
            ],
            "title": "Development of a Benchmark Corpus to Support the Automatic Extraction of Drug-related Adverse Effects from Medical Case Reports",
            "venue": "Journal of Biomedical",
            "year": 2012
        },
        {
            "authors": [
                "Karen Hambardzumyan",
                "Hrant Khachatrian",
                "Jonathan May."
            ],
            "title": "WARP: Word-level Adversarial ReProgramming",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "2022a. LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "Shengding Hu",
                "Ning Ding",
                "Huadong Wang",
                "Zhiyuan Liu",
                "Jingang Wang",
                "Juanzi Li",
                "Wei Wu",
                "Maosong Sun."
            ],
            "title": "Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification",
            "venue": "Proceedings of the 60th Annual",
            "year": 2022
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei."
            ],
            "title": "Scaling laws for neural language models",
            "venue": "arXiv preprint arXiv:2001.08361.",
            "year": 2020
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "James Henderson",
                "Sebastian Ruder."
            ],
            "title": "Compacter: Efficient low-rank hypercomplex adapter layers",
            "venue": "Advances in Neural Information Processing Systems, 34:1022\u20131035.",
            "year": 2021
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "Luke Zettlemoyer",
                "James Henderson",
                "Lambert Mathias",
                "Marzieh Saeidi",
                "Veselin Stoyanov",
                "Majid Yazdani."
            ],
            "title": "Promptfree and efficient few-shot learning with language models",
            "venue": "Proceedings of the 60th Annual Meet-",
            "year": 2022
        },
        {
            "authors": [
                "Hector J. Levesque",
                "Ernest Davis",
                "Leora Morgenstern."
            ],
            "title": "The Winograd Schema Challenge",
            "venue": "Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and",
            "year": 2012
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Kangmin Tan",
                "Chris Miller",
                "Beiwen Tian",
                "Xiang Ren."
            ],
            "title": "Unsupervised crosstask generalization via retrieval augmentation",
            "venue": "NeurIPS 2022, New Orleans, LA, USA.",
            "year": 2022
        },
        {
            "authors": [
                "Marco Lippi",
                "Przemys\u0142aw Pa\u0142ka",
                "Giuseppe Contissa",
                "Francesca Lagioia",
                "Hans-Wolfgang Micklitz",
                "Giovanni Sartor",
                "Paolo Torroni."
            ],
            "title": "Claudette: an automated detector of potentially unfair clauses in online terms of service",
            "venue": "Artificial Intelligence and",
            "year": 2019
        },
        {
            "authors": [
                "Haokun Liu",
                "Derek Tam",
                "Mohammed Muqeeth",
                "Jay Mohta",
                "Tenghao Huang",
                "Mohit Bansal",
                "Colin Raffel."
            ],
            "title": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
            "venue": "arXiv preprint arXiv:2205.05638.",
            "year": 2022
        },
        {
            "authors": [
                "Xiao Liu",
                "Yanan Zheng",
                "Zhengxiao Du",
                "Ming Ding",
                "Yujie Qian",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "Gpt understands, too",
            "venue": "arXiv:2103.10385.",
            "year": 2021
        },
        {
            "authors": [
                "Ruotian Ma",
                "Xin Zhou",
                "Tao Gui",
                "Yiding Tan",
                "Linyang Li",
                "Qi Zhang",
                "Xuanjing Huang."
            ],
            "title": "Templatefree prompt tuning for few-shot NER",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Vangelis Metsis",
                "Ion Androutsopoulos",
                "Georgios Paliouras"
            ],
            "title": "Spam filtering with naive bayes - which naive bayes",
            "venue": "In Proceedings of the 3rd Conference on Email and Anti-Spam (CEAS",
            "year": 2006
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela."
            ],
            "title": "Adversarial NLI: A new benchmark for natural language understanding",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "James O\u2019Neill",
                "Polina Rozenshtein",
                "Ryuichi Kiryo",
                "Motoko Kubota",
                "Danushka Bollegala"
            ],
            "title": "I wish I would have loved this one, but I didn\u2019t \u2013 a multilingual dataset for counterfactual detection in product review",
            "year": 2021
        },
        {
            "authors": [
                "Mohammad Taher Pilehvar",
                "Jose Camacho-Collados."
            ],
            "title": "WiC: the word-in-context dataset for evaluating context-sensitive meaning representations",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Preotiuc-Pietro",
                "Mihaela Gaman",
                "Nikolaos Aletras."
            ],
            "title": "Automatically identifying complaints in social media",
            "venue": "arXiv preprint arXiv:1906.03890.",
            "year": 2019
        },
        {
            "authors": [
                "Guanghui Qin",
                "Jason Eisner."
            ],
            "title": "Learning how to ask: Querying LMs with mixtures of soft prompts",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Jess Riedel",
                "Angelica Deibel."
            ],
            "title": "Tai safety bibliographic database",
            "venue": "Online.",
            "year": 2020
        },
        {
            "authors": [
                "Alexander K Saeri",
                "Peter Slattery",
                "Joannie Lee",
                "Thomas Houlden",
                "Neil Farr",
                "Romy L Gelber",
                "Jake Stone",
                "Lee Huuskes",
                "Shane Timmons",
                "Kai Windle"
            ],
            "title": "What works to increase charitable donations? a metareview with meta-meta-analysis",
            "year": 2022
        },
        {
            "authors": [
                "drea Santilli",
                "Thibault Fevry",
                "Jason Alan Fries",
                "Ryan Teehan",
                "Teven Le Scao",
                "Stella Biderman",
                "Leo Gao",
                "Thomas Wolf",
                "Alexander M Rush"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "In International Conference on Learning",
            "year": 2022
        },
        {
            "authors": [
                "Elvis Saravia",
                "Hsien-Chi Toby Liu",
                "Yen-Hao Huang",
                "Junlin Wu",
                "Yi-Shin Chen."
            ],
            "title": "CARER: Contextualized affect representations for emotion recognition",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "It\u2019s not just size that matters: Small language models are also fewshot learners",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "True Few-Shot Learning with Prompts\u2014A Real-World Perspective",
            "venue": "Transactions of the Association for Computational Linguistics, 10:716\u2013731.",
            "year": 2022
        },
        {
            "authors": [
                "Taylor Shin",
                "Yasaman Razeghi",
                "Robert L. Logan IV",
                "Eric Wallace",
                "Sameer Singh."
            ],
            "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Lewis Tunstall",
                "Nils Reimers",
                "Unso Eun Seo Jo",
                "Luke Bates",
                "Daniel Korat",
                "Moshe Wasserblat",
                "Oren Pereg."
            ],
            "title": "Efficient few-shot learning without prompts",
            "venue": "arXiv preprint arXiv:2209.11055.",
            "year": 2022
        },
        {
            "authors": [
                "Sowmya Vajjala",
                "Ivana Lu\u010di\u0107."
            ],
            "title": "OneStopEnglish corpus: A new corpus for automatic readability assessment and text simplification",
            "venue": "Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications,",
            "year": 2018
        },
        {
            "authors": [
                "Tu Vu",
                "Brian Lester",
                "Noah Constant",
                "Rami Al-Rfou",
                "Daniel Cer"
            ],
            "title": "SPoT: Better frozen model adaptation through soft prompt transfer",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Han Wang",
                "Canwen Xu",
                "Julian McAuley."
            ],
            "title": "Automatic multi-label prompting: Simple and interpretable few-shot classification",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Yizhong Wang",
                "Swaroop Mishra",
                "Pegah Alipoormolabashi",
                "Yeganeh Kordi",
                "Amirreza Mirzaei",
                "Anjana Arunkumar",
                "Arjun Ashok",
                "Arut Selvan Dhanasekaran",
                "Atharva Naik",
                "David Stap"
            ],
            "title": "Benchmarking generalization via in-context",
            "year": 2022
        },
        {
            "authors": [
                "Albert Webson",
                "Ellie Pavlick"
            ],
            "title": "Do promptbased models really understand the meaning of their prompts",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Seonghyeon Ye",
                "Joel Jang",
                "Doyoung Kim",
                "Yongrae Jo",
                "Minjoon Seo."
            ],
            "title": "Retrieval of soft prompt enhances zero-shot task generalization",
            "venue": "arXiv preprint arXiv:2210.03029.",
            "year": 2022
        },
        {
            "authors": [
                "Aston Zhang",
                "Yi Tay",
                "Shuai Zhang",
                "Alvin Chan",
                "Anh Tuan Luu",
                "Siu Hui",
                "Jie Fu."
            ],
            "title": "Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters",
            "venue": "International Conference on Learn-",
            "year": 2021
        },
        {
            "authors": [
                "Lucia Zheng",
                "Neel Guha",
                "Brandon R. Anderson",
                "Peter Henderson",
                "Daniel E. Ho."
            ],
            "title": "When does pretraining help? assessing self-supervised learning for law and the casehold dataset of 53,000+ legal holdings",
            "venue": "Proceedings of the Eighteenth International",
            "year": 2021
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba."
            ],
            "title": "Large language models are human-level prompt engineers",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "stall"
            ],
            "title": "2022), as well as a standard finetuned LM. SetFit is of particular relevance to us since it is the state-of-the-art prompt-free fewshot method, shown to perform competitively to T-Few",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Collecting annotated data is time-consuming and expensive. The goal of few-shot learning is to address this limitation by developing models that generalize from a small number of training examples.\nA now dominant paradigm in few-shot learning involves pre-training a large language model (PLM) on unsupervised language modelling objectives, combined with supervised fine-tuning (Kaplan et al., 2020; Wei et al., 2022b). Fine-tuning on a variety of classification tasks improves generalization to new unseen tasks even further (Sanh et al., 2022; Wei et al., 2022b; Chung et al., 2022).\n\u2217Work done while interning at Amazon Web Services. \u2020 Work done when author was working at Amazon Web\nServices.\nPrompts, instructions that describe the tasks in natural language, are crucial to successful finetuning on many tasks. Typically, prompts consist of two components: task templates and answer choices. Task templates are textual instructions about the task. Answer choices are semantic descriptions of the categorical labels. Supervised training on prompted samples, as shown in Figure 1, helps PLMs generalize when instructed via prompts on a new problem (here natural language inference). Following Lin et al. (2022), we use the term upstream model for these instructionfinetuned PLMs. These prompted upstream models provide state-of-the-art few-shot learning ( (Liu et al., 2022), yet they still rely on strenuous manual intervention from manually crafted prompts, designed by experts with domain knowledge about the underlying tasks.\nIn this paper, we are concerned with an automated few-shot classification regime, where the algorithm can only access the training samples\nand their categorical labels. While efforts have been made to automate prompting, these methods are not directly transferable to upstream models. Most techniques target prompted masked language models (i.e. encoder-only models, that make predictions over continuous embeddings via its mask token (Gao et al., 2021, inter alia). Automation methods for models with a discrete output space (i.e. a decoder over the vocabulary) are costly and limited to the automation of the task template, still relying on handcrafted descriptions of labels (Liu et al., 2021; Zhou et al., 2023).\nTo automate few-shot learning with upstream models, we analyse the role of prompts across various classification tasks and we observe that upstream models exhibit low variability towards task-unspecific templates. In contrast, the selection of suitable answer choices can be important, yet answer choices do not need to be tailored to the specific instruction (e.g. Yes/No for a polar question). These insights confirm observations by Webson and Pavlick (2022) in a broader context and they motivate a simple few-shot learning automation method for upstream models, named AuT-Few.\nAuT-Few builds on the state-of-the-art learning method T-Few (Liu et al., 2022), but crucially does not use any task-specific handcrafted prompts. AuT-Few automatically finds the most relevant templates to our target task from the collection prompts used to instruction-tune the upstream model. As illustrated in Figure 2, given an NLI task, AuTFew might retrieve templates written for paraphrase\nidentification. To automate answer choices, AuTFew generates label descriptions tailored to the retrieved templates (e.g., Yes/No for a polar question, as for the illustrated paraphrase identification template) and descriptions that capture a class\u2019 overall topic (e.g. Enron/purchase for Enron spam classification). AuT-Few selects the most appropriate configuration via cross-validation.\nAuT-Few outperforms strong baselines, including T-Few (Liu et al., 2022), by 2.1 points over a total of 12 datasets, spanning 8 tasks, without any task-specific handcrafted prompts. All but one task are unseen to the upstream models, indicating AuTFew\u2019s strong generalization capabilities. Moreover, by applying AuT-Few to a small upstream model (BART0 (Lin et al., 2022)), we achieve competitive performance and efficiency to the current state-ofthe-art prompt-free method, SetFit (Tunstall et al., 2022). Furthermore, AuT-Few achieves the best average rank across datasets on the few-shot RAFT benchmark (Alex et al., 2021). An ablation justifies the components of our automation method.1"
        },
        {
            "heading": "2 Background and Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Instruction-Finetuned Language Models",
            "text": "A language model is instruction-finetuned on prompted samples Dsrc from various tasks, such as summarization or question answering, by autoregressively generating the target answer choice through standard maximum likelihood training. In-\n1Code at: https://github.com/Raldir/AuT-Few.\nstruction tuning not only improves generalization for large decoder-only models (Wei et al., 2022a), but also for comparably smaller encoder-decoder models, like T0 (Sanh et al., 2022) or BART0 (Lin et al., 2022). Prompt knowledge bases (KB), like PromptSource (Bach et al., 2022), contain prompt instructions for hundreds of tasks. Flan-T5 (Chung et al., 2022) is an improved upstream model scaled to thousands of tasks (Wang et al., 2022b).\nInference. We are interested in using upstream models for an unseen few-shot binary or multiclass classification task Dtgttest. A prediction y\u0302 with an upstream model \u03b8 is made by computing the length-normalized log probabilities for each class y \u2208 Y , conditioned on the sample x, a handcrafted template \u03d5j \u2208 \u03a6 (i.e. task description and sample input formatting), and on the associated answer choices \u03c8j \u2208 \u03a8 (textual descriptions of labels):\nargmaxy( 1\nT \u2211 t log p\u03b8(\u03c8j(y) | x, \u03d5j , \u03c8j(y)<t),\nwith T being the length of the answer choice of y. Since the use of a single prompt might model the expectation over all possible prompts poorly, most systems handcraft multiple prompts for a target task. The expectation is then modelled by randomly drawing a template and its answer choices.\nParameter-Efficient Finetuning. Adapting upstream models to a new task or domain on a few available samples Dtgttrain via full model finetuning is often infeasible as these models consist of billions of parameters. Parameter-efficient finetuning adds or updates only a small subset of parameters \u03b8PEFT \u226a \u03b8, and largely retains the fine-tuning performance (Karimi Mahabadi et al., 2021; Zhang et al., 2021; Chen et al., 2023). Liu et al. (2022) proposed T-Few and showed that parameter-efficient finetuning an upstream model with T-Few performs better than in-context learning with GPT-3 in the few-shot learning setting. T-Few learns attention and activation re-scaling vectors by optimizing the maximum likelihood estimation and complements it with an unlikelihood loss."
        },
        {
            "heading": "2.2 Prompt Automation",
            "text": "Template Automation. To automate the instructions as input to the model, previous work uses soft representation in the input via prompt tuning (Liu et al., 2021; Hambardzumyan et al., 2021), generates discrete instructions (Shin et al., 2020;\nGao et al., 2021; Zhou et al., 2023), or combines both via semi-parametric prompt tuning (Bari et al., 2022). However, prompt tuning is brittle to optimize (Hu et al., 2022a; Liu et al., 2022), and the generation of discrete instructions requires substantial computational resources, a particular concern with upstream models as they typically have billions of parameters. The retrieval of instructions is limited to the retrieval of trained soft prompts and samples (Ye et al., 2022), prompt initialization (Vu et al., 2022), or the retrieval of multiple prompt mixtures (Qin and Eisner, 2021; Asai et al., 2022).\nAnswer Choice Automation. Methods to automate label representations are targeting BERTlike masked language models (Devlin et al., 2019), which enables optimization of the output descriptions on continuous vector representation. Shin et al. (2020) train a logistic classifier on embeddings to score tokens in the vocabulary by how well they predict the task labels. Gao et al. (2021) compute the probability for a token to be the masked classification token, by computing the dot product between both embeddings. Wang et al. (2022a) additionally ensure that label tokens belong only to a single class. Alternatively to such discrete search is learning soft output representations of labels via gradient descent (Hambardzumyan et al., 2021; Cui et al., 2022; Hu et al., 2022b; Karimi Mahabadi et al., 2022), or combining both (Ma et al., 2022). Tunstall et al. (2022) propose a fully prompt-free method using Sentence Transformers (Reimers and Gurevych, 2019).\nNovelty. Prior works on prompt automation are computationally intensive, brittle to optimize, or assume a continuous output representation for each token. By contrast, our proposed approach automates prompts for upstream models, which operate over a discrete output space. We do not insert any additional trainable parameters for automating templates. Instead, our work is the first to use retrieved instruction-finetuning templates for an unseen task directly and to use them to optimize the answer choices via the generation of distinct, semantically meaningful, answer choice configurations."
        },
        {
            "heading": "3 How Much Does the Design of Prompts Matter for Upstream Models?",
            "text": "To automate prompts, we need to understand their role in few-shot classification. While previous research suggests that the wording of instructions for\nmasked language models is crucial, Webson and Pavlick (2022) observe that the semantic relevance of a prompt is not a strong performance indicator for upstream models. However, their analysis is restricted to natural language inference whilst using the PET (Schick and Sch\u00fctze, 2021) algorithm to train the model. Yet, results in Schick and Sch\u00fctze (2022) suggest that templates do matter in principle, but PET is robust when correctly configured.\nThese results raise questions regarding the role of prompts for upstream models in the context of automated few-shot learning on unseen tasks. We conduct a systematic ablation study for both templates \u03a6 and answer choices \u03a8. We use T-Few with the T0 upstream model and 32 samples per class. We evaluate 12 datasets, spanning 8 tasks. For details on the datasets, see Appendix A.\nTemplates. We design four experiments to understand the importance of accurate task descriptions (i.e. semantics) in increasing order: concatenation of a sample\u2019s content without any additional text (null), uniform sampling of words from the training vocabulary (random), general purpose instructions (e.g. Given ..., the answer is ...) that are not tailored to the task (general), handcrafted instructions (handcrafted). We use the same handcrafted answer choices and templates across all settings (and vice versa the same templates across experiments for answer choice experiments).\nAs seen in Figure 3 (top), with a mean score\nof 62.8, 62.9, 64.0, 64.2, for each setting, respectively, we observe that simple task-unspecific templates perform surprisingly well, only performing slightly worse than more complex handcrafted ones. Templates that are not well-formed or lack an instruction entirely perform substantially worse than handcrafted ones. Note that results differ heavily between datasets. While some datasets (Enron and CR) are virtually unaffected by the design of the template, performance is strongly affected by the template for some other (e.g. RTE, WSC, Amazon).\nAnswer Choices. Similarly, for answer choices we run four experiments: reversed handcrafted answer choices (reversed), uniform sampling of a random word from the training vocabulary (random), label text as presented in a dataset itself, such as Entailment in Figure 2 (dataset), and handcrafted choices. Different handcrafted templates for the same task might have different answer choices, depending on the instruction. In contrast, there exists only a single answer choice configuration for dataset answer choices (i.e. mapping from categorical label to text), which we use across all templates.\nWe observe that unlike templates, the selection of answer choices makes a large difference in performance. However, datasets that were particularly robust regarding template design appear to be also robust here. Moreover, despite dataset choices (e.g. entailment, not_entailment) not matching a template\u2019s instruction (e.g. \u201cGiven ... does ... fol-\nlow? Yes or No?\"), and only having one configuration of choices, we observe comparable performance to handcrafted ones. Thus neither templatetailored answer choices nor multiple distinct answer choice configurations are needed. By manually selecting a single configuration of answer choices from both dataset and handcrafted choices (best-single), we easily achieve the highest average score with 66.2. An automated selection mechanism of a single configuration can subsequently perform favourably over multiple distinctly handcrafted prompts."
        },
        {
            "heading": "4 AuT-Few: Automated Few-shot Classification with Upstream Models",
            "text": "AuT-Few is a simple, yet efficient, algorithm to automate prompts for upstream models, drawing from the insights gained from Section 3. Figure 2 shows an illustration of AuT-Few\u2019s template and answer choice automation. AuT-Few deploys a lightweight template automation approach since accurate task templates are not essential to performance. It selects suitable templates from the collection of prompts the upstream model was instructionfinetuned on (Section 4.1).\nOn the other hand, the selection of answer choices has a substantial impact on performance. Searching over all possible answer choices is intractable for large upstream models and also imprecise due to the small training size. Thus, AuT-Few only considers two distinct types of answer choices (Section 4.2). One is tailored to the retrieved templates by measuring the log-likelihood on the training data (template-tailored). The other is based on capturing the topic of samples belonging to the same class (topic-specific).\nWe select the most appropriate template and answer choice configurations via cross-validation. The automated prompts are then used for training and inference of our upstream model, where we largely follow T-Few (c.f. Section 5.1 for details)."
        },
        {
            "heading": "4.1 Automated Templates via Retrieval",
            "text": "We retrieve templates that are used in instruction tuning the upstream models. This enables us to (i) adhere closely to instructions the model is familiar with and has already learned (ii) exploit the associated inductive bias on answer choices for candidate generation in the next step. Specifically, we consider the collection of all prompts used for instruction tuning, \u03a6IT , such as the ones shown\nin Figure 1 for sentiment classification and paraphrase identification. We then aim to find templates \u03a6A \u2282 \u03a6IT from the collection that are related to our downstream task. For instance, given the NLI sample from Figure 2, we rather want to retrieve templates about paraphrase identification than sentiment classification. The former is both semantically and structurally more similar to NLI, as both have two arguments in their input. For NLI they are hypothesis and premise while for paraphrase identification these are the two compared sentences.\nTo find suitable templates, we first filter the collection \u03a6IT to templates that match the target task format the most. We achieve this by matching the number of underlying arguments of our target task, against the number of arguments of individual templates in \u03a6IT . We then do a semantic search via an efficient retrieval system: we query a concatenation of a sample\u2019s argument descriptions (e.g. the strings hypothesis and premise) against all suitable templates in \u03a6IT by encoding both query and every template in the collection with a lightweight bi-encoder (Reimers and Gurevych, 2019). If the field descriptions are uninformative (e.g. numbers), we instead use the averaged representations of all samples inDtgttrain as the query. Using cosine similarity, we then select the top R templates. Finally, we adjust the retrieved templates to the downstream task via regular expressions to obtain \u03a6A."
        },
        {
            "heading": "4.2 Automated Selection of Answer Choices",
            "text": "Generation of Answer Choice Candidates. Apart from the label descriptions that appear in the dataset, which may not be meaningful, we consider the generation of two distinct types of answer choices given the retrieved templates: template-tailored and topic-specific answer choices. Template-tailored answer choices are generated by finding individual tokens for each class c that maximize the conditional likelihood over the training data of that class Dctrain, given the retrieved templates \u03d5 \u2208 \u03a6A, computed via the upstream model:\nLc = \u2211\nx\u2208Dctrain \u2211 \u03d5\u2208\u03a6A log p\u03b8(v | x, \u03d5),\nwith v \u2208 V being a token of the subword vocabulary of the upstream model. Tokens unspecific to an individual class might be ranked high across multiple classes. Thus, we further compute for every token how far its likelihood deviates from the mean 1 |C| \u2211 c\u2208C Lc. We finally select the top-ranked dis-\ntinct tokens across all classes that maximize the sum of these scores.\nRelying exclusively on the likelihood signal (and the retrieved templates) to find answer choices might amplify the inductive bias of the model and it restricts other potentially viable answer choices 2. Since our analysis indicates that answer choices not tailored to the templates can still perform strongly, we additionally consider topic-specific answer choices not generated via our upstream model. We use the high quality contextual representations of Sentence Transformers to find single-word (not token) representations that semantically express the underlying content for each class. For each sentence Sc for a particular class, we obtain a contextual representation of the sentence and each word. For every class and over the training vocabulary we then compute the cosine similarity between each sentence and word. We remove words that occur across different classes and finally use the top word for each class as the topic-specific choices.\nSelection of Best Answer Choice Configuration. We are now tasked to find the best representation for the given task. For each choice option, we consider a joint signal derived from a supervised evaluation, i.e. F1 score, on a subset of the training data Dtrain, and from a measure of the overall log probabilities on the test data Dtest. The assumption for the latter is that representative answer choices better estimate the task\u2019s distribution, resulting in overall higher log probabilities on unseen data of the target task: \u2211 y \u2211 \u03d5A\u2208\u03a6A \u2211 x\u2208Dtest( 1 T \u2211 log p\u03b8(\u03c8p(y) | x, \u03d5, \u03c8p(y)<t), with \u03c8p being the current answer choices configuration. We compute the final score for each candidate by summing the normalized scores of each metric over 3-fold cross-validation."
        },
        {
            "heading": "5 Evaluation",
            "text": ""
        },
        {
            "heading": "5.1 Experimental Setup",
            "text": "This section provides an overview of our experimental setup. We are sampling K training samples for each class yi \u2208 Y , for a total of K \u00d7 |Y| training samples3. We do not consider a validation set to exist for hyperparameter-tuning, following Alex\n2For example the input prompt and samples might have been encountered for NLI tasks, focusing on options working particularly well for this scenario.\n3While in Liu et al. (2022) samples are drawn randomly, i.e. not stratified, we largely adhere to the traditional N-WayK-shot classification setting, as data imbalance in training is an aspect to be explored separately.\net al. (2021). For baselines, and implementation specifics, including hyperparameters, see Appendix B. For used datasets, see Appendix A.\nDatasets. We conduct experiments on a total of 12 text classification datasets, spanning a total of 8 tasks. This collection is in essence a combination of evaluation datasets used in Liu et al. (2022) and Tunstall et al. (2022), minus datasets that we consider not traditional classification tasks, e.g. sentence completion, where the meaning of the class changes per instance.\nImplementation Details. AuT-Few largely follows T-Few (Liu et al., 2022) for finetuning, with some modifications to training and inference to increase robustness for our automated few-shot method. Instead of only learning rescaling vectors of the upstream model\u2019s weights ((IA)3), we additionally learn and re-scale decomposition matrices (LoRA), as proposed by Hu et al. (2022a). (IA)3 and LoRA are complementary and the gradient updates from both methods can be made persistent to the model\u2019s weights after training without inquiring additional inference costs over the upstream model itself. Another limitation of T-Few is its inference algorithm. T-Few selects a single template at random (c.f. Section 2) and it can be a poor approximation of the overall expectation, especially with noisy templates as used with AuT-Few. We instead run a Monte-Carlo approximation over all retrieved templates, computing a weighted average over the probabilities computed via each template.\nBaselines. In addition to the current state-of-theart few-shot learning method T-Few, we consider SetFit (Tunstall et al., 2022) (with a RoBERTA backbone), which is of particular relevance in our context, since it is the state-of-the-art efficient prompt-free few-shot method. We also compare against a fully-finetuned RoBERTaLARGE model, based on the baseline in Tunstall et al. (2022). The majority baseline is based on the class distribution in the test data."
        },
        {
            "heading": "5.2 Results",
            "text": "Results for K = 32 samples per class are shown in Table 1. Both T-Few and AuT-Few use T0-3B as the upstream model. We report accuracy on all datasets with the exception of Amazon-CF, where we report Matthew\u2019s correlation coefficient due to the skewed distribution, following Tunstall et al. (2022).\nAuT-Few outperforms T-Few (64.2 \u00b1 2.4) and SetFit (59.0 \u00b1 2.6), with an average score of 66.3 \u00b1 2.5. A trivial T-Few automation strategy that randomly draws answer choices from the training data (c.f Section 3) performs substantially worse than AuT-Few with much higher variability (57.7\u00b1 4.4). While AuT-Few has a higher average score than T-Few, the latter wins against AuT-Few on 8 out of 12 datasets. However, we observe a statistically significant difference4 on only 4 datasets. Out of these four datasets where we observe statistical significance, AuT-Few outperforms T-Few in three of them (WiC, Emotion, Amazon-CF).5 Moreover, we would like to emphasise that performing even comparable against T-Few is already a win since the latter uses multiple diverse handcrafted prompts for each target task while AuT-Few does not require any manual involvement by the user to optimize the prompt while maintaining comparable standard deviation.\nOn the blind test set with the best variant of T0 (T0++, 11B parameters) AuT-Few achieves an average score of 71.3 versus 70.5 for T-Few (with the same backbone), excluding WiC and WSC, as\n4We ran the two-sided Monte-Carlo permutation test with 10000 repetitions (p-value < 0.01). Significance for a dataset holds iff results are significant across all seeds.\n5Notably, the performance difference between AuT-Few and T-Few on WSC, the only dataset where AuT-Few performs substantially worse, is not statistically significant given our test: this can be explained by the very small sample size of the dataset\u2019s evaluation data of only 104 samples. Liu et al. (2022) also observed \u201cunstable results\" on WSC, see the discussion on this Github issue.\nthese datasets have been used to train T0 (see App. C.1 for detailed scores).\nWe note that the automated prompts are not always semantically coherent. As shown in Appendix D, automation choices for some datasets, such as mp3player and ipod for CR, appear odd, yet the model still achieves a very high score on them. This observation can be explained by our findings in section 3, identifying that some datasets such as CR and EnronSpam are particularly robust towards the task description and the answer choices. For CR, AuT-Few\u2019s cross-validation strategy for selecting the best answer choice subsequently measures almost identical scores for all three choice configurations (90.1, 89.8, 90.4 for the dataset, template-tailored, and topic-specific choices, respectively), resulting in the seemingly erroneously answer-choice selection.\nResults across Upstream Models & Efficiency. Results of AuT-Few with different upstream models, namely BART0, T0, and Flan-T5 are seen in Table 2. The results in the table are computed without Monte-Carlo approximation, resulting in a minor performance decline, yet simplifying the efficiency comparison. Datasets that are part of the instruction-tuning corpus of Flan-T5 or BART0 have been excluded (greyed out). BART0 being about 8 times smaller than T0 performs substantially worse, but it still substantially outperforms T-Few with BART0 and maintains a higher average score than SetFit. Flan-T5 performs on average the best on its unseen datasets, indicating the improved\ncapabilities of the model\u2019s much larger and diverse instruction-tuning. These results highlight the effectiveness of AuT-Few across upstream models of varying sizes.\nThe computational costs for training and inference are listed in Table 2. We follow the approach adopted by Liu et al. (2022) and Tunstall et al. (2022) to measure computational costs, namely FLOPs-per-token (Kaplan et al., 2020). AuT-Few requires about 7x the training cost of T-Few, yet remains computationally accessible, taking only a few hours to train on a single A10G GPU, since the number of training steps for few-shot PEFT is overall small. Similarly, while AuT-Few with BART0 takes 4.2x longer than SetFit, it still takes less than an hour of total training time. Importantly, during inference, AuT-Few is as efficient as T-Few (excluding Monte-Carlo approximation, otherwise scaling linearly with the number of retrieved templates). AuT-Few with BART0 is even more efficient than SetFit during inference, requiring only 60% of its computation while maintaining a competitive score.\nWe emphasize that while T-Few takes somewhat less computation than AuT-Few, T-Few requires significantly more human intervention, and human time is much more valuable than computer time. The difference of a couple hours of computer time is negligible when it can save orders of magnitude more human time and associated costs.\nVarying sample sizes. Figure 4 shows the performance of our baselines as well as Aut-Few over 16,\n32, and 64 samples, respectively. With K = 16, we observe slightly worse performance than T-Few with AuT-Few. The provided signal from only 16 samples is too noisy for our automation pipeline. With an increase in number of training samples follows a larger lead of AuT-Few over other models. While AuT-Few (T0) is on average 2.1 points better than T-Few with 32 samples, this lead increases to 3.1 for K = 64. Similar observation is made when comparing AuT-Few (BART0) with SetFit.\nReal-world evaluation: RAFT. RAFT (Alex et al., 2021) is a benchmark targeted towards evaluating few-shot classification methods. It consists of 11 datasets, from various domains, such as the legal or medical domain. In RAFT 50 randomly sampled training samples are provided, with a potentially imbalanced label distribution. We submitted predictions of AuT-Few with the 11B Flan-T5 backbone, with handcrafted prompts as provided by RAFT (AuT-Few (H)), as well as with our automated prompts (AuT-Few). We do not make any manual dataset adjustments, with the exception of Banking_77 as only a subset of the classes appears in its training data, c.f. App. C.2.\nResults are shown in Table 3. Our method with handcrafted prompts and the Flan-T5 upstream model achieves rank-1 with the overall highest average score. Our automated version achieves scores slightly below T-Few (the previously 2nd ranked system). This is largely due to AuT-Few\u2019s poor performance on a single dataset, Tweet-Eval-\nHate, as a result of improper selection of answer choices. However, AuT-Few has the best average rank across all five models with 2.45. It wins against T-Few on 7 out of 11 datasets. Furthermore, it has the highest overall win rate, winning against all other models we considered (including our approach with handcrafted prompts) on 4 out of 11 datasets, see Table 7. These results highlight AuTFew\u2019s robustness and generalizability to real-world classification tasks.\nAblation. Results of our ablation study for AuTFew with 32 samples per class are shown in Table 4. We ablate our template retrieval method by considering randomly selected templates from the instruction tuning KB, as well as template retrieval from the entire PromptSource collection of prompts. As seen both settings perform worse than AuT-Few, with higher standard deviation across seeds. While retrieving from the entire collection performs slightly better for tasks that appear in it (e.g. NLI, emotion classification), it strongly underperforms on unseen ones (e.g. WiC, Amazon-CF). Further, the ablation of the choice options shows that each definition of answer choices by itself performs worse than AuT-Few (including the label descriptions that appear in the dataset). Finally, we see that our modifications to T-Few\u2019s inference and training are effective, with both LoRA and (IA)3 PEFT performing worse individually. Note that AuT-Few still outperforms T-Few even when using only (IA)3, indicating AuT-Few\u2019s superiority without any architectural adjustments."
        },
        {
            "heading": "6 Conclusion",
            "text": "AuT-Few replaces hand-designed task-specific prompts with automated templates, and achieves state-of-the-art results on a wide range of datasets\nand tasks, and the best average rank across datasets on the RAFT benchmark. Machine learning, especially few-shot learning, is about automation. Although T-Few takes less computation, it requires hand-designed prompts which involves significant human intervention and expertise. Human-time is profoundly more valuable than computer time, and AuT-Few saves this valuable human time while still retaining computational tractability. Future work includes the identification of causes for the observations made in section 3, particularly for datasets that are completely unaffected by the prompt\u2019s design (e.g Enronspam and CR).\nLimitations\nThis work and the automation pipeline is constrained to classification tasks in English. The role of templates and answer choices is necessarily different for tasks such as natural language generation (e.g. summarization or question answering) where a single textual class representation does not exist. The proposed automated few-shot approach is not expected to work well under extremely low data regime or when training samples are highly imbalanced (i.e. < 8 samples per class) as some data signal is required for optimizing the choice space. While our evaluation aims to cover a diverse range of classification tasks, the list of evaluation tasks is not exhaustive. Subsequently, there is no guarantee that AuT-Few performs equally well on every unseen tasks, particular ones that divert strongly from tasks the model has seen during instruction tuning.\nEthics Statement\nOur paper makes state-of-the-art few-shot classification methods more accessible to non-experts for real-world problems. The goal of this work is not to replace human involvement in the deployment of AI systems but instead to shift human resources to\nother essential aspects of model deployment such as the analysis of data, biases, or system errors. We discussed the computational costs of our automation approach and show that they are comparable at similar model size with the most efficient few-shot systems, which themselves again are computationally much more efficient than full-data and full model fine-tuning, or in-context learning."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors would like to thank Lewis Tunstall for his help to submit AuT-Few\u2019s predictions to the RAFT leaderboard and Aditya Rawal for pointing us to relevant related work. We would also like to thank the anonymous reviewers for their time and effort giving us valuable feedback on our paper."
        },
        {
            "heading": "A Datasets",
            "text": "We conduct experiments on a total of 12 text classification datasets. The tasks we consider are 1) natural language inference: RTE (Dagan et al., 2005) , CB (de Marneffe et al., 2019), ANLI (Nie et al., 2020); 2) coreference resolution: WSC (Levesque et al., 2012); 3) word sense disambiguation: WiC (Pilehvar and Camacho-Collados, 2019); 4) counterfactual detection: Amazon-CF (O\u2019Neill et al., 2021); 5) sentiment classification: SST-5 (Socher et al., 2013), Customer reviews (CR) (Conneau and Kiela, 2018); 6) emotion classification: emotion (Saravia et al., 2018); and 7) spam detection: Enron (Metsis et al., 2006). All datasets are in English. Enron contains personal identifiable information, yet substantial efforts have been made to remove any integrity problems and samples of affected employees, see here for reference. Enron is an long-established dataset to use for classification problems and our use is in line with previous usages of it.\nB Implementation Details\nParameter-efficient fine-tuning via low-rank adaptation and rescaling While exclusively rescaling weights of an upstream model via IA3 has shown to perform remarkably well, the expressiveness of the fine-tuning process is restricted, due to \u2206h (the accumulated gradient update) being always of the form | W0 \u2212 \u03bbW0 |, with W0 being the weights of the upstream model and \u03bb being the rescaling vector. For tasks that require major adaptation capabilities, this might pose a hindrance. In contrast, LoRA explicitly models via decomposition matrices the gradient update \u2206h = BA, resulting in higher expressiveness (about 10x as many parameters as IA3), but has repeatably shown in our experiments to have substantially higher variability. We hence combine both PEFT strategies, by rescaling both the weights of the upstream model and the accumulated gradient updates jointly: h = \u03bb(W0x + BAx). After training, both \u03bb andBA can be applied toW0, making the weight updates persistent without inquiring any additional computation during inference. Following Liu et al. (2022), we pre-train the weights of the rescaling vectors in a similar fashion to the upstream model. While the authors only train the vectors for 100K steps, we observed further improvements when training them for longer (500K steps).\nInference via Monte-Carlo Approximation over Templates As outlined in section 3, in its current version the expectation over template and choice space is approximated during inference by randomly drawing a template from a collection of handcrafted ones. Besides being non-deterministic, the selected template might be a poor approximation of the overall expectation. Instead, we run a Monte-Carlo Approximation over the template space \u03a6A, by computing a weighted average over all retrieved templates:\ny\u0302 = argmaxyE\u03a6,\u03a8[p\u03b8(yi | x,\u03a6,\u03a8)]\n= argmaxy R\u2211 r=1 wrp\u03b8(yi | x, \u03d5r, \u03c8A),\nwith \u2211R\nr=1wr = 1. We determine the weights for each template by computing the log-likelihood of each template on Dtest and applying a softmax function on them, following the previously mentioned motivation.\nHyperparameters Since our joint PEFT method converges substantially faster than IA3 by itself, we set the number of total steps to 600 (contrary to 1000 used by T-Few). Further, for both T-Few and AuT-Few we use the following hyperparameters across all experiments: we use Adam, a learning rate of 1\u22123, cosine decay with a warmup ratio of 0.06, a learning rate decay of 1.0, and a batch size of 8. The contextual embeddings for template retrieval as well the topic-specific choices are generated using sentence-transformers\u2019 all-MiniLM-L6-v2 encoder model. For all main experiments, we set the number of retrieved templates to R = 5. The underlying prompt knowledge base used is PromptSource (Bach et al., 2022). For selecting the best answer choices, we split the training data using 3-fold cross-validation and train the upstream model with identical hyperparameters as our final model for every choice option.\nSystem & Code All models (550M, 3B, 11B parameters) are trained and run on a single A10G GPU with 23GB of memory by using gradient checkpointing, bfloat16 floating-point format, and in the case of the 11B model by offloading parameters using DeepSpeed 6. We produce results for the SetFit and finetune baseline using the associated repository7. We filter stopwords and punctuation from the vocabulary of topic-specific answer\n6https://github.com/microsoft/DeepSpeed 7https://github.com/huggingface/setfit\nchoices using NLTK (Bird and Loper, 2004). Our code and models will be made openly accessible under Apache License 2.0.\nBaselines In addition to the current state-of-theart (Liu et al., 2022), we consider SetFit (Tunstall et al., 2022), as well as a standard finetuned LM. SetFit is of particular relevance to us since it is the state-of-the-art prompt-free fewshot method, shown to perform competitively to T-Few in their experiments while being computationally substantially more efficient. In their comparison to T-Few, they use a very small variation of the sentence-transformer MPNET, consisting of only 110M, however, we observed substantially better performance with the larger ROBERTa sentencetransformer model (355M parameters). Hence, we report results on the latter model8. The traditionally finetuned model is a RoBERTaLARGE model, fullyfinetuned with an additional linear head, based on the baseline in (Tunstall et al., 2022)."
        },
        {
            "heading": "C Detailed Results",
            "text": "Detailed results of the experiment in Table 1 for different sample sizes are shown in Table 8.\nC.1 Results Blind Test Set\n8Albeit some sentence-transformer models are targeted for a certain domain, e.g. QA or NLI, in our experimental setup we aim to minimize human involvement, including model selection. Hence, the same pre-trained model is used across all experiments.\nC.2 Results RAFT RAFT consists of 11 datasets from different domains. The individual datasets included in RAFT are ADE Corpus V2 (medical case reports) (Gurulingappa et al., 2012), Banking77 (Casanueva et al., 2020), NeurIPS impact statement risks (Ashurst et al., 2022), Onestop English (Vajjala and Luc\u030cic\u0301, 2018), Overruling (legal domain) (Zheng et al., 2021), Systematic Review Inclusion (Saeri et al., 2022), Tai safety research (Riedel and Deibel, 2020), Terms of Service (Lippi et al., 2019), Tweet Eval Hate (Basile et al., 2019), and Twitter Complaints (Preotiuc-Pietro et al., 2019). All datasets are in English.\nSince only a small subset of the 77 classes appear in the training data of the Banking_77 dataset, we directly use the dataset\u2019s class representations for the answer choices. Banking_77 is strictly speaking a zero-shot and few-shot evaluation dataset and previous work such as SetFit that does not use a verbalizer at all also had to make use of the given class representations for that dataset9."
        },
        {
            "heading": "D Automated Choices",
            "text": "The generated and selected answer choices as used in AuT-Few with K = 32 and T0 as the upstream model on seed 0 are shown in Table 9."
        },
        {
            "heading": "E Automated Templates",
            "text": "The retrieved templates as used in AuT-Few with K = 32 and T0 as the upstream model on seed 0 are shown in Table 10.\n9https://towardsdatascience.com/ sentence-transformer-fine-tuning-setfit-\\ outperforms-gpt-3-on-few-shot-text-class\\ ification-while-d9a3788f0b4e"
        }
    ],
    "title": "Automated Few-shot Classification with Instruction-Finetuned Language Models",
    "year": 2023
}