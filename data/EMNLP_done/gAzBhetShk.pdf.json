{
    "abstractText": "In-context learning with large language models (LLMs) has recently caught increasing attention due to its superior few-shot performance on various tasks. However, its performance on text-to-SQL parsing still has much room for improvement. In this paper, we hypothesize that a crucial aspect of LLMs to improve for text-to-SQL parsing is their multi-step reasoning ability. Thus, we systematically study how to enhance LLMs\u2019 reasoning ability through chain of thought (CoT) style prompting, including the original chain-of-thought prompting (Wei et al., 2022b) and least-to-most prompting (Zhou et al., 2023). Our experiments demonstrate that iterative prompting as in Zhou et al. (2023) may be unnecessary for text-to-SQL parsing, and using detailed reasoning steps tends to have more error propagation issues. Based on these findings, we propose a new CoT-style prompting method for text-to-SQL parsing. It brings 5.2 and 6.5 point absolute gains on the Spider development set and the Spider Realistic set, respectively, compared to the standard prompting method without reasoning steps; 2.4 and 1.5 point absolute gains, compared to the least-to-most prompting method1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chang-You Tai"
        },
        {
            "affiliations": [],
            "name": "Ziru Chen"
        },
        {
            "affiliations": [],
            "name": "Tianshu Zhang"
        },
        {
            "affiliations": [],
            "name": "Xiang Deng"
        },
        {
            "affiliations": [],
            "name": "Huan Sun"
        }
    ],
    "id": "SP:6bcb8ad59eabe2cfe747bcb9fd687e5398074538",
    "references": [
        {
            "authors": [
                "Anthony Brohan",
                "Yevgen Chebotar",
                "Chelsea Finn",
                "Karol Hausman",
                "Alexander Herzog",
                "Daniel Ho",
                "Julian Ibarz",
                "Alex Irpan",
                "Eric Jang",
                "Ryan Julian"
            ],
            "title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "venue": "In Conference on Robot Learn-",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Shuaichen Chang",
                "Jun Wang",
                "Mingwen Dong",
                "Lin Pan",
                "Henghui Zhu",
                "Alexander Hanbo Li",
                "Wuwei Lan",
                "Sheng Zhang",
                "Jiarong Jiang",
                "Joseph Lilien"
            ],
            "title": "2023b. Dr. spider: A diagnostic evaluation benchmark towards text-to-sql robustness",
            "year": 2023
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Shijie Chen",
                "Ziru Chen",
                "Huan Sun",
                "Yu Su"
            ],
            "title": "2023a. Error detection for text-to-sql semantic parsing",
            "year": 2023
        },
        {
            "authors": [
                "Ziru Chen",
                "Shijie Chen",
                "Michael White",
                "Raymond Mooney",
                "Ali Payani",
                "Jayanth Srinivasa",
                "Yu Su",
                "Huan Sun."
            ],
            "title": "Text-to-SQL error correction with language models of code",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Compu-",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Deborah A Dahl",
                "Madeleine Bates",
                "Michael K Brown",
                "William M Fisher",
                "Kate Hunicke-Smith",
                "David S Pallett",
                "Christine Pao",
                "Alexander Rudnicky",
                "Elizabeth Shriberg."
            ],
            "title": "Expanding the scope of the atis task: The atis-3 corpus",
            "venue": "Human Language Tech-",
            "year": 1994
        },
        {
            "authors": [
                "Xiang Deng",
                "Ahmed Hassan",
                "Christopher Meek",
                "Oleksandr Polozov",
                "Huan Sun",
                "Matthew Richardson."
            ],
            "title": "Structure-grounded pretraining for text-to-sql",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Ahmed Elgohary",
                "Saghar Hosseini",
                "Ahmed Hassan Awadallah."
            ],
            "title": "Speak to your parser: Interactive text-to-SQL with natural language feedback",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2065\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Ahmed Elgohary",
                "Christopher Meek",
                "Matthew Richardson",
                "Adam Fourney",
                "Gonzalo Ramos",
                "Ahmed Hassan Awadallah"
            ],
            "title": "NL-EDIT: Correcting semantic parse errors through natural",
            "year": 2021
        },
        {
            "authors": [
                "Nitish Gupta",
                "Mike Lewis."
            ],
            "title": "Neural compositional denotational semantics for question answering",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2152\u20132161.",
            "year": 2018
        },
        {
            "authors": [
                "SU Hongjin",
                "Jungo Kasai",
                "Chen Henry Wu",
                "Weijia Shi",
                "Tianlu Wang",
                "Jiayi Xin",
                "Rui Zhang",
                "Mari Ostendorf",
                "Luke Zettlemoyer",
                "Noah A Smith"
            ],
            "title": "Selective annotation makes language models better fewshot learners",
            "year": 2023
        },
        {
            "authors": [
                "Srinivasan Iyer",
                "Ioannis Konstas",
                "Alvin Cheung",
                "Jayant Krishnamurthy",
                "Luke Zettlemoyer."
            ],
            "title": "Learning a neural semantic parser from user feedback",
            "venue": "55th Annual Meeting of the Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Yuntao Li",
                "Bei Chen",
                "Qian Liu",
                "Yan Gao",
                "Jian-Guang Lou",
                "Yan Zhang",
                "Dongmei Zhang."
            ],
            "title": "what do you mean by that?\u201d a parser-independent interactive approach for enhancing text-to-SQL",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods",
            "year": 2020
        },
        {
            "authors": [
                "Aiwei Liu",
                "Xuming Hu",
                "Lijie Wen",
                "Philip S Yu."
            ],
            "title": "A comprehensive evaluation of chatgpt\u2019s zero-shot text-to-sql capability",
            "venue": "arXiv preprint arXiv:2303.13547.",
            "year": 2023
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Computing Surveys, 55(9):1\u201335.",
            "year": 2023
        },
        {
            "authors": [
                "Joan C. Miller",
                "Clifford J. Maloney."
            ],
            "title": "Systematic mistake analysis of digital computer programs",
            "venue": "Commun. ACM, 6(2):58\u201363.",
            "year": 1963
        },
        {
            "authors": [
                "Sewon Min",
                "Victor Zhong",
                "Luke Zettlemoyer",
                "Hannaneh Hajishirzi."
            ],
            "title": "Multi-hop reading comprehension through question decomposition and rescoring",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2019
        },
        {
            "authors": [
                "Lingbo Mo",
                "Ashley Lewis",
                "Huan Sun",
                "Michael White."
            ],
            "title": "Towards transparent interactive semantic parsing via step-by-step correction",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 322\u2013342, Dublin, Ireland. Association",
            "year": 2022
        },
        {
            "authors": [
                "Arpit Narechania",
                "Adam Fourney",
                "Bongshin Lee",
                "Gonzalo Ramos."
            ],
            "title": "Diy: Assessing the correctness of natural language to sql systems",
            "venue": "26th International Conference on Intelligent User Interfaces, pages 597\u2013607.",
            "year": 2021
        },
        {
            "authors": [
                "Ansong Ni",
                "Srini Iyer",
                "Dragomir Radev",
                "Veselin Stoyanov",
                "Wen-Tau Yih",
                "Sida Wang",
                "Xi Victoria Lin."
            ],
            "title": "LEVER: Learning to verify language-to-code generation with execution",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Mohammadreza Pourreza",
                "Davood Rafiei"
            ],
            "title": "Din-sql: Decomposed in-context learning of textto-sql with self-correction",
            "year": 2023
        },
        {
            "authors": [
                "Jiexing Qi",
                "Jingyao Tang",
                "Ziwei He",
                "Xiangpeng Wan",
                "Yu Cheng",
                "Chenghu Zhou",
                "Xinbing Wang",
                "Quanshi Zhang",
                "Zhouhan Lin."
            ],
            "title": "RASAT: Integrating relational structures into pretrained Seq2Seq model for text-to-SQL",
            "venue": "Proceedings of the 2022 Con-",
            "year": 2022
        },
        {
            "authors": [
                "Nitarshan Rajkumar",
                "Raymond Li",
                "Dzmitry Bahdanau."
            ],
            "title": "Evaluating the text-to-sql capabilities of large language models",
            "venue": "arXiv preprint arXiv:2204.00498.",
            "year": 2022
        },
        {
            "authors": [
                "Torsten Scholak",
                "Nathan Schucher",
                "Dzmitry Bahdanau."
            ],
            "title": "Picard: Parsing incrementally for constrained auto-regressive decoding from language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Peter Shaw",
                "Ming-Wei Chang",
                "Panupong Pasupat",
                "Kristina Toutanova"
            ],
            "title": "Compositional generalization and natural language variation: Can a semantic parsing approach handle both",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Bailin Wang",
                "Richard Shin",
                "Xiaodong Liu",
                "Oleksandr Polozov",
                "Matthew Richardson."
            ],
            "title": "Rat-sql: Relation-aware schema encoding and linking for textto-sql parsers",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Jason Wei",
                "Yi Tay",
                "Rishi Bommasani",
                "Colin Raffel",
                "Barret Zoph",
                "Sebastian Borgeaud",
                "Dani Yogatama",
                "Maarten Bosma",
                "Denny Zhou",
                "Donald Metzler"
            ],
            "title": "Emergent abilities of large language models",
            "venue": "Transactions on Machine Learning Research",
            "year": 2022
        },
        {
            "authors": [
                "Tomer Wolfson",
                "Daniel Deutch",
                "Jonathan Berant."
            ],
            "title": "Weakly supervised text-to-sql parsing through question decomposition",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 2528\u20132542.",
            "year": 2022
        },
        {
            "authors": [
                "Tomer Wolfson",
                "Mor Geva",
                "Ankit Gupta",
                "Matt Gardner",
                "Yoav Goldberg",
                "Daniel Deutch",
                "Jonathan Berant."
            ],
            "title": "Break it down: A question understanding benchmark",
            "venue": "Transactions of the Association for Computational Linguistics, 8:183\u2013198.",
            "year": 2020
        },
        {
            "authors": [
                "Navid Yaghmazadeh",
                "Yuepeng Wang",
                "Isil Dillig",
                "Thomas Dillig."
            ],
            "title": "Sqlizer: query synthesis from natural language",
            "venue": "Proceedings of the ACM on Programming Languages, 1(OOPSLA):1\u201326.",
            "year": 2017
        },
        {
            "authors": [
                "Ziyu Yao",
                "Yu Su",
                "Huan Sun",
                "Wen-tau Yih."
            ],
            "title": "Model-based interactive semantic parsing: A unified framework and a text-to-SQL case study",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
            "year": 2019
        },
        {
            "authors": [
                "Ziyu Yao",
                "Yiqi Tang",
                "Wen-tau Yih",
                "Huan Sun",
                "Yu Su."
            ],
            "title": "An imitation game for learning semantic parsers from user interaction",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Tao Yu",
                "Chien-Sheng Wu",
                "Xi Victoria Lin",
                "Yi Chern Tan",
                "Xinyi Yang",
                "Dragomir Radev",
                "Caiming Xiong"
            ],
            "title": "Grappa: Grammar-augmented pre-training for table semantic parsing",
            "venue": "In International Conference on Learning Representations",
            "year": 2021
        },
        {
            "authors": [
                "Tao Yu",
                "Rui Zhang",
                "Kai Yang",
                "Michihiro Yasunaga",
                "Dongxu Wang",
                "Zifan Li",
                "James Ma",
                "Irene Li",
                "Qingning Yao",
                "Shanelle Roman"
            ],
            "title": "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql",
            "year": 2018
        },
        {
            "authors": [
                "John M Zelle",
                "Raymond J Mooney."
            ],
            "title": "Learning to parse database queries using inductive logic programming",
            "venue": "Proceedings of the national conference on artificial intelligence, pages 1050\u20131055.",
            "year": 1996
        },
        {
            "authors": [
                "Jichuan Zeng",
                "Xi Victoria Lin",
                "Steven C.H. Hoi",
                "Richard Socher",
                "Caiming Xiong",
                "Michael Lyu",
                "Irwin King."
            ],
            "title": "Photon: A robust cross-domain textto-SQL system",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Ruiqi Zhong",
                "Tao Yu",
                "Dan Klein."
            ],
            "title": "Semantic evaluation for text-to-sql with distilled test suites",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 396\u2013411.",
            "year": 2020
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Claire Cui",
                "Olivier Bousquet",
                "Quoc V Le",
                "Ed H. Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "The",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Text-to-SQL parsing, the task of mapping a natural language utterance to a SQL query, has found wide applications in building language agents for databases and piqued significant research interest in recent years (Deng et al., 2021; Yu et al., 2021; Rajkumar et al., 2022; Hongjin et al., 2023; Ni et al., 2023). To develop a text-to-SQL parser, a prevalent approach is to collect labeled data and train a model via supervised learning (Shaw et al., 2021; Scholak et al., 2021). While effective, this approach necessitates a considerable amount of training data, which\n\u2217Equal Contribution. \u2020Corresponding authors.\n1We include all our prompts in the appendix.\nis costly to obtain because annotating SQL queries requires programming expertise.\nAs an alternative to supervised learning, incontext learning (Brown et al., 2020), an emergent capability of large language models (LLMs), alleviates the need for large-scale data. With only a few examples, in-context learning enables LLMs to demonstrate performance comparable to or even better than fully supervised models on many NLP tasks, such as question answering, machine translation, and natural language inference (Chowdhery et al., 2022; Kojima et al., 2022; Wei et al., 2022b,a; Brohan et al., 2023). When applied to text-to-SQL parsing, in-context learning has also shown encouraging results (Rajkumar et al., 2022; Chang et al., 2023b; Liu et al., 2023a), but there is still much room for improvement.\nWe hypothesize that a crucial aspect of LLMs to improve for text-to-SQL parsing is their multistep reasoning ability. Even for a seemingly simple question, such as \u201cWhat is the ID of Kyle,\u201d a model has to ground it to the given database schema, infer the relational algebra among schema items, and construct syntactically correct SQL clauses. To enhance LLMs\u2019 reasoning capabilities, chain of thought (CoT) style prompting methods (Wei et al., 2022b; Zhou et al., 2023) are proposed and have shown promising results. However, how to apply CoT-style prompting to text-to-SQL parsing remains under-explored, and we fill this gap by systematically exploring CoT-style prompting for textto-SQL parsing. Specifically, we seek to answer two research questions: (RQ1) Which prompting style is better, generating all reasoning steps in one pass, or iterative prompting and problem solving? (RQ2) Do more detailed reasoning steps lead to better results for text-to-SQL parsing?\nTo address these questions, we adapt two widely used prompting methods for text-to-SQL parsing. As the first method, we apply chain-of-thought prompting (Wei et al., 2022b) by drawing an anal-\nogy between its problem-solving process and the execution procedure of a SQL query (Figure 1A). Referring to the logical execution order of SQL clauses (Narechania et al., 2021), we compose the intermediate execution steps in natural language and prompt LLMs to derive them before generating the SQL query. For the second method, we follow Zhou et al. (2023) to apply least-to-most prompting in two stages: (1) problem reduction: generate a series of sub-questions from the original question and (2) problem solving: iteratively translate each subquestion into its corresponding SQL query, with the original question as the last sub-question, as shown in Figure 1B. With a careful analysis (Section 5.2), we find that directly applying these two methods for text-to-SQL parsing tends to introduce error propagation issues frequently. Also, the iterative process in least-to-most prompting incurs more computational costs to generate each SQL query.\nTherefore, we propose a new CoT-style prompting method called question-decomposition prompting (QDecomp, Figure 1C). Similar to chainof-thought prompting, QDecomp generates a sequence of reasoning steps followed by the natural\nlanguage question in one pass. Instead of generating the intermediate execution steps, we instruct LLMs to decompose the original complex question, akin to the problem reduction stage in least-to-most prompting. Furthermore, to help LLMs ground database schemas, we design a variant of question decomposition prompting (QDecomp+InterCOL, Figure 1D) by incrementally including the table and column names involved in each sub-question.\nWe conduct comprehensive evaluations on two cross-domain text-to-SQL datasets, Spider (Yu et al., 2018) and Spider Realistic (Deng et al., 2021). Compared to the standard prompting method without reasoning steps, QDecomp + InterCOL brings 5.2 and 6.5 point absolute gains on the Spider development set and the Spider Realistic set, respectively. It also brings 2.4 and 1.5 point absolute gains compared to least-to-most prompting. Our results suggest that it may be unnecessary to perform iterative prompting, which is also computationally costly (RQ1). Besides, our analysis shows that our QDecomp+InterCOL method reduces the chance of error propagation by providing less detailed reasoning steps and generating\nthe SQL query in one pass (RQ2). Meanwhile, it includes key schema information in reasoning, which is still beneficial to database grounding. Further, we evaluate the robustness of our proposed methods by varying the number, selection, and format of in-context examples, providing useful guidelines for designing text-to-SQL prompting strategies. We also extend our evaluation to three single-domain datasets (Zelle and Mooney, 1996; Iyer et al., 2017; Yaghmazadeh et al., 2017) and show our proposed method can achieve strong performance consistently across different datasets."
        },
        {
            "heading": "2 Related Work",
            "text": "LLM and CoT-Style Prompting. As large language models (LLMs) advance (Brown et al., 2020; Chowdhery et al., 2022), in-context learning emerged as a new paradigm in natural language processing (Liu et al., 2023b). Although LLMs can achieve outstanding performance by prompting them with few-shot examples in context, they struggle with tasks that require multi-step reasoning. As a solution, Wei et al. (2022b) proposed chain-ofthought prompting. By explicitly describing intermediate reasoning steps to answer a complex question in the prompts, chain-of-thought prompting improves the accuracy of LLMs by a large margin across many natural language reasoning tasks. Besides, Zhou et al. (2023) proposed least-to-most prompting to solve complex problems in two stages. The method first prompts LLMs to generate a list of sub-questions as a decomposition of the given problem. Then, it uses the sub-questions to guide LLMs to incrementally solve each of them and derive a correct final answer. However, how to apply these two CoT-style prompting methods to text-to-SQL parsing remains under-explored.\nWe fill this gap by systematically exploring several CoT-style prompting methods for the task. In particular, we propose a new CoT-style prompting method that guides LLMs to perform reasoning via question decomposition. Question decomposition is a method that converts a complex problem into a sequence of simpler sub-questions (Gupta and Lewis, 2018; Min et al., 2019). Our work refers to existing question decomposition methods for textto-SQL parsing (Wolfson et al., 2020, 2022) and presents a novel CoT-style prompting method to improve LLMs\u2019 performance. We conduct comprehensive experiments and show that our question decomposition prompting outperforms chain-\nof-thought prompting and least-to-most prompting on several text-to-SQL datasets. Our experiments validate our hypothesis that text-to-SQL parsing indeed requires multi-step reasoning, and carefully designed CoT-style prompting can help LLMs achieve higher parsing accuracy.\nText-to-SQL Semantic Parsing. Text-to-SQL semantic parsing has long been studied to build language agents for database applications (Dahl et al., 1994; Zelle and Mooney, 1996). Since the release of Spider (Yu et al., 2018), a cross-database text-to-SQL benchmark, many parsers have been developed on top of language models to better understand various database schemas (Wang et al., 2020; Yu et al., 2021; Deng et al., 2021). Recent work starts to explore the potential of LLMs, such as Codex (Chen et al., 2021), in text-to-SQL parsing by including database schemas in the prompts (Rajkumar et al., 2022) or retrieving similar questions as few-shot examples (Hongjin et al., 2023). Orthogonal to these methods, our question decomposition prompting teaches LLM to perform multistep reasoning for text-to-SQL parsing without additional engineering efforts. With a few in-context examples, an LLM, such as Codex in our experiments, can learn to decompose natural language questions and predict table and column names (Section 3) incrementally in each step.\nOur method demonstrates comparable performance to RASAT+PICARD (Qi et al., 2022), a fine-tuned text-to-SQL parser, on the Spider development set without using relational structures or constrained decoding. Compared to other LLMbased methods, it achieves better execution accuracy than DIN-SQL (Pourreza and Rafiei, 2023) on the Spider development set in a single pass, while DIN-SQL requires iterative prompting. Although our method shows lower execution accuracy than LEVER (Ni et al., 2023), we note that LEVER\u2019s verifier model is fine-tuned on the full Spider training set, which may have extra advantages over our method. Also, LEVER uses the execution results of SQL queries, which provides extra information for better database grounding. We leave the incorporation of database contents beyond table and column names into our method as future work."
        },
        {
            "heading": "3 Prompting for Multi-Step Reasoning in Text-to-SQL",
            "text": "In this section, we outline three CoT-style prompting methods that teach an LLM to perform\nmulti-step reasoning. We first describe how we adapt chain-of-thought and least-to-most prompting for text-to-SQL parsing. Then, we propose a novel prompting method, question decomposition prompting (QDecomp), and its variant QDecomp+InterCOL. Figure 1 demonstrates different prompting methods, and we provide more examples in Appendix A. For all experiments, we use Codex (Chen et al., 2021), code-davinci-002, as the LLM. The experiments were conducted between January and March 2023 through OpenAI API, using greedy decoding with temperature 0."
        },
        {
            "heading": "3.1 Chain-of-Thought Prompting",
            "text": "Chain-of-thought prompting (Wei et al., 2022b) aims to improve LLMs\u2019 reasoning ability by generating a series of intermediate steps before predicting the final answer. For text-to-SQL parsing, one challenge is how to come up with the reasoning steps to predict the SQL query (i.e., final answer in our case). In our work, we use each clause in the SQL query to compose a reasoning step in CoT prompting. Specifically, inspired by Narechania et al. (2021), we use natural language templates to describe each SQL clause and chain them in the logical execution order of the SQL query. For example, the logical execution order for the SQL query in Figure 1A is first the FROM clause, then the WHERE clause, and finally the SELECT clause. Following this order, we assemble the natural language description of each clause in the query to compose its CoT reasoning steps."
        },
        {
            "heading": "3.2 Least-to-Most Prompting",
            "text": "Unlike chain-of-thought prompting, which instructs LLMs to generate all reasoning steps in a single pass, least-to-most prompting (Zhou et al., 2023) tackles complex questions by prompting LLMs in two stages: problem reduction and problem solving. During problem reduction, it prompts the LLM to generate a series of sub-questions from the original complex question. During problem solving, it prompts the LLM with one sub-question at a time and iteratively builds up the final solution.\nTo derive the sub-questions for problem reduction, we segment the original question following three principles: (1) If the question has multiple sentences, we treat each sentence as a sub-question. (2) We further decompose each sentence by conjunction words (such as \u201cand,\u201d \u201cor,\u201d and \u201cbut\u201d) and prepositions (such as \u201cfor,\u201d \u201cwith,\u201d and \u201cwithout\u201d).\n(3) For each decomposition, we remove words and phrases that may leak the information in any subsequent questions. This segmentation allows the LLM to focus on parsing each sub-question, thereby decreasing the complexity of the original problem (Wolfson et al., 2022).\nFor instance, the question \u201cShow first name, last name, age for all female students. Their sex is F.\u201d in Figure 1B would derive two sub-questions: (a) \u201cShow first name, last name, age for all students.\u201d (b) \u201cShow first name, last name, age for all female students. Their sex is F.\u201d This decomposition follows principle (1) and (3) by removing the second sentence and the information-leaking word \u201cfemale\u201d from the original question to construct the first step. For the first sub-question, the LLM only needs to construct the SELECT and FROM clauses. Then for the second sub-question, the LLM can build upon the SQL query generated for the first sub-question, and focus solely on the WHERE clause."
        },
        {
            "heading": "3.3 Question Decomposition Prompting",
            "text": "We propose a new prompting method, question decomposition prompting (QDecomp). Similar to chain-of-thought, QDecomp generates intermediate reasoning steps and the final SQL query in a single pass. Instead of using the logical execution procedure of SQL as in CoT, we follow the problem reduction stage in least-to-most prompting and instruct the LLM to decompose the original complex question as the reasoning steps. Through this design, we hope to explore (1) the potential advantage of using question decomposition over the logical execution procedure of SQL clauses for composing reasoning steps; (2) whether an iterative process as in least-to-most prompting is necessary.\nOn top of that, we propose a variant, QDecomp+InterCOL, to alleviate the well-known table/column linking issue in text-to-SQL parsing (Wang et al., 2020). Specifically, we augment the in-context examples to prompt the LLM to identify any corresponding table/column names when generating each sub-question. Given a subquestion and its corresponding SQL parse, we annotate all table-column pairs mentioned in the parse as ground-truth. For star operators (*), we sample a random column from tables mentioned in the same (sub-)query. If a table-column pair has been mentioned in the SQL parse of a sub-question, we would exclude it from the annotations of all subsequent steps. If a sub-question does not have\nany table-column pairs to annotate, we randomly choose one pair from preceding steps.\nWe include examples of these two methods in Figure 1C and 1D. Following the same decomposition method in least-to-most prompting, the example has two sub-questions. In Figure 1D, for the first sub-question, \u201cShow first name, last name, age for all students,\u201d we expect the model to highlight the table \u201cstudent\u201d and its columns \u201cfname,\u201d \u201clname,\u201d and \u201cage,\u201d as they appear in the SQL parse of this sub-question. Then, for the follow-up question, the model is expected to identify the table \u201cstudent\u201d and its column \u201csex,\u201d which is not mentioned in the previous step.\nIn addition to the prompting methods mentioned above, we also include the standard prompting method as the baseline in our experiments. It uses question-SQL pairs as in-context examples to prompt LLMs to directly parse a natural language question to its corresponding SQL query without generating any intermediate reasoning step."
        },
        {
            "heading": "4 Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "Spider (Yu et al., 2018). Spider is a commonly used benchmark to evaluate text-to-SQL parsing in a cross-database setting, which requires models to generalize to novel database schemas. The dataset consists of 7,000 question-query pairs in the training set and 1,034 pairs in the development set, covering 200 different databases and 138 domains. In this paper, due to the unavailability of the test set, we evaluate on the Spider development set to demonstrate the effectiveness of our question decomposition prompting methods.\nSpider Realistic (Deng et al., 2021). Spider Realistic is a more challenging version of the Spider development set. It modifies the natural language questions in Spider by removing or paraphrasing explicit mentions of column names to generate a more realistic dataset that reflects real-world scenarios, where questions rarely contain explicit mentions of column names. The final dataset comprises a total of 508 question-query pairs."
        },
        {
            "heading": "4.2 In-context Example Selection",
            "text": "To show the robustness of question decomposition prompting, we consider two ways of choosing in-context examples: random selection and\ndifficulty-based selection. In our main results, we use random selection for its simplicity and ease of replication. Additionally, in Section 5.3, we compare results obtained using random selection with those obtained using difficulty-based selection.\nFor random selection, we uniformly sample incontext examples from the Spider training set at random. For difficulty-based selection, we first group the Spider training examples into four difficulty levels, pre-defined by Yu et al. (2018), including easy, medium, hard, and extra-hard. Then, we devise three methods to randomly select in-context examples based on their difficulties: (G1) sampling an equal number of examples at each difficulty level, (G2) sampling the same number of examples from the hard level and the extra-hard level respectively, and (G3) sample all examples from the extra-hard level."
        },
        {
            "heading": "4.3 Prompt Formats",
            "text": "We also experiment with two prompt formats introduced by Rajkumar et al. (2022), API Docs and Create Table + Select 3. Both formats have their own advantages and can be utilized together with any prompting method in Section 3.\nAPI Docs format represents database schemas as Python API comments, which only includes the table and column names. This format reduces the prompt length for each example, so we may include more in-context demonstrations from databases in different domains to increase diversity. In comparison, Create Table + Select 3 format adheres more closely to the SQLite standards, but with much longer prompts2. It represents a database schema using the CREATE TABLE command, which provides more information, such as column data types and foreign key declaration. Besides, this format includes the results of executing SELECT * FROM T LIMIT 3 for each table T in the database as SQL comments. In Section 5.3, we show that API Docs format can achieve competitive performance compared to the Create Table + Select 3 format. Thus, we primarily use the API Docs format in our experiments due to its efficiency."
        },
        {
            "heading": "4.4 Evaluation Metric",
            "text": "We use test-suite execution accuracy (Zhong et al., 2020) to evaluate different prompting methods, incontext example selection strategies, and prompt\n2https://platform.openai.com/examples/ default-sql-translate\nformats. Leveraging the idea of \u201ccode coverage\u201d in software testing (Miller and Maloney, 1963), the metric synthesizes a large number of databases as \u201ctest cases\u201d and compares the execution results of the predicted and gold SQL queries on all of them. In this way, test-suite accuracy reduces the number of false positives (i.e., semantically different SQL queries that happen to have the same execution result) in standard execution accuracy, which compares execution results on only one database. As shown in Table 1, standard prompting\u2019s testsuite accuracy falls behind least-to-most prompting. However, their standard execution accuracy results are very close, which might be misleading."
        },
        {
            "heading": "5 Results and Analysis",
            "text": "By analyzing our experimental results, we seek to answer the following two research questions:\n\u2022 RQ1: Which prompting style is better, generating all reasoning steps in one pass, or iterative prompting and problem solving?\n\u2022 RQ2: Do more detailed reasoning steps lead to better results for text-to-SQL parsing?"
        },
        {
            "heading": "5.1 Main Results",
            "text": "Through comprehensive experiments on Spider Dev and Spider Realistic (Table 1), we show that our proposed question decomposition (QDecomp)\nprompting and its variant (QDecomp+InterCOL) consistently outperform two existing methods, chain-of-thought and least-to-most prompting. Specifically, QDecomp+InterCOL achieves 68.4% test-suite accuracy on the Spider development set and 56.5% on the Spider Realistic set. Compared to the standard prompting, it brings 5.2% and 6.5% point absolute gains, respectively. Compared to least-to-most prompting (the second best method), it brings 2.4% and 1.5% point absolute gains. Furthermore, when using extra-hard (G3) in-context examples, we can improve the execution accuracy of QDecomp+InterCOL prompting to 78.2%, which is comparable to RASAT+PICARD (Qi et al., 2022), a strong fine-tuned text-to-SQL parser. In contrast, least-to-most prompting does not gain too much execution accuracy (73.8%) from G3 incontext examples and even has decreased test-suite accuracy (63.3%). We will present more analysis on this contrast in Section 5.3.\nAdditionally, the experiments show that iteratively solving a series of sub-questions may not be necessary for text-to-SQL parsing (RQ1). Although chain-of-thought prompting (56.8%) underperforms least-to-most prompting (66.0%) on the Spider development set, these two methods have several distinct designs other than iterative prompting, so we cannot directly answer RQ1 by comparing them. With our QDecomp prompting,\nwe show that generating sub-questions and the SQL query in a single pass can also achieve improved accuracy. Thus, iterative prompting, which is computationally costly, is not necessary when prompting LLMs to reason for text-to-SQL parsing.\nAnother interesting finding is that chain-ofthought prompting performs even worse than the standard prompting method. We analyze the reason in the next section, which helps answer RQ2."
        },
        {
            "heading": "5.2 Error Analysis",
            "text": "We conduct a quantitative error analysis of all four prompting methods with the component matching accuracy (Yu et al., 2018) on the Spider development set. Component matching accuracy is a finegrained exact match metric that evaluates five SQL components, including SELECT clauses, WHERE clauses, GROUP BY clauses, ORDER BY clauses, and KEYWORDS (all SQL keywords, operators, and column names). Since exact match is too strict, we\nalso consider a component to be correct if the whole SQL query\u2019s test-suite accuracy is 1.\nAs shown in Table 2, our QDecomp and QDecomp+InterCOL prompts achieve better performance than other CoT-style prompting methods across all five SQL components. Further analysis shows that chain-of-thought prompting underperforms standard prompting because it provides very detailed reasoning steps. Translating such detailed steps is error-prone and incurs more error propagation issues. For example, in Table 3, Codex follows its reasoning steps faithfully to generate the corresponding SQL query, but the reasoning steps themselves have several errors, such as choosing the \u201cbreed_name\" column instead of the \u201cname\" column in the SELECT clause. Least-tomost prompting makes improvements by providing reasoning steps at a higher level (via the problem reduction phase). However, it sometimes still cannot translate a sub-question into the correct SQL\nquery, especially when involving hard components, such as JOIN clauses, GROUP BY clauses, and ORDER BY clauses (Table 2). We include an error example in Table 4. As a result, the errors are propagated to subsequent reasoning steps, leading to an incorrect final SQL parse. In contrast, QDecomp+InterCOL prompting outperforms these two methods because it does not instruct Codex to generate detailed reasoning steps or intermediate SQL queries. In this way, it reduces the possibility of accumulating mistakes in reasoning steps."
        },
        {
            "heading": "5.3 Robustness to Prompt Design",
            "text": "To further validate our conclusions in the main experiments, we conduct additional experiments to test the robustness of all four prompting methods in this section. Because chain-of-thought prompting already under-performs the standard prompting without reasoning, we omit this method in this and the next section.\nSelection of In-Context Examples. Besides random selection, we evaluate the efficacy of QDecomp+InterCOL and other prompting methods with in-context examples at various difficulty levels. As Table 5 suggests, QDecomp+InterCOL enables Codex to learn to reason for SQL queries at different difficulty levels from in-context examples. When using G1 examples, Codex learns to generate SQL queries and reasoning steps of various lengths from G1 examples. Thus, it is less likely to generate redundant SQL clauses or reasoning steps and achieves the highest accuracy for SQL queries at easy and medium level. When using G3 examples,\nCodex obtains the best performance for hard and extra-hard SQL queries. We conjecture that QDecomp+InterCOL teaches Codex to become better at generating SQL queries at difficulty levels similar to the in-context examples.\nBase on the conjecture, we extend this experiment to compare QDecomp+InterCOL and other prompting methods. As shown in Table 6, QDecomp+InterCOL prompting achieves the best performance across all settings, demonstrating its robustness. However, least-to-most prompting does not benefit from G1 or G3 examples and shows decreased accuracy. We believe this performance drop is because its iterative prompting generates one reasoning step at a time, which is relatively independent of the overall reasoning step length.\nNumber of In-Context Examples. Intuitively, performances of all prompting methods improve as the number of in-context examples increases (Table 7). We found that our QDecomp prompting is the most robust and consistently achieves better performance than standard prompting. However, least-to-most prompting underperforms standard prompting when the number of examples is less than 8. In addition, we note that in our preliminary experiments, further increasing the number of examples only leads to minor gains. Hence, we use 8 in-context examples in our main experiments.\nFormat of In-Context Examples. Finally, we show the performance of Codex using two prompt formats, API docs and Create Table + Select 3\n(Table 8). Due to OpenAI\u2019s prompt length restrictions, we use 4 in-context examples in this experiment. Although Create Table + Select 3 format includes foreign key information and database content, compared with API docs, it brings a negligible improvement in performance for standard prompting and a (slight) decrease for QDecomp and QDecomp+InterCOL prompting methods. Nonetheless, QDecomp is still the best prompting method under this format. Therefore, we use API docs as our default format due to its efficiency and leave further experiments for future work."
        },
        {
            "heading": "5.4 Results on Other Text-to-SQL Datasets",
            "text": "Besides the Spider datasets, we further compare QDecomp (+InterCOL) to standard and least-tomost prompting on other datasets including GeoQuery (Zelle and Mooney, 1996; Iyer et al., 2017), IMDB (Yaghmazadeh et al., 2017), and Yelp (Yaghmazadeh et al., 2017). Since the database schema and SQL queries in these datasets are more complex than those in the Spider datasets, we also use 4-shot in-context examples in this experiment.\nAs shown in Table 9, QDecomp (+InterCOL) consistently achieves the best performance for all three datasets. Moreover, we observe that least-tomost prompting underperforms standard prompting on IMDB and Yelp, which may be related to both iterative prompting and error propagation (Section 5.2). For example, least-to-most prompting would decompose the question \u201cFind all movies that were produced by Netflix\u201d into two sub-questions: 1) \u201cFind all movies\u201d and 2) \u201cFind all movies that were produced by Netflix.\u201d Then, in the iterative solving stage, there are many correct SQL queries using different tables and columns for the first sub-question. Without seeing the second sub-question, it is hard for the LLM to pinpoint the correct ones. As a result, the LLM would include redundant or wrong schema items in the SQL parse for the first subquestion, which are propagated to subsequent steps. Since QDecomp (+InterCOL) instructs the LLM to generate the SQL query after all sub-questions are\nderived, it maintains a global view of all reasoning steps and mitigates such error propagation issues."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "In this paper, we systematically explore CoT-style prompting to enhance LLMs\u2019 reasoning capability for text-to-SQL parsing. We design reasoning steps in order to apply two existing methods, chain-ofthought and least-to-most prompting, and propose new question decomposition prompting methods. Through comprehensive experiments, we demonstrate: (1) Iterative prompting may be not necessary for reasoning in text-to-SQL parsing. (2) Using detailed reasoning steps (in CoT) or intermediate SQL queries (in least-to-most prompting) is error-prone and aggravates error propagation.\nOur question decomposition prompting serves as one of the first attempts to mitigate the error propagation issue in LLMs\u2019 multi-step reasoning, and we highlight this problem as a meaningful future direction. For example, we can further reduce errors in intermediate reasoning steps by incorporating our method into an interactive semantic parsing framework (Yao et al., 2019, 2020; Li et al., 2020; Zeng et al., 2020; Chen et al., 2023a,b). Since the decomposed sub-questions are in natural language, this interactive approach enables database users to easily spot the errors in each sub-question. Then, they can collaborate with LLMs by editing the subquestions directly or providing natural language feedback (Elgohary et al., 2020, 2021; Narechania et al., 2021; Mo et al., 2022), which should further improve text-to-SQL parsing accuracy.\nLimitations\nExperiments on other large language models. Our study focused on conducting experiments using Codex as the LLM, since it was available at no cost and showed impressive performance in textto-SQL parsing among LLMs before GPT-4 (Rajkumar et al., 2022). To gain a comprehensive understanding of different CoT-style promptings for text-to-SQL, future research should explore the\neffects of these promptings on more recent, more powerful LLM models, such as GPT-4 (if budget allows). By doing so, we can determine whether the improvements achieved by our proposed promptings are consistent across different LLMs.\nExperiments on robustness. In our work, we mainly test robustness from the prompt design perspective such as how to select in-context examples, the number of in-context examples and the prompt format of in-context examples. It would also be valuable to investigate our prompting methods under different databases, natural language questions, or SQL perturbations (Chang et al., 2023a). This broader exploration would enable us to evaluate the robustness of our prompting methods across diverse scenarios."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank the anonymous reviewers and colleagues from the OSU NLP group for their thoughtful comments. This research was sponsored in part by a sponsored research award by Cisco Research, NSF IIS-1815674, NSF CAREER #1942980, NSF OAC-2112606, and Ohio Supercomputer Center (Center, 1987). The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein."
        },
        {
            "heading": "A Example Prompts",
            "text": "### SQLite SQL tables, with their properties: # # medicine (id, name, trade_name, fda_approved) # enzyme (id, name, location, product, chromosome, omim, porphyria) # medicine_enzyme_interaction (enzyme_id, medicine_id, interaction_type) #\nFigure 2: An example for API docs prompt format, introduced by Rajkumar et al. (2022), on Spider.\nCREATE TABLE grapes ( ID INTEGER PRIMARY KEY, Grape TEXT UNIQUE, Color TEXT ) /* 3 example rows: SELECT * FROM grapes LIMIT 3; ID Grape Color 1 Barbera Red 2 Cabernet Franc Red 3 Cabernet Sauvingnon Red /\nCREATE TABLE appellations ( No INTEGER PRIMARY KEY, Appelation TEXT UNIQUE, County TEXT, State TEXT, Area TEXT, isAVA TEXT ) /* 3 example rows: SELECT * FROM appellations LIMIT 3; No Appelation County State Area isAVA 1 Alexander Valley Sonoma California North Coast Yes 2 Amador County Amador California Sierra Foothills No 3 Amador-Mendocino-Sonoma Counties N/A California N/A No /\nCREATE TABLE wine ( No INTEGER, Grape TEXT, Winery TEXT, Appelation TEXT, State TEXT, Name TEXT, Year INTEGER, Price INTEGER, Score INTEGER, Cases INTEGER, Drink TEXT, FOREIGN KEY (Grape) REFERENCES grapes(Grape), FOREIGN KEY (Appelation) REFERENCES appellations(Appelation) ) /* 3 example rows: SELECT * FROM wine LIMIT 3; No Grape Winery Appelation State Name Year Price Score Cases Drink 1 Zinfandel Robert Biale St. Helena California Old Kraft Vineyard 2008 44 93 275 now 2 Zinfandel Chiarello Family Napa Valley California Giana 2008 35 93 480 now 3 Zinfandel Robert Biale Napa Valley California Black Chicken 2008 40 91 2700 2012 /\nFigure 3: An example for Create Table + Select 3 prompt format, introduced by Rajkumar et al. (2022), on Spider.\n### SQLite SQL tables, with their properties: # # medicine (id, name, trade_name, fda_approved) # enzyme (id, name, location, product, chromosome, omim, porphyria) # medicine_enzyme_interaction (enzyme_id, medicine_id, interaction_type) # ### What is the total count of enzymes? SELECT count(*) FROM enzyme\n### SQLite SQL tables, with their properties: # # buildings (id, name, city, height, stories, status) # companies (id, name, headquarters, industry, sales_billion, profits_billion, assets_billion, market_value_billion) # office_locations (building_id, company_id, move_in_year) # ### Show the industries shared by companies whose headquarters are \"USA\" and companies whose headquarters are \"China\". SELECT Industry FROM Companies WHERE Headquarters = \"USA\" INTERSECT SELECT Industry FROM Companies WHERE Headquarters = \"China\"\n### SQLite SQL tables, with their properties: # # stadium (stadium_id, location, name, capacity, highest, lowest, average) # singer (singer_id, name, country, song_name, song_release_year, age, is_male) # concert (concert_id, concert_name, theme, stadium_id, year) # singer_in_concert (concert_id, singer_id) # ### How many singers do we have?\nFigure 4: An example prompt under the standard API docs prompting for 2-shot on Spider.\n### SQLite SQL tables, with their properties: # # book_club (book_club_id, year, author_or_editor, book_title, publisher, category, result) # movie (movie_id, title, year, director, budget_million, gross_worldwide) # culture_company (company_name, type, incorporated_in, group_equity_shareholding, book_club_id, movie_id) # ### List categories that have at least two books after year 1989. # Let\u2019s think step by step\n# This query chooses records from the Book_Club table, followed by a WHERE clause that selects records where the year column is greater than 1989. It then groups the results by the category column. It then filters the results where the count of each category is greater than or equal to 2. It then selects the category column.\n# Thus, the answer for the question is: List categories that have at least two books after year 1989. SELECT category FROM book_club WHERE YEAR > 1989 GROUP BY category HAVING count(*) >= 2\n### SQLite SQL tables, with their properties: # # stadium (stadium_id, location, name, capacity, highest, lowest, average) # singer (singer_id, name, country, song_name, song_release_year, age, is_male) # concert (concert_id, concert_name, theme, stadium_id, year) # singer_in_concert (concert_id, singer_id) # ### How many singers do we have?\nFigure 5: An example prompt under chain-of-thought + API docs prompting for 1-shot on Spider.\n### SQLite SQL tables, with their properties: # class (class_code, crs_code, class_section, class_time, class_room, prof_num) # course (crs_code, dept_code, crs_description, crs_credit) # department (dept_code, dept_name, school_code, emp_num, dept_address) # employee (emp_num, emp_lname, emp_initial, emp_jobcode, emp_hiredate, emp_dob) # enroll (class_code, stu_num, enroll_grade) # professor (emp_num, dept_code, prof_office, prof_extension, prof_high_degree) # student (stu_num, stu_lname, stu_fname, stu_init, stu_dob, stu_hrs, stu_class, stu_gpa, stu_transfer, dept_code, stu_phone, prof_num) # To answer the question \u201cFind the first names and offices of all instructors who have taught some course and the course description and the department name.\u201d, we need to know: \u201cFind the first names and offices of all instructors.\u201d, \u201cFind the first names and offices of all instructors who have taught some course.\u201d, \u201cFind the first names and offices of all instructors who have taught some course and the course description.\u201d.\n### SQLite SQL tables, with their properties: # station (station_id, name, annual_entry_exit, annual_interchanges, total_passengers, location, main_services, number_of_platforms) # train (train_id, name, time, service) # train_station (train_id, station_id) # To answer the question \u201cShow all train names and times in stations in London in descending order by train time.\u201d, we need to know:\nFigure 6: An example prompt under least-to-most + API docs prompting (problem reduction) for 1-shot on Spider.\n### SQLite SQL tables, with their properties: # class (class_code, crs_code, class_section, class_time, class_room, prof_num) # course (crs_code, dept_code, crs_description, crs_credit) # department (dept_code, dept_name, school_code, emp_num, dept_address) # employee (emp_num, emp_lname, emp_initial, emp_jobcode, emp_hiredate, emp_dob) # enroll (class_code, stu_num, enroll_grade) # professor (emp_num, dept_code, prof_office, prof_extension, prof_high_degree) # student (stu_num, stu_lname, stu_fname, stu_init, stu_dob, stu_hrs, stu_class, stu_gpa, stu_transfer, dept_code, stu_phone, prof_num) # Q: Find the first names and offices of all instructors. A: SELECT T1.emp_fname , T2.prof_office FROM employee AS T1 JOIN professor AS T2 ON T1.emp_num = T2.emp_num\nQ: Find the first names and offices of all instructors who have taught some course. A: SELECT T2.emp_fname , T4.prof_office FROM CLASS AS T1 JOIN employee AS T2 ON T1.prof_num = T2.emp_num JOIN course AS T3 ON T1.crs_code = T3.crs_code JOIN professor AS T4 ON T2.emp_num = T4.emp_num\nQ: Find the first names and offices of all instructors who have taught some course and the course description. A: SELECT T2.emp_fname , T4.prof_office , T3.crs_description FROM CLASS AS T1 JOIN employee AS T2 ON T1.prof_num = T2.emp_num JOIN course AS T3 ON T1.crs_code = T3.crs_code JOIN professor AS T4 ON T2.emp_num = T4.emp_num\nQ: Find the first names and offices of all instructors who have taught some course and the course description and the department name. A: SELECT T2.emp_fname , T4.prof_office , T3.crs_description , T5.dept_name FROM CLASS AS T1 JOIN employee AS T2 ON T1.prof_num = T2.emp_num JOIN course AS T3 ON T1.crs_code = T3.crs_code JOIN professor AS T4 ON T2.emp_num = T4.emp_num JOIN department AS T5 ON T4.dept_code = T5.dept_code\n### SQLite SQL tables, with their properties: # station (station_id, name, annual_entry_exit, annual_interchanges, total_passengers, location, main_services, number_of_platforms) # train (train_id, name, time, service) # train_station (train_id, station_id) # Q: Show all train names and times.\nFigure 7: An example prompt under least-to-most + API docs prompting (problem solving) for 1-shot on Spider. The same prompt will be used to solve the next sub-question after we get the generated SQL query for the first sub-question.\n### SQLite SQL tables, with their properties: # document_types (document_type_code, document_description) # documents (document_id, document_type_code, grant_id, sent_date, response_received_date, other_details) # grants (grant_id, organisation_id, grant_amount, grant_start_date, grant_end_date, other_details) # organisation_types (organisation_type, organisation_type_description) # organisations (organisation_id, organisation_type, organisation_details) # project_outcomes (project_id, outcome_code, outcome_details) # project_staff (staff_id, project_id, role_code, date_from, date_to, other_details) # projects (project_id, organisation_id, project_details) # research_outcomes (outcome_code, outcome_description) # research_staff (staff_id, employer_organisation_id, staff_details) # staff_roles (role_code, role_description) # tasks (task_id, project_id, task_details, eg agree objectives) # ### Question: Find out the send dates of the documents with the grant amount of more than 5000 were granted by organisation type described as \"Research\". decompose the question\n1. Find out the send dates of the documents. 2. Find out the send dates of the documents with the grant amount of more than 5000. 3. Find out the send dates of the documents with the grant amount of more than 5000 were granted by organisation type described as \"Research\".\n# Thus, the answer for the question is: Find out the send dates of the documents with the grant amount of more than 5000 were granted by organisation type described as \"Research\". SELECT T1.sent_date FROM documents AS T1 JOIN Grants AS T2 ON T1.grant_id = T2.grant_id JOIN Organisations AS T3 ON T2.organisation_id = T3.organisation_id JOIN organisation_Types AS T4 ON T3.organisation_type = T4.organisation_type WHERE T2.grant_amount > 5000 AND T4.organisation_type_description = \u2019Research\u2019\n### SQLite SQL tables, with their properties: # stadium (stadium_id, location, name, capacity, highest, lowest, average) # singer (singer_id, name, country, song_name, song_release_year, age, is_male) # concert (concert_id, concert_name, theme, stadium_id, year) # singer_in_concert (concert_id, singer_id) # ### Question: How many singers do we have? decompose the question\nFigure 8: An example prompt under QDecomp + API docs prompting for 1-shot on Spider.\n### SQLite SQL tables, with their properties: # document_types (document_type_code, document_description) # documents (document_id, document_type_code, grant_id, sent_date, response_received_date, other_details) # grants (grant_id, organisation_id, grant_amount, grant_start_date, grant_end_date, other_details) # organisation_types (organisation_type, organisation_type_description) # organisations (organisation_id, organisation_type, organisation_details) # project_outcomes (project_id, outcome_code, outcome_details) # project_staff (staff_id, project_id, role_code, date_from, date_to, other_details) # projects (project_id, organisation_id, project_details) # research_outcomes (outcome_code, outcome_description) # research_staff (staff_id, employer_organisation_id, staff_details) # staff_roles (role_code, role_description) # tasks (task_id, project_id, task_details, eg agree objectives) # ### Question: Find out the send dates of the documents with the grant amount of more than 5000 were granted by organisation type described as \"Research\". decompose the question\n1. Find out the send dates of the documents. SQL table (column): documents (sent_date) 2. Find out the send dates of the documents with the grant amount of more than 5000. SQL table (column): grants (grant_amount, grant_id) 3. Find out the send dates of the documents with the grant amount of more than 5000 were granted by organisation type described as \"Research\". SQL table (column): organisation_Types (organisation_type_description, organisation_type), organisations (organisation_type, organisation_id)\n# Thus, the answer for the question is: Find out the send dates of the documents with the grant amount of more than 5000 were granted by organisation type described as \"Research\". SELECT T1.sent_date FROM documents AS T1 JOIN Grants AS T2 ON T1.grant_id = T2.grant_id JOIN Organisations AS T3 ON T2.organisation_id = T3.organisation_id JOIN organisation_Types AS T4 ON T3.organisation_type = T4.organisation_type WHERE T2.grant_amount > 5000 AND T4.organisation_type_description = \u2019Research\u2019\n### SQLite SQL tables, with their properties: # stadium (stadium_id, location, name, capacity, highest, lowest, average) # singer (singer_id, name, country, song_name, song_release_year, age, is_male) # concert (concert_id, concert_name, theme, stadium_id, year) # singer_in_concert (concert_id, singer_id) # ### Question: How many singers do we have? decompose the question\nFigure 9: An example prompt under QDecomp+InterCOL + API docs prompting for 1-shot on Spider."
        }
    ],
    "title": "Exploring Chain of Thought Style Prompting for Text-to-SQL",
    "year": 2023
}