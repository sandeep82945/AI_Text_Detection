{
    "abstractText": "Human evaluation is often considered to be the gold standard method of evaluating a Natural Language Generation system. However, whilst its importance is accepted by the community at large, the quality of its execution is often brought into question. In this position paper, we argue that the generation of more esoteric forms of language humour, irony and sarcasm constitutes a subdomain where the characteristics of selected evaluator panels are of utmost importance, and every effort should be made to report demographic characteristics wherever possible, in the interest of transparency and replicability. We support these claims with an overview of each language form and an analysis of examples in terms of how their interpretation is affected by different participant variables. We additionally perform a critical survey of recent works in NLG to assess how well evaluation procedures are reported in this subdomain, and note a severe lack of open reporting of evaluator demographic information, and a significant reliance on crowdsourcing platforms for recruitment.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tyler Loakman"
        },
        {
            "affiliations": [],
            "name": "Aaron Maladry"
        },
        {
            "affiliations": [],
            "name": "Chenghua Lin"
        }
    ],
    "id": "SP:9351e2cd4d296319a914a333363a224bc0cb869e",
    "references": [
        {
            "authors": [
                "Gavin Abercrombie",
                "Amanda Cercas Curry",
                "Mugdha Pandya",
                "Verena Rieser."
            ],
            "title": "Alexa, Google, Siri: What are your pronouns? gender and anthropomorphism in the design and perception of conversational assistants",
            "venue": "Proceedings of the 3rd Workshop on Gen-",
            "year": 2021
        },
        {
            "authors": [
                "Ibrahim Abu Farha",
                "Silviu Vlad Oprea",
                "Steven Wilson",
                "Walid Magdy."
            ],
            "title": "SemEval-2022 task 6: iSarcasmEval, intended sarcasm detection in English and Arabic",
            "venue": "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022),",
            "year": 2022
        },
        {
            "authors": [
                "Ibrahim Abu Farha",
                "Wajdi Zaghouani",
                "Walid Magdy."
            ],
            "title": "Overview of the WANLP 2021 shared task on sarcasm and sentiment detection in Arabic",
            "venue": "Proceedings of the Sixth Arabic Natural Language Processing Workshop, pages 296\u2013305, Kyiv, Ukraine",
            "year": 2021
        },
        {
            "authors": [
                "Fernando Alva-Manchego",
                "Carolina Scarton",
                "Lucia Specia."
            ],
            "title": "The (un)suitability of automatic evaluation metrics for text simplification",
            "venue": "Computational Linguistics, 47(4):861\u2013889.",
            "year": 2021
        },
        {
            "authors": [
                "Jacopo Amidei",
                "Paul Piwek",
                "Alistair Willis."
            ],
            "title": "The use of rating and Likert scales in natural language generation human evaluation tasks: A review and some recommendations",
            "venue": "Proceedings of the 12th International Conference on Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Emily M. Bender",
                "Batya Friedman."
            ],
            "title": "Data statements for natural language processing: Toward mitigating system bias and enabling better science",
            "venue": "Transactions of the Association for Computational Linguistics, 6:587\u2013604.",
            "year": 2018
        },
        {
            "authors": [
                "Dawn G. Blasko",
                "Victoria A. Kazmerski",
                "Shariffah Sheik Dawood."
            ],
            "title": "Saying what you don\u2019t mean: A cross-cultural study of perceptions of sarcasm",
            "venue": "Canadian Journal of Experimental Psychology / Revue canadienne de psychologie exp\u00e9rimentale, 75(2):114\u2013119.",
            "year": 2021
        },
        {
            "authors": [
                "Aur\u00e9lie Breidenbach",
                "Caroline Mahlow",
                "Andreas Schreiber"
            ],
            "title": "Implicit gender bias in computer science \u2013 a qualitative study",
            "year": 2021
        },
        {
            "authors": [
                "Gregory A Bryant",
                "Jean E Fox Tree."
            ],
            "title": "Recognizing verbal irony in spontaneous speech",
            "venue": "Metaphor and symbol, 17(2):99\u2013119.",
            "year": 2002
        },
        {
            "authors": [
                "C. Burgers."
            ],
            "title": "Verbal irony: Use and effects in written discourse",
            "venue": "Ph.D. thesis, UB Nijmegen.",
            "year": 2010
        },
        {
            "authors": [
                "Chris Callison-Burch."
            ],
            "title": "Fast, cheap, and creative: Evaluating translation quality using Amazon\u2019s Mechanical Turk",
            "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 286\u2013295, Singapore. Association for",
            "year": 2009
        },
        {
            "authors": [
                "E. Camp."
            ],
            "title": "Sarcasm, pretense, and the semantics/pragmatics distinction",
            "venue": "Nous, 46:587\u2013634.",
            "year": 2012
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Debanjan Ghosh",
                "Smaranda Muresan",
                "Nanyun Peng."
            ],
            "title": "R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Arjun Chandrasekaran",
                "Devi Parikh",
                "Mohit Bansal."
            ],
            "title": "Punny captions: Witty wordplay in image descriptions",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2018
        },
        {
            "authors": [
                "Wenhu Chen",
                "Yu Su",
                "Xifeng Yan",
                "William Yang Wang."
            ],
            "title": "KGPT: Knowledge-grounded pre-training for data-to-text generation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8635\u20138648,",
            "year": 2020
        },
        {
            "authors": [
                "Noam Chomsky."
            ],
            "title": "Syntactic structures",
            "venue": "Syntactic structures. Mouton, Oxford, England.",
            "year": 1957
        },
        {
            "authors": [
                "Elizabeth Clark",
                "Tal August",
                "Sofia Serrano",
                "Nikita Haduong",
                "Suchin Gururangan",
                "Noah A. Smith."
            ],
            "title": "All that\u2019s \u2018human\u2019 is not gold: Evaluating human evaluation of generated text",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Jennifer Coates."
            ],
            "title": "Women, men and language: A sociolinguistic account of gender differences in language",
            "venue": "Routledge.",
            "year": 2016
        },
        {
            "authors": [
                "Iria de Dios-Flores",
                "Carmen Magari\u00f1os",
                "Adina Ioana Vladu",
                "John E. Ortega",
                "Jos\u00e9 Ramom Pichel",
                "Marcos Garc\u00eda",
                "Pablo Gamallo",
                "Elisa Fern\u00e1ndez Rei",
                "Alberto Bugar\u00edn-Diz",
                "Manuel Gonz\u00e1lez Gonz\u00e1lez",
                "Sen\u00e9n Barro",
                "Xos\u00e9 Luis Regueira"
            ],
            "title": "The n\u00f3s project",
            "year": 2022
        },
        {
            "authors": [
                "European Commission",
                "Directorate-General for Research",
                "Innovation."
            ],
            "title": "She figures 2018",
            "venue": "Technical report, European Commission.",
            "year": 2019
        },
        {
            "authors": [
                "Dalya Faraj",
                "Malak Abdullah."
            ],
            "title": "SarcasmDet at SemEval-2021 task 7: Detect humor and offensive based on demographic factors using RoBERTa pretrained model",
            "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),",
            "year": 2021
        },
        {
            "authors": [
                "Elena Filatova."
            ],
            "title": "Irony and sarcasm: Corpus generation and analysis using crowdsourcing",
            "venue": "Lrec, pages 392\u2013398. Citeseer.",
            "year": 2012
        },
        {
            "authors": [
                "Ruth Filik",
                "Alexandra \u0162urcan",
                "Christina Ralph-Nearman",
                "Alain Pitiot."
            ],
            "title": "What is the difference between irony and sarcasm? an fmri study",
            "venue": "Cortex, 115:112\u2013122.",
            "year": 2019
        },
        {
            "authors": [
                "Simona Frenda",
                "Alessandro Pedrani",
                "Valerio Basile",
                "Soda Marem Lo",
                "Alessandra Teresa Cignarella",
                "Raffaella Panizzon",
                "Cristina Marco",
                "Bianca Scarlini",
                "Viviana Patti",
                "Cristina Bosco",
                "Davide Bernardi"
            ],
            "title": "Epic: Multi-perspective annotation of a corpus",
            "year": 2023
        },
        {
            "authors": [
                "Judit Garc\u00eda-Gonz\u00e1lez",
                "Patricia Forc\u00e9n",
                "Maria Jimenez-Sanchez."
            ],
            "title": "Men and women differ in their perception of gender bias in research institutions",
            "venue": "PloS one, 14(12):e0225763\u2013e0225763.",
            "year": 2019
        },
        {
            "authors": [
                "Aparna Garimella",
                "Carmen Banea",
                "Nabil Hossain",
                "Rada Mihalcea."
            ],
            "title": "judge me by my size (noun), do you?\u201d YodaLib: A demographic-aware humor generation framework",
            "venue": "Proceedings of the 28th International Conference on Computational Linguis-",
            "year": 2020
        },
        {
            "authors": [
                "Ricky Gervais"
            ],
            "title": "Ricky gervais: The difference between american and british humour",
            "year": 2011
        },
        {
            "authors": [
                "Debanjan Ghosh",
                "Avijit Vajpayee",
                "Smaranda Muresan."
            ],
            "title": "A report on the 2020 sarcasm detection shared task",
            "venue": "Proceedings of the Second Workshop on Figurative Language Processing, pages 1\u201311, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Raymond W Gibbs."
            ],
            "title": "On the psycholinguistics of sarcasm",
            "venue": "Journal of experimental psychology: general, 115(1):3.",
            "year": 1986
        },
        {
            "authors": [
                "Seraphina Goldfarb-Tarrant",
                "Tuhin Chakrabarty",
                "Ralph Weischedel",
                "Nanyun Peng."
            ],
            "title": "Content planning for neural story generation with aristotelian rescoring",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Tomas Goldsack",
                "Zhihao Zhang",
                "Chenghua Lin",
                "Carolina Scarton."
            ],
            "title": "Making science simple: Corpora for the lay summarisation of scientific literature",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2022
        },
        {
            "authors": [
                "Tomas Goldsack",
                "Zhihao Zhang",
                "Chenghua Lin",
                "Carolina Scarton."
            ],
            "title": "Domain-driven and discourseguided scientific summarisation",
            "venue": "European Conference on Information Retrieval, pages 361\u2013376.",
            "year": 2023
        },
        {
            "authors": [
                "P.H. Grice."
            ],
            "title": "Further notes on logic and conversation, volume 9, pages 113\u2013127",
            "venue": "P. Cole, Syntax and Semantics, New York: Academic Press.",
            "year": 1978
        },
        {
            "authors": [
                "David Gros",
                "Yu Li",
                "Zhou Yu."
            ],
            "title": "Robots-dont-cry: Understanding falsely anthropomorphic utterances in dialog systems",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3266\u20133284, Abu Dhabi, United Arab Emirates.",
            "year": 2022
        },
        {
            "authors": [
                "He He",
                "Nanyun Peng",
                "Percy Liang."
            ],
            "title": "Pun generation with surprise",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short",
            "year": 2019
        },
        {
            "authors": [
                "Joseph Henrich",
                "Steven J. Heine",
                "Ara Norenzayan"
            ],
            "title": "The weirdest people in the world? Behavioral and Brain Sciences, 33(2-3):61\u201383",
            "year": 2010
        },
        {
            "authors": [
                "Anita Holdcroft."
            ],
            "title": "Gender bias in research: how does it affect evidence based medicine? Journal of the Royal Society of Medicine, 100(1):2\u20133",
            "venue": "PMID: 17197669.",
            "year": 2007
        },
        {
            "authors": [
                "Nabil Hossain",
                "John Krumm",
                "Michael Gamon",
                "Henry Kautz."
            ],
            "title": "SemEval-2020 task 7: Assessing humor in edited news headlines",
            "venue": "Proceedings of the Fourteenth Workshop on Semantic Evaluation, pages 746\u2013758, Barcelona (online). International Committee",
            "year": 2020
        },
        {
            "authors": [
                "David M. Howcroft",
                "Anya Belz",
                "Miruna-Adriana Clinciu",
                "Dimitra Gkatzia",
                "Sadid A. Hasan",
                "Saad Mahamood",
                "Simon Mille",
                "Emiel van Miltenburg",
                "Sashank Santhanam",
                "Verena Rieser"
            ],
            "title": "Twenty years of confusion in human evaluation: NLG needs evaluation",
            "year": 2020
        },
        {
            "authors": [
                "Jessica Huynh",
                "Jeffrey Bigham",
                "Maxine Eskenazi"
            ],
            "title": "A survey of nlp-related crowdsourcing hits: what works and what does not",
            "year": 2021
        },
        {
            "authors": [
                "Neslihan Iskender",
                "Tim Polzehl",
                "Sebastian M\u00f6ller."
            ],
            "title": "Reliability of human evaluation for text summarization: Lessons learned and challenges ahead",
            "venue": "Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pages 86\u201396.",
            "year": 2021
        },
        {
            "authors": [
                "Tonglin Jiang",
                "Hao Li",
                "Yubo Hou."
            ],
            "title": "Cultural differences in humor perception, usage, and implications",
            "venue": "Frontiers in Psychology, 10.",
            "year": 2019
        },
        {
            "authors": [
                "Valentin Jijkoun",
                "Katja Hofmann."
            ],
            "title": "Generating a non-english subjectivity lexicon: Relations that matter",
            "venue": "Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 398\u2013405.",
            "year": 2009
        },
        {
            "authors": [
                "Faria Binte Kader",
                "Nafisa Hossain Nujat",
                "Tasmia Binte Sogir",
                "Mohsinul Kabir",
                "Hasan Mahmud",
                "Md Kamrul Hasan"
            ],
            "title": "when words fail, emojis prevail\u201d: A novel architecture for generating sarcastic sentences with emoji using valence reversal and semantic incon",
            "year": 2023
        },
        {
            "authors": [
                "Mihir Kale",
                "Abhinav Rastogi."
            ],
            "title": "Text-to-text pre-training for data-to-text tasks",
            "venue": "Proceedings of the 13th International Conference on Natural Language Generation, pages 97\u2013102, Dublin, Ireland. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Tiffany J. Lawless",
                "Conor J. O\u2019Dea",
                "Stuart S. Miller",
                "Donald A. Saucier"
            ],
            "title": "Is it really just a joke? gender differences in perceptions of sexist humor",
            "venue": "Humor (Berlin,",
            "year": 2020
        },
        {
            "authors": [
                "Elena Lloret",
                "Laura Plaza",
                "Ahmet Aker."
            ],
            "title": "The challenging task of summary evaluation: an overview",
            "venue": "Language Resources and Evaluation, 52:101\u2013148.",
            "year": 2018
        },
        {
            "authors": [
                "Tyler Loakman",
                "Chen Tang",
                "Chenghua Lin."
            ],
            "title": "TwistList: Resources and baselines for tongue twister generation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 579\u2013589, Toronto,",
            "year": 2023
        },
        {
            "authors": [
                "Fuli Luo",
                "Shunyao Li",
                "Pengcheng Yang",
                "Lei Li",
                "Baobao Chang",
                "Zhifang Sui",
                "Xu Sun."
            ],
            "title": "Pun-GAN: Generative adversarial network for pun generation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna-",
            "year": 2019
        },
        {
            "authors": [
                "Aaron Maladry",
                "Els Lefever",
                "Cynthia Van Hee",
                "V\u00e9ronique Hoste."
            ],
            "title": "The limitations of irony detection in dutch social media",
            "venue": "Language Resources and Evaluation.",
            "year": 2022
        },
        {
            "authors": [
                "J.A. Meaney."
            ],
            "title": "Crossing the line: Where do demographic variables fit into humor detection? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 176\u2013181, Online",
            "venue": "Association for",
            "year": 2020
        },
        {
            "authors": [
                "Abhijit Mishra",
                "Tarun Tater",
                "Karthik Sankaranarayanan."
            ],
            "title": "A modular architecture for unsupervised sarcasm generation",
            "venue": "Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural lan-",
            "year": 2019
        },
        {
            "authors": [
                "Anirudh Mittal",
                "Yufei Tian",
                "Nanyun Peng."
            ],
            "title": "AmbiPun: Generating humorous puns with ambiguous context",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2022
        },
        {
            "authors": [
                "Silviu Oprea",
                "Steven Wilson",
                "Walid Magdy."
            ],
            "title": "Chandler: An explainable sarcastic response generator",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 339\u2013349, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Silviu Vlad Oprea",
                "Steven Wilson",
                "Walid Magdy."
            ],
            "title": "Should a chatbot be sarcastic? understanding user preferences towards sarcasm generation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Vaishnavi Pamulapati",
                "Gayatri Purigilla",
                "Radhika Mamidi."
            ],
            "title": "A novel annotation schema for conversational humor: Capturing the cultural nuances in kanyasulkam",
            "venue": "Proceedings of the 14th Linguistic Annotation Workshop, pages 34\u201347, Barcelona, Spain.",
            "year": 2020
        },
        {
            "authors": [
                "Mihir Parmar",
                "Swaroop Mishra",
                "Mor Geva",
                "Chitta Baral."
            ],
            "title": "Don\u2019t blame the annotator: Bias already starts in the annotation instructions",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages",
            "year": 2023
        },
        {
            "authors": [
                "Ellie Pavlick",
                "Tom Kwiatkowski."
            ],
            "title": "Inherent Disagreements in Human Textual Inferences",
            "venue": "Transactions of the Association for Computational Linguistics, 7:677\u2013694.",
            "year": 2019
        },
        {
            "authors": [
                "Louise H. Phillips",
                "Roy Allen",
                "Rebecca Bull",
                "Alexandra Hering",
                "Matthias Kliegel",
                "Shelley Channon."
            ],
            "title": "Older adults have difficulty in decoding sarcasm",
            "venue": "Developmental Psychology, 51(12):1840\u20131852.",
            "year": 2015
        },
        {
            "authors": [
                "Barbara Plank."
            ],
            "title": "The \u201cproblem\u201d of human label variation: On ground truth in data, modeling and evaluation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10671\u201310682, Abu Dhabi, United Arab Emirates.",
            "year": 2022
        },
        {
            "authors": [
                "Ella Rabinovich",
                "Hila Gonen",
                "Suzanne Stevenson."
            ],
            "title": "Pick a fight or bite your tongue: Investigation of gender differences in idiomatic language usage",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 5181\u20135192,",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research, 21(140):1\u201367.",
            "year": 2020
        },
        {
            "authors": [
                "Victor Raskin."
            ],
            "title": "Semantic mechanisms of humor",
            "venue": "Synthese language library ; 24. Reidel, Dordrecht.",
            "year": 1985
        },
        {
            "authors": [
                "Ehud Reiter."
            ],
            "title": "A structured review of the validity of BLEU",
            "venue": "Computational Linguistics, 44(3):393\u2013401.",
            "year": 2018
        },
        {
            "authors": [
                "Patricia Rockwell",
                "Evelyn M. Theriot."
            ],
            "title": "Culture, gender, and gender mix in encoders of sarcasm: A self-assessment analysis",
            "venue": "Communication Research Reports, 18(1):44\u201352.",
            "year": 2001
        },
        {
            "authors": [
                "Stephanie Schoch",
                "Diyi Yang",
                "Yangfeng Ji."
            ],
            "title": "this is a problem, don\u2019t you agree?\u201d framing and bias in human evaluation for natural language generation",
            "venue": "Proceedings of the 1st Workshop on Evaluating NLG Evaluation, pages 10\u201316, Online (Dublin, Ireland).",
            "year": 2020
        },
        {
            "authors": [
                "Chen Shani",
                "Alexander Libov",
                "Sofia Tolmach",
                "Liane Lewin-Eytan",
                "Yoelle Maarek",
                "Dafna Shahaf."
            ],
            "title": "alexa, do you want to build a snowman?\u201d characterizing playful requests to conversational agents",
            "venue": "Extended Abstracts of the 2022 CHI Conference on Human",
            "year": 2022
        },
        {
            "authors": [
                "Natalie Shapira",
                "Oren Kalinsky",
                "Alexander Libov",
                "Chen Shani",
                "Sofia Tolmach."
            ],
            "title": "Evaluating humorous response generation to playful shopping requests",
            "venue": "ECIR 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Boaz Shmueli",
                "Jan Fell",
                "Soumya Ray",
                "Lun-Wei Ku."
            ],
            "title": "Beyond fair pay: Ethical implications of NLP crowdsourcing",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Gary F. Simons",
                "Abbey L.L. Thomas",
                "Chad K.K. White."
            ],
            "title": "Assessing digital language support on a global scale",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 4299\u20134305, Gyeongju, Republic of Korea. International",
            "year": 2022
        },
        {
            "authors": [
                "Anders S\u00f8gaard."
            ],
            "title": "Should we ban English NLP for a year? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5254\u20135260, Abu Dhabi, United Arab Emirates",
            "venue": "Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Jiao Sun",
                "Anjali Narayan-Chen",
                "Shereen Oraby",
                "Alessandra Cervone",
                "Tagyoung Chung",
                "Jing Huang",
                "Yang Liu",
                "Nanyun Peng."
            ],
            "title": "ExPUNations: Augmenting puns with keywords and explanations",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural",
            "year": 2022
        },
        {
            "authors": [
                "Jiao Sun",
                "Anjali Narayan-Chen",
                "Shereen Oraby",
                "Shuyang Gao",
                "Tagyoung Chung",
                "Jing Huang",
                "Yang Liu",
                "Nanyun Peng."
            ],
            "title": "Context-situated pun generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4635\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Chen Tang",
                "Hongbo Zhang",
                "Tyler Loakman",
                "Chenghua Lin",
                "Frank Guerin."
            ],
            "title": "Terminology-aware medical dialogue generation",
            "venue": "ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135.",
            "year": 2023
        },
        {
            "authors": [
                "Chen Tang",
                "Zhihao Zhang",
                "Tyler Loakman",
                "Chenghua Lin",
                "Frank Guerin."
            ],
            "title": "NGEP: A graph-based event planning framework for story generation",
            "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and",
            "year": 2022
        },
        {
            "authors": [
                "Yufei Tian",
                "Divyanshu Sheth",
                "Nanyun Peng."
            ],
            "title": "A unified framework for pun generation with humor principles",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3253\u20133261, Abu Dhabi, United Arab Emirates. Association for",
            "year": 2022
        },
        {
            "authors": [
                "Alexandra N. Uma",
                "Tommaso Fornaciari",
                "Dirk Hovy",
                "Silviu Paun",
                "Barbara Plank",
                "Massimo Poesio."
            ],
            "title": "Learning from disagreement: A survey",
            "venue": "Journal of Artificial Intelligence Research, 72:1385\u20131470.",
            "year": 2022
        },
        {
            "authors": [
                "Tim Van de Cruys."
            ],
            "title": "Automatic poetry generation from prosaic text",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2471\u20132480, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Chris van der Lee",
                "Albert Gatt",
                "Emiel van Miltenburg",
                "Sander Wubben",
                "Emiel Krahmer."
            ],
            "title": "Best practices for the human evaluation of automatically generated text",
            "venue": "Proceedings of the 12th International Conference on Natural Language Generation, pages",
            "year": 2019
        },
        {
            "authors": [
                "Cynthia Van Hee."
            ],
            "title": "Can machines sense irony? : exploring automatic irony detection on social media",
            "venue": "Ph.D. thesis, Ghent University.",
            "year": 2017
        },
        {
            "authors": [
                "Byron C Wallace",
                "Laura Kertz",
                "Eugene Charniak"
            ],
            "title": "Humans require context to infer ironic intent (so computers probably do, too)",
            "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
            "year": 2014
        },
        {
            "authors": [
                "Boshi Wang",
                "Xiang Deng",
                "Huan Sun."
            ],
            "title": "Iteratively prompt pre-trained language models for chain of thought",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2714\u20132730, Abu Dhabi, United Arab Emirates.",
            "year": 2022
        },
        {
            "authors": [
                "Orion Weller",
                "Nancy Fulda",
                "Kevin Seppi."
            ],
            "title": "Can humor prediction datasets be used for humor generation? humorous headline generation via style transfer",
            "venue": "Proceedings of the Second Workshop on Figurative Language Processing, pages 186\u2013191,",
            "year": 2020
        },
        {
            "authors": [
                "Zhiwei Yu",
                "Jiwei Tan",
                "Xiaojun Wan."
            ],
            "title": "A neural approach to pun generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1650\u20131660, Melbourne, Australia. Association for Computational",
            "year": 2018
        },
        {
            "authors": [
                "Zhiwei Yu",
                "Hongyu Zang",
                "Xiaojun Wan."
            ],
            "title": "Homophonic pun generation with lexically constrained rewriting",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2870\u20132876, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Mike Zajko."
            ],
            "title": "Conservative AI and social inequality: conceptualizing alternatives to bias through social theory",
            "venue": "AI & SOCIETY, 36(3):1047\u20131056.",
            "year": 2021
        },
        {
            "authors": [
                "Qingcheng Zeng",
                "An-Ran Li."
            ],
            "title": "A survey in automatic irony processing: Linguistic, cognitive, and multi-X perspectives",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 824\u2013836, Gyeongju, Republic of Korea.",
            "year": 2022
        },
        {
            "authors": [
                "Yue Zhang",
                "Hongliang Fei",
                "Dingcheng Li",
                "Ping Li."
            ],
            "title": "PromptGen: Automatically generate prompts using generative models",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 30\u201337, Seattle, United States. Association",
            "year": 2022
        },
        {
            "authors": [
                "Kun Zhao",
                "Bohao Yang",
                "Chenghua Lin",
                "Wenge Rong",
                "Aline Villavicencio",
                "Xiaohui Cui."
            ],
            "title": "Evaluating open-domain dialogues in latent space with next sentence prediction and mutual information",
            "venue": "Proceedings of the 61st Annual Meeting of the",
            "year": 2023
        },
        {
            "authors": [
                "Wenye Zhao",
                "Qingbao Huang",
                "Dongsheng Xu",
                "Peizhi Zhao."
            ],
            "title": "Multi-modal sarcasm generation: Dataset and solution",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 5601\u2013 5613, Toronto, Canada. Association for Computational",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The aim of a Natural Language Generation (NLG) system is to generate coherent and well-formed text of a particular type, usually given an input such as a prompt (Raffel et al., 2020; Wang et al., 2022), outline (Goldfarb-Tarrant et al., 2020; Tang et al., 2022), topic (Van de Cruys, 2020), or data (Kale and Rastogi, 2020; Chen et al., 2020). As a result, NLG contains many sub-domains such as summarisation (Goldsack et al., 2022, 2023), data-to-text generation (Kale and Rastogi, 2020; Chen et al., 2020), dialogue generation (Tang et al., 2023), story generation (Goldfarb-Tarrant et al., 2020; Tang et al., 2022), and humour generation (Sun et al., 2022b; Loakman et al., 2023). Whilst it may be possible for any competent speaker of the target language to sufficiently understand generated outputs, many language forms may be considered\n\u2217 Corresponding author\nmore esoteric, such as humour, irony, and sarcasm - where interpretations of such language are highly intertwined with more broad demographic characteristics, as well as individual and idiosyncratic factors.\nHuman evaluation is viewed as the foremost important form of evaluation for any NLG system (Howcroft et al., 2020), owing largely to its direct capture of the opinions of the (usually) human end-user.1 This is made even more prominent when considering the low concurrent validity that many easily deployed automatic evaluation metrics have with human judgements (Reiter, 2018; Alva-Manchego et al., 2021; Zhao et al., 2023a). Whilst such evaluation procedures are considered preferable over automatic metrics (particularly in the cases where no reliable automatic metrics are available), this does not mean that they are infallible. Numerous recent works have outlined the inconsistency with which human evaluation procedures are executed, and increasing attention is being brought to the knock-on effects that such oversights may cause (Zajko, 2021).\nIn this paper, we present an overview of 3 particular types of language: humour, irony and sarcasm - and demonstrate how different evaluator demographics2 may affect the interpretation of such language, and therefore its evaluation in relevant systems. Furthermore, we present a critical survey and assessment of 17 NLG research papers from between 2018 and 2023 from the ACL Anthology that concern the generation of these types of language, and analyse behaviour in regard to the level of transparency used when reporting details of the human evaluation procedure."
        },
        {
            "heading": "2 Related Works",
            "text": "In recent years, the execution of human evaluation in NLG has become an even more significant topic, with myriad top-tier venues hosting dedicated tracks and workshops specifically focused on the evaluation\n1Sometimes NLG outputs are used as inputs to further NLG models (Zhang et al., 2022).\n2In what we describe as a \"cultural melting pot\" in the title.\nof evaluation itself, such as the HumEval workshops at EACL 2021, ACL 2022 and RANLP 2023.3 As a product of such venues, multiple recent works have been proposed that aim to highlight common inadequacies in multiple aspects of human evaluation, including the design of assessment criteria, the recruitment and training of participants, the use of biased response elicitation techniques (Parmar et al., 2023), and the transparency of reporting these processes.\nHowcroft et al. (2020) assess a 20 year time-span of work within NLG (165 papers between 2000-2019) in regard to how evaluation procedures are performed, finding that there is substantial variation in how evaluation criteria are named and portrayed. Resulting from this, they produce normalised mappings between different criteria descriptions and their underlying evaluation focus and call for human evaluation criteria to be standardised, in addition to the creation of evaluation sheets outlining best practices for response elicitation from evaluators. Amidei et al. (2019) also assess NLG research in terms of response elicitation methods with a particular focus on the use of Likert scales and common deviations from best-practice within 135 papers from a 10 year period (e.g. whether or not to view them as ordinal or interval in nature, and consequently how to perform statistical significance testing). Further issues have also been identified by Schoch et al. (2020) with the framing of evaluation questions and participant recruitment methods, including the presence of demand characteristics in evaluators being more common on crowdsourcing platforms as participants vie for limited positions in HITs and wish to give researchers the results they believe they desire (under the belief it will lead to more future work).4 Furthermore, these issues are exacerbated by cases wherein researchers reject annotations outright due to not falling in line with their expectations, or pay inadequately for services on crowdsourcing platforms (Huynh et al., 2021; Shmueli et al., 2021). However, whilst automatic metrics may have many drawbacks for evaluating NLG, human evaluation has been suggested to be imperfect by its very nature, with human judgements being inconsistent. For example, Clark et al. (2021) find that human evaluators range significantly in their preconceptions of the quality of text NLG systems can produce, resulting in evaluators having biased perspectives when assessing text quality (a bias\n3https://humeval.github.io 4Demand characteristics are cues that may allow a participant to infer the nature and desired outcome of a study, often resulting in the participant then altering their behaviour to align with the perceived expectation, thereby invalidating results.\nthat is only somewhat overcome with the provision of training materials to standardise criteria definitions among evaluators). In order to partially mitigate these evaluation pitfalls, van der Lee et al. (2019) suggest a range of best practices, including the reporting of confidence intervals for results, using multiple ratings of each criteria, and using randomisation and counterbalancing when assigning evaluators to conditions (e.g. human- vs model- generated text). Additionally, due to many of the problems that we outline in this work, the domain is currently starting to demonstrate the progression of a paradigm shift towards understanding the role of human disagreement as a valuable feature, rather than noise, which is also resulting in the rise of perspectivist approaches (Pavlick and Kwiatkowski, 2019; Meaney, 2020; Plank, 2022; Uma et al., 2022). Additionally, whilst the use of such language can lead to different responses from different people, this effect is compounded in human-computer interactions, where the communicative divide is even more pronounced (Shani et al., 2022; Shapira et al., 2023)."
        },
        {
            "heading": "3 Introducing Humour, Sarcasm, and Irony",
            "text": "In the following section we discuss three primary examples of language types where interpretations and opinions are highly affected by a range of nuanced participant variables. For each type, we outline the nature of this language alongside a range of examples, and reflect upon how interpretation of these examples may differ across demographics."
        },
        {
            "heading": "3.1 Humour",
            "text": "As the first language form to discuss, humour is also one of the most complex. The general consensus within cognitive and linguistic theories of humour is that an essential element for something to be found humorous is the existence of some degree of incongruity between expectations and reality (Raskin, 1985). Consequently, the experience of humour arises from the resolution of these incongruities within someone\u2019s mind. This, in turn, is how puns may elicit humour, when the initial interpretation of a text primes the interpretation of one semantic sense of a word, before that expectation is violated when the text is viewed as a whole. For example, in \u201cthe greyhound stopped to get a hare cut\u201d, the interpretation of \u201chare\u201d as in the rabbit-like animal is primed by the mention of the \u201cgreyhound\u201d which chases hares, whilst \u201chare cut\u201d forces the reader to reinterpret \u201chare\u201d as the homophone \u201chair\u201d, which resolves the incongruity from the non-collocational phrase \u201chare cut\u201d (He et al., 2019).\nWhilst puns are the most frequently studied form of humour in the area of NLG (He et al., 2019; Mittal et al., 2022; Tian et al., 2022), primarily due to their simple exploitation of phonetics and word senses, humour more generally can arise from more complex incongruities which are more likely to be resolved by individuals with specific assumed world knowledge. Table 1 presents examples of such humour. In Example 1, the perception of humour is dependent on factors including an individual\u2019s knowledge of politics and their ideological leaning, as well as region.5 The reader is therefore assumed to have external background knowledge of US politics in order to infer that the joke concerns Donald Trump\u2019s record of reversing actions made by his predecessor, Barack Obama. Additionally, due to the joke using Trump as the target and in effect calling him petty, whether or not this joke is seen as humorous is also dependent on one\u2019s political leanings. This is due to Republicans, the political party that Trump represented, being less likely to find comments mocking him to be humorous. In Example 2, the joke is based around the shared knowledge of individuals in a patriarchal society which has, of course, traditionally benefited men. As a result, the joke uses men as the target, and comments on a presumed level of arrogance. In addition, this example requires the reader to interpret both literal and metaphorical meanings of something \u201crevolving around\u201d something else. Consequently, factors that could affect the perception of humour here are one\u2019s familiarity with a patriarchal society and culture (as it can be imagined that such assumptions about male arrogance are not shared by members of a matriarchal culture). Additionally, due to the joke targeting men, there is the possibility for some men to be offended at the assumptions that it is making, and it is known that gender generally affects the perception of certain types of jokes such as these (Lawless et al., 2020). Furthermore, a large body of work has investigated the different tendencies of particular genders towards different patterns of language use,\n5In this case, US politics is quite well known world-over, but a similar joke for a different government could be imagined, where universal knowledge would be far less likely.\nwhich in some cases may also affect the perception of different language types (Coates, 2016; Rabinovich et al., 2020). Finally, Example 3 (Chomsky, 1957) is an instance of an incongruous sentence which is semantically nonsensical (yet grammatically valid), where this incongruity does not give rise to humour (in the general case). In addition to the examples outlined and analysed here, numerous works have also ascertained a link between different demographic variables and humour perception. For example, Jiang et al. (2019) explore the literature on how individuals in the Western and Eastern worlds view humour as suitable under different conditions, and differ also in the exact forms of humour they opt to produce."
        },
        {
            "heading": "3.2 Irony",
            "text": "Although the exact definition of irony still remains a topic of discussion, NLP research has reached the general consensus that verbal irony is a form of figurative language where the producer of the language intends to convey the opposite meaning of the literal interpretation (Grice, 1978; Burgers, 2010; Camp, 2012). Consequently, similar to within the humour domain, an ironic statement violates the true beliefs of the producer and the expectations of the audience. It is for this reason that many ironic (and sarcastic) statements may also be considered humorous - a well known feature of British comedy programming (Gervais, 2011). This means that correct understanding of irony presupposes true or accepted knowledge about whatever thing, person or situation is being referred to. Such presupposed knowledge can assume multiple forms. A range of ironic texts are presented in Table 2. Irony can, for example, be factual, where it is generated by making a factually incorrect statement such as in Example 1 where a human alludes to having wings. Alternatively, it can also rely on social conventions, habits or shared knowledge within members of a community. For instance, in order to understand the irony in Example 2, the receiver is expected to know that most people would not genuinely applaud the actions of someone they call a dictator. Whilst some knowledge and social\nconventions are ubiquitous, many ironic expressions also assume specific (in-group) knowledge that is often only shared within a particular social group. For example, within the statement seen in Example 3, the receiver of the message is required to understand that \u201chalal\u201d is a most often used as a binary concept, and therefore something being a \u201cpercentage\u201d halal is infeasible, thus requiring additional background knowledge about Islam and its conventions. A single individual can, of course, be a member of several different social groups that share common ideals and knowledge, such as political conviction, free-time activities, or music taste, without even having to consider major cultural or geographical divergences. This simple fact, that irony strongly relies on common(sense) knowledge, makes it an inherently subjective phenomenon, both for detection and the evaluation of generative models. In addition, ironic statements can also rely on situational or contextual knowledge, in which case it is then classified as \u201csituational irony\u201d as opposed to \u201cverbal irony\u201d. An example of situational irony is demonstrated in Example 4, where a fire station burning down is unexpected due to containing multiple resources explicitly for the extinguishing of such fires. According to Wallace et al. (2014), contextual information may even be essential for most humans to recognise most forms of irony. Finally, Example 5 presents a non-ironic statement, where expectations about concepts are not violated, as a hospital is usually assumed to be a large building. Recent works have taken into account the plethora of variables affecting the interpretation of irony, with Frenda et al. (2023) developing a corpus of irony from a perspectivist approach, finding that irony perceptions vary with generation and nationality (even within speakers of the same language)."
        },
        {
            "heading": "3.3 Sarcasm",
            "text": "Traditional definitions usually consider sarcasm to be a sub-category of irony. In addition to having a strong\nnegative connotation and aggressive tone, this form of verbal irony is intended to ridicule someone or something (Filik et al., 2019). Table 3 presents a selection of sarcastic texts. The clearly aggressive intent inherent in sarcasm is presented in Examples 1 and 2. In Example 1, the intention is to express that someone with seemingly no special qualities is able to have a sense of pride, whilst in Example 2, the choice of the word \u201cmarinate\u201d implies that the target of the comment is wearing too much perfume, similar to how meat may marinate in a glaze for many hours before being ready. Examples 3 and 4 present non-sarcastic utterances, with Example 3 being a blatant insult, and Example 4 demonstrating the expected gratitude someone would have if they had been bought a supercar (assuming no deeper contextualised meaning). As sarcasm is considered to be a subcategory of irony, research into irony detection or generation may often include sarcasm, and treats them as one and the same (Jijkoun and Hofmann, 2009; Filatova, 2012; Van Hee, 2017; Maladry et al., 2022). Moreover, studies by Gibbs (1986) and Bryant and Fox Tree (2002) indicate that there is an ongoing semantic shift, where verbal irony is perceived more often as sarcasm in popular use. Recent papers on sarcasm generation (Mishra et al., 2019; Oprea et al., 2021) do not systematically discuss their definition of sarcasm in relation to the concept of verbal irony. However, one has to assume here that sarcasm is synonymous with verbal irony, since it would be suboptimal for generative systems to create content that might ridicule people in a hurtful way. In short, both for pragmatic reasons as well as conceptual changes, it makes sense to consider irony and sarcasm to be the same thing. Thus, the same motivations that make irony subjective apply to sarcasm as well. On top of that, detection and generation tasks concerning sarcasm specifically also demand that evaluators are able to discern the aggressive tone and retrieve the intention of the author or speaker, which can vary significantly. For example, Phillips et al. (2015) find age to be a\nsignificant contributing factor in an individual\u2019s ability to correctly perceive intended sarcasm, with older individuals struggling more than younger. Furthermore, Rockwell and Theriot (2001) find gender differences in the patterns of sarcasm use to both be affected by the gender of the speaker, as well as the gender of the recipient interlocuter. Additionally, the use of sarcasm has been shown to vary alongside socio-cultural contexts, with Blasko et al. (2021) noting that sarcasm use is more prevalent in individualist countries with a lower power distance such as the U.S than collectivist countries with higher power distance such as China.6 It logically follows, therefore, that these differences may also impact someone\u2019s ability to perceive intended sarcasm, and consequently how they would react to sarcastic utterances in different contexts."
        },
        {
            "heading": "4 Critical Survey",
            "text": "Following existing works that critically review human evaluation procedures in Natural Language Generation, we present a critical survey of works in the area of computational humour, irony, and sarcasm. We begin by reviewing the scope and range of our literature search for this review and the prerequisite criteria for incorporation. Following this, we motivate the need for increased care in this specific domain of language by briefly exploring the prevalence of research in the area, both in terms of generative works, as well as those on the detection of such language. As previously discussed, irony and sarcasm are highly related language forms, resulting in the common use of the terms interchangeably. However, to ensure that the intended definition is respected (the correctness of which cannot always be inferred with complete certainty) we report irony and sarcasm separately based on the titles of the papers.\n6\u201cPower Distance\u201d refers to the distribution of perceived power (political, financial etc.) across a society. A lower power distance means that members of a society are more \u201cequal\u201d, for example, when comparing a blue collar worker to a politician."
        },
        {
            "heading": "4.1 Scope and Selection",
            "text": "The papers included in this survey pool were all taken from the ACL Anthology for the venues of *ACL, EMNLP, COLING, LREC, INLG, and CoNLL, between 2018 and 2023.78 Initially, all papers concerning humour, irony and sarcasm were selected. This was performed by keyword matching titles on the following word stems: \u201chum-\u201d, \u201csarc-\u201d, \u201ciron-\u201d, \u201cfun-\u201d, \u201cjok-\u201d and \u201cpun-\u201d.910 The selected papers include main conference, Findings, and workshop submissions, in addition to one system demonstration, resulting in 259 papers. Submissions to shared-tasks were then excluded, reducing the pool to 135 papers.11 The resulting papers were then checked over to ensure relevance, and categorised broadly into generation or detection (the latter of which may be binary, multi-class, or estimation). These criteria resulted in 135 remaining papers, of which 22 were focused on the generation of humour, sarcasm and/or irony, and 108 on detection (with the remaining 5 being classed as \"other\"). We then excluded 4 of the remaining 22 generation papers that were surveys or dataset papers where human evaluation was performed on non-computationally generated examples. This resulted in a final selection of 18 generation papers, all of which are listed in Appendix A.1."
        },
        {
            "heading": "4.2 Subject Prevalence",
            "text": "As seen in Figure 1, there has been a significant number of accepted papers on these topics within this 6 year scope of collection. In addition to these papers, this search also revealed 5 shared tasks within this period, including a sarcasm detection shared task\n7Where the relevant venues have occurred at the time of writing.\n8INLG and CoNLL contributed 0 papers that met our criteria for inclusion, but were nevertheless surveyed.\n9To capture the most common form of humour generation. 10In order to capture \u201chumour\u201d, \u201csarcasm\u201d, \u201cirony\u201d, \u201cfunny\u201d, \u201cjoke\u201d, and \u201cpuns\u201d, including all derivations and regional spelling. 11This is due to shared task submissions often attracting non-novel approaches, and all following a standardised evaluation procedure as determined by the task organisers. Their inclusion would therefore bias the results.\nat the Figurative Language Processing workshop at ACL 2020 (Ghosh et al., 2020), Task 7 of SemEval 2020 and 2021 on humour detection (Hossain et al., 2020; Faraj and Abdullah, 2021), a WANLP 2021 shared task on sarcasm detection (Abu Farha et al., 2021), and SemEval-2022 Task 6, also on sarcasm detection (Abu Farha et al., 2022).12\nIn addition to the works captured by our literature search and the aforementioned shared tasks, it is reasonable to think that the prevalence of recently released Large Language Models (LLMs) such as ChatGPT and GPT-4 would result in further increases in research into this field, now that models are widely available that are able to better handle the generation and explanation of complex topics such as humour. Not explicitly included in this survey, but nevertheless relevant, are also more general works on anthropomorphism in language-based AI agents (Abercrombie et al., 2021; Gros et al., 2022), in which the ability to detect, generate, and respond to humour, irony and sarcasm are paramount if anthropomorphism is deemed desirable (Oprea et al., 2022)."
        },
        {
            "heading": "4.3 Reporting Transparency",
            "text": "Of the 135 remaining papers, we now move our attention to the 18 focused on generation specifically.\n12As expected, all shared tasks were on detection rather than generation, due to the more simple nature of evaluation for a shared task environment.\nAll but 1 of the 18 papers found also contained human evaluation, and therefore the remaining 17 papers will form the basis of the following analysis. Following on from earlier sections which motivated the significance of participant variables in the interpretation of these language forms, Table 4 presents the percentage of surveyed papers that report on the following evaluator demographic information: language proficiency, location and nationality, age, gender, education level, and social class.\nParticipant Demographics As Table 4 shows, less than half of the assessed works (7 of 17) reported on any demographic information regarding their selected human evaluator panels. Of those that were more transparent in regard to evaluator demographics, language proficiency and location (usually as a proxy for nationality) were the most common detail given. A single paper (Mishra et al., 2019) reported evaluator education.\nImportantly, even in those papers which did outline some demographic information, this reporting was frequently crucially inadequate. For example, Oprea et al. (2022) provide the full participant information sheet that was presented, but reduce the demographic information to \u201cWe target everyone registered as living in \u2329country\u232a on the Prolific Academic platform.\u201d, where the valuable information of which country (or which countries) is redacted. Additionally, the one paper that reported education (Mishra et al., 2019) was only to the extent that the two evaluators \u201chad a better understanding of the language and socio-cultural diversities\u201d - a statement that may range from meaning native speakers of the target language in the desired culture, to people with doctorates in sociology. None of the surveyed papers outlined evaluator age, gender or social class.\nRecruitment As can be seen in Table 5, 12 of the 17 surveyed papers contained explicit mention of participant recruitment methods. Upon further investigation into these cases, we find that all 12 instances concern the use of crowdsourcing platforms, of which 9 are Amazon Mechanical Turk (AMT), 2 are Prolific Academic (Oprea et al., 2021, 2022), and one does not explicitly mention the platform (Tian et al., 2022). Of these 12 papers that report recruitment, compensation amounts are reported in less than half (5 papers), ranging from approximately\n9.23 USD13 (Oprea et al., 2021, 2022) to 20 USD per hour (Tian et al., 2022; Mittal et al., 2022).\nOne reason that transparent reporting of recruitment methods is important is due to allowing inference of potential participant characteristics that may not be reported elsewhere. For instance, research in the social sciences has often been criticised for focusing primarily on WEIRD participants, Western and Educated individuals from Industrialised, Rich and Democratic societies (Henrich et al., 2010). However, as access to language technology becomes more democratised, cheaper to access and easier to use (Simons et al., 2022; de Dios-Flores et al., 2022) the majority of end users of these technologies are not going to be well represented by this set of characteristics that participants often have. This tendency is further reflected by the overwhelming bias towards work on the English language, which leads to the demotion of other non-digitally accessible languages, and reinforces existing inequalities (S\u00f8gaard, 2022). In addition to these characteristics, there has also historically been a bias towards male participants in scientific research (Holdcroft, 2007; Garc\u00eda-Gonz\u00e1lez et al., 2019), something that is also common in AI due to the field of computer science still being dominated by men overall (European Commission and Directorate-General for Research and Innovation, 2019; Breidenbach et al., 2021). Consequently, not only does this result in AI research not involving people from key demographics who will be affected by these technologies, but the research performed on the generated language will lack important replicability for the evaluation process, where heterogeneous eval-\n13This study paid 0.38 GBP for approximately 3 minutes of work. The USD conversion is the exchange rate at the time of writing this survey.\nuation panels are not conducive to easy comparison across studies.14 Whilst the substantial reliance on crowdsourcing platforms, particularly Mechanical Turk, may result in a different set of participants to the typical WEIRD demographic in social science and health research, the lack of reliable worker filters and allowances on what demographic data can be collected by requesters nevertheless results in a biased sample. Early works on the the use of crowdsourcing outside of this survey have even touted the low cost as an affordance of the method, whilst paying as low as 1/10th of a single US cent per individual HIT task (Callison-Burch, 2009). Perspectives such as these therefore result in biased demographics being used for evaluation and annotation at best, and the outright exploitation of people from particular socio-economic backgrounds and regions at worst.\nIn terms of inter-annotator agreement (IAA), we find that just over half of the surveyed papers include explicit mention of statistical tests for agreement levels. IAA is one method through which the variation amongst evaluator opinions can best be analysed, in order to understand how subjective a task is, and consequently how best to assess the quality of an NLG system\u2019s output where there is no \u201cground truth\u201d for a given criteria (e.g. the humour, irony and sarcasm discussed here). Finally, we note that only 4 (of 17) papers convey any information regarding the level of training given to participants, whether that constitute the presence of qualifier tasks on crowdsourcing platforms (Sun et al., 2022a,b) or extensive documentation on the nature of the task (Oprea et al., 2021, 2022). In tasks that are inherently subjective, the provision of adequate training materials can help to account for variation in the understanding of different individuals by orientating opinions, and ensuring that all evaluators are working from the same definition of a particular phenomenon (such as distinguishing between general verbal irony and sarcasm specifically). Especially in cases where IAA is not reported, the inclusion of the extent of this background training given to evaluators is useful to the research community for judging to what extent the evaluators were in-line with each other\u2019s perspectives."
        },
        {
            "heading": "5 Discussion and Proposed Action Points",
            "text": "Our analysis has shown a distinct lack of reporting on potentially relevant demographic information in ad-\n14We exclude the cases where the research was intended for a specific demographic group to start with, in which case a more homogeneous evaluator group may be desirable.\ndition to details concerning the evaluation procedure. To tackle this issue, we suggest future work on the evaluation of (especially subjective) generative tasks to include an \u201cevaluation statement\u201d, akin to the data statements proposed by Bender and Friedman (2018). In line with their work, we suggest the inclusion of the following points for such statements:\n\u2022 evaluation logistics\n\u2013 number of evaluators used \u2013 the split of samples across evaluators (e.g.\ndoes everyone rate every sample) \u2013 recruitment method used (e.g. internal\nemails, posters or crowdsourcing) \u2013 compensation given to evaluators (e.g. gift\ncards and cash - additionally, the choice to not monetarily compensate evaluators should also be divulged)\n\u2022 evaluator demographic information\n\u2013 age (e.g. range, group or generation) \u2013 language proficiency & \u201cnative\u201d language\n(L1) \u2013 gender \u2013 cultural background (e.g. religion, political\nleaning, country of origin and general education level)\n\u2013 task proficiency (i.e. expert or layperson)\n\u2022 evaluation process\n\u2013 clear and operationalised definitions of the evaluated phenomena (with explicit mention if a term with a largely accepted definition is being used in a non-standard way)\n\u2013 amendments made in light of potential pilot studies\n\u2013 training materials and examples of criteria given to evaluators\n\u2022 agreement study (i.e. reporting inter-annotator agreement)\nWhile the language proficiency of the participants is relevant to all tasks, we recognise that other characteristics such as gender and cultural background (e.g. religion and political leaning) may not always apply. Still, it would be beneficial to report which facets were taken into consideration, as they may become relevant in hindsight, and in the case of dataset annotations, facilitate more extensive use of any collected data in future works. Similarly, task proficiency may sometimes seem irrelevant for subjective and intuitive tasks. However, Iskender et al. (2021) and Lloret et al. (2018) demonstrate the preference of expert evaluators for complex linguistic tasks. Additionally, authors may be reluctant to present only moderate agreement for subjective tasks due to concerns regarding evaluation quality. However, this criterion is highly relevant as it describes the expected degree of subjectivity for a task, a concept the community should be able to embrace. In addition to these points, we also encourage increased interest in the area of perspectivist approaches to the study of these language forms, and in NLP as a whole. Finally, the intentional omission of many of these criteria (such as IAA and language proficiency, in particular) may be considered intentionally deceptive, and therefore at odds with the foundations of open science."
        },
        {
            "heading": "6 Conclusion",
            "text": "In conclusion, we reiterate the importance of the role that Human Evaluation plays in assessing the outputs of Natural Language Generation systems. We have further demonstrated why particular types of language such as humour, irony, and sarcasm demand additional attention to be paid during the construction of human evaluation processes, particularly as it pertains to the demographic variables of selected evaluators. In no way do we discourage the use of the \u201cnon-native\u201d or L2+ speakers of a target language in such evaluation (or any other minority demographic), nor do we likewise encourage the use of \u201cnative\u201d or L1 target-language speakers (or any other majority demographic) as the true \u201cgold standard\u201d. Rather, we aim to encourage additional thoughts and reflections in the community when selecting panels of human evaluators for such language, analysing results in light of their demographic characteristics as a feature, rather than a bug, whilst openly reporting such decisions wherever possible. To this end, we provide a list of suggested points that can be included as part of an \u201cevaluation statement\u201d. Additionally, we advocate for the use of stratified sampling techniques to gain more varied evaluator panels.\nLimitations\nThe limitations of this work are as follows. We only survey papers from 2018-2023 in order to capture more recent and modern behaviours in the research discipline. However, this results in a small number of available papers. This is naturally exacerbated by the choice of focusing solely on humour, sarcasm and irony, rather than language generation more generally. However, we believe that surveys of NLG broadly have been performed in recent years to high standards, and instead wish to use these niche language types as specific examples of types of language that are immensely affected by these factors, whilst conveying that transparent reporting and consideration of evaluation practices is entirely applicable to NLG as a whole, and not just the subdomains we focus on. Furthermore, we focus exclusively on the use of human evaluators in generation tasks. However, much of the same ideas apply equally to the selection of annotators during dataset creation for detection tasks. All statements and opinions expressed in this work are by nature in good faith, and we accept, appreciate and welcome perspectives from other individuals from different backgrounds to support, challenge, or revise any of the claims made herein.\nEthics Statement\nWe believe in and firmly adhere to the ACL Code of Conduct in the performance of this survey and the expression of opinions. We review only information that was publicly available in the published versions of the papers we discussed. However, whilst we encourage the collection of a lot of metadata about participants in NLG research, we emphasise that this data should only be collected where the correct ethics application procedures have been followed by the respective researchers, and that collected information should be only as fine-grained as is needed to accurately characterise the participant panel (for example, someone\u2019s hometown is not necessary when their nationality or cultural identity would suffice, and neither is someone\u2019s exact age when 10-year age brackets may be sufficient)."
        },
        {
            "heading": "Acknowledgements",
            "text": "Tyler Loakman is supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UK Research and Innovation [grant number EP/S023062/1]. Aaron Maladry is supported by Ghent University [grant BOF.24Y.2021.0019.01.]"
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Surveyed Generation Papers"
        }
    ],
    "title": "The Iron(ic) Melting Pot: Reviewing Human Evaluation in Humour, Irony and Sarcasm Generation",
    "year": 2023
}