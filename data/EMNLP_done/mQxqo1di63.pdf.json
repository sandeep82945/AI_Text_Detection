{
    "abstractText": "Social media has not only facilitated news consumption, but also led to the wide spread of fake news. Because news articles in social media are usually condensed and full of knowledge entities, existing methods of fake news detection use external entity knowledge to improve the effectiveness. However, the majority of these methods focus on news entity information and ignore the structured relation knowledge among news entities. To address this issue, in this work, we propose a Knowledge grAPh enhAnced Language Model (KAPALM) which is a novel model that fuses coarseand finegrained representations of entity knowledge from Knowledge Graphs (KGs). Firstly, we identify entities in news content and link them to entities in KGs. Then, a subgraph of KGs is extracted to provide structured relation knowledge of entities in KGs and fed into a graph neural network to obtain the coarse-grained knowledge representation. This subgraph is pruned to provide fine-grained knowledge and fed into the attentive graph pooling layer. Finally, we integrate the coarseand fine-grained entity knowledge representations with the representation of news content for fake news detection. The experimental results on two benchmark datasets show that our method is superior to state-of-the-art baselines in the full-scale setting. In addition, our model is competitive in the few-shot setting.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jing Ma"
        },
        {
            "affiliations": [],
            "name": "Chen Chen"
        },
        {
            "affiliations": [],
            "name": "Chunyan Hou"
        },
        {
            "affiliations": [],
            "name": "Xiaojie Yuan"
        }
    ],
    "id": "SP:ca7f5549b49a77083250e822276c6c8ae7db5f97",
    "references": [
        {
            "authors": [
                "Oluwaseun Ajao",
                "Deepayan Bhowmik",
                "Shahrzad Zargari."
            ],
            "title": "Sentiment aware fake news detection on online social networks",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom, May",
            "year": 2019
        },
        {
            "authors": [
                "Pritika Bahad",
                "Preeti Saxena",
                "Raj Kamal."
            ],
            "title": "Fake news detection using bi-directional lstmrecurrent neural network",
            "venue": "Procedia Computer Science, 165:74\u201382. 2nd International Conference on Recent Trends in Advanced Computing ICRTAC -",
            "year": 2019
        },
        {
            "authors": [
                "Kurt Bollacker",
                "Colin Evans",
                "Praveen Paritosh",
                "Tim Sturge",
                "Jamie Taylor."
            ],
            "title": "Freebase: a collaboratively created graph database for structuring human knowledge",
            "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of",
            "year": 2008
        },
        {
            "authors": [
                "Carlos Castillo",
                "Marcelo Mendoza",
                "Barbara Poblete."
            ],
            "title": "Information credibility on twitter",
            "venue": "Proceedings of the 20th International Conference on World Wide Web, WWW 2011, Hyderabad, India, March 28 - April 1, 2011, pages 675\u2013684. ACM.",
            "year": 2011
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "CoRR, abs/1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Yaqian Dun",
                "Kefei Tu",
                "Chen Chen",
                "Chunyan Hou",
                "Xiaojie Yuan."
            ],
            "title": "KAN: knowledge-aware attention network for fake news detection",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Ap-",
            "year": 2021
        },
        {
            "authors": [
                "Paolo Ferragina",
                "Ugo Scaiella."
            ],
            "title": "TAGME: on-the-fly annotation of short text fragments (by wikipedia entities)",
            "venue": "Proceedings of the 19th ACM",
            "year": 2010
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural Comput., 9(8):1735\u2013 1780.",
            "year": 1997
        },
        {
            "authors": [
                "Linmei Hu",
                "Tianchi Yang",
                "Luhao Zhang",
                "Wanjun Zhong",
                "Duyu Tang",
                "Chuan Shi",
                "Nan Duan",
                "Ming Zhou."
            ],
            "title": "Compare to the knowledge: Graph neural fake news detection with external knowledge",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Gongyao Jiang",
                "Shuang Liu",
                "Yu Zhao",
                "Yueheng Sun",
                "Meishan Zhang."
            ],
            "title": "Fake news detection via knowledgeable prompt learning",
            "venue": "Inf. Process. Manag., 59(5):103029.",
            "year": 2022
        },
        {
            "authors": [
                "Yoon Kim."
            ],
            "title": "Convolutional neural networks for sentence classification",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Spe-",
            "year": 2014
        },
        {
            "authors": [
                "Sejeong Kwon",
                "Meeyoung Cha",
                "Kyomin Jung",
                "Wei Chen",
                "Yajun Wang."
            ],
            "title": "Prominent features of rumor propagation in online social media",
            "venue": "2013 IEEE 13th International Conference on Data Mining, Dallas, TX, USA, December 7-10, 2013, pages",
            "year": 2013
        },
        {
            "authors": [
                "Pengfei Liu",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Recurrent neural network for text classification with multi-task learning",
            "venue": "Proceedings of the TwentyFifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July",
            "year": 2016
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Jing Ma",
                "Wei Gao",
                "Prasenjit Mitra",
                "Sejeong Kwon",
                "Bernard J Jansen",
                "Kam-Fai Wong",
                "Meeyoung Cha."
            ],
            "title": "Detecting rumors from microblogs with recurrent neural networks",
            "venue": "Proceedings of the 25th International Joint Conference on Artificial Intelli-",
            "year": 2016
        },
        {
            "authors": [
                "David N. Milne",
                "Ian H. Witten."
            ],
            "title": "Learning to link with wikipedia",
            "venue": "Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM 2008, Napa Valley, California, USA, October 26-30, 2008, pages 509\u2013518. ACM.",
            "year": 2008
        },
        {
            "authors": [
                "Kellin Pelrine",
                "Jacob Danovitch",
                "Reihaneh Rabbany."
            ],
            "title": "The surprising performance of simple baselines for misinformation detection",
            "venue": "Proceedings of the Web Conference 2021, pages 3432\u20133441.",
            "year": 2021
        },
        {
            "authors": [
                "Matthew E. Peters",
                "Mark Neumann",
                "Mohit Iyyer",
                "Matt Gardner",
                "Christopher Clark",
                "Kenton Lee",
                "Luke Zettlemoyer."
            ],
            "title": "Deep contextualized word representations",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
            "year": 2018
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Andreas R\u00fcckl\u00e9",
                "Clifton Poth",
                "Aishwarya Kamath",
                "Ivan Vuli\u0107",
                "Sebastian Ruder",
                "Kyunghyun Cho",
                "Iryna Gurevych."
            ],
            "title": "Adapterhub: A framework for adapting transformers",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
            "year": 2020
        },
        {
            "authors": [
                "Piotr Przybyla."
            ],
            "title": "Capturing the style of fake news",
            "venue": "The Thirty-Fourth Conference on Artificial Intelligence, pages 490\u2013497. AAAI Press.",
            "year": 2020
        },
        {
            "authors": [
                "Mohammadreza Samadi",
                "Maryam Mousavian",
                "Saeedeh Momtazi."
            ],
            "title": "Deep contextualized text representation and learning for fake news detection",
            "venue": "Inf. Process. Manag., 58(6):102723.",
            "year": 2021
        },
        {
            "authors": [
                "Qiang Sheng",
                "Xueyao Zhang",
                "Juan Cao",
                "Lei Zhong."
            ],
            "title": "Integrating pattern-and fact-based fake news detection via model preference learning",
            "venue": "Proceedings of the 30th ACM international conference on information & knowledge management, pages 1640\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Kai Shu",
                "Limeng Cui",
                "Suhang Wang",
                "Dongwon Lee",
                "Huan Liu."
            ],
            "title": "defend: Explainable fake news detection",
            "venue": "Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 395\u2013405.",
            "year": 2019
        },
        {
            "authors": [
                "Kai Shu",
                "Deepak Mahudeswaran",
                "Suhang Wang",
                "Dongwon Lee",
                "Huan Liu."
            ],
            "title": "Fakenewsnet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media",
            "venue": "Big Data, 8(3):171\u2013188.",
            "year": 2020
        },
        {
            "authors": [
                "Kai Shu",
                "Amy Sliva",
                "Suhang Wang",
                "Jiliang Tang",
                "Huan Liu."
            ],
            "title": "Fake news detection on social media: A data mining perspective",
            "venue": "SIGKDD Explor., 19(1):22\u201336.",
            "year": 2017
        },
        {
            "authors": [
                "Avirup Sil",
                "Alexander Yates."
            ],
            "title": "Re-ranking for joint named-entity recognition and linking",
            "venue": "22nd ACM International Conference on Information and Knowledge Management, CIKM\u201913, San Francisco,",
            "year": 2013
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Petar Velickovic",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "CoRR, abs/1710.10903.",
            "year": 2017
        },
        {
            "authors": [
                "Soroush Vosoughi",
                "Deb Roy",
                "Sinan Aral."
            ],
            "title": "The spread of true and false news online",
            "venue": "Science, 359(6380):1146\u20131151.",
            "year": 2018
        },
        {
            "authors": [
                "Denny Vrandecic",
                "Markus Kr\u00f6tzsch."
            ],
            "title": "Wikidata: a free collaborative knowledgebase",
            "venue": "Commun. ACM, 57(10):78\u201385.",
            "year": 2014
        },
        {
            "authors": [
                "Hongwei Wang",
                "Fuzheng Zhang",
                "Xing Xie",
                "Minyi Guo."
            ],
            "title": "DKN: deep knowledge-aware network for news recommendation",
            "venue": "CoRR, abs/1801.08284.",
            "year": 2018
        },
        {
            "authors": [
                "William Yang Wang."
            ],
            "title": "liar, liar pants on fire\u201d: A new benchmark dataset for fake news detection",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 422\u2013426.",
            "year": 2017
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "year": 2020
        },
        {
            "authors": [
                "Fan Yang",
                "Yang Liu",
                "Xiaohui Yu",
                "Min Yang."
            ],
            "title": "Automatic detection of rumor on sina weibo",
            "venue": "Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics, MDS \u201912, New York, NY, USA. Association for Computing Machinery.",
            "year": 2012
        },
        {
            "authors": [
                "Xueyao Zhang",
                "Juan Cao",
                "Xirong Li",
                "Qiang Sheng",
                "Lei Zhong",
                "Kai Shu."
            ],
            "title": "Mining dual emotion for fake news detection",
            "venue": "WWW \u201921: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021, pages 3465\u20133476. ACM / IW3C2.",
            "year": 2021
        },
        {
            "authors": [
                "Wanjun Zhong",
                "Jingjing Xu",
                "Duyu Tang",
                "Zenan Xu",
                "Nan Duan",
                "Ming Zhou",
                "Jiahai Wang",
                "Jian Yin."
            ],
            "title": "Reasoning over semantic-level graph for fact checking",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Jie Zhou",
                "Xu Han",
                "Cheng Yang",
                "Zhiyuan Liu",
                "Lifeng Wang",
                "Changcheng Li",
                "Maosong Sun."
            ],
            "title": "GEAR: graph-based evidence aggregating and reasoning for fact verification",
            "venue": "Proceedings of the",
            "year": 2019
        },
        {
            "authors": [
                "Xinyi Zhou",
                "Reza Zafarani."
            ],
            "title": "A survey of fake news: Fundamental theories, detection methods, and opportunities",
            "venue": "ACM Comput. Surv., 53(5):109:1\u2013 109:40.",
            "year": 2021
        },
        {
            "authors": [
                "Arkaitz Zubiaga",
                "Maria Liakata",
                "Rob Procter."
            ],
            "title": "Exploiting context for rumour detection in social media",
            "venue": "Social Informatics - 9th International Conference, SocInfo 2017, Oxford, UK, September 13-15, 2017, Proceedings, Part I, volume 10539 of Lecture",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, with the development of the Internet, social media such as Facebook and Twitter have become the main platforms for people to consume news due to their real-time and easy access. However, social media is a double-edged sword and it enables people to be exposed to fake news. The wide spread of fake news can misguide public opinion, threaten people\u2019s health, and even cause devastating effects on society (Vosoughi et al., 2018).\n\u2217Corresponding author.\nThus, the research on automatic fake news detection is desirable.\nPrior researches are based on traditional machine learning methods and hand-crafted features based on news content to combat fake news on social media (Castillo et al., 2011;Kwon et al., 2013;Zubiaga et al., 2017;Przybyla, 2020;). To avoid manual features, deep neural networks models such as Convolutional Neural Networks (CNN) and Recurrent Neural Network (RNN), have been used to learn the high-level feature representations automatically and achieve great performance in detecting fake news (Ma et al., 2016; Wang, 2017; Shu et al., 2019). For the past few years, Pre-trained language models (PLMs), such as BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019), have been the mainstream approaches which provide the contextualized representation of textual content in natural language processing field and also achieve the competitive performance for fake news detection (Pelrine et al., 2021; Sheng et al., 2021). However, these works fail to consider the knowledge entities in news articles. Although news content is condensed and full of knowledge entities by which people usually verify the veracity of news content, PLMs are not effective in capturing the knowledgelevel relationship between entities. As shown in the left of Figure 1, it is hard to detect the veracity of news article exactly without the entity knowledge about Dennis Bray and Hans von Storch.\nLarge Knowledge Graphs (KGs), such as Freebase (Bollacker et al., 2008) and Wikidata (Vrandecic and Kr\u00f6tzsch, 2014), contain a large number of structured triplets, which can serve as the entity knowledge base for detecting fake news. Existing researches have demonstrated the significant role of KGs (Pan et al., 2018; Dun et al., 2021; Jiang et al., 2022). Specifically, Dun et al. (2021) proposed knowledge attention networks to measure the importance of knowledge and incorporate both semantic-level and knowledge-level representations of news content. Jiang et al. (2022) proposed a knowledge prompt learning method which incorporated KGs into the prompt representation to make it more expressive for verbal word prediction. Hu et al. (Hu et al., 2021) explored the structural entity embedding and compared contextual entity representations with the corresponding entity representations in the knowledge graph. However, they focus on the textual representation of entities in KGs and have not explored the in-depth structured knowledge associated with the new article. As shown in Figure 1, we construct a subgraph of KGs for the news article which is named as entity graph throughout this paper, and this entity graph is helpful to detect fake news with the knowledge that \u201cDennis Bray is a biologist and Hans von Storch is a climatologist and meteorologist\u201d.\nIn this paper, we proposed a Knowledge grAPh enhAnced Language Model (KAPALM) which is enabled to explore both coarse- and fine-grained knowledge in KGs for fake news detection. Firstly, we identify entities from news content and link them to the large-scale KGs. An entity graph is constructed by extracting the first-order neighbors of identified entities, and fed into a Graph Neural Network (GNN) to obtain coarse-grained structured knowledge representations. Then, we prune the entity graph and feed it into the attentive graph pooling layer for fine-grained knowledge representations. Finally, we integrate coarse- and fine-grained knowledge representations with the representation of news content encoded by PLMs for fake news detection. The primary contributions of this paper can be concluded as follows:\n\u2022 We propose a method to construct an entity graph for each news article. Our method is enabled to extract coarse-grained structured knowledge from the entity graph and provide fine-grained knowledge by the pruned entity graph.\n\u2022 We propose a Knowledge grAPh enhAnced Language Model (KAPALM) to integrate the coarse- and fine-grained knowledge representations with the representation of news content for fake news detection.\n\u2022 We compare our model with the state-of-theart methods on two benchmark datasets in few-shot and full-scale settings. The experimental results demonstrate that our model outperforms the state-of-the-art methods in the full-scale setting. Moreover, our model is competitive in the few-shot setting."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Fake News Detection",
            "text": "In this section, we will briefly introduce the related work on fake news detection. This work (Zhou and Zafarani, 2021) outlines early methods which design handcrafted features and utilize statistical machine learning methods to detect the authenticity of a given news. These work (Castillo et al., 2011;Przybyla, 2020;Kwon et al., 2013;Zubiaga et al., 2017) proposed to utilize statistical information to leverage textual and social features to detect fake news. Also, some works (Ajao et al., 2019;Zhang et al., 2021) focus on capturing sentiment features to better detect fake news. Next, we will explore relevant research on the application of deep learning techniques in the task of detecting fake news. With the development of deep learning techniques, some models such as CNN, RNN, and Transformer(Kim, 2014;Liu et al., 2016;Vaswani et al., 2017) are being used for fake news detection(Dun et al., 2021;Ma et al., 2016;Samadi et al., 2021;Shu et al., 2019;Jiang et al., 2022). Dun et al.(2021) proposed a approach which applied a knowledge attention network to incorporate external knowledge based on Transformer. Jiang et al.(2022) proposed KPL which incorporated prompt learning on the PLMs for the first time and enriched prompt template representations with entity knowledge and achieved state-of-the-art performance on two benchmark datasets, PolitiFact(Shu et al., 2020) and Gossipcop(Shu et al., 2017)."
        },
        {
            "heading": "2.2 Knowledge Graphs",
            "text": "External knowledge can provide necessary supplementary information for detecting fake news. It is stored in a knowledge graph format, which contains information about entities in the real world,\nsuch as people, places, etc. Nowadays, knowledge graphs are used in various natural language processing applications, such as news recommendation(Wang et al., 2018), fact verification(Zhong et al., 2020;Zhou et al., 2019), and fake news detection(Dun et al., 2021;Jiang et al., 2022). Among them, Dun et al.(2021) designed two attention mechanisms of news towards entities and news towards entities and entity context to capture the different importance of entities in detecting fake news. Jiang et al.(2022) utilized a prompt template and integrated entities extracted from news articles into the prompt representation for fake news detection. Hu et al. (Hu et al., 2021) proposed a novel graph neural model, which compared the news to the knowledge graph through entities for fake news detection. The limitations of the these methods are that they all focus on news entity information and ignore the structured information of KGs associated with the news article. Our proposed model can learn knowledge from large-scale PLMs, acquire entity and topological structure information from external KGs, and improve the performance of fake news detection."
        },
        {
            "heading": "3 Model",
            "text": "We illustrate the framework of KAPALM in Figure 2. The input of KAPALM consists of the news content and the subgraph constructed from KGs. The output of the model is the label for binary classification of fake news detection. First, for each piece of news, we use a pre-trained language model with the adapter layer (Pfeiffer et al., 2020) to encode the text of news content. Next, entity linking is used to identify entities in new content and we extract\nthese entities and their corresponding first-order neighbors in the knowledge graph to construct the coarse-grained entity graph of the news. Entity extraction and entity graph construction are described in the knowledge extraction section. Then, we feed the constructed entity graph into a graph attention network (Velickovic et al., 2017) to generate a new entity graph that incorporates both the knowledge graph and entity information. We use the interaction node in the entity graph to represent coarsegrained entity knowledge. Furthermore, to obtain fine-grained entity knowledge representation, we prune the constructed entity graph and then use an attention mechanism for graph pooling. Finally, we concatenate the representations of the news content, coarse- and fine-grained entity knowledge representation, and feed them into a fully connected neural network to obtain the prediction for fake news detection."
        },
        {
            "heading": "3.1 Text Encoder",
            "text": "This module is to generate the representation of the news content. We utilize a pre-trained language model with Adapter (Pfeiffer et al., 2020) to encode the news content for capturing the semantic information contained in the news article. The pretrained language model is usually pre-trained on a large amount of textual corpus, and is able to capture the semantics of the news content. However, when the pre-training task is not similar to the downstream task, fine-tuning pre-trained language model is required to achieve the state-of-the-art performance on the downstream task. In addition, the large number of parameters in pre-trained language model can lead to the high time and space\ncosts during the full fine-tuning. Thus, adapter turning is used as a lightweight fine-tuning strategy that achieves competitive performance to full fine-tuning on downstream tasks. Adapters add a small set of additional parameters at every layer of Transformer (Vaswani et al., 2017). The parameters of pre-trained language model are kept frozen, and the parameters of adapters are trained during fine-tuning. The adapter layer can be represented as the following equation:\nh = h+WupReLU(hWdown) (1)\nwhere Wup and Wdown are parameter matrixes."
        },
        {
            "heading": "3.2 Knowledge Encoder",
            "text": ""
        },
        {
            "heading": "3.2.1 Entity Graph Construction and Pruning",
            "text": "This module is used to extract relevant entities from the knowledge graph and construct two subgraphs. Figure 3 illustrates the pipeline process which includes entity linking, entity graph construction, and entity graph pruning. Firstly, we use entity linking (Milne and Witten, 2008;Sil and Yates, 2013) to extract the entities mentions from the news content, align them with entities in the knowledge graph, and obtain their first-order neighbors in the knowledge graph. As a result, we obtain the entity set E = {e1, e2, ..., en} and its first-order neighbor set N = \u22c3 ei\u2208E Nei , where Nei is the first-order neighbor set of ei. Secondly, we construct the entity graph G. In order to better aggregate entity graph information, we create an interaction node that is connected to every ei \u2208 E, and then connect each ei to its corresponding first-order neighbors in Nei .\nDue to the large number of entities in the constructed entity graph G, the information associated with relevant entities in news content cannot be effectively captured. Therefore, we prune the entity graph to retain only the neighbors on one path of a pair of entities. In other words, we remove the first-order neighbors with a degree of one in the entity graph. In this case, we obtain the pruned entity graph G\u2032, which consists of all entities in E and their remained first-order neighbors N \u2032 = \u22c3 ei\u2208E N \u2032 ei , where N \u2032 ei is remained firstorder neighbors of ei."
        },
        {
            "heading": "3.2.2 Coarse-grained Knowledge",
            "text": "In this module, we aim to obtain coarse-grained knowledge representations for entities in the entity graph. After the construction of the entity graph,\nwe use PLMs to initialize the representations of nodes. While the representation of the interaction node is initialized with the embedding of [CLS] token for the news content, the initialization of other entity nodes is provided by encoding the text of the entity name. As shown in the bottom of Figure 2, the initialized entity graph is fed into a Graph Attention Network (Velickovic et al., 2017) to aggregate the information among all entities. The representation of interaction node is enabled to integrate entity knowledge with the contextual representation of news content. Because the entity graph includes many noisy entities, we input only the representation of interaction node which is named the coarse-grained knowledge representation. The hidden representation of coarse-grained knowledge a is calculated as follows:\na = Interaction(GAT (G)) (2)\nwhere G denotes the entity graph and function Interaction returns the representation of the interaction node."
        },
        {
            "heading": "3.2.3 Fine-grained Knowledge",
            "text": "After pruning the entity graph, we feed the pruned graph into the attentive graph pooling layer to extract the fine-grained entity knowledge representation. Entities in the entity graph, especially those first-order neighbor entities, do not have the same role in detecting the veracity of news articles. Therefore, we propose to utilize the attentive graph pooling layer which is based on the multi-head attention mechanism to measure the importance of different entities to fake news detection. The output of the attentive graph pooling layer is called the fine-grained knowledge representation. Attentive Graph Pooling Layer As shown in Figure 2, the query is the hidden state of the news article encoded by PLMs, while both the key and value have the same representation which is derived from the hidden states of ei \u2208 E \u22c3 N \u2032 where E and N \u2032 denote the entity set and the remained neighbour set respectively. Each entity is assigned a corresponding weight by calculating the similarity between the news and this entity. The attention formula is shown as follows:\nQ = WQh,K = WKc, V = WV c (3)\ns = Attention(Q,K, V ) = Softmax( QKT\u221a\ndk )V\n(4)\nwhere h and c denote the hidden states of a news article and an entity in E \u22c3 N \u2032 respectively, and WQ, WK and WV are parameter matrices."
        },
        {
            "heading": "3.3 Knowledge Fusion",
            "text": ""
        },
        {
            "heading": "3.3.1 Deep Neural Network Classifier",
            "text": "We concatenate the representations of the news content h, coarse-grained knowledge representation a, and fine-grained knowledge representation s to obtain the final representation z. Then, z is fed into a fully connected layer followed by a softmax function to predict the probability distribution P of the news article\u2019s label.\nz = Concat(h, a, s) (5)\nP = Softmax(Woz + bo) (6)\nThe model is trained to minimize the following cross-entropy loss function.\nJ = \u2212 \u2211 i\u2208T Yilog(Pi) + \u03bb 2 ||\u0398||2 (7)\nwhere T refers to the training dataset, Pi and Yi denote the distributions of the prediction and true label of the sample i, \u03bb denotes the coefficient of L2 regularization, and \u0398 denotes the model parameters."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "To evaluate the proposed model, we conduct experiments on two datasets PolitiFact and Gossipcop that are included in a benchmark datasets called FakeNewsNet (Shu et al., 2020). The detail of these news datasets are shown in Table 1. We study our proposed model in both few-shot and full-scale settings. Few-shot settings For the purpose of replicating low-resource situations in real-world scenarios, we randomly select k \u2208(2, 4, 8, 16, 100) news articles as the training set and create the validation set of the same size. The other news articles are used as the test set. We follow (Jiang et al., 2022), and sample the few-shot data by 10 random seeds and use the average value calculated after deleting the maximum and minimum scores as the final score. Full-scale settings For a dataset, we reserve 10% of the news articles as the validation set, and 5- fold cross validation is conducted on the other new articles. Finally, the average score is reported."
        },
        {
            "heading": "4.2 Implementation Details",
            "text": "TagMe (Ferragina and Scaiella, 2010) is adopted to extract knowledge mentions from news articles. We utilize the BERT-base version with adapter as the pre-trained language model (Pfeiffer et al., 2020) to extract text features, which is based on the HuggingFace Transformer Library (Wolf et al.,\n2020). For the coarse-grained entity knowledge, we use GAT (Velickovic et al., 2017) as our graph neural network model. In the attentive graph pooling layer, we set the attention head to 2. The size of hidden layer is set to 200 and the dropout rate to 0.2 in the MLP layer. Adam is used to optimize the model\u2019s parameters in the training, The learning rate is 1e-5 and the batch size is 10 for training model.\nBecause our aim focuses on detecting fake news, fake news articles are regarded as positive examples and F1-score (F1) is used as the evaluation metric to measure the classification performance."
        },
        {
            "heading": "4.3 Baselines",
            "text": "We compare our proposed model with the following baselines:\n(1) DTC (Castillo et al., 2011): DTC is the decision tree model, which detects the authenticity of news by utilizing hand-crafted features.\n(2) RFC (Kwon et al., 2013): RFC is the random forest classifier based on hand-crafted features to detect whether news is true or fake.\n(3) SVM (Yang et al., 2012): SVM denotes a classification model that uses a hyperplane to separate news into true and fake news in high dimensional feature space.\n(4) TextCNN (Kim, 2014): TextCNN is a popular deep learning model for text classification, which applies convolutional filters with various window sizes to extract text features. These features are fed into pooling layer to furtherly capture the most salient features to judge whether news is true or fake.\n(5) BiLSTM (Bahad et al., 2019;Hochreiter and Schmidhuber, 1997): BiLSTM denotes the bidirectional long short-term memory which introduces a two-directional recurrent netural\nnetwork architecture to better capture the temporal dependencies in text data. We utilize it to judege news\u2019s authenticity.\n(6) KCNN (Wang et al., 2018): KCNN is a CNNbased model which concatenates news embedding and knowledge entities embedding to learn the representation of news.\n(7) KLSTM (Liu et al., 2016;Wang et al., 2018): Similar to KCNN, KLSTM change the CNN module to BiLSTM, which has achieved comptitive results in fake news detection task.\n(8) FB (Peters et al., 2018): FB denotes the feature-based method of utilizing the pretrain language model for feature extraction. BERT base version is used in the experiments.\n(9) FFT (Devlin et al., 2018): FFT is the full fine-tune approach based on BERT base.\n(10) KAN (Dun et al., 2021): KAN is a knowledgeaware attention network which incorporates external knowledge entities through attention mechanisms to predict the veracity of news articles.\n(11) KPL (Jiang et al., 2022): KPL is the state-ofart model which designs one prompt template and incorporates external knowledge entities into the prompt representation."
        },
        {
            "heading": "4.4 Experimental Results",
            "text": "Experiments are conducted in both few-shot and full-scale settings on two datasets. Baseline models are divided into five categories: traditional statistical methods (i.e., DTC, RFC, SVM), neural network methods without external knowledge (i.e., TextCNN, BiLSTM), neural network methods with external knowledge (i.e., KCNN, KLSTM, KAN), pre-trained language model methods without external knowledge (i.e., FB, FFT), and pre-trained language model methods with external knowledge (i.e., KPL).\nThe experimental results are presented in Table 2. We draw some conclusions in the few-shot setting. First, although the performance is not stable when the number of training data varies, we observe that the increase of training data usually gives rise to the improvement. Second, the deep learning methods without external knowledge are usually worse than the statistical methods. The cause may be that\ndeep learning methods have more parameters than statistical methods and are easy to be over-fitted by the lack of training data. Third, neural network methods with external knowledge are better than those without external knowledge. The results demonstrate that knowledge integration can alleviate the over-fitting problem to some extent. Four, the pre-trained language models can improve the effectiveness of fake news detection generally. Our model is better than KPL in most cases, but wrose\nthan KPL in the few-shot setting on the PolitiFact dataset.\nIn the full-scale setting, our method outperforms all baselines on two datasets and achieves the highest F1 score. Specifically, KPL is the recent state-of-art model that incorporates large-scale pre-\ntrained models and external knowledge, and our model is better than KPL with +1.82 and +2.45 improvement on PolitiFact and Gossipcop datasets respectively. Thus, our model can effectively adopt both external knowledge and pre-training language model. In addition, neural network methods with external knowledge are superior to those without external knowledge. It suggest that knowledge can also improve the effectiveness of fake news detection when the training data is sufficient."
        },
        {
            "heading": "4.5 Analysis",
            "text": "We conduct a detailed analysis from different perspectives to demonstrate the effectiveness of the modules in our model for fake news detection."
        },
        {
            "heading": "4.5.1 Ablation study",
            "text": "We conduct the ablation study to validate the effectiveness of the coarse- and fine-grained knowledge in our approach. Table 3 presents the experimental results of the models on two datasets. The coarse-grained knowledge is represented by the interaction node in the entity graph while the fine-grained knowledge is provided by the attentive graph pooling layer. If the attentive graph pooling layer is eliminated from our model, the F1 drops by 1.34 on the PolitiFact dataset, and F1 decreases by 0.69 on the Gossipcop dataset. When the interaction node is removed from our model, the performance of our model declines by 1.6 F1 on the PolitiFact dataset and by 0.66 F1 on the Gossipcop dataset. Therefore, both the coarse-grained knowledge and fine-grained knowledge are important for our model and able to improve the effectiveness of fake news detection independently.\nTo validate the contribution of adapting tuning, we conduct an ablation study experiment on two datasets. The results listed in Table 4 demonstrate the superiority of adapter tuning. When the adapter turning method is changed to Full Fine-\nTuning (FFT) or Feature-Based (FB), our model is worse than \u201cFB+Graph\u201d and \u201cFFT+Graph\u201d on two datasets. Our model can benefit from adapter tuning.\nWe conduct the ablation study to validate the necessity of pruning the entity graph before feeding into the attentive graph pooling layer. We feed the pruned entity graph into both GAT layer and the attentive graph pooling layer. As Table 5 demonstrated, the performance of using both the coarseand fine-grained knowledge is better than just using fine-grained knowledge on two datasets. Thus, coarse-grained knowledge representations are beneficial to fake news detection."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose the Knowledge grAPh enhAnced Language Model (KAPALM) for fake news detection. The proposed model integrates coarse- and fine-grained representations of entity knowledge. Entity graph is used to enrich the knowledge representation of the news article. The entity graph is pruned and fed into the attentive graph pooling layer to represent the fine-grained knowledge. The coarse- and fine-grained knowledge representations extracted from large-scale knowledge graph are combined for improving the fake news detection. Experimental result on two benchmark datasets have shown that the proposed KAPALM outperforms the state-of-the-art baseline models. In addition, KAPALM is able to obtain the competitive performance in the few-shot setting. In future work, we will investigate other effective approaches to mine the accurate knowledge from knowledge graph for fake news detection."
        },
        {
            "heading": "6 Limitations",
            "text": "One limitation of our model is that the constructed entity knowledge graph fails to consider the multiple types of relationships or extra attribute information of entities and relationships. If the veracity of a news article is associated with the attributes of the entities or the relationships among entities in this article, it may lead to the poor performance. In addition, although graph neural networks and attentive graph pooling layers can improve the performance in fake news detection, there is still a lack of the Interpretability for our model."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was partially supported by the NSFCGeneral Technology Joint Fund for Basic Research (No. U1936206, U1936105), the National Natural Science Foundation of China (No. 62372252, 62172237, 62077031, 62176028, 62302245), Ministry of Education of the People\u2019s Republic of China Humanities and Social Sciences Youth Foundation (No. 63232114). We thank the AC, SPC, PC and reviewers for their insightful comments on this paper."
        }
    ],
    "title": "KAPALM: Knowledge grAPh enhAnced Language Model for Fake News Detection",
    "year": 2023
}