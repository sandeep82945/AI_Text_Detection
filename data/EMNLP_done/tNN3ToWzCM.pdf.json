{
    "abstractText": "We tackle Aspect Term Extraction (ATE), a task of automatically extracting aspect terms from sentences. The current Pretrained Language Model (PLM) based extractors have achieved significant improvements. They primarily benefit from context-aware encoding. However, a considerable number of sentences in ATE corpora contain uninformative or low-quality contexts. Such sentences frequently act as \u201ctroublemakers\u201d during test. In this study, we explore the context-oriented quality improvement method. Specifically, we propose to automatically rewrite the sentences from the perspectives of virtual experts with different roles, such as a \u201cchef\u201d in the restaurant domain. On this basis, we perform ATE over the paraphrased sentences during test, using the well-trained extractors without any change. In the experiments, we leverage ChatGPT to determine virtual experts in the considered domains, and induce ChatGPT to generate paraphrases conditioned on the roles of virtual experts. We experiment on the benchmark SemEval datasets, including Laptop-domain L14 and Restaurantdomain R14-16. The experimental results show that our approach effectively recalls the inconspicuous aspect terms like \u201cal di la\u201d, although it reduces the precision. In addition, it is proven that our approach can be substantially improved by redundancy elimination and multi-role voting. More importantly, our approach can be used to expand the predictions obtained on the original sentences. This yields state-of-the-art performance (i.e., F1-scores of 86.2%, 89.3%, 77.7%, 82.7% on L14 and R14-16) without retraining or fine-tuning the baseline extractors.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiaxiang Chen"
        },
        {
            "affiliations": [],
            "name": "Yu Hong"
        },
        {
            "affiliations": [],
            "name": "Qingting Xu"
        },
        {
            "affiliations": [],
            "name": "Jianmin Yao"
        }
    ],
    "id": "SP:d791df8f8b50081734409565cc989e4a1a5d765b",
    "references": [
        {
            "authors": [
                "Hao Chen",
                "Zepeng Zhai",
                "Fangxiang Feng",
                "Ruifan Li",
                "Xiaojie Wang."
            ],
            "title": "Enhanced multi-channel graph convolutional network for aspect sentiment triplet extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Jiaxiang Chen",
                "Yu Hong",
                "Qingting Xu",
                "Jianmin Yao",
                "Guodong Zhou."
            ],
            "title": "Enhancing neural aspect term extraction using part-of-speech and syntactic dependency features",
            "venue": "2022 IEEE 34th International",
            "year": 2022
        },
        {
            "authors": [
                "Wei Chen",
                "Jinglong Du",
                "Zhao Zhang",
                "Fuzhen Zhuang",
                "Zhongshi He."
            ],
            "title": "A hierarchical interactive network for joint span-based aspect-sentiment analysis",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 7013\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Yuqi Chen",
                "Chen Keming",
                "Xian Sun",
                "Zequn Zhang."
            ],
            "title": "A span-level bidirectional network for aspect sentiment triplet extraction",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4300\u20134309, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Zhuang Chen",
                "Tieyun Qian."
            ],
            "title": "Enhancing aspect term extraction with soft prototypes",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2107\u20132117, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Maryna Chernyshevich."
            ],
            "title": "IHS R&D Belarus: Cross-domain extraction of product features using CRF",
            "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 309\u2013313, Dublin, Ireland. Association for Computa-",
            "year": 2014
        },
        {
            "authors": [
                "Hongliang Dai",
                "Yangqiu Song."
            ],
            "title": "Neural aspect and opinion term extraction with mined rules as weak supervision",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5268\u20135277, Florence, Italy. Association for",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Ridong Han",
                "Tao Peng",
                "Chaohao Yang",
                "Benyou Wang",
                "Lu Liu",
                "Xiang Wan."
            ],
            "title": "Is information extraction solved by chatgpt? an analysis of performance, evaluation criteria, robustness and errors",
            "venue": "arXiv preprint arXiv:2305.14450.",
            "year": 2023
        },
        {
            "authors": [
                "Mengting Hu",
                "Yike Wu",
                "Hang Gao",
                "Yinhao Bai",
                "Shiwan Zhao."
            ],
            "title": "Improving aspect sentiment quad prediction via template-order data augmentation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Niklas Jakob",
                "Iryna Gurevych."
            ],
            "title": "Extracting opinion targets in a single and cross-domain setting with conditional random fields",
            "venue": "Proceedings of the",
            "year": 2010
        },
        {
            "authors": [
                "Akbar Karimi",
                "Leonardo Rossi",
                "Andrea Prati."
            ],
            "title": "Adversarial training for aspect-based sentiment analysis with bert",
            "venue": "2020 25th International Conference on Pattern Recognition (ICPR), pages 8797\u20138803.",
            "year": 2021
        },
        {
            "authors": [
                "Ayal Klein",
                "Oren Pereg",
                "Daniel Korat",
                "Vasudev Lal",
                "Moshe Wasserblat",
                "Ido Dagan."
            ],
            "title": "Opinionbased relational pivoting for cross-domain aspect term extraction",
            "venue": "Proceedings of the 12th Workshop on Computational Approaches to Subjectivity,",
            "year": 2022
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Yoshua Bengio",
                "Patrick Haffner."
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE, 86(11):2278\u20132324.",
            "year": 1998
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Kun Li",
                "Chengbo Chen",
                "Xiaojun Quan",
                "Qing Ling",
                "Yan Song."
            ],
            "title": "Conditional augmentation for aspect term extraction via masked sequence-tosequence generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "talia Loukachevitch",
                "Evgeniy Kotelnikov",
                "Nuria Bel",
                "Salud Mar\u00eda"
            ],
            "title": "Jim\u00e9nez-Zafra, and G\u00fcl\u015fen Eryi\u011fit",
            "venue": "SemEval",
            "year": 2016
        },
        {
            "authors": [
                "Maria Pontiki",
                "Dimitris Galanis",
                "Haris Papageorgiou",
                "Suresh Manandhar",
                "Ion Androutsopoulos."
            ],
            "title": "SemEval-2015 task 12: Aspect based sentiment analysis",
            "venue": "SemEval 2015, pages 486\u2013495, Denver, Colorado.",
            "year": 2015
        },
        {
            "authors": [
                "Maria Pontiki",
                "Dimitris Galanis",
                "John Pavlopoulos",
                "Harris Papageorgiou",
                "Ion Androutsopoulos",
                "Suresh Manandhar."
            ],
            "title": "SemEval-2014 task 4: Aspect based sentiment analysis",
            "venue": "SemEval 2014, pages 27\u201335, Dublin, Ireland.",
            "year": 2014
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "I\u00f1aki San Vicente",
                "Xabier Saralegi",
                "Rodrigo Agerri."
            ],
            "title": "EliXa: A modular and flexible ABSA platform",
            "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 748\u2013752, Denver, Colorado. Association for Compu-",
            "year": 2015
        },
        {
            "authors": [
                "Zhiqiang Toh",
                "Wenting Wang."
            ],
            "title": "DLIREC: Aspect term extraction and term polarity classification system",
            "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 235\u2013240, Dublin, Ireland. Association for Computa-",
            "year": 2014
        },
        {
            "authors": [
                "Qianlong Wang",
                "Zhiyuan Wen",
                "Qin Zhao",
                "Min Yang",
                "Ruifeng Xu."
            ],
            "title": "Progressive self-training with discriminator for aspect term extraction",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 257\u2013268,",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Zhenkai Wei",
                "Yu Hong",
                "Bowei Zou",
                "Meng Cheng",
                "Jianmin Yao."
            ],
            "title": "Don\u2019t eclipse your arts due to small discrepancies: Boundary repositioning with a pointer network for aspect extraction",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "Hu Xu",
                "Bing Liu",
                "Lei Shu",
                "Philip Yu."
            ],
            "title": "BERT post-training for review reading comprehension and aspect-based sentiment analysis",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2019
        },
        {
            "authors": [
                "Hu Xu",
                "Bing Liu",
                "Lei Shu",
                "Philip S. Yu."
            ],
            "title": "Double embeddings and CNN-based sequence labeling for aspect extraction",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pages 592\u2013598, Melbourne, Australia.",
            "year": 2018
        },
        {
            "authors": [
                "Hang Yan",
                "Junqi Dai",
                "Tuo Ji",
                "Xipeng Qiu",
                "Zheng Zhang."
            ],
            "title": "A unified generative framework for aspect-based sentiment analysis",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Yifei Yang",
                "Hai Zhao."
            ],
            "title": "Aspect-based sentiment analysis as machine reading comprehension",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 2461\u20132471, Gyeongju, Republic of Korea. International Com-",
            "year": 2022
        },
        {
            "authors": [
                "Zepeng Zhai",
                "Hao Chen",
                "Fangxiang Feng",
                "Ruifan Li",
                "Xiaojie Wang."
            ],
            "title": "COM-MRC: A COntextmasked machine reading comprehension framework for aspect sentiment triplet extraction",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods",
            "year": 2022
        },
        {
            "authors": [
                "Chen Zhang",
                "Lei Ren",
                "Fang Ma",
                "Jingang Wang",
                "Wei Wu",
                "Dawei Song."
            ],
            "title": "Structural bias for aspect sentiment triplet extraction",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 6736\u20136745, Gyeongju, Republic",
            "year": 2022
        },
        {
            "authors": [
                "Wenxuan Zhang",
                "Xin Li",
                "Yang Deng",
                "Lidong Bing",
                "Wai Lam."
            ],
            "title": "Towards generative aspect-based sentiment analysis",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Yice Zhang",
                "Yifan Yang",
                "Yihui Li",
                "Bin Liang",
                "Shiwei Chen",
                "Yixue Dang",
                "Min Yang",
                "Ruifeng Xu."
            ],
            "title": "Boundary-driven table-filling for aspect sentiment triplet extraction",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-",
            "year": 2022
        },
        {
            "authors": [
                "Shiman Zhao",
                "Wei Chen",
                "Tengjiao Wang."
            ],
            "title": "Learning cooperative interactions for multi-overlap aspect sentiment triplet extraction",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3337\u20133347, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Yichun Zhao",
                "Kui Meng",
                "Gongshen Liu",
                "Jintao Du",
                "Huijia Zhu."
            ],
            "title": "A multi-task dual-tree network for aspect sentiment triplet extraction",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 7065\u20137074, Gyeongju,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "ATE is a natural language processing task, which aims to extract aspect terms from sentences (Jakob and Gurevych, 2010). The aspect term refers to a word, phrase or named entity depicting a certain\n\u2217Corresponding author.\ndomain-specific attribute. For example, the text span \u201cal di la\u201d in (1) is specified as a restaurantdomain aspect term because it appears as the sign of an Italian trattoria.\n(1) Love al di la (Selected from SemEval-R15).\n(2) We take pride in every dish we serve at al di la (Rewritten by ChatGPT with a role of \u201cchef\u201d).\nThe current studies leverage PLMs as backbones to construct ATE models (extractors), including BERT (Devlin et al., 2019), BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) as mentioned in Section 2. They yield significant improvements, compared to conventional neural networks like CNN (LeCun et al., 1998). The advantage is primarily attributed to the strong perception ability on noteworthy contexts, as well as context-aware representation learning ability.\nHowever, such extractors frequently suffer from uninformative or low-quality contexts. For example, the context \u201cLove\u201d in (1) is uninformative for recognizing \u201cal di la\u201d. By contrast, the substitution containing a knowledge-rich context makes it easier to recognize aspect terms, such as the case in (2). Accordingly, we propose a ChatGPT-based Edition Fictionalization (CHEF) method to assist the current PLM-based extractors. CHEF acts as a domain-specific virtual expert with different roles to rewrite sentences, with the aim to refine contexts of potential aspect terms. ChatGPT is utilized for both expert generation and sentence rewriting. A series of post-processing methods are coupled with CHEF, including redundancy elimination, synonym replacement and multi-role voting.\nWe experiment on the benchmark datasets L14 and R14-16 (Pontiki et al., 2014, 2015, 2016). The well-trained BERTbase and PST (Wang et al., 2021) (SoTA) are adopted in the experiments. During test, they perform over the rewritten sentences by CHEF, without retraining and fine-tuning. The test results\ndemonstrate the effectiveness of CHEF in recalling aspects and expanding the predictions."
        },
        {
            "heading": "2 Related Work",
            "text": "Context-aware encoding contributes to ATE. It brings domain-specific contextual information into token-level representations. Therefore, CNN (LeCun et al., 1998; Karimi et al., 2021) and BERT (Devlin et al., 2019; Karimi et al., 2021; Klein et al., 2022) are widely used for ATE due to the abilities of convolving local contexts or absorbing attentive contextual information. Their expanded versions DECNN (Xu et al., 2018; Wei et al., 2020; Li et al., 2020; Chen and Qian, 2020) and BERTPT (Xu et al., 2019; Wang et al., 2021; Chen et al., 2022b) are generally adopted as backbones in the subsequent studies. In addition, BERT is utilized as the pedestal for context-aware encoding in a series of more complex tasks, including AspectSentiment Triplet Extraction (ASTE) (Chen et al., 2022a; Zhang et al., 2022b; Chen et al., 2022c; Zhang et al., 2022a; Chen et al., 2022d; Zhao et al., 2022b) and MRC-based ASTE (Yang and Zhao, 2022; Zhai et al., 2022). Recently, the generative framework is introduced into the studies of ASTE, and accordingly BART (Yan et al., 2021; Zhao et al., 2022a) and T5 (Zhang et al., 2021; Hu et al., 2022) are used. They are constructed with the transformer encoder-decoder architecture in the seq2seq paradigm, where attentive contextual information absorption is conducted on both sides."
        },
        {
            "heading": "3 Approach",
            "text": "We aim to provide knowledge-rich and high-quality sentences for ATE. Specifically, we use CHEF to rewrite sentences, and feed the rewritten sentences into an extractor to predict aspect terms. Finally, we combine the aspect terms which are respectively extracted from the original and rewritten sentences."
        },
        {
            "heading": "3.1 Extractors",
            "text": "We follow the common practice (Chernyshevich, 2014; Toh and Wang, 2014; San Vicente et al., 2015) to treat ATE as a sequence labeling task. B/I/O labels are used, which respectively signal Beginning, Inside and Outside tokens relative to aspect terms. Therefore, an extractor essentially classifies each token into one of the B/I/O labels in terms of the token-level hidden state. We use PLM to compute the hidden states of tokens, and use a Fully-connected (FC) layer for classification.\nAlgorithm 1: Rewrite with Zero-shot prompting Input: Test set S, Role set R AND Domain set D Output: All the rewritten sentences Prompt.fmt(Di,Rj ,Sk) = Rewrite Sk \u2208 S from the\nperspective of Rj \u2208 R in the domain of Di \u2208 D foreach Di \u2208 D do\nforeach Rj \u2208 R do foreach Sk \u2208 S do\nInstruction = Prompt.fmt (Di,Rj ,Sk) Prediction = ChatGPT(Instruction)\nend end\nend\nWe consider two PLMs for hidden-state computation in the experiments, including BERTbase and BERTpt-based PST (Wang et al., 2021)."
        },
        {
            "heading": "3.2 CHEF",
            "text": "CHEF comprises two stages, including role generation of domain-specific virtual experts, as well as role-based rewriting. It is coupled with three post-processors, including redundancy elimination, synonym replacement and multi-role voting.\nRole Generation\u2013 CHEF induces ChatGPT1 to generate a series of virtual experts playing different roles. The generation is prompted by the targetdomain name Di like \u201cRestaurant\u201d. The query we use is as follows: \u201cOutput the roles of experts in the domain of [Di] according to the different responsibilities.\u201d. Table 1 shows the experts.\nSentence Rewriting\u2013 Given a sentence Sk in the ATE datasets, CHEF induces ChatGPT to rewrite the sentence from the perspective of a role-specific expert Rj . Zero-shot prompting (Wei et al., 2022) is used during generation. In other words, there isn\u2019t any example provided for prompting ChatGPT. The query we use is as follows: \u201cRewrite the sentence [Sk] from the perspective of [Rj] in the domain of [Di].\u201d. We rewrite all the instances in the test sets using Algorithm 1.\nPost-Processing\u2013 We drive the extractors to pre1https://platform.openai.com/playground?mode=chat\ndict aspect terms over the role-specific rewritten sentences. Redundant results may obtained, which are specified as the aspect terms never occurring in the original sentences. For example, although the token \u201cdish\u201d in the rewritten sentence in (2) is correctly predicted as an aspect term, it is redundant due to non-occurrence in the original sentence in (1). We filter the redundant results during test.\nIn addition, we use a soft synonym replacement method to reduce false positive rates. Specifically, given an original sentence Si and the rewritten case S\u030ai, we segment both of them into n-grams. Assume a set Ui of n-grams (1 \u2264 n \u2264 5) in Si share a part of the predicted aspect term with the set U\u030ai of n-grams in S\u030ai, we calculate the similarity between each gram uij in Ui and every gram u\u030aik in U\u030ai. We rank all pairs of {uij ,\u030auik} in terms of their similarities, and select the top-1 ranked n-gram pair for synonym replacement, i.e., u\u030aik \u21d0 uij . Meanwhile, the replaced n-gram is specified as the unabridged aspect term, as shown in the example in (3). Note that Cosine similarity is computed over the embeddings of each pair {uij ,\u030auik}. The embedding of each n-gram is obtained by conducting mean pooling (Reimers and Gurevych, 2019) over the token-level hidden states.\n(3) Original: Best Indian food I have ever eaten. Rewritten: I put a lot of effort into perfecting our Indian dishes [Aspect Term]. (Rewritten by CHEF with a role of \u201cchef\u201d) Replacement: Indian dishes \u21d0 Indian food Output: Indian food\nMulti-role Voting\u2013 We conduct multi-role voting only if an extractor obtains controversial results from the original and rewritten sentences. It facilitates the combination of the extraction results.\nAssume an extractor refuses to extract a text span ti as an aspect from the original sentence, though it would like to do so from the sentences rewritten by different roles of experts, thus we regard ti as a controversial result. In this case, we define the behavior of extracting ti as the voting for acceptance, otherwise rejection. On this basis, we compute the acceptance rate vi over the rewritten\nsentences of all experts: vi=Nc/Nall, where Nc denotes the number of voting for acceptance, while Nall is the number of experts. Nall is set to 10 in our experiments. For the Restaurant domain, a controversial result ti is finally adopted only if vi is no less than a threshold of 0.7. For the Laptop domain, the threshold is set to 0.8."
        },
        {
            "heading": "4 Experimentation",
            "text": ""
        },
        {
            "heading": "4.1 Datasets and Evaluation",
            "text": "We experiment on the SemEval datasets, including L14 and R14-16 (Pontiki et al., 2014, 2015, 2016). All the instances in L14 are selected from the Laptop domain, while those in R14-16 derive from the Restaurant domain. We follow the common practice (Dai and Song, 2019; Wang et al., 2021) to split the datasets into training, validation and test sets. The statistics in them are shown in Table 2.\nIt is noteworthy that only the extractors are trained and developed using the above datasets. CHEF has nothing to do with training and development. It is conducted only on the test sets, providing rewritten sentences for the extractors and post-processes the predictions. We evaluate all the models using F1-score (Chernyshevich, 2014)."
        },
        {
            "heading": "4.2 Hyperparameters",
            "text": "We respectively use BERTbase and BERTpt-based PST (Wang et al., 2021) to construct the extractors, where PST achieved the best performance so far.\nFor BERTbase, we set the maximum sequence length to 128 and carry out training in 4 epochs. The optimization of model parameters is obtained using AdamW with a learning rate of 3e-5 and a weight decay of 0.01. We set the batch size to 10. For PST, we adopt their initial hyperparameters, setting the first round of training to 5 epochs and performing 4 rounds of self-training. The learning rate is set to 5e-5. AdamW is used as the optimizer. All the other hyperparameters remain consistent with the reported ones."
        },
        {
            "heading": "4.3 Comparison Result",
            "text": "In Table 3, we report the performance of PST which is enhanced by CHEF, along with other state-of-\nthe-art ATE models. In this case, the extraction results are obtained by combining the predictions of PST on both original and rewritten sentences, where multi-role voting is used. It can be found that CHEF enables PST to achieve better performance, slightly increasing the performance gap relative to other strong competitors.\nIn Table 4, we report the effects of CHEF on both the BERTbase and BERTpt-based PST, where multirole voting is used for prediction combination. It can be observed that CHEF enables both extractors to achieve better performance on all the test sets, without retraining and fine-tuning."
        },
        {
            "heading": "4.4 Discussion",
            "text": "In two separate experiments, we demonstrate that CHEF contributes to the salvage of the missed aspect terms, and yields more substantial improvements on short or long sentences.\nSalvage Rate\u2013 We select all the incompletelysolved sentences from the test sets, each of which contains at least one aspect term neglected by the extractor. On this basis, we use CHEF to rewrite the sentences and drive the extractor to rework them. In this experiment, we consider the BERTbase extractor, and verify the changes in recall rates. Figure\n1 shows the experimental results, where only the performances yielded by the best and worst experts are provided. It can be observed that CHEF substantially improves the recall rates for all test sets, no matter whether it plays the role of best expert or worst. The most significant improvement in recall rate reaches about 9%. Besides, the change of precision, recall and F1 score on the full test set can be found in Appendix B.\nAdaptability\u2013 CHEF applies more to short and long sentences. The former generally contains uninformative contexts, while the latter contains lowerquality contexts due to noises. We split each test set into different subsets according to the lengths of sentences. There are five subsets obtained for each,\nincluding the sentences having a length within the ranges of [1, 10], [11, 15], [16, 20], [21, 30] and no less than 30. The statistics in the subsets are shown in Table 5. Figure 2 shows the ATE performance at the original sentences of different lengths, and that at the rewritten cases by CHEF (without multirole prediction combination). It can be observed that CHEF only yields improvements for relatively short or long sentences. The most significant improvement reaches about 4% F1-score at R16."
        },
        {
            "heading": "4.5 Ablation Study",
            "text": "CHEF consists of four components, including Combination (Comb), Redundancy Elimination (RE), Synonym Replacement (SR) and Multi-role Voting, as presented in Section 3. To verify the effects of the components, we conduct an ablation experiment. Figure 3 illustrates the verification results over best and worst experts, where ALL indicates the complete CHEF method.\nIt can be observed that the simple combination (Comb) causes significant performance degradation, although many terms the baseline missed are salvaged. When redundancy elimination (RE) is used, the comparable (a little worse) performance to the baseline is achieved. At this time, the precision is still lower because the synonymous terms are regarded as negative examples during evaluation. When synonym replacement (SR) is used, the performance is increased for some roles while not for others. When multi-role voting is used, the disagreement among the roles is resolved, and thus the performance is increased to a relatively higher level. We provide examples in Appendix C to facilitate the understanding of our ablation study."
        },
        {
            "heading": "5 Conclusion",
            "text": "We utilize ChatGPT to rewrite sentences with different roles of domain-specific experts, so as to provide informative and high-quality contexts for PLM-based ATE models. Experiments show that the proposed method contributes to the salvage of the neglected aspect terms, and applies more to the short and long sentences. In the future, we will use the rewritten sentences for contrastive learning. To reduce the reliance on ChatGPT, we will develop an offline context rewriting method by knowledge distillation and domain-specific pretraining.\nLimitations\nWe propose to use ChatGPT as an auxiliary toolkit to produce informative and high-quality contexts for context-aware token-level encoding, so as to enhance PLM encoders for aspect term recognition. Our experiments show that the proposed method yields slight improvements when coupled with strong domain-specific models, and it is not only effective in recalling neglected cases, but performs better for short and long instances. Unavoidably, the proposed method has the limitations in building a self-contained model, due to the lack of training and fine-tuning. To overcome the problem, we will develop a lite comparable generator to ChatGPT by knowledge distillation and domain-specific pretraining. Furthermore, we will incorporate rolebased rewritten sentences into the training process, with the support of contrastive learning."
        },
        {
            "heading": "Acknowledgment",
            "text": "The research is supported by National Key R&D Program of China (2020YFB1313601) and National Science Foundation of China (62376182, 62076174)."
        },
        {
            "heading": "A Test Set Reorganization",
            "text": "We divide the test sets into different subsets according to the lengths of sentences. For each test set, we obtained five subsets, which can be found in Table 5."
        },
        {
            "heading": "B Change of Precision and Recall",
            "text": "We provide additional results reflecting the change in precision and recall scores, as shown in Table 6. It can be found that the precision score decreases, meanwhile, the recall score increases. The performance gain at the recall score is because of the enhancement from our post-processing methods."
        },
        {
            "heading": "C Examples for Ablation Study",
            "text": "Our method combines the extracted results from both the original and rewritten sentences. However, the combination without post-processing causes terrible performance because of noises. To solve the problem, we have introduced the post-processing method, including redundancy elimination, synonym replacement, and multi-role voting. We provide examples that have been post-processed using the above solutions in Table 7."
        }
    ],
    "title": "Smart \u201cChef\u201d: Verifying the Effect of Role-based Paraphrasing for Aspect Term Extraction",
    "year": 2023
}