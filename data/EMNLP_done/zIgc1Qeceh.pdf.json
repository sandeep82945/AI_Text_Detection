{
    "abstractText": "In this paper we report on the complexity of persuasion technique annotation in the context of a large multilingual annotation campaign involving 6 languages and approximately 40 annotators. We highlight the techniques that appear to be difficult for humans to annotate and elaborate on our findings on the causes of this phenomenon. We introduce Holistic IAA, a new word embedding-based annotator agreement metric and we report on various experiments using this metric and its correlation with the traditional Inter Annotator Agreement (IAA) metrics. However, given somewhat limited and loose interaction between annotators, i.e., only a few annotators annotate the same document subsets, we try to devise a way to assess the coherence of the entire dataset and strive to find a good proxy for IAA between annotators tasked to annotate different documents and in different languages, for which classical IAA metrics can not be applied.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nicolas Stefanovitch"
        },
        {
            "affiliations": [],
            "name": "Jakub Piskorski"
        }
    ],
    "id": "SP:6ccbb9f58f5b9e87b560f826037a2db7fa51e6f9",
    "references": [
        {
            "authors": [
                "Ivan Habernal",
                "Raffael Hannemann",
                "Christian Pollak",
                "Christopher Klamm",
                "Patrick Pauli",
                "Iryna Gurevych."
            ],
            "title": "Argotario: Computational argumentation meets serious games",
            "venue": "EMNLP.",
            "year": 2017
        },
        {
            "authors": [
                "Ivan Habernal",
                "Patrick Pauli",
                "Iryna Gurevych."
            ],
            "title": "Adapting serious game for fallacious argumentation to German: Pitfalls, insights, and best practices",
            "venue": "LREC.",
            "year": 2018
        },
        {
            "authors": [
                "Kristina Hristakieva",
                "Stefano Cresci",
                "Giovanni Da San Martino",
                "Mauro Conti",
                "Preslav Nakov."
            ],
            "title": "The spread of propaganda by coordinated communities on social media",
            "venue": "Proceedings of the 14th ACM Web Science Conference, WebSci \u201922,",
            "year": 2022
        },
        {
            "authors": [
                "Fumika Isono."
            ],
            "title": "Language agnostic multilingual sentence embedding models for semantic search",
            "venue": "https://primer.ai/developer/languageagnostic-multilingual-sentence-embedding- modelsfor-semantic-search. Accessed: 2023-06-24.",
            "year": 2022
        },
        {
            "authors": [
                "Zhijing Jin",
                "Abhinav Lalwani",
                "Tejas Vaidhya",
                "Xiaoyu Shen",
                "Yiwen Ding",
                "Zhiheng Lyu",
                "Mrinmaya Sachan",
                "Rada Mihalcea",
                "Bernhard Schoelkopf."
            ],
            "title": "Logical fallacy detection",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Jeff Johnson",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou."
            ],
            "title": "Billion-scale similarity search with GPUs",
            "venue": "IEEE Transactions on Big Data, 7(3):535\u2013547.",
            "year": 2019
        },
        {
            "authors": [
                "Maurice G Kendall."
            ],
            "title": "A new measure of rank correlation",
            "venue": "Biometrika, 30(1/2):81\u201393.",
            "year": 1938
        },
        {
            "authors": [
                "Klaus Krippendorff."
            ],
            "title": "Testing the reliability of content analysis data",
            "venue": "The content analysis reader, pages 350\u2013357.",
            "year": 2009
        },
        {
            "authors": [
                "Mary L McHugh."
            ],
            "title": "Interrater reliability: the kappa statistic",
            "venue": "Biochemia medica, 22(3):276\u2013282.",
            "year": 2012
        },
        {
            "authors": [
                "Preslav Nakov",
                "Firoj Alam",
                "Shaden Shaar",
                "Giovanni Da San Martino",
                "Yifan Zhang."
            ],
            "title": "A second pandemic? Analysis of fake news about COVID-19 vaccines in Qatar",
            "venue": "Proceedings of the International Conference on Recent Advances in Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Rebecca J Passonneau",
                "Bob Carpenter."
            ],
            "title": "The benefits of a model of annotation",
            "venue": "Transactions of the Association for Computational Linguistics, 2:311\u2013 326.",
            "year": 2014
        },
        {
            "authors": [
                "Lorena Marcaletti",
                "Enrico Rossi",
                "Alessio Spadaro",
                "Marco Verile",
                "Giovanni Da San Martino",
                "Firoj Alam",
                "Preslav Nakov."
            ],
            "title": "News categorization, framing and persuasion techniques: Annotation guidelines",
            "venue": "Technical report, European Commission",
            "year": 2023
        },
        {
            "authors": [
                "Jakub Piskorski",
                "Nicolas Stefanovitch",
                "Giovanni Da San Martino",
                "Preslav Nakov."
            ],
            "title": "Semeval2023 task 3: Detecting the category, the framing, and the persuasion techniques in online news in a multilingual setup",
            "venue": "Proceedings of the 17th Interna-",
            "year": 2023
        },
        {
            "authors": [
                "Jakub Piskorski",
                "Nicolas Stefanovitch",
                "Nikolaos Nikolaidis",
                "Giovanni Da San Martino",
                "Preslav Nakov."
            ],
            "title": "Multilingual multifaceted understanding of online news in terms of genre, framing, and persuasion techniques",
            "venue": "Proceedings of the 61st Annual",
            "year": 2023
        },
        {
            "authors": [
                "Hannah Rashkin",
                "Eunsol Choi",
                "Jin Yea Jang",
                "Svitlana Volkova",
                "Yejin Choi."
            ],
            "title": "Truth of varying shades: Analyzing language in fake news and political fact-checking",
            "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Process-",
            "year": 2017
        },
        {
            "authors": [
                "Holger Schwenk",
                "Matthijs Douze."
            ],
            "title": "Learning joint multilingual sentence representations with neural machine translation",
            "venue": "arXiv preprint arXiv:1704.04154.",
            "year": 2017
        },
        {
            "authors": [
                "H\u00f4ng-\u00c2n Sandlin",
                "Alain Mermoud"
            ],
            "title": "Robust and explainable identification of logical fallacies in natural language arguments",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In the recent years we have observed an emergence of automated tools for facilitating online media analysis for better understanding of the presented narratives around certain topics across countries, and to identify manipulative, deceptive and propagandistic content. Developing such tools requires annotated data of high quality.\nWe report on the complexity of annotating such manipulative devices, i.e., persuasion techniques, in the context of a large annotation campaign involving 6 languages and approximately 40 annotators, whose details are described in (Piskorski et al., 2023c). The persuasion technique taxonomy used in the campaign is an extension of the taxonomies used in different shared tasks, contains 23 techniques, and includes i.a., the techniques appealing to emotions, justifications and some forms of logical fallacies. The resulting dataset has been used\nin the SemEval 2023 Task 3: Detecting the Category, the Framing, and the Persuasion Techniques in Online News in a Multi-lingual Setup (Piskorski et al., 2023b). The primary objective of the work reported in this paper is threefold, namely:\n\u2022 share some lessons learned from this large multi-lingual annotation campaign that might be beneficial for other researchers planing similar tasks,\n\u2022 present a detailed analysis of the disagreements between annotators and potential causes thereof and try to measure the complexity of the annotation task, and\n\u2022 propose a new concept of measuring InterAnnotator Agreement (IAA) in a multilingual set-up, to overcome the limitations of the classical IAA metrics in such scenario.\nWe first highlight the techniques that appear to be difficult for humans to annotate using the classical Cohen\u2019s \u03ba (McHugh, 2012), and Krippendorff\u2019s \u03b1 (Krippendorff, 2009).\nClassical IAA measures impose certain limitations. First, they only capture the coherence of the annotations in texts written in the same language. Secondly, considering annotations done for a single language, there were many annotators, but annotating totally different subsets of documents. The classical IAA metrics are computed using a tiny fraction of the whole dataset: the one where the annotators annotated the same articles, despite the fact that the exact same text could be annotated in different articles by different annotators. Finally, the classical IAA measures only capture agreement at the time of the annotation, but do not tell us anything about the coherence and quality of the final curated dataset.\nIn order to overcome the aforementioned limitations, we introduce Holistic IAA, a new multilingual word embedding-based IAA metric and\nwe report on various experiments using it and its correlation with the traditional IAA metrics. However, given somewhat limited and loose interaction between annotators, i.e., only a few annotators annotate the same document subsets, we try to devise a way to assess the coherence of the entire dataset and strive to find a good proxy for IAA between annotators tasked to annotate different documents and in different languages. We present our preliminary results on this research problem with an ultimate goal of establishing a mechanism that allows to compare all annotators no matter which document they annotated, and to detect diverging annotations across languages. Our contributions can be summarized as follows: (i) we measure how confusing were the persuasion technique labels for different groups of annotators; (ii) we assess the coherence of the dataset using standard IAA measures; (iii) we introduce a new mutlilingual pancorpus IAA measure based on semantic similarity; (iv) we exploit this new measure on the raw and curated annotations of the annotators, and compare the resulting ranking of annotators to the one obtained by standard IAA measurements; (v) we comment on the self-coherence of the annotators using the new measure, as well as of the dataset language-wise.\nThis paper focuses primarily on the annotation agreement and complexity, whereas the description of the resulting dataset is kept to the minimum necessary for understanding the content. For further details please refer to (Piskorski et al., 2023c).\nThe paper is organized as follows. Section 2 reports on the related work. Section 3 introduces the persuasion technique taxonomy and describes the annotation process. Next, Section 4 reports on the annotation coherence computed using traditional IAA metrics and highlights the hard-to-annotate techniques. Subsequently, Section 5 introduces a new word embedding-based annotator agreement metric and reports on various experiments using it and correlating it with the traditional IAA metrics. We end with some concluding remarks in Section 6."
        },
        {
            "heading": "2 Related Work",
            "text": "Persuasion detection in text is related to work on propaganda detection. The work in the latter area initially focused on document-level analysis and predictions, e.g., Rashkin et al. (2017) reports on prediction of 4 classes (trusted, satire, hoax, and propaganda) of documents, whereas Barr\u00f3n-\nCedeno et al. (2019) presented a corpus of tagged either as propaganda or non-propaganda).\nIn parallel, other efforts focused on the detection of specific persuasion techniques. Habernal et al. (2017, 2018) presented a corpus annotated with 5 fallacies that directly relate to propaganda techniques. A more fine-grained analysis was done by Da San Martino et al. (2019b), who developed a corpus of English news articles labelled with 18 propaganda techniques at span/sentence level. Somewhat related work on detection of use of propaganda techniques in memes is presented in (Dimitrov et al., 2021a), the relationship between propaganda and coordination (Hristakieva et al., 2022), and work studying COVID-19 related propaganda in social media (Nakov et al., 2021a,b). Bonial et al. (2022) reported on the creation of annotated text snippet dataset with logical fallacies for Covid-19 domain. Sourati et al. (2022) presents three-stage evaluation framework of detection, coarse-grained, and fine-grained classification of logical fallacies through adapting existing evaluation datasets, and evaluate various state-of-the-art models using this framework. Jin et al. (2022) proposed the task of logical fallacy detection and a new dataset of logical fallacies found in climate change claims. All the persuasion techniques and logical fallacy taxonomies introduced in the aforementioned works do overlap to a very high degree, but are structured differently, and use different naming conventions.\nVarious related shared tasks on the detection of persuasion techniques were organized recently, and various taxonomies were introduced (Da San Martino et al., 2020, 2019a; Dimitrov et al., 2021b; Alam et al., 2022; Piskorski et al., 2023b).\nRelated work on IAA which explores going beyond the limitation of standard measures was reported in (Passonneau and Carpenter, 2014), proposing an idea similar to our in that they are able to compare all annotators between themselves, however, this comparison is done statistically on label distribution while we look at actual content of the annotate textd. Moreover, they are interested in assessing the gold label uncertainty, which is a similar concern to our effort of capturing the label definition difficulty. However, they treat it in a statistical fashion, while we provide simple descriptors. It would be an interesting future work to explore the combination of both approaches."
        },
        {
            "heading": "3 Persuasion Technique Annotation",
            "text": ""
        },
        {
            "heading": "3.1 Taxonomy",
            "text": "The taxonomy used in our annotation endeavour is an extension of the taxonomy introduced in Da San Martino et al. (2019b,c). At the top level, there are 6 coarse-grained types of persuasion techniques, namely: Attack on Reputation, Justification, Distraction, Simplification, Call, and Manipulative Wording, whose full definitions are provided in Appendix A. These core types are further subdivided into 23 fine-grained techniques. The 5 new techniques vis-a-vis the taxonomy presented in Da San Martino et al. (2019b,c) are: Appeal to Hypocrisy, Questioning the Reputation, Appeal to Values, Consequential Oversimplification, and Appeal To Time. The main drive beyond introducing these 5 new techniques is due to their frequent presence in news articles based on our empirical observations. The full two-tier taxonomy, including short definitions, and examples of each finegrained technique are provided in Figure 3 and 4 in Appendix A respectively."
        },
        {
            "heading": "3.2 Annotation Process",
            "text": "Our annotation task consisted of annotating persuasion techniques in a corpus consisting of circa 1600 news articles revolving around various globally discussed topics in six languages: English, French, German, Italian, Polish, and Russian, using the taxonomy introduced earlier. A balanced mix of mainstream media and \u201calternative\u201d media sources that could potentially spread mis/disinformation were considered for the sake of creating the dataset. Furthermore, sources with different political orientation were covered as well.\nThe pool of annotators consisted of circa 40 persons, all native or near-native speakers of the language they annotated. Most of the annotators were either media analysts or researchers and experts in (computational) linguistics, where approximately 80% of the annotators had prior experience in performing linguistic annotations of news-like texts. A thorough training was provided to all annotators which consisted of: (a) reading a 60-page annotation guidelines (Piskorski et al., 2023a) \u2014 an excerpt thereof is provided in Appendix C), (b) participating in online multi-choice question-like training, (c) carrying out pilot annotations on sample documents, and (d) joint sharing experience with other annotators and discussions with the organisers of the annotation task. Subsequently, each\ndocument was annotated by at least two annotators independently. On a weekly basis reports were sent to annotator pairs highlighting complementary and potentially conflicting annotations in order to converge to a common understanding of the task, and regular meetings were held with all annotators to align and to discuss specific annotation cases.\nAnnotations were curated in two steps. In the first step (document-level curation) the independent annotations were jointly discussed by the annotators and a curator, where the latter was a more experienced annotator, whose role was to facilitate making a decision about the final annotations, including: (a) merging the complementary annotations (tagged only by one annotator), and (b) resolving the identified potential label conflicts. In the second step (corpus-level curation) a global consistency analysis was carried out. The rationale behind this second step was to identify inconsistencies that are difficult to spot using single-document annotation view and do comparison at corpus level, e.g., comparing whether identical or near-identical text snippets were tagged with the same or a similar label (which should be intuitively the case in most situations). The global consistency analysis sketched above proved to be essential to ensure the high quality of the annotations.\nThe annotation resulted in annotation of approx. 1600 documents with ca. 37K text spans annotated. The dataset is highly imbalanced. The class distribution and some statistics are provided in Annex B"
        },
        {
            "heading": "4 Annotation Coherence & Complexity",
            "text": ""
        },
        {
            "heading": "4.1 Traditional IAA",
            "text": "We measured the Inter-Annotator Agreement (IAA) using Krippendorff\u2019s \u03b1, achieving a value of 0.342. This is lower than the recommended threshold of 0.667, but we should note that this value represents the agreement level before curation, and as such, it is more representative of the curation difficulty rather than of the quality of the final consolidated annotations. We used the IAA during the campaign to allocate curation roles and to remove low-performing annotators.\nWe further studied the IAA by ranking the annotators by their performance with respect to the ground truth on the subset of documents they annotated. We split then the annotators into two groups: top and low based on subjective assessment by the curators after the end of the curation campaign, this assessment was then further confirmed numerically\n(see Annex E for details). Their respective average \u03b1 were 0.415 and 0.250. Finally, we considered the \u03b1 of the group of the curators, in order to make an approximate estimation of the coherence of the curated dataset, as we expect these curators to consistently curate the data with at least the same coherence they had when annotating documents. There are only two such curators, whose \u03b1 is of 0.588, which is lower but close to the recommended value."
        },
        {
            "heading": "4.2 Confusion matrix",
            "text": "In Figure 1 we present the confusion matrix between the annotations of annotators. A high count denotes both a frequent class and a tendency to confuse the given pair of labels.\nOne can see that Loaded Language (MW:LL) is the single label that is most confused with any other label, and the Name Calling (AR:NCL) is the label with which it co-occurs most, and indeed, these two labels have a very similar definition. The same applies to the pair Casting Doubt (AR:D) and Questioning the Reputation (AR:QCR)."
        },
        {
            "heading": "4.3 Techniques\u2019 Annotation Complexity",
            "text": "In order to study which persuasion techniques are more difficult to annotate we again divided the annotators in 3 groups: all which contains all the annotators, top which contains half of the annotators whose performance are the highest as measured by their average Cohen\u2019s \u03ba agreement, and low which contains the rest of the annotators.\nFor each of these groups, and for each of the persuasion techniques, we measured how annotators in a given group tend to disagree with each other \u2013 irrespective of the actual ground truth. More precisely, we compute for each pair of annotators and\nfor all their overlapping annotations the percentage of disagreeing annotations for a given label divided by the total number of annotations between them with that label. Here, annotations of two annotators are considered overlapping if one is at most 10% longer or shorter than the other one, taking into account the exact position of the annotations in the text. We report these numbers in Table 1.\nIn order to interpret the results, it is also important to take into account that the 2 sub-groups, namely, top and low, also do interact with each other. We consider the following indicator of complexity: for each of the group if the disagreement is above a given threshold c that we fixed for illustration purpose at 0.25 in the table, the corresponding values are boldfaced. We also divide the techniques in the table (column \u2018difficulty\u2019) into four general annotation complexity classes based on the overall disagreement: very easy (all \u2264 .1, in light green), easy (all \u2264 .25, in green), moderate (all \u2264 .4, in orange), and difficult (all > .4, in red).\nAdditionally, we consider the following indicator: if top > all or if top > low (the techniques for which this applies are marked with an asterisk in the table).\nOne can see that a high low value does not necessarily mean that the label is actually hard, for instance, the label False Dilemma is very well understood by the top group. High low value and low top value denotes a label whose understanding is not straightforward but does not pose special learning problem, in such case improving annotations for this label requires simply insisting on more basic training.\nOn the contrary, when the top value is higher than the others (techniques marked with an asterisk), it means that at least one of the groups agrees more with the other group than top group with itself, meaning that there is an inconsistent understanding of the label within the group. This could indicate a difficult label requiring additional clarification to be made to all annotators, or a potential inconsistency in the label definition. This is, for instance, the case for the label Repetition, which is indeed inconsistent as it includes two very different definitions of repetition.\nThe overall picture of the annotation complexity classes resembles to the per-label performances of classifier systems reported in (Piskorski et al., 2023c), where with a few exceptions the \"easiest\" labels are the ones with the highest F1 score. It is\nimportant to note that these values are computed on the annotated labels before any curation had taken place, and as such do not reflect the quality of the final dataset, but are more and indication of the intrinsic difficulty of the labels for new annotators.\nThe class Doubt has one of the best reported F1 scores, however, it has a difficult annotation complexity, the reason being that it is one of the most confused classes, as it is often a subpart of other techniques.\nSome hard labels remain a challenge even for top annotators, and as such selecting \u2018reliable\u2019 annotators solely based on their overall IAA might not be sufficient to ensure the best quality of annotations, it is also important to identify for which labels additional training might be necessary.\nQuantifying the annotation complexity of an annotation campaign in such a way gives an understanding of the difficulty of the task, and allows to identify incoherent understanding of the guidelines early on, and gives a more refined understanding of the quality of the annotations than considering IAA measures alone."
        },
        {
            "heading": "4.4 Disagreement sources",
            "text": "On top of the findings on annotation complexity we additionally summarize here our findings on the sources of disagreements and annotation complexity from the continuous meetings with annotators and curators:\n\u2022 disregarding small nuances in the definition of Loaded Language and Name Calling we noticed that disagreements and annotation or non-annotation of some instances were due to subjective perception linked to cultural differences, which was apparent when comparing annotations across languages,\n\u2022 some annotators had problems with the Justification techniques, including, in particular, Appeal to Popularity, Appeal to Values, Appeal to Authority due to not understanding upfront that one subjective opinions on what is considered a value or an authority does not play a role for definition of these techniques, and not considering the role of negation, e.g., not understanding that making a reference to something not being popular falls per definition under Appeal to Popularity too,\n\u2022 many annotators, who probably did not read the guidelines thoroughly, literally interpreted some persuasion technique definitions, e.g., in the context of Simplification techniques, instead of detecting certain logic patterns in text (see Annex A for definitions), the annotators literally interpreted the word \u2018simplification\u2019 and reasoned based on the base of whether the presentation of the information is too simplistic and certain facts were downplayed or exaggerated, which is actually linked to a different technique, i.e., Exaggeration-Minimisation,\n\u2022 some of the media analysts who served as annotators were often using background knowledge (professional bias) to make decisions whether some text fragments are instances of persuasion techniques, which was strictly prohibited by the guidelines; this was mainly related to Simplifications and Distractions,\n\u2022 some of the annotators, in particular, media analysts were making a direct link of persuasion technique labeling with fact verification, which was not in line with the guidelines.\nTo sum up, for the major fraction of persuasion techniques the disagreements resulted not from subjective perceptions of the annotators, but mainly due to not sticking strictly to the definitions provided in the 60-page guidelines and/or professional background bias that lead to misinterpretation of the persuasion technique definitions."
        },
        {
            "heading": "5 Embedding-based IAA Assessment",
            "text": ""
        },
        {
            "heading": "5.1 Holistic IAA",
            "text": "We introduce a new measure, namely, Holistic IAA, which allows to compare an annotator with any other, even if they did not annotate a single document in common and annotated documents in different languages. This metric exploits the property of multilingual aligned sentence embeddings, which are able to encode with similar vector representations sentences in different language with the same meaning, and different sentences with a similar meaning in a given language.\nFormally, we introduce the following holistic agreement between two annotators as o\u03b8l,\u03b8s(a1, a2) where ai is the function that maps input texts to label for a given annotator ai; and for any two pair of strings \u03b8l is the threshold on the length ratio and \u03b8s is the threshold on the similarity measure defined for any embedding model M using the cosine distance between the embedding vector of the input strings (we denote it with o for the first letter of the word \"holos\" in Greek).\nWe define the set of Comparable Text Pairs (CTP) between two sets of texts X and Y as:\nCTP \u03b8l,\u03b8s,M X,Y = {x,y\u2208X\u00d7Y : min(|x|,|y|) max(|x|,|y|)>\u03b8l,\nsim(M(x),M(y))>\u03b8s}\nUsing this definition and defining S(ai) as the function returning all the sentences annotated by annotator ai, we define the Holistic IAA for 2 annotators:\no\u03b8l,\u03b8s,M (a1,a2) =\n\u2211 x,y\u2208CTP\n\u03b8l,\u03b8s S(a1),S(a2)\nIa1(x)=a2(y)\n|CTP \u03b8l,\u03b8s S(a1),S(a2) |\nExtending to groups of annotators A and B, we get the more generic formulation:\no\u03b8l,\u03b8s,M (A,B)=\n\u2211 a,b\u2208A\u00d7B \u2211 x,y\u2208CTP\n\u03b8l,\u03b8s S(a1),S(a2)\nIa1(x)=a2(y)\n\u2211 a,b\u2208A\u00d7B |CTP \u03b8l,\u03b8s S(a),S(b) |\nFinally, let An(D) denote the set of annotators of a dataset D. We can now define the Holistic IAA value for a dataset as:\no \u03b8l,\u03b8s,M\nD = o \u03b8l,\u03b8s,M (An(D),An(D))\nIn a first step, the embedding for each annotated text span by each annotator is computed and stored in a vector database, and is associated with the following metadata: the document id, the annotator and the label. We use FAISS for the vector\ndatabase, without quantization and with cosine distance (Johnson et al., 2019). While any multilingual embeddings could be used, we specifically use LASER embeddings (Schwenk and Douze, 2017) for simplicity reasons, i.e., our aim is to introduce a new paradigm to IAA computation, and we do not aim at determining which embeddings are the best, which is an exercise of more limited interest given the pace at which new embeddings emerge. Moreover, LASER embeddings do not require language specific thresholds. As such, one single cutoff to discriminate similar sentences can be used for all the languages, which is not generally the case for semantic similarity models (Isono, 2022). The drawback of these embeddings is that they are less discriminating than other models as the range of values corresponding to true positives largely intersects with the range of values corresponding to false positives.\nIn a second step, for each annotator and corresponding annotated text spans, we consider the set of all similar text spans. In Figure 2 we illustrate in detail the behaviour of LASER on two sample queries reporting the L2 similarity. For the more complex query Q2, all but one retrieved spans are correct, but a divergence in meaning can be seen with decreasing semantic similarity. We use cosine distance in order to avoid the length of the vector to impact the measure. Moreover, in order to avoid comparing sentences of extremely different sizes, the length of the retrieved span and the query span is constrained by a threshold on the ratio of their respective lengths, i.e., \u03b8l.\nQ1 \"\u043d\u0435\u0434\u043e\u043f\u0443\u0441\u0442\u0438\u043c\u044b\u043c\" (ru, unacceptable) : insupportable (fr, 0.03, unbearable), invisibile (it, 0.03, invisible), insostenibile (it, 0.04, unsustainable), Inacceptable (fr, 0.05, unacceptable)\nQ2 \"tout simplement, un mensonge\" (fr, all simply a lie) : \u00c8 tutta una menzogna (it, 0.04, it is all a lie), jawne k\u0142amstwo (pl, 0.06, a clear lie), questa \u00e8 una bugia (it, 0.06, this is a lie), \u00c9norme mensonge (fr, 0.07, an enormous lie), alles wieder eine gro\u00dfe L\u00fcge (de, 0.08, again a big lie), Wir glauben, dass wir belogen werden (de, 0.09, we believe we are being lied to), obficie ok\u0142amuja\u0328c (pl, 0.09, lying profusely), fatiscenti menzogne (it, 0.09, crumbling lies), \u043e\u0433\u043e\u043b\u0442\u0435\u043b\u043e\u0435 \u0432\u0440\u0430\u043d\u044c\u0435 (ru, 0.09, rabid lies), n\u2019en faire qu\u2019une bouch\u00e9e (fr, 0.09, deal with it easily), mensonges \u00e9hont\u00e9s (fr, 0.09, shameless lies)\nFigure 2: Example of a query and retrieved sentences, for each span we report the language, L2 similarity and the English translation.\nIn Table 2 we provide an example of how text spans and their associated metadata can be queried and retrieved from the database. In this example, the retrieved texts all refer to the same concept as the query text, despite spelling variations in one language and different languages being used. How-\never, we can observe that the labels can disagree: This illustrates at the same time the difficulty of distinguishing between Loaded language and Name Calling, and that some annotators are not consistent in their annotations. Notably, the high rate of confusion between Loaded language (MW:LL) and Name Calling (AR:NCL) is observable."
        },
        {
            "heading": "5.2 Validation: Methodology",
            "text": "In order to validate the approach, we perform rank correlation analysis between the ranking computed by standard IAA techniques and the ones with our approach using Kendall\u2019s Tau rank correlation coefficient (Kendall, 1938). We consider 2 datasets: the raw annotations of the annotators, and the corpus (dataset of curated documents by curators).\nThe raw annotations allow us to compute pairwise IAA with Cohen\u2019s \u03ba between annotators, who have annotated the exact same documents. For each annotator, we consider the ranking of the annotators he can be compared to and which have at least 10 annotations in common.\nGiven the raw annotations dataset, we compute the Holistic IAA o value, and for each annotator we rank all the other annotators to which it can be compared to, as measured by the average level of agreement on labels for semantically similar text spans."
        },
        {
            "heading": "5.3 Validation: Results",
            "text": "We compare the ranking of most \u2018similar\u2019 annotators for each annotator computed using Cohen\u2019s \u03ba with the ranking computed using Holistic IAA on the same subset of annotators. We consider 3 rankings: strict Cohen\u2019s \u03ba; same ranking is done on the same set of documents and annotators as the one used to compute Cohen\u2019s \u03ba; diff ranking is done on the same pair of annotators, but strictly on documents that were not jointly annotated by them.\nWe perform a simple grid search over the hyper-\nparameters \u03b8s and \u03b8l. In Table 3 we show a sample of the parameters searched, in Annex F we report the results of the full grid search performed. The correlation between strict and same is on overall higher than when comparing diff and strict as well as same and diff, and is even perfect or near perfect for a subset of the parameters. We selected the parameter as \u03b8l = 0 and \u03b8s = 0.75 for the rest of the paper, despite these being not the optimal. Optimal parameters are too conservative and as such the CTP set was too small in order to compare all annotators or groups of annotators, and a such prevented from further studying the properties of Holistic IAA.\nThis proves that the Holistic IAA can be used as a proxy for the pan-document pan-annotators agreement for some specific set of parameters, however, without the possibility to precisely link its value to other standard IAA measures, and with the caveat that the correlation is positive yet not perfect. As such, Holistic IAA can be used mainly to comment on the qualitative difference in agreement between different subsets of annotations."
        },
        {
            "heading": "5.4 Validation: Error Analysis",
            "text": "We performed an error analysis of the confusions found using Holistic IAA: using the 33k+ confusions found by the approach over the dataset, for each pair of labels we evaluate up to 5 alleged confusions and graded the similarity between the corresponding texts on a 3-tier scale. Two texts are considered: identical if the meaning is so close that minor nuance in text would not alter the label chosen (e.g. \"op\u00e9ration sp\u00e9ciale\" (fr) and \"Spezialoperation\" (de) both meaning \"special operation\"); close if the meaning is substantially different, but semantically close enough making the label debatable and worthy to be flagged to a curator for review, for instance one text could be more generic than the other one (e.g. \"fin\u00ec molto male\" (it) =\n\"it ended badly\" and \"durement mise \u00e0 mal\" (fr) = \"badly impacted\"); unrelated if the meaning is unrelated - even if the texts contain the same elements.\nA total of 502 data points were annotated. Note that only texts containing at least one space were considered. In Table 4 we report the count in each category, and the mean, min and max similarity measure as given by the LASER embeddings. When adding the close and identical classes, we get that in a bit more than half of the cases the approach is able to correctly flag potential incoherent annotations.\nWe can also see the difficulty of setting cutoff boundaries as the range of minimum and maximum semantic distance is overlapping between all the 3 classes, and with close and identical having almost the same mean boundaries. We can nevertheless observe that the mean value of close is 0.75, making it a reasonable candidate for \u03b8l.\nThese results show that about half of the annotations flagged by the system were indeed of interest to the curators. However, as such, the results are highly dependent on the model used. Future work will require to identify embeddings with a larger margin between the classes in order to make the work of the curators more efficient."
        },
        {
            "heading": "5.5 Impact of the second curation step",
            "text": "In order to further evidentiate the behavior of Holistsic IAA, we use it to quantify the impact of the corpus-level curation step. This step was performed per-language after the usual documentlevel curation step was accomplished. The data was sorted per-label and the master curators looked at the overall coherence of the annotated text-span label pairs, the context of the spans was also provided. This step lead to several corrections and is understood to have boosted the overall coherence of the dataset, and should be reflected with a higher o value for the corpus.\nIn Table 5 we consider the agreement as measured by Holistic IAA after step 1 and 2 of the curation by considering the 4 most active cura-\ntors: ai and si denote respectively the agreement percentage between annotators and the support at step i. For step 2, the o value is higher, and the average IAA is 1.6 pts higher, while the average intra-annotator agreement (self-agreement) is 3.5 pts higher. This demonstrates that Holistic IAA is able to capture and quantify the positive impact of the corpus-level curation.\nIn Table 6 we illustrate the impact of excluding Loaded Language (MW:LL) and Name Calling (AR:NCL) from the dataset as these labels constitute nearly half of the annotations and are frequently confused with each other by annotators in terms of absolute number (but not in proportion) as shown in Figure 1 and Table 1. We observe that the agreement between annotators can be label specific.\nIn Figure 8 we consider the whole curated dataset and measure the o value between pairs of languages. The columns ai report the value after step 1 and 2 considering the whole range of labels, while the columns a\u2032i exclude the two labels MW:LL and AR:NCL. Doing so gives us an understanding of the agreement for the lower populated labels. Please note that all the Attacks on Reputation (AR:*) and Manipulative Wordings (MW:*) were excluded from the second step of the curation due to time constraints - except for DE and PL. The impact of the second curation step is almost always positive for all pairs of languages, except notably for one language for which the related o values deteriorate and which drags down the intra-language coherence score.\nOverall, when considering the corpus we observe a quality increase as measured by the o value."
        },
        {
            "heading": "5.6 Multilingual Dataset Coherence Estimation",
            "text": "Knowing the dataset coherence computed using standard IAA measures in a monolingual setting,\nand comparing it with values computed using Holistic IAA, we extrapolate from it the coherence of the entire multilingual dataset. Only two curators have jointly annotated the same set of documents while acting as annotators before the curation phase and taking on the curator role, as such we can compute the Krippendorff\u2019s \u03b1 between them, which is 0.588, a little under the recommended value. The o value between them on the same data is 0.420. A group of 3 \"master curators\" covered all the languages and curated most of the dataset. Their average o value on the raw annotations is of 0.565. This higher value illustrates the fact that the coherence of the annotations in the final dataset is higher than when measured on the raw annotations.\nWe now consider only the curated dataset. In Figure 8 we can observe that the o value intra-language range has an average value of 0.538, slightly above the o value of 0.420 of the two reference annotators for which Krippendorff\u2019s \u03b1 could be computed. We can conclude that the coherence of the dataset restricted to each language is above the coherence of the reference annotators.\nHowever, most of the inter-language o values are much lower than the intra-language values. We believe this to be due to 2 factors: 1) each curation was performed per-language, ignoring the others, thereby increasing the self coherence of each language; 2) as in the case of the diff vs. strict in Figure 3 Holistic IAA is less able to capture agreement than in the case of same vs. strict, thereby denoting a limitation of our approach. This could be partially alleviated by using \u2018better\u2019 embeddings. Nevertheless, even with a lower performance, a tool based on Holistic IAA to check for annotation coherence across languages would help to increase the quality of the dataset by flagging potential inconsistent annotations.\nIn Table 7 we can observe that the o value for\nthe dataset is consistently higher after the second curation step vis-a-vis after the first step, suggesting that this new curation approach is of interest to increase the quality of annotations."
        },
        {
            "heading": "6 Conclusions",
            "text": "We reported on the complexity of annotating persuasion techniques in a large-scale multilingual annotation campaign. We introduced the Holistic IAA paradigm, a new measure to serve as a proxy of the estimation of inter-annotator agreement and actual corpus coherence in settings that are fundamentally outside the scope of usual IAA measures. We demonstrate that annotator ranking computed using this new measure is positive and can highly correlates with ranking computed using Cohen\u2019s Kappa in some settings. Using it, we can observe the beneficial impact of the second step of our 2- step curation phase, and also identify similarity and divergence between annotators for some subsets of labels. The experiment conducted in this study supports what was informally remarked regarding the estimation of the performance of the annotators and increased our confidence in the coherence of the final corpus. We believe that using Holistic IAA as part of the monitoring of multilingual or monolingual large-scale annotation campaigns could help to spot problems by flagging potential incoherence in the labels of semantically similar sentences at an early stage. In future work we envisage exploration of thresholds for finer interpretation and exploring the use of other semantic similarity models.\nLimitations\nDistribution Representativeness Although the underlying corpus of annotated news articles covers a wide range of topics as well as media from all sides of the political spectrum it should neither be seen as representative nor balanced in any specific way w.r.t. media in any country. Consequently, the distribution of the annotated persuasion techniques might, in principle, not be fully representative as well. Biases Given that human data annotation involves some degree of subjectivity we created a comprehensive 60-page annotation guidelines document to clarify important cases during the annotation process. Nevertheless, some degree of intrinsic subjectivity might have impacted the techniques picked up by the annotators during the annotation, and impacted so the distribution thereof in the final dataset. Furthermore, although the taxonomy used in this annotation campaign covers most of the \u2018popular\u2019 techniques used in the media, we identified some persuasive attempts which could not have been matched with any of the techniques in the existing taxonomy, and were tagged as OTHER (less than 3% of all annotations) and were not considered in the reported work, which once again poses a certain limitation with respect to the representativeness of persuasion technique types used in the media. Methodology Soundness Our results are limited to certain extent, in particular, the introduced IAA metric should be considered as a proof of concept since certain approximations and simplifications were made and parameters were chosen, e.g., the choice for cutoff of maximal retrieved similar sentences, the length ratio to select sentence to be compared is constrained, and the choice of similarity metrics for computing semantic similarity that exploits a specific sentence embeddings model. Different settings and choices could yield different results. Disregarding of these shortcomings, the new metric helped to circumvent the limited scope and utility of classical IAA in such a large-scale multilingual campaign. We believe that the proposed methodology presented in this paper is too some extent generic, and would be of great interest to the community.\nThe approach considers only the text of the annotation, as such their context is ignored. This limitation is mitigated in case the annotation guidelines do not specify that the span of annotation\nmust contain all necessary information to unambiguously determine the label, which is the case in the campaign whose data was used to illustrate our approach.\nEthics Statement\nBiases The news articles for the creation of the underlying dataset were sampled in such a way in order to have a balanced representation with respect to different points of view and type of media. We also strived to engage a mix of annotators with different backgrounds, i.e., both media analysts and computational linguists. Furthermore, the annotators were explicitly instructed not take their personal feeling about the particular topic and to objectively focus on identifying whether specific persuasion techniques were used. Disregarding the aforementioned efforts, the distribution of the various persuasion techniques annotated might not perfectly reflect the broader spectrum of the media landscape in the target languages, which should be taken into account in exploiting the related statistical information for any kind of analysis, etc. Analogously, the findings and statistics related to the annotation complexity are linked to the specific pool of annotators engaged in the campaign, and, consequently, they should be considered as approximative.\nIntended Use and Misuse Potential The reported work focuses solely on sharing experience with the research community on annotating persuasion techniques in news articles in a large campaign, analysis of the difficulty of annotating such techniques, and ways of measuring annotation agreement and consistency across languages. The reported work is not linked to a release of the underlying annotated dataset, which is a subject of different publication and related ethical considerations."
        },
        {
            "heading": "Acknowledgements",
            "text": "We are greatly indebted to all the annotators from different organizations, who participated in the annotation campaign, and without whom carrying out the reported study would not have been possible.\nThe work presented in this paper is financed by the Joint Research Centre of the European Commission."
        },
        {
            "heading": "A Persuasion Techniques",
            "text": "The two-tier persuasion technique taxonomy has 6 coarse-grained categories: Attack on reputation: The argument does not address the topic, but rather targets the participant (personality, experience, deeds) in order to question and/or to undermine their credibility. The object of the argumentation can also refer to a group of individuals, an organization, an object, or an activity. Justification: The argument is made of two parts, a statement and an explanation or an appeal, where\nthe latter is used to justify and/or to support the statement. Simplification: The argument excessively simplifies a problem, usually regarding the cause, the consequence or the existence of choices. Distraction: The argument takes focus away from the main topic or argument to distract the reader. Call: The text is not an argument, but an encouragement to act or to think in a particular way. Manipulative wording: the text is not an argument, but uses specific language, which contains words or phrases that are either non-neutral, confusing, exaggerating, loaded, etc., in order to impact the reader emotionally. They are further subdivided into 23 fine-grained persuasion techniques. The full list of the finegrained techniques is presented in 3, whereas some examples of text snippets representing various persuasion techniques are provided in Figure 4."
        },
        {
            "heading": "B Dataset Statistics",
            "text": "In Figure 5 we provide the distribution of the persuasion techniques per language. Name Calling and Loaded Language are by far the most populated classes across all languages, and are followed by Doubt and Questioning the Reputation. In total there were approx. 9K text spans (with persuasion techniques) annotated for English (536 documents), 7.2K for French (211 documents), 5.7K for German (177 documents), 8K for Italian (303 documents), 3.8K for Polish (194 documents), and 4.1K for Russian (191 documents)."
        },
        {
            "heading": "C Annotation guidelines excerpt",
            "text": "This section provides an excerpt from the annotation guidelines (Piskorski et al., 2023a). The following general rules should be applied when annotating persuasion techniques:\n\u2022 if one has doubts whether a given text fragment contains a persuasion technique then do not annotate it, (conservative approach)\n\u2022 select the minimal amount of text1 to annotate in case of doubts whether to include a longer text fragment or not,\n1In our guidelines we do have specific rules for each of the persuasion techniques of what the annotation should include, e.g., in case of Justification techniques the annotation should include certain appeal and the claim or idea it supports, if explicitly expressed in the immediate context, or, in case of Loaded Language only the emotionally-loaded word/phrase should be annotated, disregarding the context it appears in\n\u2022 avoid personal bias (i.e., opinion and emotions) on the topic being discussed as this has nothing to do with the annotation of persuasion techniques,\n\u2022 do not exploit external knowledge to decide whether given text fragment should be tagged as a persuasion technique,\n\u2022 do not confuse persuasion technique detection with fact checking. A given text fragment might contain a claim which is known to be true, but that does not imply there are no persuasion techniques to annotate in this particular text fragment,\n\u2022 often, authors use irony (not being explicitly part of the taxonomy), which in most cases serves a purpose to persuade the reader, most frequently to attack the reputation of someone or something. In such cases the respective persuasion technique type should be used, or other if the use of irony does not fall under any persuasion technique type in the taxonomy,\n\u2022 in case of quotations or reporting of what a given person said the annotation of the persuasion techniques within the boundaries of that quotation should be done from the perspective of that person who is making some statement or claim (point of reference) and not from the author perspective.\nFor each persuasion technique we have also specified what text fragment should be annotated in the document. The general rule is to annotate the minimum amount of text that can be considered as a trigger to spot the technique, even if it requires an understanding of the context that spans over more than one of the preceding sentences. Sometimes, the to-be-annotated text fragment might go beyond the boundaries of one single sentence. In the following we briefly summarize the rules for all the techniques.\nName Calling or Labelling: The noun phrase, the adjective that constitutes the label and/or the name. If quotation marks are used, they should be included in the annotation as well.\nGuilt by Association: The part of text that refers to an entity and a mention of someone else (considered evil/negative) doing the same or similar thing that is considered negative. The mention of the activity of the target entity might be implicit.\nCasting Doubt: Only the text fragment that questions the credibility and the object whose credibility is being questioned. There is no need to include the full context.\nAppeal to Hypocrisy: The text phrase embracing a certain activity, and another one which is used as an argument to accuse the former as being a hypocrite.\nQuestioning the Reputation: Only the text fragments that refer to something negative being mentioned about the person/group/object.\nFlag Waving: The part of the text that refers to patriotism or other group related values, and the conclusion/action it is supposed to support if it is present in the text.\nAppeal to Authority: The part of the text that refers to the authority (and potentially some of his/her statement/opinion/action), and the conclusion it supports, in case the latter is present in the text.\nAppeal to Popularity: The part of the text that refers to something that a majority does or seems to be widely supported and/or is popular together with the conclusion it is supposed to support.\nAppeal to Values: The part of the text that refers to values, and include the conclusion it is supposed to support, in case the latter is included explicitly in the text.\nAppeal to Fear, Prejudice: The part of the text that\nrefers to the fears, prejudices, e.g., of something that might happen. Strawman: When this technique is used, usually the relevant context might span across more sentences. However, one should only annotate the text fragment (sentence or part thereof), which introduces the distraction. Red Herring: When this technique is used, usually the relevant context might span across more sentences. However, one should only annotate the text fragment (sentence or part thereof), which introduces the distraction. Whataboutism: When this technique is used, usually the relevant context might span across multiple sentences. However, one should only annotate the text fragment (sentence or part thereof) that introduces the distraction. Causal Oversimplification: The minimal text fragment that matches the logical pattern should be annotated:"
        },
        {
            "heading": "Y occurred after X; therefore,",
            "text": ""
        },
        {
            "heading": "X was the only cause of Y",
            "text": ""
        },
        {
            "heading": "X caused Y; therefore, X was the only cause of Y (although A,B,C...etc. also contributed to Y.)",
            "text": "or a false conclusion drawn therefrom should be annotated, although, often not all parts of the pattern above are explicitly mentioned in the text. False Dilemma or No Choice: The minimal text fragment that matches one of the following logical patterns should be annotated: (a) Black & White Fallacy:\nThere are only two alternatives A and B to a given problem/task. It cannot be A. Therefore, the only solution is B (since A is not an option).\n(b) Dictatorship\nThe only solution to a given problem/task is A.\nalthough, often not all parts of the pattern above are explicitly mentioned in the text. Consequential Oversimplification: The entire text fragment that matches the above logical pattern should be annotated: if A will happen then B, C, D, ... will happen\nwhere: - A is something one is trying to reject (support) - B, C, D are perceived as some potential negative (positive) consequences happening if A happens.\nSlogans: The slogan only (no need to annotate the conclusion it supports), and in case it is surrounded\nby quotation marks, include them as well.\nConversation Killer: A minimal text span that triggers ending the conversation, discussion, etc.\nAppeal to Time: A minimal text span referring to the argument of time that calls for some action. Both the call and the action should be annotated.\nLoaded Language: Only the phrase containing loaded words, the context in which they appear should not be annotated. As a general rule one should consider to tag longer text fragment if and only if each of the words adds more emotional \u2018load\u2019 to the text fragment.\nObfuscation, Intentional Vagueness, Confusion: The minimal text fragment that introduces confusion: it could be a word, but also a longer piece of text that requires to be read in order to understand the confusion it causes.\nExaggeration or Minimisation: The text fragment that provides the description that downplays or exaggerates the object of criticism. The latter should be included in the annotated text as well.\nRepetition: All text fragments that repeat the same message or information that was introduced earlier. The first occurrence of the message/information is to be annotated as well. If it is not clear what exactly to annotate then the entire sentence should be annotated. Furthermore, it is important to emphasize that a repetition of something per se is not always a persuasion technique, but could sometimes be used only to refer to a topic/issue being discussed."
        },
        {
            "heading": "D Confusion Matrix based on Holistic IAA",
            "text": "In Figure 7 we report the confusion matrix found using Holistic IAA on the final curated dataset. De-\nFigure 7: Confusion matrix on final corpus, as per Holistic IAA, for a minimal support of 10\nspite overall similarity with exact confusion matrix on annotations in Figure 1 there are a few notable difference, particularly with C:CK being largely largely more confused and AR:D being largely less confused.\nIn Figure 6 we report the confusion matrix on the set of annotations. It is closer the Figure 1 than Figure 7. This indicates that the curation process actually eliminated some common confusion in the annotations. The magnitude are different for two reasons that can not be measure independently: it contains less errors, and there are less overall pair-wise comparison performed as the total set of annotation considered is about two times smaller.\nE Identifying the top and low groups of annotators\nIn order to split the annotators into two groups, in a first time the curators based on their subjective assessments established 2 groups of equal size. This was further corroborated in a second step using the following approach: the curated data was taken as ground truth and the annotators were considered as classifiers, whose annotations are considered as prediction. As such we computed the micro F1 for each annotator, and ranking them along that measure, the median split validated the first subjective assessment which contained a few more annotators, which all ranked the highest in the lower split. The average micro F1 score of the top and low groups are respectively of 0.603 +- 0.119 and 0.284 +- 0.081."
        },
        {
            "heading": "F Parameter search",
            "text": "We conducted an exhaustive parameter search in order to determine the optimal parameter \u03b8l and \u03b8s\nwhich maximise the rank correlation between the ranking of annotators produces by Cohen\u2019s Kappa and the one produces by Holistic IAA. We consider 3 groups of document-annotators pairs: strict, for which Cohen\u2019s \u03ba can be computed; same any document annotated by previous annotators; diff any documents not annotated jointly by previous annotators. The pairwise comparison of these sets with Kendall\u2019s Tau rank correlation is consider as 3 dimensions of a multi-criteria decision problem: A minimal number of 10 annotations in common is required for a pair of annotator to be considered, support is the total number of pairs being compared. The table with the result of the parameter search are reported in Table 9."
        }
    ],
    "title": "Holistic Inter-Annotator Agreement and Corpus Coherence Estimation in a Large-scale Multilingual Annotation Campaign",
    "year": 2023
}