{
    "abstractText": "With the advance of large language models (LLMs), the research field of LLM applications becomes more and more popular and the idea of constructing pipelines to accomplish complex tasks by stacking LLM API calls come true. However, this kind of methods face two limitations: narrow information coverage and low fault tolerance. In this work, we propose a novel method called ALLIES. Given an input query, ALLIES leverages LLMs to iteratively generate new queries related to the original query, enabling an iterative reasoning process. By iteratively refining and expanding the scope of the original query, ALLIES captures and utilizes hidden knowledge that may not be directly obtainable through retrieval. We take zeroshot open-domain question answering (ODQA) as an application scene and evaluate ALLIES on the widely-used benchmarks, such as NQ, WebQ and TriviaQA. The experimental results demonstrate that ALLIES significantly outperforms other zero-shot baselines, indicating its effectiveness in tackling those challenges. Our code is available in https://github.com/ microsoft/SimXNS/tree/main/ALLIES.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hao Sun"
        },
        {
            "affiliations": [],
            "name": "Xiao Liu"
        },
        {
            "affiliations": [],
            "name": "Yeyun Gong"
        },
        {
            "affiliations": [],
            "name": "Yan Zhang"
        },
        {
            "affiliations": [],
            "name": "Daxin Jiang"
        },
        {
            "affiliations": [],
            "name": "Linjun Yang"
        },
        {
            "affiliations": [],
            "name": "Nan Duan"
        }
    ],
    "id": "SP:17ab3573afe0f029623f9454c82e1f0a35e4875c",
    "references": [
        {
            "authors": [
                "Christopher Klamm",
                "Colin Leong",
                "Daniel van Strien",
                "David Ifeoluwa Adelani"
            ],
            "title": "BLOOM: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Ross Taylor",
                "Marcin Kardas",
                "Guillem Cucurull",
                "Thomas Scialom",
                "Anthony Hartshorn",
                "Elvis Saravia",
                "Andrew Poulton",
                "Viktor Kerkez",
                "Robert Stojnic"
            ],
            "title": "Galactica: A large language model for science",
            "year": 2022
        },
        {
            "authors": [
                "Mohammad Shoeybi",
                "Mostofa Patwary",
                "Raul Puri",
                "Patrick LeGresley",
                "Jared Casper",
                "Bryan Catanzaro"
            ],
            "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism",
            "year": 1909
        },
        {
            "authors": [
                "Danqi Chen",
                "Adam Fisch",
                "Jason Weston",
                "Antoine Bordes"
            ],
            "title": "Reading wikipedia to answer open-domain questions",
            "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave"
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "EACL",
            "year": 2021
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave"
            ],
            "title": "Distilling knowledge from reader to retriever for question answering",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Angeliki Lazaridou",
                "Elena Gribovskaya",
                "Wojciech Stokowiec",
                "Nikolai Grigorev"
            ],
            "title": "Internetaugmented language models through few-shot prompting for open-domain question answering",
            "venue": "arXiv preprint arXiv:2203.05115,",
            "year": 2022
        },
        {
            "authors": [
                "Fabio Petroni",
                "Aleksandra Piktus",
                "Angela Fan",
                "Patrick Lewis",
                "Majid Yazdani",
                "Nicola De Cao",
                "James Thorne",
                "Yacine Jernite",
                "Vladimir Karpukhin",
                "Jean Maillard"
            ],
            "title": "Kilt: a benchmark for knowledge intensive language tasks",
            "venue": "In Proceedings of the 2021 Conference",
            "year": 2021
        },
        {
            "authors": [
                "Ori Ram",
                "Yoav Levine",
                "Itay Dalmedigos",
                "Dor Muhlgay",
                "Amnon Shashua",
                "Kevin Leyton-Brown",
                "Yoav Shoham"
            ],
            "title": "In-context retrieval-augmented language models",
            "venue": "arXiv preprint arXiv:2302.00083,",
            "year": 2023
        },
        {
            "authors": [
                "Weijia Shi",
                "Sewon Min",
                "Michihiro Yasunaga",
                "Minjoon Seo",
                "Rich James",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Wen-tau Yih"
            ],
            "title": "Replug: Retrievalaugmented black-box language models",
            "venue": "arXiv preprint arXiv:2301.12652,",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 1907
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Jiacheng Liu",
                "Alisa Liu",
                "Ximing Lu",
                "Sean Welleck",
                "Peter West",
                "Ronan Le Bras",
                "Yejin Choi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Generated knowledge prompting for commonsense reasoning",
            "venue": "arXiv preprint arXiv:2110.08387,",
            "year": 2021
        },
        {
            "authors": [
                "Wenhao Yu",
                "Dan Iter",
                "Shuohang Wang",
                "Yichong Xu",
                "Mingxuan Ju",
                "Soumya Sanyal",
                "Chenguang Zhu",
                "Michael Zeng",
                "Meng Jiang"
            ],
            "title": "Generate rather than retrieve: Large language models are strong context generators",
            "venue": "arXiv preprint arXiv:2209.10063,",
            "year": 2022
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Xuezhi Wang",
                "Yi Tay",
                "Yiming Yang",
                "Denny Zhou"
            ],
            "title": "Recitation-augmented language models",
            "venue": "arXiv preprint arXiv:2210.01296,",
            "year": 2022
        },
        {
            "authors": [
                "Tom Kwiatkowski",
                "Jennimaria Palomaki",
                "Olivia Redfield",
                "Michael Collins",
                "Ankur Parikh",
                "Chris Alberti",
                "Danielle Epstein",
                "Illia Polosukhin",
                "Jacob Devlin",
                "Kenton Lee"
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "TACL",
            "year": 2019
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel S Weld",
                "Luke Zettlemoyer"
            ],
            "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
            "venue": "In ACL 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Berant",
                "Andrew Chou",
                "Roy Frostig",
                "Percy Liang"
            ],
            "title": "Semantic parsing on freebase from questionanswer pairs",
            "venue": "EMNLP",
            "year": 2013
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le"
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Ofir Press",
                "Muru Zhang",
                "Sewon Min",
                "Ludwig Schmidt",
                "Noah A Smith",
                "Mike Lewis"
            ],
            "title": "Measuring and narrowing the compositionality gap in language models",
            "venue": "arXiv preprint arXiv:2210.03350,",
            "year": 2022
        },
        {
            "authors": [
                "Ori Yoran",
                "Tomer Wolfson",
                "Ben Bogin",
                "Uri Katz",
                "Daniel Deutch",
                "Jonathan Berant"
            ],
            "title": "Answering questions by meta-reasoning over multiple chains of thought",
            "venue": "arXiv preprint arXiv:2304.13007,",
            "year": 2023
        },
        {
            "authors": [
                "Jeff Johnson",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Billionscale similarity search with gpus",
            "venue": "IEEE Transaction\u2019s on Big Data,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With the emergence of large language models (LLMs) [OpenAI, 2023, Scao et al., 2022, Taylor et al., 2022, Chowdhery et al., 2022], researchers have explored their potential to generate responses, including answering queries with the in-context learning method [Brown et al., 2020]. In that method, the models are prompted with demonstrations such as human-selected query-response pairs [Shoeybi et al., 2019, Rae et al., 2021, Du et al., 2022]. In this field, open-domain question answering [Chen et al., 2017, Izacard and Grave, 2021, 2020, Lazaridou et al., 2022] is an important and representative task that usually requires\n\u2217This work was done during internship at MSRA. \u2020Xiao Liu is the corresponding author.\naccess to external corpora [Petroni et al., 2021] and utilizes a retriever component for knowledge augmentation [Ram et al., 2023, Shi et al., 2023, Rashkin et al., 2021, Gao et al., 2022, Bohnet et al., 2022, Menick et al., 2022] to improve their ability to provide comprehensive and accurate answers.\nHowever, despite the advancements, these methods still face two main limitations. (1) Firstly, narrow information coverage. When incorporating relevant information, the majority of these approaches only employ the query itself to find or retrieve additional contextual information. Nonetheless, there are instances where responding to the query necessitates implicit knowledge that is related to the query but cannot be easily found solely using the given query. Consequently, the LLM may fail to acquire crucial information required for accurately responding to the query. (2) Secondly, low fault tolerance. Most of these methods follow the pipeline style, consisting of unique steps calling LLM APIs to generate responses to fulfill different needs in a single turn. It means that the model is expected to provide the correct response in a single attempt. If an internal step fails, either the whole pipeline will face the risk of exception or the error will be propagated to downstream steps. Consequently, if the model fails to find the necessary information or misinterprets the question, it may produce an incorrect response.\nTo address the aforementioned limitations, we propose a novel approach called ALLIES that applies a beam search strategy to generate responses. To better elaborate the method, we take opendomain question answering as the application scene and show an example of how ALLIES works in Figure 1. We adopt an interactive and iterative process. Initially, we generate additional queries by asking the LLM what other information they require, based on the existing query-evidence pair. These generated queries serve as prompts for retrieving relevant evidence from external sources.\nThe retrieved evidence is then added to the existing query-evidence pair. Next, we employ the LLM to respond to the initial query based on the augmented query-evidence pairs. Subsequently, we solicit the LLM to score the response, taking into account the query and the augmented query-evidence pair. This scoring process provides a measure of confidence in the generated response. The iterations continue until the score surpasses a predefined threshold, indicating a sufficiently confident answer or the maximum depth of the tree traversal is reached. Once either of these conditions is fulfilled, the process terminates, and the answer is outputted as the final result. Responding to the query using ALLIES can be conceptualized as a tree traversal process, starting from the root node and progressing towards the leaf nodes, where each internal node in the tree represents a generated query.\nThe main advantages of our method are two folds: (1) Firstly, we employ an extension strategy that extends the original question to multiple relevant questions, broadening the information coverage. This approach enables the LLM to gain a deeper understanding of the complex question by focusing on its constituent parts. By providing the LLM with more specific and targeted queries, we enhance their ability to comprehend and process the question effectively. (2) Secondly, during the iterative process, we employ a dynamic pruning technique that retains only the top B answers at each step. This increases the fault tolerance and robustness of our model by allowing the LLM to\nmake mistakes during the reasoning process. Any erroneous answers can be replaced by alternative answers, leading to more accurate and reliable responses. This flexibility and adaptability contribute to the improved performance of our approach.\nWith the idea of ALLIES, we take zero-shot opendomain question answering (ODQA) as an application scene and evaluate ALLIES in several popular benchmarks. We conduct experiments on the NQ, TriviaQA and WebQ datasets. The results demonstrate that ALLIES significantly outperforms several representative baselines while maintaining an acceptable cost. The case study further confirms the aforementioned advantages of our method.\nIn summary, our main contributions can be summarized as follows:\n1. We propose ALLIES, which leverages a beam search strategy for response generation. Within this framework, we adopt an interactive and iterative process to enhance the accuracy and robustness of the responses.\n2. By extending the original question into multiple relevant questions and employing a dynamic pruning technique, we improve the understanding of complex questions and increase the model\u2019s robustness. This allows for mistakes and alternative answers, resulting in more accurate and robust responses.\n3. By taking zero-shot ODQA as an application scene, results on the NQ, TriviaQA and WebQ\ndatasets demonstrate the significant outperformance of our method compared to baseline approaches. The case study further validates the advantages of our approach."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Open-Domain Question Answering",
            "text": "Open-domain question answering is a task that aims to provide answers to questions without relying on specific context. This task can be categorized into two settings: the open-book setting and the closed-book setting. In the open-book setting, models [Chen et al., 2017, Izacard and Grave, 2021, 2020] typically consist of a retriever and a reader component. The retriever\u2019s role is to retrieve relevant information from a corpus such as Wikipedia [Chen et al., 2017, Izacard and Grave, 2021] or web pages [Lazaridou et al., 2022, Nakano et al., 2021], while the reader focuses on answering the question based on the retrieved information.\nIn the closed-book setting, models have no access to external corpus and have to rely on its model parameters to store all the information. Recent works find that large-scale language models like T5 [Raffel et al., 2020] can already answer questions without access to the external corpus. However, small-scale language models like RoBERTa [Liu et al., 2019] or GPT-2 [Radford et al., 2019] still face challenges in accurately answering questions in this setting."
        },
        {
            "heading": "2.2 Large Language Model Enhanced Question Answering",
            "text": "In recent times, there has been a shift towards utilizing large language models (LLMs) for question answering [Chowdhery et al., 2022, Du et al., 2022, Liu et al., 2021]. This research can be broadly categorized into two lines of work. The first line of work focuses on preprocess methods [Borgeaud et al., 2022, Ram et al., 2023, Shi et al., 2023], which involve obtaining relevant documents and then utilizing LLMs to generate answers. Within this line of work, there are two main approaches. Retrieve-then-read methods [Ram et al., 2023, Shi et al., 2023] employ a retrieval model to retrieve relevant documents, while generate-then-read methods [Yu et al., 2022, Sun et al., 2022] fully leverage the capabilities of LLMs. Furthermore, researchers have demonstrated that combining generation and retrieval can lead to further gains [Yu et al., 2022].\nThe second line focuses on posthoc methods\n(like works on QA with attribution) [Rashkin et al., 2021, Gao et al., 2022, Bohnet et al., 2022, Menick et al., 2022], which involve generating an answer using an LLM and then refining it with the help of a verifier and a retriever. The retrieved documents in the second stage serve as explanations for the generated answer."
        },
        {
            "heading": "3 Main Idea",
            "text": "The main idea of ALLIES is an interactive and iterative process based on the widely-used search algorithm, beam search1. We use a tuple with five slots to represent a state, which is the element of a beam. Each state \u27e8q,Q, E , r, s\u27e9 consists of the original query q, the set of historical query completions Q, the set of historical external evidences E , the current response r, and the estimated score s according to the current state. Assume the maximum search depth is D, as illustrated in Figure 2, there are four main stages of ALLIES."
        },
        {
            "heading": "3.1 Beam Initialization",
            "text": "In the beginning, we initialize the beam by asking the LLM to answer the query directly and by answering the query based on retrieved evidence. The retrieved evidence is obtained by first retrieving related documents using the original query and then summarizing the documents. The generated tuples will be added to the beam.\n1https://archive.org/details/DTIC_ADA049288\nAlgorithm 1 The process of generating the response to a given query using ALLIES. Hyperparameters: The maximum number K of generated queries, the maximum depth D of extension, the number N of documents from retrieval, the score threshold S, and the beam size B. Input: A query q. Output: The answer a\u0302. 1: Clear the initial beam S0 = \u2205 2: Answer the query q with the model knowledge a0 = Answer(q,\u2205,\u2205). 3: Score the initial answer s0 = Score(q,\u2205,\u2205, a0). 4: Add the current tuple to the initial beam S0 = S0 \u222a {\u27e8q,\u2205,\u2205, a0, s0\u27e9}. \u25b7 The first seed. 5: Retrieve a evidence e1 = Retrieve(qori, q,N). 6: Answer the query q with the model knowledge a1 = Answer(q, {q}, {e1}). 7: Score the initial answer s1 = Score(q, {q}, {e1}, a1). 8: Add the current tuple to the initial beam S0 = S0 \u222a {\u27e8q, {q}, {e1}, a1, s1\u27e9}. \u25b7 The second seed. 9: for extension depth d in 1 \u2192 D do \u25b7 Extending within the depth. 10: Clear the beam for the current depth Sd = \u2205. 11: for each tuple in the previous beam \u27e8q,Q, E , a, s\u27e9 \u2208 Sd\u22121 do \u25b7 Iterate the previous tuples. 12: Find the extended queries Q\u2032 = Ask(q,Q, E ,K). 13: for each extended query q\u2032 \u2208 Q\u2032 do \u25b7 Try each possible extension. 14: Retrieve a evidence e\u2032 = Retrieve(qori, q\u2032, N). 15: Try to answer with all the evidences a\u2032 = Answer(q,Q\u222a {q\u2032}, E \u222a {e\u2032}). 16: Score the answer s\u2032 = Score(q,Q\u222a {q\u2032}, E \u222a {e\u2032}, a\u2032). 17: Add the current extended tuple to the beam Sd = Sd \u222a {\u27e8q,Q\u222a {q\u2032}, E \u222a {e\u2032}, a\u2032, s\u2032\u27e9}. 18: end for 19: end for 20: Trim the beam Sd by keeping only B tuples with largerest scores. \u25b7 Prune the beam. 21: if a tuple \u27e8q,Q, E , a, s\u27e9 \u2208 Sd meets s \u2265 S then \u25b7 Examine the exit. 22: SD = Sd. 23: Exit the loop. 24: end if 25: end for 26: Find the tuple \u27e8q,Q, E , a\u0302, smax\u27e9 \u2208 SD with the largest score smax and a\u0302 is the final answer."
        },
        {
            "heading": "3.2 Beam Expansion",
            "text": "During the beam search process, we iteratively pop out one element from the front of the beam. For each element, we generate queries using the Ask Function. Then, for each generated query, we retrieve relevant evidence and ask the LLM to answer the query based on both the retrieved evidence and the reasoning history. The LLM scores the generated answers based on the reasoning history, and the newly formatted tuples are added to the end of the beam."
        },
        {
            "heading": "3.3 Beam Pruning",
            "text": "At the end of each search depth, we rank the newly generated answers and keep only top B answers."
        },
        {
            "heading": "3.4 Beam Termination",
            "text": "If the highest-ranking answer in the beam has a score exceeding the predefined threshold, the search process terminates, and the answer is outputted. Otherwise, the process continues. If none of the elements in the beam reaches the threshold, we output the highest-scoring answer when the search reaches the maximum depth."
        },
        {
            "heading": "4 Detailed Approach for ODQA",
            "text": "In this section, we present the application of ALLIES in ODQA, whose algorithm is illustrated in Algorithm 1. There are four key functions used in ALLIES, each serving a specific purpose. The corresponding prompts are illustrated in Appendix C.\n4.1 Answering Function Answer(q,Q, E) This function takes as input the original query q, previously generated queries Q, and corresponding retrieval evidence E . It constructs a reasoning history {\u27e8q1, e1\u27e9 , \u27e8q2, e2\u27e9 , ...} by extracting qi \u2208 Q and ei \u2208 E . The function then asks the LLM to reason over the reasoning history and provide an answer to the original query.\n4.2 Asking Function Ask(q,Q, E ,K) Given the query q, previously generated queries Q, corresponding retrieval evidence E , and the maximum number of queries to be generated K, this function constructs a reasoning history {\u27e8q1, e1\u27e9 , \u27e8q2, e2\u27e9 , ...} by extracting qi \u2208 Q and ei \u2208 E . The LLM is then asked to reason over the reasoning history and determine what additional information it requires to answer the question. The function outputs the generated queries.\n4.3 Retrieval Function Retrieve(qori, q,N) Given the original query qori, query q, and the maximum number of documents to be retrieved N , this function uses a dense retriever to retrieve the top-N most similar documents. The LLM is then asked to extract the most useful information from the documents and summarize them, providing a concise version of the retrieved information. We can also use LLM to directly generate a background document like GENREAD [Yu et al., 2022] as an alternative and we call this function Retrieve \u2032 (qori).\n4.4 Scoring Function Score(q,Q, E , a) Given the original query q, previously generated queries Q, corresponding retrieval evidence E , and the generated answer a from the LLM, this function constructs a reasoning history {\u27e8q1, e1\u27e9 , \u27e8q2, e2\u27e9 , ...} by extracting qi \u2208 Q and ei \u2208 E . The LLM is then asked to consider the reasoning history and assess the probability that the candidate answer is the true answer. The function outputs a score representing the confidence in the generated answer."
        },
        {
            "heading": "5 Experiment",
            "text": ""
        },
        {
            "heading": "5.1 Experimental Setting",
            "text": "In this section, we conduct experiments on three open-domain question-answering (QA) datasets: NQ [Kwiatkowski et al., 2019], TriviaQA [Joshi et al., 2017], and WebQ [Berant et al., 2013]. Since we focus on zero-shot ODQA, we utilize only the complete test sets of NQ and WebQ. To reduce costs, we randomly selected 1000 samples from the TriviaQA test set for evaluation purposes. Original detailed statistics regarding these three datasets can be found in Appendix A.\nWe evaluate the performance using two metrics: the exact match (EM) score and the F1 score. Specifically, a predicted answer is considered correct only if its normalized form matches any of the normalized versions of the answers provided in the answer list. The F1 score measures the word overlap between the normalized version of the predicted answer and the answers in the provided answer list."
        },
        {
            "heading": "5.2 Implementation",
            "text": "We employ GPT-3.5-Turbo hosted by Azure OpenAI services as our large language model (LLM). As for the retriever component, we conduct separate finetuning for the NQ, TriviaQA, and WebQ datasets using their respective training sets. The\narchitecture and performance of the dense retrieval component can be found in Appendix D. For the retrieval corpus, we use the Wikipedia dump from Dec. 20, 2018 as our retrieval corpus, encompassing a collection of 21,015,324 documents."
        },
        {
            "heading": "5.3 Baselines",
            "text": "We compare our method with three groups of zeroshot QA baselines.\nThe first group comprises baselines that utilize a retriever in their approach. This includes models such as BM25 + InstructGPT, Contriever + InstructGPT, Google + InstructGPT, and DPR + InstructGPT. These models employ a retriever to retrieve relevant information, which is then used by InstructGPT for answer generation. We obtained the reported performance numbers for these baselines from GENREAD [Yu et al., 2022].\nThe second group consists of baselines that do not utilize a retriever in their approach. This group includes models such as GPT-3 [Brown et al., 2020], InstructGPT [Yu et al., 2022], FLAN [Wei et al., 2021], GLaM [Du et al., 2022], and GENREAD [Yu et al., 2022]. The reported performance numbers for these baselines are obtained from their respective original papers.\nThe third group consists of models that we implemented ourselves, including directly answer, retrieve-then-answer, GENREAD [Yu et al., 2022], self-Ask [Press et al., 2022], and MCR [Yoran et al., 2023]. Directly answer refers to the utilization of the LLM to directly answer the question. Retrievethen-answer involves retrieval before answering, where we experimented with different numbers of retrieved documents and reported their corresponding performance, which is the simplified version of ALLIES without beam search. We implemented GENREAD, self-Ask, and MCR based on their open-source code. However, we evaluate MCR only on the NQ dataset due to its high API cost. To ensure fairness among the baselines, we set the retrievers and LLM configurations to be the same."
        },
        {
            "heading": "5.4 Main Results",
            "text": "We present the main results of our zero-shot experiments in Table 1. Based on these results, several observations can be made:\n(1) Among the methods that utilize a retriever, the choice of the retriever has a significant impact on the model\u2019s performance. This indicates that the quality of the retrieved documents plays a crucial role in determining the overall system performance.\n(2) Among the methods that do not use a retriever, GENREAD achieves the highest performance. This demonstrates the effectiveness of the generate-then-read pipeline, where the model generates background documents based on its own knowledge without relying on external corpus.\n(3) Our implemented baselines, such as MCR and self-Ask, may not perform as well as expected. This is mainly because these methods heavily rely on result parsing, which limits their generalizability to other applications.\n(4) Our proposed method, ALLIES, outperforms all existing baselines and achieves the highest performance on all datasets. This confirms the effectiveness of our model and demonstrates its superiority in open-domain question answering tasks. Additionally, our method relies less on result parsing, making it more generalizable to other applications."
        },
        {
            "heading": "5.5 Ablation Study",
            "text": "In ALLIES, we utilize LLMs to ask questions and retrieve evidence based on those questions. To investigate the effects of the evidence, we conduct\nablations by removing the evidence and using different types of evidence, as shown in Table 2.\nBased on the results, we draw several conclusions: (1) When the evidence is removed, we only provide the LLM with related queries without any background information. In this case, the model\u2019s performance drops significantly, which confirms that incorporating evidence into the model can greatly improve its understanding of the query. (2) When using the LLM-generated background document (GENREAD), we observe that our model achieves slightly better results compared to retrieval & summary. This finding aligns with the observations made in GENREAD [Yu et al., 2022]. The improved performance can be attributed to the fact that LLMs have seen these related documents during pretraining, and the generated documents are more specific and refined."
        },
        {
            "heading": "5.6 Query Complementation Analysis",
            "text": "By iteratively generating new queries to complement the original query, our ALLIES is capable of expanding the information coverage of the original query and capturing hidden knowledge that may not be directly obtainable through retrieval with\nthe original query. To verify this, we conduct a query complementation analysis that compares the retrieval results of retrieve-then-answer and ALLIES. Specifically, we record the percentage of retrieval results containing the ground truth answer and present the findings in Table 3.\nFrom the result, we can find that the retrieval results of ALLIES outperform those of retrievethen-answer across all datasets, which verifies the effectiveness of ALLIES. By iteratively generating new queries, we can expand the knowledge scope of the retrieval results, leading to a more comprehensive understanding of the original query and naturally producing better answers."
        },
        {
            "heading": "5.7 Effectiveness Analysis",
            "text": "In ALLIES, the use of multiple iterations of retrieval and generation may introduce additional costs. To analyze its effectiveness, we utilize the complete set of questions from the NQ dataset to conduct the effectiveness analysis, which systematically compares the effectiveness of several methods.\nAs shown in Table 4, we can have the following conclusions: (1) Multi-turn QA methods, including ALLIES and MCR, incur higher model inference costs compared to single-turn QA methods such as Directly Answer, GENREAD, Self-Ask, and Retrieve-Then-Answer. This increase in cost is primarily due to the multiple API calls involved. (2) Among the multi-turn QA methods, although ALLIES requires more API calls, the token consumption per API is significantly lower than that of MCR, resulting in 1/6 inference cost of MCR. The higher token consumption per API in MCR can be attributed to the demonstration, which consumes a substantial number of tokens. (3) Generally, single-turn QA methods have lower token costs but exhibit lower model performance. In contrast, ALLIES achieves significantly better model performance while maintaining an acceptable token cost compared to MCR, thus demonstrating the effectiveness of our method."
        },
        {
            "heading": "5.8 Human Evaluation",
            "text": "In this section, we conducted a human evaluation to assess the accuracy of the scores generated by LLMs in our scoring function. We randomly selected 100 samples for score calculation and manually verified the generated scores.\nOur findings indicate that 93 percent of the generated scores align with the requirements for score calculation. This validation confirms the rationale behind using LLMs to calculate the scores. However, we also observed some rare cases where two answers could both potentially address the question, but one of them was more accurate. In these cases, the LLMs assigned the same score to both answers, potentially leading to the selection of the less accurate answer. This issue can be attributed to the coarse nature of the prompt used for scoring, which can only assess the general relevance score. To address this issue, one possible solution for future work is to calculate the scores using an ensemble-and-vote approach. This would involve asking LLMs to rank all possible answers instead of scoring them individually, which would potentially achieve more accurate and reliable scores."
        },
        {
            "heading": "5.9 Hyper-parameter Study",
            "text": "Beam size B and beam depth D are two important hyper-parameters in our method. We study their effects by changing one parameter while fixing\nother parameters and observing the performance trends, which are shown in Figure 3.\nStudy on Beam Size B. Beam size refers to the number of questions we keep at each layer during answer searching. From the results, we observe that the performance reaches its peak when the beam size (B) is set to 2. Values smaller or larger than this threshold lead to performance degradation. This is primarily because a larger beam size provides the model with more opportunities to make mistakes. However, when the beam size is too large, the model struggles to effectively rank the multiple candidates and select the best answer. Additionally, an increase in beam size also incurs additional computational costs.\nStudy on Beam Depth D. Beam depth refers to the maximum depth our model can reach during answer searching. From the results, we find that the performance change during beam depth tuning is relatively small. This is mainly due to the early stop mechanism we implemented, where the answer searching can terminate before reaching the maximum search depth if the answer score surpasses the threshold. However, we also observe that when the beam depth is too large (e.g., 4), the model\u2019s performance starts to decline. We be-\nlieve this is mainly because, in most cases, a beam depth of 2 provides the model with sufficient background information. Increasing the beam depth beyond that only introduces more noisy information, which may complicate the generation of the correct answer for the LLM."
        },
        {
            "heading": "5.10 Case Study",
            "text": "In this section, we provide examples that illustrate the reasoning process of our ALLIES method, which is shown in Table 5. From these examples, we draw the following conclusions:\n(1) The generated queries in our method are more specific and focused compared to the original query. This specificity improves the accuracy of the retrieval process, resulting in more accurate and relevant retrieved evidence. Consequently, the generated answers are of higher quality.\n(2) During the answer generation process, there might be instances where wrong answers are initially predicted. However, our scoring function effectively assigns lower scores to these wrong answers based on the reasoning history. As a result, the final output is the correct answer. This demonstrates the robustness of our method in handling potential mistakes and effectively filtering out incorrect answers."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we introduce ALLIES, a novel method that addresses the limitations of using large language models (LLMs) for complex tasks. By leveraging LLMs to generate related queries iteratively, ALLIES enables iterative reasoning and expands the original query\u2019s scope to capture hidden knowledge. We evaluate ALLIES in zero-shot open-domain question answering and demonstrate its superiority over other baselines on benchmarks. As for future work, we plan to apply ALLIES in other complex tasks such as mathematical reasoning and so on.\nLimitations\nIn this work, we propose an effective response generation method ALLIES. The limitations of the proposed method are as follows:\n(1) The computational cost of ALLIES is relatively high due to the need for multiple API calls and document retrieval. This can limit its practicality in resource-intensive scenarios or systems with limited computational resources.\n(2) The operation of the model is based on the designed prompt. When applied to a new application scenario, crafting effective prompts may require additional time and effort from users."
        },
        {
            "heading": "A Data Statistics",
            "text": "The statistics of used datasets are shown in Table 8."
        },
        {
            "heading": "B Hyper-parameters",
            "text": "The detailed hyper-parameters are shown in Table 6."
        },
        {
            "heading": "C Detailed Prompts of the Functions",
            "text": "C.1 Answering Function Answer(q,Q, E)\nGiven the following query-evidence pair: {query-evidence pair} Please refer to the query-evidence pair above, answer the following question with just one entity. Question: {query} The answer is:\nC.2 Asking Function Ask(q,Q, E ,K)\nGiven the question: {query} and following query-evidence pair: {query-evidence pair}. Please generate some questions that can help answer the given question with the following constraints: 1.You should output no more than k questions. 2.You should directly output the ranked subquestions based on their importance. 3.The generated questions should be diverse and focus on different aspects of the given question. 4.You should output in the following format: Ranked Questions: 1. [Question 1] . . .\nC.3 Retrieval Function Retrieve(qori, q,N)\nGiven the original question: {query} and the provided document: {doc} output the factual information from the evidence that is relevant to the question:\nC.4 Retrieval Function Retrieve\u2032(qori)\nGenerate a short background document from Wikipedia to answer the given question: {query}\nC.5 Scoring Function Score(q,Q, E , a) Given the question: {query} and the candidate answer: {answer} and the Query-evidence pair: {query-evidence pair} refer to the query-evidence pair below and utilize your own reasoning ability to assess the probability that the candidate answer is the true answer. Please provide a number between 0 and 1 as the output, following the guidelines below: If the probability is between 0 and 0.3, it signifies that the model has substantial evidence to suggest it is an incorrect answer. If the probability is between 0.3 and 0.5, it suggests that the model leans towards considering it an incorrect answer, but lacks concrete evidence. If the probability is between 0.5 and 0.7, it indicates that the model leans towards considering it a correct answer, but lacks concrete evidence. If the probability is greater than 0.7, it signifies that the model has substantial evidence to suggest it is the correct answer. If the candidate answer doesn\u2019t provide clear solution to the question, the probability should be 0. The score is:"
        },
        {
            "heading": "D Dense Retriever",
            "text": "Dual Encoder. The predominant architecture currently utilized for dense retrieval is known as the dual encoder. This architecture employs dense vector representations, denoted as q and d, to encode queries and documents, respectively. The similarity scores are then computed using the inner product as follows:\ns(q,d) = EQ(q) T \u00b7 ED(d) (1)\nwhere EQ(\u00b7) and ED(\u00b7) refer to the query encoder and document encoder, respectively. To leverage the embeddings, existing solutions typically employ approximate nearest neighbor (ANN) search algorithms such as FAISS [Johnson et al., 2021].\nPerformance of Dual Encoder. The pre-trained language model (PLM) used in the training of retrievers is COCONDENSER2. The performances of DEs on different datasets can be found in Table 7.\n2Luyu/co-condenser-marco in huggingface."
        }
    ],
    "title": "ALLIES: Prompting Large Language Model with Beam Search",
    "year": 2023
}