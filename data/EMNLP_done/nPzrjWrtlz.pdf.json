{
    "abstractText": "In the age of information overload, it is more important than ever to discern fact from fiction. From the internet to traditional media, we are constantly confronted with a deluge of information, much of which comes from politicians and other public figures who wield significant influence. In this paper, we introduce HeTrue: a new, publicly available dataset for evaluating the credibility of statements made by Israeli public figures and politicians. This dataset consists of 1021 statements, manually annotated by Israeli professional journalists, for their credibility status. Using this corpus, we set out to assess whether the credibility of statements can be predicted based on the text alone. To establish a baseline, we compare text-only methods with others using additional data like metadata, context, and evidence. Furthermore, we develop several credibility assessment models, including a feature-based model that utilizes linguistic features, and state-of-theart transformer-based models with contextualized embeddings from a pre-trained encoder. Empirical results demonstrate improved performance when models integrate statement and context, outperforming those relying on the statement text alone. Our best model, which also integrates evidence, achieves a 48.3 F1 Score, suggesting that HeTrue is a challenging benchmark, calling for further work on this task.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ben Hagag"
        },
        {
            "affiliations": [],
            "name": "Reut Tsarfaty"
        }
    ],
    "id": "SP:ef5e95d242dd584d0e39d9679601854a098209f6",
    "references": [
        {
            "authors": [
                "Sadia Afroz",
                "Michael Brennan",
                "Rachel Greenstadt."
            ],
            "title": "Detecting hoaxes, frauds, and deception in writing style online",
            "venue": "2012 IEEE Symposium on Security and Privacy, pages 461\u2013475. IEEE.",
            "year": 2012
        },
        {
            "authors": [
                "Takuya Akiba",
                "Shotaro Sano",
                "Toshihiko Yanase",
                "Takeru Ohta",
                "Masanori Koyama."
            ],
            "title": "Optuna: A nextgeneration hyperparameter optimization framework",
            "venue": "Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data",
            "year": 2019
        },
        {
            "authors": [
                "Isabelle Augenstein",
                "Christina Lioma",
                "Dongsheng Wang",
                "Lucas Chaves Lima",
                "Casper Hansen",
                "Christian Hansen",
                "Jakob Grue Simonsen."
            ],
            "title": "Multifc: A real-world multi-domain dataset for evidencebased fact checking of claims",
            "venue": "Proceedings of",
            "year": 2019
        },
        {
            "authors": [
                "Ramy Baly",
                "Mitra Mohtarami",
                "James Glass."
            ],
            "title": "Integrating stance detection and fact checking in a unified corpus",
            "venue": "Proceedings of NAACL-HLT, pages 21\u201327.",
            "year": 2018
        },
        {
            "authors": [
                "James Bergstra",
                "R\u00e9mi Bardenet",
                "Yoshua Bengio",
                "Bal\u00e1zs K\u00e9gl"
            ],
            "title": "Algorithms for hyper-parameter",
            "year": 2011
        },
        {
            "authors": [
                "Piotr Bojanowski",
                "Edouard Grave",
                "Armand Joulin",
                "Tomas Mikolov."
            ],
            "title": "Enriching word vectors with subword information",
            "venue": "Transactions of the association for computational linguistics, 5:135\u2013146.",
            "year": 2017
        },
        {
            "authors": [
                "Jianlin Cheng",
                "Zheng Wang",
                "Gianluca Pollastri."
            ],
            "title": "A neural network approach to ordinal regression",
            "venue": "2008 IEEE international joint conference on neural networks (IEEE world congress on computational intelligence), pages 1279\u20131284. IEEE.",
            "year": 2008
        },
        {
            "authors": [
                "Nadia K Conroy",
                "Victoria L Rubin",
                "Yimin Chen."
            ],
            "title": "Automatic deception detection: Methods for finding fake news",
            "venue": "Proceedings of the association for information science and technology, 52(1):1\u20134.",
            "year": 2015
        },
        {
            "authors": [
                "Jo\u00e3o Victor de Souza",
                "Jor\u00e3o Gomes Jr.",
                "Fernando Marques de Souza Filho",
                "Jairo Francisco de Souza"
            ],
            "title": "A systematic mapping on automatic classification of fake news in social media",
            "venue": "Social Network Analysis",
            "year": 2020
        },
        {
            "authors": [
                "R Dilmon."
            ],
            "title": "Linguistic differences between lie and truth in spoken hebrew",
            "venue": "Doctoral diss., Bar Ilan University, Ramat Gan, Israel.[in Hebrew].",
            "year": 2004
        },
        {
            "authors": [
                "Rakefet Dilmon."
            ],
            "title": "Fiction or fact? comparing true and untrue anecdotes",
            "venue": "Bal\u0161anwt iybriyt, 59.",
            "year": 2007
        },
        {
            "authors": [
                "Rakefet Dilmon."
            ],
            "title": "False speech, linguistic aspects in hebrew",
            "venue": "The Encyclopedia of Hebrew Language and Linguistics, Leiden, Brill, Boston and Tokyo, pages 542\u2013546.",
            "year": 2013
        },
        {
            "authors": [
                "Rotem Dror",
                "Gili Baumer",
                "Segev Shlomov",
                "Roi Reichart."
            ],
            "title": "The hitchhiker\u2019s guide to testing statistical significance in natural language processing",
            "venue": "Proceedings of the 56th annual meeting of the association for computational linguistics (volume 1:",
            "year": 2018
        },
        {
            "authors": [
                "Paul Ekman."
            ],
            "title": "An argument for basic emotions",
            "venue": "Cognition & emotion, 6(3-4):169\u2013200.",
            "year": 1992
        },
        {
            "authors": [
                "JR Flesch"
            ],
            "title": "Flesch-kincaid readability formula",
            "year": 1965
        },
        {
            "authors": [
                "Bruce Fraser."
            ],
            "title": "Questions of witness credibility",
            "venue": "Harvard Law School.",
            "year": 1991
        },
        {
            "authors": [
                "Bilal Ghanem",
                "Simone Paolo Ponzetto",
                "Paolo Rosso",
                "Francisco Rangel."
            ],
            "title": "Fakeflow: Fake news detection by modeling the flow of affective information",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Anastasia Giachanou",
                "Paolo Rosso",
                "Fabio Crestani."
            ],
            "title": "Leveraging emotional signals for credibility detection",
            "venue": "Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval, pages 877\u2013880.",
            "year": 2019
        },
        {
            "authors": [
                "Zhijiang Guo",
                "Michael Schlichtkrull",
                "Andreas Vlachos."
            ],
            "title": "A survey on automated fact-checking",
            "venue": "Transactions of the Association for Computational Linguistics, 10:178\u2013206.",
            "year": 2022
        },
        {
            "authors": [
                "Ashim Gupta",
                "Vivek Srikumar."
            ],
            "title": "X-fact: A new benchmark dataset for multilingual fact checking",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Yaakov HaCohen-Kerner",
                "Rakefet Dilmon",
                "Shimon Friedlich",
                "Daniel Nisim Cohen."
            ],
            "title": "Distinguishing between true and false stories using various linguistic features",
            "venue": "Proceedings of the 29th Pacific Asia Conference on Language, Information and",
            "year": 2015
        },
        {
            "authors": [
                "Jeffrey T Hancock",
                "Lauren E Curry",
                "Saurabh Goorha",
                "Michael Woodworth."
            ],
            "title": "On lying and being lied to: A linguistic analysis of deception in computer-mediated communication",
            "venue": "Discourse Processes, 45(1):1\u201323.",
            "year": 2007
        },
        {
            "authors": [
                "P Sol Hart",
                "Erik C Nisbet."
            ],
            "title": "Boomerang effects in science communication: How motivated reasoning and identity cues amplify opinion polarization about climate mitigation policies",
            "venue": "Communication research, 39(6):701\u2013723.",
            "year": 2012
        },
        {
            "authors": [
                "Naeemul Hassan",
                "Bill Adair",
                "James T Hamilton",
                "Chengkai Li",
                "Mark Tremayne",
                "Jun Yang",
                "Cong Yu."
            ],
            "title": "The quest to automate fact-checking",
            "venue": "Proceedings of the 2015 Computation+ Journalism Symposium.",
            "year": 2015
        },
        {
            "authors": [
                "Naeemul Hassan",
                "Gensheng Zhang",
                "Fatma Arslan",
                "Josue Caraballo",
                "Damian Jimenez",
                "Siddhant Gawsane",
                "Shohedul Hasan",
                "Minumol Joseph",
                "Aaditya Kulkarni",
                "Anil Kumar Nayak"
            ],
            "title": "Claimbuster: the first-ever end-to-end fact-checking system",
            "year": 2017
        },
        {
            "authors": [
                "Frank Hutter",
                "Holger H Hoos",
                "Kevin Leyton-Brown."
            ],
            "title": "Sequential model-based optimization for general algorithm configuration",
            "venue": "International conference on learning and intelligent optimization, pages 507\u2013523. Springer.",
            "year": 2011
        },
        {
            "authors": [
                "Stephan Lewandowsky",
                "Werner GK Stritzke",
                "Klaus Oberauer",
                "Michael Morales."
            ],
            "title": "Memory for fact, fiction, and misinformation: The iraq war 2003",
            "venue": "Psychological Science, 16(3):190\u2013195.",
            "year": 2005
        },
        {
            "authors": [
                "Yunfei Long",
                "Qin Lu",
                "Rong Xiang",
                "Minglei Li",
                "Chu-Ren Huang."
            ],
            "title": "Fake news detection through multi-perspective speaker profiles",
            "venue": "Proceedings of the eighth international joint conference on natural language processing (volume 2: Short papers), pages",
            "year": 2017
        },
        {
            "authors": [
                "Minh-Thang Luong",
                "Hieu Pham",
                "Christopher D Manning."
            ],
            "title": "Effective approaches to attentionbased neural machine translation",
            "venue": "Proceedings",
            "year": 2015
        },
        {
            "authors": [
                "Jing Ma",
                "Wei Gao",
                "Prasenjit Mitra",
                "Sejeong Kwon",
                "Bernard J Jansen",
                "Kam-Fai Wong",
                "Meeyoung Cha"
            ],
            "title": "Detecting rumors from microblogs with recurrent neural networks",
            "year": 2016
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean."
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "arXiv preprint arXiv:1301.3781.",
            "year": 2013
        },
        {
            "authors": [
                "Tanushree Mitra",
                "Eric Gilbert."
            ],
            "title": "Credbank: A large-scale social media corpus with associated credibility annotations",
            "venue": "Ninth international AAAI conference on web and social media.",
            "year": 2015
        },
        {
            "authors": [
                "Amir More",
                "Amit Seker",
                "Victoria Basmova",
                "Reut Tsarfaty."
            ],
            "title": "Joint transition-based models for morpho-syntactic parsing: Parsing strategies for MRLs and a case study from modern Hebrew",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2019
        },
        {
            "authors": [
                "Jeff Z Pan",
                "Siyana Pavlova",
                "Chenxi Li",
                "Ningxi Li",
                "Yangmei Li",
                "Jinshuo Liu."
            ],
            "title": "Content based fake news detection using knowledge graphs",
            "venue": "International semantic web conference, pages 669\u2013683. Springer.",
            "year": 2018
        },
        {
            "authors": [
                "James W Pennebaker",
                "Martha E Francis",
                "Roger J Booth."
            ],
            "title": "Linguistic inquiry and word count: Liwc 2001",
            "venue": "Mahway: Lawrence Erlbaum Associates, 71(2001):2001.",
            "year": 2001
        },
        {
            "authors": [
                "Kashyap Popat",
                "Subhabrata Mukherjee",
                "Andrew Yates",
                "Gerhard Weikum."
            ],
            "title": "Declare: Debunking fake news and false claims using evidence-aware deep learning",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
            "year": 2018
        },
        {
            "authors": [
                "Hannah Rashkin",
                "Eunsol Choi",
                "Jin Yea Jang",
                "Svitlana Volkova",
                "Yejin Choi."
            ],
            "title": "Truth of varying shades: Analyzing language in fake news and political fact-checking",
            "venue": "Proceedings of the 2017 conference on empirical methods in natural language",
            "year": 2017
        },
        {
            "authors": [
                "Julio CS Reis",
                "Andr\u00e9 Correia",
                "Fabr\u00edcio Murai",
                "Adriano Veloso",
                "Fabr\u00edcio Benevenuto."
            ],
            "title": "Supervised learning for fake news detection",
            "venue": "IEEE Intelligent Systems, 34(2):76\u201381.",
            "year": 2019
        },
        {
            "authors": [
                "Arjun Roy",
                "Kingshuk Basak",
                "Asif Ekbal",
                "Pushpak Bhattacharyya."
            ],
            "title": "A deep ensemble framework for fake news detection and classification",
            "venue": "arXiv preprint arXiv:1811.04670.",
            "year": 2018
        },
        {
            "authors": [
                "Natali Ruchansky",
                "Sungyong Seo",
                "Yan Liu."
            ],
            "title": "Csi: A hybrid deep model for fake news detection",
            "venue": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pages 797\u2013 806.",
            "year": 2017
        },
        {
            "authors": [
                "Amit Seker",
                "Elron Bandel",
                "Dan Bareket",
                "Idan Brusilovsky",
                "Refael Shaked Greenfeld",
                "Reut Tsarfaty."
            ],
            "title": "Alephbert: A hebrew large pre-trained language model to start-off your hebrew nlp application with",
            "venue": "arXiv preprint arXiv:2104.04052.",
            "year": 2021
        },
        {
            "authors": [
                "LLOYD SHAPLEY."
            ],
            "title": "A value for n-person games",
            "venue": "contributions to the theory of games ii, kuhn, h., tucker, a.",
            "year": 1953
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos."
            ],
            "title": "Automated fact checking: Task formulations, methods and future directions",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 3346\u20133359.",
            "year": 2018
        },
        {
            "authors": [
                "Reut Tsarfaty",
                "Shoval Sadde",
                "Stav Klein",
                "Amit Seker."
            ],
            "title": "What\u2019s wrong with hebrew nlp? and how to make it right",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
            "year": 2019
        },
        {
            "authors": [
                "UBOHO Victor."
            ],
            "title": "Robust semi-supervised learning for fake news detection",
            "venue": "Ph.D. thesis, Prairie View A&M University Prairie View, TX, USA.",
            "year": 2020
        },
        {
            "authors": [
                "Marco Viviani",
                "Gabriella Pasi."
            ],
            "title": "Credibility in social media: opinions, news, and health information\u2014a survey",
            "venue": "Wiley interdisciplinary reviews: Data mining and knowledge discovery, 7(5):e1209.",
            "year": 2017
        },
        {
            "authors": [
                "Andreas Vlachos",
                "Sebastian Riedel."
            ],
            "title": "Fact checking: Task definition and dataset construction",
            "venue": "Proceedings of the ACL 2014 workshop on language technologies and computational social science, pages 18\u201322.",
            "year": 2014
        },
        {
            "authors": [
                "Soroush Vosoughi",
                "Deb Roy",
                "Sinan Aral."
            ],
            "title": "The spread of true and false news online",
            "venue": "Science, 359(6380):1146\u20131151.",
            "year": 2018
        },
        {
            "authors": [
                "William Yang Wang."
            ],
            "title": "liar, liar pants on fire\u201d: A new benchmark dataset for fake news detection",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 422\u2013426.",
            "year": 2017
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "year": 2020
        },
        {
            "authors": [
                "Lina Zhou",
                "Judee K Burgoon",
                "Jay F Nunamaker",
                "Doug Twitchell."
            ],
            "title": "Automating linguisticsbased cues for detecting deception in text-based asynchronous computer-mediated communications",
            "venue": "Group decision and negotiation, 13(1):81\u2013106.",
            "year": 2004
        },
        {
            "authors": [
                "Xinyi Zhou",
                "Reza Zafarani."
            ],
            "title": "Fake news: A survey of research, detection methods, and opportunities",
            "venue": "arXiv preprint arXiv:1812.00315.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Our society is struggling with an unprecedented amount of falsehoods, hyperboles, and half-truths (Hassan et al., 2017). False information, commonly termed \u2019fake news\u2019, is now viewed as one of the greatest threats to democracy, journalism, and freedom of expression. Distributing false content has become a significant concern in recent years when it is claimed to have helped change public opinion in the US elections of 2016 (Zhou and Zafarani, 2018). Our economies are not immune\nto the spread of fake news either, with fake news being connected to stock market fluctuations and large trades. For example, fake news claiming that Barack Obama, the 44th President of the United States, was injured in an explosion, wiped out $130 billion in stock value (Zhou and Zafarani, 2018). Also, recent studies have shown that false content reaches \u201cfarther\u201d and a wider audience of readers than real content (Vosoughi et al., 2018).\nEven if a correction of the false information reaches the misinformed audience, simply providing the correct information is ineffective, as continued reliance on misinformation is likely when the misinformation conforms to a person\u2019s preexisting belief system, yet the correction does not (Lewandowsky et al., 2005). Retracting misinformation that runs counter to a person\u2019s worldview can ironically even strengthen the to-be-corrected information, a phenomenon known as the worldview backfire effect (Hart and Nisbet, 2012).\nAddressing the spread of fake news necessitates a comprehensive approach to assess the credibility of information. Credibility assessment, distinct yet sometimes used interchangeably with fake news detection, involves evaluating the veracity of statements or claims made by individuals, particularly those in influential positions. While fake news explicitly entails deception, credibility assessment encompasses a broader spectrum, scrutinizing the authenticity and reliability of information regardless of the intent behind it.\nManual credibility assessment is not sufficient in effectively combating the rapid spread of false information, leading to a pressing need for alternative approaches. In particular, there is a need for automatic credibility assessment tools, which leverages statistical and machine learning methods to analyze and determine the credibility of statements or news as they come in, thereby avoiding the human-labor bottleneck.\nTo effectively combat fake news and assess the\ncredibility of information, there is a pressing need for comprehensive datasets that facilitate the development and evaluation of automatic detection tools. Existing datasets primarily focus on English language content (Guo et al., 2022), leaving a substantial gap in the study of other languages, including Hebrew, which known for its unique linguistic characteristics (Tsarfaty et al., 2019). Additionally, many available datasets lack crucial supplementary information such as context, semantic features, and metadata associated with the statements in question. Including these features is essential for building robust NLP models, enhancing their understanding and accuracy in credibility assessment tasks.\nThe need for such fake-news detectors is particularly pressing in areas of which socio-political situations may be sensitive yet may have world-wide effects \u2014 such as the Middle East. To mitigate this, we introduce the HeTrue dataset, the first-ever Hebrew fake-news detection benchmark.\nThe HeTrue dataset is a unique Hebrew resource composed of 1021 statements made by politicians. Each statement is meticulously labeled for truthfulness and supplemented with context information. The dataset is further augmented with metadata and semantic features, which have been validated and annotated by nonpartisan fact-checkers, compiled with the International Fact-Checking Network (IFCN). While the HeTrue dataset addresses a significant gap in Hebrew language resources for credibility assessment, its comprehensive set of features also raises the bar for similar datasets in other languages, including English.\nUtilizing this novel benchmark, in this paper we focus mainly on linguistic-based approaches for automatic credibility assessment \u2014 that is, one that relies solely on the claim, or the claim\u2019s context as input for detection. Linguistic approaches are frequently utilized to tackle this task (Wang, 2017, Roy et al., 2018), as they can operate without additional resources or data. This makes these approaches more scalable and straightforward to implement in real-time applications with minimal effort and budget.\nThese approaches hinge on the assumption that individuals tend to express themselves differently, whether verbally or in writing, when conveying false information compared to true information. Fraser (1991) asserts that these differences stem from a sense of stress, manifesting in a decrease in cognitive integration capacity, precision, organiza-\ntion, and prioritization. These challenges result in a change in the normal elements of the speaker\u2019s language. Prior works have found that these linguistic signals can also specifically assist in distinguishing between truth and falsehood in politicians\u2019 statements (Wang, 2017).\nWe hypothesize that linguistic signals indicative of a statement\u2019s credibility exist in Hebrew and that these signals correlate with the credibility assessments made by professional journalists, who collaborated with us to create our gold-standard benchmark. To provide a broader view, we have also explored alternative methods, which will be discussed in the following sections.\nAs Hassan et al. (2015) first introduced, achieving the \u2019Holy Grail\u2019 in end-to-end credibility assessment systems, also referred to as fact-checking, requires a fully automatic platform. This platform should consist of three main components: a ClaimSpotter to detect claims that need validation in real time, a Claim-Checker to evaluate the veracity of these claims, and a Fact-Check Reporter to justify the evaluations with convincing evidence. This system would make determinations by analyzing the claim\u2019s text, source, claimant profile, and retrieving evidence from databases of previously checked claims. Our work advances towards this \"Holy Grail,\" focusing on a fully automated credibility assessment system for Hebrew texts. The unique features of our dataset1 and our experiments could also benefit credibility assessment in other languages. We call on the computing and journalism communities to join in on this pursuit, given the importance and timeliness of this task.\nThe contribution of our paper is, hence, manifold. First, we introduce HeTrue, the first publicly available Hebrew dataset for fake news detection. This dataset comprises 1021 statements from politicians and public figures, manually labeled by professional journalists, and serves as a valuable resource for researchers investigating fake news detection in Hebrew. To the best of our knowledge, this is the first claim assessment dataset that accompanies each claim with its context, alongside semantic and metadata features.\nSecond, we conduct an extensive experimental analysis, exploring a variety of modeling types and dataset setups. To establish a baseline performance for the task, we employ several methods including a hand-crafted feature-based model, a recurrent neu-\n1See comparison with other datasets in Appendix A.7\nral network (RNN) with static embeddings, and a model extending AlephBERT\u2014a Hebrew monolingual pre-trained encoder (Seker et al., 2021). Our experiments with these models demonstrate the presence of linguistic cues in Hebrew text that can distinguish between credible and non-credible statements. we provide an analysis of the key features from the hand-crafted feature model, emphasizing their significance.\nThirdly, we demonstrate the importance of integrating context and evidence in credibility assessment. Our novel model, which combines statement context and an evidence-retrieval mechanism, results in significant performance enhancements. We further compare our results with metadata-based models, revealing that our linguistic approach outperforms these models, and demonstrating that the addition of metadata features to a linguistic (handcrafted feature model) does not have an additive effect on the model\u2019s performance 2."
        },
        {
            "heading": "2 Related work",
            "text": "Credibility assessment is an interdisciplinary field that involves evaluating the believability, trustworthiness, reliability, accuracy, fairness, and objectivity of claims or statements (Viviani and Pasi, 2017). It is closely related to fact checking and deception detection, and focuses on assessing the quality of information and the level of trust that can be placed with respect to claims. The goal of credibility assessment is to differentiate between credible and non-credible claims, without necessarily determining the root cause of false information, such as whether it is due to incorrect factual claims or the intent of the speaker (Giachanou et al., 2019).\nThere are three categories of approaches for credibility assessment and fake news detection: linguistic (text-based), evidence-retrieval, and metadatabased, which typically incorporate machine learning techniques for training effective and accurate classifiers (de Souza et al., 2020).\nLinguistic approaches attempt to identify clues in the text in order to verify veracity (Conroy et al., 2015, Zhou et al., 2004). The work of Afroz et al. (2012) posits that changes in certain linguistic characteristics can signal an attempt to conceal writing style, aiding in the detection of deceptive texts. This research present the effectivness of specialized set of lying-detection features including quantity\n2The HeTrue dataset and models are publicly available at: https://github.com/OnlpLab/HeTrue.\nmetrics (such as syllable and word counts), vocabulary and grammatical complexity, and an analysis of the usage of words denoting uncertainty, specificity, and expressiveness. On the other hand, Hancock et al. (2007) focus on linguistic traits more frequently associated with deceptive behavior. Their examination of 242 transcripts revealed that individuals inclined to deception tend to produce lengthier texts, utilize a greater number of sensory expressions (such as references to sight or touch), and exhibit a preference for using pronouns that shift focus away from themselves and towards others, in comparison to when they are being truthful. Reis et al. (2019) incorporates various psycholinguistic features derived from LIWC (Pennebaker et al., 2001), to detect persuasive and biased language. In our work, we systematically adapt and extend these methodologies to assess statement credibility in Hebrew.\nThe second approach is Evidence retrieval, which aims to find sources supporting or refuting the claim. One strategy involves the construction of knowledge graphs, facilitating the identification of relationships between textual segments and graph instances Pan et al. (2018). Alternatively, preestablished knowledge bases can be employed to garner pertinent evidence, subsequently leveraging models that capitalize on this retrieved information for text classification Thorne and Vlachos (2018).\nThe third approach is metadata-based, and it is often used alongside other methods. It involves analyzing text-related information like publication date, claimant\u2019s demographics, and source type (Long et al., 2017). Additionally, with the rise of fake news on social networks (de Souza et al., 2020), behavioral analysis becomes important to identify unreliable patterns through user or message interactions (Ruchansky et al., 2017).\nIn terms of the credibility levels to be predicted, the work by Rashkin et al. (2017) was first to introduce the task of credibility assessment in scale of 1-6, in their work with the Liar dataset (Wang, 2017). They used LSTM with GLOVE word embedding and showed that stylistic cues can help determine the truthfulness of text. Roy et al. (2018) applied CNN and BiLSTM models to the same dataset and task, aiming to extract patterns from short statements and understand the unique behaviors of source speakers using different dataset attributes, underscoring the importance of including speakers\u2019 profile information for fake news classifi-\ncation.\nGiachanou et al. (2019) incorporated emotional signals for credibility assessment, which presented improved performance, relying on the fact that those signals can play a vital role in detecting false information (Ekman, 1992 Vosoughi et al., 2018).\nRecent progress in this field encapsulates a variety of methodologies. Victor (2020), for instance, present a semi-supervised deep learning (SSDL) pipeline employing an attention RNN-based model. Additionally, models like X-Fact (Gupta and Srikumar, 2021) emphasize the integration of evidence with attention architecture. A distinctive approach is taken by FakeFlow (Ghanem et al., 2021), which focus on news articles and study the importance of information flow to detect fake news.\nContext plays a pivotal role in understanding and verifying claims. However, its inclusion in datasets is rare, and also varies significantly (Guo et al., 2022). Some datasets, such as those created by Mitra and Gilbert (2015) and Ma et al. (2016), incorporate context derived from related threads. In these instances, a claim is contextualized with a set of pertinent posts, often originating from the same thread, to compensate for the limited context within an individual post. On the other hand, Ghanem et al. (2021) adopt a distinctive strategy, considering the segments of an entire article to analyze information flow. Our work uniquely provides context for sentence-level claims, drawing directly from preceding text or speech. This approach captures the immediate discourse surrounding the claim, highlighting the critical role of textual interactions in credibility assessment. We believe this direct integration of granular context at the sentence level is a novel contribution to the field.\nWhile English remains the predominant language for fake news detection datasets, other languages are often underrepresented. As observed, most efforts to date, such as those by Vlachos and Riedel (2014); Wang (2017), have extracted realworld claims from dedicated English-based websites like Politifact. However, Gupta and Srikumar (2021) are notable exceptions, having curated claims from 25 languages. Their multilingual baselines, including models like Claim Only, Attentionbased Evidence Aggregator, and Augmenting Metadata Model, have established a promising foundation for multilingual fact-checking. Another initiative is by Baly et al. (2018), who compiled a dataset of 219 Arabic statements. Both these stud-\nies employed Google\u2019s evidence retrieval to bolster claim veracity modeling. Yet, their scope appears limited, especially in terms of integrating context and a rich set of metadata/semantic features. No Hebrew-specific dataset for fact verification or fake news detection existed until our contribution, focusing on real-life statements.\nTowards Hebrew Fake-News Detection. The availability of labeled benchmark datasets is crucial for building statistical approaches for automatic fake news detection in new languages. English has been extensively studied with a large amount of annotated data, while other, less-researched languages, have limited or no annotated data available (Guo et al., 2022).\nAs of yet, no publicly available Hebrew dataset on fake news detection or claim credibility assessment is publicly available. Furthermore, a limited number of works exist in the field of automatic credibility and falsehood detection in Hebrew. The studies of Dilmon (2004, 2007, 2013) examine the linguistic differences between truthful and deceptive discourse in Hebrew and aim to develop a primary test for the cognitive and emotional processes involved in deception (HaCohen-Kerner et al., 2015).\nAlthough these studies do not analyze natural inputs and instead focus on laboratory-created inputs, they offer valuable insights into the cognitive and emotional processes involved in deception in the Hebrew language. In their corpus, comprising 48 pairs of stories told by 48 subjects, they found several distinguishing features:\n\u2022 Morphological criteria: False stories exhibit increased use of 3rd person verbs and decreased use of 1st person verbs, while true stories show intensified use of past tense verbs.\n\u2022 Syntactic criteria: False stories tend to have increased use of dependent clauses and decreased use of independent and conjunction clauses.\n\u2022 Semantic aspects: False stories involve intensified use of synonym words, relative pronouns, negation, and generalized words.\nLeveraging insights from these Hebrew-focused studies, alongside the English-based works, we enhance credebility assessment in Hebrew. Our work extends the existing knowledge base, constructing comprehensive linguistic feature-based models and evaluating their effectiveness on our task.\nWhile our contribution heralds a significant leap in Hebrew-centric datasets for fact verification, it is not confined to that. It further includes a wide range of experiments and the introduction of empirical evaluation of novel models tailored explicitly for this task: Context-Based Model (CBM), Attentionbased Evidence Aggregator (Attn-AE) and Context and Evidence Model (CCEM)."
        },
        {
            "heading": "3 HeTrue: a New Benchmark Dataset for Hebrew Credibility Assessment",
            "text": "In this work, we present HeTrue, a unique and firstof-its-kind Hebrew dataset for credibility assessment, meticulously compiled through a collaboration with professional journalists from \u201cThe Whistle\", an Israeli fact-checking organization. The dataset comprises 1021 statements from Israeli politicians and public figures, each accompanied by its credibility score.\n\u201cThe Whistle\" operated as an independent NGO from 2017 to 2018 before integrating into Globes newspaper in January 2019. It is noteworthy for being the only Israeli institute complying with the International Fact-Checking Network (IFCN), upholding transparency, impartiality, and fairness in fact-checking. Our partnership with \u201cThe Whistle\" ensured the integrity of data and alignment with scientific research standards, involving necessary adjustments during collection and annotation.\nThe statements, spanning one to two sentences,3 were manually collected by journalists from \u201cThe Whistle\" in the area between February 2017 to June 2023.\nEach statement was assessed on a nuanced 5- point truthfulness scale, ranging from \u2019True\u2019 to \u2019False\u2019, with intermediary labels including \u2019MostlyTrue\u2019, \u2019Partly-True\u2019, and \u2019Mostly-False\u2019. Consistently with recent studies (Gupta and Srikumar, 2021), we also introduced two additional labels: \u2019Unverifiable\u2019 for claims lacking sufficient evidence and \u2019Other\u2019 for cases that do not fit into any of the aforementioned categories. This approach is widely adopted by most fact-checkers (Guo et al., 2022).\nTo ensure label accuracy, \u201cThe Whistle\", in partnership with the authors, implemented a rigorous three-stage inter-annotator agreement process. Each statement was initially examined by one professional journalist, followed by an independent review by a second. If disagreements occurred, a\n3Average statement length (tokens) is 25.\nthird journalist made the final decision, ensuring a highly reliable and consistent dataset. Although this rigorous process resulted in a smaller dataset size than the initially available set of claims, it guarantees a high-quality and reliable dataset.\nIn addition to the sentence-level statements and their journalist\u2019s verified label, we introduce in this dataset the following features: \u2018claim context\u2019 and \u2018journalist-edited claim\u2019.4 Crucially, the context encapsulates the sentences around the claim,5 adding a new dimension to claim assessment. In our work, we examine the improvement gained by incoporating context with the statement itself.\nAdditionally, we analyzed semantic features including Field, Subject, Title, and tags. This twofold process first involved a \u201cThe Whistle\" journalist, followed by an editor who reviewed the suggested values and refined them as needed. The full feature list including additional metadata features can be found in Appendix A.6. The label distribution of the dataset is presented in Table 3 in the Appendix.\nAssessment Scenarios and Data Splits To gain a better understanding of the underlying task, we created four instances of our dataset for further experimentation. \u2022 FULL-SPECTRUM: The entire dataset, five credibility scores. \u2022 TF-ONLY: Statements only with a \"True\" or \"False\" label. Other labels are removed. \u2022 TF-BINS: Following previous studies (Popat\net al., 2018; Giachanou et al., 2019), labels are grouped into binary classes, with true, mostly true and half true forming one class (i.e., true), and the rest as false.\n\u2022 FS-WRITTEN: Statements published in a written media, such as Facebook posts or newsletters. Excluding transcribed statements. For the experiments described below, we excluded the \"Unverifiable\" and \"Other\" claims, leaving five credibility scores for our empirical investigation."
        },
        {
            "heading": "4 HeTrue Credibility Assessment",
            "text": ""
        },
        {
            "heading": "4.1 Experimental settings",
            "text": "In this paper, we examine the claim credibility assessment task by establishing a rigorous experimental framework featuring a variety of computational models. Our approach employs both traditional\n4Journalists often revise the original claim for brevity and clarity prior to publication\n5Average context length is 51 tokens.\nlinguistic methodologies and advanced deep learning algorithms, each utilizing different aspects of the dataset. This diversified strategy provides us with a broader understanding of the task and its complexities, and sets a comprehensive benchmark for future research. The implemented models are presented in Section 4.3."
        },
        {
            "heading": "4.2 Evaluation",
            "text": "In line with previous work (Guo et al., 2022), we employ the macro F1 score as the evaluation metric. To ensure the statistical robustness of our findings, we employ a bootstrap technique (Dror et al., 2018) that involves generating 10,000 resampled variations of the test set. By calculating the standard deviation of the performance metric across these iterations, we evaluate the stability of our results.\nTraining Setup We employed nested cross validation to evaluate the performance of our models. This approach allows us to tune the hyperparameters of the model in an inner loop while evaluating its performance on unseen data in an outer loop. This helps in avoiding overfitting and providing a more robust estimate of model\u2019s performance on unseen data. We used k=5 for both inner and outer loop. The hyperparameters are optimized by training the model using different values on the subfolds, and then evaluating the performance of each set of hyperparameters on the sub-fold reserved for testing. The final performance of the model was determined by computing the mean F1 score and standard deviation of the outer loop held-out test set 10,000 times using bootstrap resampling. This evaluation was conducted after optimizing the model through the inner loop. Hence, hyperparameters are optimized independently of the testing data. For hyperparameter optimization we used Optuna (Akiba et al., 2019), a framework for sequential modelbased optimization (SMBO) (Hutter et al., 2011) with TPE as the sampling algorithm (Bergstra et al., 2011), to find the best combination of hyperparameter values for a given machine learning model. For further details on preprocessing and hyperparameter tuning, please refer to Appendix A."
        },
        {
            "heading": "4.3 Models",
            "text": "Strong Linguistic Baseline Drawing from our hypothesis that linguistic cues vary with the credibility of a claim, we engineered linguistic features, based on previous works, from both the statement and its full context, elaborated in Appendix A.\nThe results, described in Table 1, reveal several noteworthy observations. Firstly, it is clear that the models significantly outperform the metadata models across all four experimental setups. A marked improvement is observed between the fullspectrum and written setups, which is not found in the metadata models. The latter achieved 5% more compared to the former setup. These results align with two key conclusions. First, that there are linguistic cues within the text that can attest to its credibility, which is evident across all four setups and corresponds to the model performance. Second, that these cues are stronger in written statements as opposed to transcribed ones. This finding likely indicates that information is lost or that noise is introduced during the transcription process, such as punctuation marks or writing bias, which impairs the model\u2019s learning.\nFurthermore, when examining the performance of the model that takes context into account against the model that only considers the statement, it is observed that the former yields superior results (see Table 2). This suggests that additional information present in the context is crucial for prediction. This conclusion aligns with the findings of similar task in news articles (Ghanem et al., 2021). See Appendix A.3 for full feature importance analysis.\nMetadata-based Models We examine the performance of a model based solely metadata and semantic features, and analyze the combination between these features and the strong linguistic baseline. The Feature list and the preprocessing steps are described in Appendix A.4.\nWe outline our empirical results in Table 1. We observe that in most dataset setups, the metadata models perform better than their counterparts. In the TF Bins setup, the majority baseline outperforms. We hypothesize that the binning technique might have an adverse impact on the data and feature distribution, affecting the results. Both the metadata feature-based model and the combined model obtain significant improvements and outperform the majority baseline in other setups, indicating that substantial information correlated with claim credibility exists in those features.\nAdditionally, the combined model achieved better results than the metadata-only model. Analysis of feature importance using Shapley values (SHAPLEY, 1953), shows that the top 5 features in the allfeatures metadata model are: media source, party, Knesset, gender (objective features), and field (Se-\nmantic features). The inclusion of semantic features improves performance significantly.\nFinally, we examine the effect of integrating metadata with hand-crafted linguistic features. We found that results were comparable. This was somewhat surprising and it suggests a connection between the linguistic and metadata features. Further research is needed to better understand this relationship. Table 1 summarizes all results including the comparison to a Majority baseline."
        },
        {
            "heading": "4.4 Deep Learning Models",
            "text": "Manual extraction of linguistic features might be a demanding task. In addition, such methodology might not take into account hidden patterns in text, and such patterns may be unknown to the researchers or hard to manually extract.\nHence, recent works on credibility assessment and fake news detection exploited various deep neural architectures. In this work, we explore deep learning methods and incorporate recent advancements in the field including state-of-the-art transformers-based contextualized word embeddings in Hebrew (Seker et al., 2021). Additionally, we suggest a novel architecture that combines both evidence and context to enhance the performance on our task.\nWe conducted experiments in two setups: Statement-Focus and Context-Aware. For each setup we have conducted several experiments as will be described below. We ran our experiments with the Huggingface transformers library Wolf et al. (2020) available under an Apache-2.0 license. For all models, we used the publicly available base checkpoints on Huggingface.6"
        },
        {
            "heading": "4.4.1 Statement-Focus Models",
            "text": "The first category of our models focuses only on the statements. In this setup, we implemented and compared three models.\nModel 1: RNN with Static Word Embeddings The first model is based on RNNs using static word embeddings. We experimented with Word2Vec (Mikolov et al., 2013) and fastText (Bojanowski et al., 2017) embeddings, and selected fastText due to its superior performance on our task.\nModel 2: AlephBERT with a Transformer Architecture. To capture more complex patterns and dependencies in the data, we employed a\n6https://huggingface.co/models\ncontextualized-based model with a transformer architecture. We utilized the state-of-the-art AlephBERT (Seker et al., 2021) which was pre-trained on 17.6G of Hebrew text and we fine-tuned it on our task.\nModel 3: Attention-based Evidence Aggregator (Attn-EA) Building upon the attention-based evidence aggregation (Attn-EA) model proposed by Gupta and Srikumar (2021) \u2013 a significant work identified by Guo et al. (2022) \u2013 we made refinements to the evidence collection pipeline. Our revised attention-based evidence aggregation model, aggregates evidence from the top five Google search snippets associated with each claim. To reduce potential bias, we curated an \u2018Excluded Websites\u2019 list, which filters out the original source of the claim or any websites that disclose the claim\u2019s veracity, such as \u201cThe Whistle\". Moreover, snippets containing more than 85% of the original claim are omitted. We employed AlephBERT (Augenstein et al., 2019) to separately encode the claim and each piece of evidence, extracting the output of the CLS token: c, [e1, e2, ..., en]. The dot-product attention mechanism (Luong et al., 2015) is then applied to compute attention weights [\u03b11, \u03b12, ..., \u03b1n] and a corresponding linear combination: e = \u03a3i\u03b1iei. The combined representation is subsequently concatenated with c and fed to the classification layer."
        },
        {
            "heading": "4.4.2 Context-Aware Models",
            "text": "The second category of our models incorporates the full context into the analysis. In this setup, we developed and evaluated three models.\nModel 1: RNN with FastText and Full Context This model uses the same RNN-based architecture as the statement-focus setup but incorporates the full context into the analysis.\nModel 2: Context-Based Model (CBM) We propose a model architecture that leverages the power of the AlephBERT model for text understanding and incorporates an attention mechanism to capture the joint interaction between the statement and its context. Given a statement S and its context C, we tokenize and encode them using AlephBERT, yielding embeddings OS and OC . We then employ an attention mechanism, with OS as the query and OC as the key and value matrices. The attention mechanism computes the attention weights and generates the merged representation OSC . This merged representation captures the con-\ntextual interactions between the statement and its context. It is then passed through a fully connected layer with weights W and bias b to produce the output Z. Finally, the class probabilities P are obtained by applying the softmax function to Z.\nModel 3: Combined Context and Evidence Model (CCEM) Recent studies showed the importance of evidences and context to claimassessments tasks. However, models doing so not exist. The Combined Context and Evidence Model (CCEM) architecture leverage the principles of both the Context-Based Model (CBM) and the Attention-based Evidence Aggregator. Initially, similar to the CBM, it forms a merged contextstatement representation OSC using the statement S and its context C. Concurrently, the model applies the evidence aggregation approach of the Attention-based Evidence Aggregator to obtain a combined evidence representation E, derived from the top Google search snippets associated with the claim. The representations OSC and E are then processed through a transformer-based integration layer. This layer uses OSC and E as inputs, conducting a multi-head self-attention operation. The attention mechanism considers OSC as the query and E as the key and value matrices, resulting in\na fused representation OSCE that encapsulates the interactions between the statement, its context, and the external evidence. Finally, the class probabilities P are extracted from OSCE using a softmax function."
        },
        {
            "heading": "4.4.3 Results",
            "text": "Table 2 highlights several significant patterns and key findings from our experiments.\nOne primary observation from our experiments is the consistent outperformance of deep learning models, compared to simpler hand-crafted featurebased models. The latter approaches do highlight some useful features for this task and manage to surpass a number of baseline models, indicating that there are explicit features that can be used for Hebrew credibility assessment task. However, the more complex architectures, including models with static word embeddings and transformer-based models with contextualized embedding, prove to be more effective and deliver better performance overall. A surprising result is the improvement in performance exhibited by the \u2019Evidence-based\u2019 model in the \u2019Statement only\u2019 setup. This model surpasses the performance of the advanced AlephBERT model, even with the CBM architecture, highlighting the significant impact of integrating\nexternal evidence aggregation on model effectiveness.\nIn the Context-Aware setup, the \u2019Combined Context and Evidence Model\u2019 (CCEM) marginally outperforms other models, highlighting the effectiveness of utilizing both full context and external evidence. The RNN + fastText models also demonstrated substantial improvement when transitioning from \u2019Statement only\u2019 to \u2019Full context\u2019. This enhancement emphasizes the impact of context in textual understanding, pointing out the limitations of models that focus solely on individual statements.\nInterestingly, statement-focused AlephBERT outperforms the Context-Aware RNN model. This could be attributed to AlephBERT\u2019s advanced pretraining, which enhances its capabilities in natural language understanding, generalization, and possibly even in encoding writing styles. Throughout the work, we observe that written statements consistently lead in terms of performance. This trend strengthens the hypothesis that there are linguistic and other cues not faithfully captured or altered during text transcription."
        },
        {
            "heading": "5 Conclusions",
            "text": "Our study introduces HeTrue, a unique and rich dataset designed for automatic credibility assessment in Hebrew. This dataset stands out not only as the first of its kind in Hebrew, but also as the first to incorporate a comprehensive set of features including the statement context, the statement itself, the journalist\u2019s version, and additional semantic and metadata features - annotated by professional journalists in a rigours process.\nThe findings from our experiments underscore the effectiveness of linguistic approaches in the challenging task of credibility assessment. Our results consistently demonstrate enhanced performance when context is integrated into the models across various setups. Furthermore, we show that incorporating evidence-retrieval mechanisms alongside context further contributes to the models\u2019 performance.\nDespite the advancements made, credibility assessment, particularly in morphologically rich languages like Hebrew, remains a challenging task. While pre-trained language models like AlephBERT show promise in effectively understanding and generalizing language even with limited context, the quest for better accuracy continues. Future research will aim to refine and further improve the\nmethodologies introduced in this study. Finally, the HeTrue dataset, with its unique combination of features, holds potential for a broader range of NLP applications such as stance classification, argument mining, topic modeling, and rumor detection. This broad utility further underscores the value of our contribution and opens new pathways towards more sophisticated fake news detection methodologies."
        },
        {
            "heading": "6 Limitations",
            "text": "The dataset is curated by Israeli professional journalists who carefully analyze claims made by public figures and politicians in Hebrew media. However, as stated in the paper, some of the claims are written by the public figures themselves, and others are transcribed by \u201cThe Whistle\u201d journalists. This could lead to potential nuances in the way the claims are transcribed, which could influence the results of the analysis. One might assume that the transcription is biased towards the outcome of the claim\u2019s truthfulness. To address this issue, we created the written-only setup that evaluates statements that are originally from written media type, such as Facebook, Twitter or opinion articles. Additionally, the dataset used may reflect some collection bias, as the journalists collected the claims that they view as most interesting to the public. This may result in the collection of statements that use provocative language, are false, or have other controversial characteristics. This may limit the generalizability of the findings for naturally occurring texts. Future work could focus on discerning the effects of transcription nuances and overcoming potential biases."
        },
        {
            "heading": "7 Ethics Statement",
            "text": "The automatic detection of fake news is a complex task that raises important ethical considerations. One of the key concerns is the risk of using claimants profiling, which could lead to the stigmatization of certain individuals or groups based on their data, such as their gender or political party. Furthermore, there is a risk of bias towards certain ways of expression, thus, linguistic approaches also need to be carefully considered to avoid any form of bias. Additionally, the use of automatic detection systems raises questions about accountability and transparency. The models used in this task can be opaque and difficult to interpret, making it challenging to understand how and why certain decisions are being made. This can make it difficult\nto identify and correct errors, and can also make it difficult to hold those responsible accountable for any negative consequences that may arise from the use of these systems. These concerns are amplified in today\u2019s information landscape where data can be created or manipulated by Generative Artificial Intelligence, further complicating the discernment of facts from fiction and truths from lies. Given these ethical considerations, we urge careful handling of automatic credibility assessment and encourage inclusive, responsible system development."
        },
        {
            "heading": "8 Broader Impact",
            "text": "This paper is submitted in the wake of a tragic terrorist attack perpetrated by Hamas, which has left our nation profoundly devastated.\nOn October 7, 2023, thousands of Palestinian terrorists infiltrated the Israeli border, launching a brutal assault on 22 Israeli villages. They methodically moved from home to home brutally torturing and murdering more than a thousand innocent lives, spanning from infants to the elderly. In addition to this horrifying loss of life, hundreds of civilians were abducted and taken to Gaza. The families of these abductees have been left in agonizing uncertainty, as no information, not even the status of their loved ones, has been disclosed by Hamas. This is not fake new. This is first-hand evidence from our family, friends, relatives and acquaintances, the ones that survived the horror.\nThe heinous acts committed during this attack, which include acts such as shootings, sexual assaults, burnings, and beheadings, are beyond any justification.\nBeyond the loss we suffered as a nation and as human beings due to this violence, many of us feel abandoned and betrayed by members of our research community who did not reach out and were even reluctant to publicly acknowledge the inhumanity and total immorality of these acts.\nThis horrific situation highlights the devastating impact of misinformation, fake news and propaganda, and what they can inflict on society. In this case, manipulated stories and misleading narratives contributed to justifying or diminishing the severity of Hamas\u2019 atrocities, underscoring the urgent need for improved methods in assessing the credibility of information, such as the methods advocated for and developed in this work.\nWe fervently call for the immediate release of all those who have been taken hostage and urge the\nacademic community to unite in condemnation of these unspeakable atrocities committed by Hamas, who claim to be acting in the name of the Palestinian people. We call all to join us in advocating for the prompt and safe return of the abductees, as we stand together in the pursuit of justice and peace.\nThis paper was finalized in the wake of these events, under great stress while we grieve and mourn. It may not be as polished as we would like it to be."
        },
        {
            "heading": "9 Acknowledgments",
            "text": "We would like to thank \"The Whistle\" at \"Globes\" for their essential support in creating the HeTrue dataset, serving as a foundational element of our paper and facilitating future research in this domain. This research was funded by the Israeli Ministry of Science and Technology (MOST) grant No. 3- 17992, and an Israeli Innovation Authority grant (IIA) KAMIN grant, for which we are grateful. In addition, This project has received funding from the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme, grant agreement No. 677352."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Text preprocessing",
            "text": "Before creating and extracting the linguistic features, we prepare our data for that. Cleansing(cleaning) the dataset to correct corrupt or inaccurate records and extracting basic linguistics elements that will be used later to create the features. To do so , we used YAP (Yet Another Parser)More et al., 2019 developed by Reut Tsarfaty and Amir More. To preprocess the statements, the following stages were taken:\n\u2022 Removing irrelevant special characters \u2022 Removing double qoutes for acronyms \u2022 Removing Stop words (but save the original\ntext separately) \u2022 Tokenization - for specific features that are\ntoken-based. \u2022 Stammer - for specific features that are stem-\nbases. \u2022 Lemmatizer - for specific features that are\nlemma-based.\nIn addition we used YAP to extract the following information to later use for feature extraction:\n\u2022 POS tagging \u2022 Dependencies parts extractions \u2022 Grammatical person \u2022 Verbs tense"
        },
        {
            "heading": "A.2 Linguistic features extraction",
            "text": "We developed a set of hand-crafted linguistic features based on previous studies that have indicated a correlation between these features and the psychological state of claimants. This psychological state can be used to infer the truthfulness of a claimant statement, as it is reflected in the language of the claim. The features are divided into the following categories:\n\u2022 Lexical features \u2013 word based \u2013 Bow tf-idf \u2013 Complexity measure \u2013 Readability measure \u2013 Average word length \u2013 Count of H\u2019 hayedia \u2013 Count of distinct words \u2013 Count of words \u2013 Count of syllables \u2013 Usage of uncertainty words \u2013 First names usage\n\u2013 Last names usage \u2022 Lexical features \u2013 char based\n\u2013 Total number of chars \u2013 Digits percentage \u2013 Letters percentage \u2013 Bag of chars tf-idf\n\u2022 Syntactic features \u2013 Count of punctuation marks \u2013 Count of special chars including\n@$%\u02c6<> \u2013 Count per dependency parts : depen-\ndencies between words (sub-obj for instance) \u2013 Count per POS \u2013 Count per Person(first,second,third) in a\nstatement. \u2013 Count per verb tense (BEINONI, PAST,\nFUTURE, IMPERATIVE). \u2022 Structural features\n\u2013 Number of inner quoutes \u2013 Hashtags count\n\u2022 Semantic features \u2013 Positive, negative, and objective seman-\ntic based on Bert based model built on tagged tweets (Twitter).\nWe extracted these features on both claim and context."
        },
        {
            "heading": "A.3 Strong Linguistic Baseline - Features Analysis",
            "text": "We conducted a feature importance analysis using Shapley values on our best XGBoost model after optimization using the nested cross validation approach. The key finding are as follows. First, false statements tend to make greater use of relative clauses and punctuation marks, particularly commas. Second, true statements tend to have a more positive sentiment and make greater use of complement clauses and inseparable prepositions. The use of definite markers is more prevalent in false statements. Next, true statements tend to make use of words with greater complexity as measured by the Flesch-Kincaid index (Flesch, 1965). In addition, the overall statement length was not found to be a significant factor in determining truthfulness."
        },
        {
            "heading": "A.4 Metadata features",
            "text": "The dataset we curated from \"The Whistle\" contains in addition to the claim additional metadata features. These features can be divided to semantic\nfeatures and objective features. The objective features are: date, speaker, role, party, media, knesset and gender. The semantic features are: subject, title, field, and tags.\nMetadata preprocessing One of the \"metadata features\" is \u2019media\u2019. The \u2019media\u2019 feature describes the platform in which the claim was produced. The way \"The Whistle\" journalists described the platform is in the most granular level. For example, a claim produced in a radio station called \"GLTZ\" in a radio program called \"Nahon Le-Haboker\", was described in the \u2019media\u2019 feature as \"Nahon LeHaboker GLTZ\". We hypothesis that adding the type of media, the media source and the producer type(spoken/written) will improve the accuracy of our predictions. This is because different types of media can have different effects on people. For example, a video on YouTube may have a different effect than a news article on a website. Additionally, different media sources can have different biases, which may influence how an individual interprets and reacts to a piece of media."
        },
        {
            "heading": "A.5 Hyperparameter Optimization",
            "text": "As stated in the training setup section, we employed nested cross validation to evaluate the performance of our models and to choose the best hyperparameters. For the hand-crafted feature model , the metadata models and the integrated (metadata + hand-crafted features) models. We used XGBoost as the classification model and we searched among the following parameters: we searched booster in dart ,gbtree ,gblinear and chose gbtree. We searched lambda in 1e-8, 1.0 and chose 6.421 . We searched alpha in 1e-8, 1.0 and chose 0.0008 . We searched min_child_weight in 1-10 and chose 6. We searched subsample in 0.01 - 1.0 and chose 0.385. We searched colsample_bytree in 0.01 - 1.0 and chose 0.4487. We searched max_depth in [1-9] and chose 5. We searched eta in [1e-8 - 1.0] and chose 0.7542. We searched gamma in [1e-8 - 1.0] and chose 1.005 . We searched grow_policy in \"depthwise\", \"lossguide\" and chose lossguide. Also we implemented feature selection using sklearn SelectKBest method. We search k in [100- 8000] and chose 2127. Also, we examine the use of up-sampling method to overcome the unbalance of our data. We chose between upsampling the True score statements, the True score statements + Mostly true statements or upsampling all to count"
        },
        {
            "heading": "Label # Claims",
            "text": "as the majority class. We finally chose to upsample only the True score statements. We chose if to use the original statement with or without stopwords, and chose without. For the full context as input we chose with stopwords(original) as it performed the best.\nFor the Deep learning models, we experiment with two variations: classification approach using softmax in the final layer and ordinal regression approach using the method that was introduced by (Cheng et al., 2008) leveraging the ordinal characteristic of our labels.\nFor the RNN model we examined both word2vec and fastText and we chose fastText. Also, we compare the performance of GRU vs LSTM and we chose GRU. The architecture chose is: Dropout 0.5 after the embedding layer. than GRU layer with dim=8. Than dropout of 0.2 and sigmoid as activation function for the final layer (for ordinal model) and softmax (for classification model). The batch size chosen is 32 and number of epochs 10. We also used the same upsampling method desribed above. Here, we also considered the use of stopwords, and the decision was the same as above.\nFor the AlephBERT model we searched the learning rate in 1e-6, 5e-5, 1e-5 and chose with 1e-4. We experiment two architectures one with two fully connected layers on top of pooled output of the pre-train AlephBERT and the other is one fully connected layer. Both with dropout layer and ReLU as activation function. We finally chose : dropout 0.5 (searched in [0-0.5] , and one fully connected layer.We searched the batch size in 16, 32, 64 and chose 8. To optimize performance we used gradient accumulation for a total batch size of 16. We searched weight decay in 0, 0.1 and chose 1e-4. We also used the same upsampling method desribed above. Here, we also considered the use of stopwords, and the decision was the same as above."
        },
        {
            "heading": "A.6 HeTrue Dataset Features and Definitions",
            "text": "Dataset label distribution can be found in table table 3 Feature list of HeTrue dataset described below.\nJournalist-edited claim The claim as appeared on \u201cThe Whistle\u201d website. Sometimes the claim is being transformed by the website editors for various reasons. Original claim The claim as produced by the claimant and appeared in the original media. Claim Full context The original claim and the surrounding sentences, up to two sentences before and after the claim. Claim date The claim\u2019s producing date. Claimant The person full name who made the claim. Gender Male/Female. Role The claimant\u2019s role. Party The claimant\u2019s political party. Field General subject area or discipline that the claim relates to. Subject Specific topic that the claim is about. Title A brief, descriptive phrase that summarizes the main idea of the claim. Media The media in which a claim is produced. Knesset# The Knesset number in which the claimant is part of. Tags Added by \u201cThe Whistle\u201d journalists to label and categorize the claims. Label \u201cTruth score\u201d given by the \u201cThe Whistle\u201d\njournalists."
        },
        {
            "heading": "A.7 Comparison of Credibility Assessment Datasets",
            "text": "See Table 4 for a comprehensive comparison of credibility assessment datasets, focusing on sentence-level claims. For each dataset, we present the number of instances, the language, the number of labels and the source of the dataset. Additionally, we highlight the comprehensiveness of each dataset in terms of additional features: semantic annotation, metadata features, and context. Semantic annotations pertain to features extracted by professional journalists. Metadata features cover attributes related to the claim and claimant. Context encompasses surrounding text relevant to the statement. HeTrue stands out as the sole dataset integrating all three of these additional features."
        },
        {
            "heading": "A.8 Dataset Example",
            "text": "An example from the HeTrue dataset can be found in Table 5 below."
        }
    ],
    "title": "The Truth, The Whole Truth, and Nothing but the Truth: A New Benchmark Dataset for Hebrew Text Credibility Assessment",
    "year": 2023
}