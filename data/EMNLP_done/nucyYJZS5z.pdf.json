{
    "abstractText": "The practice of transferring knowledge from a sophisticated, proprietary large language model (LLM) to a compact, open-source LLM has garnered considerable attention. Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those of the teacher models to a set of instructions. Nevertheless, they overlooked the possibility of incorporating any \u201cfeedback\u201d\u2014identifying challenging instructions where the student model\u2019s performance falls short\u2014to boost the student model\u2019s proficiency iteratively. To this end, we propose a novel adversarial distillation framework for a more efficient knowledge transfer. Leveraging the versatile role adaptability of LLMs, we prompt the teacher model to identify \u201chard\u201d instructions and generate new \u201chard\u201d instructions for the student model, creating a three-stage adversarial loop of imitation, discrimination, and generation. By applying this adversarial framework, we successfully transfer knowledge from ChatGPT to a student model (named Lion), using a mere 70k training data. Our results show that Lion-13B not only achieves comparable open-ended generation capabilities to ChatGPT but surpasses conventional state-of-the-art (SOTA) instruction-tuned models like Vicuna13B by 55.4% in challenging zero-shot reasoning benchmarks such as BIG-Bench Hard (BBH) and 16.7% on AGIEval.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuxin Jiang"
        },
        {
            "affiliations": [],
            "name": "Chunkit Chan"
        },
        {
            "affiliations": [],
            "name": "Mingyang Chen"
        },
        {
            "affiliations": [],
            "name": "Wei Wang"
        }
    ],
    "id": "SP:1170317a7c9118035dedc34c170b7173b5442e6e",
    "references": [
        {
            "authors": [
                "Sravanti Addepalli",
                "Gaurav Kumar Nayak",
                "Anirban Chakraborty",
                "Venkatesh Babu Radhakrishnan."
            ],
            "title": "Degan: Data-enriching gan for retrieving representative samples from a trained classifier",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelli-",
            "year": 2020
        },
        {
            "authors": [
                "Yejin Bang",
                "Samuel Cahyawijaya",
                "Nayeon Lee",
                "Wenliang Dai",
                "Dan Su",
                "Bryan Wilie",
                "Holy Lovenia",
                "Ziwei Ji",
                "Tiezheng Yu",
                "Willy Chung",
                "Quyet V. Do",
                "Yan Xu",
                "Pascale Fung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hal",
            "year": 2023
        },
        {
            "authors": [
                "Chunkit Chan",
                "Tsz Ho Chan."
            ],
            "title": "Discourse-aware prompt for argument impact classification",
            "venue": "Proceedings of the 15th International Conference on Machine Learning and Computing, ICMLC 2023, Zhuhai, China, February 17-20, 2023, pages 165\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Chunkit Chan",
                "Jiayang Cheng",
                "Weiqi Wang",
                "Yuxin Jiang",
                "Tianqing Fang",
                "Xin Liu",
                "Yangqiu Song."
            ],
            "title": "Chatgpt evaluation on sentence level relations: A focus on temporal, causal, and discourse relations",
            "venue": "CoRR, abs/2304.14827.",
            "year": 2023
        },
        {
            "authors": [
                "Chunkit Chan",
                "Xin Liu",
                "Tsz Ho Chan",
                "Jiayang Cheng",
                "Yangqiu Song",
                "Ginny Y. Wong",
                "Simon See."
            ],
            "title": "Self-consistent narrative prompts on abductive natural language inference",
            "venue": "CoRR, abs/2309.08303.",
            "year": 2023
        },
        {
            "authors": [
                "Chunkit Chan",
                "Xin Liu",
                "Jiayang Cheng",
                "Zihan Li",
                "Yangqiu Song",
                "Ginny Y. Wong",
                "Simon See."
            ],
            "title": "Discoprompt: Path prediction prompt tuning for implicit discourse relation recognition",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Akshay Chawla",
                "Hongxu Yin",
                "Pavlo Molchanov",
                "Jose Alvarez."
            ],
            "title": "Data-free knowledge distillation for object detection",
            "venue": "Proceedings of the IEEE/CVF",
            "year": 2021
        },
        {
            "authors": [
                "Yen-Chun Chen",
                "Zhe Gan",
                "Yu Cheng",
                "Jingzhou Liu",
                "Jingjing Liu."
            ],
            "title": "Distilling knowledge learned in bert for text generation",
            "venue": "arXiv preprint arXiv:1911.03829.",
            "year": 2019
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Gongfan Fang",
                "Kanya Mo",
                "Xinchao Wang",
                "Jie Song",
                "Shitao Bei",
                "Haofei Zhang",
                "Mingli Song."
            ],
            "title": "Up to 100x faster data-free knowledge distillation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6597\u20136604.",
            "year": 2022
        },
        {
            "authors": [
                "Gongfan Fang",
                "Jie Song",
                "Chengchao Shen",
                "Xinchao Wang",
                "Da Chen",
                "Mingli Song."
            ],
            "title": "Data-free adversarial distillation",
            "venue": "CoRR, abs/1912.11006.",
            "year": 2019
        },
        {
            "authors": [
                "Arnav Gudibande",
                "Eric Wallace",
                "Charlie Snell",
                "Xinyang Geng",
                "Hao Liu",
                "Pieter Abbeel",
                "Sergey Levine",
                "Dawn Song."
            ],
            "title": "The false promise of imitating proprietary llms",
            "venue": "CoRR, abs/2305.15717.",
            "year": 2023
        },
        {
            "authors": [
                "Byeongho Heo",
                "Minsik Lee",
                "Sangdoo Yun",
                "Jin Young Choi."
            ],
            "title": "Knowledge distillation with adversarial samples supporting decision boundary",
            "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Ap-",
            "year": 2019
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Yuxin Jiang",
                "Linhan Zhang",
                "Wei Wang."
            ],
            "title": "Improved universal sentence embeddings with promptbased contrastive learning and energy-based learning",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Cheng Jiayang",
                "Lin Qiu",
                "Tsz Ho Chan",
                "Tianqing Fang",
                "Weiqi Wang",
                "Chunkit Chan",
                "Dongyu Ru",
                "Qipeng Guo",
                "Hongming Zhang",
                "Yangqiu Song",
                "Yue Zhang",
                "Zheng Zhang"
            ],
            "title": "Storyanalogy: Deriving story-level analogies from large language",
            "year": 2023
        },
        {
            "authors": [
                "Sanjay Kariyappa",
                "Atul Prakash",
                "Moinuddin K Qureshi."
            ],
            "title": "Maze: Data-free model stealing attack using zeroth-order gradient estimation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13814\u201313823.",
            "year": 2021
        },
        {
            "authors": [
                "Haoran Li",
                "Yulin Chen",
                "Jinglong Luo",
                "Yan Kang",
                "Xiaojin Zhang",
                "Qi Hu",
                "Chunkit Chan",
                "Yangqiu Song"
            ],
            "title": "Privacy in large language models: Attacks, defenses and future directions",
            "year": 2023
        },
        {
            "authors": [
                "Haoran Li",
                "Dadi Guo",
                "Wei Fan",
                "Mingshi Xu",
                "Yangqiu Song."
            ],
            "title": "Multi-step jailbreaking privacy attacks on chatgpt",
            "venue": "CoRR, abs/2304.05197.",
            "year": 2023
        },
        {
            "authors": [
                "Paul Micaelli",
                "Amos J. Storkey."
            ],
            "title": "Zero-shot knowledge transfer via adversarial belief matching",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December",
            "year": 2019
        },
        {
            "authors": [
                "Paul Micaelli",
                "Amos J Storkey."
            ],
            "title": "Zero-shot knowledge transfer via adversarial belief matching",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Hannaneh Hajishirzi"
            ],
            "title": "Cross-task generalization via natural language crowdsourcing instructions",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Subhabrata Mukherjee",
                "Arindam Mitra",
                "Ganesh Jawahar",
                "Sahaj Agarwal",
                "Hamid Palangi",
                "Ahmed Hassan Awadallah."
            ],
            "title": "Orca: Progressive learning from complex explanation traces of GPT-4",
            "venue": "CoRR, abs/2306.02707.",
            "year": 2023
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "CoRR, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "TB OpenAI."
            ],
            "title": "Chatgpt: Optimizing language models for dialogue",
            "venue": "OpenAI.",
            "year": 2022
        },
        {
            "authors": [
                "Tribhuvanesh Orekondy",
                "Bernt Schiele",
                "Mario Fritz."
            ],
            "title": "Knockoff nets: Stealing functionality of blackbox models",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4954\u20134963.",
            "year": 2019
        },
        {
            "authors": [
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "CoRR, abs/2203.02155.",
            "year": 2022
        },
        {
            "authors": [
                "Nicolas Papernot",
                "Patrick McDaniel",
                "Ian Goodfellow",
                "Somesh Jha",
                "Z Berkay Celik",
                "Ananthram Swami."
            ],
            "title": "Practical black-box attacks against machine learning",
            "venue": "Proceedings of the 2017 ACM on Asia conference on computer and communications secu-",
            "year": 2017
        },
        {
            "authors": [
                "Ilija Radosavovic",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Georgia Gkioxari",
                "Kaiming He."
            ],
            "title": "Data distillation: Towards omni-supervised learning",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4119\u20134128.",
            "year": 2018
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "drea Santilli",
                "Thibault F\u00e9vry",
                "Jason Alan Fries",
                "Ryan Teehan",
                "Teven Le Scao",
                "Stella Biderman",
                "Leo Gao",
                "Thomas Wolf",
                "Alexander M. Rush"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "year": 2022
        },
        {
            "authors": [
                "Aarohi Srivastava",
                "Abhinav Rastogi",
                "Abhishek Rao",
                "Abu Awal Md Shoeb",
                "Abubakar Abid",
                "Adam Fisch",
                "Adam R Brown",
                "Adam Santoro",
                "Aditya Gupta",
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating",
            "year": 2022
        },
        {
            "authors": [
                "Mirac Suzgun",
                "Nathan Scales",
                "Nathanael Sch\u00e4rli",
                "Sebastian Gehrmann",
                "Yi Tay",
                "Hyung Won Chung",
                "Aakanksha Chowdhery",
                "Quoc V. Le",
                "Ed H. Chi",
                "Denny Zhou",
                "Jason Wei"
            ],
            "title": "Challenging big-bench tasks and whether chain-of-thought",
            "year": 2022
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Jean-Baptiste Truong",
                "Pratyush Maini",
                "Robert J Walls",
                "Nicolas Papernot."
            ],
            "title": "Data-free model extraction",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4771\u20134780.",
            "year": 2021
        },
        {
            "authors": [
                "Peiyi Wang",
                "Lei Li",
                "Liang Chen",
                "Dawei Zhu",
                "Binghuai Lin",
                "Yunbo Cao",
                "Qi Liu",
                "Tianyu Liu",
                "Zhifang Sui."
            ],
            "title": "Large language models are not fair evaluators",
            "venue": "CoRR, abs/2305.17926.",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "CoRR, abs/2212.10560.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "The Tenth International Conference on Learning Representa-",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "The Tenth International Conference on Learning Representa-",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V. Le",
                "Denny Zhou."
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Can Xu",
                "Qingfeng Sun",
                "Kai Zheng",
                "Xiubo Geng",
                "Pu Zhao",
                "Jiazhan Feng",
                "Chongyang Tao",
                "Daxin Jiang"
            ],
            "title": "Wizardlm: Empowering large language models to follow complex instructions",
            "year": 2023
        },
        {
            "authors": [
                "Hongxu Yin",
                "Pavlo Molchanov",
                "Jose M Alvarez",
                "Zhizhong Li",
                "Arun Mallya",
                "Derek Hoiem",
                "Niraj K Jha",
                "Jan Kautz."
            ],
            "title": "Dreaming to distill: Datafree knowledge transfer via deepinversion",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer",
            "year": 2020
        },
        {
            "authors": [
                "Wanjun Zhong",
                "Ruixiang Cui",
                "Yiduo Guo",
                "Yaobo Liang",
                "Shuai Lu",
                "Yanlin Wang",
                "Amin Saied",
                "Weizhu Chen",
                "Nan Duan."
            ],
            "title": "Agieval: A human-centric benchmark for evaluating foundation models",
            "venue": "CoRR, abs/2304.06364.",
            "year": 2023
        },
        {
            "authors": [
                "\u2022 LLaMA (Touvron"
            ],
            "title": "2023) is a collection of foundation language models ranging from 7B to 65B parameters. It is trained on trillions of tokens from publicly available datasets and is demonstrated to outperform larger-size",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) capable of following natural language instructions have exhibited tremendous success in generalizing zero-shot to new tasks (Mishra et al., 2022; Wei et al., 2022a). Due to various concerns, the most advanced LLMs, such as ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) that boasting billions of parameters, are\n\u2217The two authors have equal contributions. 1Code and model can be found at https://github.com/\nYJiangcm/Lion.\ntypically proprietary, comprising both the model parameter and the training data. To foster increased transparency regarding their intricate operational mechanics, a surge in research efforts focusing on knowledge distillation from a proprietary \u201cteacher\u201d LLM to an open-source \u201cstudent\u201d LLM. This is typically accomplished by aligning the responses of the student model with those of the teacher model to a set of instructions, which can be manually or automatically generated (Wang et al., 2022; Taori et al., 2023; Chiang et al., 2023; Xu et al., 2023).\nHowever, previous works employ a unidirectional approach to knowledge transfer (solid arrow in Figure 1), where the teacher imparts knowledge to the student without considering any \u201cfeedback\u201d.\nTo better illustrate this using a tangible classroom scenario, the \u201cfeedback\u201d refers to identifying the \u201chard\u201d examples or problems where the student\u2019s performance falls short. This feedback guarantees that the teacher can provide bespoke training that centers on \u201chard\u201d examples, thereby paving the way for more effective and tailored learning experiences for the student.\nInspired by adversarial knowledge distillation (AKD), which aims to iteratively improve the student model\u2019s performance by learning from generated hard samples (Fang et al., 2019; Micaelli and Storkey, 2019a; Heo et al., 2019), we propose an adversarial framework for distilling a proprietary LLM into a compact student model. Nevertheless, these AKD methodologies necessitate accessibility to the weights or gradients of the teacher model, which cannot be directly adapted to our setting. To circumvent this problem, we leverage the unparalleled role adaptability of LLMs, which can be effectively employed through a diverse range of prompts (Sanh et al., 2022). In particular, we prompt the proprietary teacher LLM to serve as a \u201creferee\u201d to discriminate hard instructions where there exists a significant performance discrepancy between the teacher\u2019s and student\u2019s responses, and serve as a \u201cgenerator\u201d to produce new instructions that emulate the data distributions corresponding to the discriminated hard instructions. Our framework, as depicted in Figure 2, consists of three stages in an iteration: 1) an imitation stage to align the student\u2019s response with the teacher\u2019s response; 2) a discrimination stage to identify hard instructions; 3) A generation stage to produce new hard instructions for escalating the challenges presented to the student model. In essence, our adversarial framework forms a positive feedback loop that efficiently bootstraps the student model\u2019s proficiency.\nTo verify the efficiency and efficacy of our method, we apply our AKD framework to transfer the knowledge of ChatGPT 2 onto an open-source foundation LLM, known as LLaMA (Touvron et al., 2023). We select Alpaca\u2019s training data (generated from only 175 manually selected seed instructions) as the initial training instructions and execute three iterations of AKD, resulting in a total of 70K data that our model is trained on. We\u2019ve christened our model as Lion, drawing inspiration from the art of \u201cdistillation\u201d. By conducting extensive exper-\n2We access ChatGPT using the OpenAI API (gpt-3.5-turbo model).\niments on open-ended generation and reasoning datasets, which include a total of 40 sub-tasks, our Lion-13B showcases superior performance surpassing instruction-tuned baseline models such as Vicuna (Chiang et al., 2023). Our main contributions are as follows:\n\u2022 Our work is the first attempt to adopt the idea of adversarial knowledge distillation to large language models.\n\u2022 Our proposed framework demonstrates impressive efficiency and efficacy. With instruction tuning performed on 70k data without any human annotation, our Lion-13B approximates ChatGPT\u2019s capabilities on open-ended generation dataset and largely outperforms the current SOTA model Vicuna-13B on reasoning tasks.\n\u2022 The versatility of our framework allows for broad application: it is not exclusive to ChatGPT but can be conveniently adapted to suit a variety of other proprietary LLMs."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Instruction-Following Language Models",
            "text": "With the impressive ability of instruction-following large language models such as ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023), the techniques of instruction tuning (Wei et al., 2022b) have attracted a lot of attention (Wei et al., 2022c; Bubeck et al., 2023; Bang et al., 2023; Chan et al., 2023a). The early research of instruction tuning aims to enhance the generalization ability of language models, allowing these models to perform new tasks by comprehending task descriptions without relying on a few examplars. By fine-tuning these instruction-following language models (e.g., T5 (Raffel et al., 2020), FLAN (Aribandi et al., 2022), T0 (Sanh et al., 2022), and ExT5 (Aribandi et al., 2022)) on multi-task datasets in the form of natural language phrased as instructions, these models have been shown to perform well on unseen tasks with the instructions.\nHowever, these models are only fine-tuned on simple task-specific instructions, and it is challenging to comprehend the sophisticated and diverse intent of users in real-world scenarios. Therefore, InstructGPT (Wei et al., 2022b), ChatGPT (OpenAI, 2022), and GPT-4 (OpenAI, 2023) trained on the diverse forms and abundant task types of\nhuman-crafted instructions annotated by a considerable number of annotators. Since these instructions were not open-sourced, recent works such as Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), and WizardLM (Xu et al., 2023) investigate how to generate high-quality instructions and fine-tune the open-source large language model LLaMA (Touvron et al., 2023) with them to approach the performance of ChatGPT."
        },
        {
            "heading": "2.2 Knowledge Distillation",
            "text": "Knowledge Distillation (KD) (Hinton et al., 2015; Radosavovic et al., 2018; Chen et al., 2019) represents a crucial strategy within the sphere of model compression and acceleration, wherein a compact student model is instructed to emulate the performance traits of a more cumbersome teacher model. In practical contexts, the availability of training data is often constrained due to concerns regarding privacy, legality, security, or confidentiality. To address the absence of training data, data-free KD methods were proposed to align the student model to the teacher model, capitalizing on either related proxy data (Orekondy et al., 2019; Papernot et al., 2017) or synthetic data generated by learnable generators (e.g., Generative Adversarial Network (GAN)) (Addepalli et al., 2020; Fang et al., 2019; Micaelli and Storkey, 2019b) or teacher model inversions (Yin et al., 2020; Chawla et al., 2021; Fang et al., 2022). Nevertheless, these KD methodologies necessitate the accessibility to the weights or gradients of the teacher model. Consequently, an alternative line of research, commonly denoted as data-free model extraction (or stealing), endeavors to bridge this gap by employing zero-order estimation methodologies to approximate the authentic gradients of the teacher model to guide the update of the optimized generators (Kariyappa et al., 2021; Truong et al., 2021). However, adapting these methods to our distillation task presents two main hurdles. First, these techniques are primarily designed for image-based classification tasks, assuming access to a continuous softmax vector from the teacher model. Estimating zero-order gradients becomes problematic in our case, as responses are typically sequence-oriented. Second, developing an effective instruction generator capable of producing diverse, high-quality instructions that mirror the teacher model\u2019s training data distribution proves more challenging than in the image domain."
        },
        {
            "heading": "3 Methodology",
            "text": "Harnessing the learned knowledge of a sophisticated teacher model T (x; \u03b8T ) where the parameter \u03b8T is inaccessible, our goal is to craft a more lightweight student model S(x; \u03b8S). Ideally, a student model is optimal if the expectation of model discrepancy (which indicates the prediction differences between teacher T and student S) on the uniform data distribution is minimized. Inspired by the success of adversarial knowledge distillation (AKD) (Fang et al., 2019; Micaelli and Storkey, 2019a; Heo et al., 2019), we turn to optimize an upper bound of the expectation \u2014the expectation of the model discrepancy on \u201chard samples\u201d, where the teacher T and the student S have a relatively large performance gap. These \u201chard samples\u201d are inclined to dominate the expectation of the model discrepancy. Thus, the overall expected model discrepancy can be effectively and efficiently reduced by optimizing the student model S on these \u201chard samples\u201d. The underlying rationale is rather straightforward and can be analogized to a realworld educational scenario: continuously concentrating on the \u201chard\u201d knowledge that the student finds challenging to grasp is the most effective manner of enhancing a student\u2019s proficiency.\nHowever, in the process of training the student model S, hard samples will be mastered by the student and converted into easy samples. Hence we need a mechanism to continuously generate hard samples, which can be achieved by an adversarial framework.\nThe whole framework of our Adversarial Knowledge Distillation is depicted in Figure 2, which contains three stages in an iteration: 1) an imitation stage to align the student\u2019s response with the teacher\u2019s response; 2) a discrimination stage to identify hard samples; 3) A generation stage to produce new hard samples for escalating the challenges presented to the student model."
        },
        {
            "heading": "3.1 Initilization",
            "text": "As shown in Figure 2, four roles and two data pools are established in our framework, and we will comprehensively illustrate their functions later. We initialize our student model S using a foundation LLM such as LLaMA (Touvron et al., 2023). We initialize our teacher model T , referee R, and generator G by using the same proprietary LLM such as ChatGPT (OpenAI, 2022). The multiple roles that this proprietary LLM serves are accomplished\nthrough the use of varied prompt templates. We start the iteration from a given initial Train Pool XA = {xAi }i\u2208[1,NA], where xAi is the i-th instruction in XA, and NA is the number of samples in XA. The Cache Pool XB is initialized as identical to XA, consisting of instructions to evaluate the performance of S and T ."
        },
        {
            "heading": "3.2 Imitation Stage",
            "text": "To impart the knowledge of the teacher to the student, we construct the instruction-response data {xAi , T (xAi )}i\u2208[1,NA] by forward propagating instructions in the Train Pool XA through the teacher T . The prompt template used for model inference is shown in Table 10. Like the imitation training of previous work (Taori et al., 2023; Chiang et al., 2023), we fine-tune our student model S to align the response of the teacher model, by optimizing the autoregressive language modeling objective."
        },
        {
            "heading": "3.3 Discrimination Stage",
            "text": "Figure 2 demonstrates that the discrimination stage starts from the Cache Pool, denoted as XB . Even though this pool begins with the same initialization as the Train Pool, their uses diverge. The Train Pool is rejuvenated by replacing its existing instructions with freshly generated instructions, whereas the Cache Pool is enriched by incorporating these generated instructions. As a result, the growing storage capacity of the Cache Pool provides a more extensive space for evaluating the performance gap between teacher T and student S. This allows for\nmore thorough detection of hard instructions. In the discrimination stage, we ask the proprietary LLM to serve as a \u201creferee\u201d, which quantifies the performance gap between T and S. Specifically, we feed each instruction xBi in the Cache Pool XB through both the teacher T and student S to generate the outputs T (xBi ) and S(xBi ), respectively. Then we ask the referee R to quantitatively measure the quality difference between teacher\u2019s response T (xBi ) and student\u2019s response S(xBi ), conditioned on xBi :\ndi = R(T (xBi ),S(xBi ) | xBi ) (1)\nThe above process is conducted by using the prompt template (as shown in Table 11) inspired by (Chiang et al., 2023), which requires the LLM to consider the helpfulness, relevance, accuracy, and level of detail of two responses and output two scores. To mitigate the positional bias (Wang et al., 2023) of the LLM referee, we conduct two runs by exchanging the positions of the teacher\u2019s response and the student\u2019s response and compute the final score as the average of the two runs. Then di is calculated as the difference between the teacher\u2019s score and the student\u2019s score. By setting a threshold \u03c4 (1.0 used in our experiments), we discriminate hard instructions as those instructions with di \u2265 \u03c4 , and the others are identified as easy ones. Figure 3b provides a clear and intuitive demonstration of which kinds of instructions are discriminated as hard in the first iteration. Compared with the instructions in the Cache Pool (Figure 3a), the dis-\ntribution of the identified hard instructions is quite different, focusing more on complex tasks such as math, coding, etc."
        },
        {
            "heading": "3.4 Generation Stage",
            "text": "After carefully discerning the hard instructions, the generation stage aims to produce samples that mirror the data distributions corresponding to these challenging directives. This process is achieved by employing the proprietary LLM as a generator, denoted as G, leveraging its exceptional prowess in content creation. Inspired by (Xu et al., 2023), we randomly sample an instruction from the hard instructions and prompt the generator G to generate a new instruction. The newly generated instruction is required to pertain to the same domain and match the task type of the sampled instruction. The template utilized for this prompt is exhibited in Table 12. As shown in Figure 3c, the distribution of the newly generated hard instructions appears to be comparable to that of the previously identified hard instructions. To mitigate the issue of catastrophic forgetting and to augment the diversity of the generated instructions, we also randomly sample an instruction from the easy instructions and prompt the generator G to generate a new instruction that belongs to the same domain as the sampled one, but exhibit a more long-tailed distribution. The template we use to prompt this process is displayed in Table 13.\nIn each iteration, we define N as the total count of newly generated instructions and maintain a 1:1 ratio r between the generated hard instructions and the generated easy instructions. To promote diversity, a new instruction will be deemed valid only if its ROUGE-L overlap with any existing instructions in the Cache Pool is below 0.7. Finally, as\naforementioned in Section 3.3, we proceed to rejuvenate the Train Pool, replacing its existing instructions with freshly generated ones. Concurrently, we enrich the Cache Pool by incorporating these newly generated instructions."
        },
        {
            "heading": "3.5 Min-Max Game Interpretation",
            "text": "Our adversarial knowledge distillation framework can be interpreted as a dynamic min-max game: in the imitation stage, we fine-tune our student to minimize the model discrepancy between itself and the teacher on hard samples; in the discrimination and generation stage, we craft new hard samples to maximize the model discrepancy, based on the learning progress of the student model. This dialectic framework propels the student model towards uncovering otherwise hidden knowledge, paving the way to complete understanding. As the training progresses through several iterations, the system should ideally achieve equilibrium. This is the point where the student model has mastered all the hard samples and the referee R can no longer distinguish between the student S and teacher T models. At this juncture, S becomes functionally indistinguishable from T ."
        },
        {
            "heading": "4 Experiments Setting",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "In our experiments, we implemented a comprehensive LLM evaluation protocol that considers a diverse range of abilities, such as writing, coding, commonsense, math, and logical reasoning. The datasets we utilized can be classified into two main categories: open-ended generation and reasoning."
        },
        {
            "heading": "4.1.1 Open-ended Generation Datasets",
            "text": "Vicuna-Instructions (Chiang et al., 2023) is a set of 80 questions spanning 9 distinct task categories. This dataset has gained extensive usage in evaluating the capabilities of LLMs. Within our work, we examine LLMs\u2019 performance on this dataset in two different settings:\n\u2022 Setting1: Following Vicuna (Chiang et al., 2023), we leverage GPT-4 to automatically assess the quality of responses (rated on a scale of 1 to 10) between a reference model (ChatGPT) and a candidate model. Subsequently, we calculate the candidate model\u2019s performance as the percentage of the total score it achieves compared to the reference model.\n\u2022 Setting2: A recent work (Wang et al., 2023) pointed out that a systematic bias may exist in the above-mentioned GPT-4 automatic evaluation. To mitigate this, they propose two strategies, namely Multiple Evidence Calibration and Balanced Position Calibration, to obtain closer alignment with human judgments."
        },
        {
            "heading": "4.1.2 Reasoning Datasets",
            "text": "AGIEval (Zhong et al., 2023) is a well-known benchmark that quantifies the reasoning capability of foundation models in the context of humancentric standardized exams, including college entrance exams, math competitions, lawyer qualification tests, etc. We choose all English multiplechoice questions (8 tasks, 2,546 samples) among AGIEval for our experiments. The data statistics are shown in Table 6.\nBIG-Bench Hard (BBH) (Suzgun et al., 2022) consists of a suite of challenging tasks from BIGBench (Srivastava et al., 2022), designed to assess the capabilities and limitations of large language models. These are the tasks on which prior language models underperform the average human rater. We choose all tasks that can be formatted into multiple-choice questions (23 tasks, 5,511 samples) among BBH for our experiments. The data statistics are shown in Table 7.\nSetting We evaluate reasoning capabilities under a zero-shot setting without any exemplars and without Chain-of-Thought (CoT). For both AGIEval and BBH, we use the prompt format and parsing following (Zhong et al., 2023; Mukherjee et al.,\n2023). Given the free-form response from the generative models, only the first capital character in the response is considered to compare with the gold answer (exact match). The result we report is accuracy (%)."
        },
        {
            "heading": "4.2 Baselines",
            "text": "We select five superior LLMs as baselines, including LLaMA (Touvron et al., 2023), Alpaca (Taori et al., 2023), WizardLM (Xu et al., 2023), Vicuna (Chiang et al., 2023), and ChatGPT (OpenAI, 2022). It is worth noting that Vicuna has consistently ranked as the top open-source language model on multiple leaderboards, such as Chatbot Arena3. Therefore, we will conduct a comprehensive comparison with Vicuna. See detailed descriptions of these baselines in Appendix B."
        },
        {
            "heading": "4.3 Implementation Details",
            "text": "Training Details Our student model is initialized using the pre-trained LLaMA. The Train Pool and Cache Pool are initialized with the 52K automatically generated instructions from Alpaca (Taori et al., 2023). The total number of iterations is set to 3, with 6K newly generated instructions added at each iteration. This results in a total of 70K data that our model is trained on in order to make a fair comparison with current SOTA baselines, including WizardLM and Vicuna. The training hyperparameters are listed in Appendix C.\nInference Details To draw inferences from Lion and ChatGPT, we calibrated the temperature to 0.7 and set the maximum generation length at 1024. All other parameters adhere to their default settings. For LLaMA, Alpaca, WizardLM, and Vicuna, we configured their inference parameters in line with the specifications given in their respective original papers. When engaging with the gpt-3.5-turbo API for various roles, we employ an array of hyperparameters, the specifics of which can be located in Appendix C."
        },
        {
            "heading": "5 Experimental Results",
            "text": ""
        },
        {
            "heading": "5.1 Results for Open-ended Generation",
            "text": "Table 1 shows the performance comparison of various models against ChatGPT as the reference model, where GPT-4 is used as a referee/rater. Our Lion-7B and Lion-13B remarkably outperform their counterparts under two evaluation settings.\n3https://chat.lmsys.org/?arena\nNoticeably, Lion-13B shows an 8-point improvement over Vicuna-13B on aggregate, achieving 98.38% capabilities of ChatGPT.\nTo comprehensively compare with other baseline models on the capability to generate high-quality responses on various types of instruction, the relative response quality (Setting2) among different task categories is depicted in Figure 4. Our model impressively and slightly surpasses ChatGPT in the generic, knowledge, common-sense, and counterfactual task categories. Furthermore, for the two difficulty task categories described in the previous study (Chiang et al., 2023; Xu et al., 2023), our model significantly outperforms other baseline models with at least 32.32% relative score in the math task category while exceeding most of the baseline in the coding generation task category."
        },
        {
            "heading": "5.2 Results for Reasoning",
            "text": "AGIEval Results Table 2 presents the standard zero-shot performance comparison between Lion and baseline models on the AGIEval benchmark for multiple-choice English questions. Lion demonstrates significantly stronger performance compared to Vicuna, surpassing it in most task cate-\ngories and achieving an average relative improvement of over 16%. However, Lion-13B still significantly lags behind ChatGPT, only retaining 72.5% of its reasoning capability.\nBIG-Bench Hard Results Table 3 displays the zero-shot performance comparison between Lion and baseline models on BIG-Bench Hard with standard zero-shot prompting. Similar to AGIEval, Vicuna exhibits poor performance on sophisticated reasoning tasks within this benchmark, while Lion substantially surpasses Vicuna by around 50% on average. Particularly, Lion demonstrates significant performance enhancements of over 100% on tasks involving data understanding, semantic understanding (Disambiguation QA and Snarks), logical and geometric reasoning (Logical Deduction and Geometric Shapes), and position reasoning (Tracking Shuffled Objects). Despite achieving an average ability of nearly 74% compared to ChatGPT on BBH, Lion-13B surpasses ChatGPT in several tasks, including Movie Recommendation, Snarks (identifying sarcastic sentences from two nearly-identical ones), and Tracking Shuffled Objects. This demonstrates the effectiveness of our method."
        },
        {
            "heading": "6 Analyses",
            "text": ""
        },
        {
            "heading": "6.1 Ablation Studies",
            "text": "The threshold \u03c4 for distinguishing between hard and easy instructions We systematically explored \u03c4 ranging from 0.0 to 2.0 and documented its influence on average performance across three datasets. Table 4 reveals an optimal range of \u03c4 between 1.0 and 1.5 for all datasets. Notably, elevating \u03c4 from 0.0 to 1.0 consistently enhances performance across all datasets, indicating effective differentiation between hard and easy instructions. However, a continuous increase from 1.0 to 2.0 gradually degrades performance due to decreased diversity in hard instructions. The ablation results demonstrate that our method is not quite sensitive to a large value of \u03c4 .\nThe ratio r of generated hard and easy instructions We change the ratio of generated hard instructions to generated easy instructions from 1:0 (all hard) to 0:1 (all easy) and investigate its impact on average performance across three datasets. It can be seen from Table 5 that higher ratios of hard to easy instructions generally lead to improved performance, with a balanced ratio of 1:1 yielding the\nhighest average scores."
        },
        {
            "heading": "6.2 The Learning Dynamics of Lion",
            "text": "In Figure 5, we delve into the learning dynamics of Lion by visualizing its performance on AGIEval and BBH throughout the training iterations. The results clearly demonstrate that our adversarial knowledge distillation framework consistently enhances the performance of the student model as the iterations progress. Notably, the most significant improvement in capability occurs in the first iteration, suggesting the usefulness of the identification of challenging example patterns (refer Figure 3b)."
        },
        {
            "heading": "6.3 Case Studies",
            "text": "To clearly compare the generated response quality between our model and other baselines, we provide nine case studies sampled from Vicuna-instruction, AGIEval, and BBH in Appendix E. Table 14 showcases the responses of various models to a math instruction. It can be seen that only Lion and ChatGPT provide the correct answer and follow the correct problem-solving steps. A counterfactual case is shown in Table 15, where ChatGPT provides a relevant answer that considers the potential impacts of Newton focusing on biology instead of physics, but it lacked details and depth. Lion, on\nthe other hand, offered a more detailed and engaging response that explored different possibilities such as the development of biophysics or discovering new principles that could be applied to both fields. Lion\u2019s response also considered the potential implications of Newton\u2019s work on motion, force, gravity, and thermodynamics in biology, providing a more comprehensive answer."
        },
        {
            "heading": "7 Conclusion",
            "text": "This paper presents an innovative adversarial knowledge distillation framework for distilling a proprietary LLM into a compact, open-source student model. While previous methodologies have concentrated on unidirectional knowledge transfer, our approach seeks to integrate \u201cfeedback\u201d into the learning process. Leveraging the versatile role adaptability of LLMs, we prompt the proprietary model to identify \u201chard\u201d instructions and generate new \u201chard\u201d instructions for the student model, creating a three-stage adversarial loop of imitation, discrimination, and generation. This approach al-\nlows us to refine the student model\u2019s performance iteratively, efficiently bootstrapping its proficiency. We aspire that our model, named Lion, may serve as a baseline to reflect the performance of ChatGPT, especially the open-source instruction-following language model baseline for our community.\nLimitations and Discussions\nThe Model Capability We have identified that Lion is subject to certain constraints: 1) A recent study (Gudibande et al., 2023) asserts that \u201cmodel imitation is a false promise\u201d since imitation models are adept at mimicking ChatGPT\u2019s style but fall short in improving LMs across more challenging tasks. While Lion still lags behind its teacher model ChatGPT in handling intricate reasoning tasks (as shown in our experiments), it demonstrates promising improvements compared to previous imitation models. Therefore, our adversarial knowledge distillation framework may provide a more effective way for knowledge transfer. 2) Since our training data doesn\u2019t encompass dialogues, Lion struggles to manage multi-turn conversations. 3) Due to computational resource constraints, Lion\u2019s maximum sequence length is limited to 1024. Consequently, it faces challenges when dealing with long documents. Despite these limitations, we envision Lion serving as an accessible springboard for future research endeavors aimed at addressing these limitations.\nThe Training Process To train a single student model, we request the gpt-3.5-turbo API around 450k times, a number that is roughly 70% of the WizardLM\u2019s usage of 624k (Xu et al., 2023).\nNonetheless, this utilization incurs a considerable expense, nearing $900. In contrast to methods like Alpaca (Taori et al., 2023) and WizardLM (Xu et al., 2023), which only fine-tune the student model once, our adversarial knowledge distillation method employs iterative parametric updates to the student model. While this iterative approach inevitably leads to slower iteration speed, it offers additional benefits. Finally, different from traditional adversarial knowledge distillation where the weights of the generator are iteratively updated, we use a black-box and parameter-frozen LLM (ChatGPT in our paper) to serve the role. Therefore, the quality of the LLM is quite essential in the generation of new instructions.\nThe Evaluation Metrics Though automated evaluations leveraging GPT-4 have showcased promising prospects in appraising chatbot performance, the technique is yet to reach a level of maturity and accuracy, especially considering the propensity of large language models to generate non-existent or \u201challucinated\u201d information. Evaluating the efficacy of LLM across various tasks presents a considerable challenge since different tasks require quite different expertise (Wang et al., 2022). Therefore, the creation of a comprehensive, standardized evaluation system for chatbots is a prevailing research challenge that demands additional exploration and study.\nEthics Statement\nInherited Biases It is important to consider that the behavior of our distilled student models may exhibit potential toxicity, biases, or privacy issues (Li et al., 2023a,b) inherited from the larger teacher LLM. We anticipate that the advancements made in reducing anti-social behaviors in LLMs can also be utilized to enhance student language models.\nLicense and Legality Based on Stanford Alpaca\u2019s guidelines (Taori et al., 2023), we have determined that the weights of Lion will be exclusively licensed for research purposes in the future. Utilizing Lion\u2019s weights alongside LLaMA\u2019s original weights must adhere to Meta\u2019s LLaMA License Agreement. Users are responsible for acquiring and utilizing LLaMA in accordance with the license agreement.\nSafety Unlike ChatGPT (OpenAI, 2022), Lion does not rely on human feedback to mitigate undesired behaviors. Instead, Lion learns to avoid such\nbehaviors by imitating ChatGPT. However, it is important to acknowledge the potential risks associated with using Lion for malicious purposes, especially upon releasing its weights in the future. For future work, we aim to incorporate the technique of Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) to enhance access control. Additionally, Meta has implemented an access application process that can help regulate the distribution of LLaMA models and minimize the potential risks associated with their usage, providing an alternative option."
        },
        {
            "heading": "Acknowledgements",
            "text": "W. Wang was also affiliated with Guangzhou Municipal Key Laboratory of Materials Informatics, The Hong Kong University of Science and Technology (Guangzhou), China. He was supported by HKUST(GZ) Grant G0101000028, GZU-HKUST Joint Research Collaboration Grant GZU22EG04, CCF-HuaweiDBC202302, and Guangzhou Municipal Science and Technology Project (No. 2023A03J0003)."
        },
        {
            "heading": "A Data Statistics",
            "text": "Table 6 and Table 7 show the data statistics of AGIEval and BIG-Bench Hard, respectively."
        },
        {
            "heading": "B Baselines",
            "text": "\u2022 LLaMA (Touvron et al., 2023) is a collection of foundation language models ranging from 7B to 65B parameters. It is trained on trillions of tokens from publicly available datasets and is demonstrated to outperform larger-size LLMs such as GPT-3 (175B) across a multitude of benchmarks. We use the official code from LLaMA 4.\n\u2022 Alpaca (Taori et al., 2023) is a project initiated by Stanford University with the objective of developing and disseminating an opensource model that adeptly follows instructions. It is based on LLaMA and fine-tuned on 52K instruction-following examples generated by\n4https://github.com/facebookresearch/llama\nquerying OpenAI\u2019s text-davinci-003 model. On the self-instruct evaluation set, Alpaca mirrors text-davinci-003, but is notably more compact and cost-effective to reproduce. We use the official code from Alpaca 5.\n\u2022 WizardLM (Xu et al., 2023) employs LLMs instead of humans to automatically massproduce open-domain instructions of various difficulty levels, to improve the performance of LLMs. It uses an Evol-Instruct method to bootstrap the 52k instruction-following examples of Alapca into a larger set of 250k more intricate instructions. Out of this larger set, 70k examples were selected to fine-tune LLaMA. We use WizardLM-7B-V1.0 from the official code 6.\n\u2022 Vicuna (Chiang et al., 2023), a superior opensource chatbot, excels in generating fluid and captivating responses to user queries. It is based on LLaMA and fine-tuned on 70K user-shared conversations collected from ShareGPT, a platform designed for sharing interactions with ChatGPT. Its impressive capabilities make it one of the leading open instruction-following models today. Vicuna achieves competitive performance against proprietary models such as ChatGPT and Bard (Google, 2023). We use Vicuna-7B-V1.1 and Vicuna-13B-V1.1 from FastChat 7.\n\u2022 ChatGPT (OpenAI, 2022), a product of OpenAI, is an advanced AI chatbot renowned for its ability to interact with users in an authentically human and engaging manner. The chatbot is built on powerful LLMs such as GPT3.5 and GPT-4, which are trained on a vast corpus of internet text data. ChatGPT undergoes fine-tuning via both supervised and reinforcement learning techniques, with the human trainers providing necessary feedback and direction.\nC Implementation Details\nTraining Hyperparameters The training process is conducted on 8 A100 GPUs. During each iteration of adversarial knowledge distillation, the hyperparameters for training are shown in Table 8.\n5https://github.com/tatsu-lab/stanford_alpaca 6https://github.com/nlpxucan/WizardLM 7https://github.com/lm-sys/FastChat\nQuerying the gpt-3.5-turbo API We use different sets of hyperparameters when querying the gpt-3.5-turbo API for different roles (Teacher, Referee, Generator). These hyperparameters are found to work well and we listed them in Table 9."
        },
        {
            "heading": "D Prompt Templates for Our Adversarial Distillation Framework",
            "text": "Fine-tuning an LLM (i.e. ChatGPT) is costly and intricate, human-tailored prompt templates are utilized to solve various tasks (Wei et al., 2022d; Chan et al., 2023b,c; Jiang et al., 2022; Jiayang et al., 2023; Chan and Chan, 2023). The prompt template of the Teacher for generating responses is shown in Table 10. The prompt template of the Referee for comparing the quality of two responses generated by two AI assistants is shown in Table 11. The prompt templates of the Generator for generating new hard instructions and new easy instructions are shown in Table 12 and Table 13, respectively."
        },
        {
            "heading": "E Case Studies",
            "text": "Here we show 3 cases in Table 14, 15, and 16 to clearly compare the open-ended generation performance among various models including our Lion13B, LLaMA-13B, Alpaca-13B, Vicuna-13B, and ChatGPT.\nBesides, we show 6 cases in Table 17, 18, 19, 20, 21, and 22 to clearly compare the reasoning capability among various models including our Lion-13B, Vicuna-13B, and ChatGPT. We utilize \u2713 and \u2717 to denote whether the response is correct or incorrect, respectively."
        }
    ],
    "title": "Lion: Adversarial Distillation of Proprietary Large Language Models",
    "year": 2023
}