{
    "abstractText": "The growing capabilities of large language models (LLMs) have inspired recent efforts to integrate LLM-generated dialogue into video games. However, evaluation remains a major challenge: how do we assess the player experience in a commercial game augmented with LLM-generated dialogue? To explore this question, we introduce a dynamic evaluation framework for the dialogue management systems that govern the task-oriented dialogue often found in roleplaying video games. We first extract dialogue from the widely-acclaimed role-playing game Disco Elysium: The Final Cut, which contains 1.1M words of dialogue spread across a complex graph of utterances where node reachability depends on game state (e.g., whether a certain item is held). Using this dataset, we have GPT-4 perform dialogue infilling to generate grounded utterances based on game state represented via code. In a statistically robust study of 28 players recruited from the r/DiscoElysium subreddit, the LLM outputs are evaluated against the game designers\u2019 writing via both preference judgments and freeform feedback using a web interface that recreates the game\u2019s core conversation functionality. Overall, the game designers\u2019 prose is significantly preferred to GPT-4 generations, with participants citing reasons such as improved logical flow and grounding with the game state. To spur more principled future research in this area, we release our web interface and tools to enable researchers to build upon our work.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Nader Akoury"
        },
        {
            "affiliations": [],
            "name": "Qian Yang"
        },
        {
            "affiliations": [],
            "name": "Mohit Iyyer"
        }
    ],
    "id": "SP:cd36c01ece2af2182057dc11b63838a4069d214c",
    "references": [
        {
            "authors": [
                "Mohammad Bavarian",
                "Heewoo Jun",
                "Nikolas Tezak."
            ],
            "title": "Efficient Training of Language Models to Fill in the Middle",
            "venue": "arXiv:2207.14255 [cs], page 30.",
            "year": 2022
        },
        {
            "authors": [
                "Marc G. Bellemare",
                "Yavar Naddaf",
                "Joel Veness",
                "Michael Bowling."
            ],
            "title": "The arcade learning environment: An evaluation platform for general agents",
            "venue": "CoRR, abs/1207.4708.",
            "year": 2012
        },
        {
            "authors": [
                "Hayet Brabra",
                "Marcos B\u00e1ez",
                "Boualem Benatallah",
                "Walid Gaaloul",
                "Sara Bouguelia",
                "Shayan Zamanirad."
            ],
            "title": "Dialogue management in conversational systems: A review of approaches, challenges, and opportunities",
            "venue": "IEEE Transactions on Cognitive and",
            "year": 2022
        },
        {
            "authors": [
                "Chris Callison-Burch",
                "Gaurav Singh Tomar",
                "Lara Martin",
                "Daphne Ippolito",
                "Suma Bailis",
                "David Reitter."
            ],
            "title": "Dungeons and dragons as a dialog challenge for artificial intelligence",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Nat-",
            "year": 2022
        },
        {
            "authors": [
                "Dallas Card",
                "Peter Henderson",
                "Urvashi Khandelwal",
                "Robin Jia",
                "Kyle Mahowald",
                "Dan Jurafsky."
            ],
            "title": "With little power comes great responsibility",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Morikawa",
                "Alec Radford",
                "Matthew Knight",
                "Miles Brundage",
                "Mira Murati",
                "Katie Mayer",
                "Peter Welinder",
                "Bob McGrew",
                "Dario Amodei",
                "Sam McCandlish",
                "Ilya Sutskever",
                "Wojciech Zaremba"
            ],
            "title": "Evaluating Large Language Models Trained on Code",
            "year": 2021
        },
        {
            "authors": [
                "Elizabeth Clark",
                "Tal August",
                "Sofia Serrano",
                "Nikita Haduong",
                "Suchin Gururangan",
                "Noah A. Smith."
            ],
            "title": "All that\u2019s \u2018human\u2019 is not gold: Evaluating human evaluation of generated text",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Elizabeth Clark",
                "Noah A. Smith."
            ],
            "title": "Choose your own adventure: Paired suggestions in collaborative writing for evaluating story generation models",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Philip Cohen."
            ],
            "title": "Foundations of collaborative taskoriented dialogue: What\u2019s in a slot? In Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue, pages 198\u2013209, Stockholm, Sweden",
            "venue": "Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Orianna Demasi",
                "Yu Li",
                "Zhou Yu."
            ],
            "title": "A multipersona chatbot for hotline counselor training",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Chris Donahue",
                "Mina Lee",
                "Percy Liang."
            ],
            "title": "Enabling language models to fill in the blanks",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2492\u2013 2501, Online. Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Kawin Ethayarajh",
                "Dan Jurafsky."
            ],
            "title": "The authenticity gap in human evaluation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6056\u20136070, Abu Dhabi, United Arab Emirates. Association for Com-",
            "year": 2022
        },
        {
            "authors": [
                "Milan Gritta",
                "Gerasimos Lampouras",
                "Ignacio Iacobacci."
            ],
            "title": "Conversation graph: Data augmentation, training, and evaluation for non-deterministic dialogue management",
            "venue": "Transactions of the Association for Computational Linguistics, 9:36\u201352.",
            "year": 2021
        },
        {
            "authors": [
                "Barbara Grosz."
            ],
            "title": "The structure of task oriented dialogs",
            "venue": "IEEE Symposium on Speech Recognition: Contributed Papers. Carnegie Mellon University Computer Science Dept., Pittsburgh, Pennsylvania, volume 10.",
            "year": 1974
        },
        {
            "authors": [
                "Matthew Hausknecht",
                "Prithviraj Ammanabrolu",
                "MarcAlexandre C\u00f4t\u00e9",
                "Xingdi Yuan."
            ],
            "title": "Interactive Fiction Games: A Colossal Adventure",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7903\u20137910.",
            "year": 2020
        },
        {
            "authors": [
                "Marzena Karpinska",
                "Nader Akoury",
                "Mohit Iyyer."
            ],
            "title": "The perils of using Mechanical Turk to evaluate open-ended text generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1265\u20131285, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Marzena Karpinska",
                "Mohit Iyyer"
            ],
            "title": "Large language models effectively leverage document-level context for literary translation, but critical errors persist",
            "year": 2023
        },
        {
            "authors": [
                "Marzena Karpinska",
                "Nishant Raj",
                "Katherine Thai",
                "Yixiao Song",
                "Ankita Gupta",
                "Mohit Iyyer."
            ],
            "title": "DEMETR: Diagnosing evaluation metrics for translation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Michal Kempka",
                "Marek Wydmuch",
                "Grzegorz Runc",
                "Jakub Toczek",
                "Wojciech Ja\u015bkowski."
            ],
            "title": "ViZDoom: A Doom-based AI research platform for visual reinforcement learning",
            "venue": "2016 IEEE Conference on Computational Intelligence and Games (CIG),",
            "year": 2016
        },
        {
            "authors": [
                "Alexander Koller",
                "Timo Baumann",
                "Arne K\u00f6hn."
            ],
            "title": "Dialogos: Simple and extensible dialogue modeling",
            "venue": "Interspeech.",
            "year": 2018
        },
        {
            "authors": [
                "Robert Kurvitz",
                "Helen Hindepere",
                "Argo Tuulik",
                "Cash De Cuir",
                "Olga Moskvina."
            ],
            "title": "Disco Elysium: The Final Cut",
            "venue": "ZA/UM Studios.",
            "year": 2021
        },
        {
            "authors": [
                "Gaetan Lopez Latouche",
                "Laurence Marcotte",
                "Ben Swanson."
            ],
            "title": "Generating video game scripts with style",
            "venue": "Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023), pages 129\u2013 139, Toronto, Canada. Association for Computational",
            "year": 2023
        },
        {
            "authors": [
                "Michael Mateas",
                "Andrew Stern."
            ],
            "title": "Demonstration: The Interactive Drama Fa\u00e7ade",
            "venue": "Proceedings of the First AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE\u201905, pages 153\u2013155, Marina del Rey, California.",
            "year": 2005
        },
        {
            "authors": [
                "David A. Papa",
                "Igor L. Markov."
            ],
            "title": "Hypergraph partitioning and clustering",
            "venue": "Handbook of Approximation Algorithms and Metaheuristics.",
            "year": 2007
        },
        {
            "authors": [
                "Mark O. Riedl",
                "Robert Michael Young."
            ],
            "title": "An intent-driven planner for multi-agent story generation",
            "venue": "Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, 2004. AAMAS 2004., pages 186\u2013193.",
            "year": 2004
        },
        {
            "authors": [
                "Alane Suhr",
                "Claudia Yan",
                "Jack Schluger",
                "Stanley Yu",
                "Hadi Khader",
                "Marwa Mouallem",
                "Iris Zhang",
                "Yoav Artzi."
            ],
            "title": "Executing instructions in situated collaborative interactions",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Ryan Volum",
                "Sudha Rao",
                "Michael Xu",
                "Gabriel DesGarennes",
                "Chris Brockett",
                "Benjamin Van Durme",
                "Olivia Deng",
                "Akanksha Malhotra",
                "Bill Dolan"
            ],
            "title": "2022. Craft an iron sword: Dynamically generating interactive game characters",
            "year": 2022
        },
        {
            "authors": [
                "Lingzhi Wang",
                "Mrinmaya Sachan",
                "Xingshan Zeng",
                "Kam-Fai Wong."
            ],
            "title": "Strategize before teaching: A conversational tutoring system with pedagogy self-distillation",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 2268\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Yuxin Wang",
                "Jieru Lin",
                "Zhiwei Yu",
                "Wei Hu",
                "B\u00f6rje F. Karlsson"
            ],
            "title": "2023b. Open-world story generation with structured knowledge enhancement: A comprehensive survey",
            "year": 2023
        },
        {
            "authors": [
                "Nathaniel Weir",
                "Ryan Thomas",
                "Randolph D\u2019Amore",
                "Kellie Hill",
                "Benjamin Van Durme",
                "Harsh Jhamtani"
            ],
            "title": "Ontologically faithful generation of non-player character dialogues",
            "year": 2023
        },
        {
            "authors": [
                "Fangyuan Xu",
                "Yixiao Song",
                "Mohit Iyyer",
                "Eunsol Choi."
            ],
            "title": "A Critical Evaluation of Evaluations for Long-form Question Answering",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, Toronto, CA. Association",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Dialogue in most narrative-driven video games has historically been static: players may choose from a small number of pre-written dialogue options that depend on the game\u2019s state (e.g., items held or goals achieved). The advent of large language models (LLMs) has inspired efforts to dynamically generate dialogue in video game environments, such as\n1https://pl.aiwright.dev\nthose by AI Dungeon (Walton, 2019), InWorld AI (Gelfenbeyn et al., 2021), and ConvAI (Mukherjee, 2022), which can potentially imbue games with endless variety.2 However, evaluating the impacts of LLM-generated dialogue on the player experience has yet to be tackled in a principled manner.\nIn this paper, we directly evaluate the player experience by asking video gamers to interact with LLM-generated dialogue injected into Disco Elysium: The Final Cut, a highly-acclaimed dialoguecentered video game (Kurvitz et al., 2021).3 To mitigate difficulties with evaluating open-ended text generation (Karpinska et al., 2021), we specifically examine a constrained dialogue generation task in which an LLM must decide how to update a dialogue to match the corresponding game state.\nTake for example a scene in which the game\u2019s protagonist (an amnesiac detective) is interrogating an uncooperative suspect. If the player decides to act like the suspect\u2019s friend to obtain more information, the variable seafort.deserter_sugg_you_are_buddies is set to true, and the corresponding line of dialogue is:\nYou can tell me, here. It won\u2019t be *that* usable.\nContinuing the example, we then prompt an LLM to appropriately modify the dialogue when the variable seafort.deserter_i_am_also_communist is set to true. We evaluate the generated dialogue against the game writers\u2019 original line:\nYou can tell a comrade. It won\u2019t be *that* usable.\nWhile the semantics of the utterance remains mostly unchanged, the player\u2019s assumed communist persona is reflected in the second dialogue. These dialogue options, and any associated nonplayer character (NPC) responses, are defined us-\n2See for example Nvidia\u2019s recent ACE demo. 3Disco Elysium: The Final Cut is currently rated the #1\nPC video game of all time on Metacritic.\ning a graph structure commonly referred to as a \u201cdialogue tree\u201d in the video game industry. Crucially, our task setup does not expect an LLM to generate all of the game\u2019s dialogue (as in e.g., AI Dungeon), but rather provides the LLM with human-written dialogue as input and tweaks it to fit various dynamic aspects of the game state. To condition on the game state, we devise a clever approach of encoding the graph structure as a mix of code and natural language, which also opens the possibility in future work of modifying the game state in response to a generated utterance. This constrained setup makes the task tractable to evaluate and also practically relevant to future LLM-human collaborative game writing applications.\nCan an LLM understand enough about the game state to appropriately modify dialogue in a way that is logically and tonally consistent with the game state? More generally, how does LLM-generated dialogue stack up with the award-winning dialogue written by Disco Elysium\u2019s designers, and what do video gamers think are the biggest issues with it? By choosing a popular roleplaying game with a dedicated following, we are more easily able to find participants familiar with the expected tone and lore needed to effectively assess our generated dialogue. We evaluate OpenAI\u2019s state-of-the-art GPT-4 LLM (OpenAI, 2023) via a statistically robust user study, asking Disco Elysium fans to provide preference judgments and free-form feedback within an interface designed to mimic the game\u2019s dialogue engine.\nPerhaps unsurprisingly, players strongly prefer the original dialogue (H) compared to LLMgenerated dialogue (G), with participants citing reasons such as better logical consistency (H: 61% vs G: 28%) and flow (H: 67% vs G: 21%). However, participants note that GPT-4 begins to close the gap at providing interesting dialogue options (H: 47% vs G: 36%) that advance their goals (H: 57% vs G: 33%), though further work is required to ground the dialogue to the game state as 32% of generations rated by at least one player are deemed illogical upon reading the next utterance. To facilitate future research in player-centered video game dialogue generation, we release our annotation interface and tools to reproduce our dataset."
        },
        {
            "heading": "2 Related Work",
            "text": "Commercial video games have become increasingly popular testbeds for neural approaches to\ngrounded language (Suhr et al., 2019) and reinforcement learning (Bellemare et al., 2012; Kempka et al., 2016). To the best of our knowledge, only the sandbox game Minecraft has been explored as a testbed for interactive dialogue research (Volum et al., 2022), despite some commercial video game dialogue being explored in noninteractive settings (Lopez Latouche et al., 2023; Weir et al., 2023).\nDialogue systems in roleplaying video games are task-oriented (Grosz, 1974), where quests in the game act as tasks that the player must complete within the constraints of the game world. The static pre-written dialogue graphs are a form of finitestate dialogue management (Brabra et al., 2022). Using such a rigidly constrained dialogue management approach ensures the authorial intent of the game writers at the expense of more natural conversation flows. Rather than upend these familiar techniques for more flexible approaches which have yet to gain traction (Riedl and Young, 2004; Mateas and Stern, 2005), likely due to their complexity, we opt to augment the existing approaches actively in use in commericial video games. Our work bridges the tightly scripted scenarios common to video games, with more natural speech that offers humans more agency over their interaction with virtual agents, leading to easier to design agent interactions useful for training simulations (Demasi et al., 2020) and tutoring (Wang et al., 2023a).\nNarrative-driven video games fall under the umbrella of interactive storytelling, which can take on many forms including tabletop roleplaying games like Dungeons and Dragons (Callison-Burch et al., 2022), choose your own adventure books (Clark and Smith, 2021), and interactive fiction (Hausknecht et al., 2020). More broadly, graph representations have been used for finite-state dialogue management (Koller et al., 2018), and as knowledge bases (Gritta et al., 2021) for slot-filling approaches (Cohen, 2019) to task-oriented dialogue.\n3 The Disco Elysium Dataset\nIn Disco Elysium: The Final Cut, the player takes on the role of a down-on-his-luck detective in order to solve a murder mystery in a dystopian city. The majority of in-game interactions are in the form of dialogue, including interactions with not only other characters but also inanimate objects (e.g., a ceiling fan and a bathroom mirror from the first scene of the game), which makes the game well-suited for\nour experiments. Additionally, the game\u2019s state is encoded in Lua4 via descriptively-named boolean variables and getter/setter functions that trigger on certain nodes of the dialogue graph. In this section, we describe how we extract and process the rich dialogue graph (Figure 1) and game state from Disco Elysium for our constrained dialogue generation task.\n3.1 Extracting data from Disco Elysium\nWe begin by extracting a catalog of all top-level entities (characters, items, conversations, dialogue entries, and game state variables) from a purchased PC version of Disco Elysium: The Final Cut using the open source tool AssetStudio.5 Its prose, with over 70K utterances consisting of roughly 1.1M words of dialogue (Table 1), is nearly twice the length of Atlas Shrugged. As with many games, Disco Elysium encodes game state variables and functions into Lua expressions that are run by the game engine based on the player\u2019s actions.\n4https://www.lua.org 5https://github.com/Perfare/AssetStudio\nDialogue and game state encoding: The game state of Disco Elysium is an exhaustive mix of variables and functions that are descriptively named and commented. All dialogue entries in the graph include metadata about the character who is speaking, as well as any preconditions (boolean-valued expressions) required to speak the utterance. For example, the first dialogue option in Figure 2\u2019s Lua\ninto a Lua script that contains game logic and dialogue. Finally, we <MASK> one utterance from the cluster and ask GPT-4 (an LLM trained on code and natural language) to infill the masked dialogue.\nscript contains the following precondition:\nif CheckPassiveSkill(\"suggestion\")\nWe also observe comments written by the game developers used to document the intent of a variable or function, as in this comment before setting the variable seafort.deserter_hl_threaten_with_pain:\npain for talking and respect. does not actually work\nThese comments provide a rich, natural source of context to better explain the vast array of entities (Table A1) referenced throughout the game logic. Each dialogue entry may also contain functions that can alter game state when uttered that also serve as important context to understand the dialogue, as in:\nSetVariableValue( \"seafort.deserter_sugg_you_are_buddies\", true )\nCreating dataset splits: Since the dialogue in Disco Elysium ultimately tells a single overarching story, it is not possible to create a dataset split in which each fold contains a disjoint set of characters, items, and game state variables. The game\u2019s dialogue graph implicitly forms a hypergraph, with hyperedges defined by the Lua variables. Optimal partitioning of a hypergraph is known to be NP-hard, and an exhaustive enumeration is often\nmore efficient for smaller hypergraphs than specialized algorithms (Papa and Markov, 2007). We use the branch and bound algorithm to enumerate all valid partitions of the dialogue graph that satisfy an \u03f5 = 1.5% variation from the desired splits of 90%/5%/5% train/valid/test.6 The final split is achieved with minimal overlap in game variables (Table 1).7"
        },
        {
            "heading": "4 Grounded dialogue infilling",
            "text": "As prior text generation and dialogue research has clearly demonstrated (Karpinska et al., 2021; Clark et al., 2021), evaluating LLM-generated dialogue is a daunting challenge. We examine a more constrained subtask in which an LLM is given multiple lexically-similar human-written responses to a given utterance, each of which is slightly different from each other based on the game state. One of these responses is masked out, and an LLM is asked to generate it based on cues from the game state (e.g., that communists have historically referred to one another as comrade). While this task is strictly easier than open-ended dialogue generation,\n6Luckily, many distinct conversations are connected by one or more dialogue edges, making it such that a small handful of connected components in the graph make up roughly 70% of the dialogue and thus are required to be in the training set.\n7While the rest of this paper uses the validation set, including as a source of few-shot demonstrations to prompt GPT-4, we describe preliminary fine-tuning experiments on the training set in Appendix C.\nwe find that state-of-the-art LLMs still struggle to solve it. This section details the data filtering and preprocessing steps we performed, as well as the few-shot prompting strategy we use with GPT-4."
        },
        {
            "heading": "4.1 Clustering lexically-similar utterances",
            "text": "We detect lexically-similar utterances by applying a simple token-based clustering algorithm on the nodes in the dialogue graph. Starting from a source dialogue node, we traverse all outgoing directed paths which terminate upon encountering a dialogue node; we collect all such sets by using each dialogue node as a source. Then, we tokenize the dialogue in each node set on whitespace and punctuation boundaries while preserving common contractions (e.g., \u2019ll, \u2019s, etc). Next, we compute bagof-words F1 among utterances within each set to measure similarity, and also apply the same procedure to the associated Lua conditions. Finally, we consider all disjoint subsets for which F1>= 0.5 (for utterances or conditions), which we qualitatively validated as producing clusters of high lexical similarity.8"
        },
        {
            "heading": "4.2 Linearizing clusters into Lua scripts",
            "text": "Now that we have a set of clusters, we convert each cluster into a single Lua script (see Figure 2, right) that can be fed to a language model. We prefix all the characters, items, comments, and variables referenced by the clustered nodes at the top of the script, along with default values and metadata. Each node in the cluster is visited in sequential order, and its Lua conditions, dialogue, and any associated post-speech game state altering actions are included in the script. Lastly, we enumerate all variants of each script by masking out each instance in a cluster one-by-one. Our masked infilling prompts use a prefix-suffix-mask (Donahue et al., 2020; Bavarian et al., 2022) ordering, where the prefix contains the portion of the script before the masked utterance, followed by the text <MASK> ; the suffix contains the remainder of the script, followed by <MASK> =; and the mask contains the utterance we want the model to infill, followed by <MASK:END> ."
        },
        {
            "heading": "4.3 Few-shot prompting to infill dialogue",
            "text": "From our set of linearized Lua scripts, we build few-shot prompts to perform dialogue infilling using GPT-4. We first prefix each prompt with the\n8See Appendix B for more details on alternate clustering algorithms that we experimented with.\nfollowing instruction to guide the model to generate dialogue constrained to the game state:\nYou are a creative game designer writing engaging dialogue for a roleplaying game. For each self-contained dialogue script fill in the <MASK> with interesting dialogue using only facts from the script.\nThen for a given script, we select demonstrations from our validation set that do not contain any utterances in common with the target script. We fill the full 8k context of GPT-4 with demonstrations, which we qualitatively find to produce utterances that best match the game\u2019s writing style (Table 2).9"
        },
        {
            "heading": "5 Setting up a strong user study",
            "text": "Evaluating LLM-generated dialogue within a large commercial video game is a complex undertaking. Both automatic and crowdsourced evaluation lead to misleading and unreliable conclusions for creative generation tasks (Karpinska et al., 2021; Wang et al., 2023b), which motivates expert annotation (Xu et al., 2023; Karpinska and Iyyer, 2023). More importantly, we want to collect evaluations from people who are actually invested in Disco Elysium, as a primary goal of our study is to characterize how LLM-generated dialogue affects the player experience. This entails collecting evaluations within an interactive setting rather than having annotators rate utterances in isolation. In this section, we specify our evaluation setup, which involves (1) designing an interface that mimics the Disco Elysium dialogue engine; (2) conducting pilot tasks to determine common error categories and further refine the interface to reduce annotator burden; and (3) recruiting participants from Reddit who have completed one or more playthroughs of Disco Elysium to complete our main user study.\n9Additional details, including our full prompt template, can be found in Appendix D.\nStep 1: read conversation context and select preferred response\nStep 2: provide justification for preference\nStep 3: read next utterance and update preference if desired\n5.1 Designing an interface to recreate Disco Elysium\u2019s dialogue engine\nIn Disco Elysium, players control a virtual representation of the main character that can move around and interact with the environment, including initiating conversations. This freedom makes it difficult for us to constrain our study within the confines of the video game. Thus, rather than creating a game mod that includes LLM-generated dialogue,10 we design a custom web app11 that recreates the core gameplay systems that underpin the game\u2019s dialogue system. This interface allows us to present a specific conversation taken from our validation split to players (i.e., study participants).\nWhat annotations do we collect? Players are presented with the original human-written dialogue for all characters from the game, while the main character\u2019s dialogue options are paired alongside generated utterances from GPT-4 (Section 4), which we randomly shuffle and label with the subscripts a and b. Players are then asked to provide a preference judgment over the candidate utterances:\n10See for example the InWorld AI-based Skyrim mod. 11We use the React framework with an integrated Lua interpreter that executes the gameplay logic as defined in our linearized Lua scripts (Section 4.2).\nchoose which candidate best fits their goals while taking into account the previous conversation history and story context (Figure 3a). After making a preference judgment, they are asked to justify their choice via both predefined tags (e.g., advances my goals or matches desired mood) as well as optional free-form comments (Figure 3b). Finally, they are shown the ground-truth human-written next line of dialogue, and they are asked whether they would change their judgment in retrospect given this knowledge; if so, they are again asked to justify their decision (Figure 3c)."
        },
        {
            "heading": "5.2 Running usability studies to refine the annotation task",
            "text": "We conduct two usability studies to better understand common types of free-form participant feedback; additionally, these studies guide the refinement of our interface to reduce cognitive load for participants (e.g., switching to the two-stage annotation flow presented in Figure 3b & c). These usability studies also led to the coding and integration of predefined tags discussed above. Our first usability study enlists six college students (each of whom had previously played through Disco Elysium) to spend one hour with our web app. We also conducted a follow-up controlled observational study\nwith two more college students, using the Nielsen Norman Group Observer Guidelines12 to assess the user experience implications of our interface.13\nManually coding human feedback: We manually code the human-written feedback from our first usability study to build a list of common justifications for player preferences. This leads to a categorization of 13 high-level justifications of a player\u2019s initial preference, and 8 reasons for retroactively updating their preference. To improve annotator efficiency, we update our annotation interface to provide a list of common justification tags participants can choose from in addition to free-form text.14"
        },
        {
            "heading": "5.3 Statistically robust Reddit study",
            "text": "Our task necessitates a study with many players since post-utterance functions can alter the current game state, potentially affecting reachability in the graph, which leads to dozens of paths through the conversation chosen from our validation set that we use for our evaluation. For that reason we recruit 28 fans of the game from the r/DiscoElysium subreddit to take part in our study (Table 3).15 Based on our usability studies, we estimated players require between 1-2 hours to complete our study, and we provide $25 gift cards for participation in the study.\nAll participants have played Disco Elysium before: We limit participation in our study to players who have completed at least one full playthrough of Disco Elysium: The Final Cut in English.16 This is necessary since the game weaves a complex narrative that incorporates the player\u2019s\n12https://www.nngroup.com/articles/observer-g uidelines/\n13See Appendix E for more details about our usability studies and refinements.\n14See Appendix F for specifics on each tag. 15Our study was approved by IRB review, and all partici-\npants are at least 18 years of age. 16We leave evaluation of other languages to future work.\ndialogue choices. For example, if a player often chooses dialogue options that indicate the main character is a fascist, the player will more frequently be presented with dialogue options that reflect this world view. Thus, each player will have a unique trajectory through the dialogue graph and must mentally keep track of their choices, a skill players learn through experience with the game. Furthermore, due to the dataset split (Section 3.1), our validation data is from a late stage in the game, so participants must know the full story context to understand the nuances of each dialogue choice.\nStudy parameters: Approximately 20% of utterances a player reads during our study contain LLM-generated dialogue. This is another reason why we hired so many participants, as it requires substantial reading time between annotations. In aggregate, 112 unique utterances are rated by our participants, though since each player performs a unique walk of the dialogue graph, on average each player rates 41 utterances, and only 100 utterances receive at least three ratings (Table 4).17"
        },
        {
            "heading": "6 Results & analysis",
            "text": "Overall, our study reveals a strong preference for human-written dialogue over LLM-generated dialogue. Participants most commonly cite reasons for their preferences such as increased appropriateness, better match with their gameplay goals, and stylistic properties (Figure 4). Because assessing the generative capabilities of LLMs is confounded by the subjective nature of rating narrative quality (Ethayarajh and Jurafsky, 2022; Wang et al., 2023b), we also conduct a fine-grained analysis of free-form player justifications to uncover where GPT-4 succeeds and where it needs improvement.\n17Following Card et al. (2020), our study design has a statistical power of 0.96 for a margin of 10%, though our reported margins in Section 6.1 are often much larger."
        },
        {
            "heading": "6.1 Participants prefer human-written dialogue",
            "text": "Overall, out of the 1,158 total judgements we collect from players, 702 (61%) state a preference for human-written dialogue while 456 prefer GPT-4. When aggregating at the instance level (i.e., computing the majority vote on all instances for which we collected annotations from at least three different players), we note a stronger preference for human-written dialogues. Specifically, upon their first assessment (Figure 3b), players prefer humanwritten dialogue to that of GPT-4 (H: 64% vs G: 23%; rest ties), and after retrospectively updating their preference (Figure 3c), annotators prefer the original dialogue even more (H: 66% vs G: 23%).\nReasons for preference: When players prefer human-written dialogue over model-generated dialogue, they cite reasons such as increased logical consistency (H: 61% vs G: 28%) and flow (H: 67% vs G: 21%). After seeing the next utterance, the most common reason for players to change their preference is that the selected utterance was illogical in hindsight (Figure 4b), which affects 32% of GPT-4 generations rated by at least one player. Overall, these results suggest that future research should focus on better grounding of LLM generations to the game state. However, GPT-4 does close the gap on certain aspects of the generated dialogue, including providing interesting dialogue options (H: 47% vs G: 36%) that contain more specifics (H: 43% vs G: 46%) and advance player goals (H: 57% vs G: 33%), which shows the potential of collaborative human-LLM dialogue."
        },
        {
            "heading": "6.2 Fine-grained analysis",
            "text": "While the overall results show a strong preference for human-written dialogue, we note that the task is inherently subjective, and player preference is not always related to the quality of the options. Some level of disagreement between participants is expected, which motivates us to perform a more fine-grained analysis to uncover common facets that provide insight on these disagreements and highlight where GPT-4 excels and struggles.\nInconsistencies with the game world: Many of the justifications for players\u2019 preferences mention appropriateness or logical consistency / flow with the conversation history. In general, the world created by the game designers has a depth and consistency that is difficult for models to understand, especially when it is encoded programmatically (e.g., in Lua scripts) rather than in unstructured text. In the following example, four players justify their preference (one shown below) for the humanwritten text by noting that the generated utterance does not conform to the game\u2019s notion of an \u201cultraliberal\u201d character, which requires an understanding of the game\u2019s various political factions.\n(1) a. That\u2019s a *choice*. You could have become selfemployed. Create the system.\nHUMAN-WRITTEN\nb. I steal from the rich, redistribute wealth, and fight for a borderless world.\nGPT-4 GENERATED\ni. As an Ultraliberal character, this option feels more appropriate in this playthrough.\nfeels more appropriate PREFERS HUMAN-WRITTEN\nAwkward articulation: Sometimes, the competing objectives of generating a fluent utterance and\nstaying faithful to the game state result in awkward generations that can impact logical and stylistic coherence, as in:\n(2) a. We\u2019re not \u2019Coalition-appointed.\u2019 We just try to help people.\nHUMAN-WRITTEN\nb. I\u2019m not \u2019Coalition-appointed.\u2019 We just try to help people.\nGPT-4 GENERATED\ni. Since Kim is with me, it\u2019s more appropriate to say \"we\".\nfeels more appropriate, matches desired mood PREFERS HUMAN-WRITTEN\nii. I never know if we can trust Kim feels more appropriate, matches desired mood,\nadvances my goals, contains more specifics PREFERS GPT-4 GENERATED\nHere, six players prefer the original utterance, in which both sentences use the plural pronoun \u201cwe\u201d, and call out the awkwardness of excluding the main character\u2019s partner Kim in the second GPT-4 generated sentence; one example justification is shown in (2i). Despite this conflict, however, two players prefer the GPT-4 generation as they believe it better matches the mood (2ii).\nFit with play style: An attractive aspect of roleplaying games is that players can mold the game\u2019s narrative to their individual play style. Disco Elysium allows for a huge variety of play styles; for example, players can choose to tackle the game as an analytic Sherlock Holmes-type detective or as a physically imposing but dim-witted enforcer. GPT-4 is able to provide diverse dialogue that is amenable with certain play styles:\n(3) a. [Pick up the gun lying in the sand.] HUMAN-WRITTEN\nb. Mind if I examine your gun, Mr. Dros? GPT-4 GENERATED\ni. He threw it away \"like an amputated limb\" so I don\u2019t think asking him for permission rhetorical or not seems appropriate\nfeels more appropriate PREFERS HUMAN-WRITTEN\nii. I am being passive aggressive here, I do not, in fact, care if he minds.\nfeels more appropriate PREFERS GPT-4 GENERATED\nIn the above example, one player thinks it does not make sense to ask permission to look at the gun (3i), while the other prefers the GPT-4 generation because they intentionally want to be passive aggressive towards Mr. Dros (3ii). Both players marked feels more appropriate as a justification, which is not a contradiction since they each have different play styles and objectives.\nParaphrasing: Dialogue in games tends to be static: speaking with a non-player character often leads to the same utterances being repeated in the absence of relevant game state changes. Fortunately, recent LLMs like GPT-4 are quite adept at paraphrasing text. When the model correctly reproduces an utterance semantically similar to the human written dialogue, players often randomly choose between the two:\n(4) a. One more time: what have you used this gun for?\nHUMAN-WRITTEN\nb. Alright, I\u2019ll ask again. What have you been using this gun for?\nGPT-4 GENERATED\ni. These are both basically the same so I just picked one at random\nrandomly selected PREFERS HUMAN-WRITTEN\nHowever, in some cases the generated paraphrases include small extraneous information that feel off to the players. In the following example, two players specifically call out GPT-4\u2019s phrasing:\n(5) a. Stop changing the subject \u2013 we have the murder weapon. (Point to it.)\nHUMAN-WRITTEN\nb. Enough squirming. I have the murder weapon, and Kim here can confirm it.\nGPT-4 GENERATED\ni. I don\u2019t think Kim is a figure of authority at this point - he knows about as much about the murder weapon as Harry does.\nfeels more appropriate, advances my goals, seems more logical\nPREFERS HUMAN-WRITTEN"
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we perform a user study of LLMgenerated dialogue integrated into video games, hiring fans of Disco Elysium: The Final Cut to provide fine-grained insights about issues in this domain. We examine a constrained dialogue generation task, in which the game state is integrated into the LLM prompt via Lua scripts that encode game variables, functions, and dialogue. We develop a web interface that reproduces Disco Elysium\u2019s dialogue engine to conduct the evaluation. Humanwritten dialogue is strongly preferred over GPT-4 generations for reasons such as improved logical flow, appropriateness, and tonal consistency. Future work can build on our framework to consider user play style, faithfulness to the game state, and dynamically updating game state as important components of the modeling and evaluation process."
        },
        {
            "heading": "8 Acknowledgements",
            "text": "Thanks to all of the Disco Elysium fans who took part in the studies described in the paper. Also, thanks to the reviewers and to Brendan O\u2019Connor for valuable feedback which helped improve the manuscript. Finally, thanks to Ronan Salz and George Wei who conducted an early exploration of the Disco Elysium data. This project was partially supported by awards IIS-2202506 and IIS-2046248 from the National Science Foundation (NSF)."
        },
        {
            "heading": "9 Limitations",
            "text": "While the dialogue for Disco Elysium is available in Simplified Chinese, Traditional Chinese, English, French, German, Japanese, Korean, Polish, Portuguese-Brazilian, Russian, Spanish, and Turkish, our study focuses exclusively on the English version of the game, which was necessary due to the large number of players required for adequate coverage of the dialogue graph. Additionally, the generated dialogue we show participants is generated once from GPT-4, such that each player rates the same generated utterance. This reduces variation in the annotation task allowing us to have high statistical power for our results. Though due to the static nature of the generated dialogue we do not account for the player\u2019s unique walk of the dialogue graph, which would require dynamically generated utterances, even though this could better reflect the interactive nature of video games. We live this to future work. We also note that Disco Elysium represents a niche genre of narrative-driven games that does not reflect the full diversity of narratives seen in video games, thus our approach is unlikely to generalize to the diverse catalog of video game narratives. Though our work does apply to a large class of popular video games such as the recently released Baldur\u2019s Gate 3 and Starfield18, both of which have enthralled millions of gamers within a month of their release19."
        },
        {
            "heading": "10 Ethical Considerations",
            "text": "Disco Elysium contains adult themes, including discussions of suicide, murder, and rape. For this reason, we ensure participants in our study are at least 18 years of age as required by our institutional\n18https://www.ign.com/articles/starfield-hit s-10-million-players-in-less-than-three-weeks\n19https://gameworldobserver.com/2023/10/19/bg3 -topped-starfield-player-engagement-steam-us-cir cana\nreview board. We do not collect any demographic information of the participants beyond age verification. Additionally, we ensure participants in our study are fairly compensated for their time, offering $25 gift cards for Reddit participants, $30 gift cards for our observational study participants, and either $25 or $50 gift cards for our initial pilot participants depending on the length of time they spent on the study.\nWe also note that Disco Elysium: The Final Cut is a copyrighted game, so we take special care to ensure we respect the intellectual property of the game\u2019s designers.20 We do not release any models trained on the game\u2019s data, nor do we widely release our web interface, which requires registering an account with an authorization token to take part in the study. Finally, the web app only has access to a small portion of the overall game data taken from the validation set which is needed to conduct the study, and that data is only ever kept in memory, never persisted to disk."
        },
        {
            "heading": "A Dataset Splits",
            "text": "We provide additional details regarding our dataset splits. As we note in the main body of the paper, a key challenge in splitting the Disco Elysium dataset into train, valid, and test splits is the high degree of interconnectedness across conversations in the game. While it is not possible to create a dialogue split with a disjoint set of game state variables, we minimize overlaps amongst the splits.\nVariable Overlap Dataset Totals Train \u22c2 Valid 2897 Items 259\nTrain \u22c2 Test 2871 Characters 424\nValid \u22c2 Test 303 Conversations 610\n(a) (b)\nTable A1: For the Disco Elysium dataset: (a) we take special care to minimize the number of referenced variable overlaps amongst the splits; (b) though we do not attempt to disentangle Characters and Items across splits."
        },
        {
            "heading": "B Additional Clustering Details",
            "text": "We experiment with a number of algorithms for clustering the game\u2019s dialogue, including the Levenshtein distance, Jaccard index, and the Dice coefficient. We also vary the features used for clustering by splitting the words into characters, grouping by ngrams, and through the use of lowercasing. We conduct a manual inspection of the various approaches to clustering, including a hyperparameter sweep of the similarity threshold. This inspection indicated that solely clustering based on the dialogue utterances would either systematically miss semantically similar text, or cluster dissimilar utterances when the similarity threshold was made more permissive.\nTo combat this tendency, we additionally tried clustering nodes by inspecting the associated Lua conditions. We first parse the Lua expression, extract identifiers (which often refer to functions) and string literals (which often refer to variables). We then split the literals into their constituent words (e.g., whirling.dreamone_brave becomes whirling, dreamone, brave), before running the above battery of clustering approaches. In the end, we find that clustering based on a combination of dialogue and Lua expressions produced the best results, without the need for the extra feature engineering, while only relying on the simple Dice coefficient with a threshold d >= 0.5."
        },
        {
            "heading": "C Preliminary Experiments",
            "text": "We conduct experiments using two LLMs: GPT3 Curie and Codex (Table A2). GPT-3 Curie is a strong generation model for natural language (Brown et al., 2020), especially when finetuned on a downstream task, while Codex is an extremely capable few-shot LM for code (Chen et al., 2021). As our task contains elements of both natural language and code, it is important to assess the capabilities of each model paradigm.\nModel Class\nPrompt Tokens\nModel Type\nOpenAI API Name\nCurie 1 2048 Finetuned curie Codex 8000 Few-Shot code-davinci-002\nTable A2: Details of the models used in our experiments. As OpenAI does not provide parameter counts or details on finetuning, we also provide the API name for the models to help reproducibility.\n1Likely 6.7B parameters, see: https://blog.eleuther.ai/gpt3-model-sizes/\nSince the two models perform different tokenization2 and support different context lengths, we filter the clusters, keeping only those that fit the smallest context length (2048 tokens) using the GPT-3 tokenizer. We then generate all the linearized scripts representing semantically related text for the next turn of dialogue. After filtering and generating masked variants of the clusters, we are left with 30,501 training examples and 2,668 validation examples.\nWe finetune Curie for 1 epoch, with a batch size of 32 examples and a learning rate of 0.2\u00d7 the learning rate of the pretrained model and we weight the loss for the prompt tokens by 0.01. For the few-shot Codex model, we prefix each linearized Lua script with several samples from the validation set such that they take up nearly the full context window (we reserve 100 tokens of the context for generation). We also ensure there are no overlaps in dialogue between the few-shot examples and the script. Consequently, each Codex script has 7 few-shot examples on average.\nWe choose to measure the performance of the models on the validation set using a bag-of-words F1, as the clustered utterances have a large overlap with the masked text the model is tasked with\n2Codex uses a modified tokenizer that collapses whitespace since it is commonly used in code formatting.\nModel Examples Tokens BLEURT F1 Curie 2,668 3,041,299 41.9 25.6 Codex 2,668 21,077,200 44.2 29.5\nTable A3: Preliminary experiments over the validation set show that few-shot Codex outperforms a finetuned Curie model for generating context-aware dialogue.\ninfilling. In addition, we use BLEURT which has proven to be robust for semantic similarity of generated text (Karpinska et al., 2022). Both metrics favor Codex slightly, though given the low F1 score, it\u2019s clear the models have much room for improvement on this simplified form of our task. That is to say, na\u00efvely applying our preliminary approach to all the dialogue in the game, not just to the subset of dialogue clustered via similarity, is even more likely to fail. We also posit that Codex likely outperforms Curie since it is a larger model that is explicitly trained on a large corpus of code, even though it uses a few-shot approach to inference.\nTo better understand the performance difference between the two models we also conduct a small analysis of each model\u2019s output. We find that both models tend to copy from the prompt (Table A4), but Codex does it nearly twice as often.\nModel Examples Copied Curie 2,668 235 Codex 2,668 455 (8)\u2020\nTable A4: We find that both Curie and Codex occassionally copy dialogue from the prompt, and in 8\u2020 instances Codex directly copies a completion from the few-shot examples.\nA qualitative inspection of the generations from the Codex model (our best performer) seem to indicate the model may struggle to generate plausible completions due to a lack of historical context to the current conversation. Our script-based prompts do not include any previous dialogue utterances, but rather rely only on the combination of dialogue that can be emitted next and conditional game logic gating those options. It is clear the models also do not make effective use of the game designer\u2019s annotations to fill in the gaps. While these comments are likely useful reference for the writers of the game, they may not contain enough context alone to guide generation. Considering Codex has a very long context window and performs better than a finetuned Curie (Table A3), future experiments could attempt\nFew-Shot Prompt Statistics Min Avg Max\nExample Length 158 330 2228 Examples per Prompt 15 33 45 Prompt Length 7547 7566 8089\nTable A5: Here we report statistics for token lengths of each example and the overall prompts, along with the number of examples per prompt.\nto include previous turns of dialogue in the prompt to see if that improves generation quality."
        },
        {
            "heading": "D Few-shot Prompting",
            "text": "Since we target GPT-4 for our main study, we provide few shot prompts using their chat format. All prompts are prefixed with the following system message:\nYou are a creative game designer writing engaging dialogue for a roleplaying game. For each self-contained dialogue script fill in the <MASK> with interesting dialogue using only facts from the script.\nAdditionally, our few shot examples are encoded as chat conversations where a user message provides the model with a script, and the the assistant responds with the completion. In terms of the linearization we describe in Section 4.2, the user message consists of the prefix and suffix, while the assistant message consists of the mask. Note we, experimented with other prompting approaches, but found the above worked best. We investigated zero-shot and few-shot prompts containing various numbers of examples. The resultant GPT-4 generations often did not match the style of the game writing, frequently leading to verbose utterances. We also tried interleaving instructions before each few-shot example, and that seemed to have no noticeable difference is quality.\nE Interface adjustments from usability studies\nTwo-step annotation flow In our initial interface players are presented with a single screen in which to provide free-form feedback on both the selected utterance and whether they want to change their mind after seeing the next utterance. This unified feedback screen led players to conflate the reason they chose a dialogue option with their post-hoc reasoning after seeing the next utterance. Before\nPredefined Tag Description randomly selected You randomly selected between the paired options. accidentally selected You accidentally selected the dialogue option. advances my goals The selected dialogue option advances your goals. exploring my options You are just trying to explore all the dialogue options. feels more appropriate The selected dialogue option fits the conversation better than it\u2019s counterpart. matches desired mood The selected dialogue option matches the mood you are going for. contains more specifics The selected dialogue option contains more context specific information than it\u2019s counterpart. other option is a repeat The paired dialogue option that was not selected repeats something that was already stated. other option is irrelevant The paired dialogue option that was not select is irrelevant to the current context. references earlier info The selected dialogue option references information from earlier in the conversation. seems interesting The selected dialogue option seems more interesting than it\u2019s counterpart. seems more logical The selected dialogue option fits the current context more logically than it\u2019s counterpart. other Please explain in your own words why you selected the dialogue option.\nTable A6: List of predefined tags and their associated description from our web app for justifying why a player initially chose a candidate utterance.\nour observational study, we split the feedback process into two screens (Figure 3b & c) and note that this obviates the player confusion seen in our initial study.\nUpdating the interface The second observational study highlighted three major concerns. First, the actual Disco Elysium game provides visual cues that were missing from the web interface that players relied upon to follow the conversation. We address this concern by tweaking our interface to better match the one from the game, including highlighting actor names using the same colors from the game and updating the icons for the player statistics shown at the bottom of the screen in Figure 3a. Second, we decided to instruct players to exhaust all dialogue options within the conversation, which allows us to improve our coverage of the generated utterances in the dialogue graph. This process differs from the actual gameplay, in which players are free to skip through many dialogues that serve to provide a backstory to the game world. Finally, it became clear that players might not complete the annotation task within one session, so we added support for automatically saving and resuming the\ntask starting where the player left off."
        },
        {
            "heading": "F Predefined Tags",
            "text": "We manually code the free-form comments from our first pilot study to understand common justifications for expressed preferences. We do this both for comments on a player\u2019s initial preference and upon retroactively updating their preference based upon seeing the next utterance. We then update our interface to include our manually coded categorizations as predefined tags that players can select to justify their choices. For the full list of predefined tags and the description we provide for the tag in our interface, please see Table A6 (initial preference) and Table A7 (upon retrospectively changing preference).\nPredefined Tag Description better advances my goals After seeing the next line of dialogue, it turns out that the other dialogue option better advances your goals. better explores my options After seeing the next line of dialogue, it turns out that the other dialogue option better explores the conversation. better matches desired mood After seeing the next line of dialogue, it turns out that the other dialogue option better matches the mood your are going for. contains more specifics After seeing the next line of dialogue, it turns out that the other dialogue option actually contains more context specific information. illogical in hindsight After seeing the next line of dialogue, it turns out that the other dialogue option does not make sense in context. is a repeat in hindsight After seeing the next line of dialogue, it turns out that the other dialogue option repeats something that was previously stated. seems more interesting now After seeing the next line of dialogue, it turns out that the other dialogue option is actually more interesting. other Please explain in your own words why you changed the selected dialogue option.\nTable A7: List of predefined tags and their associated description from our web app for justifying why a player retrospectively changed their preference."
        },
        {
            "heading": "G Rating Trends",
            "text": "Due to the nature of the game using a dialogue graph where node reachability is altered depending on the dialogue option chosen, on average each player only rates 41 utterances out of the 112 unique utterances rated by all players (Table 4). To understand how the player preference changes as the number of ratings a particular utterance receives, we produce stacked histograms (Figure A1 & Figure A2) where each bar represents player preference given the number of players rating an utterance.\n5 10 15 200\n50\n100 Tied GPT-4 Preferred Human Preferred\nMinimum Number of Users Rating an Utterance\nU tte\nra nc\nes\nOriginal Preference\nFigure A1: Histogram of initial candidate preferences based on minimum number of players rating the utterance.\n5 10 15 200\n50\n100 Tied GPT-4 Preferred Human Preferred\nMinimum Number of Users Rating an Utterance\nU tte\nra nc\nes\nUpdated Preference\nFigure A2: Histogram of retroactively updated preferences based on minimum number of players rating the utterance."
        }
    ],
    "title": "A Framework for Exploring Player Perceptions of LLM-Generated Dialogue in Commercial Video Games",
    "year": 2023
}