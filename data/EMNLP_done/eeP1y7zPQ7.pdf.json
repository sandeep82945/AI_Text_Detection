{
    "abstractText": "To tackle the high inference latency exhibited by autoregressive language models, previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds. Consequently, we propose a Fast and Robust EarlyExiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding. Our framework enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens. Furthermore, as parallel decoding allows us to observe predictions from both shallow and deep models, we present a novel adaptive threshold estimator that exploits a Beta mixture model to determine suitable confidence thresholds. We empirically demonstrated the superiority of our proposed framework on extensive generation tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sangmin Bae"
        },
        {
            "affiliations": [],
            "name": "Jongwoo Ko"
        },
        {
            "affiliations": [],
            "name": "Hwanjun Song"
        },
        {
            "affiliations": [],
            "name": "Se-Young Yun"
        }
    ],
    "id": "SP:b106158c739ed0ccc8192c7f68f418e2de2292f3",
    "references": [
        {
            "authors": [
                "Sangmin Bae",
                "Sungnyun Kim",
                "Jongwoo Ko",
                "Gihun Lee",
                "Seungjong Noh",
                "Se-Young Yun."
            ],
            "title": "Self-contrastive learning: Single-viewed supervised contrastive framework using sub-network",
            "venue": "arXiv preprint arXiv:2106.15499.",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Mauro Cettolo",
                "Marcello Federico",
                "Luisa Bentivogli",
                "Niehues Jan",
                "St\u00fcker Sebastian",
                "Sudoh Katsuitho",
                "Yoshino Koichiro",
                "Federmann Christian."
            ],
            "title": "Overview of the iwslt 2017 evaluation campaign",
            "venue": "Proceedings of the 14th International Workshop on",
            "year": 2017
        },
        {
            "authors": [
                "Mauro Cettolo",
                "Marcello Federico",
                "Luisa Bentivogli",
                "Jan Niehues",
                "Sebastian St\u00fcker",
                "Katsuhito Sudoh",
                "Koichiro Yoshino",
                "Christian Federmann."
            ],
            "title": "Overview of the IWSLT 2017 evaluation campaign",
            "venue": "Proceedings of the 14th International Conference",
            "year": 2017
        },
        {
            "authors": [
                "Charlie Chen",
                "Sebastian Borgeaud",
                "Geoffrey Irving",
                "Jean-Baptiste Lespiau",
                "Laurent Sifre",
                "John Jumper."
            ],
            "title": "Accelerating large language model decoding with speculative sampling",
            "venue": "arXiv preprint arXiv:2302.01318.",
            "year": 2023
        },
        {
            "authors": [
                "Tim Dettmers",
                "Mike Lewis",
                "Younes Belkada",
                "Luke Zettlemoyer."
            ],
            "title": "Llm",
            "venue": "int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339.",
            "year": 2022
        },
        {
            "authors": [
                "Maha Elbayad",
                "Jiatao Gu",
                "Edouard Grave",
                "Michael Auli."
            ],
            "title": "Depth-adaptive transformer",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Fabbri",
                "Irene Li",
                "Tianwei She",
                "Suyi Li",
                "Dragomir Radev."
            ],
            "title": "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Alexander R Fabbri",
                "Irene Li",
                "Tianwei She",
                "Suyi Li",
                "Dragomir R Radev."
            ],
            "title": "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model",
            "venue": "arXiv preprint arXiv:1906.01749.",
            "year": 2019
        },
        {
            "authors": [
                "Mingqi Gao",
                "Jie Ruan",
                "Renliang Sun",
                "Xunjian Yin",
                "Shiping Yang",
                "Xiaojun Wan."
            ],
            "title": "Human-like summarization evaluation with chatgpt",
            "venue": "arXiv preprint arXiv:2304.02554.",
            "year": 2023
        },
        {
            "authors": [
                "Marjan Ghazvininejad",
                "Omer Levy",
                "Yinhan Liu",
                "Luke Zettlemoyer."
            ],
            "title": "Mask-predict: Parallel decoding of conditional masked language models",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
            "year": 2019
        },
        {
            "authors": [
                "Bogdan Gliwa",
                "Iwona Mochol",
                "Maciej Biesek",
                "Aleksander Wawer."
            ],
            "title": "SAMSum corpus: A humanannotated dialogue dataset for abstractive summarization",
            "venue": "Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70\u201379, Hong",
            "year": 2019
        },
        {
            "authors": [
                "Alex Graves."
            ],
            "title": "Adaptive computation time for recurrent neural networks",
            "venue": "arXiv preprint arXiv:1603.08983.",
            "year": 2016
        },
        {
            "authors": [
                "Jiatao Gu",
                "James Bradbury",
                "Caiming Xiong",
                "Victor O.K. Li",
                "Richard Socher."
            ],
            "title": "Non-autoregressive neural machine translation",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Jiatao Gu",
                "Xiang Kong."
            ],
            "title": "Fully nonautoregressive neural machine translation: Tricks of the trade",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 120\u2013133, Online. Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Mandy Guo",
                "Joshua Ainslie",
                "David Uthus",
                "Santiago Ontanon",
                "Jianmo Ni",
                "Yun-Hsuan Sung",
                "Yinfei Yang."
            ],
            "title": "LongT5: Efficient text-to-text transformer for long sequences",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 724\u2013",
            "year": 2022
        },
        {
            "authors": [
                "dan Damoc",
                "Aurelia Guy",
                "Simon Osindero",
                "Karen Simonyan",
                "Erich Elsen",
                "Oriol Vinyals",
                "Jack William Rae",
                "Laurent Sifre"
            ],
            "title": "An empirical analysis of compute-optimal large language model training",
            "venue": "In Advances in Neural Information Processing Sys-",
            "year": 2022
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen."
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoqi Jiao",
                "Yichun Yin",
                "Lifeng Shang",
                "Xin Jiang",
                "Xiao Chen",
                "Linlin Li",
                "Fang Wang",
                "Qun Liu."
            ],
            "title": "TinyBERT: Distilling BERT for natural language understanding",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4163\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Sehoon Kim",
                "Karttikeya Mangalam",
                "Jitendra Malik",
                "Michael W Mahoney",
                "Amir Gholami",
                "Kurt Keutzer."
            ],
            "title": "Big little transformer decoder",
            "venue": "arXiv preprint arXiv:2302.07863.",
            "year": 2023
        },
        {
            "authors": [
                "Jongwoo Ko",
                "Seungjoon Park",
                "Minchan Jeong",
                "Sukjin Hong",
                "Euijai Ahn",
                "Du-Seong Chang",
                "Se-Young Yun."
            ],
            "title": "Revisiting intermediate layer distillation for compressing language models: An overfitting perspective",
            "venue": "Findings of the Association for Com-",
            "year": 2023
        },
        {
            "authors": [
                "Yaniv Leviathan",
                "Matan Kalman",
                "Yossi Matias."
            ],
            "title": "Fast inference from transformers via speculative decoding",
            "venue": "International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings",
            "year": 2023
        },
        {
            "authors": [
                "nas",
                "Alexander M. Rush",
                "Thomas Wolf"
            ],
            "title": "Datasets: A community library for natural language processing",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,",
            "year": 2021
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu."
            ],
            "title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
            "venue": "arXiv preprint arXiv:2303.16634.",
            "year": 2023
        },
        {
            "authors": [
                "Yijin Liu",
                "Fandong Meng",
                "Jie Zhou",
                "Yufeng Chen",
                "Jinan Xu."
            ],
            "title": "Faster depth-adaptive transformers",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13424\u201313432.",
            "year": 2021
        },
        {
            "authors": [
                "Yuandong Tian",
                "Christopher R\u00e9",
                "Beidi Chen."
            ],
            "title": "Deja vu: Contextual sparsity for efficient llms at inference time",
            "venue": "International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings",
            "year": 2023
        },
        {
            "authors": [
                "Zhanyu Ma",
                "Arne Leijon."
            ],
            "title": "Bayesian estimation of beta mixture models with variational inference",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(11):2160\u20132173.",
            "year": 2011
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Bowen Zhou",
                "C\u00edcero Nogueira dos Santos",
                "\u00c7aglar G\u00fcl\u00e7ehre",
                "Bing Xiang."
            ],
            "title": "Abstractive text summarization using sequence-tosequence rnns and beyond",
            "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural",
            "year": 2016
        },
        {
            "authors": [
                "RH Norden."
            ],
            "title": "A survey of maximum likelihood estimation",
            "venue": "International Statistical Review/Revue Internationale de Statistique, pages 329\u2013354.",
            "year": 1972
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Geondo Park",
                "Gyeongman Kim",
                "Eunho Yang."
            ],
            "title": "Distilling linguistic context for language model compression",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 364\u2013378, Online and Punta Cana, Dominican",
            "year": 2021
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "venue": "arXiv preprint arXiv:1606.05250.",
            "year": 2016
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Andrea Santilli",
                "Silvio Severino",
                "Emilian Postolache",
                "Valentino Maiorca",
                "Michele Mancusi",
                "Riccardo Marin",
                "Emanuele Rodol\u00e0."
            ],
            "title": "Accelerating transformer inference for translation via parallel decoding",
            "venue": "Proceedings of the 61st Annual Meeting of",
            "year": 2023
        },
        {
            "authors": [
                "Nikolay Savinov",
                "Junyoung Chung",
                "Mikolaj Binkowski",
                "Erich Elsen",
                "Aaron van den Oord."
            ],
            "title": "Stepunrolled denoising autoencoders for text generation",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Tal Schuster",
                "Adam Fisch",
                "Jai Gupta",
                "Mostafa Dehghani",
                "Dara Bahri",
                "Vinh Tran",
                "Yi Tay",
                "Donald Metzler."
            ],
            "title": "Confident adaptive language modeling",
            "venue": "Advances in Neural Information Processing Systems, 35:17456\u201317472.",
            "year": 2022
        },
        {
            "authors": [
                "Abigail See",
                "Peter J. Liu",
                "Christopher D. Manning."
            ],
            "title": "Get to the point: Summarization with pointergenerator networks",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Eva Sharma",
                "Chen Li",
                "Lu Wang."
            ],
            "title": "BIGPATENT: A large-scale dataset for abstractive and coherent summarization",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2204\u20132213, Florence, Italy. Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Noam Shazeer",
                "Mitchell Stern."
            ],
            "title": "Adafactor: Adaptive learning rates with sublinear memory cost",
            "venue": "International Conference on Machine Learning, pages 4596\u20134604. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "Surat Teerapittayanon",
                "Bradley McDanel",
                "HsiangTsung Kung."
            ],
            "title": "Branchynet: Fast inference via early exiting from deep neural networks",
            "venue": "2016 23rd International Conference on Pattern Recognition (ICPR), pages 2464\u20132469. IEEE.",
            "year": 2016
        },
        {
            "authors": [
                "Yonglong Tian",
                "Dilip Krishnan",
                "Phillip Isola."
            ],
            "title": "Contrastive representation distillation",
            "venue": "arXiv preprint arXiv:1910.10699.",
            "year": 2019
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Wenhui Wang",
                "Furu Wei",
                "Li Dong",
                "Hangbo Bao",
                "Nan Yang",
                "Ming Zhou."
            ],
            "title": "Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers",
            "venue": "Advances in Neural Information Processing Systems, 33:5776\u20135788.",
            "year": 2020
        },
        {
            "authors": [
                "Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Mengzhou Xia",
                "Zexuan Zhong",
                "Danqi Chen."
            ],
            "title": "Structured pruning learns compact and accurate models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1513\u20131528, Dublin, Ireland.",
            "year": 2022
        },
        {
            "authors": [
                "Guangxuan Xiao",
                "Ji Lin",
                "Micka\u00ebl Seznec",
                "Hao Wu",
                "Julien Demouth",
                "Song Han."
            ],
            "title": "Smoothquant: Accurate and efficient post-training quantization for large language models",
            "venue": "International Conference on Machine Learning, ICML 2023, 23-29 July 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Linfeng Zhang",
                "Jiebo Song",
                "Anni Gao",
                "Jingwei Chen",
                "Chenglong Bao",
                "Kaisheng Ma."
            ],
            "title": "Be your own teacher: Improve the performance of convolutional neural networks via self distillation",
            "venue": "Proceedings of the IEEE/CVF International Conference",
            "year": 2019
        },
        {
            "authors": [
                "Linfeng Zhang",
                "Zhanhong Tan",
                "Jiebo Song",
                "Jingwei Chen",
                "Chenglong Bao",
                "Kaisheng Ma."
            ],
            "title": "Scan: A scalable neural networks framework towards compact and efficient models",
            "venue": "Advances in Neural Information Processing Systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Xinghua Zhang",
                "Bowen Yu",
                "Haiyang Yu",
                "Yangyu Lv",
                "Tingwen Liu",
                "Fei Huang",
                "Hongbo Xu",
                "Yongbin Li."
            ],
            "title": "Wider and deeper llm networks are fairer llm evaluators",
            "venue": "arXiv preprint arXiv:2308.01862.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent advancements in autoregressive language models (Brown et al., 2020; Raffel et al., 2020; Hoffmann et al., 2022; Touvron et al., 2023) have revolutionized the quality of language generation in various generative tasks, including question answering (Rajpurkar et al., 2016a), summarization (Nallapati et al., 2016; Fabbri et al., 2019b), and machine translation (Cettolo et al., 2017a). Nevertheless, these large transformer models have shown high inference latency due to the considerable number of layers and the autoregressive decoding step. As the multiple stacks of transformer layers have to be computed sequentially for each individual token, the inference process poses sig-\n\u2217equal contribution \u2020corresponding authors\nnificant computational burdens and hinders their real-time adaptability (Jiao et al., 2020).\nIn light of the necessity to expedite inference latency, the early-exiting framework (Elbayad et al., 2020; Liu et al., 2021; Schuster et al., 2022) emerges as a promising approach that dynamically allocates computation paths based on the complexity of generation for each token. As illustrated in Figure 1a, tokens that are relatively easy to predict the subsequent token yield consistent predictions with only a few layer computations, while those with higher difficulty require computations across a larger number of layers to generate accurate predictions. In an ideal scenario, the early-exiting method empowers models to achieve notable acceleration in inference without compromising the generation quality when compared to that of a full model.\nHowever, our extensive analysis identified four challenges in the early-exiting framework. Firstly, despite the potential to exit at earlier layers, key and value states for remaining layers are still required for processing subsequent tokens. While previous works have proposed the state copying mechanism (Elbayad et al., 2020; Schuster et al., 2022) to efficiently compute these states by reusing hidden states from the early-exited layer, our findings reveal that this method performs poorly with larger models and longer output sequences (see Section 4.1). Additionally, setting all layers as possible exit positions does not guarantee faster inference due to (1) the defective performance of earlier layers that can generate abnormally long sequence outputs, and (2) the computational overhead from confidence measurement at every layer (see Section 4.2 and 4.3). Lastly, achieving the desired level of latency and accuracy with early-exiting heavily depends on selecting the appropriate confidence threshold for the target task. This often entails significant efforts and additional computational overhead (see Section 4.4). Hence, these challenges call for a new approach that consistently\ndemonstrates high performance and low latency across diverse language models and datasets.\nIn this paper, we introduce a Fast and Robust Early-Exiting (FREE) framework that incorporates a shallow-deep module and synchronized parallel decoding. Our framework not only offers consistent speedup and performance even for larger models and longer output sequences, but also eliminates the need for the computationally expensive process of finding the appropriate exit threshold.\nSpecifically, the shallow-deep module bifurcates the computation paths into a shallow model (with a specified number of early layers) and a deep model (including all layers). Our synchronized parallel decoding accumulates consecutive early-exited tokens that only pass through the shallow model until a non-exiting token is encountered. Thereby, we synchronize the decoding process of the current non-exiting token with the previously stacked tokens, as shown in the left of Figure 1b. This prevents performance degradation by utilizing actual attention computed key and value instead of approximated states through state copying, while it also achieves a more efficient approach compared to decoding each token autoregressively. Furthermore, we devise a novel adaptive threshold estimator, as shown in the right of Figure 1b, by leveraging the fact that parallel decoding outputs predictions even for early-exited tokens from the deep model. This estimator uses a Beta mixture model (BMM) to capture the correlation between confidence scores and prediction alignment of two models, determining the proper confidence threshold for each dataset. In practice, we demonstrate the efficiency of our FREE framework on extensive generation tasks."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Early-exiting Framework",
            "text": "As the size of language models has significantly increased, there have been numerous efforts to develop efficient decoding methods that reduce the computational cost of language generation tasks. Motivated by prior literature (Teerapittayanon et al., 2016; Graves, 2016; Zhang et al., 2019a), Elbayad et al. (2020) introduced an early-exiting framework for faster inference, which dynamically adjusts the depth of the decoder for each token generation by making predictions at an intermediate layer. To achieve the better trade-off between speed and accuracy, Schuster et al. (2022) recently explored confidence thresholding methodologies, including various confidence measures, a decaying threshold function, and a calibration method.\nHowever, their experiments were primarily conducted on small-sized decoder models, necessitating further validation on larger models. In addition, their approaches require additional training time for statistical tests on the extra calibration sets, which prevents them from real deployment scenarios."
        },
        {
            "heading": "2.2 Parallel Decoding",
            "text": "The non-autoregressive decoding, which generates multiple output tokens in parallel, was initially proposed by Gu et al. (2018). Several works (Ghazvininejad et al., 2019; Gu and Kong, 2021; Savinov et al., 2022; Santilli et al., 2023) have since focused on enhancing generation quality in machine translation tasks. Subsequently, Leviathan et al. (2023) introduced speculative decoding for sequence generation tasks. In this approach, an\napproximation model (small size) predicts outputs autoregressively, while a target model (large size) runs in parallel to verify the acceptance of predictions made by the approximation model. With only accepted tokens, they resample the next token from an adjusted distribution. Related approaches have been proposed by Chen et al. (2023) and Kim et al. (2023), where they also utilize two models of varying depth and focus on refining the small model through speculative sampling or a rollback policy in a non-autoregressive manner.\nOur approach is notably distinct from the aforementioned works as we focus on early-exiting framework by introducing synchronized parallel decoding within a single network, which incorporates a shallow-deep module. While we also leverage the advantage of simultaneously obtaining predictions from models of different depth, we rather aim to develop a novel and effective estimation methodology to adaptively determine the optimal threshold for each dataset. It is worth noting that their refining strategies may result in unbounded latency increase as they restart from incorrect predictions."
        },
        {
            "heading": "3 Preliminary",
            "text": "A Transformer network (Vaswani et al., 2017) is composed of L layers, where each layer consists of two sublayers, a multi-head attention (MHA) layer and a feed-forward network (FFN) layer. The computation for hidden states at time step t+1 via stacked Transformer blocks is as follows:\nh\u2113t+1 = Transformer\u2113(h \u2113\u22121 t+1), \u2113 \u2208 [1, L],\nwhere h0t+1 is the embedding layer outputs of yt that represents the generated token at time step t.\nAfter Lth layer of the decoder network, the predicted token y\u0302t+1, is determined by the probability output from a softmax classifier WL:\np(yt+1|hLt+1) = softmax(W \u22ba Lh L t+1)\nHowever, unlike the standard LMs, the earlyexiting framework enables the generation of a subsequent token in earlier layers by using p(yt+1|h\u2113t+1). If the confidence score c\u2113 is larger than the predefined threshold, we can make a prediction at time step t+1 as argmax p(yt+1|h\u2113t+1). While classifiers can be parameterized independently or shared across the L layers, most earlyexiting methods (Elbayad et al., 2020; Liu et al., 2021; Schuster et al., 2022) utilize the shared classifier due to its large number of parameters caused by enormous vocabulary size.\nAfter the current token is early-exited at the \u2113th layer, we need to calculate the key and value states for all deeper blocks in order to perform the self-attention for the subsequent tokens that pass through deeper blocks. For a more efficient approach of caching key and value states, the earlyexiting frameworks employ the state copying mechanism. It duplicates the hidden states of the earlyexited layer (i.e., hit+1 = h \u2113 t+1, \u2200i \u2208 [\u2113 + 1, L]), allowing us to compute the approximate key and value states required for the self-attention of Transformer networks. Schuster et al. (2022) have verified that state copying from lower layers does not have a detrimental effect on performance in the case of small-sized T5 models (Raffel et al., 2020)."
        },
        {
            "heading": "4 Re-evaluating Early-exit Framework",
            "text": "In this section, we present four new findings from our re-evaluation of the early-existing framework. We utilized different model sizes of T5 (Raffel et al., 2020) on SAMSum (Gliwa et al., 2019) and CNN/DailyMail (See et al., 2017), and LongT5-base (Guo et al., 2022) architectures on Multi-News (Fabbri et al., 2019a) and BIGPATENT (Sharma et al., 2019)."
        },
        {
            "heading": "4.1 Lack of Robustness to Model Size and Output Sequence Length",
            "text": "We first re-evaluate the state copying mechanism which is an essential component of the early-exiting framework. Following Schuster et al. (2022), we use an oracle confidence measure that enables tokens to exit at the earliest layer, such that their predictions are identical to those of the final layer. Notably, as observed in Table 1, the degradation of the generation quality with the state copying gets severe on larger models and datasets with the longer sequence (\u25b7 Obs. 1). For instance, when considering the oracle-exiting results, the T5-small model\ndemonstrates the degradation of only 0.67 on the SAMSum dataset, whereas the T5-large model experiences a much larger drop of 1.24. Similarly, on datasets such as Multi-News and BIGPATENT, which consist of relatively long output sequences, the oracle-exiting results exhibit a decrease of 7.99 and 4.69, respectively.\nTo strengthen the supporting evidence, we further discover the substantial variation in the distribution of hidden states across different layers. In Table 1, we also reported cosine similarity between the hidden states of the final layer and the oracleexited layer. Even though the hidden states of the final and oracle-exited layers yield the same predictions, the cosine similarity between them decreases significantly as the decoder network gets larger and the output sequences become longer."
        },
        {
            "heading": "4.2 Performance Drop by Exit Position",
            "text": "To facilitate early-exiting for all decoder layers, the training objectives need to be a combination of the training objectives for each individual layer. We can present as follows:\nL = L\u2211 i=1 \u03b1iLi where \u2211 i \u03b1i = 1, (1)\nLi and \u03b1i is negative log-likelihood loss function and weight coefficient for ith layer, respectively. Especially, previous work set \u03b1i as 1/L (unweighted average; Elbayad et al. 2020) or i/ \u2211 i i (weighted average; Schuster et al. 2022). They demonstrated that these weighting rules effectively facilitate learning in earlier layers without compromising the overall performance of the full model on smallsized decoder models.\nHowever, as shown in Figure 2, we observed a notable decrease in the performance of staticexiting, which utilizes the same number of layers for all tokens, when utilizing only a small portion of the early layers from the T5-large model. (\u25b7 Obs. 2). For instance, if all tokens are exited in the first or second layers, the model achieved nearly zero ROUGE-L scores. Furthermore, when we apply the early-exiting framework to these models during inference, we verified that the T5-large model generates abnormally long sentences, actually consuming more inference time. Based on these results, in the subsequent experiments, we have excluded the first two or four layers from the candidates for early-exiting layers of base and large models, respectively."
        },
        {
            "heading": "4.3 Non-negligible Computational Cost",
            "text": "During our analysis, we observed that the conventional early-exiting framework not only presents performance disadvantages but also poses challenges for inference latency. In Figure 3, we conducted a breakdown of the computational costs associated with a decoder model across three summarization datasets. Surprisingly, early-exiting has often shown an unexpected increase in total decoding time when compared to the baseline model without using early-exiting.\nThis can be attributed to the non-negligible computational cost involved in measuring confidence at each layer, particularly due to the softmax operations with the large vocabulary size. In addition, although the state copying method aims to reduce computation time in the MHA and FFN layers of the remaining layers, the computation of key and value states using duplicated hidden states incurs additional non-negligible overhead (\u25b7 Obs. 3)."
        },
        {
            "heading": "4.4 Disparate Optimal Confidence Threshold",
            "text": "Determining the appropriate threshold for exit confidence is a crucial challenge in the early-exiting framework as it directly impacts the trade-off between performance and latency (Zhang et al., 2019b; Schuster et al., 2022). As summarized in Table 2, our observations indicate that the optimal confidence thresholds for achieving the lowest latency in the same performance significantly vary across datasets (\u25b7 Obs. 4). For instance, SQuAD and CNN/DailyMail datasets can maintain performance with relatively lower exit thresholds, whereas higher threshold values are required in the case of the IWSLT dataset. Previous work (Schuster et al., 2022) has leveraged distribution-free risk control techniques for confident generations. However, these methods require additional training time for statistical tests on the extra calibration set before the deployment, where time can be also influenced by the size of the threshold candidate sets."
        },
        {
            "heading": "5 Novel Early-Exiting Framework: FREE",
            "text": "Building upon the discoveries in Section 4, we introduce a Fast and Robust Early-Exiting framework named FREE, leveraging a shallow-deep module and capitalizing on the structure of parallel decoding. Furthermore, we present a confidence estimation algorithm designed to enhance the robustness of early-exiting within the FREE framework."
        },
        {
            "heading": "5.1 Shallow-Deep Module",
            "text": "We present an effective shallow-deep module, which strategically assigns a predetermined number of early layers (LS) as a shallow model, while all the layers as a deep model. This module tackles the performance degradation associated with\nco-training numerous exiting layers in the conventional early-exiting framework.\nTo enhance the performance of the shallow model, we exploit layerwise knowledge distillation (KD) as an additive loss term to Eq. (1) with \u03b1Ls = Ls/(L+ Ls) and \u03b1L = L/(L+ Ls):\nLKD = 1\n|LS | LS\u2211 i=1 MSE(HiS ,H m(i) D ),\nwhere m(i) indicates the layer in the deep model that extracts knowledge into the corresponding layer i of the shallow model. HS and HD are hidden states from shallow and deep models.\nWe have experimented with the distillation from the last layer (KD-last; Wang et al. 2020; Ko et al. 2023), from fixed uniform mapped layers (KD-unif; Jiao et al. 2020; Park et al. 2021), and from dynamically mapped layers (KD-dyna; Xia et al. 2022). Especially, dynamic mapping function allows us to align each deep model layer with its closest counterpart in the shallow model:\nm(i) = argmin j\nMSE(HiS ,H j D)\nwhere j denotes the layer indices of the deep model selected by the total number of LS , and the condition of m(1) \u2264 \u00b7 \u00b7 \u00b7 \u2264 m(LS) should be satisfied. Based on the consistently superior performance of KD-dyna loss (see Appendix D.2), we utilized it for all experiments with the shallow-deep module."
        },
        {
            "heading": "5.2 Synchronized Parallel Decoding",
            "text": "We present synchronized parallel decoding as an alternative to the state copying mechanism, which is a key component of the conventional early-exiting framework but can lead to a significant performance decrease, as demonstrated in Section 4.1. In contrast to traditional approaches that have multiple exit points, our method incorporates the shallowdeep module, enabling us to stack consecutive\nearly-exited tokens in the shallow model until a non-exiting token is encountered. When decoding the token with the deep model, we enhance efficiency and effectiveness through parallel decoding, synchronously computing the key and value states of previously stacked tokens. The example of the parallel decoding process is depicted in Figure 4.\nThe underlying principle of this approach is to leverage the enhanced parallelism offered by modern hardware accelerators. This allows for efficient computations to be carried out simultaneously on the large number of sequences. Thus, by employing synchronized parallel decoding, we can directly compute multiple hidden states similar to a single token processing time. Besides, this can eliminate the potential performance degradation that may arise from inaccurate approximations of hidden states resulting from the state copying mechanism."
        },
        {
            "heading": "5.3 Adaptive Threshold Estimation",
            "text": "We propose a novel adaptive threshold estimation method that updates the threshold to be retailed for different datasets. Unlike the previous methods that utilize extra calibration sets (Schuster et al., 2022), we quickly adapt the threshold by using the information of early-stage instances, regardless of the initial threshold values. Especially, during parallel decoding, we collect samples to evaluate the correspondence between the confidence scores of the shallow model and the prediction alignment between shallow and deep models.\nAs depicted in Figure 1b, we observe that when the predictions of the deep and shallow models are identical, the confidence tends to skew towards one, otherwise it skews towards zero. To model this skewed distribution over [0, 1], we utilize a beta mixture model (BMM; Ma and Leijon 2011) due to its flexibility and the appropriate support set of the beta distribution. The probability density function of beta distribution over x \u2208 [0, 1] is defined as:\np(x|\u03b1, \u03b2) = \u0393(\u03b1+ \u03b2) \u0393(\u03b1)\u0393(\u03b2) x\u03b1\u22121(1\u2212 x)\u03b2\u22121\nThe parameters of the BMM are updated using the maximum likelihood estimator (MLE; Norden 1972) with observed data points.\n\u03b1k = c\u0304k\n( c\u0304k(1\u2212c\u0304k)\ns2k \u2212 1\n) , \u03b2k =\n\u03b1k(1\u2212c\u0304k) c\u0304k , (2)\nwhere c\u0304k being a average of the confidence {cLsi } |Dc| i=1 for corresponding k. k is set to 1 if the predictions of the two models are identical, and 0\nAlgorithm 1 Adaptive Threshold Estimation Input: empty calibration dataset Dc, initial confidence threshold \u03bb0c , posterior condition \u03b6, update number T Output: updated confidence threshold \u03bbc\n1: initialize t\u2190 0, \u03bbc \u2190 \u03bb0c 2: while t \u2264 T do 3: Generate tth sentence with Nt tokens 4: /* Update Dc */ 5: Dc \u2190 Dc \u222a {cLsi , I(y\u0302 Ls i = y\u0302 L i )} Nt i=1 6: /* Find threshold with Eq.(2)-(4)*/ 7: \u03b1k, \u03b2k \u2190 MLEBMM(Dc) for k \u2208 {0, 1} 8: \u03bbc \u2190 argmin\u03bb:p(k=1|\u03bb)\u2265\u03b6 \u03bb 9: update t\u2190 t+ 1\n10: end while\notherwise. Similarly, sk is the standard deviation of confidence of related k.\nc\u0304k = \u2211N i=1 \u03b3ic Ls i\u2211N\ni=1 \u03b3i , s\u03042k =\n\u2211N i=1 \u03b3i(c Ls i \u2212c\u0304k)\n2\u2211N i=1 \u03b3i , (3)\nwhere \u03b3i := I(y\u0302Lsi = y\u0302 L i ) denote whether the prediction of two models are same. After updating the BMM, we find an appropriate threshold for future tokens by identifying the point at which the posterior probability, defined as below, reaches \u03b6:\np(k = 1|\u03bbc) = p(k=1)p(\u03bbc|\u03b11,\u03b21)\u2211 j\u2208{0,1} p(k=j)p(\u03bbc|\u03b1j ,\u03b2j) . (4)\nHere, as we observe the severe imbalance between the case of k = 0 and 1, we restrict the prior value of each class to 0.5 for the balance between two cases (i.e., p(k = j) = 0.5 \u2200j). As this restriction makes us to use a smaller value of \u03b6, we na\u00efvely set it as 0.4. A detailed algorithm can be found in Algorithm 1."
        },
        {
            "heading": "6 Experiments",
            "text": ""
        },
        {
            "heading": "6.1 Experimental Setup",
            "text": "We conducted experiments on various sequence modeling tasks, including question answering (SQuAD; Rajpurkar et al. 2016b), machine translation (IWSLT 2017 En-De; Cettolo et al. 2017b), and text summarization tasks using SAMSum, CNN/DailyMail, Multi-News, and BIGPATENT datasets. The LongT5-base model was used for the Multi-News and BIGPATENT datasets, while the T5-large model was used for the other datasets. All implementations are based on PyTorch using Huggingface (Wolf et al., 2020; Lhoest et al., 2021). Further details can be found in Appendix B.\nTable 3: Comparison between early-exiting frameworks on various datasets. For CALM and FREE\u2020, we reported the performance using the smallest threshold value that achieves 99% performance of the full model, fine-tuned by weighted average or KD-dyna losses, respectively. The parentheses denote relative speedup based on the first row.\nSUM QA MT\nMethod SAMSum CNN/DailyMail Multi-News BIGPATENT SQuAD IWSLT De-En\nFull Model 48.82 (\u00d71.00) 41.15 (\u00d71.00) 37.62 (\u00d71.00) 49.68 (\u00d71.00) 90.63 (\u00d71.00) 39.19 (\u00d71.00) CALM 48.37 (\u00d70.72) 40.78 (\u00d70.86) 37.27 (\u00d70.85) 49.21 (\u00d70.65) 90.09 (\u00d72.03) 39.19 (\u00d71.00) Full Model 49.11 (\u00d71.00) 41.09 (\u00d71.00) 39.20 (\u00d71.00) 49.68 (\u00d71.00) 91.90 (\u00d71.00) 39.39 (\u00d71.00) FREE\u2020 48.65 (\u00d71.50) 40.89 (\u00d71.80) 38.93 (\u00d71.07) 49.51 (\u00d71.62) 91.31 (\u00d72.76) 39.04 (\u00d71.07) FREE 48.66 (\u00d71.47) 40.99 (\u00d71.65) 38.66 (\u00d71.23) 49.47 (\u00d71.58) 91.82 (\u00d72.16) 38.17 (\u00d71.18)"
        },
        {
            "heading": "6.2 Experimental Results",
            "text": "In order to investigate the effect of the individual component of our proposed framework, we evaluate both FREE without and with an adaptive threshold estimator, denoted as FREE\u2020 and FREE.\nOverall performance. In Figure 5, we present a comparison of the quality of generated output (ROUGE-L) and the inference latency between the FREE framework and baselines, including staticexiting and the conventional early-exiting method (CALM; Schuster et al. 2022). CALM method exhibited poorer performance compared to a simple static-exiting approach on all datasets, likely due to the state copying mechanism and the presence of numerous exit positions, as observed in Section 4. In contrast, FREE\u2020 demonstrated robust performance and the larger AUC (area under the curve) across datasets by adjusting exit thresholds.\nAdaptive threshold evaluation. In the earlyexiting framework, choosing the appropriate confidence threshold is crucial for achieving the best trade-off between generation quality and latency. Unlike previous calibration methods (Schuster et al., 2022) that require an extra calibration set\nand training time, our methodology effectively addresses this challenge by leveraging the byproduct of parallel decoding. As summarized in Table 3, FREE with adaptive threshold estimation successfully achieved significant speedup, by up to \u00d72.16, when preserving the 99% of full model performance. Furthermore, in Figure 5, the estimated threshold demonstrated nearly the maximum achievable speed improvement without sacrificing performance, represented by red stars.\nLarge language models. Recently, various studies (Dettmers et al., 2022; Xiao et al., 2023; Leviathan et al., 2023; Liu et al., 2023b) have\naimed at boosting the inference speed of large language models (LLMs). To validate the applicability of the FREE framework on LLMs, we conducted experiments utilizing the T5-3B model (Raffel et al., 2020) on the SAMSum and CNN/DailyMail datasets. Due to substantial computational overhead, we utilized the LoRA adapter (Hu et al., 2022), targeting both self-attention and feed-forward layers with a rank of 64. Figure 6 summarized a comprehensive comparison of earlyexiting methods. Our method maintained superiority over the baselines in terms of latency and ROUGE-L scores, showing the consistent performance trend observed in the T5-large model. Thus, we are confident that our proposed framework would demonstrate consistent level of inference acceleration, even with larger language models."
        },
        {
            "heading": "6.3 Ablation Study",
            "text": "Different depth of shallow model. In Table 4, we also ablate on the number of layers for the shallow model to observe the trade-offs. While our method demonstrated a trend towards higher speedup gains as the depth of the shallow model decreases, we experienced some decreases in performance and speed gain when the depth of the model is reduced too much (e.g., four layers). We assumed that this is due to incorrect and redundant output sentences, similarly observed in the conventional early-exiting framework. Consequently, with enough depth (e.g., six layers), FREE consistently showed robust performance and inference speedup.\nRobustness of parallel decoding. In order to verify the robustness of our decoding mechanism, we conducted a comparative analysis between synchronized parallel decoding (SPD) and state copying (SC), both implemented with the shallow-deep\nmodule. Synchronized parallel decoding consistently outperformed state copying across all three datasets by much higher ROUGE-L metrics, as summarized in Table 5. This improvement can be attributed to the updated hidden states that are obtained through the accurate computation of Transformer layers during parallel decoding. These findings suggest that our efficient decoding method for early-exited tokens can enhance the overall performance of the early-exiting framework as well.\nDependency on size of calibration set. By using the early-stage instances as the calibration set, we iteratively update the adaptive confidence threshold to converge to the appropriate value. Here, we have observed the sample efficiency of the adaptive threshold estimator by varying the sizes of this calibration set. Interestingly, even with only 3% of the total samples, our estimator can approximate the threshold, measured by the full sample set, as shown in Table 6. This ensures minimal additional computation time required for threshold estimation.\nRefining shallow model predictions. Prior works (Leviathan et al., 2023; Chen et al., 2023; Kim et al., 2023) have proposed refinement methods to correct erroneous outputs from an approximation model. Specifically, when a wrong token is detected in previous sequences, they remove all subsequently generated tokens and restart the generation process from that point. In Table 7, we conducted experiments in order to evaluate the effects of this refinement method (Kim et al., 2023)\nin our early-exiting framework. We observed that when the refinement threshold is set low, allowing for more correction by the deep model, the performance improvement is minimal compared to the increase in latency. Our findings suggest that these approaches that cannot guarantee an upper bound on latency increase may not be well-suited for integration into the early-exiting framework.\nHuman-like summarization evaluation. Recent studies (Gao et al., 2023; Liu et al., 2023a; Zhang et al., 2023) have argued that existing summarization evaluation metrics like ROUGE-L do not accurately represent the true summarization capabilities. Instead, they explored the human-like evaluation using LLMs based on their strong correlation with human judgment. Thereby, we conducted two human-like evaluation methods, Likert scale scoring and pairwise comparison (Gao et al., 2023), using ChatGPT API (gpt-3.5-turbo-0613). We compared a full model and our FREE framework on 100 instances, randomly drawn from the CNN/DailyMail dataset. Figure 7 and 8 provide the templates used for each evaluation task. For the full model, we observed scores of [4.73, 3.83, 3.87, 3.77], while our FREE method returned scores of [4.68, 3.84, 3.84, 3.72] across the four dimensions. Besides, the win counts for each method were 101 and 99, respectively. Given ROUGE-L scores of 41.09 (\u00d71.00) for the full model and 40.99 (\u00d71.65) for the FREE method, our method is certainly capable of yielding predictions of similar quality, while notably reducing computational overhead."
        },
        {
            "heading": "7 Conclusion",
            "text": "We proposed FREE framework to address the challenges of conventional early-exiting frameworks for autoregressive language models. Our approach incorporates three key components: (1) shallowdeep module, (2) synchronized parallel decoding,\nand (3) adaptive threshold estimation. Through extensive experiments on various generation tasks, we empirically demonstrated the superior performance of FREE framework, achieving significant acceleration in latency without compromising the quality of the generated output.\nLimitations. Our work addressed a fast and robust existing framework that can be efficiently utilized without concerns about performance degradation. However, our approach does have a few limitations which we discuss below: (1) Our method requires additional computational resources to finetune the shallow model. However, as we have demonstrated, parameter-efficient fine-tuning methods would be a promising solution to overcome this limitation. (2) While our work demonstrates robustness in the depth of the shallow model, further investigation is required to determine the appropriate depth for various language models. This aspect remains an area for additional research.\nAcknowledgement. This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by Korea government (MSIT) [No. 2021-0-00907, Development of Adaptive and Lightweight EdgeCollaborative Analysis Technology for Enabling Proactively Immediate Response and Rapid Learning, 90%] and [No. 2019-0-00075, Artificial Intelligence Graduate School Program (KAIST), 10%]."
        },
        {
            "heading": "A Dataset Description",
            "text": "We apply FREE on various generation tasks including summarization, question answering, and machine translation. We provide detailed descriptions of the datasets used.\n\u2022 SAMSum (Summarization): SAMSum (Gliwa et al., 2019) consists of 16K messenger-like conversations that are annotated with a summary for providing a concise overview of the conversation\u2019s content in the third person.\n\u2022 CNN/DailyMail (Summarization): CNN/ DailyMail (See et al., 2017) consists of over 300K English news articles that were originally designed for machine-reading and comprehension as well as abstractive question answering, but it now also supports extractive and abstractive summarization.\n\u2022 Multi-News (Summarization): Multi-News (Fabbri et al., 2019a) comprises 45K news articles and corresponding summaries, where each summary is professionally crafted and provides links to the original articles referenced.\n\u2022 BIGPATENT (Summarization): BIGPATENT (Sharma et al., 2019) contains 1.3M records of U.S. patent documents, each accompanied by abstractive summaries written by humans. In our work, we specifically focus on the Fixed Constructions category, which is one of the nine classification categories available in the dataset.\n\u2022 SQuAD (Question Answering): The Stanford Question Answering (SQuAD, Rajpurkar et al. 2016b) is a collection of 87.6K reading comprehension tasks. It includes questions generated by crowd workers based on a set of Wikipedia articles.\n\u2022 IWSLT 2017 (Machine Translation): IWSLT 2017 (Cettolo et al., 2017b) addresses text translation, using a single machine translation (MT) system for multiple language directions such as English and German. Here, we specifically focus on a German-to-English translation task."
        },
        {
            "heading": "B Detailed Experimental Setup",
            "text": "Training hyperparameters. In this section, we describe the detailed hyperparameter values for our work. We utilize the NVIDIA RTX 3090 GPUs for training the language models, and we summarize\nthe training configuration in Table 8. For all dataset, we use AdaFactor (Shazeer and Stern, 2018) optimizer with the learning rate of 1e-4. For the adaptive threshold estimation, we set the initial threshold value \u03bb0c as 0.9, \u03b6 as 0.4, T as 3% of total sample number (refer to Algorithm 1).\nPerformance metrics. To numerically measure the output quality of our method, we utilize the F1 score for SQuAD, BLEU score (Papineni et al., 2002) for IWSLT2017, and ROUGE score (Lin, 2004) for the four summarization tasks.\nC Inference Latency Evaluation\nFor measuring inference speed, we execute 500 inference predictions for each dataset under each examined configuration in PyTorch (Paszke et al., 2019) compiled function in a single server with a single NVIDIA GeForce RTX 3039 GPU and 12th Gen Intel(R) Core(TM) i7-12700K CPU. For each inference prediction, we use batch size 1, which is a common use case for online serving (Schuster et al., 2022). Also, we use to generate output sequences through greedy sampling with a beam size of 1. We measure the time including all decoding steps until completion."
        },
        {
            "heading": "D Additional Experimental Results",
            "text": "In this section, we provide additional experimental results to demonstrate the effectiveness of our proposed method and its individual components.\nD.1 Performance on Different Datasets\nIn this section, we present a comparison of the quality of the generated output (F1 or BLEU) and the inference latency on the SQuAD and IWSLT 2017 datasets, similar to the experiments in Figure 5. Figure 9 illustrates that both FREE\u2020 and FREE consistently outperform the CALM and static-exiting baselines in the SQuAD dataset, which aligns with our previous findings.\nHowever, their performance advantages in the IWSLT dataset are slightly reduced compared to other datasets. This can be attributed to the larger vocabulary size of mT5 compared to T5, resulting in longer processing times for the confidence measurement. The CALM approach, which also utilizes large linear classifiers, exhibits much lower\neffectiveness in this dataset as well. We believe that this challenge, regarding the large vocabulary size, can be mitigated by employing a vocabulary sizeindependent confidence measure that proposed in previous work (Schuster et al., 2022). Nonetheless, our proposed algorithm still outperforms the other baselines on various datasets.\nD.2 Layerwise Knowledge Distillation\nGiven the only two exit positions in our shallowdeep module, since their performance significantly impacts the overall robustness of the early-exiting approach, we carefully design the loss function for training. In Figure 10, we observed the performance trends of four different loss functions as we varied the exit thresholds. While the differences are not significant, the KD-dyna loss demonstrates better trade-offs compared to a weighted average or other KD-based losses. Specifically, the lower performance of KD-unif on the SAMSum dataset suggests that dynamically determining the layer mapping can facilitate more effective knowledge transfer between the deep and shallow models. Consequently, we trained our shallow-deep module using the KD-dyna loss for all experiments, and left the exploration of additional loss functions, such as contrastive distillation losses (Tian et al., 2019; Bae et al., 2021), for future work.\nD.3 Comparison with Small-sized Models\nWe conducted a comparison between the inference speed of FREE using T5-large model and a directly trained T5-base model. To ensure a fair comparison, we manually selected the appropriate confidence threshold for FREE\u2020 (without relying on an adaptive threshold estimator) to align its performance closely with that of T5-base. The results, presented in Table 9, demonstrate that our proposed method exhibited a competitive speedup in inference performance on the CNN/DailyMail dataset. Moreover, it demonstrated a superior F1 score and significantly higher speedup on the SQuAD dataset.\nWe believe that the variance in speedup across the datasets can be attributed to the performance achievable by a directly trained smaller model, as well as a shallow model within the FREE framework. In the case of SQuAD, the T5-base model (12 layers) achieved a ROUGE-L score of 90.50, whereas a shallow model (6 layers) of our FREE framework yielded a similar score of 90.24. Our method effectively leverage these inherent benefits, thereby facilitating the inference speedup through exiting at lower layers.\nD.4 Various Decoding Strategies To evaluate the applicability of FREE on various decoding methods, we conducted experiments with top-k sampling (Radford et al., 2019) and nucleus sampling (top-p sampling; Holtzman et al. 2020). top-k sampling samples the next word from the top k most probable choices, instead of aiming to decode text that maximizes likelihood. On the other hand, nucleus sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. As detailed in Table 10, FREE method exhibited consistent and robust performance while achieving a larger speedup compared to CALM. These results affirm that our FREE framework can be widely applied, irrespective of the chosen decoding method."
        }
    ],
    "title": "Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding",
    "year": 2023
}