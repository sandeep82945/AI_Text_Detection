{
    "abstractText": "Much work has explored lexical and semantic variation in online communities, and drawn connections to community identity and user engagement patterns. Communities also express identity through the sociolinguistic concept of stancetaking. Large-scale computational work on stancetaking has explored community similarities in their preferences for stance markers \u2013 words that serve to indicate aspects of a speaker\u2019s stance \u2013 without considering the stance-relevant properties of the contexts in which stance markers are used. We propose representations of stance contexts for 1798 Reddit communities and show how they capture community identity patterns distinct from textual or marker similarity measures. We also relate our stance context representations to broader interand intra-community engagement patterns, including cross-community posting patterns and social network properties of communities. Our findings highlight the strengths of using rich properties of stance as a way of revealing community identity and engagement patterns in online multi-community spaces.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jai Aggarwal"
        },
        {
            "affiliations": [],
            "name": "Brian Diep"
        },
        {
            "affiliations": [],
            "name": "Julia Watson"
        },
        {
            "affiliations": [],
            "name": "Suzanne Stevenson"
        }
    ],
    "id": "SP:e11e19558f47975220fcca5e722235d085ea687e",
    "references": [
        {
            "authors": [
                "Jason Baumgartner",
                "Savvas Zannettou",
                "Brian Keegan",
                "Megan Squire",
                "Jeremy Blackburn."
            ],
            "title": "The Pushshift Reddit Dataset",
            "venue": "Proceedings of the international AAAI conference on web and social media, volume 14, pages 830\u2013839.",
            "year": 2020
        },
        {
            "authors": [
                "Erin D Bennett",
                "Noah D Goodman."
            ],
            "title": "Extremely costly intensifiers are stronger than quite costly ones",
            "venue": "Cognition, 178:147\u2013161.",
            "year": 2018
        },
        {
            "authors": [
                "Douglas Biber",
                "Edward Finegan."
            ],
            "title": "Styles of stance in English: Lexical and grammatical marking of evidentiality and affect",
            "venue": "Text-interdisciplinary journal for the study of discourse, 9(1):93\u2013124.",
            "year": 1989
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Lisa Green",
                "Brendan O\u2019Connor"
            ],
            "title": "Demographic dialectal variation in social media: A case study of African-American English. arXiv preprint arXiv:1608.08868",
            "year": 2016
        },
        {
            "authors": [
                "Axel Bohmann",
                "Wiebke Ahlers."
            ],
            "title": "Stance in narration: Finding structure in complex sociolinguistic variation",
            "venue": "Journal of Sociolinguistics, 26(1):65\u201383.",
            "year": 2022
        },
        {
            "authors": [
                "Gemma Boleda",
                "Sabine Schulte im Walde",
                "Toni Badia."
            ],
            "title": "Modeling regular polysemy: A study on the semantic classification of Catalan adjectives",
            "venue": "Computational Linguistics, 38(3):575\u2013616.",
            "year": 2012
        },
        {
            "authors": [
                "Dwight Bolinger."
            ],
            "title": "Degree words",
            "venue": "Degree Words. De Gruyter Mouton.",
            "year": 1972
        },
        {
            "authors": [
                "Mary Bucholtz"
            ],
            "title": "From stance to style: Gender, interaction, and indexicality in Mexican immigrant youth slang",
            "year": 2009
        },
        {
            "authors": [
                "Mary Bucholtz",
                "Kira Hall."
            ],
            "title": "Identity and interaction: A sociocultural linguistic approach",
            "venue": "Discourse studies, 7(4-5):585\u2013614.",
            "year": 2005
        },
        {
            "authors": [
                "John D Burger",
                "John Henderson",
                "George Kim",
                "Guido Zarrella."
            ],
            "title": "Discriminating gender on twitter",
            "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1301\u20131309.",
            "year": 2011
        },
        {
            "authors": [
                "Jenny Cheshire."
            ],
            "title": "Variation in an English dialect: A sociolinguistic study",
            "venue": "Cambridge Studies in Linguistics London, 37.",
            "year": 1982
        },
        {
            "authors": [
                "Cristian Danescu-Niculescu-Mizil",
                "Moritz Sudhof",
                "Dan Jurafsky",
                "Jure Leskovec",
                "Christopher Potts."
            ],
            "title": "A computational approach to politeness with application to social factors",
            "venue": "arXiv preprint arXiv:1306.6078.",
            "year": 2013
        },
        {
            "authors": [
                "Marco Del Tredici",
                "Raquel Fern\u00e1ndez."
            ],
            "title": "Semantic variation in online communities of practice",
            "venue": "Proceedings of the 12th International Conference on Computational Semantics.",
            "year": 2017
        },
        {
            "authors": [
                "Marco Del Tredici",
                "Raquel Fern\u00e1ndez",
                "Gemma Boleda."
            ],
            "title": "Short-term meaning shift: A distributional exploration",
            "venue": "arXiv preprint arXiv:1809.03169.",
            "year": 2018
        },
        {
            "authors": [
                "Thomas G Dietterich."
            ],
            "title": "Approximate statistical tests for comparing supervised classification learning algorithms",
            "venue": "Neural computation, 10(7):1895\u20131923.",
            "year": 1998
        },
        {
            "authors": [
                "John W Du Bois."
            ],
            "title": "Stance and consequence",
            "venue": "Annual Meeting of the American Anthropological Association, New Orleans.",
            "year": 2002
        },
        {
            "authors": [
                "John W Du Bois."
            ],
            "title": "The stance triangle",
            "venue": "Stancetaking in discourse: Subjectivity, evaluation, interaction, 164(3):139\u2013182.",
            "year": 2007
        },
        {
            "authors": [
                "Penelope Eckert."
            ],
            "title": "Language variation as social practice: The linguistic construction of identity in Belten High",
            "venue": "Wiley-Blackwell.",
            "year": 2000
        },
        {
            "authors": [
                "Penelope Eckert"
            ],
            "title": "Variation and the indexical field",
            "year": 2008
        },
        {
            "authors": [
                "Penelope Eckert",
                "Sally McConnell-Ginet."
            ],
            "title": "Think practically and look locally: Language and gender as community-based practice",
            "venue": "Annual review of anthropology, 21(1):461\u2013488.",
            "year": 1992
        },
        {
            "authors": [
                "William L Hamilton",
                "Kevin Clark",
                "Jure Leskovec",
                "Dan Jurafsky."
            ],
            "title": "Inducing domain-specific sentiment lexicons from unlabeled corpora",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 595\u2013605.",
            "year": 2016
        },
        {
            "authors": [
                "Jack Hessel",
                "Chenhao Tan",
                "Lillian Lee."
            ],
            "title": "Science, askscience, and badscience: On the coexistence of highly related communities",
            "venue": "Proceedings of the international AAAI conference on web and social media, volume 10, pages 171\u2013180.",
            "year": 2016
        },
        {
            "authors": [
                "Janet Holmes",
                "Miriam Meyerhoff."
            ],
            "title": "The community of practice: Theories and methodologies in language and gender research",
            "venue": "Language in society, 28(2):173\u2013183.",
            "year": 1999
        },
        {
            "authors": [
                "Abraham Israeli",
                "Shani Cohen",
                "Oren Tsur."
            ],
            "title": "Unsupervised discovery of non-trivial similarities between online communities",
            "venue": "Expert Systems with Applications, 206:117900.",
            "year": 2022
        },
        {
            "authors": [
                "Rika Ito",
                "Sali Tagliamonte."
            ],
            "title": "Well weird, right dodgy, very strange, really cool: Layering and recycling in English intensifiers",
            "venue": "Language in society, 32(2):257\u2013279.",
            "year": 2003
        },
        {
            "authors": [
                "Alexandra Jaffe."
            ],
            "title": "Stance: Sociolinguistic Perspectives",
            "venue": "Oxford University Press.",
            "year": 2009
        },
        {
            "authors": [
                "Dan Jurafsky",
                "Elizabeth Shriberg",
                "Barbara Fox",
                "Traci Curl."
            ],
            "title": "Lexical, prosodic, and syntactic cues for dialog acts",
            "venue": "Discourse Relations and Discourse Markers.",
            "year": 1998
        },
        {
            "authors": [
                "Elise K\u00e4rkk\u00e4inen."
            ],
            "title": "Stance taking in conversation: From subjectivity to intersubjectivity",
            "venue": "Text & talk, 26(6):699\u2013731.",
            "year": 2006
        },
        {
            "authors": [
                "Scott F Kiesling."
            ],
            "title": "Dude",
            "venue": "American speech, 79(3):281\u2013305.",
            "year": 2004
        },
        {
            "authors": [
                "Scott F Kiesling."
            ],
            "title": "Style as stance",
            "venue": "Stance: sociolinguistic perspectives, pages 171\u2013194.",
            "year": 2009
        },
        {
            "authors": [
                "Scott F Kiesling",
                "Umashanthi Pavalanathan",
                "Jim Fitzpatrick",
                "Xiaochuang Han",
                "Jacob Eisenstein."
            ],
            "title": "Interactional stancetaking in online forums",
            "venue": "Computational Linguistics, 44(4):683\u2013718.",
            "year": 2018
        },
        {
            "authors": [
                "Wang Ling",
                "Chris Dyer",
                "Alan W Black",
                "Isabel Trancoso."
            ],
            "title": "Two/too simple adaptations of word2vec for syntax problems",
            "venue": "Proceedings of the 2015 conference of the North American chapter of the Association for Computational Linguistics: Human",
            "year": 2015
        },
        {
            "authors": [
                "Li Lucy",
                "David Bamman."
            ],
            "title": "Characterizing English variation across social media communities with BERT",
            "venue": "Transactions of the Association for Computational Linguistics, 9:538\u2013556.",
            "year": 2021
        },
        {
            "authors": [
                "Li Lucy",
                "Julia Mendelsohn."
            ],
            "title": "Using sentiment induction to understand variation in gendered online communities",
            "venue": "Proceedings of the Society for Computation in Linguistics (SCiL) 2019, pages 156\u2013166.",
            "year": 2019
        },
        {
            "authors": [
                "Yiwei Luo",
                "Dan Jurafsky",
                "Beth Levin."
            ],
            "title": "From insanely jealous to insanely delicious: Computational models for the semantic bleaching of English intensifiers",
            "venue": "Proceedings of the 1st International Workshop on Computational Approaches to Histori-",
            "year": 2019
        },
        {
            "authors": [
                "Lesley Milroy."
            ],
            "title": "Language and social networks, volume 2",
            "venue": "Wiley-Blackwell.",
            "year": 1987
        },
        {
            "authors": [
                "Saif Mohammad."
            ],
            "title": "Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 English words",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (volume 1: Long papers), pages 174\u2013184.",
            "year": 2018
        },
        {
            "authors": [
                "Saif M Mohammad."
            ],
            "title": "Ethics sheet for automatic emotion recognition and sentiment analysis",
            "venue": "Computational Linguistics, 48(2):239\u2013278.",
            "year": 2022
        },
        {
            "authors": [
                "Claude Nadeau",
                "Yoshua Bengio."
            ],
            "title": "Inference for the generalization error",
            "venue": "Machine Learning, 52:239\u2013 281.",
            "year": 2003
        },
        {
            "authors": [
                "Dong Nguyen",
                "Rilana Gravel",
                "Dolf Trieschnigg",
                "Theo Meder."
            ],
            "title": "How old do you think I am?\u201d A study of language and age in Twitter",
            "venue": "Proceedings of the International AAAI Conference on Web and Social Media, volume 7, pages 439\u2013448.",
            "year": 2013
        },
        {
            "authors": [
                "Bill Noble",
                "Jean-Philippe Bernardy."
            ],
            "title": "Conditional language models for community-level linguistic variation",
            "venue": "Proceedings of the Fifth Workshop on Natural Language Processing and Computational Social Science (NLP+ CSS), pages 59\u201378.",
            "year": 2022
        },
        {
            "authors": [
                "Elinor Ochs."
            ],
            "title": "Constructing social identity: A language socialization perspective",
            "venue": "Research on language and social interaction, 26(3):287\u2013306.",
            "year": 1993
        },
        {
            "authors": [
                "Umashanthi Pavalanathan",
                "Jim Fitzpatrick",
                "Scott F Kiesling",
                "Jacob Eisenstein."
            ],
            "title": "A multidimensional lexicon for interpersonal stancetaking",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Ellie Pavlick",
                "Joel Tetreault."
            ],
            "title": "An empirical analysis of formality in online communication",
            "venue": "Transactions of the Association for Computational Linguistics, 4:61\u201374.",
            "year": 2016
        },
        {
            "authors": [
                "Hans Peters."
            ],
            "title": "Degree adverbs in Early Modern English",
            "venue": "Studies in early modern English, 13:269\u2013 288.",
            "year": 1994
        },
        {
            "authors": [
                "Mirka Rauniomaa."
            ],
            "title": "Stance accretion",
            "venue": "Language, Interaction, and Social Organization Research Focus Group, University of California, Santa Barbara, February.",
            "year": 2003
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "arXiv preprint arXiv:1908.10084.",
            "year": 2019
        },
        {
            "authors": [
                "James A Russell."
            ],
            "title": "Core affect and the psychological construction of emotion",
            "venue": "Psychological review, 110(1):145.",
            "year": 2003
        },
        {
            "authors": [
                "Farhan Samir",
                "Barend Beekhuizen",
                "Suzanne Stevenson."
            ],
            "title": "A formidable ability: detecting adjectival extremeness with DSMs",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4112\u20134125.",
            "year": 2021
        },
        {
            "authors": [
                "Devyani Sharma",
                "Robin Dodsworth."
            ],
            "title": "Language variation and social networks",
            "venue": "Annual Review of Linguistics, 6:341\u2013361.",
            "year": 2020
        },
        {
            "authors": [
                "Sali Tagliamonte",
                "Chris Roberts."
            ],
            "title": "So weird; so cool; so innovative: The use of intensifiers in the television series friends",
            "venue": "American speech, 80(3):280\u2013 300.",
            "year": 2005
        },
        {
            "authors": [
                "Isaac Waller",
                "Ashton Anderson."
            ],
            "title": "Generalists and specialists: Using community embeddings to quantify activity diversity in online platforms",
            "venue": "The World Wide Web Conference, pages 1954\u20131964.",
            "year": 2019
        },
        {
            "authors": [
                "Justine Zhang",
                "William Hamilton",
                "Cristian DanescuNiculescu-Mizil",
                "Dan Jurafsky",
                "Jure Leskovec."
            ],
            "title": "Community identity and user engagement in a multi-community landscape",
            "venue": "Proceedings of the international AAAI conference on web and social",
            "year": 2017
        },
        {
            "authors": [
                "Ling"
            ],
            "title": "From this initial set of seed stance markers, we search for other stance markers on Reddit used in similar distributional contexts",
            "venue": "To do so,",
            "year": 2015
        },
        {
            "authors": [
                "Pavalanathan"
            ],
            "title": "2017), we train Wang2vec on all words that occur at least 5 times in our sample of 25M comments from 2014, resulting in a vocabulary of size 1.2M",
            "year": 2014
        },
        {
            "authors": [
                "garwal"
            ],
            "title": "2020) by training Beta regression",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Communities vary in their language choices, often in ways that are indicative of the community\u2019s shared interests, values, and norms \u2013 their community identity. Sociolinguists refer to such spaces as communities of practice (Eckert and McConnellGinet, 1992; Holmes and Meyerhoff, 1999), and have explored how they vary in their in-person linguistic practices (Cheshire, 1982; Eckert, 2000). Computational work has explored variations of word and sense usage in online communities of practice (e.g., Zhang et al., 2017; Del Tredici et al., 2018; Lucy and Bamman, 2021), highlighting relationships between linguistic and non-linguistic aspects of community identity, including user retention, community size, and network structure.\nCommunity identity is also expressed through stancetaking (Bucholtz and Hall, 2005), in which\nspeakers position themselves in various ways \u2013 affectively, evaluatively, epistemically, etc. \u2013 relative to a topic or a conversational partner (e.g., Du Bois, 2007). Sociolinguistic work (e.g., Kiesling, 2004; Bucholtz, 2009) has studied how communities vary in their use of stance markers. Stance markers are words that demarcate stance, including, among others, intensifiers (really, insanely, terribly), modals (might, should), and evaluative words (like, love, hate). For instance, compare these sentences from three Reddit communities:\n1. I hope you are insanely proud of yourself! [r/pornfree, helping people overcome porn addiction]\n2. I hope you are incredibly proud of yourself! [r/progresspics, support for sharing fitness progress]\n3. Both of you are incredibly stupid [r/watchredditdie, critical of censorship on Reddit]\nIn both (1) and (2), the speakers indicate similarly positive stances towards their interlocutor, but vary in their choice of stance marker to demarcate this stance (insanely vs. incredibly).\nImportantly, sociolinguistic work has also argued that the contexts in which such markers appear is a critical aspect of stancetaking behavior (Kiesling et al., 2018; Bohmann and Ahlers, 2022). For example, sentences (2) and (3) use the same stance marker (incredibly), but in (3) the speaker evaluates their interlocutors in an especially negative light.\nSociolinguistic work posits that repeated stancetaking in a consistent way leads to stable associations of stance and identity (Eckert, 2008; Rauniomaa, 2003, cited in Bucholtz and Hall, 2005). For example, positive evaluations like (2) reinforce the identity of r/progresspics as a supportive community, while negative evaluations like (3) contribute to the critical tenor of r/watchredditdie. This illustrates that, in addition to stance marker preferences, stance contexts are crucial for connecting\nstancetaking and community identity. However, while large-scale research has shown that variation in stance marker usage captures patterns of user cross-posting (Pavalanathan et al., 2017), to our knowledge, no large-scale work has considered how variation in stance contexts relate to community identity and cross-community engagement.\nHere we address that gap by proposing a method for representing stance-relevant properties of a sentence that go beyond stance marker usage. Specifically, our stance context representations capture higher-level linguistic properties theorized to relate to stancetaking, including affect, politeness, and formality (Jaffe, 2009; Kiesling, 2009; Pavalanathan et al., 2017; Kiesling et al., 2018).\nWe focus our attention on the stance contexts in which intensifiers appear. Intensifiers, like insanely and incredibly above, are a subset of stance markers that emphasize that which they modify. Sociolinguistic work has shown that intensifier usage varies across linguistic and social contexts (Ito and Tagliamonte, 2003; Bolinger, 1972), which makes them ideal for studying community variation in stancetaking.\nWe first show that our stance context representations reveal aspects of community identity in a manner that complements both general textual representations, as well as stance marker preference representations. We then demonstrate how similarities in stance context relate to broader inter- and intra-community engagement patterns, including user co-participation across Reddit communities, and social factors of communities, including size, activity, loyalty, and density. Our work thus broadens the research linking community linguistic practices to social and network properties by considering higher-level factors beyond lexical variation. Future work can leverage these representations to shed light on additional aspects of platform dynamics, including community creation (e.g. are new communities sometimes created so that users may discuss the same topic but taking an alternative stance?) and user diversity (e.g. on Reddit, a platform dominated by men, do the stances communities take affect the participation and engagement of women and people with a (stereotypically) feminine linguistic style?).1\n1Our code and datasets can be found at https://github. com/jaikaggarwal/emnlp_2023_stancetaking"
        },
        {
            "heading": "2 Related Work",
            "text": "To study the relationship between language and community identity online, work in computational sociolinguistics has begun to shift from relating language choices and macrosocial categories like age, gender, and race (Burger et al., 2011; Nguyen et al., 2013; Blodgett et al., 2016), to a more nuanced approach to identity, adopting the framework of communities of practice (Eckert and McConnellGinet, 1992; Holmes and Meyerhoff, 1999). Communities of practice put the focus on local identity categories that better reflect individuals\u2019 various group membership choices \u2013 e.g., presenting as a gamer, a sports fan, or an animal lover.\nMuch sociolinguistic work has explored the connection between linguistic variation and social network structure in communities of practice (e.g., Cheshire, 1982; Milroy, 1987; Eckert, 2000; Sharma and Dodsworth, 2020). For example, Eckert (2000) noted the interrelation between how high school \u201cjocks\u201d and \u201cburnouts\u201d varied in their linguistic patterns and in the communities they interacted with. Work of this kind has focused on variation within a single community or a small number of communities.\nOnline social media platforms support research relating linguistic and non-linguistic aspects of community identity at scale through analysis of multi-community settings (e.g., Hamilton et al., 2016; Israeli et al., 2022; Noble and Bernardy, 2022). For example, Noble and Bernardy (2022) show positive correlations between similarities in community linguistic practices and usercommunity co-occurrence patterns. Further work has shown that communities with more distinct words better retain users (Zhang et al., 2017), and tend to be smaller, denser, more active, and have more local engagement (Lucy and Bamman, 2021). Similar findings have been shown for communities with distinct word senses (Del Tredici and Fern\u00e1ndez, 2017; Lucy and Bamman, 2021). We extend computational sociolinguistic work on community identity and engagement patterns by motivating and investigating variation of a different kind: variation in higher level properties of stancetaking.\nStancetaking is key to understanding how language variation reflects identity, because so many of a speaker\u2019s linguistic choices pertain to expression of stance (e.g., Jaffe, 2009). Moreover, much work has argued that, rather than linguistic forms being directly associated with social iden-\ntities, linguistic forms are first associated with stances, which in turn are associated with social identities (e.g., Ochs, 1993; Du Bois, 2002; Rauniomaa, 2003; Bucholtz and Hall, 2005; Eckert, 2008). Thus, variation in higher-level properties of stancetaking needs to be given more attention in analyses of language and community identity.\nYet there has been limited computational work on stancetaking. Pavalanathan et al. (2017) studied variation in use of stance markers across online communities, and their word-based approach allowed them to study stance at scale. Kiesling et al. (2018) developed a more nuanced approach for representing stance, drawing on the framework from Du Bois (2007); however, the approach depended heavily on manual annotation, limiting analysis to a relatively small number of online communities. In our work, by automatically assessing stancerelevant properties of contexts, we are able to maintain the scale of analysis from Pavalanathan et al. (2017), while getting closer to the richness of representation from Kiesling et al. (2018)."
        },
        {
            "heading": "3 Representing Stance Contexts",
            "text": "We study stancetaking \u2013 and stance contexts in particular \u2013 on Reddit, due to its nature as a multicommunity platform with free-form commenting and rich engagement patterns among its communities (subreddits). We take intensifiers \u2013 a kind of stance marker \u2013 and the stance contexts in which they appear as our testbed. Intensifiers, like insanely and incredibly in (1)\u2013(3) above, are adverbs that give emphasis to what they modify.\nIntensifiers are an ideal domain for studying community variation. Extensive empirical work in sociolinguistics has established that intensifiers vary based on social factors (e.g., Ito and Tagliamonte, 2003; Tagliamonte and Roberts, 2005); moreover, they also exhibit much versatility in their contexts and undergo frequent change (e.g., Bolinger, 1972; Peters, 1994). While past computational work has studied semantic change in intensifiers over time (Luo et al., 2019; Samir et al., 2021), to our knowledge computational work has not examined community variation in intensifiers, and their context of use, as we do here.\nFurthermore, intensifiers are a practical choice: Stance markers cover a range of grammatical classes (verbs, adjectives, interjections, and others), and focusing on intensifiers (which are adverbs), ensures that the variation we find in stance contexts\nis not due to variation in the grammatical contexts in which the markers appear.\nHere we describe how we identify a set of intensifiers and extract their contexts from a large set of communities (subreddits). We then explain how we represent these contexts with linguistic features relevant to stancetaking, presenting the features, how we calculate them, and how we use them to form a vector-based representation of each community in our dataset."
        },
        {
            "heading": "3.1 Extracting the Data",
            "text": "We start with the list of single-word intensifiers from Bennett and Goodman (2018, Studies 1a and 1b), as they are shown to express a range of degree of intensification. To ensure that these are appropriate to our study of stancetaking, we intersected this list of 89 intensifiers with a list of 1118 stance markers (including intensifiers and other words) found on Reddit. The latter list was created using the methodology of Pavalanathan et al. (2017), applying Wang2Vec (Ling et al., 2015) to expand a seed set of 448 stance markers released by Biber and Finegan (1989).2 The Wang2Vec model is trained on Reddit data and used to extract words used in similar distributional contexts to the seed stance markers. After intersecting the two lists, we removed two words that occurred in a stopword list (most and very) to ensure we included informative words.\nThis procedure yielded a set of 38 intensifiers that serve as our stance markers. This stance marker set, and full details of our method for obtaining it, are described in Appendix A. In Appendix B, we show additional analyses conducted on a larger set of 252 intensifiers, and find that our methodology is robust to the set of intensifiers used.\nTo create our stance context representations of Reddit communities, we needed to extract the contexts of use of these intensifiers. We drew on Reddit data from all of 2019 and retrieved all comments from the top 10K subreddits by activity (as determined by Waller and Anderson, 2019) using the Pushshift data dumps (Baumgartner et al., 2020)3. We applied preprocessing as described in Appendix C, yielding 1.2B comments across the 10K subreddits.\nFrom each comment, we extract any sentence that contains an intensifier from our list, so that\n2Wang2Vec is an adaptation of Word2Vec that accounts for syntactic information.\n3https://files.pushshift.io/reddit/comments/\nour stance context representations will reflect the local semantic context of this stance marker. We retain only those sentences with exactly one usage of an intensifier, and whose length is at least 6 tokens (to contain sufficient content to reliably extract values for our linguistic features). To ensure sufficient and comparable data per community, we only perform analyses on communities with at least 10K such sentences, and use exactly 10K sentences from each (subsampling when necessary), resulting in 17.98M sentences across 1798 communities."
        },
        {
            "heading": "3.2 Stance Context Representations",
            "text": "Our aim is to represent the prototypical stance context of each community as a vector of feature values averaged across its 10K sentences. We first describe how we assess properties of sentences, and then explain how we combine these to generate community-level representations."
        },
        {
            "heading": "3.2.1 Properties of Stance Contexts",
            "text": "To capture salient properties of stance contexts, we use linguistic features known to relate to stance demarcation, including affect, politeness, and formality (Jaffe, 2009; Pavalanathan et al., 2017; Kiesling et al., 2018). Affect was broken down into the 3 features of valence (positivity), arousal (intensity), and dominance (level of control), following the VAD framework commonly used in psycholinguistics for representing emotions (Russell, 2003). These 5 features are particularly relevant here, as intensifiers are often used for affective impact, and vary widely in politeness and formality. Moreover, such features have robust methods for assessment at the sentence level.\nFor each feature, we create a model for inferring an appropriate value given a sentence represented using SBERT (Reimers and Gurevych, 2019). SBERT generates high-quality sentence embeddings shown to be useful for inferring sentiment, a high-level property of text similar to our stance context properties (Reimers and Gurevych, 2019). We evaluated the SBERT models for each feature on held-out portions of the human-annotated datasets and achieved accuracies comparable to prior work. We summarize model details below; full details can be found in Appendix D.\nFor VAD, we follow Aggarwal et al. (2020) and train a Beta regression model to predict each of these scores (constrained to the range [0, 1]) given the SBERT representations of the 20K words in the NRC-VAD lexicon (Mohammad, 2018). We\ncan then apply this model to assess the VAD of full sentences, based on their SBERT representations.\nFor politeness, we build a logistic regression model, adapting the method of Danescu-NiculescuMizil et al. (2013) to give normally-distributed continuous politeness scores by taking the log-odds of the predicted politeness probability, and then min-max scaling the values to the range [0, 1].\nFor formality, we used the annotated corpus compiled by Pavlick and Tetreault (2016), focusing on the Answers and Blogs domains. The former generalizes well to stylistically-different domains (Pavlick and Tetreault, 2016), while the latter is the most similar domain to Reddit (Pavalanathan et al., 2017). We rescaled the formality annotations to the range [0, 1] using min-max scaling, and then trained and selected a formality prediction model using 10-fold cross-validation."
        },
        {
            "heading": "3.2.2 Community-Level Representations",
            "text": "We apply each of the above models to the sentences in our dataset. We are interested in the properties of the context in which an intensifier is used, rather than properties of the intensifier itself. Thus we replaced the intensifier in a sentence with the [MASK] token, and generated SBERT representations for these masked sentences. We then applied our models described above to find the valence, arousal, dominance, politeness, and formality values for each sentence. Table 1 shows example sentences that vary along the five properties.\nOur aim was to average the values of the features over all sentences in a subreddit to yield a community-level representation \u2013 its prototypical stance context vector. However, we observed that the stance contexts within a subreddit may fall at both extremes of a scale (e.g., both many highly positive sentences and many highly negative sentences), and averaging would obscure such a pattern. This type of behavior seems natural, because intensifiers are often derived from extreme adjectives \u2013 those that express a property towards the extremes of a scale (Samir et al., 2021) \u2013 and such extremeness may itself be part of the stance tenor of a community.\nWe thus added an \u201cextremeness\u201d version of each feature introduced in Section 3.2.1 to our stance context representation, calculated by centering each feature at 0 and computing its absolute value. Including the extremeness features resulted in each sentence being represented by a 10- dimensional vector. We then represent each com-\nmunity\u2019s overall stance context as the mean of these 10-D vectors of its 10K sentences.4"
        },
        {
            "heading": "4 Additional Representations/Methods",
            "text": "Here we present two other community representations to which we will compare our stance context representation: one that captures community preferences for the stance markers, and the other simple overall textual similarity. We also show how we calculate community similarity using the three representations. Finally, some analyses look at groups of subreddits by topic, which are described here.\nMarker Preference Representations. To capture community preferences for particular stance markers, we used the methodology of Pavalanathan et al. (2017), applied to our 17.98M instances of intensifiers, for comparability to our stance context representations. We create a subreddit-marker count matrix and apply a positive pointwise mutual information (PPMI) transformation to the matrix. We then apply truncated SVD and retain the top 11 dimensions (based on elbowing in a Scree plot).\nTextual Representations. We create textual representations to show that our stance context representations capture more than just textual similarity between communities. Following Lucy and Mendelsohn (2019), we construct a tf-idf weighted subreddit-word matrix from our 17.98M comments, with a vocabulary size of 158K. We then apply truncated SVD and retain the 7 most informative dimensions (based on elbowing in a Scree plot).\nComputing Community Similarity. We use cosine similarity to compute how similar two communities are on a particular representation. Let R be one of our three representations, c, c\u2032 be two communities (subreddits), and Rc be the embedding for\n4To avoid anisotropy in our vector space, which resulted in almost all communities having near perfect cosine similarity with each other, we applied Z-score normalization to each of the 10 features.\ncommunity c in representation R. Then:\nSim(R, c, c\u2032) = cos_sim(Rc, Rc\u2032)\nCommunity Topic Groups. To investigate general trends in how communities vary with respect to each of our three representations, we group our subreddits by topic, using the r/ListofSubreddits5 categorization, as in Lucy and Bamman (2021). These groups yield an interpretable community structure that broadly reflects commonalities in user interests.6 The list divides Reddit into 12 major topics, with further subdivisions.\nWe designate any subtopics with at least 50 subreddits in our dataset as their own topic. For any unassigned subreddits that start with \u201cask\u201d, or include cue words such as \u201cadvice\u201d or \u201cquestions\u201d, we add them to the Discussion subtopic. This results in 1228 subreddits assigned to 15 topics, and 570 subreddits without a topic label. The topics and number of subreddits assigned to them are shown in Appendix E.\nComputing Topic Group Similarity. To assess similarity of two topic groups, we find the mean pairwise similarities for all subreddits in each of the topics. Let TA, TB be the sets of communities for topics A and B. We compute the topic similarity TS(R, TA, TB) for a representation R as:\nTS(R, TA, TB) =\n\u2211 c\u2208TA \u2211 c\u2032\u2208TB Sim(R, c, c\u2032)\n|TA||TB|"
        },
        {
            "heading": "5 Comparing the Three Representations",
            "text": "We show how the 3 kinds of community representations (textual, stance context, and marker preference) capture distinct facets of community identity.\n5https://www.reddit.com/r/ListOfSubreddits/ wiki/listofsubreddits/, updated as of April 2023.\n6The topics are specified by Redditors, and capture subreddit similarity based on perceptions of what may drive people to engage with communities, rather than simple textual overlap. For instance, both r/law and r/learnart are under Education, but r/learnart is most textually similar to subreddits like r/crossstitch and r/artistlounge, which are under Hobbies.\nWe find that they do not correlate highly with each other; moreover, they can reveal subtle differences between pairs of seemingly similar subreddits."
        },
        {
            "heading": "5.1 Pairwise Correlations",
            "text": "For each representation, we compute the community similarity of each of the 1.6M pairs of subreddits, and find the pairwise Pearson correlations between these values. Table 2 shows that the representations have only low to moderate positive correlations. This provides preliminary evidence that we have representations that capture three distinct facets of subreddits: the content they discuss, the stances they tend to take with respect to this content, and the stance markers they use to demarcate these stances. In the next section, we illustrate how these representations capture distinct patterns across the multi-community landscape."
        },
        {
            "heading": "5.2 Topic-Topic Similarities",
            "text": "To visualize the differences in what these three representations capture, we compute TS(R, TA, TB) for all pairs of topics A and B (excluding the Other and General categories as they lack a topical focus).7 The plots in Figure 1 show much variation across the three representations. To dig deeper into the nature of this variation, our qualitative analysis focuses on three pairs of topics that illustrate how communities can vary in their degree of textual, stance context, and marker preference similarity; see Figure 2. Further visualizations comparing the differences between representations can be found in Appendix F.\nThe Animals and Humor topics have a not uncommon pattern in which they are moderately to very similar across all three representations. The linking of animals and humor is not surprising, although the range of expressed stances is fairly broad. For example, r/animalsbeingderps, r/animalsbeingjerks, and r/animalsbeingbros involve observing animal behavior that is either awkward, mean, or kind, respectively; their high sim-\n7Since the range and variance of similarity values varies across representations, we apply Z-score normalization to the set of topic-topic similarities.\nilarity with humor subreddits indicate that these may be varied aspects of humor as well.\nThe other two pairs of topics, Politics/Education, and Sports/Video Games, have low to moderate textual similarity, but show very high similarity in either stance context or marker preference. The Politics and Education topics have somewhat higher formal stance contexts relative to other subreddits, which may arise from the local identity categories that surface in these kinds of communities, as individuals may seek to position themselves as intelligent. This higher formality leads to very consistent stance marker use, as 8 of their top 10 most preferred markers overlap, including largely, enormously, wholly, and exceedingly. These markers are 4 out of the top 5 markers used to demarcate formal stances across our whole dataset. This example highlights how community stance similarity on a particular dimension can guide marker preference similarity.\nOne of the starkest contrasts across the three representations is between the Sports and Video Games topics. Despite having dissimilar textual representations, this pair of topics is among the most similar with respect to stance contexts (in fact, there is less variation between the stances of these communities than within most topics on the diagonal). Analysis reveals that they are similar on most dimensions, but are noticeably distinct from the rest of the platform with respect to their high arousal and less extreme politeness. The high arousal (intensity) of these topics makes sense given the competitive, game-oriented nature of the subreddits in both these groups. However, despite this similarity in stance contexts, the Sports and Video Games subreddits have low marker preference similarity, highlighting that communities that take similar stances may differ greatly in how they choose to demarcate these stances."
        },
        {
            "heading": "5.3 Discussion",
            "text": "These findings reveal the differences in textual, stance context, and marker preference representations. Each provides a distinct glimpse into facets of community identity, and using all three simultaneously provides further insights into axes on which Reddit communities tend to vary. Examining the Politics and Education topics shows how similarity on just a single dimension of stance can be informative as to marker preference similarity, as communities seek to demarcate stance in a way that\nis consistent with particular traits. Our final example illustrates that communities with almost no similarity in their content (Sports and Video Games) can be extremely similar in how their stances relate to that content (highlighting a critical difference between stance contexts and textual similarity), but may yet demarcate their stances differently (highlighting the difference between stance contexts and stance markers). Overall, our results show that our three representations are distinct yet complementary, and provide a holistic view of community variation in language and community identity."
        },
        {
            "heading": "6 Stance and Engagement Behavior",
            "text": "Our analysis in the preceding section has established that stance context is distinct from both textual similarity and stance marker preference, and has shown preliminary evidence that these differences allow us to capture high-level properties of communities that relate to social identity. In this section, we demonstrate how the two aspects of stancetaking, stance context and stance marker preferences, relate to community engagement patterns on Reddit. We first examine how well they predict subreddit cross-posting patterns. We then explore how community distinctiveness in both average\nstance context and stance marker preferences relates to community structure. These findings illustrate the explanatory power offered by stancebased representations, particularly stance context representations, when investigating inter- and intracommunity dynamics online."
        },
        {
            "heading": "6.1 Cross-Posting",
            "text": "Core to platforms like Reddit is that people may engage with a number of communities of practice. Among the many reasons users may crossparticipate in such communities, we hypothesize that users are attracted to communities that have similar linguistic practices \u2013 in particular, those that are similar in how their members express stance. Pavalanathan et al. (2017) showed evidence of stance demarcation patterns predicting user crossposting patterns, and we extend this to looking at community similarity in stance marker contexts.\nOur classification task follows the same procedure described in Pavalanathan et al. (2017), adapted for the top 100 most popular subreddits in our dataset by commenting volume. This results in 429 low cross-posting pairs and 353 high crossposting pairs. For each representation and a given subreddit pair (c, c\u2032), we use a logistic regression model to predict whether c and c\u2032 have high or low cross-posting using Sim(R, c, c\u2032). (We also replicated the model used in Pavalanathan et al. (2017) that treats each vector dimension as a separate predictor and found a similar pattern of results to our cosine similarity metric, but with lower accuracies.)\nTo evaluate our models, we compare their accuracy at predicting cross-posting using the average accuracy of a 10x10 repeated cross-validation,8\n8Similarly to Boleda et al. (2012), we use the corrected resampled t-test developed by Nadeau and Bengio (2003) for performing statistical tests, as the lack of independence\nshown in Table 3. Using only our 38 intensifiers, the marker preference representation performs surprisingly well, substantiating the importance of intensifiers in communicating stance. However, our stance context representations perform even better at predicting cross-posting (t = 4.28, p < 0.001, Cohen\u2019s d = 1.79), confirming the key role of stance context features in community linguistic practices.\nTo determine which features of stance affect how users cross-participate on subreddits, we also examined the standardized coefficients of a regression model that treats each predictor separately. We found that 7 of our 10 stance context features were significant predictors of cross-posting, including 4 of the 5 extremeness features. Similarity in formality and extremeness of valence were especially predictive of cross-posting. The full results can be found in Appendix G.\nThus we show here that variation in stance contexts is indeed predictive of the community membership decisions that Redditors make. Despite only capturing how communities use intensifiers, both marker preference similarity and stance context similarity do reasonably well at predicting cross-posting patterns on Reddit. Our stance context representations obtained 7% higher accuracy on the task, suggesting that higher-level contextual properties related to stance are particularly important for shaping user participation patterns. Furthermore, our interpretable stance context properties allowed us to study the aspects of stancetaking that most shape co-participation online. Each of these findings helps shed light on how people engage across multiple communities of practice."
        },
        {
            "heading": "6.2 Distinctiveness in Stancetaking Style",
            "text": "Our cross-posting analysis confirmed that the ways people express stance are related to how they choose to engage across multiple communities. We now turn to the extensive sociolinguistic findings referenced in Section 2 that linguistic practices also interrelate with community structure. For example, Lucy and Bamman (2021) showed that the use\nbetween training sets across folds results in an inflated Type I error for the standard paired t-test (Dietterich, 1998).\nof specialized vocabulary varies with four social network properties in online communities. Specifically, Lucy and Bamman (2021) found that communities with more distinctive use of words and of word senses are smaller (fewer users), more active (higher average participation among users), more loyal (more users whose participation is focused in that community), and more dense (more interactions among users). We sought to explore whether stancetaking patterns similarly in online communities \u2013 do smaller, more active, more loyal, and denser communities develop a more distinctive manner of expressing stance?\nFor this question, we explore distinctiveness in stancetaking style with respect to both stance contexts and marker preferences, and relate these to the four social factors explored in Lucy and Bamman (2021).9 We conceptualize distinctiveness D(c,R) of a community c as how dissimilar it is from all other communities on average, using representation R. Let C be our set of 1798 communities:\nD(c,R) =\n\u2211 c\u2032\u2208C (1\u2212 Sim(R, c, c\u2032))\n|C|\nThen, for each representation R (stance context and marker preference), we perform correlations between the D(c,R) values for all 1798 communities, and the values of each of the four social factors calculated for the communities; see Table 4.\nFirst, we see that distinctiveness in marker preference is significantly correlated with loyalty; this is consistent with the finding of Lucy and Bamman (2021) that more loyal communities have more distinctive vocabulary usage. However, in contrast to Lucy and Bamman (2021), distinctive usage of stance markers is not correlated with any other network properties. Presumably the set of stance markers we consider are general enough vocabulary items that their preference patterns are not\n9We adopted the formulas used by Lucy and Bamman (2021) for these factors, described in Appendix H.\nhighly associated with how users engage within a community.\nSecond, and more notably, stance context distinctiveness patterns in a manner inverse to what Lucy and Bamman (2021) found for distinctiveness of vocabulary.10 Communities that are more distinct in their stance contexts tend to be larger, less active, less loyal, and less dense. To investigate what drives stance context distinctiveness to pattern in this way, we examine the communities by topic grouping. Specifically, we find that of the 303 least distinctive quartile of communities that have a topic assigned, 41% are Sports communities and 26% are Video Games communities. This is coherent with our results in Section 5, where we showed that despite having low textual similarity, Sports and Video Games communities have rather high stance context similarity. Since they form a sizable collection of subreddits, their mutual similarity leads to them having among the lowest average distinctiveness of our communities.\nMoreover, among the 13 contentful topic groups (excluding General, Other, and Not Assigned), we find that Sports communities tend to be the smallest, densest, most loyal, and most active overall, while Video Game subreddits are larger but also among the highest in density, loyalty, and activity. Qualitatively, we suspect Sports and Video Game communities foster this degree of local engagement given allegiances people have to their preferred teams and games.\nThus, our findings on distinctiveness in stancetaking context reveal novel aspects of the relationship between linguistic practice and community structure. Because Sports and Video Game communities are highly represented on Reddit overall, it\u2019s not surprising that these break down into a large number of subcommunities that are more topically fine-grained (on particular sports, teams, and games). These communities will then be among the smaller, denser, more loyal, and more active subreddits, but because they have stance context properties in common, these social network properties are associated with the least distinctive communities in stance context.\nFurthermore, the contrast with distinctiveness in vocabulary \u2013 both our marker preferences here and specialized vocabulary in Lucy and Bamman (2021) \u2013 allow us to view Reddit platform dynam-\n10We replicated our results on the communities from Lucy and Bamman (2021) with similar results; see Appendix I.\nics and community structure from a different lens than previous work. Our findings illustrate that, as higher level properties of linguistic practice, stancetaking contexts may hold in common across different detailed communities. Our results overall highlight the importance of investigating the rich and varied linguistic means for expressing community identity."
        },
        {
            "heading": "7 Conclusion",
            "text": "We extend work on community variation in stancetaking by constructing stance context representations using theoretically-motivated linguistic properties of stance. Since stancetaking plays a key role in linguistic variation (Jaffe, 2009), considering these higher level properties of stance may help shed light on why community members present themselves in particular ways. We showed that our stance context representations capture aspects of community linguistic identity distinct from existing methods. Furthermore, we found that these representations were related to inter- and intracommunity engagement patterns.\nOur stance contexts approach also allowed us to reveal \u201cmega\u201d communities: those like Sports and Video Games that share stancetaking properties while having numerous fine-grained subcommunities within them. Further work with stance contexts may help shed additional light on the nuances of mega community structure: in addition to establishing communities for different subtopics (such as different video games), are \u201cspin-off\u201d communities (cf. Hessel et al., 2016) sometimes created so that users may discuss the same topic while taking an alternative stance (e.g. r/pokemontrades vs r/casualpokemontrades)?\nAdditionally, by showing that higher level properties of stance relate to community identity and engagement patterns, we pave the way for much richer models of the reasons that people are attracted to, or discouraged from, online participation in various groups. For example, on Reddit, a platform dominated by men, high-arousal stances are the most common due to the overrepresentation of Sports and Video Games communities. Future work could explore whether other kinds of stances (e.g. more polite or higher valence stances) lead to increased participation and engagement from women and people with a (stereotypically) feminine linguistic style."
        },
        {
            "heading": "8 Limitations",
            "text": "We first discuss limitations related to our specific methodological choices, and then discuss limitations of our approach more generally."
        },
        {
            "heading": "8.1 Methodological choices",
            "text": "First, we studied the contexts where speakers use intensifiers \u2013 a kind of stance marker \u2013 but we considered only 38 intensifiers in our main analyses. As a follow-up, we ran additional experiments that included a set of 252 intensifiers from Luo et al. (2019) across 448 of our communities selected for another project. These experiments confirm that both our cross-posting and social factors results hold for a much larger set of intensifiers. Full details of the results can be found in Appendix B.\nHowever, it may be fruitful to replicate our analyses with a larger set of stance markers (including markers beyond intensifiers), such as the set of 812 stance markers considered by Pavalanathan et al. (2017). This would allow us to ensure our insights hold for stance markers generally.\nThere are also limitations to our method for assessing stance-relevant properties. We focused on valence, arousal, dominance, politeness, and formality, but other linguistic properties have been identified as relevant to stance contexts, including subjectivity and certainty (K\u00e4rkk\u00e4inen, 2006; Kiesling et al., 2018). Future work on stancetaking may benefit from the development of methods for assessing these properties."
        },
        {
            "heading": "8.2 General approach",
            "text": "There are also some limitations of our stance contexts approach more generally. First, our approach to assessing stance properties doesn\u2019t account for community-level semantic variation. For instance, work has shown that the same word can vary in sentiment, depending on the community (Hamilton et al., 2016; Lucy and Mendelsohn, 2019) . Because our approach for assessing stance-relevant properties depends on context words (which can vary in meaning across communities), it may miss out on some community-specific meanings of stance contexts.\nRelatedly, our approach doesn\u2019t account for variation in how stances are communicated over time; the datasets we use to train our models for assessing stance-relevant properties (Mohammad, 2018; Danescu-Niculescu-Mizil et al., 2013; Pavlick and Tetreault, 2016) are 5\u201310 years old, and likely miss\nout on novel words or word senses used in online contexts, and their effect on our five linguistic property values. Future research should work to understand how the stance-relevant properties of contexts vary across communities and over time.\nFurthermore, although we drew on literature from sociolinguistics and linguistic anthropology in developing our stance contexts representations, our approach does not capture the full richness of stance as understood in these fields. For example, the highly influential stance triangle framework from Du Bois (2007) theorizes that stancetaking involves a speaker evaluating something (a \u201cstance object\u201d), and that \u2013 in so doing \u2013 the speaker positions themself relative to their interlocutor. Because these components of stance are sometimes mentioned in the sentential context (which we use to generate our stance representations), we may implicitly capture them in some cases. However, within our framework, it is not straightforward to compare these components of stance to human intepretations (cf. Kiesling et al., 2018).\nRelated to this, our method only captures local aspects of stance contexts within the sentence in which intensifiers were used. However, work on stance contexts has discussed how the sequential context of the preceding utterances in a dialogue shape the construction and interpretation of one\u2019s stance (Kiesling et al., 2018; Bohmann and Ahlers, 2022). For example, the preceding utterances could inform whether a stance is interpreted as sarcastic or not. Future research should work to develop even richer representations of stance, which are also scalable."
        },
        {
            "heading": "8.3 Data Access",
            "text": "One additional limitation is related to the future accessibility of our data, as the Pushshift Data Dumps we used to extract Reddit data are no longer available at their original link (as of May 2023). Future research will need to use the official Reddit API to access the data we used."
        },
        {
            "heading": "9 Ethical Considerations",
            "text": "User privacy is a concern inherent to using online data. The data we use was public when collected by Baumgartner et al. (2020), and we take care to remove data from any individuals that had deleted their accounts at any point prior to the data\u2019s collection, adhering as best we can to a user\u2019s right to be forgotten.\nMohammad (2022) mentions that in cases where one automatically infers emotional properties (such as VAD scores), it is critical to be cognizant that users may not want their data analyzed, as well of the potential harms of associating such information with individual users. Furthermore, the automatic assessment of emotional properties of text may not match the intended affect of the user, and this problem may be more pronounced for users who are part of marginalized groups (Mohammad, 2022). With these concerns in mind, we remove user-level information prior to extracting values for our linguistic features, such that our downstream analyses have no connection between individual users and the emotional aspects of their language. Our textual and marker preference representations also make no use of user identifiers. We only make use of useridentifying information for computing community engagement behavior, and only release information about the aggregated statistics per community. Although the sentences we use in our dataset were created by individual users, we mitigate user privacy concerns as best as possible in the ways listed above.\nAnother ethical concern pertains to the amount of compute power we used for our analyses, particularly for data extraction and creating our sentence embeddings (see Appendix J). For the latter process, we minimized unnecessary computation by computing the embeddings for each sentence only once. This means that we did not need to recompute embeddings for each of our five linguistic properties, and that we can extend our methodology to include other linguistic properties without needing to use any more GPU power for creating embeddings. To mitigate additional compute costs for researchers seeking to replicate our work, we make our sentence-level stance representations public."
        },
        {
            "heading": "Acknowledgements",
            "text": "We acknowledge the support of NSERC of Canada (through grant RGPIN-2017-06506 to SS), as well as the support of the Data Sciences Institute, University of Toronto (through a Catalyst Grant to SS and JW)."
        },
        {
            "heading": "A Constructing the Stance Marker and Intensifier Lists",
            "text": "To extract a set of stance markers common to Reddit, we replicated the lexical expansion procedure used by Pavalanathan et al. (2017). We initially focused our analyses on Reddit data from 2014, and performed lexicon expansion using this dataset.\nWe started with the seed set of stance markers released by Biber and Finegan (1989), which includes 448 markers in total. For a separate project, we required the stance categories that this corpus assigns markers to (e.g. affective stance markers, emphatic stance markers, etc.), so this set was more appropriate than the stance markers released in the Switchboard corpus (Jurafsky et al., 1998), which does not contain category-level information.\nFrom this initial set of seed stance markers, we search for other stance markers on Reddit used in similar distributional contexts. To do so, we use Ling et al. (2015) (an adaptation of Word2Vec that accounts for syntactic information), following the method of Pavalanathan et al. (2017). Unlike Pavalanathan et al. (2017), we train Wang2vec on all words that occur at least 5 times in our sample of 25M comments from 2014, resulting in a vocabulary of size 1.2M. We adjust their thresholds for similarity, extracting all candidate words having a cosine similarity of at least 0.85 with one of our seed markers, and a general frequency of at least 10\u22126.\nThis process resulted in 1118 stance markers in total, of which 40 are intensifiers found in Bennett and Goodman (2018). We remove the two stop words most and very to arrive at our final set of 38 intensifiers, shown in Table 5."
        },
        {
            "heading": "B Expanded Analyses",
            "text": "In our main analyses, we focused on a relatively small number of intensifiers (38), which may raise concerns about whether our findings generalize to other intensifiers. To address this, we performed the same cross-posting and social factors analyses on the 252 intensifiers from Luo et al. (2019) on a subset of 448 subreddits (selected for a different project).\nOf our 38 stance markers, 28 are found in this set of 252 markers. The 10 markers that are not in the set of 252 markers are: downright, fantastically, incredibly, largely, plain, pretty, quite, really, suspiciously, and unbelievably. We repeat our analyses on the 252 markers collected for the new dataset, as well as on the 28 markers both datasets have in common.\nAcross both our original and additional analyses, our stance context representations achieve very consistent results. We expand on each analysis in the sections below.\nB.1 Cross-Posting Results\nFor the cross-posting analysis, we recompute high and low cross-posting pairs using the same methodology as in the main text, but for the set of 448 communities in this analysis. Table 6 shows the results, where the leftmost column corresponds to the results in the main text. The stance context representations significantly outperform the marker preference representations in all cases, at \u03b1 = 0.05 using the same 10x10 cross-validation procedure used in the main text. This suggests that stance context representations are consistently better at predicting cross-posting than the marker preference similarity representations.\nB.2 Social Factors Analysis\nWe repeat the social factors analysis for the stance context representations using the same procedure outlined in the main text. In the dataset with 448 communities, distictiveness is calculated only with respect to these 448 communities. Table 7 shows our results.\nAs in the case of the cross-posting analysis, the correlations between stance context distinctiveness and the four social factors also shows the same pattern as in the main text results across all the new analyses, albeit with some reduction in the power and magnitude of correlation when only using the 28 overlapping intensifiers. This suggests that our\nrepresentations, as well as our overall findings, are robust to the selection of intensifiers."
        },
        {
            "heading": "C Data Preprocessing",
            "text": "We apply preprocessing to the comment text extracted from Reddit, including: We replace all links with a [LINK] token, replace all mentions of another username with a [USER] token, and replace irregular unicode characters, extraneous parentheses, and newlines with whitespace. Malformed quotation marks and apostrophes are also replaced with the appropriate token. We then strip tokens used to indicate bold or italicized text and replace multi-whitespace characters with a single character.\nAt the author-level, we remove any comments written by deleted authors, AutoModerators, and account names that end in \u201cbot\u201d, regardless of case."
        },
        {
            "heading": "D Details of Models to Infer Linguistic Properties",
            "text": "To create SBERT representations of sentences in our data, we use bert-large-nli-mean-tokens, as it performs the best on the STS task without fine-tuning (Reimers and Gurevych, 2019).\nFor valence, arousal, and dominance, we have access to the NRC-VAD lexicon (Mohammad, 2018), which provides values of these features for 20K words, but we need a model that assigns such values to sentences. We follow the method of Aggarwal et al. (2020) by training Beta regression models on SBERT representations of words from the NRC-VAD lexicon, and then applying those models to our Reddit sentence data. We adjusted the methodology of Aggarwal et al. (2020) by training the models on 80% of the data, stratified over quintiles of each of the VAD scores. This stratification ensured that our training data was approximately uniform across the [0, 1] interval. We repeated this procedure 10 times and chose the model that performed best for each feature. Our best models achieve Pearson correlations of 0.85, 0.77, and 0.80 on their held-out sets for valence, arousal, and dominance respectively, comparable to those found by Aggarwal et al. (2020).\nFor politeness, we followed the methodology of Danescu-Niculescu-Mizil et al. (2013) to build a logistic regression model to predict whether a given sentence is polite. We adapted their approach to give normally-distributed continuous politeness scores by taking the log-odds of the predicted politeness probability, and then min-max scaled the\nresulting values to the range [0, 1]. We trained and tested our models on both their Wikipedia corpus and their StackExchange corpus, performing both in-domain and cross-domain tests. In-domain tests were conducted using 3x10 cross-validation, and the cross-domain tests were conducted by training the model on the entire training set and testing on the entire testing set. We found that the model trained on the Wikipedia text performed better than the model trained on the StackExchange text, with in-domain and cross-domain accuracies of 84.1% and 65.2% respectively, comparable to the models in Danescu-Niculescu-Mizil et al. (2013).\nFor the formality model, we selected the best model after 10-fold cross validation. The Spearman correlation of our best model with its hold-out set was 0.76, higher than those found in Pavlick and Tetreault (2016)."
        },
        {
            "heading": "E Subreddit Topic Groups",
            "text": "The list of topic groups we used, and the number of our 1798 subreddits assigned to each, are shown in Table 8."
        },
        {
            "heading": "F Topic-Topic Differences in Representations",
            "text": "To further visualize differences in how topics differ across our three representations, we show the graphs in Figure 3. These graphs were generated by subtracting the values in each cell of Figure 1 between the corresponding heatmaps. For instance, the top-left cell of the leftmost subfigure of Figure 3 can be interpreted as follows: subreddits in\nthe Animals topic are more similar to each other according to textual representations than stance context representations."
        },
        {
            "heading": "G Stance Properties and Crossposting",
            "text": "To assess each feature\u2019s individual contribution to predicting crossposting, we used a logistic regression model with 10 predictors: the differences between a pair of communities on a particular stance property dimension. To improve our model for this task, we unit-normalized each community stance context representation. The coefficients are shown in Table 9."
        },
        {
            "heading": "H Computing Social Factors",
            "text": "As mentioned, we adopt the formulas used by Lucy and Bamman (2021) for computing the social factors of size, activity, density, and loyalty. All data is gathered from the set of 1.2B comments written in 2019.\nFor community size, we use the number of distinct users that have ever posted in a community. For user activity, we divide the number of total comments in a community in 2019 by the community size. To compute network density, we first construct an undirected parent-reply network for the top 20% of users by commenting volume in 2019. For each community, we then compute the density as the number of users who have an edge between them, dividing by the total number of possible edges in the graph. Finally, to compute loyalty, we find the proportion of users in a community that have at least 50% of their top-level comments in 2019 in a particular community."
        },
        {
            "heading": "I Social Factors Analysis for Vocabulary Distinctiveness",
            "text": "In past work, Lucy and Bamman (2021) examined how social factors relate to distinctiveness in vo-\ncabulary. We wanted to build on their work by exploring the role of contextual properties beyond individual words and word senses, so in our main analyses, we focused on our novel contribution of how the social factors relate to distinctiveness in aspects of stancetaking.\nHere, we directly compare how stance context distinctiveness and vocabulary distinctiveness relate to social factors. To do this, we use the data released by Lucy and Bamman (2021) and compute vocabulary distinctiveness scores for the 418 communities that our dataset has in common with theirs.11\nThe results for our stance context and marker preference distinctiveness, as well as vocabulary distinctiveness, are shown in Table 10. For this set of subreddits, we find the same pattern of results for both of our distinctiveness measures as on the full set of our subreddits, and we see the same pattern of results for vocabulary distinctiveness as reported in Lucy and Bamman (2021) for their full set of sub-\n11Link to Github for Lucy and Bamman (2021): https: //github.com/lucy3/ingroup_lang\nreddits. Here we also found that Sports and Video Games subreddits are again (as in our full results) the most represented topics among communities in the lowest quartile of stance context distinctiveness (48% and 31%, respectively). Conversely, those topic groups are also most represented in the highest quartile of vocabulary distinctiveness (38% and 23%, respectively)."
        },
        {
            "heading": "J Hardware",
            "text": "To extract our data, we used 12 CPUs in parallel, each of which took roughly 8 hours to extract data from the Pushshift data dumps. For computing sentence embeddings, we used two Nvidia Titan X GPUs, which processed about 1.1M sentences per hour (in batches of size 32). This took roughly 8 hours per GPU, for a total of about 16 GPU hours."
        }
    ],
    "title": "Investigating Online Community Engagement through Stancetaking",
    "year": 2023
}