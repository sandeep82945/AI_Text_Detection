{
    "abstractText": "Recent studies have revealed the remarkable cross-lingual capability of multilingual pretrained language models (mPLMs), even when pre-trained without parallel corpora (monomPLMs). Intuitively, semantic alignments may be the reason behind such capability but remain under-explored. In this work, we investigate the alignment properties from the token perspective in mono-mPLMs and find that the alignments correspond to the geometric similarity of embedding space across different languages. Nevertheless, mono-mPLMs tend to damage this geometric similarity at the higher layers due to the lack of cross-lingual interactions, thus limiting their cross-lingual transfer capabilities. To address this issue, we introduce token-level and semantic-level code-switched masked language modeling, employing the selfinduced token alignments to explicitly improve cross-lingual interactions over layers of monomPLMs without relying on parallel sentences. We evaluate our method on various natural language understanding tasks and unsupervised machine translation tasks. The results demonstrate that our methods outperform the strong baselines and achieve comparable performance with mPLMs trained with parallel corpora.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Jinliang Lu"
        },
        {
            "affiliations": [],
            "name": "Yu Lu"
        },
        {
            "affiliations": [],
            "name": "Jiajun Zhang"
        }
    ],
    "id": "SP:e61873a4d43d4a6321e87dc3704b9c5cdfe5a657",
    "references": [
        {
            "authors": [
                "Xi Ai",
                "Bin Fang."
            ],
            "title": "On-the-fly cross-lingual masking for multilingual pre-training",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 855\u2013876, Toronto, Canada. Association",
            "year": 2023
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Gorka Labaka",
                "Eneko Agirre."
            ],
            "title": "Learning bilingual word embeddings with (almost) no bilingual data",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 451\u2013462,",
            "year": 2017
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Holger Schwenk."
            ],
            "title": "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
            "venue": "Transactions of the Association for Computational Linguistics, 7:597\u2013610.",
            "year": 2019
        },
        {
            "authors": [
                "Steven Cao",
                "Nikita Kitaev",
                "Dan Klein."
            ],
            "title": "Multilingual alignment of contextual word representations",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Aditi Chaudhary",
                "Karthik Raman",
                "Krishna Srinivasan",
                "Jiecao Chen."
            ],
            "title": "Dict-mlm: Improved multilingual pre-training using bilingual dictionaries",
            "venue": "arXiv preprint arXiv:2010.12566.",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Te Chen",
                "Yandi Xia",
                "Keiji Shinzato."
            ],
            "title": "Extreme multi-label classification with label masking for product attribute value extraction",
            "venue": "Proceedings of the Fifth Workshop on e-Commerce and NLP (ECNLP 5), pages 134\u2013140, Dublin, Ireland. Association",
            "year": 2022
        },
        {
            "authors": [
                "Zewen Chi",
                "Li Dong",
                "Shuming Ma",
                "Shaohan Huang",
                "Saksham Singhal",
                "Xian-Ling Mao",
                "Heyan Huang",
                "Xia Song",
                "Furu Wei."
            ],
            "title": "mT6: Multilingual pretrained text-to-text transformer with translation pairs",
            "venue": "Proceedings of the 2021 Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Zewen Chi",
                "Li Dong",
                "Furu Wei",
                "Nan Yang",
                "Saksham Singhal",
                "Wenhui Wang",
                "Xia Song",
                "Xian-Ling Mao",
                "Heyan Huang",
                "Ming Zhou."
            ],
            "title": "InfoXLM: An information-theoretic framework for cross-lingual language model pre-training",
            "venue": "Proceedings of the",
            "year": 2021
        },
        {
            "authors": [
                "Zewen Chi",
                "Li Dong",
                "Bo Zheng",
                "Shaohan Huang",
                "XianLing Mao",
                "Heyan Huang",
                "Furu Wei."
            ],
            "title": "Improving pretrained cross-lingual language models via self-labeled word alignment",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Guillaume Lample."
            ],
            "title": "Crosslingual language model pretraining",
            "venue": "Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
            "year": 2019
        },
        {
            "authors": [
                "Alexis Conneau",
                "Ruty Rinott",
                "Guillaume Lample",
                "Adina Williams",
                "Samuel Bowman",
                "Holger Schwenk",
                "Veselin Stoyanov."
            ],
            "title": "XNLI: Evaluating crosslingual sentence representations",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Nat-",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Junjie Hu",
                "Sebastian Ruder",
                "Aditya Siddhant",
                "Graham Neubig",
                "Orhan Firat",
                "Melvin Johnson."
            ],
            "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
            "venue": "Proceedings of the 37th International",
            "year": 2020
        },
        {
            "authors": [
                "Ganesh Jawahar",
                "Beno\u00eet Sagot",
                "Djam\u00e9 Seddah."
            ],
            "title": "What does BERT learn about the structure of language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651\u20133657, Florence, Italy",
            "venue": "Association for",
            "year": 2019
        },
        {
            "authors": [
                "Karthikeyan K",
                "Zihan Wang",
                "Stephen Mayhew",
                "Dan Roth."
            ],
            "title": "Cross-lingual ability of multilingual bert: An empirical study",
            "venue": "CoRR, abs/1912.07840.",
            "year": 2019
        },
        {
            "authors": [
                "Guillaume Lample",
                "Alexis Conneau",
                "Marc\u2019Aurelio Ranzato",
                "Ludovic Denoyer",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Word translation without parallel data",
            "venue": "In International Conference on Learning Representations",
            "year": 2018
        },
        {
            "authors": [
                "Patrick Lewis",
                "Barlas Oguz",
                "Ruty Rinott",
                "Sebastian Riedel",
                "Holger Schwenk."
            ],
            "title": "MLQA: Evaluating cross-lingual extractive question answering",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7315\u2013",
            "year": 2020
        },
        {
            "authors": [
                "moyer",
                "Zornitsa Kozareva",
                "Mona Diab",
                "Veselin Stoyanov",
                "Xian Li"
            ],
            "title": "Few-shot learning with multilingual generative language models",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Zehui Lin",
                "Xiao Pan",
                "Mingxuan Wang",
                "Xipeng Qiu",
                "Jiangtao Feng",
                "Hao Zhou",
                "Lei Li."
            ],
            "title": "Pretraining multilingual neural machine translation by leveraging alignment information",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Jiatao Gu",
                "Naman Goyal",
                "Xian Li",
                "Sergey Edunov",
                "Marjan Ghazvininejad",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "Multilingual denoising pretraining for neural machine translation",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2020
        },
        {
            "authors": [
                "Fuli Luo",
                "Wei Wang",
                "Jiahao Liu",
                "Yijia Liu",
                "Bin Bi",
                "Songfang Huang",
                "Fei Huang",
                "Luo Si"
            ],
            "title": "VECO: Variable and flexible cross-lingual pre-training",
            "year": 2021
        },
        {
            "authors": [
                "Joakim Nivre",
                "Mitchell Abrams",
                "\u017deljko Agi\u0107",
                "Lars Ahrenberg",
                "Lene Antonsen",
                "Maria Jesus Aranzabe",
                "Gashaw Arutie",
                "Masayuki Asahara",
                "Luma Ateyah",
                "Mohammed Attia"
            ],
            "title": "Universal dependencies 2.2",
            "year": 2018
        },
        {
            "authors": [
                "Xuan Ouyang",
                "Shuohuan Wang",
                "Chao Pang",
                "Yu Sun",
                "Hao Tian",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "ERNIE-M: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora",
            "venue": "Proceedings of the 2021 Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Xiaoman Pan",
                "Boliang Zhang",
                "Jonathan May",
                "Joel Nothman",
                "Kevin Knight",
                "Heng Ji."
            ],
            "title": "Cross-lingual name tagging and linking for 282 languages",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2017
        },
        {
            "authors": [
                "Telmo Pires",
                "Eva Schlinger",
                "Dan Garrette."
            ],
            "title": "How multilingual is multilingual BERT? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996\u20135001, Florence, Italy",
            "venue": "Association for Computational Linguis-",
            "year": 2019
        },
        {
            "authors": [
                "Machel Reid",
                "Mikel Artetxe."
            ],
            "title": "PARADISE: Exploiting parallel data for multilingual sequenceto-sequence pretraining",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2022
        },
        {
            "authors": [
                "Shuo Ren",
                "Yu Wu",
                "Shujie Liu",
                "Ming Zhou",
                "Shuai Ma."
            ],
            "title": "Explicit cross-lingual pre-training for unsupervised machine translation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna-",
            "year": 2019
        },
        {
            "authors": [
                "Shuo Ren",
                "Long Zhou",
                "Shujie Liu",
                "Furu Wei",
                "Ming Zhou",
                "Shuai Ma."
            ],
            "title": "SemFace: Pre-training encoder and decoder with a semantic interface for neural machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Uma Roy",
                "Noah Constant",
                "Rami Al-Rfou",
                "Aditya Barua",
                "Aaron Phillips",
                "Yinfei Yang."
            ],
            "title": "LAReQA: Language-agnostic answer retrieval from a multilingual pool",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Kaitao Song",
                "Xu Tan",
                "Tao Qin",
                "Jianfeng Lu",
                "Tie-Yan Liu."
            ],
            "title": "Mass: Masked sequence to sequence pretraining for language generation",
            "venue": "International Conference on Machine Learning, pages 5926\u20135936. PMLR.",
            "year": 2019
        },
        {
            "authors": [
                "Chau Tran",
                "Yuqing Tang",
                "Xian Li",
                "Jiatao Gu."
            ],
            "title": "Cross-lingual retrieval for iterative self-supervised training",
            "venue": "Advances in Neural Information Processing Systems, 33:2207\u20132219.",
            "year": 2020
        },
        {
            "authors": [
                "Amos Tversky",
                "Itamar Gati."
            ],
            "title": "Similarity, separability, and the triangle inequality",
            "venue": "Psychological review, 89(2):123.",
            "year": 1982
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Sebastian Ruder",
                "Anders S\u00f8gaard."
            ],
            "title": "Are all good word vector spaces isomorphic? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3178\u20133192, Online",
            "venue": "Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Xiangpeng Wei",
                "Rongxiang Weng",
                "Yue Hu",
                "Luxi Xing",
                "Heng Yu",
                "Weihua Luo."
            ],
            "title": "On learning universal representations across languages",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Shijie Wu",
                "Mark Dredze."
            ],
            "title": "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Jian Yang",
                "Shuming Ma",
                "Dongdong Zhang",
                "Shuangzhi Wu",
                "Zhoujun Li",
                "Ming Zhou."
            ],
            "title": "Alternating language modeling for cross-lingual pre-training",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9386\u20139393.",
            "year": 2020
        },
        {
            "authors": [
                "Zhen-Ru Zhang",
                "Chuanqi Tan",
                "Songfang Huang",
                "Fei Huang"
            ],
            "title": "Veco 2.0: Cross-lingual language model pre-training with multi-granularity contrastive learning",
            "venue": "arXiv preprint arXiv:2304.08205",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent studies show that multilingual pre-trained language models (mPLMs) significantly improve the performance of cross-lingual natural language processing tasks. Conventional mPLMs (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Xue et al., 2021) typically adopt multiple monolingual corpora (mono-mPLMs) to perform masked language modeling during pretraining, obtaining impressive and stable multilingual capabilities, which may intuitively result\n\u2217Corresponding author 1Our code is available in https://github.com/\nJinliangLu96/Self-Improving-Multilingual-PT\nfrom the semantic alignments but remains underexplored. Another line of research involves improving the multilingual pre-training by incorporating the cross-lingual parallel corpora (para-mPLMs) into pre-training (Conneau and Lample, 2019; Cao et al., 2020; Chi et al., 2021a,b,c; Luo et al., 2021; Wei et al., 2021; Ouyang et al., 2021). However, parallel sentences are not always available, especially for low-resource languages (Tran et al., 2020). And collecting such data often entails substantial costs. Therefore, exploring approaches to improve multilingual pre-training without using parallel corpora is important and worthy of study.\nTo achieve this, we first conduct analyses to investigate the token alignment properties of XLMR, a strong mono-mPLM that only uses multiple monolingual corpora for pre-training. Our empirical study demonstrates that the cross-lingual token alignments occur at the embedding layer (alignment location) with surprisingly high alignment accuracy (alignment degree) but they become weaker at the higher layers (Figure 1 (a)). We also find that the alignments are geometrically aligned instead of absolutely aligned (alignment format), as shown in Figure 1 (b). The phenomenon shows that token embeddings of different languages are separately distributed but geometrically similar.\nWe further compare the differences in geometric similarities of representations from the bottom layer to the top layer using mono-mPLMs and paramPLMs. And we find that the representations be-\ncome geometrically dissimilar at higher layers of mono-mPLMs while para-mPLMs would alleviate the problem by using parallel sentences, obtaining better cross-lingual transfer capability. It shows the necessity of explicit cross-lingual interactions.\nBased on the above observation, we propose self-improving methods to encourage cross-lingual interactions using self-induced token alignments. Intuitively, the masked tokens can be predicted with semantic-equivalent but slightly language-mixed contexts. Therefore, we first utilize self-induced alignments to perform token-level code-switched masked language modeling (TCS-MLM), which requests the model to predict original masked tokens with the semantic-equivalent but code-switched surrounding text. Considering that vanilla replacements usually lack diversity, we further propose a novel semantic-level code-switched masked language modeling (SCS-MLM), which replaces the context tokens with a weighted combination of multiple semantically similar ones in other languages. SCS-MLM involves on-the-fly semantic replacements during training, further enhancing the diversity of code-switched examples and cross-lingual interactions.\nWe evaluate our methods on various crosslingual transfer tasks. Specifically, we conduct experiments on natural language understanding tasks, including XNLI (Conneau et al., 2018) and PAWS-X (Hu et al., 2020) for sentence-pair classification, Wikiann (Pan et al., 2017) and UDPOS (Nivre et al., 2018) for structural prediction, MLQA (Lewis et al., 2020) for question answering, and Tatoeba (Artetxe and Schwenk, 2019) for sentence retrieval. We also perform experiments on unsupervised machine translation to evaluate the performance of the generation task. Experimental results demonstrate that our methods significantly improve the performance compared with the strong baselines, even surpassing some mPLMs pre-trained using parallel corpora. Further analysis demonstrates that our methods improve the geometric similarity of representations for different languages, and thus promoting the cross-lingual transfer capability.\nOur contributions are summarized as follows:\n\u2022 Our empirical study shows the existence of cross-lingual token alignments in monomPLMs. We further measure their accuracy, identify the location, and verify the format.\n\u2022 Comparing mono-mPLMs with para-mPLMs, we find that mono-mPLMs tend to disturb\nthe geometric similarities between representations at higher layers while para-mPLMs remain unaffected, showing the necessity of cross-lingual interactions during pre-training.\n\u2022 We propose token-level/semantic-level codeswitched masked language modeling to encourage cross-lingual interactions during pretraining, improving the cross-lingual transfer capability without relying on parallel corpora."
        },
        {
            "heading": "2 A Closer Look at Multilinguality",
            "text": "In this section, we take the commonly used XLMR, the strong mono-mPLM, as an example to show our observation2. Specifically, we first investigate the properties of cross-lingual token alignments in mono-mPLMs, showing their relation to geometric similarity. Then, we explore the variation of geometric similarity of representations across different layers and demonstrate that the geometric similarity at higher layers would be disturbed due to the lack of cross-lingual interactions, hindering the cross-lingual transfer capability."
        },
        {
            "heading": "2.1 Language-Specific Vocabulary",
            "text": "Generally, mPLMs adopt a huge vocabulary shared across 100+ languages. Different languages always have shared and independent tokens. Previous studies (Conneau and Lample, 2019; Pires et al., 2019; Wu and Dredze, 2019) regard the shared token as the source of cross-lingual capability. However, the latent relevance between language-specific tokens is not fully exploited.\nSuppose that each language l in the language set has a corresponding corpus Cl. We first record the tokens whose frequencies are larger than 100 in Cl, obtaining the vocabulary Vl for the specific language. Then, we remove shared tokens when processing two languages la and lb to avoid the impact of overlapping. Finally, we obtain the languagespecific vocabularies independent of each other:\nV\u0302la = {t|t \u2208 Vlaand t /\u2208 Vlb , freq(t) > 100} V\u0302lb = {t|t /\u2208 Vlaand t \u2208 Vlb , freq(t) > 100}"
        },
        {
            "heading": "2.2 Token-Alignments in Mono-mPLMs",
            "text": "After obtaining the language-specific vocabularies from other languages to English, we calculate the\n2We also conduct analyses on mT5 (Encoder-Decoder) and X-GLM (Decoder). The phenomenon also exists regardless of the architecture, which is included in Appendix C.\nsimilarity among token embeddings of XLM-R and directly export high-quality cross-lingual token alignments as dictionaries.\nSpecifically, we adopt the cross-domain similarity local scaling (CSLS) (Lample et al., 2018) to compute the token similarity from language X to language Y. For token embeddings x and y in two languages, the CSLS score is computed as:\nCSLS(x, y) = 2 cos(x, y)\u2212 rK(x)\u2212 rK(y) (1)\nwhere rK(x) is the average score from x to the Knearest target neighbourhoods N (x). And rK(y) is vice versa.\nrK(x) = 1\nK \u2211 y\u0302t\u2208N (x) cos(x, y\u0302t) (2)\nAccuracy of Token-Alignments To measure the quality of dictionaries, we collect golden dictionaries from wiki-dict3 and MUSE (Lample et al., 2018). The accuracy scores are shown in Table 1.\nWe find that the exported cross-lingual dictionaries have good quality, demonstrating that monomPLMs learn the alignments between different languages using monolingual corpora only. Particularly, distant language pairs, such as En-Ja (63.97%) and En-Ko (62.52%), have higher accuracy scores, while they usually have little overlapping tokens as anchor points for alignment. The phenomenon directly proves that the cross-lingual ability does not only depend on the overlapping tokens in different languages. Another potential factor could be the occurrence of words with similar meanings at comparable frequencies across languages (K et al., 2019). Using language modeling as the objective may unearth such regularities and stimulate the cross-lingual transfer capability.\n3https://github.com/onny/wikidict\nFormat of Token-Alignments The second question is whether the alignments are absolute alignment or geometrical alignment (Figure 1 (b)). Absolute alignment requests the token embeddings are language-agnostic (Artetxe et al., 2017; Lample et al., 2018) while geometrical alignment focuses on the correspondence between tokens in different languages (Vulic\u0301 et al., 2020). The latter just need to have a similar geometric spatial structure while keeping language characteristics (Roy et al., 2020).\nThus, we visualize the embedding of tokens in the exported dictionaries. The results across five diverse languages are shown in Figure 2 (a). We can find that the token embeddings are separately distributed in space according to the language similarity instead of aggregating together, showing that the alignments are geometrical alignment. We also use RSIM4 to measure the geometric similarity in Table 1. By contrast, token representations at the top layer aggregate together (Figure 2 (b)).\nLocation of Token-Alignments Since top-layer hidden states aggregate, whether the accuracy of token alignments improves from the bottom to the top layers becomes a question. To answer the question, we compute the token alignment accuracy, average\n4RSIM means relational similarity, which measures the Pearson correlation coefficient between the cosine similarity matrices of intra-language tokens. It measures the degree of isomorphism (geometric similarity) of embedding spaces. And the calculation method is included in the appendix B.\ncosine similarity, and RSIM scores using hidden states of different layers. Figure 3 (a) shows that with the layers becoming higher, token alignment accuracy decreases but the average cosine similarity between translation pairs increases, demonstrating that cross-lingual token alignments mainly exist in the embedding layer while higher layers focus on the aggregation of language-specific token representations. Moreover, Figure 3 (b) shows that the geometric similarities between language-specific token representations at top layers become weaker."
        },
        {
            "heading": "2.3 Cross-Lingual Interactions Matters for Geometric Similarity Maintenance",
            "text": "\u00a72.2 shows that language-specific token representations of XLM-R at higher layers aggregate together (Figure 2 (b)) but the alignment and geometric similarity are disturbed (Figure 3). Since para-mPLMs usually obtain better performance on cross-lingual transfer tasks, we compare the difference between para-mPLMs and XLM-R (mono-mPLM) in the above aspects. Specifically, we choose VECO (Luo et al., 2021) and INFOXLM (Chi et al., 2021b) as representatives of para-mPLMs, which are pretrained with monolingual and parallel corpora, obtaining improvements compared with XLM-R.\nFigure 4 (a) shows the phenomenon of token alignment accuracy and cosine similarity across layers. We find that different mPLMs exhibit similar behavior, wherein both mono-mPLM and paramPLMs tend to aggregate token representations while ignoring alignments at the higher layers. The reason behind this may lie in that higher layers of\nPLMs prioritize complex semantic combinations rather than token features (Jawahar et al., 2019).\nFigure 4 (b) compares the average RSIM scores of different mPLMs. VECO and INFOXLM have higher RSIM scores than XLM-R cross layers, showing that parallel corpora would improve the geometric similarity between languages. Furthermore, RSIM scores of VECO/INFOXLM across layers are more balanced than XLM-R. It demonstrates that explicit cross-lingual interactions (parallel corpora) are useful in maintaining geometric similarity in mPLMs, which could be one of the factors contributing to better cross-lingual transfer capability than mono-mPLMs."
        },
        {
            "heading": "3 Our Method",
            "text": "\u00a72.2 demonstrates that mono-mPLMs learn crosslingual token alignments and can export them as high-quality dictionaries. \u00a72.3 shows explicit crosslingual interactions may enhance cross-lingual transfer capability. These observations motivate us to explore self-improving methods to increase cross-lingual interactions without relying on parallel corpora. Next, we introduce our proposed tokenlevel/semantic-level code-switch masked language modeling for multilingual pre-training."
        },
        {
            "heading": "3.1 Token-Level Code-Switch MLM (TCS)",
            "text": "Previous code-switch methods either rely on the existing bilingual dictionaries (Lin et al., 2020; Chaudhary et al., 2020) or the alignment tools to build the alignment pairs using parallel sentences (Ren et al., 2019; Yang et al., 2020). Our work proves that the self-induced dictionaries from mono-mPLMs are accurate enough, which would help mono-mPLMs self-improving and do not require the use of prior bilingual knowledge.\nTherefore, we replace 10\u223c15% tokens using the dictionary to construct multilingual code-switched contexts but keep the masked positions unchanged, forcing the model to predict the masked tokens with different but semantic-equivalent contexts. For example, the original English token sequence (after cutting) is converted into a code-switch one using the self-induced dictionary (En-De):\n_A _cat _sit _on _the [mask] . \u21d2LMLM \u21d3\n_A _Katze _sit _on _the [mask] . \u21d2LTCS-MLM Both the original and code-switched sentences are fed into mono-mPLMs to perform masked lan-\nguage modeling. The training loss is:\nL = LMLM + LTCS-MLM (3)"
        },
        {
            "heading": "3.2 Semantic-Level Code-Switch MLM (SCS)",
            "text": "Considering that token replacements often lack diversity, we propose a novel semantic-level codeswitch method, which replaces 10\u223c15% tokens with the average weighting of its neighbors in another language, as shown in Figure 5.\nConsidering that mPLMs provide contextual output distributions across the vocabulary and avoid polysemy problems (Tversky and Gati, 1982), we first utilize the mono-mPLM to obtain the output probability distribution across the vocabulary for tokens. Then, we choose top-k5 tokens according to probabilities and average-weight their embeddings as the contextual token representation x\u0302:\nx\u0302 = \u2211K\ni=1 pi \u00b7 ei (4)\nwhere pi is the normalized probability of i-th token in the top-k tokens.\nAfter obtaining the contextual representations, we adopt the embedding table to search for corresponding translations on-the-fly instead of directly using the discrete dictionaries, which would improve the diversity of training examples and keep the semantics. Similarly, we also keep top-k translation candidates and average-weighting their embedding as the replacement y\u0302:\ny\u0302 = \u2211K\nj=1 qj \u00b7 ej (5)\nwhere qj is the normalized CSLSon-the-fly scores6 across the top-k tokens in the corresponding language-specific vocabulary V\u0302 .\nqj = exp(q\u0302j)\u2211K\nm=1 exp(q\u0302j) (6)\nq\u0302j = CSLSon-the-fly(x\u0302, yj), yj \u2208 V\u0302 (7)\nSame as \u00a73.1, we request the mono-mPLMs to perform masked language modeling based on the original examples and semantically code-switched ones. The final training loss is:\nL = LMLM + LSCS-MLM (8) 5k is set as 8 in our experiments. 6During training, we compute CSLSon-the-fly scores for tokens in the same batch to avoid expensive computational costs."
        },
        {
            "heading": "4 Pre-training Settings",
            "text": "Pre-training Data We collect monolingual corpora from Common Crawl Corpus, which contains about 890GB data for 50 languages. Different from previous studies, we do not use any bilingual corpus. Following (Conneau and Lample, 2019), we sample multilingual data according to a multinomial distribution with probabilities. Considering the pre-training corpora in N languages with ni training instances for the i-th language, the probability for i-th language can be calculated as:\npi = n\u03b1i\u2211N i=1 n \u03b1 k\n(9)\nwhere \u03b1 is set as 0.7.\nModel Configuration Due to the restriction of resources, we conduct experiments on the Transformer encoder models to verify the effectiveness of our method. For fair comparisons with previous studies on natural language understanding tasks, we pre-train a 12-layer Transformer encoder as the BASE model (768 hidden dimensions and 12 attention heads) and a 24-layer Transformer encoder as the LARGE model (1024 hidden dimensions and 16 attention heads) using fairseq toolkit. The activation function used is GeLU. Following (Chi et al., 2021b; Luo et al., 2021; Ouyang et al., 2021), we initialize the parameters with XLM-R.\nWe also pre-train the 6-layer Transformer encoder (1024 hidden dimensions and 8 attention heads), which is adopted to evaluate the performance on unsupervised machine translation.\nOptimization Settings We use the Adam optimizer to train our model, whose learning rate is scheduled with a linear decay with 4000 warm-up steps. The peaking learning rates are separately set as 2e-4 and 1e-4 for BASE and LARGE model. Pretraining is conducted using 8 Nvidia A100-80GB GPUs with 2048 batch size. The BASE model takes about 1 month and the LARGE model takes about 2 months for pre-training. Appendix A shows more details about the pre-training settings."
        },
        {
            "heading": "5 Experiments on Downstream Tasks",
            "text": ""
        },
        {
            "heading": "5.1 Natural Language Understanding",
            "text": ""
        },
        {
            "heading": "5.1.1 Experimental Settings",
            "text": "We consider four kinds of cross-lingual NLU tasks:\nSentence-Pair Classification We choose XNLI (Conneau et al., 2018) for cross-lingual language inference and PAWS-X (Hu et al., 2020) for crosslingual paraphrase identification.\nStructural Prediction We choose UDPOS (Nivre et al., 2018) for pos-tagging and Wikiann (Pan et al., 2017) for name entity recognition.\nQuestion Answering We choose MLQA (Lewis et al., 2020) for cross-lingual question answering.\nCross-lingual Retrieval We choose Tatoeba (Artetxe and Schwenk, 2019) for parallel sentence identification."
        },
        {
            "heading": "5.1.2 Experimental Results",
            "text": "We conduct experiments on the above crosslingual NLU tasks to evaluate the cross-lingual transfer capability of our method: fine-tune the model with the English training set and evaluate\nthe foreign language test sets. We separately describe the results as follows:\nSentence-Pair Classification The cross-lingual natural language inference (XNLI) aims to determine the relationship between the two input sentences, entailment, neural or contradiction. And PAWS-X aims to judge whether the two sentences are paraphrases or not.\nAs shown in Table 2, our SCS-MLMBASE surpasses the baseline models including mBERT, XLM, XLM-RBASE and Unicoder. Moreover, our SCS-MLMLARGE obtains equivalent performance with some pre-trained models using parallel sentences, including VECOLARGE and HICTLLARGE. In contrast, although TCS-MLM also obtains improvements, it is not as good as SCS-MLM. We suppose that the limited dictionaries would lead to insufficient cross-lingual interactions.\nStructural Prediction Structural prediction task contains UDPOS and Wikiann. Given a sentence, UDPOS aims to label the pos-tagging for tokens\nand Wikiann aims to identify name entities. We reported the average F1 score for each dataset.\nTable 3 shows the results of our models. Compared with previous studies, our proposed SCSMLMLARGE obtains the best results on UDPOS, achieving 76.0 F1 score. For Wikiann, our TCSMLMLARGE and SCS-MLMLARGE also obtain significant improvements compared with the strong baseline XLM-R. We suppose that the induced dictionaries contain the relations of entities and postagging across different languages, which promotes improvements in the structural prediction tasks.\nCross-lingual Question Answering MLQA aims to answer questions based on the given paragraph, which contains 7 languages.\nThe F1/EM scores are shown in Table 4. We can find that our proposed TCS-MLM and SCS-MLM are significantly better than the strong baseline XLM-R and even surpass some models pre-trained with parallel sentences, such as VECO, VECO 2.0 and HICTL. Although our methods cannot surpass ERNIE-MLARGE, they narrow the gaps between mPLMs training with or without parallel sentences, demonstrating the effectiveness of our methods.\nCross-lingual Retrieval To evaluate the crosslingual sentence retrieval capability of our models, we choose a subset of the Tatoeba dataset (36 language pairs), which aims to identify the parallel sentence among 1000 candidates. Following previous studies, we used the averaged representation in the middle layer of different models (XLM-RBASE, + TCS-MLMBASE and + SCS-MLMBASE) to evaluate the retrieval task.\nThe results are shown in Figure 6. We can\nfind that our proposed SCS-MLMBASE obtains better retrieval accuracy scores (average acc. 60.73) than TCS-MLMBASE (+0.49 acc.) and XLM-RBASE (+7.46 acc.) across language directions, demonstrating the effectiveness of our method."
        },
        {
            "heading": "5.2 Natural Language Generation - UNMT",
            "text": "As our proposed pre-training methods do not rely on parallel sentences, we choose the harder task - unsupervised neural machine translation (UNMT) to evaluate the performance on the generation task."
        },
        {
            "heading": "5.2.1 Experimental Results",
            "text": "Table 5 shows the translation performance on WMT14 En-Fr, WMT16 En-De, WMT16 En-Ro test sets. We can find that our proposed SCS-MLM can improve the translation quality compared with the strong baselines, XLM and MASS. For example, SCS-MLM respectively outperforms XLM and MASS by 1.1 and 1.2 BLEU scores in WMT16 En\u2192Ro. SCS-MLM also surpasses previous studies on average, verifying its effectiveness. Moreover, the results also show that our method is suitable for the seq2seq model - MASS (Figure 8 in\nappendix A.3), demonstrating that our method is independent of the model architectures."
        },
        {
            "heading": "5.3 SCS-MLM Improves Geometric Similarity at Higher Layers",
            "text": "\u00a72.3 shows that para-mPLMs would maintain the geometric similarity of language-specific token representations across layers. As our method incorporate explicit cross-lingual interactions into pretraining, a similar phenomenon should occur.\nTherefore, we plot the RSIM scores across 24- layer LARGE models for measurement. As shown in Figure 7 (a), compared with the baseline model, our proposed SCS-MLM increases the geometric similarity of different languages across layers. Figure 7 (b) shows the RSIM improvements focus on the higher layers, thereby achieving balanced geometric similarities, akin to the observations of para-mPLMs in Figure 4 (b). It could illustrate the reason why our method is effective on various cross-lingual transfer tasks."
        },
        {
            "heading": "6 Related Work",
            "text": "Multilingual pre-trained language models begin with mBERT (Devlin et al., 2019) and XLM (Conneau and Lample, 2019), which learn the shared\nfeature space among languages using multiple monolingual corpora. XLM-R (Conneau et al., 2020) shows the effects of models when trained on a large-scale corpus, establishing strong baselines for subsequent studies.\nBased on the observation that parallel corpora would help cross-lingual alignment in (Conneau and Lample, 2019), many studies pay attention to the usage of the parallel corpus. Unicoder (Huang et al., 2019) employs a multi-task learning framework to learn cross-lingual semantic representations. ALM (Yang et al., 2020) and PARADISE (Reid and Artetxe, 2022) uses parallel sentences to construct code-switch sentences. INFOXLM (Chi et al., 2021b) and HICTL (Wei et al., 2021) respectively employ sentence-level and token-level contrastive learning for cross-lingual semantic alignments. VECO (Luo et al., 2021) proposes a variable framework to enable the model to process understanding and generation tasks. ERNIE-M (Ouyang et al., 2021) generates pseudo-training examples, further improving the overall performance on downstream tasks. (Ai and Fang, 2023) explores the MASK strategy (prototype-word) to improve cross-lingual pre-training.\nBesides the above discriminative models, generative models, such as MASS (Song et al., 2019), mBART (Liu et al., 2020), mT5 (Xue et al., 2021), XGLM (Lin et al., 2022), BLOOM (Scao et al., 2022), also show impressive performance on generation tasks. MASS, mT5 and mBART pre-train the denoising seq2seq model, handling both NLU and NLG tasks. XGLM and BLOOM pre-train extremely large auto-regressive models using extremely large multiple monolingual corpora, showing surprising in-context learning (Brown et al.,\n2020) capability on cross-lingual tasks. Different from previous studies, our work first investigates the properties of token alignments behind multilinguality and then proposes selfimproving methods for multilingual pre-training with only monolingual corpora, alleviating the need for parallel sentences."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we first investigate the properties of cross-lingual token alignments in mono-mPLMs, and then make a comparison between monomPLMs and para-mPLMs, demonstrating that geometric similarities of higher-layer representations would be damaged without explicit cross-lingual interactions, hindering the multilinguality. Therefore, we propose token-level and semantic-level codeswitch masked language modeling to improve the cross-lingual interactions without relying on parallel corpora. Empirical results on language understanding and generation tasks demonstrate the effectiveness of our methods. Future work would adapt our methods to much larger language models."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by National Key R&D Program of China 2022ZD0160602 and the Natural Science Foundation of China 62122088.\nLimitations\nIn this paper, we investigate the token-level crosslingual alignments but do not focus on the sentencelevel alignments, which occur in the middle layer of mPLMs. We leave it as our future work. Then, we propose token-level and semantic-level codeswitch masked language modeling for multilingual pre-training. As we analyzed in Appendix C, various types of mono-mPLMs are capable of forming alignments, but they are unable to preserve geometric similarities across layers. This phenomenon is observed irrespective of the language modeling methods employed. However, due to resource limitations, we only conduct experiments on XLM-R and MASS models. We will verify the effectiveness of our methods on much larger language models in the future, such as XGLM and BLOOM."
        },
        {
            "heading": "A Experimental Settings",
            "text": "A.1 Pre-Training Data\nWe use the open-source CC-100 corpora7 for the pre-training of BASE and LARGE models. Due to the resource and memory restrictions, we just select 50 languages that cover the downstream tasks and conduct random sampling following(Luo et al., 2021). Table 6 shows the statistics of the monolingual data in each language.\nA.2 Hyperparameters for Pre-training\nTable 7 shows the hyperparameters for pre-training different size models. SMALL models are initialized by XLM or MASS. And they are used to conduct experiments on unsupervised machine translation. BASE model and LARGE model are initialized by XLM-R model and used for various natural language understanding tasks.\n7https://data.statmt.org/cc-100/\nA.3 Experimental Settings of UNMT\nWe follow the common practices to conduct experiments on UNMT benchmarks. For evaluation, we separately adopt newsdev/test 2014 En-Fr, newsdev/test 2016 En-De, newsdev/test 2016 En-Ro as development and test sets.\nFor a fair comparison with previous studies, we pre-train the cross-lingual language models with the same model architecture of XLM and MASS. The pre-training data for UNMT is shown in Table 8. We compare SCS-MLM with other UNMT pretraining methods (Ren et al., 2019, 2021; Song et al., 2019; Ai and Fang, 2022), which have the equivalent number of parameters.\nDuring inference, we use the beam size 1 and length penalty 1.0. To be consistent with previous works, we use multi-bleu.perl to measure the translation quality.\nThe illustration of our method on MASS is shown in Figure 8, which is similar to Figure 5 but needs to predict multiple adjacent tokens using the sequence-to-sequence model."
        },
        {
            "heading": "B RSIM - Relational Similarity",
            "text": "In NLP, the geometric similarity between two embedding spaces can be measured by Relational Similarity (RSIM). Given s translation pairs, we first calculate pairwise cosine similarities among intralanguage tokens and obtain two vectors a add b:\na = cos(x0, x1), cos(x0, x2), \u00b7 \u00b7 \u00b7 , cos(xs, xs\u22121) b = cos(y0, y1), cos(y0, y2), \u00b7 \u00b7 \u00b7 , cos(ys, ys\u22121)\nThen, we compute the Pearson\u2019s correlation between a add b, which is known as Relational Similarity (Vulic\u0301 et al., 2020)."
        },
        {
            "heading": "C Cross-Lingual Alignments on Different mPLMs",
            "text": "We also evaluate the accuracy of the token alignments across different language models. The results are shown in Table 9. Some examples are included in Figure 11.\nWe find that different kinds of language models form cross-lingual alignments. It demonstrates that pre-trained language models automatically learn cross-lingual mapping based on language modeling regardless of the specific modeling methods (i.e. masked language modeling, text span prediction, or casual language modeling). For different kinds\nof pre-trained models, the alignment accuracy increases with the size of the parameters. Moreover, all the models show a similar pattern that distant language pairs have higher alignment accuracy.\nFurthermore, we also draw the RSIM scores across layers of different mono-mPLMs in Figure 10. We find that different models share a similar phenomenon that RSIM scores are higher at the lower layers but lower at higher layers. None of them could keep the geometric similarity balanced like para-mPLMs, VECO, or InfoXLM, as shown\nin Figure 4. Therefore, we argue that explicit crosslingual interactions still matter regardless of different architectures."
        },
        {
            "heading": "D Ablation Study - Effect of k",
            "text": "In the proposed method, SCS-MLM, k plays an important role. Considering that the pre-training of BASE and LARGE models are time-consuming, we pre-train SMALL models with different k and\nevaluate the performance on UNMT task (WMT16 En\u2194Ro). As shown in Figure 9, we can find that SCS-MLM obtains the best performance when k is set as 8. Therefore, we set k=8 for all the experiments in our paper."
        },
        {
            "heading": "E Experimental Results Details",
            "text": "Due to space limitations, we just report the average cross-lingual transfer metric scores in the main paper. The details for each language test set are listed in Table 10-14.\nTranslate-train-all is another evaluation method for multilingual pre-trained language models. It means fine-tuning a multilingual model on the concatenation of all data (English training corpus and translated training corpus in other languages). Although our method mainly focuses on improving cross-lingual transfer capability, it can also bring improvements in Translate-train-all settings.\nThese results are also provided in Table 10-11."
        }
    ],
    "title": "Take a Closer Look at Multilinguality! Improve Multilingual Pre-Training Using Monolingual Corpora Only",
    "year": 2023
}