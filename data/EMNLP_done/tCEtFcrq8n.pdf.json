{
    "abstractText": "Few-shot named entity recognition (NER) has shown remarkable progress in identifying entities in low-resource domains. However, fewshot NER methods still struggle with out-ofdomain (OOD) examples due to their reliance on manual labeling for the target domain. To address this limitation, recent studies enable generalization to an unseen target domain with only a few labeled examples using data augmentation techniques. Two important challenges remain: First, augmentation is limited to the training data, resulting in minimal overlap between the generated data and OOD examples. Second, knowledge transfer is implicit and insufficient, severely hindering model generalizability and the integration of knowledge from the source domain. In this paper, we propose a framework, prompt learning with type-related features (PLTR), to address these challenges. To identify useful knowledge in the source domain and enhance knowledge transfer, PLTR automatically extracts entity type-related features (TRFs) based on mutual information criteria. To bridge the gap between training and OOD data, PLTR generates a unique prompt for each unseen example by selecting relevant TRFs. We show that PLTR achieves significant performance improvements on in-domain and cross-domain datasets. The use of PLTR facilitates model adaptation and increases representation similarities between the source and unseen domains.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Zihan Wang"
        },
        {
            "affiliations": [],
            "name": "Ziqi Zhao"
        },
        {
            "affiliations": [],
            "name": "Zhumin Chen"
        },
        {
            "affiliations": [],
            "name": "Pengjie Ren"
        },
        {
            "affiliations": [],
            "name": "Maarten de Rijke"
        },
        {
            "affiliations": [],
            "name": "Zhaochun Ren"
        }
    ],
    "id": "SP:8c0670ec91908b75a250b13a7d9347bcafb4ac8e",
    "references": [
        {
            "authors": [
                "Jonathan Baxter"
            ],
            "title": "A model of inductive bias",
            "year": 2000
        },
        {
            "authors": [
                "Eyal Ben-David",
                "Nadav Oved",
                "Roi Reichart."
            ],
            "title": "PADA: example-based prompt learning for on-the-fly adaptation to unseen domains",
            "venue": "Trans. Assoc. Comput. Linguistics, 10:414\u2013433.",
            "year": 2022
        },
        {
            "authors": [
                "Jiawei Chen",
                "Qing Liu",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun."
            ],
            "title": "Few-shot named entity recognition with self-describing networks",
            "venue": "ACL, pages 5711\u2013 5722.",
            "year": 2022
        },
        {
            "authors": [
                "Shuguang Chen",
                "Gustavo Aguilar",
                "Leonardo Neves",
                "Thamar Solorio."
            ],
            "title": "Data augmentation for crossdomain named entity recognition",
            "venue": "EMNLP, pages 5346\u20135356.",
            "year": 2021
        },
        {
            "authors": [
                "Yanru Chen",
                "Yanan Zheng",
                "Zhilin Yang."
            ],
            "title": "Prompt-based metric learning for few-shot NER",
            "venue": "CoRR, abs/2211.04337.",
            "year": 2022
        },
        {
            "authors": [
                "Leyang Cui",
                "Yu Wu",
                "Jian Liu",
                "Sen Yang",
                "Yue Zhang."
            ],
            "title": "Template-based named entity recognition using BART",
            "venue": "ACL-IJCNLP, pages 1835\u20131845.",
            "year": 2021
        },
        {
            "authors": [
                "Leyang Cui",
                "Yue Zhang."
            ],
            "title": "Hierarchicallyrefined label attention network for sequence labeling",
            "venue": "EMNLP-IJCNLP, pages 4115\u20134128.",
            "year": 2019
        },
        {
            "authors": [
                "Sarkar Snigdha Sarathi Das",
                "Arzoo Katiyar",
                "Rebecca J. Passonneau",
                "Rui Zhang."
            ],
            "title": "Container: Fewshot named entity recognition via contrastive learning",
            "venue": "ACL, pages 6338\u20136353.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT, pages 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Guanting Dong",
                "Zechen Wang",
                "Jinxu Zhao",
                "Gang Zhao",
                "Daichi Guo",
                "Dayuan Fu",
                "Tingfeng Hui",
                "Chen Zeng",
                "Keqing He",
                "Xuefeng Li",
                "Liwen Wang",
                "Xinyue Cui",
                "Weiran Xu"
            ],
            "title": "A multi-task semantic decomposition framework with task-specific pre-training",
            "year": 2023
        },
        {
            "authors": [
                "Jinyuan Fang",
                "Xiaobin Wang",
                "Zaiqiao Meng",
                "Pengjun Xie",
                "Fei Huang",
                "Yong Jiang."
            ],
            "title": "MANNER: A variational memory-augmented model for cross domain few-shot named entity recognition",
            "venue": "ACL, pages 4261\u20134276.",
            "year": 2023
        },
        {
            "authors": [
                "Yu Su."
            ],
            "title": "Thinking about GPT-3 in-context learning for biomedical IE? think again",
            "venue": "EMNLP, pages 4497\u20134512.",
            "year": 2022
        },
        {
            "authors": [
                "Ridong Han",
                "Tao Peng",
                "Chaohao Yang",
                "Benyou Wang",
                "Lu Liu",
                "Xiang Wan."
            ],
            "title": "Is information extraction solved by ChatGPT? An analysis of performance, evaluation criteria, robustness and errors",
            "venue": "CoRR, abs/2305.14450.",
            "year": 2023
        },
        {
            "authors": [
                "Jinpeng Hu",
                "Dandan Guo",
                "Yang Liu",
                "Zhuo Li",
                "Zhihong Chen",
                "Xiang Wan",
                "Tsung-Hui Chang."
            ],
            "title": "A simple yet effective subsequence-enhanced approach for cross-domain NER",
            "venue": "AAAI, pages 12890\u2013 12898.",
            "year": 2023
        },
        {
            "authors": [
                "Jiaxin Huang",
                "Chunyuan Li",
                "Krishan Subudhi",
                "Damien Jose",
                "Shobana Balakrishnan",
                "Weizhu Chen",
                "Baolin Peng",
                "Jianfeng Gao",
                "Jiawei Han."
            ],
            "title": "Fewshot named entity recognition: An empirical baseline study",
            "venue": "EMNLP, pages 10408\u201310423.",
            "year": 2021
        },
        {
            "authors": [
                "Chen Jia",
                "Xiaobo Liang",
                "Yue Zhang."
            ],
            "title": "Crossdomain NER using cross-domain language modeling",
            "venue": "ACL, pages 2464\u20132474.",
            "year": 2019
        },
        {
            "authors": [
                "Chen Jia",
                "Yue Zhang."
            ],
            "title": "Multi-cell compositional LSTM for NER domain adaptation",
            "venue": "ACL, pages 5906\u20135917.",
            "year": 2020
        },
        {
            "authors": [
                "Dong-Ho Lee",
                "Akshen Kadakia",
                "Kangmin Tan",
                "Mahak Agarwal",
                "Xinyu Feng",
                "Takashi Shibuya",
                "Ryosuke Mitani",
                "Toshiyuki Sekiya",
                "Jay Pujara",
                "Xiang Ren"
            ],
            "title": "Good examples make a faster learner: Simple demonstration-based learning for low-resource NER",
            "year": 2022
        },
        {
            "authors": [
                "Ji Young Lee",
                "Franck Dernoncourt",
                "Peter Szolovits."
            ],
            "title": "Transfer learning for named-entity recognition with neural networks",
            "venue": "LREC.",
            "year": 2018
        },
        {
            "authors": [
                "Dongfang Li",
                "Baotian Hu",
                "Qingcai Chen."
            ],
            "title": "Prompt-based text entailment for low-resource named entity recognition",
            "venue": "COLING, pages 1896\u2013 1903.",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zihan Liu",
                "Yan Xu",
                "Tiezheng Yu",
                "Wenliang Dai",
                "Ziwei Ji",
                "Samuel Cahyawijaya",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "CrossNER: Evaluating crossdomain named entity recognition",
            "venue": "AAAI, volume 35, pages 13452\u201313460.",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Ruotian Ma",
                "Zhang Lin",
                "Xuanting Chen",
                "Xin Zhou",
                "Junzhe Wang",
                "Tao Gui",
                "Qi Zhang",
                "Xiang Gao",
                "Yun Wen Chen."
            ],
            "title": "Coarse-to-fine few-shot learning for named entity recognition",
            "venue": "ACL, pages 4115\u20134129.",
            "year": 2023
        },
        {
            "authors": [
                "Ruotian Ma",
                "Xin Zhou",
                "Tao Gui",
                "Yiding Tan",
                "Linyang Li",
                "Qi Zhang",
                "Xuanjing Huang."
            ],
            "title": "Templatefree prompt tuning for few-shot NER",
            "venue": "NAACL, pages 5721\u20135732.",
            "year": 2022
        },
        {
            "authors": [
                "Xuezhe Ma",
                "Eduard Hovy."
            ],
            "title": "End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF",
            "venue": "ACL, pages 1064\u20131074.",
            "year": 2016
        },
        {
            "authors": [
                "David Nadeau",
                "Satoshi Sekine."
            ],
            "title": "A survey of named entity recognition and classification",
            "venue": "Lingvisticae Investigationes, 30(1):3\u201326.",
            "year": 2007
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "CoRR, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Sinno Jialin Pan",
                "Zhiqiang Toh",
                "Jian Su."
            ],
            "title": "Transfer joint embedding for cross-domain named entity recognition",
            "venue": "ACM Trans. Inf. Syst., 31(2):1\u201327.",
            "year": 2013
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using siamese BERTnetworks",
            "venue": "EMNLP-IJCNLP, pages 3980\u20133990.",
            "year": 2019
        },
        {
            "authors": [
                "Erik F. Tjong Kim Sang",
                "Fien De Meulder."
            ],
            "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
            "venue": "CoNLL, pages 142\u2013147.",
            "year": 2003
        },
        {
            "authors": [
                "Zheyan Shen",
                "Jiashuo Liu",
                "Yue He",
                "Xingxuan Zhang",
                "Renzhe Xu",
                "Han Yu",
                "Peng Cui."
            ],
            "title": "Towards out-of-distribution generalization: A survey",
            "venue": "CoRR, abs/2108.13624.",
            "year": 2021
        },
        {
            "authors": [
                "Xiaofei Sun",
                "Linfeng Dong",
                "Xiaoya Li",
                "Zhen Wan",
                "Shuhe Wang",
                "Tianwei Zhang",
                "Jiwei Li",
                "Fei Cheng",
                "Lingjuan Lyu",
                "Fei Wu",
                "Guoyin Wang."
            ],
            "title": "Pushing the limits of ChatGPT on NLP tasks",
            "venue": "CoRR, abs/2306.09719.",
            "year": 2023
        },
        {
            "authors": [
                "Xing Xie",
                "Yue Zhang."
            ],
            "title": "USB: A unified semi-supervised learning benchmark for classification",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Yidong Wang",
                "Hao Chen",
                "Qiang Heng",
                "Wenxin Hou",
                "Yue Fan",
                "Zhen Wu",
                "Jindong Wang",
                "Marios Savvides",
                "Takahiro Shinozaki",
                "Bhiksha Raj",
                "Bernt Schiele",
                "Xing Xie."
            ],
            "title": "Freematch: Self-adaptive thresholding for semi-supervised learning",
            "venue": "ICLR.",
            "year": 2023
        },
        {
            "authors": [
                "Zihan Wang",
                "Hongye Song",
                "Zhaochun Ren",
                "Pengjie Ren",
                "Zhumin Chen",
                "Xiaozhong Liu",
                "Hongsong Li",
                "Maarten de Rijke."
            ],
            "title": "Cross-domain contract element extraction with a bi-directional feedback clause-element relation network",
            "venue": "SIGIR, pages",
            "year": 2021
        },
        {
            "authors": [
                "Sam Wiseman",
                "Karl Stratos."
            ],
            "title": "Label-agnostic sequence labeling by copying nearest neighbors",
            "venue": "ACL, pages 5363\u20135369.",
            "year": 2019
        },
        {
            "authors": [
                "Ikuya Yamada",
                "Akari Asai",
                "Hiroyuki Shindo",
                "Hideaki Takeda",
                "Yuji Matsumoto."
            ],
            "title": "LUKE: Deep contextualized entity representations with entityaware self-attention",
            "venue": "EMNLP, pages 6442\u20136454.",
            "year": 2020
        },
        {
            "authors": [
                "Linyi Yang",
                "Lifan Yuan",
                "Leyang Cui",
                "Wenyang Gao",
                "Yue Zhang."
            ],
            "title": "Factmix: Using a few labeled in-domain examples to generalize to cross-domain named entity recognition",
            "venue": "COLING, pages 5360\u2013 5371.",
            "year": 2022
        },
        {
            "authors": [
                "Yi Yang",
                "Arzoo Katiyar."
            ],
            "title": "Simple and effective few-shot named entity recognition with structured nearest neighbor learning",
            "venue": "EMNLP, pages 6365\u2013 6375.",
            "year": 2020
        },
        {
            "authors": [
                "Zhilin Yang",
                "Ruslan Salakhutdinov",
                "William W. Cohen."
            ],
            "title": "Transfer learning for sequence tagging with hierarchical recurrent networks",
            "venue": "ICLR.",
            "year": 2017
        },
        {
            "authors": [
                "Xiangji Zeng",
                "Yunliang Li",
                "Yuchen Zhai",
                "Yin Zhang."
            ],
            "title": "Counterfactual generator: A weaklysupervised method for named entity recognition",
            "venue": "EMNLP, pages 7270\u20137280.",
            "year": 2020
        },
        {
            "authors": [
                "Xinghua Zhang",
                "Bowen Yu",
                "Yubin Wang",
                "Tingwen Liu",
                "Taoyu Su",
                "Hongbo Xu."
            ],
            "title": "Exploring modular task decomposition in cross-domain named entity recognition",
            "venue": "SIGIR, pages 301\u2013311.",
            "year": 2022
        },
        {
            "authors": [
                "Yufeng Zhang",
                "Xueli Yu",
                "Zeyu Cui",
                "Shu Wu",
                "Zhongzhen Wen",
                "Liang Wang."
            ],
            "title": "Every document owns its structure: Inductive text classification via graph neural networks",
            "venue": "ACL, pages 334\u2013339.",
            "year": 2020
        },
        {
            "authors": [
                "Junhao Zheng",
                "Haibin Chen",
                "Qianli Ma."
            ],
            "title": "Cross-domain named entity recognition via graph matching",
            "venue": "ACL, pages 2670\u20132680.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Named entity recognition (NER) aims to detect named entities in natural languages, such as locations, organizations, and persons, in input text (Zhang et al., 2022; Sang and Meulder, 2003; Yang et al., 2017). This task has gained significant attention from both academia and industry due to\n\u2217Corresponding author. 1Our code is available at https://github.com/\nWZH-NLP/PLTR.\nits wide range of uses, such as question answering and document parsing, serving as a crucial component in natural language understanding (Nadeau and Sekine, 2007; Ma and Hovy, 2016; Cui and Zhang, 2019; Yamada et al., 2020). The availability of labeled data for NER is limited to specific domains, leading to challenges for generalizing models to new domains (Lee et al., 2022; Cui et al., 2021; Ma et al., 2022).\nTo overcome this issue, recent research focuses on enabling models to effectively learn from a few labeled examples in new target domains (Lee et al., 2022; Ma et al., 2022; Das et al., 2022; Chen et al., 2022a; Wang et al., 2022, 2023) or on exploring data augmentation techniques, leveraging automatically generated labeled examples to enrich the training data (Zeng et al., 2020). However, these methods still require manual labeling for target domains, limiting their applicability in zero-shot scenarios with diverse domains.\nRecently, Yang et al. (2022) have explored a new task, few-shot cross-domain NER, aiming to generalize an entity recognizer to unseen target domains using a small number of labeled in-domain examples. To accomplish this task, a data augmentation technique, named FactMix, has been devised. FactMix generates semi-fact examples by replacing the original entity or non-entity words in training instances, capturing the dependencies between entities and their surrounding context. Despite its success, FactMix faces two challenges: Augmentation is limited to the training data. Since the target domain is not accessible during training, FactMix exclusively augments the training data from the source domain. As a result, there is minimal overlap between the generated examples and the test instances at both the entity and context levels. For instance, only 11.11% of the entity words appear simultaneously in both the generated data (by FactMix) and the AI dataset (target domain). At the context level, as demonstrated\nin Fig. 1(a), the average sentence similarity between the augmented instances and the test examples is remarkably low. These gaps pose severe challenges in extrapolating the model to OOD data. To address this problem, we incorporate natural language prompts to guide the model during both training and inference processes, mitigating the gap between the source and unseen domains. Knowledge transfer is implicit and insufficient. Intuitively, better generalization to unseen domains can be accomplished by incorporating knowledge from the source domain (Ben-David et al., 2022). However, in FactMix, the transfer of knowledge from the source domain occurs implicitly at the representation level of pre-trained language models. FactMix is unable to explicitly identify the typerelated features (TRFs), i.e., tokens strongly associated with entity types, which play a crucial role in generalization. E.g., as illustrated in Fig. 1(b), the words \u201cestablished\u201d and \u201calong with\u201d exhibit a close relationship with organization and person entities, respectively, in both domains. This knowledge can greatly assist in recognizing organizations and persons in the target domain.\nTo tackle this limitation, we introduce mutual information criteria to extract informative TRFs from the source domain. Furthermore, we construct a unique prompt for each unseen instance by selecting relevant TRFs. Intuitively, these generated prompts serve as distinctive signatures, linking unfamiliar examples to the knowledge within the source domain. Contributions. In this paper, we present a framework, named prompt learning with type-related features (PLTR) for few-shot cross-domain NER,\nto effectively leverage knowledge from the source domain and bridge the gap between training and unseen data. As Fig. 2 shows, PLTR is composed of two main phases: (i) type-related feature extraction, and (ii) prompt generation and incorporation. To identify valuable knowledge in the source domain, PLTR uses mutual information criteria to extract entity type-related features (TRFs). PLTR implements a two-stage framework to mitigate the gap between training and OOD data. Firstly, given a new example, PLTR constructs a unique sequence by selecting relevant TRFs from the source domain. Then, the constructed sequences serve as prompts for performing entity recognition on the unseen data. Finally, a multi-task training strategy is employed to enable parameter sharing between the prompt generation and entity recognition. Similar to FactMix, PLTR is a fully automatic method that does not rely on external data or human interventions. PLTR is able to seamlessly integrate with different few-shot NER methods, including standard fine-tuning and prompt-tuning approaches.\nIn summary, our contributions are: (i) to the best of our knowledge, ours is the first work to study prompt learning for few-shot cross-domain NER; (ii) we develop a mutual information-based approach to identify important entity type-related features from the source domain; (iii) we design a two-stage scheme that generates and incorporates a prompt that is highly relevant to the source domain for each new example, effectively mitigating the gap between source and unseen domains; and (iv) experimental results show that our proposed PLTR achieves state-of-the-art performance on both in-domain and cross-domain datasets."
        },
        {
            "heading": "2 Related work",
            "text": "Cross-domain NER. The task of cross-domain NER aims to transfer NER models across diverse text styles (Pan et al., 2013; Liu et al., 2021; Chen et al., 2021; Lee et al., 2018; Yang et al., 2017; Jia et al., 2019; Jia and Zhang, 2020; Zheng et al., 2022; Zhang et al., 2022; Hu et al., 2023; Wang et al., 2021). Yang et al. (2017) train NER models jointly in the source and target domains, while Jia et al. (2019) and Jia and Zhang (2020) leverage language models for cross-domain knowledge transfer. Zhang et al. (2022) introduce a modular learning approach that decomposes NER into entity span detection and type classification subtasks. However, these methods still rely on NER annotations or raw data in the target domain. Few-shot NER and prompt-based learning. Fewshot named entity recognition (NER) is the task of identifying predefined named entities using only a small number of labeled examples (Wiseman and Stratos, 2019; Yang and Katiyar, 2020; Das et al., 2022; Zeng et al., 2020; Ma et al., 2023). Various approaches have been proposed to address this task. For instance, Huang et al. (2021) investigate the effectiveness of self-training methods on external data using distance-based approaches, where the label of the nearest neighbors is copied. Zeng et al. (2020) involves generating counterfactual examples through interventions to augment the original dataset. Additionally, prompt-based learning, which has gained prominence in natural language processing, has also been applied to fewshot NER (Cui et al., 2021; Ma et al., 2022; Lee et al., 2022; Das et al., 2022; Chen et al., 2022b; Li et al., 2022; Dong et al., 2023; Fang et al., 2023). In particular, Das et al. (2022) incorporate contrastive\nlearning techniques with prompts to better capture label dependencies. Furthermore, Ma et al. (2022) develop a template-free approach to prompt NER, employing an entity-oriented objective. Recently, several studies have conducted analyses of the performance of current large language models (LLMs), such as the GPT series (Brown et al., 2020; OpenAI, 2023), in the context of the few-shot NER task (Gutierrez et al., 2022; Han et al., 2023; Sun et al., 2023). Nevertheless, these investigations have revealed a substantial performance gap between recent LLMs and state-of-the-art methods. Consequently, due to their high running costs and underwhelming performance, we do not consider recent LLMs as the basic model of our proposed framework (refer to Sec. 3.2). As mentioned in Sec. 1, previous few-shot NER methods primarily focus on in-domain settings and require manual annotations for the target domain, which poses a challenge for generalizing to OOD examples.\nThe field of few-shot cross-domain learning is inspired by the rapid learning capability of humans to recognize object categories with limited examples, known as rationale-based learning (Brown et al., 2020; Shen et al., 2021; Chen et al., 2022a; Baxter, 2000; Zhang et al., 2020). In the context of NER, Yang et al. (2022) introduce the fewshot cross-domain setting and propose a two-step rationale-centric data augmentation method, named FactMix, to enhance the model\u2019s generalization ability.\nIn this paper, we focus on few-shot cross-domain NER. The most closely related work is FactMix (Yang et al., 2022). FactMix faces two challenging problems: (i) augmentation is limited to the training data, and (ii) the transfer of knowledge from the source domain is implicit and insufficient.\nIn our proposed PLTR, to identify useful knowledge in the source domain, mutual information criteria are designed for automatic type-related feature (TRF) extraction. In addition, PLTR generates a unique prompt for each unseen example based on relevant TRFs, aiming to reduce the gap between the source and unseen domains."
        },
        {
            "heading": "3 Preliminaries",
            "text": ""
        },
        {
            "heading": "3.1 Task settings",
            "text": "A NER system takes a sentence x = x1, . . . , xn as input, where x is a sequence of n words. It produces a sequence of NER labels y = y1, . . . , yn, where each yi belongs to the label set Y selected from predefined tags {Bt, It, St, Et, O}. The labels B, I , E, and S indicate the beginning, middle, ending, and single-word entities, respectively. The entity type is denoted by t \u2208 T = {PER,LOC,ORG,MISC, . . .}, while O denotes non-entity tokens. The source dataset and out-ofdomain dataset are represented by Din and Dood , respectively. Following Yang et al. (2022), we consider two settings in our task, the in-domain setting and the out-of-domain (OOD) setting. Specifically, we first train a model Min using a small set of labeled instances from Din. Then, for in-domain and OOD settings, we evaluate the performance of Min on Din and Dood, respectively."
        },
        {
            "heading": "3.2 Basic models",
            "text": "Since our proposed PLTR is designed to be modelagnostic, we choose two popular NER methods, namely standard fine-tuning and prompt-tuning respectively, as our basic models. As mentioned in Sec. 2, due to their high costs and inferior performance on the NER task, we do not consider recent large language models (e.g., GPT series) as our basic models. Standard fine-tuning method. We employ pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) to generate contextualized word embeddings. These embeddings are then input into a linear classifier with a softmax function to predict the probability distribution of entity types. The process involves feeding the input token x into the feature encoder PLM to obtain the corresponding contextualized word embeddings h:\nh = PLM(x), (1)\nwhere h represents the sequence of contextualized\nword embeddings derived from the pre-trained language models. To recognize entities, we optimize the cross-entropy loss LNER as:\nLNER = \u2212 N\u2211 c=1 yo,c log (po,c) , (2)\nwhere N denotes the number of classes, y is a binary indicator (0 or 1) indicating whether the gold label c is the correct prediction for observation o, and p is the predicted probability of c for o. Prompt-tuning method. The prompt-tuning method for NER tasks involves the use of maskand-infill techniques based on human-defined templates to generate label words. We adopt the recent EntLM model proposed by Ma et al. (2022) as our benchmark for this method. First, a label word set Vl is constructed through label word engineering, which is connected to the label set using a mapping function M : Y \u2192 Vl. Next, entity tokens at entity positions are replaced with the corresponding label word M(yi). The resulting modified input is then denoted as xEnt = {x1, . . . ,M (yi) , . . . , xn}. The language model is trained by maximizing the probability P ( xEnt | x ) . The loss function for generating the prompt and performing NER is formulated as:\nLNER = \u2212 N\u2211 i=1 logP ( xi = x Ent i | x ) , (3)\nwhere N represents the number of classes. The initial parameters of the predictive model are obtained from PLMs."
        },
        {
            "heading": "4 Method",
            "text": "In this section, we present the two primary phases of the proposed PLTR method, as depicted in Fig. 2: (i) type-related feature extraction (see Sec. 4.1), and (ii) prompt generation and incorporation (see Sec. 4.2)."
        },
        {
            "heading": "4.1 Type-related feature extraction",
            "text": "As mentioned in Sec. 1, type-related features (TRFs), which are tokens strongly associated with entity types, play a crucial role in the few-shot cross-domain NER task. To extract these features, we propose a mutual information based method for identifying TRFs from the source domain. Here, we define Si as a set that contains all sentences from the source domain where entities of the ith type appear, and S\\Si as a set that contains\nsentences without entities of the i-th type. In our method, we consider a binary variable that indicates examples (texts) from Si as 1, and examples from S\\Si as 0. To find tokens closely related to Si, we first calculate the mutual information between all tokens and this binary variable, and then select the top l tokens with the highest mutual information scores. However, the mutual information criteria may favor tokens that are highly associated with S\\Si rather than with Si. Thus, we introduce a filtering condition as follows:\nCS\\Si(wm)\nCSi(wm) \u2264 \u03c1, CSi(wm) > 0, (4)\nwhere CSi(wm) represents the count of the mgram wm = xp, . . . , xp+m\u22121 in Si. CS\\Si(wm) represents the count of this m-gram wm in all source domains except for Si, and \u03c1 is an m-gram frequency ratio hyperparameter. By applying this criterion, we ensure that wm is considered part of the TRF set of Si only if its frequency in Si is significantly higher than its frequency in other entity types (S\\Si). Since the number of examples in Si is much smaller than the number of examples in S\\Si, we choose \u03c1 \u2265 1 but avoid setting it to a large value. This allows for the inclusion of features that are associated with Si while also being related to other entity types in the TRF set of Si. In our experiments, we set \u03c1 = 3 and only consider 1-gram texts for simplicity.\nNote that the type-related feature extraction module we designed is highly efficient with a computational complexity of O(|Din | \u00b7 lavg \u00b7 |T |), where |Din |, lavg , and T represent the number of sentences in the training dataset, the average sentence length, and the entity type set, respectively. This module is able to compute the mutual information criteria in Eq. 4 for all entity types in T and each token by traversing the tokens in every training sentence just once."
        },
        {
            "heading": "4.2 Prompt generation and incorporation",
            "text": "To connect unseen examples with the knowledge within the source domain, we generate and incorporate a unique prompt for each input instance. This process involves a two-stage mechanism: first, relevant TRFs are selected to form prompts, and then these prompts are input into the PLM-based basic model for entity label inference. Automatic type-related feature selection. Given an input sentence x and the extracted TRF set R,\nwe formulate the selection of relevant TRFs as a cloze-style task for our PLM-based basic model Mb (refer to Sec. 3.2). Specifically, we define the following prompt template function f(\u00b7) with K [MASK] tokens:\nf(x) = \u201cx[SEP]type-related features:[MASK]...[MASK]\u201d. (5)\nBy inputting f(x) into Mb, we compute the hidden vector h[MASK] of [MASK]. Given a token r \u2208 R, we compute the probability that token r can fill the masked position:\np([MASK] = r|f(x))) = exp(r \u00b7 h[MASK])\u2211 r\u0303\u2208R exp(r\u0303 \u00b7 h[MASK]) , (6)\nwhere r is the embedding of the token r in the PLM Mb. For each [MASK], we select the token with the highest probability as the relevant TRF for x, while discarding any repeating TRFs. For example, as illustrated in Fig. 2, for the sentence \u201cBolton\u2019s spokesperson told CBS News.\u201d, the most relevant TRFs include \u201cSpokesmen\u201d, \u201cNews\u201d and \u201cCorp\u201d.\nTo train Mb for TRF selection, we define the loss function Lgen as follows:\nLgen =\n\u2212 1|Din | \u2211\nx\u2208Din\nK\u2211 i=1 log p([MASK]i = \u03d5(x, i)|f(x)), (7)\nwhere \u03d5(x, i) denotes the label for the i-th [MASK] token in x. To obtain \u03d5(x), we compute the Euclidean distance between the PLM-based embeddings of each r \u2208 R and each token in x, selecting the top-K features. Note that our designed automatic selection process effectively filters out irrelevant TRFs for the given input sentence, substantially reducing human interventions in TRF extraction (refer to Sec. 7). Prompt incorporation. To incorporate the entity type information into prompts, we generate a unique prompt given the selected relevant TRFs R\u2032(x) \u2286 R for input x. This is achieved using the following prompt template function f \u2032(x):\nf \u2032(x) = \u201cx[SEP]t1:R\u2032(x, t1)[SEP]...[SEP]t|T |:R\u2032(x, t|T |)\u201d, (8)\nwhere ti \u2208 T is the entity type name (e.g., PER or ORG). Given sentence x, R\u2032(x, ti) \u2286 R\u2032(x) represents selected TRFs related to entity type ti. Note that, if R\u2032(x, ti) = \u2205, the entity type name, and relevant TRFs R\u2032(x, ti) are excluded from f \u2032(x). For example, as depicted in\nFig. 2, the unique prompt f \u2032(x) corresponding to x = \u201cBolton\u2019s spokesperson told CBS News.\u201d can be represented as follows:\nf \u2032(x) = \u201cBolton\u2019s spokesperson told CBS News. [SEP]PER:Spokesmen[SEP]ORG:News, Corp\u201d. (9)\nThen, we input f \u2032(x) into Mb to recognize entities in the given sentence x."
        },
        {
            "heading": "4.3 Joint training",
            "text": "To enable parameter sharing between prompt generation and incorporation, we train our model using a multi-task framework. The overall loss function is defined as follows:\nL = \u03b1 \u00b7 L\u2032NER + (1\u2212 \u03b1) \u00b7 Lgen , (10)\nwhere L\u2032NER denotes the normalized loss function for the NER task loss LNER (refer to Sec. 3.2). \u03b1 is the weight assigned to L\u2032NER with prompts as inputs. The weight 1 \u2212 \u03b1 is assigned to the loss function Lgen for type-related feature selection. In our experiments, we optimize the overall loss function using AdamW (Loshchilov and Hutter, 2019). Sec. A.1 gives the detailed training algorithm of PLTR."
        },
        {
            "heading": "5 Experiments",
            "text": "We aim to answer the following research questions: (RQ1) Does PLTR outperform state-of-the-art fine\u2013 tuning methods on the few-shot cross-domain NER task? (Sec. 6.1) (RQ2) Can PLTR be applied to prompt-tuning NER methods? (Sec. 6.2) Micro F1 is adopted as the evaluation metric for all settings."
        },
        {
            "heading": "5.1 Datasets",
            "text": "Detailed statistics of both in-domain and out-ofdomain datasets are shown in Table 1. In-domain dataset. We conduct in-domain experiments on the CoNLL2003 dataset (Sang and\nMeulder, 2003). It consists of text in a style similar to Reuters News and encompasses entity types such as person, location, and organization. Additionally, to examine whether PLTR is extensible to different source domains and entity types, we evaluate PLTR using training data from OntoNotes (Weischedel et al., 2013) (refer to Sec. A.3). OntoNotes is an English dataset consisting of text from a wide range of domains and 18 types of named entities, such as Person, Event, and Date. Out-of-domain datasets. We utilize the OOD dataset collected by Liu et al. (2021), which includes new domains such as AI, Literature, Music, Politics, and Science. The vocabulary overlaps between these domains are generally small, indicating the diversity of the out-of-domain datasets (Liu et al., 2021). Since the model trained on the source domain dataset (CoNLL2003) can only predict person, location, organization, and miscellaneous entities, we assign the label O to all unseen labels in the OOD datasets."
        },
        {
            "heading": "5.2 Experimental settings and baselines",
            "text": "We compare PLTR with recent baselines in the following two experimental settings: Fine-tuning. Following Yang et al. (2022), we employ the standard fine-tuning method (Ori) based on two pre-trained models with different parameter sizes: BERT-base, BERT-large, RoBERT-base, and RoBERT-large. All backbone models are implemented using the transformer package provided by Huggingface.2 For fine-tuning the NER models in a few-shot setting, we randomly select 100 instances per label from the original dataset (CoNLL2003) to ensure model convergence. The reported performance of the models is an average across five training runs. Prompt-tuning. Similar to Yang et al. (2022), we adopt the EntLM model proposed by Ma et al. (2022) as the benchmark for prompt-tuning. The EntLM model is built on the BERT-base or BERTlarge architectures. We conduct prompt-based experiments using a 5-shot training strategy (Ma et al., 2022). Additionally, we select two representative datasets, TechNews and Science, for the OOD test based on the highest and lowest word overlap with the original training domain, respectively.\nAdditionally, we include a recent data augmentation method CF (Zeng et al., 2020) and the state-\n2https://huggingface.co/models\nof-the-art cross-domain few-shot NER framework FactMix (Yang et al., 2022) as baselines in both of the above settings. Note that, we report the results of FactMix\u2019s highest-performing variant for all settings and datasets."
        },
        {
            "heading": "5.3 Implementation details",
            "text": "Following Yang et al. (2022), we train all models for 10 epochs and employ an early stopping criterion based on the performance on the development dataset. The AdamW optimizer (Loshchilov and Hutter, 2019) is used to optimize the loss functions. We use a batch size of 4, a warmup ratio of 0.1, and a learning rate of 2e-5. The maximum input and output lengths of all models are set to 256. For PLTR, we search for the optimal loss weight \u03b1 from {0.1, 0.25, 0.5, 0.75, 0.9}. The frequency ratio hyperparameter \u03c1 is set to 3 for all domains."
        },
        {
            "heading": "6 Experimental results",
            "text": "To answer RQ1 and RQ2, we assess the performance of PLTR on both in-domain and crossdomain few-shot NER tasks. This evaluation is conducted in two settings: a fine-tuning setting with 100 training instances per type, and a prompttuning setting with 5 training instances per type."
        },
        {
            "heading": "6.1 Results on few-shot fine-tuning (RQ1)",
            "text": "Table 2 and 3 show the in-domain and cross-domain performance in the fine-tuning setting, respectively. Based on the results, we have the following observations: (i) PLTR achieves the highest Micro F1 scores for all datasets and settings, indicating its superior performance. For instance, when using RoBERTa-large as the backbone, PLTR achieves an 88.03% and 75.14% F1 score on the CoNLL2003 and TechNews datasets, respectively. (ii) PLTR significantly outperforms the previous state-of-the-art baselines in both in-domain and cross-domain NER. For example, PLTR exhibits a 1.46% and 10.64% improvement over FactMix, on average,\non in-domain and cross-domain datasets, respectively. (iii) Few-shot cross-domain NER is notably more challenging than the in-domain setting, as all methods obtain considerably lower F1 scores. The performance decay in TechNews is smaller than in other domains, due to its higher overlap with the training set. In summary, PLTR demonstrates its effectiveness in recognizing named entities from both in-domain and OOD examples. The use of type-related features (TRFs), along with the incorporation of prompts based on TRFs, are beneficial for in-domain and cross-domain few-shot NER."
        },
        {
            "heading": "6.2 Results on few-shot prompt-tuning (RQ2)",
            "text": "To explore the generalizability of PLTR, we report in-domain and OOD results for the prompt-tuning setting in Table 4 and 5, respectively. We obtain the following insights: (i) Due to data sparsity, the overall performance for the prompt-tuning setting is considerably lower than the results of 100-shot fine\u2013 tuning. (ii) Even with only 5-shot training instances per entity type, PLTR achieves the highest performance and outperforms the state-of-the-art baselines by a significant margin, demonstrating the effectiveness and generalizability of PLTR. For example, in the in-domain and cross-domain datasets, PLTR achieves an average improvement of 11.58% and 18.24% over FactMix, respectively. In summary, the PLTR framework not only effectively generalizes fine-tuning-based NER methods to unseen domains, but also attains the highest F1 scores in the prompt-tuning setting."
        },
        {
            "heading": "7 Analysis",
            "text": "Now that we have answered our research questions, we take a closer look at PLTR to analyze its performance. We examine whether the prompts are designed appropriately. Besides, we study how the number of training samples and selected type-related features influence the performance (Sec. A.2), how PLTR affects representation similarities between the source and target domains, and whether PLTR is extensible to different source domains and entity types (Sec. A.3). Furthermore, we provide insights into the possible factors that limit further improvements. Ablation studies. To investigate the appropriateness of our prompt design, we conduct ablation studies on few-shot cross-domain NER in both finetuning and prompt-tuning settings. The results are presented in Table 6. In the \u201cNP\u201d variant, prompts\nare removed during test-time inference. In this case, the F1 scores across all datasets and settings suffer a significant drop compared to our proposed PLTR. This demonstrates the crucial role of incorporating prompts during both the training and inference processes. In the \u201cRDW\u201d and \u201cREW\u201d variants, prompts are constructed using randomly selected words from the source domain and the given example, respectively. The performance of both the \u201cRDW\u201d and \u201cREW\u201d model variants consistently falls short of PLTR, indicating that PLTR effectively identifies important knowledge from the source domain and establishes connections between unseen examples and the knowledge within the source domain.\nAdditionally, to explore the efficacy of typerelated feature selection (refer to Sec. 4.2), we conducted an evaluation of PLTR (BERT-base) using various frequency ratios \u03c1 (in Eq. 4). The results are presented in Table 7. As the value of \u03c1 increases, TRFs extracted using Eq.4 become less closely associated with the specified entity type but become more prevalent in other types. When the value of \u03c1 is raised from 3 to 9, we observed only a slight decrease in the F1 scores of PLTR. When the value of \u03c1 is raised to 20, the F1 score of PLTR drops, but still surpasses the state-of-the-art\nbaseline FactMix. These results indicate that PLTR effectively identifies relevant TRFs for OOD examples, considerably mitigating human interventions in the feature extraction process. The influence of training samples. To examine the impact of the number of training samples, we compare the performance of PLTR and FactMix on few-shot cross-domain NER using 100, 300, and 500 training samples per entity type. Fig. 3 displays the results based on the BERT-base-cased model. PLTR exhibits the largest improvements over FactMix when the dataset comprises only 100 training instances per entity type, as opposed to the 300 and 500 training instances scenarios. Furthermore, PLTR consistently outperforms the prior state-of-the-art approach, FactMix, across all experimental settings with varying numbers of training examples, demonstrating its superiority. Analysis of sentence similarities. In our analysis of sentence similarities, we investigate the impact of PLTR on the representation similarities between the source and target domains. We compute the\naverage SBERT similarities for sentence representations in PLTR (BERT-base) between the source and target domains; the results are presented in Fig. 4. With the prompts generated by PLTR, the representation similarities between the source and unseen domains noticeably increase. This is, PLTR facilitates a more aligned and connected representation space, mitigating the gap between the source and target domains. Error analysis. Although our proposed PLTR outperforms state-of-the-art baselines, we would like to analyze the factors restricting further improvements. Specifically, we compare the performance of PLTR (BERT-base) on sentences of different lengths in the test sets of the CoNLL2003 (Indomain), AI, and Science datasets. The results of the standard fine-tuning setting are provided in\nTable 8. We observe that the F1 scores of PLTR on sentences with more than 35 words (\u201c> 35\u201d) are substantially higher than the overall F1 scores. In contrast, the F1 scores on sentences with 25 to 35 words (\u201c25\u201335\u201d) or less than 25 words (\u201c< 25\u201d) consistently fall below the overall F1 scores. This suggests that it may be more challenging for PLTR to select TRFs and generate appropriate prompts with less context."
        },
        {
            "heading": "8 Conclusions",
            "text": "In this paper, we establish a new state-of-the-art framework, PLTR, for few-shot cross-domain NER. To capture useful knowledge from the source domain, PLTR employs mutual information criteria to extract type-related features. PLTR automatically selects pertinent features and generates a unique prompt for each unseen example, bridging the gap between domains. Experimental results show that PLTR not only effectively generalizes standard fine-tuning methods to unseen domains, but also demonstrates promising performance when incorporated with prompt-tuning-based approaches. Additionally, PLTR substantially narrows the disparity between in-domain examples and OOD instances, enhancing the similarities of their sentence representations.\nLimitations\nWhile PLTR achieves a new state-of-the-art performance, it has several limitations. First, the number of type-related features for prompt construction needs to be manually preset. Second, PLTR relies on identifying TRFs, which are tokens strongly associated with entity types. Extracting and incorporating more complex features, such as phrases, represents a promising direction for future research. In the future, we also plan to incorporate PLTR with different kinds of pre-trained language models, such as autoregressive language models.\nEthics statement\nThe paper presents a prompt-based method for recognizing named entities in unseen domains with limited labeled in-domain examples. However, the constructed prompts and model-predicted results still have a considerable amount of misinformation. Besides, the reliance on black-box pre-trained language models raises concerns. Hence, caution and further research are required prior to deploying this method in real-world applications."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was supported by the National Key R&D Program of China (2020YFB1406704, 2022YFC3303004), the Natural Science Foundation of China (62272274, 61972234, 62072279, 62102234, 62202271), the Natural Science Foundation of Shandong Province (ZR2021QF129, ZR2022QF004), the Key Scientific and Technological Innovation Program of Shandong Province (2019JZZY010129), the Fundamental Research Funds of Shandong University, the China Scholarship Council under grant nr. 202206220085, the Hybrid Intelligence Center, a 10-year program funded by the Dutch Ministry of Education, Culture and Science through the Netherlands Organization for Scientific Research, https:// hybrid-intelligence-centre.nl, and project LESSEN with project number NWA.1389.20.183 of the research program NWA ORC 2020/21, which is (partly) financed by the Dutch Research Council (NWO)."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Training algorithm of PLTR Algorithm 1 gives the detailed training algorithm of PLTR. To start, we establish a basic model Mb based on Pre-trained Language Models (PLM) and initialize its parameters \u0398 (lines 1-2). To capture knowledge from the source domain, PLTR identifies type-related features using mutual information criteria (line 3). Next, given an input sentence x \u2208 Din , PLTR automatically selects relevant TRFs R\u2032(x) \u2286 R by formulating the selection process as a cloze-style task for Mb (line 7). Furthermore, to incorporate entity type information into prompts, PLTR constructs a unique prompt\nAlgorithm 1 Training Algorithm for PLTR. Require: The source dataset Din; the basic model\nMb with parameters \u0398; the frequency ratio \u03c1; the number of selected type-related features K; the loss weight \u03b1; the number of epochs epoch . Ensure: The extracted type-related features R and the trained basic model M\u2032b;\n1: Establish the basic model Mb; 2: Initialize model parameters \u0398; 3: Extract type-related features R for all entity\ntypes from the source dataset Din (Eq. 4); 4: while i \u2264 epoch do 5: for Sample a batch X \u2286 Din do 6: for all sentences x \u2208 X do 7: Select relevant TRFs R\u2032(x) for in8: put x (Eq. 5 and 6); 9: Transform x into the prompt tem10: plate f \u2032(x) (Eq. 8); 11: Input f \u2032(x) into Mb for prediction; 12: end for 13: Update \u0398 by optimizing L (Eq. 10); 14: end for 15: end while\nf \u2032(x) for each input x, and these prompts are then fed into Mb for entity recognition (lines 8-9). Finally, we iteratively refine the parameters \u0398 by jointly optimizing two loss functions: the NER task loss function L\u2032NER and the TRF selection loss function Lgen (line 11). Note that, during inference, PLTR generates a unique prompt for each sentence within the unseen target domain using extracted TRFs R. In this way, knowledge from the source domain is explicitly integrated into both the training and inference phases.\nA.2 Influence of the number of selected type-related features\nWe evaluate PLTR based on BERT-base in finetuning setting, with the number of selected relevant type-related features K varying from 10 to 60. The results are shown in Fig. 5. Our observations indicate that as the number of type-related features increases, the performance (F1 score) of PLTR initially improves because the model incorporated with more features is able to encode more useful knowledge from the source domain. But notice that the performance drops when the number of typerelated features is too large. In our experiments, we set the number of type-related features to 40 on all\ndatasets.\nA.3 Influence of source domains We explore the performance of our proposed PLTR when trained on data from different source domains, i.e., CoNLL2003 and OntoNotes. Results in both the fine-tuning and prompt-tuning settings are shown in Table 9. Our observations indicate that our proposed PLTR consistently outperforms FactMix when trained on different domains. For instance, PLTR achieves an average improvement of 5.42% and 4.22% over FactMix for \"LOC\" entities when using CoNLL2003 and OntoNotes as source datasets, respectively. This highlights PLTR\u2019s capacity to extend to various source domains and entity types."
        }
    ],
    "title": "Generalizing Few-Shot Named Entity Recognizers to Unseen Domains with Type-Related Features",
    "year": 2023
}