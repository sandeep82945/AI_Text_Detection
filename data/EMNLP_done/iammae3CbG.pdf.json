{
    "abstractText": "Parameter-efficient fine-tuning (PEFT) has shown its effectiveness in adapting the pretrained language models to downstream tasks while only updating a small number of parameters. Despite the success, most existing methods independently adapt to each task without considering knowledge transfer between tasks and are limited to low-data regimes. To overcome this issue, we propose Prototype-based HyperAdapter (PHA), a novel framework built on the adapter-tuning and hypernetwork. It introduces an instance-dense retriever and a prototypical hypernetwork to generate the conditional modules in a sample-efficient manner. This leads to comparable performance improvements against existing PEFT methods on multi-task learning and few-shot transfer learning. More importantly, when the available data size gets smaller, our method outperforms other strong baselines by a large margin. Based on our extensive empirical experiments across various datasets, we demonstrate that PHA strikes a better trade-off between trainable parameters, accuracy on stream tasks, and sample efficiency. Our code is publicly available at https://github.com/Bumble666/PHA",
    "authors": [
        {
            "affiliations": [],
            "name": "Hao Zhao"
        },
        {
            "affiliations": [],
            "name": "Jie Fu"
        },
        {
            "affiliations": [],
            "name": "Zhaofeng He"
        }
    ],
    "id": "SP:3c277eb642afdb3719cbfc8557f917ef781e90ab",
    "references": [
        {
            "authors": [
                "Armen Aghajanyan",
                "Anchit Gupta",
                "Akshat Shrivastava",
                "Xilun Chen",
                "Luke Zettlemoyer",
                "Sonal Gupta"
            ],
            "title": "Muppet: Massive multi-task representations",
            "year": 2021
        },
        {
            "authors": [
                "Akari Asai",
                "Mohammadreza Salehi",
                "Matthew Peters",
                "Hannaneh Hajishirzi."
            ],
            "title": "ATTEMPT: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natu-",
            "year": 2022
        },
        {
            "authors": [
                "Mathilde Caron",
                "Ishan Misra",
                "Julien Mairal",
                "Priya Goyal",
                "Piotr Bojanowski",
                "Armand Joulin."
            ],
            "title": "Unsupervised learning of visual features by contrasting cluster assignments",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages 9912\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "I\u00f1igo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
            "venue": "Proceedings of the 11th International Workshop on Semantic",
            "year": 2017
        },
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova."
            ],
            "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "NAACL.",
            "year": 2019
        },
        {
            "authors": [
                "Ganqu Cui",
                "Shengding Hu",
                "Ning Ding",
                "Longtao Huang",
                "Zhiyuan Liu."
            ],
            "title": "Prototypical verbalizer for prompt-based few-shot tuning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Marie-Catherine De Marneffe",
                "Mandy Simons",
                "Judith Tonhauser."
            ],
            "title": "The commitmentbank: Investigating projection in naturally occurring discourse",
            "venue": "Sinn und Bedeutung 23.",
            "year": 2019
        },
        {
            "authors": [
                "Dorottya Demszky",
                "Kelvin Guu",
                "Percy Liang."
            ],
            "title": "Transforming question answering datasets into natural language inference datasets",
            "venue": "arXiv preprint arXiv:1809.02922.",
            "year": 2018
        },
        {
            "authors": [
                "Ning Ding",
                "Yujia Qin",
                "Guang Yang",
                "Fuchao Wei",
                "Zonghan Yang",
                "Yusheng Su",
                "Shengding Hu",
                "Yulin Chen",
                "Chi-Min Chan",
                "Weize Chen"
            ],
            "title": "Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models",
            "year": 2022
        },
        {
            "authors": [
                "Ning Ding",
                "Xiaobin Wang",
                "Yao Fu",
                "Guangwei Xu",
                "Rui Wang",
                "Pengjun Xie",
                "Ying Shen",
                "Fei Huang",
                "Hai-Tao Zheng",
                "Rui Zhang."
            ],
            "title": "Prototypical representation learning for relation extraction",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "William B. Dolan",
                "Chris Brockett."
            ],
            "title": "Automatically constructing a corpus of sentential paraphrases",
            "venue": "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
            "year": 2005
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xu Han",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Hybrid attention-based prototypical networks for noisy few-shot relation classification",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 6407\u20136414.",
            "year": 2019
        },
        {
            "authors": [
                "Danilo Giampiccolo",
                "Bernardo Magnini",
                "Ido Dagan",
                "Bill Dolan."
            ],
            "title": "The third PASCAL recognizing textual entailment challenge",
            "venue": "Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1\u20139, Prague. Association for",
            "year": 2007
        },
        {
            "authors": [
                "Anchun Gui",
                "Han Xiao."
            ],
            "title": "HiFi: Highinformation attention heads hold for parameterefficient model adaptation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2023
        },
        {
            "authors": [
                "David Ha",
                "Andrew M. Dai",
                "Quoc V. Le."
            ],
            "title": "Hypernetworks",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "Junxian He",
                "Chunting Zhou",
                "Xuezhe Ma",
                "Taylor BergKirkpatrick",
                "Graham Neubig."
            ],
            "title": "Towards a unified view of parameter-efficient transfer learning",
            "venue": "ICLR.",
            "year": 2022
        },
        {
            "authors": [
                "Yun He",
                "Steven Zheng",
                "Yi Tay",
                "Jai Gupta",
                "Yu Du",
                "Vamsi Aribandi",
                "Zhe Zhao",
                "Yaguang Li",
                "Zhao Chen",
                "Donald Metzler",
                "Heng-Tze Cheng",
                "Ed H. Chi."
            ],
            "title": "HyperPrompt: Prompt-based task-conditioning of transformers",
            "venue": "Proceedings of the 39th Interna-",
            "year": 2022
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for NLP",
            "venue": "Proceedings of the 36th International Conference",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "Hamish Ivison",
                "Matthew Peters."
            ],
            "title": "Hyperdecoders: Instance-specific decoders for multi-task NLP",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1715\u20131730, Abu Dhabi, United Arab Emirates. Association for",
            "year": 2022
        },
        {
            "authors": [
                "Tian Jin",
                "Zhun Liu",
                "Shengjia Yan",
                "Alexandre Eichenberger",
                "Louis-Philippe Morency."
            ],
            "title": "Language to network: Conditional parameter adaptation with natural language descriptions",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "James Henderson",
                "Sebastian Ruder."
            ],
            "title": "Compacter: Efficient low-rank hypercomplex adapter layers",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 1022\u20131035. Curran Associates, Inc.",
            "year": 2021
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "Sebastian Ruder",
                "Mostafa Dehghani",
                "James Henderson."
            ],
            "title": "Parameterefficient multi-task fine-tuning for transformers via shared hypernetworks",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Tushar Khot",
                "Ashish Sabharwal",
                "Peter Clark."
            ],
            "title": "Scitail: A textual entailment dataset from science question answering",
            "venue": "AAAI.",
            "year": 2018
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Hector Levesque",
                "Ernest Davis",
                "Leora Morgenstern."
            ],
            "title": "The winograd schema challenge",
            "venue": "Thirteenth international conference on the principles of knowledge representation and reasoning.",
            "year": 2012
        },
        {
            "authors": [
                "Junnan Li",
                "Pan Zhou",
                "Caiming Xiong",
                "Steven Hoi."
            ],
            "title": "Prototypical contrastive learning of unsupervised representations",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Baohao Liao",
                "Yan Meng",
                "Christof Monz."
            ],
            "title": "Parameter-efficient fine-tuning without introducing new latency",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4242\u20134260, Toronto,",
            "year": 2023
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Weng Tam",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Xiaodong Liu",
                "Pengcheng He",
                "Weizhu Chen",
                "Jianfeng Gao."
            ],
            "title": "Multi-task deep neural networks for natural language understanding",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4487\u20134496, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "James Henderson",
                "Sebastian Ruder."
            ],
            "title": "Compacter: Efficient low-rank hypercomplex adapter layers",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals."
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748.",
            "year": 2018
        },
        {
            "authors": [
                "jani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Aishwarya Kamath",
                "Andreas R\u00fcckl\u00e9",
                "Kyunghyun Cho",
                "Iryna Gurevych."
            ],
            "title": "AdapterFusion: Non-destructive task composition for transfer learning",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Sebastian Ruder",
                "Ivan Vuli\u0107",
                "Edoardo Maria Ponti."
            ],
            "title": "Modular deep learning",
            "venue": "arXiv preprint arXiv:2302.11529.",
            "year": 2023
        },
        {
            "authors": [
                "Mohammad Taher Pilehvar",
                "Jose Camacho-Collados."
            ],
            "title": "WiC: the word-in-context dataset for evaluating context-sensitive meaning representations",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits",
            "year": 2020
        },
        {
            "authors": [
                "drea Santilli",
                "Thibault Fevry",
                "Jason Alan Fries",
                "Ryan Teehan",
                "Teven Le Scao",
                "Stella Biderman",
                "Leo Gao",
                "Thomas Wolf",
                "Alexander M Rush"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "In International Conference on Learning",
            "year": 2022
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Tu Vu",
                "Brian Lester",
                "Noah Constant",
                "Rami Al-Rfou",
                "Daniel Cer"
            ],
            "title": "SPoT: Better frozen model adaptation through soft prompt transfer",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Advances in Neural Information",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Zhen Wang",
                "Rameswar Panda",
                "Leonid Karlinsky",
                "Rogerio Feris",
                "Huan Sun",
                "Yoon Kim."
            ],
            "title": "Multitask prompt tuning enables parameter-efficient transfer learning",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Alex Warstadt",
                "Amanpreet Singh",
                "Samuel R. Bowman."
            ],
            "title": "Neural network acceptability judgments",
            "venue": "Transactions of the Association for Computational Linguistics, 7:625\u2013641.",
            "year": 2019
        },
        {
            "authors": [
                "2022 Asai et al"
            ],
            "title": "2021b); Ivison and Peters (2022), the length of the sequence is 128 at the encoder and the decoder",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Fine-tuning a pre-trained language model (PLM) yields extraordinary potential for simultaneous adaptation to multiple downstream tasks in a multitask setting. However, fine-tuning all the parameters of models induces substantial storage and deployment costs, especially as pre-trained model sizes are growing rapidly. To address this issue, several works (Houlsby et al., 2019; Lester et al., 2021; Karimi Mahabadi et al., 2021a; Hu et al., 2022; Ding et al., 2022; Gui and Xiao, 2023; Zeng et al., 2023; Liao et al., 2023; Xie and Lukasiewicz, 2023) have developed parameterefficient fine-tuning which trains compact modules\n\u2217Equal technical contribution, co-first authors. \u2020Corresponding authors.\nper task and adapts PLMs to downstream tasks. Nonetheless, these methods require learning different modules to adapt to diverse tasks, and the cost of the parameter increases proportionally with the number of tasks. On the other hand, training taskspecific modules separately fails to reap benefits from other relative tasks.\nRecent work (Karimi Mahabadi et al., 2021b; Ivison and Peters, 2022) has proposed training a hypernetwork to generate the parameters of these modules to achieve a better trade-off between parameter efficiency and adaption for downstream tasks. These methods encourage the multi-task learning model to capture the shared information by leveraging task-shared hypernetwork while eliminating negative task interference by generating con-\nditioned modules individually. Despite these methods\u2019 success in multi-task learning, there are still some issues:(1) hypernetwork-based methods generally optimize the specific embedding and shared hypernetwork together by end-to-end training without any regularization. Task-specific information is inseparably intertwined, which suppresses the efficiency of the hypernetwork, especially in resourcelimited settings. (2) these existing approaches generalize to new tasks that require task-specific prior knowledge or knowledge from frozen pre-trained models.\nThese works (Karimi Mahabadi et al., 2021b; Pfeiffer et al., 2023) indicate that the task-shared hypernetwork serves as a cross-task information captor, while a specific embedding should encapsulate task-level semantic features in order to extract pertinent information from the hypernetwork for generating corresponding module parameters. Empirically, task-level features are typically implicitly represented by related instance features. A natural idea to encourage embedding generation is to calculate the central points (prototypes) of task-specific instance features.\nIn this paper, we introduce the Prototype-based HyperAdapter(PHA), a novel framework built on adapter-tuning that achieves both multi-task learning and generalization to new tasks in a sampleefficient manner. As depicted in Figure 1, PHA consists of two main components, Instance-dense Retriever and Prototypical HyperNetworks. The first part aims to train a retriever to discriminate the instances from different tasks in embedding space. For the second part, we aim to estimate the task-specific prototypes with the instance-level features and keep the prototypes as embeddings to be trained with the hypernetwork.\nSpecifically, we project the encoded instance features into embedding space using the retriever. To avoid instances interference in embedding space, we train the retriever with the InfoNCE estimator (Oord et al., 2018). As a result, it clusters intratask instances and increases the distances between inter-task instances. The projected features here can be deemed as instance-level semantic features that are used to estimate task-level embeddings. Inspired by PCL (Li et al., 2021), we estimate the task-specific embedding using the contrastive prototypical loss, which encourages the prototypes to become the center points of instance-level features. Compared with the existing method, where\nthe specific embeddings are optimized directly during tuning, our method efficiently learns specific embedding with side information, which helps to optimize the embedding space in low-data regimes. During the adaptation of new tasks, since we maintain the previous task-level semantic features as prototypes that align with the instances in the embedding space, we match the corresponding prototype for the current new task by calculating the distance between the new instances and the previous prototypes.\nWe evaluate PHA on 13 NLP datasets across diverse tasks. Extensive experiments show the effectiveness of PHA, especially in low-data regimes. Meanwhile, PHA is able to achieve a few-shot domain adaption with 4-32 shots. For example, PHA outperforms the strong multitask adapter transfer baseline by 1.0% with lower trainable parameters on the GLUE benchmark. In low resource regimes where only 100 samples per task from the GLUE benchmark is available, PHA outperforms adaptertuning by 8.0%. Our analysis shows that PHA efficiently captures specific information and shared information while reducing negative transfer. We also present a detailed analysis to demonstrate that the instances from different tasks can be identified by corresponding prototypes and used for new task adaption."
        },
        {
            "heading": "2 Background",
            "text": "HyperNetworks. A hypernetwork(Ha et al., 2017) can generate parameters to be used by networks or modules. Specifically, the hypernetwork, denoted as hw, leverages an embedding I to generate the module parameters \u03d5:\n\u03d5 = hw(I) (1)\nAdapter. Adapter(Houlsby et al., 2019) is commonly used in parameter-efficient tuning that aims to apply PLM to downstream tasks. Specifically, adapter-based tuning inserts trainable task-specific modules into transformer layers, while keeping the PLM fixed. The adapter Al(x) for layer l is defined as:\nAl(x) = Dl(ReLU((U l(x))) + x, (2)\nwhere Dl \u2208 Rd\u00d7b and U l \u2208 Rb\u00d7d are down/upprojection matrices. x \u2208 Rd refers to the input. d is the hidden dimension of PLM, b is the bottleneck size satisfying b \u226a d.\nHe et al. (2022a) propose using the parallel adapter, which is different from the traditional sequential insertion method. They demonstrate the more efficient parameter-efficient fine-tuning that the parallel adapter offers."
        },
        {
            "heading": "3 Method",
            "text": "Problem Setup. We are following a general multitask learning problem. Given a pre-trained language model M\u03b8 with parameters \u03b8 and a set of target tasks {D} = {D1,D2, . . . ,D\u03c4}, where \u03c4 is the total number of tasks and {Di} = {xni , yni } Ni n=1 represents the training data of the i-th task with Ni samples. The main objective of our study is to fine-tune M\u03b8 for downstream tasks {D} using a multi-task learning setup and to ensure that it is capable of generalizing to new tasks. Method Overview. The key idea of our approach is to directly learn prototype embeddings for each task from training instances which is acquired by using a task-shared encoder, then generating taskspecific adapter layers by retrieving prototype embedding and feeding it into a hypernetworks. As shown in Figure 1, the encoded instances are projected into retrieval vectors by the instance-dense retriever for prototype learning. These prototypes that represent the task-specific information enable hypernetwork to update efficiently. This potentially allows more sample-efficient fine-tuning and fewshot transfer learning."
        },
        {
            "heading": "3.1 Instance-dense Retriever",
            "text": "We define an instance-dense retriever for better generalization. Let hi \u2208 Rd denote the last layer\u2019s mean-pooled hidden state of the training sample, which belongs to the i-th task. To align the embedding feature with the training sample in the latent space, an instance-dense retriever G(\u00b7) is applied to construct the retrieval vector zi = G(hi), where G(\u00b7) is an MLP consisting of two feed-forward layers and a ReLU non-linearity. Furthermore, we need the retriever to have the ability which explicitly encourage alignment between instances from the same task, as well as push away instances from different tasks.\nTo efficiently learn the discriminative retriever, we introduce the following loss function LIR based on the InfoNCE:\nLi= \u2211 zi\u2208D \u22121 Ni \u2212 1 \u2211 zj\u2208D\u0302i log exp f(zi \u00b7 zj)\u2211 zm\u2208S(i) exp f(zi \u00b7 zm) ,\n(3)\nLIR= 1\n\u03c4 \u03c4\u2211 i=1 Li, (4)\nwhere Li is the learning objective for task i, f(\u00b7) is the cosine similarity function. D\u0302i is a set of positive samples of zi and S(i) denotes a set of negative samples for zi.\nThe instance-dense retriever aggregates instancelevel information from the same task and enables flexible reuse of knowledge used for few-shot transfer learning."
        },
        {
            "heading": "3.2 Prototypical HyperNetworks",
            "text": "Simply using the learned task embedding to encapsulate task-specific information biases the taskshared hypernetwork to overfit the training data distribution, which means that inadequate sample efficiency and mixed knowledge are more susceptible to changes in distribution during cross-task transferring.\nTo overcome this issue, we propose to implicitly exploit the instance-level information to instruct the task embedding instead of end-to-end training. To be more specific, we found the contrastive formulation is an efficient strategy for learning robust and sample-efficient Hypernetworks. We first initialize a set of embedding {ki}\u03c4i=1, where {ki} \u2208 Rd is a trainable vector to learn the specific information of the i-th task. The learning objective for an embedding is\nLi = \u2211 zi\u2208D \u22121 Ni \u2212 1 log exp f(zi \u00b7 ki)\u2211\nkm\u2208V (i) exp f(zi \u00b7 km)\n,\n(5)\nLPro = 1\n\u03c4 \u03c4\u2211 i=1 Li, (6)\nwhere V (i) is a set of negative embedding. The objective forces each embedding to make use of the relative relationships between samples across tasks and avoid sample-inefficient knowledge transfer.\nTo generate specific parameters for different transformer layers and reduce the number of trainable parameters, we introduce a learnable layer embedding denoted as em, following a similar recipe as in Hyperformer (Karimi Mahabadi et al., 2021b). m denotes the m-th layer of transformer model.\nLet H(\u00b7) denote the HyperNetwork which generates the weight matrices Dmi and U m i for task conditional adapter Ami :\n(Dmi , U m i ) = H(C(ki, em)), (7)\nwhere C(\u00b7) is a project network to concatenate the task embedding and layer embedding into a mixed embedding Imi .\nInspired by Pfeiffer et al. (2021) and He et al. (2022a), we only insert these conditional parameters (Adapter) into the Feed-Forward Networks (FFN) sub-layer in parallel:\ny = FFN(LN(x)) + A(LN(x)), (8)\nwhere LN(\u00b7) represents the LayerNorm layer. This enables efficient decoupling of knowledge from different tasks to task prototypes and adapts the changeable data distribution during transfer learning."
        },
        {
            "heading": "3.3 Multi-task Tuning and New Task Generalization",
            "text": "PHA achieves sample-efficient multi-task learning and few-shot adaption with different training methods. Multi-task Tuning. We follow a general multi-task learning setup, where the task identity is included, and the different datasets are concatenated together. To achieve efficient fine-tuning, the encoder with task-shared adapters is used for encoding the training sample, and we estimate the embedding corresponding to the retrieval vector given the task identity via the loss function in Equation 5. The decoder is attached by the specific adapters conditioned on contextual information. The whole model is trained in a sequence-to-sequence setting with the following objective function:\nLTotal = LPLM + \u03bb(LIR + LPro), (9)\nwhere LPLM = \u2211\u03c4\ni=1 Li denotes the cross-entropy loss for all training tasks and \u03bb is a scalar balancing factor.\nPHA allows the specific embedding to efficiently capture contextual information which helps the hypernetwork to generate the parameters of adapter layers in sample-efficient adaption. New Task Generalization. For few-shot adaption, we retrieve the adaption embedding ka by calculating the similarity scores of retrieval vector z and learned embeddings {k} after multi-task training:\nka = argmax i\nf(ki | z). (10)\nDuring training, we feed the adaption embedding to hypernetworks that generate the weight matrices\nof adapters for new tasks and optimize with the cross-entropy loss.\nOur method enables efficient generalization to new tasks with limited training examples, owing to the retrieved adaption embedding containing knowledge similar to that required for the new task."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "Following prior works on multi-task learning for natural language understanding (NLU) tasks, we consider 8 datasets from GLUE (Wang et al., 2019b) benchmark and 4 datasets from SuperGLUE (Wang et al., 2019a) benchmark to evaluate the performance of our models. This benchmarks is a collection of text classification tasks including CoLA (Warstadt et al., 2019) for sentence acceptability, SST-2 (Socher et al., 2013) for sentiment analysis, MNLI (Williams et al., 2018), QNLI (Demszky et al., 2018), RTE (Giampiccolo et al., 2007), CB (De Marneffe et al., 2019) for natural language inference, STS-B (Cer et al., 2017) for sentence similarity, MRPC (Dolan and Brockett, 2005), QQP (Wang et al., 2019c) for paraphrasing similarity, WSC (Levesque et al., 2012) for coreference resolution, BoolQ (Clark et al., 2019) for question answering and WiC (Pilehvar and CamachoCollados, 2019) for word sense disambiguation. In addition, we also introduced an additional dataset: SciTail (Khot et al., 2018) for few-shot adaption."
        },
        {
            "heading": "4.2 Baselines",
            "text": "To evaluate the effectiveness of our proposed method, we conduct an analysis against several established methods that serve as strong baselines for multi-task learning: Adapter (Houlsby et al., 2019) and Shared-Adapter, we train adapters on a single task or a group of tasks and place them into transformer layers. Hyperformer (Karimi Mahabadi et al., 2021b) and Hyperdecoder (Ivison and Peters, 2022) that use task-conditioned or sample-conditioned Hypernetworks to generate adapters and place them into transformer layers. In addition, We compare our method with Fully fine-tuning(FT), and Shared-FT share the model across different tasks. We also compare the state-ofart prompt transfer methods: Prompt tuning(PT) (Lester et al., 2021), prompt tuning prepends tunable embeddings to the input layer, and the embeddings are initialized with each task respectively. SPoT (Vu et al., 2022) and ATTEMPT (Asai et al.,\n2022), MPT (Wang et al., 2023) adapts to the target tasks with the shared prompts obtained by distilling knowledge from the source tasks."
        },
        {
            "heading": "4.3 Experiments Details",
            "text": "Following the setting of Karimi Mahabadi et al. (2021b), when an original testing set is unavailable, the validation set is utilized as the testing set. In situations where the dataset contains less than 100k records, the validation set is divided into two sets: validation and testing. Conversely, larger datasets utilize 1000 training set samples selected for validation, with the original validation set used for testing. For the multi-task adaptation experiment, we performed multi-task learning on 8 datasets from GLUE and 4 datasets from SuperGLUE. For the low-data adaption experiment, we separately sample each individual task in GLUE with different proportions and quantities (100, 500, 1000, 2000, 4000, 1%, 3%, 5%). As for the evaluation strategy, we use Pearson Correlation for STS-B and accuracy for other tasks as metrics. We save a checkpoint every 1000 steps for all models and report the average performance of all tasks on a single checkpoint. In the few-shot adaption experiment, we randomly sample k = 4, 16, 32 instances from the training set while the entire test set is used for testing. We mainly use T5-Base (220M) model (Raffel et al., 2020) as the pre-trained language model. In addition, we also use T5-Small (60M) and T5-Large (770M) to explore the effect of model size on PHA performance in Section 4.4.5. Unless specified, we train for 65k steps using the AdamW (Loshchilov and Hutter, 2019) optimizer and set the batch size as 128 for all experiments. During training, the initial learning rate is set to\n3e-4 with linear decay and 500 warm-up steps. We set the balancing factor \u03bb = 0.1 in Eq. 9 and keep it fixed for all our experiments. All experiments run for 5 times with different seeds and we report the average for each result. The detailed configurations per method on diverse datasets are shown in Appendix A."
        },
        {
            "heading": "4.4 Results and Analysis",
            "text": ""
        },
        {
            "heading": "4.4.1 Multi-task Adaptation",
            "text": "Table 1 shows the evaluation results on GLUE and SuperGLUE. The results indicate that PHA outperforms all comparative methods regarding performance improvement while maintaining parameter efficiency. Note that we do not compare with SPoT, ATTEMPT, and MPT since they require pretraining prompts to save the knowledge from source tasks and transfer them to target tasks. Extending these methods to the same setting where only pretrained models are available is beyond our scope. Therefore, under the experimental setup of multitask learning, our method cannot achieve a fair comparison with them. It is worth mentioning that MPT, ATTEMPT, and our method both use the same two-step training method in the few-shot transfer setting (Section 4.4.3). Specifically, our PHA approach achieves a performance increase of +1.0% over Adapter while only using 3\u00d7 fewer trainable parameters. When we compared with the state-of-art adapter-based multi-task methods including recent Hyperformer++ and Hyperdecoder that use a hypernetwork to generate conditioned parameters similar to our approach, PHA achieves 0.8% and 1.8% point accuracy improvements respectively on GLUE while utilizing the same or lower number of trainable parameters. This demon-\nstrates the potential of our method to reduce the negative interference between tasks better. In addition, we observe that FT performs the best in all experimental methods, while it requires 220M trainable parameters on a single task. We find that our PHA is as competitive as that of the FT(85.5 vs. 84.9) and reduce the trainable parameters from 100% to 0.28%. We also analyze the effectiveness of PHA in parameter efficiency, as detailed in Appendix B."
        },
        {
            "heading": "4.4.2 Low-data Adaptation",
            "text": "In our framework, we posit that the task prototypes, which are estimated by instance-level features, can better represent task information and improve the performance of hypernetworks in a sample-efficient manner during multi-task adaptation. To verify that our proposed method generalizes better when there are only limited available resources, we conduct low-data adaption experiments on the GLUE benchmark. Following Karimi Mahabadi et al. (2021b), we train all models(Finetuning, Adapter, Hyperformer++, and Hyperdecoder) for 15k steps. More details are in Section 4.3.\nAs shown in Figure 2, PHA outperforms other baselines consistently across different configurations. We obverse that Fine-tuning performs the worst in cases of limited available data. A potential reason is that tuning all parameters of the base model makes it susceptible to keep the model in the state of over-fitting, especially in the low-data regimes. As for other experimental methods, we observe that when the number of available samples is relatively smaller, the existing state-of-theart multi-task methods with hypernetworks close to the performance of Adapter until the number of available samples increases. This observation indicates that hypernetworks struggle to generate optimal parameters when using randomly initialized task embeddings or instances as contextual information under low-data regimes. Moreover, to better simulate the training task distribution in the GLUE benchmark, we randomly sample each individual task in GLUE for different proportions. Figure 2(b) shows the comparison between PHA and adapter-based methods. Similar to the results in Figure 2(a), our proposed method outperforms others by a large margin. We also conduct experiments on 4 datasets (BoolQ, WiC, CB, WSC) from SuperGLUE, as detailed in Appendix C.\nThe superior performance of PHA over all competing methods indicates that our proposed task prototype efficiently captures contextual information to enable hypernetwork to generate the parameters of adapter layers."
        },
        {
            "heading": "4.4.3 Few-shot Adaptation",
            "text": "We explore how our proposed method performs when adapting to new tasks with sample-efficient. Specifically, following Wang et al. (2023), we conduct few-shot experiments on BoolQ, CB, SciTail and compare PHA with the strong baselines, including Fine-tuning, Adapter, Prompt tuning, SPoT, Hy-\nperformer, ATTEMPT, HyperDecoder, and MPT. The results in Table 2 are obtained by training an 8-task adaptation for GLUE and fine-tuned with few-shot samples from BoolQ, CB, and SciTail. More details are in Section 4.3.\nTable 2 summarizes the results on the few-shot adaption setting. Among Adapter-based transfer methods, PHA brings around 3% \u223c 20% absolute improvements over the Adapter across different settings. While Hyperformer achieves better generation for new tasks, it requires us to have a precise understanding of target tasks. PHA significantly improves the performance of Hyperformer without requiring task-specific prior knowledge. In addition, our proposed method significantly outperforms other prompt-based transfer methods in most settings.\nOur results demonstrate that our approach effectively generalizes to new tasks despite the limited availability of training samples."
        },
        {
            "heading": "4.4.4 Ablation Study",
            "text": "We perform an ablation study on the GLUE benchmark to evaluate the effectiveness of the proposed modules. The prototype design and the instancedense retriever are removed independently for this purpose. As shown in Table 3 (row 2), when we remove the prototype design and use the retrieval instances to train, we observe the performance has a significant drop. This shows that the instance-level\ninformation hinders the positive transfer across tasks under the limitation of hypernetwork capacity, while our design of task prototypes allows hypernetwork to capture shared-task information well, which is vital for enabling positive transfer across tasks. Table 3 (row 3) removes the retriever. The task prototypes are estimated by the originally encoded instances. This results in intertwined task prototypes due to the relative dispersion of instance information in the embedding space. The decrease in performance suggests that adding the instancedense retriever enables the prototype to encode task-specific knowledge better. Furthermore, we provide a visualization of the encoded instances from the GLUE benchmark to compare the effect of adding and removing the instance-dense retriever, as shown in Figure 4. While samples belonging to the same task tend to be located near each other in the latent space, samples from different classes (e.g., STS-B, MRPC, RTE) still interleave with each other. After the retriever is added, instances from the same task are tightly clustered, while different tasks are widely separated."
        },
        {
            "heading": "4.4.5 Impact of Model Scale",
            "text": "To verify that our method is applicable to different pre-trained model sizes, we also experiment T5 with sizes from Small (60M) to Large (770M) on GLUE datasets, while reporting the average result of PHA as well as fully Fine-tuning (FT), Adapter (AD), Hyperformer++ (HF) and Hyperdecoder(HD). As shown in Figure 3, under three scales of pre-trained model, we find that PHA achieves superior and competitive performances in low-data and full-data regimes, respectively. This indicates that our proposed prototypical strategy is still able to achieve the best sample efficiency when the size of large-scale transformer models increases."
        },
        {
            "heading": "4.4.6 Effect of Retriever for Generalization.",
            "text": "The retriever plays an important role in adapting to new tasks. To explore how the retriever works after training under a multi-task setting, we consider the similarity scores, described in Eq. 10, to measure retrieval results. Specifically, we randomly sample each individual task in GLUE and calculate the similarity scores through the trained task prototypes and retrieval vectors transferred by the trained retriever. Figure 5(a) shows a visualization of similarity scores. We find that the retriever precisely retrieved the task identity of the corresponding task instance. This suggests that task prototypes and instance vector has aligned in embedding space to enable more efficient capture of single-task common features.\nWe also demonstrate that the retriever has the ability to match the corresponding task prototype to target tasks that require generalization. Figure 5(b) illustrates that the similarity score is relatively high for related tasks such as CB and MNLI, SciTail, and STSB, all of which belong to the NLI task family. As for QNLI and BoolQ, since the task prototype trained on GLUE does not include Boolean Question Answering (QA) tasks, the retriever has matched QNLI prototype, which is in the same domain as BoolQ. Therefore, our proposed method can naturally generalize to new tasks when the related task prototype and the hypernetwork containing cross-task knowledge are both available."
        },
        {
            "heading": "5 Related Work",
            "text": "Multi-task Learning and Transfer. Multi-task learning(MTL) aims to take advantage of the\nshared information between the different tasks and train a unified model to simultaneously solve multiple tasks. In the context of NLP, this is typically achieved by sharing certain layers across all tasks while using task-specific layers for specific tasks (Liu et al., 2019). With the popularization of large language models (LLMs), Raffel et al. (2020) explores the training of LLMs on various tasks which are transformed into a unified format, and some works (Aghajanyan et al., 2021; Aribandi et al., 2022; Sanh et al., 2022; Wei et al., 2022) indicate that the LLMs can be better generalized to new tasks through large-scale multitasking training. More recent work (Pfeiffer et al., 2021; Vu et al., 2022; Asai et al., 2022; Wang et al., 2023) focuses on multi-task transfer with parameter-efficient finetuning as the increasing LM size. Though the effectiveness of multitask learning is improved, they need to finetuning the LLMs twice on source tasks, which are carefully selected, and multiple target tasks. This limits the applicability of the methods. Differently, our proposed method only requires a pre-trained model to achieve multi-task learning and transfer to a new task.\nSeveral works (Jin et al., 2020; Karimi Mahabadi et al., 2021b; Ivison and Peters, 2022; He et al., 2022b) introduce hypernetworks (Ha et al., 2017) to share the cross-task information by generating the parameters of adapter layers (Houlsby et al., 2019) from specific embeddings during multi-task learning. Our work is motivated by Ivison and Peters (2022), but proposes to use task-level information represented by prototypes to optimize the embedding distribution of hypernetworks, which reduces negative transfer between different tasks and improves the performance of adaption across tasks, especially in low-resource regimes.\nPrototype Learning. Prototype learning is widely used to improve the representation ability of networks for few-shot learning. Some works (Gao et al., 2019; Caron et al., 2020; Ding et al., 2021; Li et al., 2021) indicate that the prototype is forced to learn some common features of samples within the class by prototype estimation. Cui et al. (2022) propose to construct a verbalizer for prompt-based few-shot tuning by estimating prototypes with contrastive learning. This differs from our method, which uses a prototypical strategy to explore the specific information for corresponding tasks."
        },
        {
            "heading": "6 Conclusion",
            "text": "We introduce Prototype-based HyperAdapter (PHA), a novel framework built on adapter-tuning. PHA achieves both multi-task adaption and adaption to a new task in a sample-efficient manner. It generates parameters of adapter layers conditioned on task-specific prototypes which are calculated by the corresponding instance-level features. In addition, the specific prototype is retrieved and transferred to new tasks to be further tuned. The resulting method significantly outperforms previous SOTA on full/low-data multi-task adaption and few-shot adaption.\nLimitations\nOur work has demonstrated strong experimental results and sample efficiency in multitasking adaption. However, there are several limitations: Firstly, in few-shot adaption, the method we proposed, which tunes the base model on 8 NLP tasks, can generalize to new target tasks efficiently. But tuning on more large-scale tasks may result in better generalization improvements. Secondly, as shown in Figure 5, a new task may be related to multiple task prototypes, rather than a single one. In our method, we only select the most relevant prototypes, which may ignore the transfer of some weakly related knowledge. In addition, we use adapters in this work, but our method could possibly also benefit from other parameter-efficient approaches (Lester et al., 2021; Mahabadi et al., 2021; Li and Liang, 2021; Hu et al., 2022; Liu et al., 2022)."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by National Key R&D Program of China (Grant No. 2022YFF1202400), Major Science and Technology Innovation Program of Hangzhou (Grant No. 2022AIZD0154), Natural Science Foundation of China (Grant No. 62176025, 62301066), Beijing Nova Program (Grant No. 20220484161), the Fundamental Research Funds for the Central Universities (Grant No. 2023RC72) and Theme-based Research Scheme (T45-205/21N), Research Grants Council of Hong Kong."
        },
        {
            "heading": "B Parameter Efficiency",
            "text": "Our proposed method not only balances sample efficiency and accuracy on stream tasks but also achieves parameter efficiency. We compare the trainable parameters of PHA with other baselines. We assume that the basic model is a transformer model with an L-layer encoder-decoder structure for \u03c4 -tasks and d is the model dimension. The tasksshare adapter is attached to the encoder layer and the condition-generate adapter is attached to the decoder layer in our work. The bottleneck dimension of Adapter is b. Adapter for the single layer costs 2db+ b+ d. Given bottleneck size d\u2032, the instancedense retriever G(\u00b7) with bottleneck architecture to construct the retrieval vector z \u2208 Rd\u2032 , which result in dd\u2032 + d\u20322 parameters. We have the task prototype ki \u2208 Rd \u2032 for i-th task and layer embedding em \u2208 Rd \u2032\nfor m-th layer. These have a total of \u03c4d\u2032 + Ld\u2032 parameters. The project network C(\u00b7) is used for concatenating the task prototype ki \u2208 Rd \u2032 and layer embedding em \u2208 Rd \u2032\ninto a mixed embedding Imi \u2208 Rdh , which result in 2d\u2032dh parameters. The hypernetworks costs dh(2db+ b+ d) parameters. The total cost of PHA is dd\u2032+d\u20322+(\u03c4d\u2032+ Ld\u2032)+ 2d\u2032dh+ dh(2db+ b+ d)+L(2db+ b+ d). There are a large number of tasks and transformer layers in our work settings. For the adapter, its parameter count increases linearly with the task and number of layers. For Hyperformer, its parameter count is more than our method due to using two\nhypernetworks. We used a bottleneck structured retriever to reduce trainable parameters compared to Hyperdecoder."
        },
        {
            "heading": "C Additional Results",
            "text": "Here, we conduct more experiments to demonstrate the sample efficiency of our method on different datasets (BoolQ, WiC, CB, and WSC) with T5Base pre-trained model. The result are included in Table 4. The performance PHA with T5-Base achieves the best performance in terms of accuracy and sample efficiency in the 4 datasets from SuperGLUE. In addition, to verify the effectiveness of our method on NLU tasks, we conducted sample efficiency experiments on three datasets: Xsum, WMT16 Ro-En, and WMT16 En-Ro. Due to limited computation resources, we sampled 1% of the data and conducted experiments on three backbone models of different scales(small, base, and large). The results are shown in Table 5."
        }
    ],
    "title": "Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning",
    "year": 2023
}