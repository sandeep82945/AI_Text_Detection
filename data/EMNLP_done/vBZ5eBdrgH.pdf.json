{
    "abstractText": "In this work, we focus on the task of machine translation (MT) from extremely low-resource language (ELRLs) to English. The unavailability of parallel data, lack of representation from large multilingual pre-trained models, and limited monolingual data hinder the development of MT systems for ELRLs. However, many ELRLs often share lexical similarities with high-resource languages (HRLs) due to factors such as dialectical variations, geographical proximity, and language structure. We utilize this property to improve cross-lingual signals from closely related HRL to enable MT for ELRLs. Specifically, we propose a novel unsupervised approach, SELECTNOISE, based on selective candidate extraction and noise injection to generate noisy HRLs training data. The noise injection acts as a regularizer, and the model trained with noisy data learns to handle lexical variations such as spelling, grammar, and vocabulary changes, leading to improved cross-lingual transfer to ELRLs. The selective candidates are extracted using BPE merge operations and edit operations, and noise injection is performed using greedy, top-p, and top-k sampling strategies. We evaluate the proposed model on 12 ELRLs from the FLORES-200 benchmark in a zero-shot setting across two language families. The proposed model outperformed all the strong baselines, demonstrating its efficacy. It has comparable performance with the supervised noise injection model. Our code and model are publicly available1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Maharaj Brahma"
        },
        {
            "affiliations": [],
            "name": "Kaushal Kumar Maurya"
        },
        {
            "affiliations": [],
            "name": "Maunendra Sankar Desarkar"
        }
    ],
    "id": "SP:be893b54a1f9d62a76e7172cef08c71f0cbbde96",
    "references": [
        {
            "authors": [
                "No\u00ebmi Aepli",
                "Rico Sennrich."
            ],
            "title": "Improving zeroshot cross-lingual transfer between closely related languages by injecting character-level noise",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 4074\u20134083, Dublin, Ireland.",
            "year": 2022
        },
        {
            "authors": [
                "Eneko Agirre",
                "Daniel Cer",
                "Mona Diab",
                "Aitor Gonzalez-Agirre."
            ],
            "title": "SemEval-2012 task 6: A pilot on semantic textual similarity",
            "venue": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Proceedings of the",
            "year": 2012
        },
        {
            "authors": [
                "Roee Aharoni",
                "Melvin Johnson",
                "Orhan Firat."
            ],
            "title": "Massively multilingual neural machine translation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2019
        },
        {
            "authors": [
                "neth Heafield"
            ],
            "title": "Copied monolingual data",
            "year": 2017
        },
        {
            "authors": [
                "Chaudhary"
            ],
            "title": "Beyond english-centric mul",
            "year": 2021
        },
        {
            "authors": [
                "Xavier Garcia",
                "Aditya Siddhant",
                "Orhan Firat",
                "Ankur Parikh."
            ],
            "title": "Harnessing multilinguality in unsupervised machine translation for rare languages",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "CoRR, abs/1904.09751.",
            "year": 2019
        },
        {
            "authors": [
                "Pratik Joshi",
                "Sebastin Santy",
                "Amar Budhiraja",
                "Kalika Bali",
                "Monojit Choudhury."
            ],
            "title": "The state and fate of linguistic diversity and inclusion in the NLP world",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Omer Levy",
                "Jacob Eisenstein",
                "Marjan Ghazvininejad."
            ],
            "title": "Training on synthetic noise improves robustness to natural noise in machine translation",
            "venue": "Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),",
            "year": 2019
        },
        {
            "authors": [
                "Yash Khemchandani",
                "Sarvesh Mehtani",
                "Vaidehi Patil",
                "Abhijeet Awasthi",
                "Partha Talukdar",
                "Sunita Sarawagi."
            ],
            "title": "Exploiting language relatedness for low web-resource language model adaptation: An Indic languages study",
            "venue": "Proceedings of the 59th",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Philipp Koehn",
                "Rebecca Knowles."
            ],
            "title": "Six challenges for neural machine translation",
            "venue": "Proceedings of the First Workshop on Neural Machine Translation, pages 28\u201339, Vancouver. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "I. Dan Melamed."
            ],
            "title": "Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons",
            "venue": "Third Workshop on Very Large Corpora.",
            "year": 1995
        },
        {
            "authors": [
                "Graham Neubig",
                "Junjie Hu."
            ],
            "title": "Rapid adaptation of neural machine translation to new languages",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 875\u2013880, Brussels, Belgium. Association for Com-",
            "year": 2018
        },
        {
            "authors": [
                "Toan Q. Nguyen",
                "David Chiang."
            ],
            "title": "Transfer learning across low-resource, related languages for neural machine translation",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Marta R. Costa-juss\u00e0"
            ],
            "title": "No language left behind: Scaling human-centered machine translation",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Vaidehi Patil",
                "Partha Talukdar",
                "Sunita Sarawagi."
            ],
            "title": "Overlap-based vocabulary generation improves cross-lingual transfer among related languages",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Maja Popovi\u0107."
            ],
            "title": "chrF: character n-gram F-score for automatic MT evaluation",
            "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392\u2013395, Lisbon, Portugal. Association for Computational Linguistics.",
            "year": 2015
        },
        {
            "authors": [
                "Ivan Provilkov",
                "Dmitrii Emelianenko",
                "Elena Voita."
            ],
            "title": "BPE-dropout: Simple and effective subword regularization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1882\u20131892, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Amy Pu",
                "Hyung Won Chung",
                "Ankur P Parikh",
                "Sebastian Gehrmann",
                "Thibault Sellam."
            ],
            "title": "Learning compact metrics for mt",
            "venue": "Proceedings of EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Reinhard Rapp."
            ],
            "title": "Similar language translation for Catalan, Portuguese and Spanish using Marian NMT",
            "venue": "Proceedings of the Sixth Conference on Machine Translation, pages 292\u2013298, Online.",
            "year": 2021
        },
        {
            "authors": [
                "Thibault Sellam",
                "Dipanjan Das",
                "Ankur Parikh."
            ],
            "title": "BLEURT: Learning robust metrics for text generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881\u20137892, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Edinburgh neural machine translation systems for WMT 16",
            "venue": "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 371\u2013376, Berlin, Germany. Asso-",
            "year": 2016
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Aditya Siddhant",
                "Ankur Bapna",
                "Orhan Firat",
                "Yuan Cao",
                "Mia Xu Chen",
                "Isaac Caswell",
                "Xavier Garcia."
            ],
            "title": "Towards the next 1000 languages in multilingual machine translation: Exploring the synergy between supervised and self-supervised learning",
            "venue": "arXiv",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Sperber",
                "Jan Niehues",
                "Alex Waibel."
            ],
            "title": "Toward robust neural machine translation for noisy input sequences",
            "venue": "Proceedings of the 14th International Conference on Spoken Language Translation, pages 90\u201396.",
            "year": 2017
        },
        {
            "authors": [
                "Amane Sugiyama",
                "Naoki Yoshinaga."
            ],
            "title": "Data augmentation using back-translation for contextaware neural machine translation",
            "venue": "Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 35\u201344, Hong Kong,",
            "year": 2019
        },
        {
            "authors": [
                "Xu Tan",
                "Jiale Chen",
                "Di He",
                "Yingce Xia",
                "Tao Qin",
                "Tie-Yan Liu."
            ],
            "title": "Multilingual neural machine translation with language clustering",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates,",
            "year": 2017
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Xinyi Wang",
                "Hieu Pham",
                "Philip Arthur",
                "Graham Neubig."
            ],
            "title": "Multilingual neural machine translation with soft decoupled encoding",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Xinyi Wang",
                "Hieu Pham",
                "Zihang Dai",
                "Graham Neubig."
            ],
            "title": "SwitchOut: an efficient data augmentation algorithm for neural machine translation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 856\u2013861,",
            "year": 2018
        },
        {
            "authors": [
                "Zhirui Zhang",
                "Shuangzhi Wu",
                "Shujie Liu",
                "Mu Li",
                "Ming Zhou",
                "Tong Xu."
            ],
            "title": "Regularizing neural machine translation by target-bidirectional agreement",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 443\u2013450.",
            "year": 2019
        },
        {
            "authors": [
                "Wenhao Zhu",
                "Hongyi Liu",
                "Qingxiu Dong",
                "Jingjing Xu",
                "Lingpeng Kong",
                "Jiajun Chen",
                "Lei Li",
                "Shujian Huang."
            ],
            "title": "Multilingual machine translation with large language models: Empirical results and analysis",
            "venue": "arXiv preprint arXiv:2304.04675.",
            "year": 2023
        },
        {
            "authors": [
                "Barret Zoph",
                "Deniz Yuret",
                "Jonathan May",
                "Kevin Knight."
            ],
            "title": "Transfer learning for low-resource neural machine translation",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1568\u20131575, Austin, Texas.",
            "year": 2016
        },
        {
            "authors": [
                "D Datasets"
            ],
            "title": "Detailed statistics of datasets used in our experiments are shown in Table 9. For performing analysis on less-related language, we use the general test set of IndicTrans2 (AI4Bharat",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The modern neural machine translation (NMT; Aharoni et al. (2019); Garcia et al. (2021); Siddhant et al. (2022)) has achieved remarkable performance for many languages, but their performance heavily relies on the availability of large parallel or mono-\n*Equal contributions 1code and model checkpoints link: https://github.\ncom/maharajbrahma/selectnoise\nlingual corpora (Koehn and Knowles, 2017). However, the linguistic diversity across the world is vast, with over 7000 languages spoken2. This linguistic landscape includes a long tail of languages (Joshi et al., 2020) that face significant challenges due to the lack of available resources for model development and are referred as extremely low-resource languages (ELRLs). ELRLs present unique challenges for the development of MT systems as they lack parallel datasets, are excluded from large multilingual pre-trained language models, and possess limited monolingual datasets. Consequently, the majority of research efforts in NMT have primarily focused on resource-rich languages (Bender, 2019), leaving ELRLs with limited attention and fewer viable solutions. Towards these concerns, this work is positioned as a step towards enabling MT technology for ELRLs. Primarily focused on zero-shot setting for scalability.\nMore recently, there has been active research to develop MT systems for LRLs. One direction is\n2https://www.ethnologue.com/\nmultilingual training-based models (Aharoni et al., 2019; Garcia et al., 2021; Siddhant et al., 2022). These models are trained with multiple HRL languages, enabling cross-lingual transfer capabilities to improve translation performance for LRLs. Another line of work focuses on data augmentation techniques (Sennrich et al., 2016a; Wang et al., 2018) to generate more training data. However, these methods do not fully exploit the lexical similarity between HRLs and ELRLs. Many HRLs and ELRLs exhibit surface-level lexical similarities due to dialect variations, loan words, and geographical proximity (Khemchandani et al., 2021). For example, the word \u201cMonday\u201d is somvar in Hindi and somar in Bhojpuri. They are lexically very similar. To leverage this lexical similarity, recent studies have explored techniques like learning overlapping vocabulary (Patil et al., 2022) or injecting random noise (Aepli and Sennrich, 2022; Blaschke et al., 2023) in HRLs to resemble LRLs. These methods are only evaluated for natural language understanding tasks (NLU) and not for MT, which is a more challenging task. Inspired by these advancements, in this paper, we propose a novel unsupervised noise injection approach to develop an MT system for ELRLs.\nThe proposed model is based on character noise injection and consists of two stages: Selective Candidate Extraction and Noise Injection. In the selective candidate extraction phase, candidate characters are extracted in an unsupervised manner using small monolingual data from closely related HRL and ELRLs. It relies on BPE merge operations and edit operations that take into account lexical similarity and linguistic properties. In the noise injection phase, noise is injected into the source side of parallel data of HRL using greedy, top-k, and top-p sampling algorithms. This noise injection serves as a regularizer and a model trained with this noisy HRL data enhances robustness to spelling, grammar, or vocabulary changes and facilitates improved cross-lingual transfer for ELRLs. The evaluation is done in the zero-shot setting, ensuring scalability. The proposed model is referred as the SELECTNOISE: Unsupervised Selective Noise Injection model. Fig. 1 illustrates the effect of noise injection with SELECTNOISE model. In this paper, we investigate two hypotheses: (a) the selective noise injection model is expected to outperform random noise injection, and (b) the performance of the selective (unsupervised) noise injection model\nshould be comparable to the supervised noise injection model.\nOur key contributions are: (1) We propose a novel unsupervised selective character noise injection approach, SELECTNOISE, to enable and improve MT for ELRLs to English. The injection of selective candidate noise facilitates better cross-lingual transfer for ELRLs in the zero-shot setting. (2) We have developed an unsupervised mechanism to extract candidate characters based on BPE merge operations and edit operations. Furthermore, the noise injection employs greedy, top-k, and top-p sampling techniques, ensuring diversity. (3) We evaluated the model\u2019s performance using 12 ELRLs from the FLORES-200 evaluation set across two typologically diverse language families. Evaluations were conducted with both automated and human evaluation metrics. (4) The proposed model outperformed all baselines and has comparable performance with the supervised selective noise injection-based MT model. Additionally, we performed several analyses to demonstrate the robustness of the proposed model."
        },
        {
            "heading": "2 Methodology",
            "text": "This section presents the details of the proposed SELECTNOISE model. As discussed in section 1, the SELECTNOISE model has two stages: Selective Candidate Extraction and Noise Injection. In the selective candidate extraction stage, the noise injection candidate characters are extracted through an unsupervised approach. Specifically, we consider small monolingual data for HRL (DH) and for related (lexically similar) LRLs (DL). DL comprises small monolingual datasets from multiple extremely ELRLs. During the process of building the Byte Pair Encoding (BPE) vocabulary, we extract the BPE operations separately for the HRL (BH) and ELRLs (BL). Next, we design an algorithm A to extract selective candidate characters from BH and BL and store them in candidate pool SC . Candidates are extracted with an approach inspired by edit-operations. In other words, we obtain SC as a result of A (BH, BL). In noise injection stage, the candidates are sampled from SC and injected into the source sentences of HRL corpus (H) from the large parallel data PH = {(h, e)|lang(h) = H, lang(e) = En}. The injections are done using a noise function \u03b7, resulting in a noise-injected (augmented) parallel data: P\u0302H = {(h\u0302, e)|lang(h\u0302) = H\u0302, lang(e) =\nEn}, where H\u0302 = \u03b7(H). The H\u0302 acts as proxy training data for ELRLs. A BPE vocabulary V is learned with H\u0302. Then, we train the standard encoder-decoder transformers model (M; Vaswani et al. (2017b) from scratch with H\u0302 and V to obtain trained model M\u0302. Finally, zero-shot evaluation is done for ELRLs with M\u0302 and V . Now, we present details of each model component. The overview of the proposed SELECTNOISE model is depicted in Figure 2."
        },
        {
            "heading": "2.1 SELECTNOISE: Unsupervised Noise Injection",
            "text": "The formal procedure for unsupervised noise injection is presented in Algorithm 1. In the next subsections, we will deep dive into each stage of the proposed model in details:"
        },
        {
            "heading": "2.1.1 Selective Candidate Extraction",
            "text": "The first stage in the proposed approach involves extracting candidate characters that will subsequently be utilized for noise injection. Given DH and DL, we extract all BPE merge operations BH and BL, respectively. Each merge operation consists of tuples \u27e8(p, q)\u27e9 \u2208 BH and \u27e8(r, s)\u27e9 \u2208 BH. We pair each merge tuple of BH with each tuple of BL (i.e., cartesian setup). If BH and BL have n and m merge operations, respectively, we obtain a total of t = m \u00b7 n pairs. We consider only those pairs where either p and r or q and s are the same while discarding the rest. For the considered tuples \u27e8(p, q), (r, s)\u27e9, we calculate the characterlevel edit-distance operations between non-similar elements of the tuple. For instance, if p and r are the same, the edit operations are obtained using q and s elements. These operations are collected in the candidate pool Sc, which includes insertions,\ndeletions, and substitutions, and are referred to as the selective candidates.\nAs discussed, the extracted selective candidates are stored in the candidate pool Sc, a dictionary data structure encompassing HRL and ELRL characters. The Sc consists of HRL characters, ELRL characters, edit operations, and their respective frequencies. An element of Sc has following template: ci : {I : fins, D : fdel, S : {c\u20321 : f1, c\u20322 : f2, c \u2032 k : fk}}. The operations are: insertion (I), deletion (D) and substitution (S). The character ci represents the ith element of Sc, which is an HRL character, c\u20321 . . . c \u2032 k denote the corresponding substituting candidates from ELRLs and f is the associated frequencies. A few examples of selective candidate extraction are illustrated in Fig. 3. Sample candidate pool (Sc) is shown in Fig. 6.\nIntuitively, with this candidate pool Sc, we have learned transformative entities that can be used to resemble an HRL to lexically similar ELRLs, which results in bridging the lexical gap between HRL and LRLs. Training with such modified HRL data enhances the effectiveness of crosslingual transfer signals for ELRLs. As candidates are extracted by considering the vocabulary wordformation strategy from BPE and edit operations, they indirectly consider the linguistic cues/information."
        },
        {
            "heading": "2.1.2 Noise Injection to HRL",
            "text": "In the second stage, we sample selective candidates from Sc and inject into the source sentences of HRL corpus (H) from the parallel dataset PH = {(h, e)|lang(h) = H, lang(e) = En} using a noise function \u03b7, resulting in a noise injected (augmented) parallel dataset P\u0302H = {(h\u0302, e)|lang(h\u0302) = H\u0302, lang(e) = En}, where H\u0302 = \u03b7(H). Details of\nthe noise function and candidate sampling strategy are presented below: Noise Function: The noise injection function (\u03b7) is designed as follows: Initially, we randomly select 5%-10%3 of character indices from a sentence s \u2208 H. Subsequently, we uniformly choose between insertion, deletion, or substitution operations with equal probability. If the selected operation is insertion or substitution, we sample a candidate character from Sc to perform the noise injection operation. For deletion, the charter is simply deleted. These steps are repeated for all sentences in H to obtain the final H\u0302. Candidate Character Sampling: While noise injection for deletion operation, we simply delete the character. For insertion and substitution, we sample the candidate character for injection from Sc using the greedy, top-p (nucleus), and top-k sampling techniques inspired by decoding algorithms commonly employed in NLG (Holtzman et al., 2019). Before applying these sampling techniques, the frequencies of the candidate characters are transformed into probability scores using the softmax operation. Intuitively, with the sampling technique, we aim to explore not only frequent candidate characters but also diverse candidates.\nThe performance of any learning model depends on the quality of the training data. The presence of noise hampers the learning, and the outputs of the learned model exhibit the different nuances of the noise present in the data. In our specific case: (i) We train a model using data that contains noise, resulting in the model\u2019s increased robustness to minor lexical variations in different languages, par-\n3after conducting several ablation experiments, this range provides the best performance\nticularly those related to ELRLs. (ii) The noise is added for a small portion of characters (5-10%), making the HRLs training data closely resemble how sentences appear in ELRLs. As a result, the model is able to do a robust cross-lingual transfer to the ELRL in a zero-shot setting. In another perspective, the injection of noise acts as a regularizer (Aepli and Sennrich, 2022), contributing to an overall enhancement in the model\u2019s performance."
        },
        {
            "heading": "2.2 Supervised Noise Injection",
            "text": "We have also investigated in a supervised setting akin to the proposed SELECTNOISE approach. The key distinction lies in how the candidate pool Ssc is derived from a limited parallel dataset between HRL and ELRLs. For each HRL and ELRL pair,\nAlgorithm 1 SELECTNOISE: Unsupervised Noise Injection Require: [Inputs] HRL monolingual data DH; closely re-\nlated ELRLs monolingual dataDL; number of merge operationsMO; HRL parallel data PH(H, En); Noise injection percentage range [p1% - p2%]; candidate sampling strategy SM; EXTRACTSELECTIVECANDS (A)\nEnsure: [Output] Noisy source HRL H\u0302 Sc = EXTRACTSELECTIVECANDS(DH,DL,MO) for sentence s inH do\nidxs\u2190 randomly select [p1% - p2%] indices of s for idx in idxs do\nops\u2190 randomly sample operation {insert, delete, substitute} if ops equals delete then Remove character at index idx end if if ops equals Insert or ops equals substitute then\nc = sample candidate char, i.e., SM(Sc, ops) Perform operation ops at index idx with c\nend if end for\nend for procedure EXTRACTSELECTIVECANDS(DH,DL,MO)\nInitialize candidate pool Sc \u2190 \u2205 to store candidates Compute merge operations BH = BPE(DH,MO) Compute merge operations BL = BPE(DL,MO) for n in BH do\nfor m in BL do \u25b7 where n = tuple \u27e8(p, q)\u27e9, m = tuple \u27e8(r, s)\u27e9\nif n equals m or (p not equals r and q not equals s) then No operation is performed with n and m end if if p equals r then\nCompute edit-operations(q, s) & update Sc end if if q equals s then\nCompute edit-operations(p, r) & update Sc end if\nend for end for Return Sc end procedure\nAlgorithm 2 Supervised Noise Injection Require: [Inputs] joint parallel data for all considered HRL-\nELRL pairs PS(S, EL); HRL parallel data PH(H, En); noise injection percentage range [p1% - p2%]; candidate sampling strategy SM\nEnsure: [Output] Noisy source HRL H\u0302 Ssc = SUPEXTRACTSELECTIVECANDS(PS ) for sentence s inH do\nidxs\u2190 randomly select [p1% - p2%] indices of s for idx in idxs do\nops\u2190 randomly sample operation {insert, delete, substitute} if ops equals delete then Remove character at index idx end if if ops equals Insert or ops equals substitute then\nc = sample candidate char, i.e., SM(Ssc, ops) Perform operation ops at index idx with c\nend if end for\nend for procedure SUPEXTRACTSELECTIVECANDS(PS )\nInitialize candidate pool Ssc \u2190 \u2205 to store candidates for each \u27e8(s, e)\u27e9 in PS do\nCompute edit-operations(s, e) & update Ssc end for return Ssc\nend procedure\nwe extract a candidate set using edit operations and subsequently combine all the candidate sets in Sc. The rest of the modeling steps are similar to the SELECTNOISE. We hypothesize that the unsupervised method should exhibit competitive performance compared to the supervised approach. In the supervised candidate extraction, we assume the availability of a limited amount of parallel data of approximately 1000 examples. A formal algorithm outlining in the Algorithm 2."
        },
        {
            "heading": "2.3 Model Training and Zero-shot Evaluation",
            "text": "The stranded encoder-decoder transformers model (M) is trained from scratch using the noisy highresource parallel dataset P\u0302H and V to obtain a trained model M\u0302. Where V is learned BPE vocabulary with P\u0302H. Subsequently, we use M\u0302 to perform zero-shot generation for ELRLs. We have not used any parallel training data for ELRLs and directly employ M\u0302 for inference, making this modeling setup zero-shot. The trained model transfers knowledge across languages, enabling coherent and meaningful translation for ELRLs."
        },
        {
            "heading": "3 Experimental Setup",
            "text": "We designed our experimental setup to address the following set of questions: (1) Does noise injection improve performance for NLG tasks, i.e., MT in\nour case? (2) Does selective noise injection with the proposed SELECTNOISE model outperform the random noise injection model (Aepli and Sennrich, 2022)? (3) Does the model\u2019s performance persist across different language families? and (4) Does the unsupervised SELECTNOISE model demonstrate competitive performance with supervised approach? Based on these research questions, we have designed our experimental setup."
        },
        {
            "heading": "3.1 Datasets",
            "text": "The primary constraint of the proposed approach is to select closely related HRLs and ELRLs. With this criterion in mind, we have chosen two language families: Indo-Aryan and Romance. Within the Indo-Aryan family, we have selected Hindi (hi) as the HRL and 8 ELRLs were Awadhi (awa), Bhojpuri (bho), Chhattisgarhi (hne), Kashmiri (kas), Magahi (mag), Maithili (mai), Nepali (npi), and Sanskrit (san), based on their lexical similarity. For the Romance family, Spanish (es) served as the HRL, and the 4 ELRLs were Asturian (ast), Catalan (cat), Galician (glg), and Occitan (oci). We conducted separate experiments for each language family, training the model with the HRL to English MT task and evaluating it in a zero-shot setting with corresponding ELRLs.\nIn total, we have 3 HRLs (English, Hindi, and Spanish) and 12 ELRLs. All the test datasets are sourced from FLORES-200 (NLLB Team, 2022), while the hi-en dataset is obtained from AI4Bharat (Ramesh et al., 2022), and the es-en dataset is from Rapp (2021). The development set of FLORES200 was utilized as a parallel dataset for supervised noise injection. A small amount of monolingual data was used for SELECTNOISE and other baseline methods. Here, we used 1000 examples for each ELRL. Detailed dataset statistics and data sources are presented in Appendix D. In Appendix C, we provide an overview of the lexical similarity between HRLs and ELRLs."
        },
        {
            "heading": "3.2 Baselines",
            "text": "We compare the SELECTNOISE model with several strong baselines, including a traditional data augmentation model, lexical similarity-based models, and a model based on random noise injection. Details of each baseline are presented below:\n\u2022 Vanilla NMT: A standard transformer-based NMT model (Vaswani et al., 2017a) with BPE algorithm (Sennrich et al., 2016b).\n\u2022 Word-drop (Sennrich et al., 2016a): In this baseline, 10% words embeddings from each sentence of the source HRL is set to zero. This is a data augmentation technique to create training data ELRLs. The rest of the steps are similar to the SELECTNOISE model.\n\u2022 BPE-drop: This approach is similar to the word-drop baseline but uses BPE tokens instead of words.\n\u2022 SwitchOut (Wang et al., 2018): In this baseline, we randomly replace 10% of the source and target words with randomly sampled words from their respective vocabularies. We use the officially released implementation.\n\u2022 OBPE (Patil et al., 2022): OBPE modifies the learned BPE vocabulary to incorporate overlapping tokens from both HRL and LRLs, even if the token is not frequent. We utilized the official implementation.\n\u2022 BPE Dropout (Provilkov et al., 2020): It is based on the BPE algorithm to learn the vocabulary and generates non-deterministic segmentations for input text on-the-fly during training. We use a dropout value of 0.1.\n\u2022 Random Char Noise (Aepli and Sennrich, 2022): This baseline methodology is similar to the proposed SELECTNOISE approach; but, noise injections are done randomly."
        },
        {
            "heading": "3.3 Evaluation Metrics",
            "text": "All the model performances are compared using both automated and human evaluation metrics. In line with recent research on MT for LRLs, we employ two types of automated evaluation metrics (NLLB Team, 2022; Siddhant et al., 2022). Specifically, lexical match-based metrics: BLEU (Papineni et al., 2002) and chrF (Popovic\u0301, 2015) and learning-based metrics: BLEURT (Sellam et al., 2020) and COMET (Pu et al., 2021).\nWe further conducted the human evaluation to ensure the reliability of the performance gain. Three languages from the Indo-Aryan family ( Bhojpuri, Nepali, and Sanskrit) were selected based on their high, moderate, and low lexical similarity with the HRL (Hindi). To manage the annotators\u2019 workload effectively, we limited our evaluation to three models: Vanilla NMT, BPE Dropout, and SELECTNOISE. For each language, the human evaluation set consisted of 24 examples, and translations were obtained from above mentioned three models. Two annotators were employed for each language to ensure the inter-annotator agreement, and two ratings were obtained for each example from these annotators. All annotators held at least a master\u2019s degree, were native speakers of the respective language and demonstrated proficiency in English. We use Crosslingual Semantic Text Similarity (XSTS) metric (Agirre et al., 2012), which is widely adopted in the MT research for human evaluation. The XSTS metric employs a 1-5 evaluation scale, where 1 represents a very bad translation and 5 represents a very good translation."
        },
        {
            "heading": "4 Results and Discussions",
            "text": "In this section, we will discuss results, observations and findings. The zero-shot automated evaluation scores are presented in Tables 1, 2, 3 and 4. The results are reported with greedy, top k (k = 50), and top-p (p = 0.25) sampling strategies. Table 5 presents the human evaluation results. SELECTNOISE vs. Baselines: The proposed and other models that incorporate lexical similarity have demonstrated superior performance compared to the Vanilla NMT model. While general data augmentation techniques like Word-drop and SwitchOut exhibit performance similar to the Vanilla NMT model, they perform poorly when compared to OBPE and BPE-Dropout models. These results indicate the importance of considering monolingual data from ELRLs in the modeling However, random noise injection and the SELECTNOISE approach outperform the OBPE and BPEDropout models, indicating the effectiveness of noise injection-based modeling techniques. In conclusion, the careful selection of noise candidates, as done in the SELECTNOISE approach, has outperformed the random noise model (second best) and emerged as the state-of-the-art model. Selective vs. Random Noise injection: Unsupervised selective noise injection approaches exhibit a\nlarger performance gain compared to the random noise injection model. This observation emphasizes the importance of a systematic selective candidate extraction and noise injection process. Lexical vs. Learned Evaluation Metrics: We observe a strong correlation between lexical matchbased metrics, such as BLEU and chrF scores. Further, semantic-based metrics like BLEURT and COMET exhibit similar trends to lexical match metrics, indicating a high level of correlation. This emphasizes the reliability of evaluation scores. Automated vs. Human Evaluation: The proposed SELECTNOISE model outperforms both baselines in human evaluation across all three languages. The model demonstrates acceptable zeroshot performance for ELRLs, with a strong correlation with automated evaluation scores. Performance across Language Families: Unsupervised selective noise injection consistently outperforms all the baselines across ELRLs, with few exceptions. The model exhibits similar performance trends across both language families. Unsupervised vs. Supervised Noise Injection: The unsupervised SELECTNOISE model performs comparably to the supervised model, with slight variations depending on the language and family. The performance gap between the two models is minimal, indicating their equal strength. Performance vs. Sampling Strategies: The performance with different sampling techniques is compared, and it is observed that the greedy approach for SELECTNOISE performs better for the majority of languages. This finding indicates the existence of one-to-one lexical mapping across HRL and ELRLs. However, other sampling approaches are also effective for a subset of ELRLs. Overall Performance: As we can observe from the average automated evaluation scores, the proposed SELECTNOISE model outperforms all the baselines by a significant margin. It also exhibits comparable performance to the supervised model, and this performance persists across different language families. These findings satisfy our hypothesis, leading us to conclude that the proposed SELECTNOISE model is a state-of-the-art model for English-to-ELRLs MT systems."
        },
        {
            "heading": "5 Further Analyses",
            "text": "In this section, we perform a detailed analysis with SELECTNOISE to understand factors contributing to performance gain and analyze robustness.\nPerformance Trend with Top-k and Top-p: In Figure 4, the performance trend of the proposed model with varying values of k and p for top-p and top-k sampling is depicted. The candidate pool consists of a maximum of 61 characters (a range for k-value selection). The model performs best with a k-value of 50 and a p-value of 0.25, offering valuable insights for optimizing its performance through parameter selection. Impact of Monolingual data size: The proposed SELECTNOISE model relies on the small monolingual dataset of ELRLs. We investigate the impact of a large monolingual dataset on the model\u2019s performance for ELRLs. Table 6 demonstrates that a larger dataset leads to a performance boost, suggesting the extraction of more meaningful noise injection candidates. Language similarity Vs. Performance: Figure 5 illustrates the comparative trend of lexical similarity score between ELRLs and HRLs and performance (ChrF score). It can be observed that lexically similar languages boost the model\u2019s performance, leading to an improved cross-lingual transfer for the SELECTNOISE model. For example, languages like Kashmiri (kas), which have the lowest similarity, exhibit the lowest performance, whereas Chhattisgarhi, with the highest lexical similarity, demonstrates the highest performance. Performance with Less related Languages: We evaluate the zero-shot translation performance of Vanilla NMT and proposed SELECTNOISE models with two relatively less lexically similar ELRLs. These two languages belong to distinct language families, namely Bodo (Sino-Tibetan) and Tamil\n(Dravidian). Bodo has Devanagari script, while Tamil employs script conversion to match HRL (Hindi) script. The results are reported in Table 7. It is observed that the performance gain is minimal due to the dissimilarity of ELRLs with the corresponding HRL."
        },
        {
            "heading": "6 Related Work",
            "text": "MT for Low-resource Languages: Limited parallel corpora for many LRLs lead to active research in multilingual MT. These are grounded with transfer learning to enable cross-lingual transfer (Nguyen and Chiang, 2017; Zoph et al., 2016) and allow related languages to learn from each other (Fan et al., 2021; Costa-juss\u00e0 et al., 2022; Siddhant et al., 2022). Further, these approaches can be extended by grouping training data based on relatedness (Neubig and Hu, 2018) or clustering similar languages (Tan et al., 2019) to improve performance for LRLs. In another direction, monolingual corpora are combined with parallel corpora to enhance translation quality (Currey et al., 2017) or used for unsupervised NMT (Artetxe et al., 2018; Lewis et al., 2020), reducing the reliance on parallel data. Back-translated data is also widely utilized for training MT systems for LRLs (Sugiyama and Yoshinaga, 2019; Edunov et al., 2018). More recently, models powered by large multilingual pretrained language models (mLLMs) enable MT with limited language resources (NLLB Team, 2022; Zhu et al., 2023). These models have shown ac-\nceptable performance for many LRLs. However, adapting these models to ELRLs is challenging because they often lack parallel data, have limited monolingual data, and are absent from mPLMs. In contrast, we propose a model that only requires a small monolingual data (1000 examples). Data Augmentation for Low-resource MT: The limited availability of parallel data leads to a wide range of data augmentation approaches (Zhang et al., 2019; Gao et al., 2019; Cotterell and Kreutzer, 2018). Traditional approaches include perturbation at the word level, such as word dropout (Sennrich et al., 2016a), word replacement (Wang et al., 2018) and soft-decoupling (SDE; Wang et al. (2019)) to improve the cross-lingual transfer for LRLs. Such perturbation acts as a regularizer and enhances robustness to spelling variations; however, their impact is limited (Aepli and Sennrich, 2022). In a different research direction, noise injection-based modeling (Sperber et al., 2017; Karpukhin et al., 2019) has been explored to test the robustness of MT systems. More recently, lexical match-based models have been explored to improve the cross-lingual transfer by vocabulary overlapping (Patil et al., 2022), non-deterministic segmentation (Provilkov et al., 2020) and noise injection (Aepli and Sennrich, 2022; Blaschke et al., 2023). In contrast to these methods, we propose a linguistically inspired systematic noise injection approach for ELRLs."
        },
        {
            "heading": "7 Conclusion",
            "text": "This study presents an effective unsupervised approach, SELECTNOISE, for cross-lingual transfer from HRLs to closely related ELRLs through systematic character noise injection. The approach involves extracting selective noise injection candidates using BPE merge operations and edit operations. Furthermore, different sampling techniques are explored during the noise injection to ensure diverse candidate sampling. The model required only a small (1K example) monolingual data in ELRLs. The proposed model consistently outperformed strong baselines across 12 ELRLs from two diverse language families in the ELRLs-to-English MT task. The cumulative gain is 11.3% (chrF) over vanilla NMT. Furthermore, the model demonstrated comparative performance to a supervised noise injection model. In the future, we will extend SELECTNOISE to English-to-ELRL MT task, as well as other NLG tasks and languages.\nLimitations\nThe present study is based on the assumption of closely related high-resource languages and extremely low-resource languages to facilitate improved cross-lingual transfer. However, the proposed method may not be effective for ELRLs that have a different script and lack a script conversion or transliteration tool. Additionally, the model\u2019s performance may be suboptimal for languages that share the same script but exhibit significant lexical differences, such as Hindi and Bodo. Furthermore, this study focuses on ELRLs to English translation, and it remains to be explored whether the noise injection approach is beneficial for the English to ELRLs translation task."
        },
        {
            "heading": "Acknowledgements",
            "text": "We extend our heartfelt gratitude to all human evaluators whose meticulous assessment of the translation performance of the proposed model and various baselines played a pivotal role in the success of this research. Furthermore, we wish to express our deep appreciation to the anonymous reviewers and the esteemed meta-reviewer for their invaluable and insightful suggestions, which greatly improve the quality of the manuscript."
        },
        {
            "heading": "B Performance for HRLs",
            "text": "Table 8 analyzes the performance of the proposed model for HRLs across both language families. It demonstrates comparable performance with the vanilla NMT model for HRLs while boosting the performance of ELRLs. This highlights the effectiveness of the proposed model in handling both HRLs and ELRLs."
        },
        {
            "heading": "C HRL-ELRL lexical similarity measurement",
            "text": "Figure 7 shows the lexical similarity between HRL and related ELRLs for both language families. Lexical similarity is obtained using the longest common subsequence (LCS; Melamed (1995))."
        },
        {
            "heading": "D Datasets",
            "text": "Detailed statistics of datasets used in our experiments are shown in Table 9. For performing analysis on less-related language, we use the general test set of IndicTrans2 (AI4Bharat et al., 2023) for Bodo and FLORES200 test set for Tamil. For\nOBPE baseline, we use a dev set of FLORES-200 consisting of 997 as a monolingual corpus for learning the overlap vocabulary.\nE Implementation Details\nOur vanilla NMT model is based on standard transformer architecture consisting of 6 encoder and decoder layers. We trained our model for a maximum epoch of 15. We use Adam (Kingma and Ba, 2015) optimizer with \u03b21 = 0.9 and \u03b22 = 0.98. We set a learning rate of 0.0005. We use a dropout of 0.2. We performed data normalization and preprocessing using IndicNLP library4. We perform our experiments using fairseq5 library. For evaluation we use the lexical match-based BLEU metric6 (Pa-\n4https://github.com/anoopkunchukuttan/indic_ nlp_library\n5https://github.com/pytorch/fairseq 6nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1\npineni et al., 2002), chrF7 (Popovic\u0301, 2015) metric, semantic-based BLEURT8 (Sellam et al., 2020), and COMET9 (Pu et al., 2021) metrics."
        },
        {
            "heading": "F Sample Translations",
            "text": "Fig. 8 presents sample translations from Random Character Noise, SELECTNOISE and Supervised Character Noise injection models.\n7nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.3.1 8Reported using BLEURT20 checkpoint 9Reported using wmt22-comet-da model"
        }
    ],
    "title": "SELECTNOISE: Unsupervised Noise Injection to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages",
    "year": 2023
}