{
    "abstractText": "Current language models are mainly trained on snap-shots of data gathered at a particular time, which decreases their capability to generalize over time and model language change. To model the time variable, existing works have explored temporal language models (e.g., TempoBERT) by directly incorporating the timestamp into the training process. While effective to some extent, these methods are limited by the superficial temporal information brought by timestamps, which fails to learn the inherent changes of linguistic components. In this paper, we empirically confirm that the performance of pre-trained language models (PLMs) is closely affiliated with syntactically changed tokens. Based on this observation, we propose a simple yet effective method named Syntax-Guided Temporal Language Model (SG-TLM), which could learn the inherent language changes by capturing an intrinsic relationship between the time prefix and the tokens with salient syntactic change. Experiments on two datasets and three tasks demonstrate that our model outperforms existing temporal language models in both memorization and generalization capabilities. Extensive results further confirm the effectiveness of our approach across different model frameworks, including both encoder-only and decoder-only models (e.g., LLaMA). Our code is available at https:// github.com/zhaochen0110/TempoLM.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhaochen Su"
        },
        {
            "affiliations": [],
            "name": "Juntao Li"
        },
        {
            "affiliations": [],
            "name": "Zikang Zhang"
        },
        {
            "affiliations": [],
            "name": "Zihan Zhou"
        },
        {
            "affiliations": [],
            "name": "Min Zhang"
        },
        {
            "affiliations": [],
            "name": "Amba Hombaiah"
        }
    ],
    "id": "SP:8c2c661c53e3a8f7bb2b5b429b83e61b29b7ada1",
    "references": [
        {
            "authors": [
                "Oshin Agarwal",
                "Ani Nenkova."
            ],
            "title": "Temporal effects on pre-trained models for language processing tasks",
            "venue": "Transactions of the Association for Computational Linguistics, 10:904\u2013921.",
            "year": 2022
        },
        {
            "authors": [
                "Spurthi Amba Hombaiah",
                "Tao Chen",
                "Mingyang Zhang",
                "Michael Bendersky",
                "Marc Najork."
            ],
            "title": "Dynamic language models for continuously evolving content",
            "venue": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,",
            "year": 2021
        },
        {
            "authors": [
                "Pierpaolo Basile",
                "Barbara McGillivray."
            ],
            "title": "Exploiting the web for semantic change detection",
            "venue": "International Conference on Discovery Science, pages 194\u2013208. Springer.",
            "year": 2018
        },
        {
            "authors": [
                "Joan Bybee."
            ],
            "title": "Mechanisms of change in grammaticization: The role of frequency",
            "venue": "The handbook of historical linguistics, pages 602\u2013623.",
            "year": 2017
        },
        {
            "authors": [
                "Ricardo Campos",
                "V\u00edtor Mangaravite",
                "Arian Pasquali",
                "Al\u00edpio M\u00e1rio Jorge",
                "C\u00e9lia Nunes",
                "Adam Jatowt."
            ],
            "title": "Yake! collection-independent automatic keyword extractor",
            "venue": "European Conference on Information Retrieval, pages 806\u2013810. Springer.",
            "year": 2018
        },
        {
            "authors": [
                "Shuyang Cao",
                "Lu Wang."
            ],
            "title": "Time-aware prompting for text generation",
            "venue": "arXiv preprint arXiv:2211.02162.",
            "year": 2022
        },
        {
            "authors": [
                "Chi Seng Cheang",
                "Hou Pong Chan",
                "Derek F Wong",
                "Xuebo Liu",
                "Zhaocong Li",
                "Yanming Sun",
                "Shudong Liu",
                "Lidia S Chao."
            ],
            "title": "Temposum: Evaluating the temporal generalization of abstractive summarization",
            "venue": "arXiv preprint arXiv:2305.01951.",
            "year": 2023
        },
        {
            "authors": [
                "Gabriella Chronis",
                "Katrin Erk."
            ],
            "title": "When is a bishop not like a rook? when it\u2019s like a rabbi! multiprototype bert embeddings for estimating semantic relationships",
            "venue": "Proceedings of the 24th Conference on Computational Natural Language Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-",
            "year": 2019
        },
        {
            "authors": [
                "Bhuwan Dhingra",
                "Jeremy R Cole",
                "Julian Martin Eisenschlos",
                "Daniel Gillick",
                "Jacob Eisenstein",
                "William W Cohen."
            ],
            "title": "Time-aware language models as temporal knowledge bases",
            "venue": "Transactions of the Association for Computational Linguistics, 10:257\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Zhifang Sui."
            ],
            "title": "A survey for in-context learning",
            "venue": "arXiv preprint arXiv:2301.00234.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Eisenstein",
                "Brendan O\u2019Connor",
                "Noah A Smith",
                "Eric P Xing"
            ],
            "title": "Diffusion of lexical change in social media",
            "venue": "PloS one,",
            "year": 2014
        },
        {
            "authors": [
                "Aina Gar\u00ed Soler",
                "Marianna Apidianaki."
            ],
            "title": "Let\u2019s play mono-poly: Bert can reveal words\u2019 polysemy level and partitionability into senses",
            "venue": "Transactions of the Association for Computational Linguistics, 9:825\u2013 844.",
            "year": 2021
        },
        {
            "authors": [
                "Mario Giulianelli",
                "Marco Del Tredici",
                "Raquel Fern\u00e1ndez"
            ],
            "title": "Analysing lexical semantic change",
            "year": 2020
        },
        {
            "authors": [
                "Mario Giulianelli",
                "Andrey Kutuzov",
                "Lidia Pivovarova."
            ],
            "title": "Do not fire the linguist: Grammatical profiles help language models detect semantic change",
            "venue": "Proceedings of the 3rd Workshop on Computational Approaches to Historical Language Change,",
            "year": 2022
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Ana Marasovi\u0107",
                "Swabha Swayamdipta",
                "Kyle Lo",
                "Iz Beltagy",
                "Doug Downey",
                "Noah A Smith."
            ],
            "title": "Don\u2019t stop pretraining: Adapt language models to domains and tasks",
            "venue": "Proceedings of the 58th Annual Meeting of the",
            "year": 2020
        },
        {
            "authors": [
                "William L Hamilton",
                "Jure Leskovec",
                "Dan Jurafsky."
            ],
            "title": "Diachronic word embeddings reveal statistical laws of semantic change",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1489\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Mary Hare",
                "Jeffrey L Elman."
            ],
            "title": "Learning and morphological change",
            "venue": "Cognition, 56(1):61\u201398.",
            "year": 1995
        },
        {
            "authors": [
                "Valentin Hofmann",
                "Janet Pierrehumbert",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Dynamic contextualized word embeddings",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Minlie Huang",
                "Borui Ye",
                "Yichen Wang",
                "Haiqiang Chen",
                "Junjun Cheng",
                "Xiaoyan Zhu."
            ],
            "title": "New word detection for sentiment analysis",
            "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2014
        },
        {
            "authors": [
                "Xiaolei Huang",
                "Michael Paul."
            ],
            "title": "Neural temporality adaptation for document classification: Diachronic word embeddings and domain adaptation models",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Xiaolei Huang",
                "Michael J Paul."
            ],
            "title": "Examining temporality in document classification",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2.",
            "year": 2018
        },
        {
            "authors": [
                "Yatu Ji",
                "Hongxu Hou",
                "Chen Junjie",
                "Nier Wu."
            ],
            "title": "Improving mongolian-chinese neural machine translation with morphological noise",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,",
            "year": 2019
        },
        {
            "authors": [
                "Yoon Kim",
                "Yi-I Chiu",
                "Kentaro Hanaki",
                "Darshan Hegde",
                "Slav Petrov."
            ],
            "title": "Temporal analysis of language through neural language models",
            "venue": "ACL 2014, page 61.",
            "year": 2014
        },
        {
            "authors": [
                "Sinan Kurtyigit",
                "Maike Park",
                "Dominik Schlechtweg",
                "Jonas Kuhn",
                "Sabine Schulte im Walde."
            ],
            "title": "Lexical semantic change discovery",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Andrey Kutuzov",
                "Lidia Pivovarova",
                "Mario Giulianelli."
            ],
            "title": "Grammatical profiling for semantic change detection",
            "venue": "Proceedings of the 25th Conference on Computational Natural Language Learning, pages 423\u2013434.",
            "year": 2021
        },
        {
            "authors": [
                "William Labov."
            ],
            "title": "Principles of linguistic change, volume 3: Cognitive and cultural factors, volume 3",
            "venue": "John Wiley & Sons.",
            "year": 2011
        },
        {
            "authors": [
                "Severin Laicher",
                "Sinan Kurtyigit",
                "Dominik Schlechtweg",
                "Jonas Kuhn",
                "Sabine Schulte im Walde."
            ],
            "title": "Explaining and improving bert performance on lexical semantic change detection",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the",
            "year": 2021
        },
        {
            "authors": [
                "Angeliki Lazaridou",
                "Adhi Kuncoro",
                "Elena Gribovskaya",
                "Devang Agrawal",
                "Adam Liska",
                "Tayfun Terzi",
                "Mai Gimenez",
                "Cyprien de Masson d\u2019Autume",
                "Tomas Kocisky",
                "Sebastian Ruder"
            ],
            "title": "Mind the gap: Assessing temporal generalization in neural language",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Loureiro",
                "Francesco Barbieri",
                "Leonardo Neves",
                "Luis Espinosa Anke",
                "Jose Camacho-Collados."
            ],
            "title": "Timelms: Diachronic language models from twitter",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Kelvin Luu",
                "Daniel Khashabi",
                "Suchin Gururangan",
                "Karishma Mandyam",
                "Noah A. Smith."
            ],
            "title": "Time waits for no one! analysis and challenges of temporal misalignment",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the",
            "year": 2022
        },
        {
            "authors": [
                "Christopher D Manning",
                "Mihai Surdeanu",
                "John Bauer",
                "Jenny Rose Finkel",
                "Steven Bethard",
                "David McClosky."
            ],
            "title": "The stanford corenlp natural language processing toolkit",
            "venue": "Proceedings of 52nd annual meeting of the association for computational linguis-",
            "year": 2014
        },
        {
            "authors": [
                "Ryan McDonald",
                "Fernando Pereira",
                "Kiril Ribarov",
                "Jan Hajic."
            ],
            "title": "Non-projective dependency parsing using spanning tree algorithms",
            "venue": "Proceedings of human language technology conference and conference on empirical methods in natural language",
            "year": 2005
        },
        {
            "authors": [
                "William Merrill",
                "Gigi Stark",
                "Robert Frank."
            ],
            "title": "Detecting syntactic change using a neural part-ofspeech tagger",
            "venue": "Proceedings of the 1st International Workshop on Computational Approaches to Historical Language Change, pages 167\u2013174.",
            "year": 2019
        },
        {
            "authors": [
                "Stefano Montanelli",
                "Francesco Periti."
            ],
            "title": "A survey on contextualised semantic shift detection",
            "venue": "arXiv preprint arXiv:2304.01666.",
            "year": 2023
        },
        {
            "authors": [
                "Syrielle Montariol",
                "Matej Martinc",
                "Lidia Pivovarova."
            ],
            "title": "Scalable and interpretable semantic change detection",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Mohammad Taher Pilehvar",
                "Jose Camacho-Collados."
            ],
            "title": "Wic: the word-in-context dataset for evaluating context-sensitive meaning representations",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Wenjun Qiu",
                "Yang Xu."
            ],
            "title": "Histbert: A pre-trained language model for diachronic lexical semantic analysis",
            "venue": "arXiv preprint arXiv:2202.03612.",
            "year": 2022
        },
        {
            "authors": [
                "Xipeng Qiu",
                "Tianxiang Sun",
                "Yige Xu",
                "Yunfan Shao",
                "Ning Dai",
                "Xuanjing Huang."
            ],
            "title": "Pre-trained models for natural language processing: A survey",
            "venue": "Science China Technological Sciences, 63(10):1872\u2013 1897.",
            "year": 2020
        },
        {
            "authors": [
                "Shruti Rijhwani",
                "Daniel Preo\u0163iuc-Pietro."
            ],
            "title": "Temporally-informed analysis of named entity recognition",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7605\u20137617.",
            "year": 2020
        },
        {
            "authors": [
                "Guy D Rosin",
                "Ido Guy",
                "Kira Radinsky."
            ],
            "title": "Time masking for temporal language models",
            "venue": "Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, pages 833\u2013841.",
            "year": 2022
        },
        {
            "authors": [
                "Guy D Rosin",
                "Kira Radinsky."
            ],
            "title": "Temporal attention for language models",
            "venue": "arXiv preprint arXiv:2202.02093.",
            "year": 2022
        },
        {
            "authors": [
                "Paul R\u00f6ttger",
                "Janet Pierrehumbert."
            ],
            "title": "Temporal adaptation of bert and performance on downstream document classification: Insights from social media",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2400\u20132412.",
            "year": 2021
        },
        {
            "authors": [
                "Maja Rudolph",
                "David Blei."
            ],
            "title": "Dynamic embeddings for language evolution",
            "venue": "Proceedings of the 2018 world wide web conference, pages 1003\u20131011.",
            "year": 2018
        },
        {
            "authors": [
                "Violeta Seretan"
            ],
            "title": "Syntax-based collocation extraction",
            "year": 2011
        },
        {
            "authors": [
                "Anders S\u00f8gaard",
                "Sebastian Ebert",
                "Jasmijn Bastings",
                "Katja Filippova."
            ],
            "title": "We need to talk about random splits",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1823\u20131832.",
            "year": 2021
        },
        {
            "authors": [
                "Milan Straka",
                "Jana Strakov\u00e1"
            ],
            "title": "Tokenizing, pos tagging, lemmatizing and parsing ud 2.0 with udpipe",
            "venue": "In Proceedings of the CoNLL",
            "year": 2017
        },
        {
            "authors": [
                "Zhaochen Su",
                "Zecheng Tang",
                "Xinyan Guan",
                "Juntao Li",
                "Lijun Wu",
                "Min Zhang."
            ],
            "title": "Improving temporal generalization of pre-trained language models with lexical semantic change",
            "venue": "arXiv preprint arXiv:2210.17127.",
            "year": 2022
        },
        {
            "authors": [
                "Nina Tahmasebia",
                "Lars Borina",
                "Adam Jatowtb."
            ],
            "title": "Survey of computational approaches to lexical semantic change detection",
            "venue": "Computational approaches to semantic change, 6:1.",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Adam Tsakalidis",
                "Marya Bazzi",
                "Mihai Cucuringu",
                "Pierpaolo Basile",
                "Barbara McGillivray."
            ],
            "title": "Mining the uk web archive for semantic change detection",
            "venue": "Proceedings of the International Conference on Recent Advances in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Jindong Wang",
                "HU Xixu",
                "Wenxin Hou",
                "Hao Chen",
                "Runkai Zheng",
                "Yidong Wang",
                "Linyi Yang",
                "Wei Ye",
                "Haojun Huang",
                "Xiubo Geng"
            ],
            "title": "On the robustness of chatgpt: An adversarial and out-ofdistribution perspective",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "While Pre-trained Language Models (PLMs) have achieved remarkable success on most of NLP tasks (Qiu et al., 2020), they neglect the time variable due to their training on snap-shots of data collected at a particular point of time (Devlin et al., 2019; Liu et al., 2019). However, our language is constantly evolving and changing with new words being created (Huang et al., 2014; Rudolph and Blei, 2018; Amba Hombaiah\n\u2217Juntao Li is the Corresponding Author.\net al., 2021) and existing words changing in meanings and usage (Labov, 2011; Eisenstein et al., 2014; Giulianelli et al., 2020; Montanelli and Periti, 2023). This \u201cstatic\u201d training paradigm prevents PLMs from generalizing over time and modeling language change (Lazaridou et al., 2021; S\u00f8gaard et al., 2021; Loureiro et al., 2022).\nTo make the model training more \u201cdynamic\u201d, existing studies have explored temporal language models (TLMs), which model temporality by incorporating timestamp directly into representation when pre-training LMs, i.e., TempoBERT (Rosin et al., 2022), TempoT5 (Dhingra et al., 2022). Their methods involve prepending a special time token to each sequence in the training data. Through stacking multiple attention layers, each token at different positions can capture the temporal information brought by time. With different time prefixes, PLMs can adaptively compute the corresponding temporal representations. Moreover, TLMs have better generalization1 capability over time as standard LMs are unaware of which data is \u201cnew\u201d and which is \u201cold\u201d due to the absence of timestamps during the training process (Dhingra et al., 2022).\nThough the methods mentioned above can cap-\n1Temporal generalization (Lazaridou et al., 2021) refers to a model is trained on data before time T but is tested after T .\nture temporal information to a certain degree, they can merely incorporate \u201csuperficial\u201d temporal information provided by the time prefix with the Masked Language Model (MLM) objective. Thus, it is natural to leverage more temporal-specific information captured by textual tokens, e.g., utilizing a small set of lexicons with salient lexical semantic change (Hamilton et al., 2016; Giulianelli et al., 2020; Tahmasebia et al., 2021) in the very recent LMLM method (Su et al., 2022). In this paper, we launch a thorough study to explore the effects of different methods for lexicon selection based on statistical patterns or linguistic attributes and find that the distributional change of syntactic roles (Kutuzov et al., 2021) is the most effective strategy in temporal-specific lexicon selection. As shown in Figure 1, the discrete syntactic role distribution of the word \u201coval\u201d dramatically changes over time, while the word \u201cjustify\u201d is stable across years.\nBased on the above observations, we propose a Syntax-Guided Temporal Language Model (SGTLM), which consists of two masking strategies: a Syntax-Guided Masking (SGM) and a TemporalAware Masking (TAM) strategy. Experimental results demonstrate that our proposed method significantly improves performance over other TLMs methods on two datasets and three tasks. Extensive results further confirm the efficiency of our method than the state-of-the-art lexicon selection solution based on semantic change, remarkable transferability across various model frameworks, and its positive impact on adaption to future data.\nSummary of Contributions: (i) We explore the task of efficient syntax-guided lexicon selection, which is more challenging for static PLMs to predict on the time-stratified data. (ii) We propose a simple yet effective Syntax-Guided Temporal Language Model (SG-TLM). (iii) SG-TLM exhibits excellent performance than other TLMs in terms of memorization and generalization for downstream tasks. (iv) Our method demonstrates superior efficiency than the SOTA solution, exhibiting high transferability across different model frameworks and positive adaptive ability to future data."
        },
        {
            "heading": "2 Preliminary Study",
            "text": ""
        },
        {
            "heading": "2.1 Temporal Language Model",
            "text": "Previous works have explored temporal language models to enhance the capability of PLMs in modeling language change and generalizing over time.\nOne popular method is to prepend a timestamp in different forms to a textual sequence, e.g., \u201c<2015> Sydney defeats Paris by points at Oval.\u201d (Rosin et al., 2022), \u201cyear: 2015 text: Sydney defeat Paris by points at Oval.\u201d (Dhingra et al., 2022), and utilize MLM objective to capture the temporal information brought by the time prefix. Through interacting each time prefix with the correlated textual tokens equally, temporal information can be injected into the pre-trained representation, which ignores the diachronic change degree of different tokens, e.g., time-specific tokens and time-agnostic tokens2. Thus, it is natural to enhance existing temporal language models by \u201caccurately\u201d injecting time information into these time-specific tokens, i.e., the core of building better temporal language models is to select these tokens with the time attribute. Normally, it is difficult to directly compute or estimate the diachronic change degree or the time attribute of tokens. Existing works mainly leverage the discrepancy of data across different periods to approximate the diachronic change degree of tokens. For instance, Su et al. (2022) measures the statistical distance of token representation across years to select these tokens (i.e., a lexicon) with salient lexical semantic change. Though effective, existing semantic-based lexicon selection methods require forwarding all training data with a large-scale language model in the data process stage and neglect structure information within the language. To accelerate the lexicon selection process and leverage structure information within languages, we explore the potential of syntactic role changes of different tokens in this paper that may benefit from the speed superiority of various syntactic parsing tools. We elaborate on more details of syntax-based lexicon selection below."
        },
        {
            "heading": "2.2 Syntax-Guided Lexicon Selection",
            "text": "We construct \u201csyntax-guided lexicons\u201d based on diachronic differences in syntax. According to Su et al. (2022), we first adopt YAKE! (Campos et al., 2018), a feature-based and unsupervised system to extract the candidate keywords Wt = {wt1, wt2, \u00b7 \u00b7 \u00b7 , wtk} from the texts Dt of time t. Then, we utilize off-the-shelf Stanza3 (Manning et al., 2014) to automatically parse the syntax information for each sentence in the texts Dt and\n2Time-specific tokens refer to these with salient (larger than a threshold) lexical semantic change, while the left tokens treated as time-agnostic ones in Su et al. (2022).\n3https://github.com/stanfordnlp/stanza\ncount the frequency of syntactic roles for each candidate word wi and store them in the set Rt = (rt1, r t 2, \u00b7 \u00b7 \u00b7 , rti , \u00b7 \u00b7 \u00b7 , rtN ). This set is a collection of dictionaries, with each dictionary representing the syntactic roles and their frequencies, which is structured as follows: rti = {ktj : vtj} |rti | j=1, where k t j represents the syntactic role for word wi in time t, and vtj is its frequency. For example, if the candidate word \u201coval\u201d has the syntactic roles \u201camod\u201d and \u201cnmod\u201d in time t with frequencies 150 and 100, respectively, the corresponding dictionary in Rt would be rtoval = {amod : 150, nmod : 100}.\nUsing these syntactic dictionaries, we create feature vectors a\u20d7t and a\u20d7t\u2032 to represent the syntactic profiles of the candidate words in different periods. The size of the feature vectors a\u20d7t and a\u20d7t\u2032 may vary across words since we create separate feature lists for each word, including the corresponding syntactic roles. To align the vectors for each time, we pad the vectors with 0 for any missing syntactic roles. Finally, we calculate the cosine distance between a\u20d7t and a\u20d7t\u2032 to measure the difference between the syntactic profiles of the candidate words Wt. We use a hyper-parameter k to control the degree of syntactic change, ranking the candidate words Wt based on their cosine values and selecting the top-k words as the syntax-guided lexicon, which consists of the tokens with significantly changed syntactic roles across different periods. Our proposed lexicon selection method is much faster than those used in LMLM (Su et al., 2022), in which their computation cost will be discussed in Section 4.4."
        },
        {
            "heading": "2.3 Discussions and Observations",
            "text": "In Section 2.2, we propose a direct and efficient syntax-guided approach for obtaining the lexicons which have undergone significant syntactic change over time. Following Su et al. (2022), we mask the tokens in the selected lexicons and utilize perplexity (ppl.) as a qualitative measure to compare the influence of different lexicon selection methods on static PLMs. To complete this, we build a time-stratified corpus from publicly released crawl news4 datasets, which contains 1M English news articles for each year between 2014 and 2018. We post-tune the BERT5 model with the data from 20146 and evaluate the four testing sets after 2015.\n4https://data.statmt.org/news-crawl/ 5We initialize parameters from BERT-BASE-UNCASED. 6To eliminate the impact of domain divergence (Gururangan et al., 2020) on the performance of testing, we post-tune the uniform BERT with the news data in 2014.\nMethods for Comparison We introduce six approaches to lexicon construction: (i) We first adopt two methods for extracting lexicons: random selection and frequency-aware selection. (ii) Besides, we introduce two approaches to selecting the lexicons with salient linguistic changes: lexical semantic change, i.e., LMLM (Su et al., 2022) and syntactic role change, i.e., our proposed syntaxguided method (SMLM) introduced in Section 2.2. (iii) Additionally, we consider words that are dependent on the tokens with significant diachronic change, as identified from the syntactic parsing process7, i.e., the head node in the dependency parsing tree (McDonald et al., 2005), and propose two methods to include these words dependent on extracted lexicons based on LMLM and SMLM, named L-DMLM and S-DMLM, respectively.\nThe Influence of Lexicon Selection Methods The results are shown in Figure 2. We can see that: (i) In the absence of adding dependent words, the ppl. of SMLM is much higher than the other three lexicon selection methods, which indicates that it is more challenging for static PLMs to predict the lexicons selected from the syntactic perspective. (ii) After adding dependency information, both SMLM and LMLM methods show an apparent increase in their ppl. values, i.e., L-DMLM and S-DMLM, suggesting the positive impact of adding dependent words from syntactic parsing in lexicon selection. (iii) Above all, S-DMLM (marked with ) achieves the highest ppl. values among six methods, which can select diachronic change lexicons that impose the most significant challenge to static LMs.\n7This allows us to capture more nuanced and subtle temporal information present in the text, which is crucial for tasks that require temporal understanding and knowledge retention."
        },
        {
            "heading": "3 Syntax-Guided TLM",
            "text": "The Masked Language Model (MLM) objective (Devlin et al., 2019) is a widely-adopted selfsupervised training method that involves randomly masking a certain percentage of the tokens in a text sequence and training a model to predict the masked tokens based on their context. Previous TLMs add a timestamp token at the beginning of the input sequence and utilize the MLM objective to predict the random masked tokens based on the context and the timestamp. However, these methods disregard inherent temporal-specific information provided by lexicons with salient change tokens. Based on the aforementioned observation8, we propose Syntax-Guided Temporal Language Model (SG-TLM), which consists of two main components: a Syntax-Guided Masking (SGM) scheme and a Temporal-Aware Masking (TAM) method. Our proposed model is illustrated in Figure 3.\nSyntax-Guided Masking (SGM) We construct the syntax-guided9 lexicons based on the distributional change of syntactic role across timestamps. Formally, given the text set Dt = {dt1, dt2, \u00b7 \u00b7 \u00b7 , dtn} at time t, we first rank the lexicons according to the word\u2019s cosine values of syntactic change. Then, we select k (k \u2208 {100, 200, \u00b7 \u00b7 \u00b7 , 500}) words with relative high scores as the masking candidate words Wmaskt. Considering the effectiveness of adding dependent words, we treat the words that are dependent by the candidate words Wmaskt within\n8In Section 2.3, we discover that utilizing the distributional change of syntactic roles is the most effective strategy when selecting temporal-specific lexicons.\n9In Appendix C, we will investigate the influence of parsing toolkit\u2019s performance on our SG-TLM.\nthe sentence as additional temporal information. Specifically, given the masking ratio \u03b110, we prioritize masking the words in Wmaskt and their corresponding words in the dependency relationship11. We randomly mask the other tokens from the sequence if there are no sufficient masking candidates to meet the required number of masking tokens. Assuming it masks m tokens in total and the sequence after masking at time t is dt\n\u2032 i . The\noptimization objective of SGM can be written by:\nLSGM = \u2212 m\u2211 j=1 logP (x = wj |t, dt \u2032 i ; \u03b8). (1)\nTemporal-Aware Masking (TAM) Unlike previous work (Rosin et al., 2022), we predict masked tokens with salient syntactic role change and time tokens, given the remaining unmasked words within the sequence. Formally, given a sequence {di} at time t, we denote d(0)i for its timestamp and prepend the time token t to di. Now the inputs to the model are a sequence di = [t, d (1) i , d (2) i , d (3) i , \u00b7 \u00b7 \u00b7 , d (n) i ]. Assuming the set of masked token is {d(2)i }, we predict the time t by the whole input text: p(t) = p(t|d(1)i , <MASK>, d (3) i , \u00b7 \u00b7 \u00b7 , d (n) i ). As for the granularity of t, different values can be used according to the use case. In this work, we experiment with the granularity of a year for the WMT dataset, and a month for the RTC dataset.\n10In Appendix 4.4, we will analyze suitable values for the masking ratio a and word counts k.\n11In Section 4.2, we also explore training the model on the tokens with salient syntactic change, without taking the corresponding dependent words (STLM) into account, and the results are shown in Table 1 (marked with \u2020)."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Datasets We conduct continual pre-training on two datasets: WMT NEWS CRAWL (WMT) and REDDIT TIME CORPUS (RTC), respectively. The WMT12 dataset, an open-domain dataset, consists of 4 million news articles published between 2015 and 2018: {D2015,D2016,D2017,D2018}. On the other hand, the RTC (R\u00f6ttger and Pierrehumbert, 2021) is a monthly time-stratified dataset from March 2017 to February 2020. Unlike the WMT dataset, it specifically focuses on the political domain, enabling us to explore temporal dynamics within the specific domain. We select three months {D2017\u221204,D2018\u221208,D2019\u221208} for pre-training, each containing 1 million unlabeled comments.\nEvaluation Following the pre-training period, we evaluate the models\u2019 memorization (Dhingra et al., 2022) and generalization (Lazaridou et al., 2021) abilities by measuring their performance on downstream tasks13. To evaluate memorization, the model is tested on the same time steps as the pre-training data S1...T = {S1,S2, . . . ,ST }. To evaluate generalization, the model\u2019s performance is measured on future times (ST \u02dc ST+n), which is invisible during the post-tuning stage.\nAfter continual training on the WMT dataset, two tasks are used for model evaluation: political affiliation classification (POLIAFF) (Luu et al., 2022) and named entity recognition (TWINER) (Rijhwani and Preot\u0327iuc-Pietro, 2020). The POLIAFF task involves fine-tuning the model with 10,000 labeled sentences from 2015. Testing includes {S2015,S2016,S2017,S2018} for memorization and {S2019,S2020} for generalization, with each year containing 2,000 specific sentences. For the TWINER task, 2,000 labeled tweets from 2015 are selected for fine-tuning. We evaluate memorization abilities using datasets {S2016,S2017,S2018} and generalization capabilities using dataset {S2019}. Following pre-training on the RTC dataset, the model is evaluated on the political subreddit prediction (PSP) (R\u00f6ttger and Pierrehumbert, 2021) task. Specifically, the model is fine-tuned on 20,000 labeled data samples from\n12https://data.statmt.org/news-crawl/ 13Previous works (R\u00f6ttger and Pierrehumbert, 2021; Lazaridou et al., 2021) have discovered that the model\u2019s performance deteriorates as the gap between the training and testing time increases. Following their setting, we evaluate the model that was fine-tuned on the oldest time step.\nApril 2017, extracted from the same dataset used for pre-training. Memorization and generalization are tested on {S2017\u221204,S2018\u221208,S2019\u221208} and {S2020\u221201,S2020\u221202}, respectively. We calculate the F1-score as the testing results for all our experiments. More details about the task and our model\u2019s training are shown in Appendix A."
        },
        {
            "heading": "4.2 Baselines",
            "text": "We establish several baselines that encapsulate different continual pre-training strategies. Firstly, we consider two naive baselines that do not incorporate timestamps during the pre-training stage: (i) BERT (w/o) (Devlin et al., 2019), which is directly fine-tuned on the downstream task without pre-training. (ii) Uniform involves training the model with mixed pre-trained data. Additionally, we adopt three up-to-date TLMs, which utilize timestamps during pre-training: (iii) TAda (R\u00f6ttger and Pierrehumbert, 2021), which involves continual pre-training on specific time buckets to obtain separate models specialized for different periods. (iv) Temporal (Dhingra et al., 2022) integrates time t as a prefix to the input during pre-training, with temporal-specific lexicons randomly generated. (v) LMLM (Su et al., 2022) is the SOTA of temporal adaptation, which strengthens PLMs\u2019 generalization with salient lexical semantic change. We utilize their method of lexicon construction. STLM and SG-TLM are our methods, with the distinction of whether to add dependent words during the lexicons construction. Appendix B offers additional training details on the compared TLMs."
        },
        {
            "heading": "4.3 Main Results",
            "text": "Table 1 presents the results on the WMT dataset. TLMs demonstrate superior performance compared to models that do not consider timestamps. However, previous TLMs such as TAda and Temporal show only marginal improvements over the Uniform model, indicating limited learned temporality information from the timestamps. Conversely, incorporating linguistic information into TLM training significantly improves both memorization and generalization. Among the evaluated baselines, SG-TLM achieves the highest average F1-scores on both datasets, i.e., 66.77 and 66.67, highlighting the effectiveness of leveraging syntax and dependency information within languages. Table 2 presents the results for the RTC dataset. Similar to the previous findings, SG-TLM consistently achieves the best performance.\nHowever, the performance differences among the methods in the RTC dataset are relatively minor compared to the WMT dataset, which can be attributed to the shorter time intervals and the relatively stable and slight dynamic temporality of the RTC dataset. Moreover, SG-TLM also shows a notable drop in future years due to the inherent uncertainty of future language changes (Lazaridou et al., 2021) in both datasets. In Section 4.5, we will investigate the approaches to refreshing the models as new data arrives and compare the temporal adaptation (R\u00f6ttger and Pierrehumbert, 2021; Luu et al., 2022) performance of different methods."
        },
        {
            "heading": "4.4 Detailed Analysis",
            "text": "Efficiency Comparison Table 3 shows the comparison of lexicon construction efficiency between LMLM and SG-TLM. LMLM provides a fairly complex and time-consuming method to select semantic-based lexicons, i.e., 200 minutes for representing and 360 minutes for clustering. However, SG-TLM benefits from the speed superiority of syn-\ntactic parsing tools rather than large-scale PLMs, resulting in a speedup ratio of 5.5\u00d7 compared to representation and 180\u00d7 compared to measuring. Furthermore, SG-TLM outperforms LMLM on the TWINER task by achieving a 1.45 higher F1-score, demonstrating its superior effectiveness.\nAblation Study To investigate the impacts of different components within SG-TLM, we remove individual components from the complete model and observe the resulting performance. The results are shown in Table 4. Notably, excluding the SGM objective leads to the most significant decline in performance, highlighting its pivotal role within the SG-TLM framework. Furthermore, each component contributes positively to the overall performance, indicating the utility and significant contributions of all SG-TLM components in improving the model\u2019s effectiveness.\nScale Effects in Performance We also explore whether our proposed SG-TLM would keep effective over random masking when increasing the amount of data. Using the WMT dataset, we successively expand the training data for both models,\nmultiplying the volume by factors of 2, 3, 4, and 5. The results of this scaling experiment are summarized in Table 5. It is clear that the SG-TLM consistently outperforms the uniform masking approach across all years and data scales, demonstrating the robustness of our approach with increasing data.\nHyper-Parameter Analysis Considering the correlation between the masking ratio and the model\u2019s performance, we conduct experiments to explore the most suitable masking ratio a and word counts k for the SG-TLM objective. The results are shown in Figure 5. Our SG-TLM achieves the best performance when the masking ratio a is set to 30% and the number of candidate words k is 200. To better understand the insights of our presented SG-TLM, we conduct token-level analysis on the selected lexicons in Appendix D."
        },
        {
            "heading": "4.5 Temporal Adaptation to New Data",
            "text": "Unlike domain adaptation, temporal adaptation (R\u00f6ttger and Pierrehumbert, 2021) updates models with current data to mitigate temporal mis-\nalignment14. In this section, we consider the scenario where we already have a trained model on the 2015-18 slices and new data from the 2019 slice. We attempt to update the model by continuing pretraining TLMs on the unlabelled target year data. To compare the adaptability of different TLMs on target year, we evaluate their performance on the TWINER dataset. Precisely, we fine-tune the adaptation models on the labeled source year data15 and then test models on 2019 data. Experiments are conducted adapting three TLMs with three lexicon construction methods, totaling nine settings. Results are shown in Figure 4."
        },
        {
            "heading": "4.6 Transferability Across PLM Frameworks",
            "text": "To verify the transferability of our methods across different model frameworks, we implement our method in both encoder-only and decoder-only models and utilize random lexicon construction as the baseline for comparison.\n14In Appedix E, we will provide further analysis on SGTLM\u2019s adaptability to temporal changes from the perspective of token prediction.\n15In previous experiments, we use 2015 as the source year.\nEffectiveness on Encoder-only PLMs We implement our method on two popular encoder-only PLMs, i.e., BERT and RoBERTa. As shown in Table 6, we observe a significant improvement in each PLM using our SG-TLM. On average, the BERT model improves by 0.77 points, and the RoBERTa model improves by 1.31 points. These findings illustrate the versatility of our proposed SG-TLM, as it enhances the performance of various PLMs by incorporating syntax information.\nEffectiveness on Decoder-only PLM We also conduct experiments on large-scale decoder-only language models, i.e., LLaMA-7B (Touvron et al., 2023). In these experiments, we extract the word selection component from our method and compare two data selection methods: one based on our Syntax-Guided (SyG.) approach and the other on random selection. The perplexity of LLaMA-7b is evaluated on the 2,000 sentences selected using these two methods. As shown in Table 7, SG-TLM yields higher perplexity than random selection in the RTC datasets16. This highlights the complexity and diversity of our selected data, indicating the effectiveness of incorporating syntax into data selection and the potential to enhance temporal capture in large language models.\n16Since the training data for LLaMA is cut off at 2022, we crawl the latest Reddit data from https://files. pushshift.io to complete the experiment."
        },
        {
            "heading": "5 Related work",
            "text": "Temporal Language Model Several works have explored the temporal effects in language models (Huang and Paul, 2018, 2019; Rijhwani and Preot\u0327iuc-Pietro, 2020; Lazaridou et al., 2021; S\u00f8gaard et al., 2021; Agarwal and Nenkova, 2022; Loureiro et al., 2022; Cao and Wang, 2022; Cheang et al., 2023). Recently, existing works have investigated the temporal language model to model temporality information and generalize over time. Dhingra et al. (2022) and Rosin et al. (2022) directly prefix the time token to text sequences and fine-tune on time-stratified data. Hofmann et al. (2021) and Rosin and Radinsky (2022) modify the structure of the language model to create time-specific contextualized word representations. Su et al. (2022) recently proposed a semantic-based, lexical masking strategy to enhance PLMs\u2019 temporal generalization. We extend this work with a detailed study using syntactic role changes to harness temporal-specific information efficiently.\nDiachronic Language Change Diachronic language change can be mainly divided into semantic change (Kurtyigit et al., 2021; Montanelli and Periti, 2023), morphological change (Hare and Elman, 1995; Ji et al., 2019; Giulianelli et al., 2022), and syntactic change (Kroch, 2001; Seretan, 2011; Bybee, 2017; Merrill et al., 2019). Previous work focused on discovering the words that have undergone diachronic change under the supervised\nsettings (Kim et al., 2014; Basile et al.; Basile and McGillivray, 2018; Tsakalidis et al., 2019; Kurtyigit et al., 2021). Recently, several works have demonstrated that contextualized word representations have dynamic representation capabilities (Pilehvar and Camacho-Collados, 2019; Chronis and Erk, 2020; Gar\u00ed Soler and Apidianaki, 2021; Laicher et al., 2021; Qiu and Xu, 2022), which are adopted with unsupervised methods to represent, cluster, and differentiate words across different time periods (Giulianelli et al., 2020; Montariol et al., 2021). Our method utilizes a syntax-based method to detect the salient change words within the text sequence, making the process more interpretable (Merrill et al., 2019; Ryzhova et al.; Kutuzov et al., 2021). To our best knowledge, this is the first work that incorporates syntactic knowledge into the training of temporal language models."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we enhance the temporal language model from the syntactic perspective and discover that predicting the syntax-guided lexicons is more challenging for static PLMs compared to other methods. Building upon these findings, we propose a syntax-guided temporal language model (SG-TLM) that incorporates time information into tokens with significant syntactic changes. Our SGTLM achieves the SOTA performance, reduces computational costs during lexicon construction, and demonstrates excellent transferability to new data and frameworks compared to other baselines."
        },
        {
            "heading": "7 Limitation",
            "text": "There are still some limitations in our work which are listed below:\n\u2022 While we introduce a data selection strategy that incorporates syntactic changes to identify time-specific sentences, we only conduct preliminary validation of our method\u2019s transferability on Large Language Models (LLMs), without involving training and inference stages. Recent studies highlight that LLMs continue to struggle with effective generalization when it comes to emerging data (Wang et al., 2023). As a potential solution, our future work aims to integrate our method with in-context learning (Dong et al., 2022) to enhance the temporal generalization capabilities of LLMs. \u2022 Recent studies (Kutuzov et al., 2021; Giulianelli et al., 2022) show the effectiveness of utilizing\nsyntactic features in detecting lexical semantic changes. This prompts us to investigate the compatibility of our method with the semantic lexicon solution. Given the lexicon constructed by semantic change Wl and syntactic change Ws, we conduct three straightforward methods to combine the lexicons between Wl and Ws: Wl \u2229Ws, Wl\u222aWs, Wl \\Ws. Consistent with the previous experiment, the masking ratio a is 30%, and k is 200. The results are shown in Table 8. Contrary to our expectations, the combined model\u2019s performance is inferior as compared to the original model, suggesting that this method fails to merge information effectively from multiple dimensions. In future work, we will explore more suitable methods to integrate semantic and syntactic information."
        },
        {
            "heading": "Acknowledgement",
            "text": "We would like to thank the anonymous reviewers for their constructive comments. This work was supported by the National Science Foundation of China (NSFC No. 62206194) and the Natural Science Foundation of Jiangsu Province, China (Grant No. BK20220488)."
        },
        {
            "heading": "C Parsing Toolkit Analysis",
            "text": "Since there is a strong correlation between the parsing toolkit\u2019s capability and the performance of our SG-TLM, we compare the selected Stanza with another commonly adopted parsing tool, i.e., UDPipe19. As shown in Table 9, the results of utilizing Stanza as the parsing method outperforms UDPipe in all the timestamps, i.e., the average accuracy of Stanza is 68.27, while UDPipe is 64.78, which indicates that utilizing a more accurate parsing tool can significantly improve the model\u2019s performance. Though the UDPipe does not depend on GPU resources, this toolkit is unsuitable for parsing syntactic roles in the selection process of lexicons.\n19UDPipe (Straka and Strakov\u00e1, 2017) utilizes fast transition-based neural dependency parser that follows the same annotation schemes as Stanza."
        },
        {
            "heading": "2015 1,073 19 0 18,690 0 74,074 3,654",
            "text": ""
        },
        {
            "heading": "2016 1,715 0 0 26,718 12 68,690 6,887",
            "text": ""
        },
        {
            "heading": "2017 817 11 0 41,173 0 70,919 4,651",
            "text": ""
        },
        {
            "heading": "2018 673 493 1 25,688 0 56,576 4,171",
            "text": ""
        },
        {
            "heading": "D Token-level Analysis",
            "text": "From Figure 5, it is surprising that there is no positive correlation between the word counts and the model\u2019s performance. To understand the reason behind this phenomenon, we select the top 500 words from the candidates Wtmask according to the cosine value. The distribution of those words is shown in Table 10, which indicates that only about 20% words have relatively significant syntactic change (cosine value \u2265 0.01). This suggests that performance mainly comes from correctly predicting a small number of keywords, such as topic words and newly emerging words, which have relatively salient syntactic change. Furthermore, we also show the distribution of these lexicons by Part of Speech (POS) across various years in Table 11. From the data, it\u2019s evident that the distribution of POS remains relatively consistent year-over-year. Nouns dominate the distribution, implying their higher propensity for syntactic variation."
        },
        {
            "heading": "E Superiority in Adapting Temporal Change",
            "text": "This section aims to demonstrate the superior performance of the SG-TLM model in adapting to temporal changes compared to other Temporal Language Models (TLMs). Specifically, we compare the SG-TLM model against established baselines, i.e., Uniform, Temporal, and LMLM, as previously introduced in Section 4.2. All models are further pre-trained on BERT using 4 million data instances spanning from 2015 to 2018 in the WMT dataset and evaluated to predict masked tokens at different timestamps using 2,000 data samples from the\nsame source spanning six years (2015-2020). This experimental setting has also been used to verify the ability to adapt to temporal changes in R\u00f6ttger and Pierrehumbert (2021). The results of this comparison are presented in Figure 6. We can observe that the SG-TLM consistently outperforms the other models across all examined years, achieving the lowest perplexity scores, which illustrates the superior adaptability of the SG-TLM model to temporal shifts. Notably, the SG-TLM model also exhibits superior generalization capability with respect to the 2019 and 2020 data that were not present during the training period, outperforming other baseline models, which further demonstrates its robustness and reliability in handling temporal shifts."
        }
    ],
    "title": "Efficient Continue Training of Temporal Language Model with Structural Information",
    "year": 2023
}