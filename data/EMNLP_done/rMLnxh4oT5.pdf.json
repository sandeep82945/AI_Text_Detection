{
    "abstractText": "LLMs have demonstrated impressive zero-shot performance on NLP tasks thanks to the knowledge they acquired in their training. In multiplechoice QA tasks, the LM probabilities are used as an imperfect measure of the plausibility of each answer choice. One of the major limitations of the basic score is that it treats all words as equally important. We propose CASE, a Commonsense-Augmented Score with an Expanded Answer Space. CASE addresses this limitation by assigning importance weights for individual words based on their semantic relations to other words in the input. The dynamic weighting approach outperforms basic LM scores, not only because it reduces noise from unimportant words, but also because it informs the model of implicit commonsense knowledge that may be useful for answering the question. We then also follow prior work in expanding the answer space by generating lexically-divergent answers that are conceptually-similar to the choices. When combined with answer space expansion, our method outperforms strong baselines on 5 commonsense benchmarks. We further show these two approaches are complementary and may be especially beneficial when using smaller LMs.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wenkai Chen"
        },
        {
            "affiliations": [],
            "name": "Vered Shwartz"
        }
    ],
    "id": "SP:fed7e5aa1e205974086438e3e18e4c61263d454c",
    "references": [
        {
            "authors": [
                "Mostafa Abdou",
                "Vinit Ravishankar",
                "Maria Barrett",
                "Yonatan Belinkov",
                "Desmond Elliott",
                "Anders S\u00f8gaard."
            ],
            "title": "The sensitivity of language models and humans to Winograd schema perturbations",
            "venue": "Proceedings of the 58th Annual Meeting of the Asso-",
            "year": 2020
        },
        {
            "authors": [
                "Forough Arabshahi",
                "Jennifer Lee",
                "Antoine Bosselut",
                "Yejin Choi",
                "Tom Mitchell."
            ],
            "title": "Conversational multi-hop reasoning with neural commonsense knowledge and symbolic logic rules",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Nat-",
            "year": 2021
        },
        {
            "authors": [
                "Lisa Bauer",
                "Yicheng Wang",
                "Mohit Bansal."
            ],
            "title": "Commonsense for generative multi-hop question answering tasks",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4220\u20134230, Brussels, Belgium.",
            "year": 2018
        },
        {
            "authors": [
                "Antoine Bosselut",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "Dynamic neuro-symbolic knowledge graph construction for zero-shot commonsense question answering",
            "venue": "Proceedings of the AAAI conference on Artificial Intelligence, volume 35, pages 4923\u20134931.",
            "year": 2021
        },
        {
            "authors": [
                "Antoine Bosselut",
                "Hannah Rashkin",
                "Maarten Sap",
                "Chaitanya Malaviya",
                "Asli Celikyilmaz",
                "Yejin Choi."
            ],
            "title": "COMET: Commonsense transformers for automatic knowledge graph construction",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "2020b. Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Ricardo Campos",
                "V\u00edtor Mangaravite",
                "Arian Pasquali",
                "Al\u00edpio M\u00e1rio Jorge",
                "C\u00e9lia Nunes",
                "Adam Jatowt."
            ],
            "title": "Yake! collection-independent automatic keyword extractor",
            "venue": "Advances in Information Retrieval: 40th European Conference on IR Research, ECIR",
            "year": 2018
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Yejin Choi",
                "Vered Shwartz."
            ],
            "title": "It\u2019s not rocket science: Interpreting figurative language in narratives",
            "venue": "Transactions of the Association for Computational Linguistics, 10:589\u2013606.",
            "year": 2022
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Debanjan Ghosh",
                "Smaranda Muresan",
                "Nanyun Peng."
            ],
            "title": "R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Qianglong Chen",
                "Feng Ji",
                "Haiqing Chen",
                "Yin Zhang."
            ],
            "title": "Improving commonsense question answering by graph-based iterative retrieval over multiple knowledge sources",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova."
            ],
            "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Peter Clark",
                "Isaac Cowhey",
                "Oren Etzioni",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Carissa Schoenick",
                "Oyvind Tafjord."
            ],
            "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
            "venue": "arXiv preprint arXiv:1803.05457.",
            "year": 2018
        },
        {
            "authors": [
                "Antonia Creswell",
                "Murray Shanahan",
                "Irina Higgins."
            ],
            "title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "venue": "arXiv preprint arXiv:2205.09712.",
            "year": 2022
        },
        {
            "authors": [
                "Joe Davison",
                "Joshua Feldman",
                "Alexander Rush."
            ],
            "title": "Commonsense knowledge mining from pretrained models",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference",
            "year": 2019
        },
        {
            "authors": [
                "Yuwei Fang",
                "Shuohang Wang",
                "Yichong Xu",
                "Ruochen Xu",
                "Siqi Sun",
                "Chenguang Zhu",
                "Michael Zeng."
            ],
            "title": "Leveraging knowledge in multilingual commonsense reasoning",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Gordon",
                "Zornitsa Kozareva",
                "Melissa Roemmele."
            ],
            "title": "SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
            "venue": "*SEM 2012: The First Joint Conference on Lexical and Computational Seman-",
            "year": 2012
        },
        {
            "authors": [
                "Jian Guan",
                "Yansen Wang",
                "Minlie Huang."
            ],
            "title": "Story ending generation with incremental encoding and commonsense knowledge",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):6473\u20136480.",
            "year": 2019
        },
        {
            "authors": [
                "Ari Holtzman",
                "Peter West",
                "Vered Shwartz",
                "Yejin Choi",
                "Luke Zettlemoyer"
            ],
            "title": "Surface form competition: Why the highest probability answer isn\u2019t",
            "year": 2021
        },
        {
            "authors": [
                "Canming Huang",
                "Weinan He",
                "Yongmei Liu."
            ],
            "title": "Improving unsupervised commonsense reasoning using knowledge-enabled natural language inference",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4875\u20134885, Punta",
            "year": 2021
        },
        {
            "authors": [
                "Jena D. Hwang",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Jeff Da",
                "Keisuke Sakaguchi",
                "Antoine Bosselut",
                "Yejin Choi."
            ],
            "title": "Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs",
            "venue": "AAAI.",
            "year": 2021
        },
        {
            "authors": [
                "Tushar Khot",
                "Peter Clark",
                "Michal Guerquin",
                "Peter Jansen",
                "Ashish Sabharwal."
            ],
            "title": "Qasc: A dataset for question answering via sentence composition",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8082\u20138090.",
            "year": 2020
        },
        {
            "authors": [
                "Seungone Kim",
                "Se June Joo",
                "Hyungjoo Chae",
                "Chaehyeong Kim",
                "Seung-won Hwang",
                "Jinyoung Yeo."
            ],
            "title": "Mind the gap! injecting commonsense knowledge for abstractive dialogue summarization",
            "venue": "Proceedings of the 29th International Conference",
            "year": 2022
        },
        {
            "authors": [
                "Tassilo Klein",
                "Moin Nabi."
            ],
            "title": "Towards zeroshot commonsense reasoning with self-supervised refinement of language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8737\u20138743, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Xinyue Chen",
                "Jamin Chen",
                "Xiang Ren."
            ],
            "title": "KagNet: Knowledge-aware graph networks for commonsense reasoning",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Jiacheng Liu",
                "Alisa Liu",
                "Ximing Lu",
                "Sean Welleck",
                "Peter West",
                "Ronan Le Bras",
                "Yejin Choi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Generated knowledge prompting for commonsense reasoning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Bodhisattwa Prasad Majumder",
                "Harsh Jhamtani",
                "Taylor Berg-Kirkpatrick",
                "Julian McAuley."
            ],
            "title": "Like hiking? you probably enjoy nature: Personagrounded dialog with commonsense expansions",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Huanru Henry Mao",
                "Bodhisattwa Prasad Majumder",
                "Julian McAuley",
                "Garrison Cottrell."
            ],
            "title": "Improving neural story generation by targeted common sense grounding",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Ninareh Mehrabi",
                "Pei Zhou",
                "Fred Morstatter",
                "Jay Pujara",
                "Xiang Ren",
                "Aram Galstyan."
            ],
            "title": "Lawyers are dishonest? quantifying representational harms in commonsense knowledge resources",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Nat-",
            "year": 2021
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Peter Clark",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Nasrin Mostafazadeh",
                "Nathanael Chambers",
                "Xiaodong He",
                "Devi Parikh",
                "Dhruv Batra",
                "Lucy Vanderwende",
                "Pushmeet Kohli",
                "James Allen."
            ],
            "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
            "venue": "Proceedings of the 2016",
            "year": 2016
        },
        {
            "authors": [
                "Yilin Niu",
                "Fei Huang",
                "Jiaming Liang",
                "Wenkai Chen",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "A semanticbased method for unsupervised commonsense question answering",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Sahithya Ravi",
                "Aditya Chinchure",
                "Leonid Sigal",
                "Renjie Liao",
                "Vered Shwartz."
            ],
            "title": "Vlc-bert: Visual question answering with contextualized commonsense knowledge",
            "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vi-",
            "year": 2023
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Melissa Roemmele",
                "Cosmin Adrian Bejan",
                "Andrew S Gordon."
            ],
            "title": "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
            "venue": "AAAI spring symposium: logical formalizations of commonsense reasoning, pages 90\u201395.",
            "year": 2011
        },
        {
            "authors": [
                "Amrita Saha",
                "Shafiq Joty",
                "Steven C.H. Hoi."
            ],
            "title": "Weakly supervised neuro-symbolic module networks for numerical reasoning over text",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 36(10):11238\u201311247.",
            "year": 2022
        },
        {
            "authors": [
                "Maarten Sap",
                "Ronan Le Bras",
                "Emily Allaway",
                "Chandra Bhagavatula",
                "Nicholas Lourie",
                "Hannah Rashkin",
                "Brendan Roof",
                "Noah A Smith",
                "Yejin Choi."
            ],
            "title": "Atomic: An atlas of machine commonsense for ifthen reasoning",
            "venue": "Proceedings of the AAAI con-",
            "year": 2019
        },
        {
            "authors": [
                "Maarten Sap",
                "Hannah Rashkin",
                "Derek Chen",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "Social IQa: Commonsense reasoning about social interactions",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Vered Shwartz",
                "Peter West",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi."
            ],
            "title": "Unsupervised commonsense question answering with self-talk",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Robyn Speer",
                "Joshua Chin",
                "Catherine Havasi"
            ],
            "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Alon Talmor",
                "Jonathan Herzig",
                "Nicholas Lourie",
                "Jonathan Berant."
            ],
            "title": "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Alexandre Tamborrino",
                "Nicola Pellican\u00f2",
                "Baptiste Pannier",
                "Pascal Voitot",
                "Louise Naudin."
            ],
            "title": "Pretraining is (almost) all you need: An application to commonsense reasoning",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Trieu H. Trinh",
                "Quoc V. Le"
            ],
            "title": "A simple method for commonsense reasoning",
            "year": 2018
        },
        {
            "authors": [
                "Jiawei Wang",
                "Hai Zhao."
            ],
            "title": "ArT: All-round thinker for unsupervised commonsense question answering",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 1490\u20131501, Gyeongju, Republic of Korea. Interna-",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Jiangnan Xia",
                "Chen Wu",
                "Ming Yan."
            ],
            "title": "Incorporating relation knowledge into commonsense reading comprehension with multi-task learning",
            "venue": "Proceedings of the 28th ACM International Conference on Information and Knowledge Management, pages",
            "year": 2019
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D. Manning."
            ],
            "title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
            "venue": "Proceedings of the 2018 Conference on Empiri-",
            "year": 2018
        },
        {
            "authors": [
                "Biao Zhang",
                "Philip Williams",
                "Ivan Titov",
                "Rico Sennrich."
            ],
            "title": "Improving massively multilingual neural machine translation and zero-shot translation",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Xuhui Zhou",
                "Yue Zhang",
                "Leyang Cui",
                "Dandan Huang."
            ],
            "title": "Evaluating commonsense in pretrained language models",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 9733\u20139740.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) have demonstrated strong few-shot and zero-shot performance across various NLP tasks, with the larger models often matching earlier fine-tuned approaches that relied on task-specific labeled data (Radford et al., 2019; Brown et al., 2020a; Touvron et al., 2023). We focus on the zero-shot setup, which assumes that the knowledge needed to perform a specific task is already present in the LLM (Petroni et al., 2019; Zhou et al., 2020; Saha et al., 2022). Zero-shot learning has been employed for tasks such as translating between unseen language pairs (Zhang et al., 2020), summarization (Brown et al., 2020a), commonsense reasoning (Shwartz et al., 2020; Klein\nThe woman hired a lawyer because A. she decided to sue her employer. B. she decided to run for office. C. she wanted to sue her former employer.\nA. she decided to sue her employer.\nB. she decided to run for office.\nC. she wanted to sue her former employer."
        },
        {
            "heading": "LM score = 2.53 CASE score = 2.70",
            "text": "LM score = 2.35 CASE score = 2.76\nLM score = 1.80 CASE score = 1.89\nshe wanted to sue her former employer .\nshe decided to sue her employer .\nshe decided to run for office .\nFigure 1: An example from COPA. A and B are the original options, while option C was generated by GPT-2 as part of the answer space expansion step. The top line in each heatmap represent the LM (cross-entropy) score and the bottom line represents our CASE score. Higher scores and blue blocks correspond to lower plausibility. CASE correctly predicts option A (and option C which is an expansion of A) as more plausible than option B, while the LM-score incorrectly predicts option B.\nand Nabi, 2021; Liu et al., 2022; Fang et al., 2022), and more.\nIn multiple-choice question answering (MCQA) tasks, zero-shot methods typically rely on the language model (LM) probabilities as a proxy for plausibility, predicting the answer choice with the highest probability conditioned on the question. LM score is a na\u00efve proxy for plausibility, since it confounds factors such as length, unigram frequency, and more (Holtzman et al., 2021; Niu et al., 2021). Indeed, in Figure 1, a GPT-2 based LM score incorrectly predicts that the woman hired a lawyer\nbecause she decided to run for office, rather than because she decided to sue her employer.\nIn this paper, we propose to address one of the major limitations of the LM score. By summing or averaging the token-level probabilities, the LM score treats all tokens as equally important. A person reading this question would likely pay attention to option A because the word \u201csue\u201d is highly relevant in the context of a lawyer. This signal might be weaker in a basic LM score where the word \u201csue\u201d is conditioned on each other token in the question and previous tokens in the answer. Furthermore, the LM might miss non-trivial connections between related words.\nTo address this challenge, we propose CASE: a Commonsense-Augmented Score with an Expanded Answer Space. CASE is a post-hoc dynamic weight scoring algorithm that prioritizes important words in the sentence. The importance of each individual word is determined based on its relationship with other words in ConceptNet (Speer et al., 2017). For example, ConceptNet provides the information that \u201csue requires having a lawyer\u201d. We use the word-level importance scores to re-weigh the LM probability scores. Indeed, in the second line of option A in Figure 1, the importance of the word \u201csue\u201d increases the score of the entire sentence, leading to correctly predicting A as the correct answer.\nWe further adopt the strategy suggested by Niu et al. (2021) to expand the answer space by using a LM to generate additional answers and then mapping semantically-similar generated answers into the original space. This mitigates the LM score\u2019s sensitivity to infrequent words. Figure 1 demonstrates that a generated option C, \u201cshe wanted to sue her former employer\u201d, which is conceptually similar to A, further yields a higher probability score with our method.\nWe tested CASE on 5 popular commonsense MCQA datasets. CASE outperformed the broad range of strong baselines that we compared with, confirming that it is an effective method for zeroshot MCQA. We further study the impact of different model sizes, answer candidates of varying qualities, and different weight assignment strategies on the performance.1\n1Our code is available at Github."
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 Plausibility Scoring",
            "text": "Although the plausibility score of a sentence can be easily calculated by accumulating the probability assigned by the LM for each token, this approach suffers from various statistical biases such as sensitivity to the number of tokens, subword tokenization, and word frequency (Abdou et al., 2020; Holtzman et al., 2021). To address these biases, several improvements have been proposed. With respect to the length bias, prior work normalized the score by length (Mao et al., 2019; Brown et al., 2020b), or focused on the conditional probabilities of the question, which unlike the answer choices has a fixed length (Trinh and Le, 2018; Tamborrino et al., 2020). To factor out word frequency, Holtzman et al. (2021) proposed Domain Conditional Pointwise Mutual Information (DCPMI), which normalizes the conditional probability of the answer given the question by the prior probability of the answer. This is computed as the conditional probability of the answer given a domain-specific prefix such as \u201cThe sentiment of the movie is\u201d for sentiment analysis or \u201cThe answer is\u201d for general QA tasks. SEQA (Niu et al., 2021) mitigates the sensitivity to word choice by generating answers using GPT-2, and selecting the answer choice most similar to the generated answers.\nExisting methods solely focus on the relationship between words in the choices and words in the question, ignoring the importance of each word for the decision. In this paper, we propose a new tokenlevel weighting method to consider the importance of different words within the sentence based on their relationship to other words."
        },
        {
            "heading": "2.2 Knowledge-Enhanced Models",
            "text": "Zero-shot LM-based scoring methods implicitly reason about which answer is more likely based on the token-level probabilities. However, many tasks require multiple steps of reasoning to reach the correct answer (e.g., Mihaylov et al., 2018; Yang et al., 2018; Khot et al., 2020). A common approach is to retrieve relevant commonsense knowledge from knowledge bases (KBs) such as ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019a; Hwang et al., 2021), in order to enhance the neural model and explicate the reasoning steps (e.g., Bauer et al., 2018; Xia et al., 2019; Lin et al., 2019; Guan et al., 2019; Chen et al., 2020; Huang et al., 2021). More recent work used the COMET model\n(Bosselut et al., 2019; Hwang et al., 2021), which is a LM fine-tuned on the aforementioned KBs, to enhance models with high-coverage contextualized commonsense inferences (e.g., Majumder et al., 2020; Bosselut et al., 2021; Kim et al., 2022; Chakrabarty et al., 2022; Ravi et al., 2023).\nAn alternative recent approach which doesn\u2019t rely on external KBs prompts a LM to generate additional knowledge which is then incorporated back into the LM to make the prediction. Shwartz et al. (2020) and later Liu et al. (2022) used a LM to generate questions and answers about an MCQA instance. The answers to the questions are then incorporated into the LM-based scoring model as additional knowledge. Wei et al. (2022) proposed the popular chain-of-thought (COT) prompting approach in which the LM is taught through examples to generate multiple steps of reasoning followed by the answer to the question. In the zero-shot version, the LM is instructed to \u201cthink step-by-step\u201d. Finally, following concerns about the faithfulness of CoT inferences, Creswell et al. (2022) proposed to iteratively select parts of the inputs and draw inferences on them."
        },
        {
            "heading": "3 Method",
            "text": "We propose CASE, a Commonsense-Augmented Scoring method with an Expanded Answer Space.\nCASE can be used for zero-shot MCQA tasks. It is based on LM score (Section 3.1). However, rather than treating all words in the context and answers as equally important, we propose a weighted score where the conditional probability is weighed by the importance of a word. The weights are determined using a commonsense KB in order to provide information that humans might implicitly be reasoning about when answering such questions (Section 3.2). Following Niu et al. (2021), we expand the set of answer candidates by generating free-text answers, to increase the scorer\u2019s robustness to lexical variability (Section 3.3). An overview of the method is shown in Figure 2."
        },
        {
            "heading": "3.1 Basic Scoring Method",
            "text": "The basic scoring method directly uses the LM score, which is calculated by accumulating the conditional probabilities assigned by the LM for each token given the prefix. Given a question Q = q1...qnQ and an answer choice Ai = ai,1...ai,nAi , we convert Q into a declarative statement s (see Appendix A), and define the LM score of answer choice Ai as follows:\nPAi = P (Ai|s)\n= 1 ns + nAi \u00b7 nAi\u220f j=1 P (ai,j |s, ai,1, \u00b7 \u00b7 \u00b7 , ai,j\u22121) (1)\nwhere ns is the number of tokens in s.\nFinally, we can determine the most plausible choice A\u0302 among the answer choices based on their corresponding scores:\nA\u0302 = argmax i PAi (2)"
        },
        {
            "heading": "3.2 Commonsense Augmented Scoring",
            "text": "The importance of individual words in the question and their contribution to choosing the correct answer varies greatly. Take for example the instance in Figure 1, taken from the COPA dataset (Gordon et al., 2012). Determining the cause of the event \u201cThe woman hired a lawyer\u201d involves reasoning about the circumstances in which one might hire a lawyer, such as if they are suing someone. In this case, the keywords \u201clawyer\u201d from the context and \u201csue\u201d from the answer choice, and the semantic relation between them (i.e., suing someone requires a lawyer), supports the correct prediction. To that end, CASE first identifies important keywords from the question and answer choices (Section 3.2.1). Each keyword is assigned an importance score, and the conditional probability PA is updated by considering the importance of each token in the answer choice (Sec 3.2.2)."
        },
        {
            "heading": "3.2.1 Keywords Extraction",
            "text": "Given a question Q and an answer choice A, we use YAKE (Campos et al., 2018), an unsupervised automatic keyword extraction method, to extract a set of keywords KeyQ \u2282 Q and KeyA \u2282 A. In particular, we are interested in finding the keywords from each answer choice that are important in the context of the question Q, which we denote KeyA|Q \u2282 KeyA. To that end, we use ConceptNet (Speer et al., 2017), a commonsense knowledge base, to find paths between terms in KeyQ and KeyA, and include in KeyA|Q keywords from the answer choice that are connected in ConceptNet to keywords from the question:\nKeyA|Q = a \u2208 KeyA \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2203q \u2208 KeyQ \u2227 \u2203p = a ; q \u2208 CN \u2227 |p| \u2264 k  (3) where p denotes a path in ConceptNet (CN) with up to k edges."
        },
        {
            "heading": "3.2.2 Weight Assigning",
            "text": "We assign a weight to each token a \u2208 KeyA|Q based on the strength of its connection to keywords in KeyQ. To that end, we look at all the ConceptNet paths that connect a with keywords in KeyQ, which\nwe denote Pathsa;. We convert the path to a set of sentences by expressing each edge as a natural language sentence, based on relation templates (see Appendix B). For example, the path sue related to\u2212\u2212\u2212\u2212\u2212\u2192 law\nin context of\u2190\u2212\u2212\u2212\u2212\u2212\u2212 lawyer is expressed as S1 = \u201csue is related to law\u201d and S2 = \u201clawyer is a word used in the context of law\u201d. We use the LM to score a single path Pa;q as follows. First, the score S(Ei) of edge Ei = (xi, Ri, yi) is calculated as the conditional probability of generating the second node yi following the textual template of relation Ri, to which we assign the first node xi, such as P(law|sue is related to). We use the chain rule for conditional probability to compute the score of the entire path:\nS(Pa;q) = 1\n|Pa;q|+ 1 |Pa;q|\u2211 1 logS(Ei) + logS(E \u2032)  (4)\nwhere E\u2032 is an artificial summary edge from x1 to yPa;q with the \u201cis related to\u201d relation, such as \u201csue is related to lawyer\u201d.\nTo get an aggregated score for a token a, we sum the scores of all paths in Pathsa;:\nSPathsa; = \u2211\nPa;q\u2208Pathsa;\nS(Pa;q) (5)\nFinally, the weight for each token ai,j in Ai is computed as follows.\nWai,j = { 1 + \u03bbSPathsai,j; , if ai,j \u2208 KeyAi|Q 1, if ai,j /\u2208 KeyAi|Q (6)\nwhere \u03bb is a hyperparameter (\u00a74.3). With the weights for each token, we can now update the LM score defined in Equation 1 to a weight-based plausibility score as follows:\nPAi = n\u220f j=1 Wai,j \u00b7 P (ai,j |s, ai,1, \u00b7 \u00b7 \u00b7 , ai,j\u22121) (7)"
        },
        {
            "heading": "3.3 Expanded Answer Space",
            "text": "The final addition to our model aims at reducing the LM sensitivity to the phrasing of the correct answer. For example, an infrequent word in the correct answer choice can reduce the overall probability of the choice and make the LM predict another option as more plausible (Holtzman et al., 2021). To mitigate this issue, we follow Niu et al. (2021) and expand the set of answer candidates by using a causal LM to generate open ended answers A\u2217 =\n{A\u22171, ..., A\u2217nA\u2217}. The idea is to allow the model to consider various phrasings of the same conceptual answer. For example, in Figure 2, the generated answer C1 is a paraphrase of answer choice A.\nWe treat the generated answer choices A\u2217 the same as the original answer choices A and compute the score for each answer A\u2217i \u2208 A\u2217 using Equation 7. To map the answer choices back into the original answer space A, we attempt to match each A\u2217i \u2208 A\u2217 to Ai \u2208 A based on two criteria: sentence similarity and keyword connections.\nSentence Similarity. We use the SentenceTransformer package (Reimers and Gurevych, 2019) to represent the answers, and compute the cosine similarity between the representations of each generated answer in A\u2217 and original answer in A. The similarity score between the sentence pair should be above ssim.\nKeyword Connections. We calculate the connection score between the keywords in each generated answer in A\u2217 and each original answer in A using the method introduced in Sec 3.2.2. We require the connection score to be greater than 0.\nA candidate can only be assigned to a group if it meets both thresholds, and we discard generated answers that are not mapped into answer choices in A. Once we mapped generated answers to original answers, the final prediction of the model modifies Equation 2 to select the highest scores of all answers within the same cluster:\nA\u0302 = argmax i argmax j PAi,j (8)\nwhere Ai,j is the jth answer in cluster Ai."
        },
        {
            "heading": "4 Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We evaluated our method on five multiple-choice commonsense question answering datasets described below.\nCOPA. The goal in the Choice of Plausible Alternatives dataset (COPA; Roemmele et al., 2011) is, given a premise event, to choose the more plausible cause or effect among two alternatives.\nSCT. The Story Cloze Test dataset (SCT; Mostafazadeh et al., 2016) is a collection of foursentence stories with two possible endings. The goal is to predict which ending is more plausible following the beginning of the story.\nSocialIQA. The Social Interaction Question Answering (SocialIQA; Sap et al., 2019b) dataset tests models on their understanding of social situations and human behavior. Each question presents a hypothetical scenario followed by a question and 3 answer choices.\nARC. The AI2 Reasoning Challenge (ARC; Clark et al., 2018) consists of 7,787 science exam questions drawn from a variety of sources. The questions are divided into Easy (ARC-E) and Challenging (ARC-C) sets.\nOBQA. The OpenBookQA (OBQA; Mihaylov et al., 2018) dataset contains questions that require multi-step reasoning, use of commonsense knowledge, and rich text comprehension. The dataset has roughly 6,000 questions.\nSince the test set of SCT and SocialIQA are not publicly-available, we report the accuracy on the development set for all datasets."
        },
        {
            "heading": "4.2 Baselines",
            "text": "We compare our proposed method with the basic LM-based scoring method described in Section 3.1, as well as more advanced LM-based scoring methods described below.\nSelf-talk (Shwartz et al., 2020) consists of two causal LMs. The knowledge generator LM generates clarification questions conditioned on the context and pre-defined prefixes, and their corresponding answers. The scoring LM computes the probability of each answer choice conditioned on the context and question as well as the additionally generated knowledge.2\nDC-PMI (Holtzman et al., 2021) aims to eliminate the effect of the number of synonyms and the word frequency on the LM score by dividing the conditional probability (Eq 1) by a domainconditional prior probability for the answer choice.\nSEQA (Niu et al., 2021) uses a LM to generate a set of answer candidates. These candidates then \u201cvote\u201d for an original answer candidate based on their semantic similarity to each candidate, and the top-voted answer is selected as the final answer. For a fair comparison with the other model, we changed the voting model from SRoBERTaNLI to the origin SRoBERTa that was not further fine-tuned on an NLI dataset.\n2We don\u2019t compare with follow-up work by Liu et al. (2022) since they targeted a different set of tasks.\nCDG (Bosselut et al., 2021) uses knowledge from COMET (Bosselut et al., 2019) to construct a local commonsense knowledge graph for reasoning and inference.\nArT (Wang and Zhao, 2022) consists of two steps: notes taking and reverse thinking. In the notes taking step, the LM generates templated inferences pertaining to key phrases in the context, which are later added as additional knowledge. The reverse thinking step aggregates the scores of different orders of the answer and question (e.g. \u201cx because y\u201d vs. \u201cy therefore x\u201d)."
        },
        {
            "heading": "4.3 Setup and Hyper-parameters",
            "text": "We used GPT-2 via the HuggingFace Transformers library (Wolf et al., 2020) for the scoring part, and GPT-2 XL and GPT-3 davinci-003 for the answer space expansion step. In the keyword extraction step (\u00a73.2.1), we included ConceptNet paths with up to k = 3 edges. In the weight assigning step (\u00a73.2.2) we set the coefficient \u03bb to 10.\nIn the answer space expansion step (\u00a73.3), we generated nA\u2217 = 100 answers from GPT-2 and nA\u2217 = 50 answers from GPT-3 for each question. Similarly to SEQA, we used nucleus sampling (Holtzman et al., 2021) with p = 0.9 and set a maximum length of 15 tokens for both LMs. We set the sentence similarity threshold to ssim = 0.5 for GPT2 x-large and ssim = 0.6 for GPT-3.\nHyper-parameter values were selected based on preliminary experiments on the training sets and were not tuned on the dev sets."
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 Main Results",
            "text": "The performance of the various scoring methods on the 5 benchmarks are presented in Table 1. For fair comparison with the baselines, the table shows the performance when GPT2xlarge is used. We report the accuracy on the dev set. CAS stands for Commonsense-Augmented Scoring, i.e. it excludes the candidate generation.\nThe performance of CAS shows that weighting leads to substantial improvements upon the simpler baselines. CAS also stands out in the competition with DCPMI, which can also be regarded as a special weight-scoring method.\nWhen combined with candidate generation, CASE outperforms nearly all baselines, except for the SocialIQA dataset, on which ArT and Self-talk perform better. Notably, both baselines rely on human-designed prompts to generate additional information, which might give them an advantage.\nThe gap in performance from SEQA, which also expands the answer space by generating candidate answers, further demonstrates the effectiveness of dynamic weighting."
        },
        {
            "heading": "5.2 Effect of the Scoring LM Size",
            "text": "Table 2 shows the performance of CAS, CASE and the simple baselines when using different sizes of GPT-2 models in the scoring part.\nBigger is better. Across the various methods, bigger LMs perform better than smaller LMs.\nDataset Methods GPT2S GPT2M GPT2L GPT2XL\nCOPA LMsum 60.0 66.6 69.2 69.0 LMavg 62.6 65.4 67.0 68.4 CAS 62.0 67.2 69.4 70.4 CASEGPT2 69.6 72.0 72.2 73.8 CASEGPT3 75.4 76.4 77.4 78.2\nSCT\nLMsum 58.2 62.7 64.4 67.9 LMavg 60.4 66.4 68.8 71.5 CAS 61.9 67.5 70.9 73.0 CASEGPT2 74.0 75.2 75.7 76.1 CASEGPT3 76.7 78.6 79.0 83.2\nSIQA LMsum 39.7 41.4 42.0 43.1 LMavg 41.8 44.1 44.9 45.8 CAS 42.8 44.6 45.7 46.0 CASEGPT2 43.9 43.7 44.1 44.5 CASEGPT3 47.6 48.4 48.5 48.5\nARC-E LMsum 44.2 48.8 50.4 53.5 LMavg 37.9 40.2 45.1 47.4 CAS 46.1 49.8 53.0 55.8 CASEGPT2 46.5 49.6 52.0 54.4 CASEGPT3 54.2 59.1 60.0 63.2\nARC-C LMsum 19.7 23.1 22.7 25.4 LMavg 23.4 23.7 25.4 28.7 CAS 26.4 26.4 27.4 28.8 CASEGPT2 28.1 29.4 27.8 30.8 CASEGPT3 33.4 35.3 33.8 36.5\nOBQA LMsum 16.2 18.2 21.8 22.4 LMavg 23.0 26.8 30.0 30.8 CAS 25.6 28.6 31.4 32.6 CASEGPT2 26.0 26.6 27.4 30.2 CASEGPT3 32.2 35.4 37.4 35.2\nTable 2: Accuracy when using GPT2 models with different sizes for the scoring. Takeaways: CAS consistently outperforms standard LM scoring methods, and is outperformed by CASE. For CASE, the best performance is achieved when using large GPT2 models for scoring and more importantly, GPT3 for candidate generation.\nSmaller LMs gain more from candidate generation. While all LMs benefit from weighting and candidate generation, smaller LMs gain bigger improvements. For example, candidate generation with GPT-3 adds 13.4 points on COPA to a GPT2S CAS scorer, but only 8.2 points for GPT2XL. We hypothesize that the model performance is more sensitive to the LM quality when a single sentence is considered, while expanding the answer space makes even the lower-quality LMs more robust."
        },
        {
            "heading": "5.3 Effect of the No. of Generated Candidates",
            "text": "Figure 3 shows the effect of the number of generated candidates on the performance, focusing on COPA. We summarize the findings below.\nGenerating more candidates leads to higher accuracy. When generating few (< 20) candidates, the model\u2019s performance is unstable and relatively low. This might happen due to the generated answers being conceptually different from the original candidate answers, in which case they might not meet the mapping thresholds in Section 3.3 and\nbe filtered out. This means that CASE effectively degenerates to CAS. Thus, it\u2019s important to generate a large number of candidates. This reassesses the findings in Niu et al. (2021).\nLarger models require fewer candidates. Larger LMs generate higher quality text which is more likely to be fluent, relevant to the context, logically correct, and consistent with commonsense knowledge. Therefore, we can expect fewer candidates to be filtered out. In addition, the generated candidates may be conceptually similar and better phrased than the original choice."
        },
        {
            "heading": "5.4 Effect of the Weighting Strategy",
            "text": "Table 3 compares the COPA performance of different weighting strategies. Two baselines, LMsum and LMavg, already introduced in Section 3.1, treat all tokens equally, summing or averaging the tokenlevel probabilities. Conversely, the static weighting strategy (SW and SWC, with or without candidate generation), assigns a static number (1.5) to each selected key token. Finally, the dynamic weighting strategies (CAS and CASE) not only distinguish key tokens from unimportant ones but also assign different scores to each key token based on its semantic relevance to the question.\nThe results show that while the static weighting strategy outperforms the baseline when no additional candidates are generated (SW vs. LM), these strategies perform similarly when additional candidates are generated (SWC vs. LM+c). In both cases,\nthe static weighting strategy underperforms compared to the dynamic strategy. This result confirms that commonsense knowledge can help inform the model about the keywords that are important for the current question."
        },
        {
            "heading": "6 Qualitative Analysis",
            "text": "We focus on CASE and look at the individual token scores and corresponding ConceptNet paths to better understand the model decision-making process.\nFigure 4 shows an example from SCT where CASE predicted the correct answer. The word \u201cupset\u201d in the correct answer choice was assigned a high weight by CASE thanks to ConceptNet paths such as upset related to\u2190\u2212\u2212\u2212\u2192 depression causes\u2190\u2212\u2212\u2212 stress\nrelated to\u2190\u2212\u2212\u2212\u2192 work. Conversely, in Figure 5, CASE predicted the incorrect answer choice for another SCT example. The model focused on the word \u201cleft\u201d due to its semantic relation to the word \u201cdrove\u201d, failing to understand that Priya drove to and not away from the restaurant."
        },
        {
            "heading": "7 Conclusion",
            "text": "We presented CASE, a novel LM-based plausibility score for zero-shot MCQA tasks. CASE uses a commonsense KB to assign importance weights to words in the input. The weighting strategy outperforms basic LM scoring methods. When combined with generating additional answer candidates, CASE outperforms the baselines on 5 popular MCQA benchmarks. We further showed that the two approaches are complementary and are especially beneficial when using smaller LMs. In the future, we plan to explore a more selective approach\nfor knowledge retrieval from the KB, and adapt CASE for additional NLP tasks.\nLimitations\nComputational complexity. CASE is more computationally expensive than using a basic LM score, as it involves finding relevant paths from an external knowledge base and then estimating their likelihood with a LM, in order to gauge the importance of keywords.\nConcept coverage. The weight assignment strategy in CASE is based on ConceptNet. The knowledge in KBs such as ConceptNet is not contextualized, which means that some facts pertaining to concepts in the instance might not be relevant to the specific context. In addition, it has limited coverage. COMET (Hwang et al., 2021) has been used in prior work (Majumder et al., 2020; Chakrabarty et al., 2020; Ravi et al., 2023) to overcome this limitation. However, finding relevant paths using COMET requires an iterative multi-hop reasoning approach (Arabshahi et al., 2021) which is more complex, and more computationally-intensive. We aim to explore efficient ways to achieve this in future work.\nAnswer format. Since our method assigns a weight for each word in the input, it is only ap-\nplicable for MCQA tasks in which the answer is a sentence. The weighting would be trivial for tasks with single word answers such as CommonsenseQA (Talmor et al., 2019) and BoolQ (Clark et al., 2019).\nPerformance limit. Our model demonstrates a significant performance improvement over other zero-shot baselines across a majority of datasets. However, it is worth noting that the state-of-theart performance on the datasets in this paper is achieved with more supervision (i.e. supervised or few-shot models).\nEthics Statement\nData. All the datasets and knowledge bases used in this work are publicly available. We used ConceptNet as a source of commonsense knowledge. Since ConceptNet was crowdsourced, some of the knowledge may contain societal biases or prejudices held by the annotators (Mehrabi et al., 2021).\nModels. The GPT-2 models are publicly accessible via HuggingFace, while GPT-3 is a closed model behind an API. All language models may generate offensive statements if prompted with specific inputs, however, our model only generates text internally while the end output is a choice between human-written answer candidates."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was funded, in part, by the Vector Institute for AI, Canada CIFAR AI Chairs program, an NSERC discovery grant, and a research gift from AI2."
        },
        {
            "heading": "A Question Prompts",
            "text": "Table 4 shows the prompts used for each dataset. For tasks with several specific question type such as COPA and SocialIQa, we convert each question type to a natural language proxy following previous work (e.g. Shwartz et al., 2020). For tasks that present an open-ended question, we append the prefix \u201cThe answer is\u201d. Finally, for tasks that are already designed to expect the next word or sentence (such as SCT), we use the instance as is."
        },
        {
            "heading": "B Relation Templates",
            "text": "Table 5 displays the templates we used to convert edges with different relation types in ConceptNet to natural language sentences, following Davison et al. (2019)."
        }
    ],
    "title": "CASE: Commonsense-Augmented Score with an Expanded Answer Space",
    "year": 2023
}