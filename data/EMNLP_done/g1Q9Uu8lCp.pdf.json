{
    "abstractText": "Recent advancements in reference-free learned metrics for open-domain dialogue evaluation have been driven by the progress in pre-trained language models and the availability of dialogue data with high-quality human annotations. However, current studies predominantly concentrate on English dialogues, and the generalization of these metrics to other languages has not been fully examined. This is largely due to the absence of a multilingual dialogue evaluation benchmark. To address the issue, we introduce xDial-Eval, built on top of open-source English dialogue evaluation datasets. xDialEval includes 12 turn-level and 6 dialoguelevel English datasets, comprising 14930 annotated turns and 8691 annotated dialogues respectively. The English dialogue data are extended to nine other languages with commercial machine translation systems. On xDialEval, we conduct comprehensive analyses of previous BERT-based metrics and the recentlyemerged large language models. Lastly, we establish strong self-supervised and multilingual baselines. In terms of average Pearson correlations over all datasets and languages, the best baseline outperforms OpenAI\u2019s ChatGPT by absolute improvements of 6.5% and 4.6% at the turn and dialogue levels respectively, albeit with much fewer parameters. The data and code are publicly available at https: //github.com/e0397123/xDial-Eval.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chen Zhang"
        },
        {
            "affiliations": [],
            "name": "Luis Fernando D\u2019Haro"
        },
        {
            "affiliations": [],
            "name": "Chengguang Tang"
        },
        {
            "affiliations": [],
            "name": "Ke Shi"
        },
        {
            "affiliations": [],
            "name": "Guohua Tang"
        },
        {
            "affiliations": [],
            "name": "Haizhou Li"
        }
    ],
    "id": "SP:d37427709e2c5389fbc5513fef42a227b8a74325",
    "references": [
        {
            "authors": [
                "Ebtesam Almazrouei",
                "Hamza Alobeidli",
                "Abdulaziz Alshamsi",
                "Alessandro Cappelli",
                "Ruxandra Cojocaru",
                "Merouane Debbah"
            ],
            "title": "Falcon-40B: an open large language model with state-of-the-art performance",
            "year": 2023
        },
        {
            "authors": [
                "Yuntao Bai",
                "Saurav Kadavath",
                "Sandipan Kundu",
                "Amanda Askell",
                "Jackson Kernion",
                "Andy Jones"
            ],
            "title": "Constitutional AI: Harmlessness from AI feedback",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yi Chen",
                "Rui Wang",
                "Haiyun Jiang",
                "Shuming Shi",
                "Ruifeng Xu."
            ],
            "title": "Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study",
            "venue": "arXiv preprint arXiv: Arxiv-2304.00723.",
            "year": 2023
        },
        {
            "authors": [
                "Zhihong Chen",
                "Feng Jiang",
                "Junying Chen",
                "Tiannan Wang",
                "Fei Yu",
                "Guiming Chen",
                "Hongbo Zhang",
                "Juhao Liang",
                "Chen Zhang",
                "Zhiyi Zhang",
                "Jianquan Li",
                "Xiang Wan",
                "Benyou Wang",
                "Haizhou Li"
            ],
            "title": "Phoenix: Democratizing ChatGPT across languages",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang"
            ],
            "title": "Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts"
            ],
            "title": "PaLM: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Marta R. Costa-juss\u00e0",
                "James Cross",
                "Onur \u00c7elebi",
                "Maha Elbayad",
                "Kenneth Heafield",
                "Kevin Heffernan"
            ],
            "title": "No language left behind: Scaling humancentered machine translation",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Jinlan Fu",
                "See-Kiong Ng",
                "Zhengbao Jiang",
                "Pengfei Liu"
            ],
            "title": "GPTScore: Evaluate as you desire",
            "year": 2023
        },
        {
            "authors": [
                "Sarik Ghazarian",
                "Behnam Hedayatnia",
                "Alexandros Papangelis",
                "Yang Liu",
                "Dilek Hakkani-Tur."
            ],
            "title": "What is wrong with you?: Leveraging user sentiment for automatic dialog evaluation",
            "venue": "Findings of the Association for Computational Linguistics: ACL",
            "year": 2022
        },
        {
            "authors": [
                "Dublin",
                "Ireland"
            ],
            "title": "Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Sarik Ghazarian",
                "Nuan Wen",
                "Aram Galstyan",
                "Nanyun Peng."
            ],
            "title": "DEAM: Dialogue coherence evaluation using AMR-based semantic manipulations",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2022
        },
        {
            "authors": [
                "Prakhar Gupta",
                "Cathy Jiao",
                "Yi-Ting Yeh",
                "Shikib Mehri",
                "Maxine Eskenazi",
                "Jeffrey Bigham."
            ],
            "title": "InstructDial: Improving zero and few-shot generalization in dialogue through instruction tuning",
            "venue": "Proceedings of the 2022 Conference on Empirical",
            "year": 2022
        },
        {
            "authors": [
                "Prakhar Gupta",
                "Shikib Mehri",
                "Tiancheng Zhao",
                "Amy Pavel",
                "Maxine Eskenazi",
                "Jeffrey Bigham."
            ],
            "title": "Investigating evaluation of open-domain dialogue systems with human generated multiple references",
            "venue": "Proceedings of the 20th Annual SIGdial Meeting",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "Lishan Huang",
                "Zheng Ye",
                "Jinghui Qin",
                "Liang Lin",
                "Xiaodan Liang."
            ],
            "title": "GRADE: Automatic graphenhanced coherence metric for evaluating opendomain dialogue systems",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Jessica Huynh",
                "Cathy Jiao",
                "Prakhar Gupta",
                "Shikib Mehri",
                "Payal Bajaj",
                "Vishrav Chaudhary",
                "Maxine Eskenazi."
            ],
            "title": "Understanding the effectiveness of very large language models on dialog evaluation",
            "venue": "The 13th International Workshop on Spoken Dia-",
            "year": 2023
        },
        {
            "authors": [
                "Tianbo Ji",
                "Yvette Graham",
                "Gareth Jones",
                "Chenyang Lyu",
                "Qun Liu."
            ],
            "title": "Achieving reliable human assessment of open-domain dialogue systems",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2022
        },
        {
            "authors": [
                "Tian Lan",
                "Xian-Ling Mao",
                "Wei Wei",
                "Xiaoyan Gao",
                "Heyan Huang."
            ],
            "title": "PONE: A novel automatic evaluation metric for open-domain generative dialogue systems",
            "venue": "ACM Trans. Inf. Syst., 39(1).",
            "year": 2020
        },
        {
            "authors": [
                "Hugo Lauren\u00e7on",
                "Lucile Saulnier",
                "Thomas Wang",
                "Christopher Akiki",
                "Albert Villanova del Moral",
                "Teven Le Scao"
            ],
            "title": "The bigscience ROOTS corpus: A 1.6TB composite multilingual dataset",
            "venue": "In Thirty-sixth Conference on Neural Information Pro-",
            "year": 2022
        },
        {
            "authors": [
                "Yanran Li",
                "Hui Su",
                "Xiaoyu Shen",
                "Wenjie Li",
                "Ziqiang Cao",
                "Shuzi Niu."
            ],
            "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Yen-Ting Lin",
                "Yun-Nung Chen"
            ],
            "title": "LLM-Eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models",
            "year": 2023
        },
        {
            "authors": [
                "Zhaojiang Lin",
                "Zihan Liu",
                "Genta Indra Winata",
                "Samuel Cahyawijaya",
                "Andrea Madotto",
                "Yejin Bang",
                "Etsuko Ishii",
                "Pascale Fung."
            ],
            "title": "XPersona: Evaluating multilingual personalized chatbot",
            "venue": "Proceedings",
            "year": 2021
        },
        {
            "authors": [
                "Chia-Wei Liu",
                "Ryan Lowe",
                "Iulian Serban",
                "Mike Noseworthy",
                "Laurent Charlin",
                "Joelle Pineau."
            ],
            "title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
            "venue": "Proceedings of",
            "year": 2016
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu."
            ],
            "title": "G-Eval: NLG evaluation using GPT-4 with better human alignment",
            "venue": "arXiv preprint arXiv: Arxiv-2303.16634.",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A robustly optimized BERT pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Shikib Mehri",
                "Maxine Eskenazi."
            ],
            "title": "Unsupervised evaluation of interactive dialog with DialoGPT",
            "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 225\u2013235, 1st virtual meeting. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Shikib Mehri",
                "Maxine Eskenazi."
            ],
            "title": "USR: An unsupervised and reference free evaluation metric for dialog generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 681\u2013707, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Shikib Mehri",
                "Yulan Feng",
                "Carla Gordon",
                "Seyed Hossein Alavi",
                "David Traum",
                "Maxine Eskenazi."
            ],
            "title": "Interactive evaluation of dialog track at DSTC9",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 5731\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Mohsen Mesgar",
                "Sebastian B\u00fccker",
                "Iryna Gurevych."
            ],
            "title": "Dialogue coherence assessment without explicit dialogue act labels",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1439\u20131450, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Jeff Mitchell",
                "Mirella Lapata."
            ],
            "title": "Vector-based models of semantic composition",
            "venue": "Proceedings of ACL-08: HLT, pages 236\u2013244, Columbus, Ohio. Association for Computational Linguistics.",
            "year": 2008
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Thomas Wang",
                "Lintang Sutawika",
                "Adam Roberts",
                "Stella Biderman",
                "Teven Le Scao"
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in neural information processing systems.",
            "year": 2022
        },
        {
            "authors": [
                "Bo Pang",
                "Erik Nijkamp",
                "Wenjuan Han",
                "Linqi Zhou",
                "Yixian Liu",
                "Kewei Tu."
            ],
            "title": "Towards holistic and automatic evaluation of open-domain dialogue generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Guilherme Penedo",
                "Quentin Malartic",
                "Daniel Hesslow",
                "Ruxandra Cojocaru",
                "Alessandro Cappelli",
                "Hamza Alobeidli",
                "Baptiste Pannier",
                "Ebtesam Almazrouei",
                "Julien Launay"
            ],
            "title": "The RefinedWeb dataset for Falcon LLM: outperforming curated corpora",
            "year": 2023
        },
        {
            "authors": [
                "Vitou Phy",
                "Yang Zhao",
                "Akiko Aizawa."
            ],
            "title": "Deconstruct to reconstruct a configurable evaluation metric for open-domain dialogue systems",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 4164\u20134178, Barcelona, Spain (On-",
            "year": 2020
        },
        {
            "authors": [
                "Ricardo Rei",
                "Nuno M. Guerreiro",
                "Jos\u00e9 Pombal",
                "Daan van Stigt",
                "Marcos Treviso",
                "Luisa Coheur",
                "Jos\u00e9 G.C. de Souza",
                "Andr\u00e9 F.T. Martins."
            ],
            "title": "Scaling up COMETKIWI: Unbabel-IST 2023 submission for the quality estimation shared task",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Ricardo Rei",
                "Marcos Treviso",
                "Nuno M. Guerreiro",
                "Chrysoula Zerva",
                "Ana C Farinha",
                "Christine Maroti",
                "Jos\u00e9 G.C. de Souza",
                "Taisiya Glushkova",
                "Duarte Alves",
                "Luisa Coheur",
                "Alon Lavie",
                "Andr\u00e9 F.T. Martins"
            ],
            "title": "CometKiwi: IST-unbabel 2022",
            "year": 2022
        },
        {
            "authors": [
                "Mario Rodr\u00edguez-Cantelar",
                "Chen Zhang",
                "Chengguang Tang",
                "Ke Shi",
                "Sarik Ghazarian",
                "Jo\u00e3o Sedoc",
                "Luis Fernando D\u2019Haro",
                "Alexander Rudnicky"
            ],
            "title": "Robust and multilingual automatic evaluation metrics",
            "year": 2023
        },
        {
            "authors": [
                "Ananya B. Sai",
                "Akash Kumar Mohankumar",
                "Siddhartha Arora",
                "Mitesh M. Khapra."
            ],
            "title": "Improving dialog evaluation with a multi-reference adversarial dataset and large scale pretraining",
            "venue": "Transactions of the Association for Computational Linguistics, 8:810\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Motoki Sato",
                "Hiroki Ouchi",
                "Yuta Tsuboi."
            ],
            "title": "Addressee and response selection for multilingual conversation",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 3631\u20133644, Santa Fe, New Mexico, USA. As-",
            "year": 2018
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow"
            ],
            "title": "Bloom: A 176b-parameter open-access multilingual language model",
            "year": 2023
        },
        {
            "authors": [
                "Abigail See",
                "Stephen Roller",
                "Douwe Kiela",
                "Jason Weston."
            ],
            "title": "What makes a good conversation? how controllable attributes affect human judgments",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Thibault Sellam",
                "Dipanjan Das",
                "Ankur Parikh."
            ],
            "title": "BLEURT: Learning robust metrics for text generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881\u20137892, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Koustuv Sinha",
                "Prasanna Parthasarathi",
                "Jasmine Wang",
                "Ryan Lowe",
                "William L. Hamilton",
                "Joelle Pineau."
            ],
            "title": "Learning an unreferenced metric for online dialogue evaluation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Eric Smith",
                "Orion Hsu",
                "Rebecca Qian",
                "Stephen Roller",
                "Y-Lan Boureau",
                "Jason Weston."
            ],
            "title": "Human evaluation of conversations is an open problem: comparing the sensitivity of various methods for evaluating dialogue agents",
            "venue": "Proceedings of the 4th",
            "year": 2022
        },
        {
            "authors": [
                "Ekaterina Svikhnushina",
                "Anastasiia Filippova",
                "Pearl Pu."
            ],
            "title": "iEval: Interactive evaluation framework for open-domain empathetic chatbots",
            "venue": "Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 419\u2013431,",
            "year": 2022
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford Alpaca: An instruction-following LLaMa model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei"
            ],
            "title": "2023b. Llama 2: Open foundation and fine-tuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language models with self-generated instructions",
            "year": 2023
        },
        {
            "authors": [
                "Aiyuan Yang",
                "Bin Xiao",
                "Bingning Wang",
                "Borong Zhang",
                "Ce Bian",
                "Chao Yin"
            ],
            "title": "Baichuan 2: Open large-scale language models",
            "year": 2023
        },
        {
            "authors": [
                "Yi-Ting Yeh",
                "Maxine Eskenazi",
                "Shikib Mehri."
            ],
            "title": "A comprehensive assessment of dialog evaluation metrics",
            "venue": "The First Workshop on Evaluations and Assessments of Neural Conversation Systems, pages 15\u201333, Online. Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Chen Zhang",
                "Yiming Chen",
                "Luis Fernando D\u2019Haro",
                "Yan Zhang",
                "Thomas Friedrichs",
                "Grandee Lee",
                "Haizhou Li"
            ],
            "title": "2021a. DynaEval: Unifying turn and dialogue level evaluation",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Chen Zhang",
                "Luis Fernando D\u2019Haro",
                "Rafael E. Banchs",
                "Thomas Friedrichs",
                "Haizhou Li"
            ],
            "title": "2021b. Deep AM-FM: Toolkit for Automatic Dialogue Evaluation, pages 53\u201369",
            "year": 2021
        },
        {
            "authors": [
                "Chen Zhang",
                "Luis Fernando D\u2019Haro",
                "Qiquan Zhang",
                "Thomas Friedrichs",
                "Haizhou Li"
            ],
            "title": "2022a. FineDeval: Fine-grained automatic dialogue-level evaluation",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Chen Zhang",
                "Luis Fernando D\u2019Haro",
                "Qiquan Zhang",
                "Thomas Friedrichs",
                "Haizhou Li"
            ],
            "title": "PoE: A panel of experts for generalized automatic dialogue assessment",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2023
        },
        {
            "authors": [
                "Chen Zhang",
                "Grandee Lee",
                "Luis Fernando D\u2019Haro",
                "Haizhou Li"
            ],
            "title": "2021c. D-Score: Holistic dialogue evaluation without reference",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Chen Zhang",
                "Jo\u00e3o Sedoc",
                "Luis Fernando D\u2019Haro",
                "Rafael Banchs",
                "Alexander Rudnicky"
            ],
            "title": "Automatic evaluation and moderation of open-domain dialogue systems",
            "venue": "In DSTC10 Workshop in AAAI 2022",
            "year": 2022
        },
        {
            "authors": [
                "Mozhi Zhang",
                "Wei Wang",
                "Budhaditya Deb",
                "Guoqing Zheng",
                "Milad Shokouhi",
                "Ahmed Hassan Awadallah."
            ],
            "title": "A dataset and baselines for multilingual reply suggestion",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Saizheng Zhang",
                "Emily Dinan",
                "Jack Urbanek",
                "Arthur Szlam",
                "Douwe Kiela",
                "Jason Weston"
            ],
            "title": "Personalizing dialogue agents: I have a dog, do you have pets too",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "BERTScore: Evaluating text generation with bert",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Zhao",
                "Divesh Lala",
                "Tatsuya Kawahara."
            ],
            "title": "Designing precise and robust dialogue response evaluators",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 26\u201333, Online. Association for Computational Lin-",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Open-domain dialogue evaluation is a long-lasting challenge to dialogue system research (Mehri et al., 2022a). Currently, human evaluation is the most reliable way to holistically judge the quality of the dialogue. Due to the high costs and low reproducibility of human evaluation, automatic metrics are proposed to complement it.\n1All methods examined in the paper are not trained on data with human quality annotations. xDial-Eval is purely used for testing purposes.\nThere are two distinct paradigms of automatic evaluation, reference-based, and reference-free. Reference-based metrics, such as BLEU (Papineni et al., 2002) and Embedding Average (Mitchell and Lapata, 2008) are widely adopted in the research community due to their easy implementation and general applicability to various types of dialogues. Yet, they can be misleading due to the poor correlation with human judgment (Liu et al., 2016; Mehri et al., 2022a). On the other hand, reference-free metrics bypass the reliance on references and directly estimate the quality of a single response (turn level) or a multi-turn dialogue (dialogue level).\nCurrently, there\u2019s a growing interest in creating model-based, reference-free metrics. One line of work focuses on learning a discriminative metric with self-supervised learning - a model is trained to distinguish high-quality responses/dialogues from low-quality responses/dialogues based on weak supervision signals that are automatically constructed from human-human dialogue data (Yeh et al., 2021). These metrics benefit from the BERTbased language models (Devlin et al., 2019; Liu et al., 2019) and the availability of high-quality dialogue corpora (Li et al., 2017; Zhang et al., 2018). With the recent advancement of large language models (LLMs) (Brown et al., 2020; Touvron et al., 2023a; Ouyang and et al., 2022; OpenAI, 2023), an emerging line of work is to treat the LLMs as annotators, which judge the quality of responses/dialogues through prompting (Gupta et al., 2022; Huynh et al., 2023; Fu et al., 2023). Different from the BERT-based metrics, such metrics are generative in nature.\nA common attribute of both metric categories is that they are not trained on dialogue evaluation data with human quality annotations, yet they exhibit significant potential in simulating how humans perform evaluations at the turn or dialogue level. Despite the significant progress in the field, current research predominantly focuses on the En-\nglish language, other languages receive insufficient attention. To expand the scope of automatic dialogue evaluation research beyond English and thoroughly investigate the language generalization potential of model-based reference-free metrics, we propose a large-scale multilingual open-domain dialogue evaluation benchmark, named xDial-Eval. To construct xDial-Eval, we curate 12 turn-level and 6 dialogue-level open-source English evaluation datasets. In total, the turn-level and dialoguelevel datasets comprise 14930 human-annotated turns and 8691 human-annotated multi-turn dialogues respectively. Then, we translate the English data into nine different languages with commercial machine translation models.\nIn addition, we comprehensively assess the performance of the two metric categories on xDialEval. Pertaining to the discriminative category, previous state-of-the-art (SoTA) BERT-based methods are analyzed, while for the generative category, we evaluate the multilingual dialogue evaluation capability of recent LLMs, especially the instructiontuned variants, such as ChatGPT2, Alpaca (Taori et al., 2023), and Vicuna (Chiang et al., 2023). In our analysis, we systematically explore the effects of training data, training strategies, and pretrained models on multilingual dialogue evaluation.\nLastly, we introduce strong multilingual selfsupervised baselines on xDial-Eval. Specifically, we fine-tune the LLMs on synthetic instruction data built from human-human dialogues. The finetuned models are found to exhibit much stronger multilingual dialogue evaluation capability than the original LLMs. Motivated by the complementary nature of generative and discriminative metrics, we perform metric ensemble, which yields strong correlations with human evaluation and language generalization capability on xDial-Eval, even outperforming the powerful ChatGPT, which has been recently proven to be a superior reference-free text evaluator (Chen et al., 2023a; Liu et al., 2023)."
        },
        {
            "heading": "2 Related Work",
            "text": "A long-lasting goal of automatic dialogue evaluation is to fully approximate human evaluation, which is quantified by strong correlations (0.8+) with ground-truth human quality annotations (Mehri et al., 2022a). Recent research is dedicated to developing powerful model-based\n2 https://openai.com/blog/chatgpt\nreference-free metrics for automatic dialogue evaluation (Yeh et al., 2021).\nBERT-Based Discriminative Metrics - A series of works (Sai et al., 2020; Huang et al., 2020; Sinha et al., 2020; Lan et al., 2020; Mehri and Eskenazi, 2020b; Phy et al., 2020; Pang et al., 2020; Zhang et al., 2021c) targets turn-level evaluation and leverages self-supervised learning. They rely on negative sampling strategies, such as random utterance replacement and word order shuffling, to generate synthetic data for training discriminative models. Another group of metrics is learned to holistically judge the quality of multi-turn dialogues (Mesgar et al., 2020; Zhang et al., 2021a; Ghazarian et al., 2022b; Zhang et al., 2022a) with a similar idea that a model is trained to distinguish original humanhuman dialogues from negative samples generated by strategies, such as utterance order shuffling.\nAll the metrics rely on either BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019) for dialogue context understanding and modeling the utterancelevel interactions. Although they exhibit good correlations with human evaluation on English benchmark datasets, their transferability across different languages has not been explored. In this study, we select representative discriminative metrics and conduct correlation analyses on the multilingual xDial-Eval benchmark.\nLLM-Based Generative Metrics - The evolution of large language models (LLM) has drastically changed the NLP landscape (Brown et al., 2020; Chowdhery et al., 2022; Muennighoff et al., 2023; Touvron et al., 2023a). Especially, the line of works on instruction-tuning of LLMs (Ouyang and et al., 2022; Bai et al., 2022; Chung et al., 2022; OpenAI, 2023; Taori et al., 2023; Chiang et al., 2023; Chen et al., 2023b) has led to general-purpose AI assistants that exhibit remarkable ability in understanding and following users\u2019 instructions.\nIn the context of automatic dialogue evaluation, these LLMs can serve as unified evaluators of both the response and the multi-turn dialogue quality through prompting with task-specific templates. For example, Gupta et al. (2022) introduces InstructDial, which consists of 48 diverse dialogue tasks in a unified text-to-text format. Models tuned on the InstructDial dataset demonstrate good zero-shot performance in the dialogue evaluation task. Huynh et al. (2023) conduct a comprehensive analysis of the dialogue evaluation capability of LLMs with varying model types, sizes, choices of\ntraining data, etc. They observe that the smaller language models fine-tuned on instructions and dialogue-specific data can outperform very large language models. We move beyond both works by comprehensively exploring the LLMs\u2019 ability in evaluating multilingual dialogue.\nMore recently, several works (Chen et al., 2023a; Liu et al., 2023; Lin and Chen, 2023) study the dialogue evaluation capability of closed-source instruction-following LLMs via prompting, such as OpenAI\u2019s ChatGPT and Anthropic Claude (Bai et al., 2022). These closed-source LLMs exhibit strong zero-shot correlations with human evaluation. Yet, a major limitation is that it is not always feasible to adapt such models to custom data. Our work serves as the first study exploring the dialogue evaluation capability of both the closed-source and the recent open-source instruction-tuned LLMs on multilingual dialogue evaluation data. Additionally, we move beyond prompt engineering by performing task-specific finetuning of the open-source LLMs to boost the dialogue evaluation ability and language generalization of such models.\nMultilingual Open-Domain Dialogue - Existing studies on multilingual open-domain dialogue mainly target dialogue response selection and dialogue generation (Lin et al., 2021) with little emphasis on automatic dialogue evaluation. For example, Sato et al. (2018) constructs a multilingual dataset, which contains the Ubuntu IRC logs in 12 different languages for response selection. Zhang et al. (2021d) proposes MRS, a multilingual reply suggestion dataset with ten languages. More related to our work, Rodr\u00edguez-Cantelar et al. (2023) recently held the shared task on \"Robust and Multilingual Automatic Evaluation Metrics for OpenDomain Dialogue Systems\" at DSTC113. They release a series of multilingual evaluation datasets and a multilingual deep AM-FM (Zhang et al., 2021b) baseline. Yet, their released data is only limited to 3 languages. Most of the data target turnlevel evaluation with a limited amount of dialoguelevel annotated data. On the contrary, our proposed benchmark, xDial-Eval, contains large-scale annotated data at both turn and dialogue levels, spanning 10 different languages.\n3 https://dstc11.dstc.community/tracks"
        },
        {
            "heading": "3 The xDial-Eval Benchmark",
            "text": ""
        },
        {
            "heading": "3.1 Benchmark Details",
            "text": "The xDial-Eval benchmark is built on top of opensource English evaluation datasets. The statistics of the datasets are outlined in Table 1. The English datasets comprise 14930 annotated turns and 8691 annotated multi-turn dialogues. We utilize the Microsoft Azure translation service4 to translate all the datasets into nine diverse languages: Chinese (ZH), Spanish (ES), German (DE), French (FR), Japanese (JA), Korean (KO), Hindi (HI), Arabic (AR), and Russian (RU). In total, our translation effort comprised 183,495 unique utterances and cost approximately 400 USD. We keep the original dialogue quality annotations from the English datasets for the newly translated dialogue data. Examples of the translated data are included in Appendix B."
        },
        {
            "heading": "3.2 Translation Quality Assurance",
            "text": "We verify the translated data quality with both automatic and human evaluation. Due to the high costs of running a full evaluation, we randomly sample 100 utterances from each translated dataset, summing up to 1700 translated utterances per nonEnglish language and a total of 15300 translation pairs5. Both the automatic and human evaluation results suggest a high quality of the translated xDialEval data.\nAutomatic Measures Due to the absence of target language references, we apply direct qual-\n4 https://learn.microsoft.com/en-gb/azure/\ncognitive-services/translator/ 5ConTureE-Turn & ConTurE-Dial share the same data.\nity estimation with three different models: OpenAI\u2019s GPT-4 model (OpenAI, 2023), Unbabel\u2019s wmt22-cometkiwi-da (Rei et al., 2022) and wmt23cometkiwi-da-xl (Rei et al., 2023) models. In addition to quality estimation, we perform additional back-translation of the translated content to English using Azure MT and then conduct a referencebased evaluation comparing the English source and back-translated utterances.\nFor quality estimation with GPT-4, we prompt the model to evaluate the adequacy of each translation pair on a scale of 1 to 5, with 1 denotes poor adequacy and 5 denotes perfect adequacy. The instruction template is outlined in Appendix A. For the reference-based evaluation, we adopt sacreBLEU6 (Papineni et al., 2002), BERTScore7 (Zhang et al., 2020), and BLEURT8 (Sellam et al., 2020) to assess the 15300 English source and back-translation pairs. The automatic evaluation results are summarized in Table 2. We can observe that the scores of the automatic metrics are generally high.\nHuman Evaluation We also conduct the human evaluation to cross-validate the translation quality. Specifically, we collaborate with a service provider to recruit native speakers, who are proficient in both English and their native language, such as Chinese, Spanish, etc. The bilingual speakers are instructed to evaluate the quality of translations from English to their mother tongue. Each speaker assesses 350 translation pairs, rating them on a 1-5 scale, where 1 indicates poor translation and 5 indicates excellent translation. The human evaluation covered nine\n6 https://huggingface.co/spaces/\nevaluate-metric/sacrebleu 7 https://github.com/Tiiiger/bert_score.\n8 https://github.com/google-research/bleurt/\nblob/master/checkpoints.md\nlanguage pairs, totaling 3150 instances. Quality check on a random subset of the human evaluation data is conducted to ensure their reliability. Additionally, for the 350 English-to-Chinese translations, three authors of the paper manually evaluated the translation. The inter-annotator agreement is 0.578, indicating medium agreement. The average human evaluation scores for EN-ZH, EN-ES, ENDE, EN-FR, EN-JA, EN-KO, EN-HI, EN-AR, and EN-RU language pairs are 4.75, 4.68, 4.59, 4.53, 4.39, 4.32, 4.17, 4.41, and 4.71 respectively. The human evaluation results agree with the automatic results, showcasing that the translation quality of xDial-Eval is good."
        },
        {
            "heading": "4 Model-Based Reference-Free Metrics",
            "text": "Mathematical Formulation - Let Di,l denote a dialogue evaluation dataset in xDial-Eval with index i and language l. Di,l either comprises N number of multi-turn dialogues (dialogue-level) or J number of context-response pairs (turn-level). Each data instance can be either a multi-turn dialogue d i,l j or a context-response pair (c i,l j , r i,l j ) \u2208 D i,l where j \u2208 {1, ..., N}. Each di,lj or r i,l j is evaluated by human judges using a predefined Likert scale to measure specific aspects of dialogue or response quality. Given the multifaceted nature of quality and our particular interest in the language generalization of the automatic metrics, we are concentrating our analysis on \"coherence\" at the dialogue level and \"context relevance\" at the turn level, which are the most studied dimensions in the dialogue evaluation literature (Yeh et al., 2021). We will explore other dimensions in future works.\nWe denote the mean quality score assigned by human annotators to a specific data instance as hi,lj . The metric model, M , assigns metric score si,lj to d i,l j or r i,l j . The performance of M on D\ni,l is assessed by computing the Pearson or Spearman correlations, \u03c1i,l, between Si,l = {si,l1 , . . . , s i,l N } and H i,l = {hi,l1 , . . . , h i,l N }. The higher the \u03c1\ni,l, the better the performance of M on Di,l. M with strong evaluation capability achieves high 1\u2223\u2126\u2223 \u2211i\u2208\u2126 \u03c1 i,l where \u2126 is either the collection of turn-level datasets or the collection of dialogue-level datasets in xDial-Eval. The language generalization of M can be assessed by 1\u2223L\u2223 \u2211l\u2208L( 1\u2223\u2126\u2223 \u2211i\u2208\u2126 \u03c1\ni,l) where L is the set of languages covered by xDial-Eval.\nBERT-Based Discrminative Metrics - In this\nstudy, we select two SoTA BERT-based metrics in the open-domain dialogue evaluation literature: PoE (Zhang et al., 2023) and FineD-Eval (Zhang et al., 2022a). Both are self-supervised metrics designed for turn-level and dialogue-level evaluation respectively. Their detailed descriptions are outlined in Appendix C.1. The original PoE and FineD-Eval are based on RoBERTa-Large (354M) and RoBERTa-Base (125M) (Liu et al., 2019) respectively. To adapt them for the multilingual dialogue evaluation task, we reimplement them with XLM-R-Large (550M) and XLM-Rbase (270M) (Conneau et al., 2020) respectively.\nTo finetune the models, we need multilingual training data. The original synthetic data for training PoE comprises approximately 2M English context-response pairs while that of FineD-Eval contains roughly 90K multi-turn dialogues. Both training datasets contain a balanced number of positive and negative data instances9. We employ the open-source NLLB-200-3.3B machine translation model (Costa-juss\u00e0 et al., 2022) to convert the English data into the other 9 languages. For easy reference, we denote the multilingual training datasets as xPoE-Turn and xFined-Dial10. We didn\u2019t use the high-quality Azure MT service for training data translation because the costs are exorbitant and we can also check whether training on multilingual data with lower translation quality causes a signifi-\n9For PoE, positive and negative refer to relevant and irrelevant responses while for FineD-Eval, they refer to coherent and incoherent dialogues.\n10Given that xPoE-Turn and xFined-Dial sizes are 10 times their original English datasets, we sample a subset equal to the original English size for model training.\ncant negative impact on model performance. When scoring di,lj with FineD-Eval or r i,l j with PoE, the flattened token sequence of di,lj or (ci,lj , r i,l j ) is input into FineD-Eval or PoE respectively. The scalar normalized score si,lj in the range [0, 1] is output from the models accordingly.\nLLM-Based Generative Metrics - We select a diverse set of popular LLMs with different backbones and pretraining data. Due to the fast development pace of LLMs, the following list is not exhaustive: LLaMA-7B (Touvron et al., 2023a), LLaMA-27B (Touvron et al., 2023b), Baichuan-2-7B (Yang et al., 2023), Alpaca-7B (Taori et al., 2023), Vicuna7B-V1.1 (Chiang et al., 2023), BLOOM-7B (Scao et al., 2023), Phoenix-7B (Chen et al., 2023b), Falcon-7B (Almazrouei et al., 2023), and OpenAI\u2019s ChatGPT (gpt-3.5-turbo-0301). Due to computation limitations, the larger LLMs variants are not explored in this study. The detailed descriptions of each model are included in the Appendix C.2.\nTo score di,lj or r i,l j with the LLMs, we first need to convert the input instance to the corresponding model- and task-specific instruction-based prompt templates. Take Alpaca as an example, table 3 displays the specific prompts for both dialoguelevel and turn-level evaluation tasks11. Following Gupta et al. (2022), we frame the evaluation task as a binary classification problem. Given an input prompt, we specifically focus on the probabilities related to the label words \"Yes\" and \"No\" as generated by the language model. Then, we\n11Appendix D includes more example instruction templates for other models and languages.\nnormalize the probability of \u201cYes\" as P (Yes) = P (Yes)/(P (Yes) + P (No)) and P (Yes) serves as si,lj of d i,l j or r i,l j . As we do not have access to the output probabilities of the closed-source ChatGPT, we prompt ChatGPT to explicitly provide a numerical score to di,lj or r i,l j on the scale of 1 to 5.\nProposed Metrics - A drawback of LLMs is their lack of specialized focus. While these models are constructed to function as versatile AI assistants capable of managing a variety of NLP tasks, their performance may not measure up to domain-specific experts when dealing with specialized tasks or particular domains. Hence, we propose to further adapt the open-source LLMs to custom data designed for automatic dialogue evaluation.\nAn additional note is that current LLM finetuning often uses human-annotated data. However, due to challenges in scaling up high-quality, humanannotated training data collection for open-domain dialogue evaluation, and the proven success of SoTA BERT-based metrics using automaticallyconstructed synthetic dialogue data, we propose to investigate whether LLMs can also benefit from finetuning with synthetic data.\nSpecifically, we reuse the xPoE-Turn and xFinedDial datasets described in the previous section to perform instruction-tuning of the LLMs. To speed up the experiments, we sample a subset of 100k (10k per language) from each dataset. Subsequently, we transform the data into an instruction format using model-specific prompt templates, similar to those in Table 3. Then, the LLMs are finetuned on the 200K multilingual instruction data using the Low-Rank Adaptation (LoRA) technique (Hu et al., 2022). The dialogue/response scoring process of the finetuned LLMs is the same binary classification formulation discussed above."
        },
        {
            "heading": "5 Experiment Setup",
            "text": "We explore the effects on multilingual dialogue evaluation with both zero-shot prompting and finetuning the LLMs on different types of data12. For LLM finetuning, we first investigate the crosslingual generalization of the LLMs when only finetuned on the English synthetic data. Models with different pretrained backbones are chosen for investigation: PoE, Fined-Eval, Alpaca-7B, Phoenix-7B, LLaMA-2-7B, and Baichuan-2-7B.\nSecondly, we finetune the LLMs using the syn-\n12The computation details are outlined in Appendix F.\nthetic multilingual dialogue data to determine if this strategy improves their language generalization. In particular, we examine two groups of LLMs. The first is vanilla LLMs without instruction tuning, including LLaMA-7B, BLOOM-7B, LLaMA-2-7B, and Baichuan-2-7B. The second group includes the instruction-tuned variants: Alpaca-7B and Phoenix7B. By comparing these two groups after finetuning on the multilingual synthetic data, we study whether a two-stage instruction finetuning is useful, i.e., an LLM is first finetuned on general-purpose instruction data and then further adapted to custom data. Additionally, we also want to find out which 7B open-source LLM possesses the strongest multilingual generalization.\nFurthermore, we explore the ensemble of the finetuned LLMs and the BERT-based discriminative metrics and examine whether their difference in training objectives helps complement each other in their dialogue evaluation abilities. Since the finetuned LLMs and the BERT-based metrics produce scores ranging from 0 to 1 (described in \u00a74), we achieve the ensemble by simply calculating the arithmetic mean of their respective output scores."
        },
        {
            "heading": "6 Results & Analysis",
            "text": "Table 4 and 5 present the key experiment results13."
        },
        {
            "heading": "6.1 Zero-Shot Performance of LLMs",
            "text": "Vanilla vs Instruction-Tuned LLMs - Table 5\u2019s \u201cLLMs-Zeroshot\" category shows that vanilla LLMs exhibit low correlations with human evaluations across all languages. In contrast, instructiontuned LLMs outperform their vanilla counterparts, with better average Pearson correlations at both turn and dialogue levels. For example, Alpaca7B achieves average Pearson improvements of 0.218 and 0.336 over LLaMA-7B at turn and dialogue levels respectively. Similar improvements are seen when comparing Vicuna-7B to LLaMA7B and Phoenix-7B to BLOOM-7B. These significant boosts in performance are credited to the closer alignment of instruction-tuned LLMs with humans\u2019 task-solving abilities.\nImpact of Backbone Models - Baichuan-2-7B and Falcon-7B perform much better than other vanilla\n13Full experiment results on each dataset can be found at https://docs.google.com/spreadsheets/d/ 1w2PoIk2BqlkYEYiZ_LbOEGCS1qTtEVmtlz5Ex9YOQ_M/ edit?usp=sharing and Additional supporting analyses are included in Appedix E\nLLMs in terms of correlation at both turn and dialogue levels, possibly due to its pretraining data\u2019s similarity with xDial-Eval benchmark data. Unlike LLaMA, pretrained on general web text like CommonCrawl and Wikipedia, or BLOOM, pretrained on 498 HuggingFace NLP datasets, Falcon7B uses a blend of filtered web data and curated high-quality corpora like social media conversations (Penedo et al., 2023). Baichuan-2-7B is pretrained on large-scale and diverse multilingual data, totaling 2.6 trillion tokens. Alpaca-7B and Vicuna-7B generally perform better in Latin and Cyrillic languages. However, Phoenix-7B exhibits more consistent performance across languages. For instance, Alpaca-7B achieves > 0.25 average Pearson correlations in English, Spanish, German, French, and Russian, but < 0.2 in other languages at the turn level. Phoenix-7B shows less performance variation. Similar trends are observed at the dialogue level. Differences in language generalization are largely due to their backbone models and instruction-tuning data. BLOOM and Baichuan are multilingual LLMs while LLaMA and Falcon are mainly pretrained on English text. Additionally, Phoenix-7B is finetuned with multilingual instruction data while the other instruction-tuned models are mainly finetuned with English instruction data. In \u00a76.3, we explore whether further finetuning the LLMs on our multilingual dialogue data leads to language generalization improvement. ChatGPT Performance - Without finetuning, ChatGPT consistently excels over other LLMs in all languages, demonstrating its outstanding multilingual evaluation ability. This aligns with prior studies (Chen et al., 2023a). The superior performance is attributed to its instruction-tuning on a stronger foundation model, in terms of both param-\neters and pretraining data. It also benefits from higher quality instruction data for finetuning than models like Alpaca-7B and Phoenix-7B, and further gains from reinforcement learning from human feedback (RLHF), enhancing its alignment with human problem-solving skills."
        },
        {
            "heading": "6.2 Effects of Training on English Data Only",
            "text": "Table 4 shows that all models perform optimally on English evaluation datasets at both turn and dialogue levels, as expected. We can observe that PoE and FineD-Eval perform consistently well across languages. Interestingly, their performance matches their respective multilingual finetuned variants (Table 5), implying that XLM-R is a strong cross-lingual encoder.\nFinetuning the LLMs on the English synthetic data not only brings improvements in English but also in other languages on turn-level datasets. The extent of improvements of different models can differ significantly across various languages. For example, the Alpaca-7B model, as shown in Table 4 and in the \u201cLLMs-Zeroshot\" section of Table 5, sees a more substantial performance improvement in Latin languages (>0.14), compared to Hindi and Arabic (<0.04). On the other hand, Phoenix-7B has a more consistent performance boost across different languages.\nAt the dialogue level, the average correlation score of LLaMA-2-7B after finetuning on the English synthetic data improves by over twice as much compared to when using zero-shot prompting (0.121 -> 0.292). For Baichuan-2-7B, the absolute improvement is around 5% (0.241 -> 0.293). However, the improvement brought by finetuning is less prominent for the instruction-tuned LLMs. For example, a slight improvement from 0.255 to 0.267\nis observed for Phoenix-7B. The performance of Alpaca-7B even drops from 0.336 to 0.291. A possible explanation is that the instruction-tuned LLMs already possess certain knowledge necessary for multi-turn coherence evaluation. Finetuning on the synthetic data doesn\u2019t bring much additional knowledge."
        },
        {
            "heading": "6.3 Effects of Training on Multilingual Data",
            "text": "BERT-Based Metrics - We can observe from Table 5 that both PoE and FineD-Eval are strong metrics for multilingual dialogue evaluation. PoE achieves an average Pearson score of 0.431 at the turn level, outperforming ChatGPT and all the finetuned LLMs, except for LLaMA-2-7B and Baichuan-2-7B. FineD-Eval achieves 0.358 at the\ndialogue level. Its performance is only slightly worse than ChatGPT zero-shot prompting and the finetuned LLaMA-2-7B. Second, the performance of both metrics is quite consistent across different languages. Hence, we can conclude that BERTbased discriminative metrics that work well on English dialogues can also generalize to other languages with strong multilingual encoders and multilingual training data.\nFinetuned LLMs - Comparing \"LLMs-FT\" models with their \"LLMs-Zeroshot\" counterparts shows that finetuning the LLMs with multilingual synthetic dialogue data significantly improves both dialogue evaluation and language generalization. The observation confirms our claim in \u00a71. For instance, LLaMA-7B zero-shot prompting, with an average Pearson correlation of 0.038, performs poorly in all languages. Finetuning LLaMA-7B on the multilingual synthetic data boosts the performance to 0.258 with a nearly 0.2 absolute improvement in most languages. At the dialogue level, LLaMA7B\u2019s average Pearson correlation increases from 0.143 to 0.195. Similar observations can be made w.r.t. other \"LLMs-FT\" models. Notably, finetuning LLaMA-2-7B leads to the most significant improvement at both turn and dialogue levels, from 0.064 to 0.465 and 0.121 to 0.386 respectively.\nWhen comparing Alpaca-7B to LLaMA-7B or Phoenix-7B to BLOOM-7B (in the \u201cLLMs-FT\" category), we can observe that the Alpaca-7B outperforms LLaMA-7B (or Phoenix-7B outperforms BLOOM-7B) by significant margins in all languages at both turn and dialogue levels. The observation suggests that a two-stage finetuning process, i.e., finetuning on general-purpose instruction data followed by finetuning on custom data, helps achieve stronger LLM-based dialogue evaluators.\nAdditionally, we can observe that the improvements of instruction-based LLMs (Alpaca-7B and Phoenix-7B) at the dialogue level are less significant than at the turn level. Such a finding is similar to what we have observed in \u00a76.2. Future work may explore how to introduce high-quality data that carries richer information and benefits the coherence evaluation of multi-turn dialogues.\nLastly, while finetuning with multilingual data boosts LLMs\u2019 performance in all languages, variations still occur depending on their pretrained backbone model. For instance, Alpaca-7B, LLaMA7B, and Falcon-7B, which use an English-focused pretrained backbone, perform optimally in Latin\nlanguages. Meanwhile, Phoenix-7B and Baichuan2-7B, with a multilingual pretrained backbone, display more consistent performance in different languages.\nMetric Ensemble - The ensemble of the LLMs and the BERT-based metrics yields strong multilingual dialogue evaluators at both turn and dialogue levels. The best combinations, LLaMA-2-7B + PoE and LLaMA-2-7B + FineD-Eval outperform ChatGPT by 6.5% and 4.6% in terms of the average Pearson correlations at the turn and dialogue levels respectively, albeit without RLHF and the size of their trainable parameters is less than 7.5B. Finally, we can observe that ensemble generally leads to better multilingual dialogue evaluation capability than individual BERT-based or LLM-based metrics."
        },
        {
            "heading": "7 Conclusion",
            "text": "This paper introduces xDial-Eval, a multilingual dialogue evaluation benchmark featuring 14930 annotated turns and 8691 dialogues in 10 languages. Both automatic and human evaluation validate the high quality of xDial-Eval. Additionally, we examine the performance of BERT-based metrics and emerging LLMs on this benchmark. Key takeaways include: (1) SoTA BERT-based metrics, backed by strong cross-lingual encoders and multilingual training data, can effectively handle multilingual dialogue evaluation. (2) Recent generalpurpose instruction-following LLMs show promise as unified multilingual dialogue evaluators. Future work could refine optimization techniques and use high-quality, task-specific data for fine-tuning the LLMs. (3) Ensembling BERT-based metrics and LLMs outperforms ChatGPT in correlation with human judgment and achieves good language generalization (4) Despite the ensemble approach\u2019s good correlations, multilingual automatic dialogue evaluation remains unsolved. xDial-Eval could serve as a valuable benchmark for tracking future research progress in this field.\nLimitations\nOur investigations involving open-source LLMs are restricted to their 7B variants. Future research should explore whether improving the scale of these LLMs, such as using their respective 13B, 40B, or 65B variants, enhances their capability for zero-shot dialogue evaluation, and whether larger LLMs can better generalize to various languages after fine-tuning with multilingual dialogue data.\nAdditionally, open-domain dialogue evaluation is multi-faceted in nature, i.e., there are many evaluation aspects to consider at both turn and dialogue levels. Our research concentrates primarily on the context relevance of responses and the coherence of multi-turn dialogues respectively. Future studies might consider investigating how to prompt LLMs to assess various aspects, as well as exploring strategies to fine-tune these LLMs for optimization of multi-dimensional evaluation.\nFurthermore, multilingual automatic dialogue evaluation, particularly at the dialogue level, remains an unsolved challenge. Despite the solid performance of BERT-based metrics and LLMs ensembles on xDial-Eval, there\u2019s considerable room for improvement. There are two major limitations:\n(1) Subjectivity in dialogue evaluation - Determining a \"good\" or \"successful\" dialogue can be highly subjective, relying on factors such as the conversation\u2019s goals, participants, and social and cultural context. Sometimes, even human judges find it challenging (Smith et al., 2022). Possible solutions could involve generating expert-annotated dialogue data of high quality or creating custom metrics designed for varying evaluation scenarios.\n(2) Long context modeling - Some dialogues in xDial-Eval can be quite lengthy, exceeding the maximum token limits of open-source LLMs (> 2048 tokens). We currently address this by truncation, which, unfortunately, results in information loss. Future research could focus on improving the modeling of long dialogues.\nEthics Statement\nThe xDial-Eval benchmark originates from publicly accessible English datasets. To support and propel future exploration in the domain of automatic dialogue evaluation, and in harmony with the initiatives of other researchers, we commit to making the data and all our models open-source for future research endeavors."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work is supported by Human Robot Collaborative AI under its AME Programmatic Funding Scheme (Project No. A18A2b0046), the National Natural Science Foundation of China (Grant No. 62271432), and the Internal Project Fund from Shenzhen Research Institute of Big Data under Grant No. T00120220002. This work is also a result of the\nprojects: ASTOUND (101071191 - HORIZONEIC-2021-PATHFINDERCHALLENGES-01) funded by the European Commission, BEWORD (PID2021-126061OB-C43) funded by MCIN/AEI/10.13039/501100011033 and, as appropriate, by \u201cERDF A way of making Europe\u201d, by the \u201cEuropean Union\u201d, and the Research Grants for Young Investigators from Universidad Polit\u00e9cnica de Madrid (GENIUS:APOYO-JOVENES-21TAXTYC-32-K61X37) funded by Comunidad de Madrid. Finally, we also would like to thank Tencent AI Lab for providing support to this work."
        },
        {
            "heading": "A MT Evaluation Prompt Template",
            "text": "Table 6 is an example prompt we use for translation quality evaluation of xDial-Eval data with GPT-4."
        },
        {
            "heading": "B Examples of xDial-Eval Data",
            "text": "Table 7 and Table 8 show example data instances from the DailyDialog-Zhao (Zhao et al., 2020) turn-level and the IEval (Svikhnushina et al., 2022) dialogue-level datasets respectively."
        },
        {
            "heading": "C Model Descriptions",
            "text": "C.1 BERT-Based Discriminative Metrics\nPoE - consists of a pre-trained transformer encoder and a collection of domain-specific sub-metrics. Each sub-metric contains an adapter (Houlsby et al.,\n.\n2019) and a scoring head. It is trained in a multitask manner with large-scale multi-domain dialogue data. PoE scores a response by averaging the scores of all domain-specific sub-metrics.\nFineD-Eval - targets multi-dimensional dialoguelevel evaluation. It consists of three sub-metrics focusing on coherence, likability, and informativeness evaluation respectively. The sub-metrics are trained to rank the good-quality dialogue ahead of its poor-quality counterpart. FineD-Eval scores a dialogue by computing the arithmetic mean of individual sub-metric scores assigned to the dialogue.\nC.2 LLM-Based Generative Metrics\nLLaMA-Series - LLaMA is a pre-trained causal LLM from Meta. It is currently the most popular open-source substitute for the closed-source GPT-3 (Brown et al., 2020). Its smallest variant, LLaMA-7B, is trained on 1 trillion tokens (mainly in English). LLaMA-2 is an optimized version of LLaMA with a better mixture of pretraining data and training strategies. LLaMA-2-7B is pretrained on 2T tokens, which is double that of LLaMA-7B. Alpaca is finetuned from LLaMA-7B on 52K instruction-following demonstrations. The data are generated from OpenAI\u2019s text-davinci-003 model with self-instruct (Wang et al., 2023). Vicuna is finetuned on 70K user-shared conversations collected from ShareGPT. Both models adopt the Low-Rank Adaptation (LoRA) (Hu et al., 2022) approach for finetuning. Both Alpaca and Vicuna\nare developed to mimic the general instructionfollowing capability of ChatGPT.\nBLOOM-Series - BLOOM is a large-scale multilingual causal LLM released in the BigScience workshop. It is pretrained on the ROOTS corpus (Lauren\u00e7on et al., 2022) in 46 natural languages and 13 programming languages. Phoenix is an open-source multilingual ChatGPT-like model that can handle 40+ languages. It is finetuned from\nBLOOMZ14 on mixed multilingual instruction data and conversation data. The instruction data of Phoenix is collected with self-instruction (Wang et al., 2023) while the conversation data is mainly ChatGPT-distilled. As the collected data are mainly in English, they are then translated into other languages to support the post-training process of\n14A variant of BLOOM finetuned on diverse multilingual data, which cover a wide array of NLP tasks, such as question answering, topic classification, and program synthesis.\nPhoenix.\nFalcon - Falcon (Almazrouei et al., 2023) is a largescale pretrained causal language model released by The Technology Innovation Institute. It is trained on 1,500B tokens of RefinedWeb (Penedo et al.,\n2023) enhanced with curated corpora, such as social media conversations, books, and technical papers. The training data is mainly in English and French.\nBaiChuan-2 - BaiChuan-2 (Yang et al., 2023) en-\ncompasses a series of large-scale multilingual language models, consisting of models with 7 billion and 13 billion parameters, and is trained from scratch on a massive dataset comprising 2.6 trillion tokens. The pretraining data is collected from a variety of sources, such as general internet webpages, books, research papers, codebases, and more.\nChatGPT - is a closed-source general-purpose\ninstruction-following conversational AI developed by OpenAI. The training of ChatGPT follows the instructGPT development pipeline (Ouyang and et al., 2022): (1) supervised finetuning from a strong pre-trained large language model on highquality human-collected instruction data; (2) Align the finetuned language model with humans\u2019 intentions and goals leveraging reinforcement from hu-\nman feedback. ChatGPT and its successor, GPT4 (OpenAI, 2023) are currently the most powerful AI assistants that are highly capable of solving various NLP tasks. Many open-source LLMs, such as those described in the LLaMA-series and BLOOMSeries, are trained to mimic their abilities."
        },
        {
            "heading": "D More Examples on Instruction Template",
            "text": "This section presents the instruction templates of Vicuna-7B (Table 9), Phoenix-7B (Table 10), Falcon-7B (Table 11), LLaMA-2-7B (Table 12), Baichuan-2-7B (Table 13), and ChatGPT (Table 14). We try to closely follow the instruction templates of the open-source LLMs used during their supervised finetuning stage."
        },
        {
            "heading": "E Additional Analyses",
            "text": "Table 16 and Table 17 are the corresponding Spearman correlations of Table 4 and Table 5 respectively. Similar observations to the ones in \u00a76 can be made with Table 16 and Table 17.\nNatural Non-English Dialogues - In addition to the translation-based xDial-Eval benchmark, we analyze the performance of finetuned models on organic non-English dialogue data using five Chinese dialogue evaluation datasets15 released by Rodr\u00edguez-Cantelar et al. (2023). Table 15 presents the detailed results. We can observe that Baichuan-2-7B is the best among all models at the turn level while LLaMA-2-7B performs the best at the dialogue level. Similar to the observations in \u00a76.3, the ensemble of BERT-based metrics and LLMs leads to even stronger correlations with human evaluation. The results demonstrate that our proposed metrics are not just proficient in managing translation-based multilingual dialogue data, but they also deliver strong performance when applied to natural non-English data.\nCorrelations Among Languages - In this section, we analyze the interdependence of judgments given to dialogue data in different languages by different models. Specifically, we choose the FED-Turn (Mehri and Eskenazi, 2020a) and FEDDial (Mehri and Eskenazi, 2020a) datasets for analysis. The purpose is to check whether the models can provide consistent judgments to multilingual dialogue data with the same semantic meanings.\n15 https://chateval.org/dstc11\nFigure 1 and 2 show the inter-lingual Pearson correlations of FineD-Eval / PoE, ChatGPT, LLaMA7B, Alpaca-7B, BLOOM-7B, and Phoenix-7B on FED-Dial and FED-Turn respectively.\nWe can observe that FineD-Eval and PoE both display very consistent inter-lingual correlation patterns. The correlations among language pairs are \u223c 0.9 for FineD-Eval and > 0.7 for PoE. The observation showcases that both metrics can provide a consistent evaluation of translation-based multilingual dialogue data. Additionally, we can observe that ChatGPT displays stronger inter-lingual correlations on FED-Dial than on FED-Turn. The observation partially explains why ChatGPT has a more consistent performance across different languages at the dialogue level than at the turn level (refer to Table 5 and Table 17).\nFurthermore, Phoenix-7B and Alpaca-7B exhibit stronger inter-lingual correlations than their respective backbone models, LLaMA-7B and BLOOM7B. The observation supports our conclusion in \u00a76.3 that a two-stage finetuning process (adapt instruction-tuned models on custom multilingual data) yields dialogue evaluators that are more robust across different languages.\nLastly, we observe that the inter-lingual correlations of Alpaca-7B are notably stronger within Latin-language pairs, as compared to other language combinations. In contrast, Phoenix-7B displays a more evenly distributed correlation pattern among various language pairs. This observation reinforces the findings presented in Section 6.3, suggesting that Alpaca-7B\u2019s performance excels in Latin languages, likely due to its English-centric pretrained backbone and initial fine-tuning on English instruction data. On the other hand, Phoenix, equipped with a multilingual pretrained backbone and an initial fine-tuning on multilingual instruction data, exhibits a more uniform performance across all languages.\nCorrelations Among Metrics - We delve deeper into examining the extent of agreement among the evaluations provided by various model-based metrics. To accomplish this, we calculate the correlations between different pairs of metrics, focusing specifically on their performance on turn-level datasets: FED-Turn & DailyDialog-Zhao (Zhao et al., 2020), and dialogue-level datasets: FED-Dial & IEval-Dial (Svikhnushina et al., 2022) in English. This approach allows us to discern whether these metrics complement each other. Figure 3\npresents the inter-metric Pearson correlations. We can observe that the judgments provided by different metrics are not consistent and the correlation patterns differ on different datasets. Furthermore, while the correlations between PoE and Alpaca-7B, and between PoE and Phoenix-7B, are reasonably good, they aren\u2019t excessively strong. This suggests that these pairs of metrics complement each other without being overly similar. This insight hints at the potent performance of an ensemble comprising PoE and Alpaca-7B, or PoE and Phoenix-7B, particularly on turn-level datasets. Similar observations can be made w.r.t. FineD-Eval and Alpaca-7B and FineD-Eval and Phoenix-7B. These insights provide a rationale for the conclusions we drew in the \"Metric Ensemble\" subsection of \u00a7 6.3, where we found that an ensemble of BERT-based metrics and LLMs creates highly effective multilingual\ndialogue evaluators that outperform ChatGPT. F Reproducibility\nComputation Details - All experiments were carried out using a single 40GB A100 GPU card. We\nuse the alpaca-lora library16 for LLM fine-tuning with low-rank adaptation. During finetuning, we utilize a batch size of 128 with 4 gradient accumulation steps. The learning rate, cutoff length, dropout rate, and number of epochs are set to 0.0003, 1024, 0.05, and 3 respectively. For the LoRA parameters, the rank (r) and alpha are set to 8 and 16 corre-\n16 https://github.com/tloen/alpaca-lora\nspondingly. The target module for low-rank adaptation includes the query and key projection matrices. The total number of trainable parameters is approximately 4M, which accounts for around 5.5% to 6% of the LLMs\u2019 full parameter size. The finetuning of each LLM with LoRA takes around 100 hours on 200K data. For training PoE and FineD-Eval, we follow the training procedures and hyperparameter settings introduced in their respective papers."
        }
    ],
    "title": "xDial-Eval: A Multilingual Open-Domain Dialogue Evaluation Benchmark",
    "year": 2023
}