{
    "abstractText": "Grammatical error correction (GEC) is a wellexplored problem in English with many existing models and datasets. However, research on GEC in morphologically rich languages has been limited due to challenges such as data scarcity and language complexity. In this paper, we present the first results on Arabic GEC using two newly developed Transformer-based pretrained sequence-to-sequence models. We also define the task of multi-class Arabic grammatical error detection (GED) and present the first results on multi-class Arabic GED. We show that using GED information as an auxiliary input in GEC models improves GEC performance across three datasets spanning different genres. Moreover, we also investigate the use of contextual morphological preprocessing in aiding GEC systems. Our models achieve SOTA results on two Arabic GEC shared task datasets and establish a strong benchmark on a recently created dataset. We make our code, data, and pretrained models publicly available.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Bashar Alhafni"
        },
        {
            "affiliations": [],
            "name": "Go Inoue"
        },
        {
            "affiliations": [],
            "name": "Christian Khairallah"
        },
        {
            "affiliations": [],
            "name": "Nizar Habash"
        },
        {
            "affiliations": [],
            "name": "Abu Dhabi"
        }
    ],
    "id": "SP:7c523007f089a1bd40026e7a16973f7bd500613e",
    "references": [
        {
            "authors": [
                "Ahmed Abdelali",
                "Sabit Hassan",
                "Hamdy Mubarak",
                "Kareem Darwish",
                "Younes Samih"
            ],
            "title": "Pre-training bert on arabic tweets: Practical considerations",
            "year": 2021
        },
        {
            "authors": [
                "Muhammad Abdul-Mageed",
                "AbdelRahim Elmadany",
                "El Moatez Billah Nagoudi."
            ],
            "title": "ARBERT & MARBERT: Deep bidirectional transformers for Arabic",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
            "year": 2021
        },
        {
            "authors": [
                "Abdullah Alfaifi",
                "Eric Atwell."
            ],
            "title": "Arabic learner corpora (alc): a taxonomy of coding errors",
            "venue": "The 8th International Computing Conference in Arabic.",
            "year": 2012
        },
        {
            "authors": [
                "Abdullah Alfaifi",
                "Eric Atwell",
                "Ghazi Abuhakema"
            ],
            "title": "Error annotation of the Arabic learner corpus",
            "year": 2013
        },
        {
            "authors": [
                "Manar Alkhatib",
                "Azza Abdel Monem",
                "Khaled Shaalan."
            ],
            "title": "Deep learning for Arabic error detection and correction",
            "venue": "ACM Trans. Asian Low-Resour. Lang. Inf. Process., 19(5).",
            "year": 2020
        },
        {
            "authors": [
                "Wissam Antoun",
                "Fady Baly",
                "Hazem Hajj."
            ],
            "title": "AraBERT: Transformer-based model for Arabic language understanding",
            "venue": "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language",
            "year": 2020
        },
        {
            "authors": [
                "Hiroki Asano",
                "Tomoya Mizumoto",
                "Kentaro Inui."
            ],
            "title": "Reference-based metrics can be replaced with reference-less metrics in evaluating grammatical error correction systems",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language",
            "year": 2017
        },
        {
            "authors": [
                "Dana Awad."
            ],
            "title": "La ponctuation en Arabe: histoire et r\u00e8gles",
            "venue": "etude constrative avec le fran\u00e7ais et l\u2019anglais.",
            "year": 2013
        },
        {
            "authors": [
                "Abhijeet Awasthi",
                "Sunita Sarawagi",
                "Rasna Goyal",
                "Sabyasachi Ghosh",
                "Vihari Piratla."
            ],
            "title": "Parallel iterative edit models for local sequence transduction",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Riadh Belkebir",
                "Nizar Habash."
            ],
            "title": "Automatic error type annotation for Arabic",
            "venue": "Proceedings of the 25th Conference on Computational Natural Language Learning, pages 596\u2013606, Online. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Samuel Bell",
                "Helen Yannakoudakis",
                "Marek Rei."
            ],
            "title": "Context is key: Grammatical error detection with contextual word representations",
            "venue": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 103\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Fethi Bougares",
                "Houda Bouamor."
            ],
            "title": "UMMU@QALB-2015 shared task: Character and word level SMT pipeline for automatic error correction of Arabic text",
            "venue": "Proceedings of the Second Workshop on Arabic Natural Language Pro-",
            "year": 2015
        },
        {
            "authors": [
                "Christopher Bryant",
                "Mariano Felice",
                "\u00d8istein E. Andersen",
                "Ted Briscoe."
            ],
            "title": "The BEA-2019 shared task on grammatical error correction",
            "venue": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52\u201375,",
            "year": 2019
        },
        {
            "authors": [
                "Christopher Bryant",
                "Mariano Felice",
                "Ted Briscoe"
            ],
            "title": "Automatic annotation and evaluation of error",
            "year": 2017
        },
        {
            "authors": [
                "Christopher Bryant",
                "Zheng Yuan",
                "Muhammad Reza Qorib",
                "Hannan Cao",
                "Hwee Tou Ng",
                "Ted Briscoe"
            ],
            "title": "Grammatical error correction: A survey of the state of the art",
            "year": 2023
        },
        {
            "authors": [
                "Tim Buckwalter."
            ],
            "title": "Issues in Arabic orthography and morphology analysis",
            "venue": "Proceedings of the Workshop on Computational Approaches to Arabic Script-based Languages, pages 31\u201334, Geneva, Switzerland. COLING.",
            "year": 2004
        },
        {
            "authors": [
                "Martin Chodorow",
                "Joel Tetreault",
                "Na-Rae Han."
            ],
            "title": "Detection of grammatical errors involving prepositions",
            "venue": "Proceedings of the Fourth ACLSIGSEM Workshop on Prepositions, pages 25\u201330, Prague, Czech Republic. Association for Computa-",
            "year": 2007
        },
        {
            "authors": [
                "Leshem Choshen",
                "Dmitry Nikolaev",
                "Yevgeni Berzak",
                "Omri Abend."
            ],
            "title": "Classifying syntactic errors in learner language",
            "venue": "Proceedings of the 24th Conference on Computational Natural Language Learning, pages 97\u2013107, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V. Le",
                "Christopher D. Manning."
            ],
            "title": "Electra: Pre-training text encoders as discriminators rather than generators",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "C. o. E. Council of Europe."
            ],
            "title": "Common european framework of reference for languages: learning, teaching, assessment",
            "venue": "Cambridge University Press.",
            "year": 2001
        },
        {
            "authors": [
                "Daniel Dahlmeier",
                "Hwee Tou Ng."
            ],
            "title": "Grammatical error correction with alternating structure optimization",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 915\u2013923, Port-",
            "year": 2011
        },
        {
            "authors": [
                "Daniel Dahlmeier",
                "Hwee Tou Ng."
            ],
            "title": "Better evaluation for grammatical error correction",
            "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2012
        },
        {
            "authors": [
                "Daniel Dahlmeier",
                "Hwee Tou Ng",
                "Siew Mei Wu."
            ],
            "title": "Building a large annotated corpus of learner English: The NUS corpus of learner English",
            "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages",
            "year": 2013
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Fadhl Eryani",
                "Nizar Habash",
                "Houda Bouamor",
                "Salam Khalifa."
            ],
            "title": "A spelling correction corpus for multiple Arabic dialects",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4130\u20134138, Marseille, France. European",
            "year": 2020
        },
        {
            "authors": [
                "Ramy Eskander",
                "Nizar Habash",
                "Owen Rambow",
                "Nadi Tomeh."
            ],
            "title": "Processing spontaneous orthography",
            "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 585\u2013595, Atlanta,",
            "year": 2013
        },
        {
            "authors": [
                "Tao Fang",
                "Shu Yang",
                "Kaixin Lan",
                "Derek F. Wong",
                "Jinpeng Hu",
                "Lidia S. Chao",
                "Yue Zhang"
            ],
            "title": "Is chatgpt a highly fluent grammatical error correction system? a comprehensive evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Noura Farra",
                "Nadi Tomeh",
                "Alla Rozovskaya",
                "Nizar Habash."
            ],
            "title": "Generalized character-level spelling error correction",
            "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 161\u2013167,",
            "year": 2014
        },
        {
            "authors": [
                "Mariano Felice",
                "Ted Briscoe."
            ],
            "title": "Towards a standard evaluation method for grammatical error detection and correction",
            "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2015
        },
        {
            "authors": [
                "Mariano Felice",
                "Zheng Yuan",
                "\u00d8istein E. Andersen",
                "Helen Yannakoudakis",
                "Ekaterina Kochmar."
            ],
            "title": "Grammatical error correction using hybrid systems and type filtering",
            "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language",
            "year": 2014
        },
        {
            "authors": [
                "Charles F Ferguson."
            ],
            "title": "Diglossia",
            "venue": "Word, 15(2):325\u2013 340.",
            "year": 1959
        },
        {
            "authors": [
                "Roman Grundkiewicz",
                "Marcin Junczys-Dowmunt",
                "Kenneth Heafield."
            ],
            "title": "Neural grammatical error correction systems with unsupervised pre-training on synthetic data",
            "venue": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building",
            "year": 2019
        },
        {
            "authors": [
                "Nizar Habash",
                "Mona Diab",
                "Owen Rambow."
            ],
            "title": "Conventional orthography for dialectal Arabic",
            "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC\u201912), pages 711\u2013718, Istanbul, Turkey. European Lan-",
            "year": 2012
        },
        {
            "authors": [
                "Zalmout",
                "Sara Hassan",
                "Faisal Al shargi",
                "Sakhar Alkhereyf",
                "Basma Abdulkareem",
                "Ramy Eskander",
                "Mohammad Salameh",
                "Hind Saddiki."
            ],
            "title": "Unified Guidelines and Resources for Arabic Dialect Orthography",
            "venue": "Proceedings of the International",
            "year": 2018
        },
        {
            "authors": [
                "Nizar Habash",
                "David Palfreyman."
            ],
            "title": "ZAEBUC: An annotated Arabic-English bilingual writer corpus",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 79\u201388, Marseille, France. European Language Resources Association.",
            "year": 2022
        },
        {
            "authors": [
                "Nizar Habash",
                "Owen Rambow",
                "Mona Diab",
                "Reem Kanjawi-Faraj."
            ],
            "title": "Guidelines for Annotation of Arabic Dialectness",
            "venue": "Proceedings of the Workshop on HLT & NLP within the Arabic World, Marrakech, Morocco.",
            "year": 2008
        },
        {
            "authors": [
                "Nizar Habash",
                "Ryan Roth."
            ],
            "title": "Using deep morphology to improve automatic error detection in Arabic handwriting recognition",
            "venue": "Proceedings of the Conference of the Association for Computational Linguistics (ACL), pages 875\u2013884, Portland, Oregon,",
            "year": 2011
        },
        {
            "authors": [
                "Nizar Habash",
                "Abdelhadi Soudi",
                "Tim Buckwalter."
            ],
            "title": "On Arabic Transliteration",
            "venue": "A. van den Bosch and A. Soudi, editors, Arabic Computational Morphology: Knowledge-based and Empirical Methods, pages 15\u201322. Springer, Netherlands.",
            "year": 2007
        },
        {
            "authors": [
                "Nizar Y Habash."
            ],
            "title": "Introduction to Arabic natural language processing, volume 3",
            "venue": "Morgan & Claypool Publishers.",
            "year": 2010
        },
        {
            "authors": [
                "Go Inoue",
                "Bashar Alhafni",
                "Nurpeiis Baimukan",
                "Houda Bouamor",
                "Nizar Habash."
            ],
            "title": "The interplay of variant, size, and task type in Arabic pre-trained language models",
            "venue": "Proceedings of the Sixth Arabic Natural Language Processing Workshop, pages 92\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Go Inoue",
                "Salam Khalifa",
                "Nizar Habash."
            ],
            "title": "Morphosyntactic tagging with pre-trained language models for Arabic and its dialects",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1708\u20131719, Dublin, Ireland. Association for",
            "year": 2022
        },
        {
            "authors": [
                "Marcin Junczys-Dowmunt",
                "Roman Grundkiewicz."
            ],
            "title": "The AMU system in the CoNLL-2014 shared task: Grammatical error correction by data-intensive and feature-rich statistical machine translation",
            "venue": "Proceedings of the Eighteenth Conference on Com-",
            "year": 2014
        },
        {
            "authors": [
                "Marcin Junczys-Dowmunt",
                "Roman Grundkiewicz."
            ],
            "title": "Phrase-based machine translation is state-ofthe-art for automatic grammatical error correction",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2016
        },
        {
            "authors": [
                "Marcin Junczys-Dowmunt",
                "Roman Grundkiewicz",
                "Shubha Guha",
                "Kenneth Heafield."
            ],
            "title": "Approaching neural grammatical error correction as a low-resource machine translation task",
            "venue": "Proceedings of the 2018 Conference of the North Ameri-",
            "year": 2018
        },
        {
            "authors": [
                "Moussa Kamal Eddine",
                "Nadi Tomeh",
                "Nizar Habash",
                "Joseph Le Roux",
                "Michalis Vazirgiannis."
            ],
            "title": "AraBART: a pretrained Arabic sequence-to-sequence model for abstractive summarization",
            "venue": "Proceedings of the The Seventh Arabic Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Masahiro Kaneko",
                "Mamoru Komachi"
            ],
            "title": "Multihead multi-layer attention to deep language representations for grammatical error detection",
            "year": 2019
        },
        {
            "authors": [
                "Satoru Katsumata",
                "Mamoru Komachi."
            ],
            "title": "Stronger baselines for grammatical error correction using a pretrained encoder-decoder model",
            "venue": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Ekaterina Kochmar",
                "\u00d8istein Andersen",
                "Ted Briscoe."
            ],
            "title": "HOO 2012 error recognition and correction shared task: Cambridge University submission report",
            "venue": "Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages",
            "year": 2012
        },
        {
            "authors": [
                "Wuwei Lan",
                "Yang Chen",
                "Wei Xu",
                "Alan Ritter."
            ],
            "title": "An empirical study of pre-trained transformers for Arabic information extraction",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4727\u20134734,",
            "year": 2020
        },
        {
            "authors": [
                "V.I. Levenshtein."
            ],
            "title": "Binary Codes Capable of Correcting Deletions, Insertions and Reversals",
            "venue": "Soviet Physics Doklady, 10:707.",
            "year": 1966
        },
        {
            "authors": [
                "Zuchao Li",
                "Kevin Parnow",
                "Hai Zhao."
            ],
            "title": "Incorporating rich syntax information in grammatical error correction",
            "venue": "Information Processing & Management, 59(3):102891.",
            "year": 2022
        },
        {
            "authors": [
                "Nora Madi",
                "Hend Al-Khalifa."
            ],
            "title": "Error detection for Arabic text using neural sequence labeling",
            "venue": "Applied Sciences, 10(15).",
            "year": 2020
        },
        {
            "authors": [
                "Koki Maeda",
                "Masahiro Kaneko",
                "Naoaki Okazaki."
            ],
            "title": "IMPARA: Impact-based metric for GEC using parallel data",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 3578\u20133588, Gyeongju, Republic of Korea. In-",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Mallinson",
                "Jakub Adamek",
                "Eric Malmi",
                "Aliaksei Severyn."
            ],
            "title": "EdiT5: Semi-autoregressive text editing with t5 warm-start",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2126\u20132138, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Mallinson",
                "Aliaksei Severyn",
                "Eric Malmi",
                "Guillermo Garrido."
            ],
            "title": "FELIX: Flexible text editing through tagging and insertion",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1244\u20131255, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Eric Malmi",
                "Sebastian Krause",
                "Sascha Rothe",
                "Daniil Mirylenka",
                "Aliaksei Severyn."
            ],
            "title": "Encode, tag, realize: High-precision text editing",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Stuart Mesham",
                "Christopher Bryant",
                "Marek Rei",
                "Zheng Yuan"
            ],
            "title": "An extended sequence tagging vocabulary for grammatical error correction",
            "year": 2023
        },
        {
            "authors": [
                "Rada Mihalcea",
                "Ted Pedersen."
            ],
            "title": "An evaluation exercise for word alignment",
            "venue": "Proceedings of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond, pages 1\u201310.",
            "year": 2003
        },
        {
            "authors": [
                "Behrang Mohit",
                "Alla Rozovskaya",
                "Nizar Habash",
                "Wajdi Zaghouani",
                "Ossama Obeid."
            ],
            "title": "The first QALB shared task on automatic text correction for Arabic",
            "venue": "Proceedings of the EMNLP 2014 Workshop on Arabic Natural Language Processing",
            "year": 2014
        },
        {
            "authors": [
                "El Moatez Billah Nagoudi",
                "AbdelRahim Elmadany",
                "Muhammad Abdul-Mageed."
            ],
            "title": "AraT5: Text-totext transformers for Arabic language generation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2022
        },
        {
            "authors": [
                "Courtney Napoles",
                "Keisuke Sakaguchi",
                "Matt Post",
                "Joel Tetreault."
            ],
            "title": "Ground truth for grammatical error correction metrics",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Confer-",
            "year": 2015
        },
        {
            "authors": [
                "Courtney Napoles",
                "Keisuke Sakaguchi",
                "Joel Tetreault."
            ],
            "title": "There\u2019s no comparison: Referenceless evaluation metrics in grammatical error correction",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Process-",
            "year": 2016
        },
        {
            "authors": [
                "Michael Nawar."
            ],
            "title": "CUFE@QALB-2015 shared task: Arabic error correction system",
            "venue": "Proceedings of the Second Workshop on Arabic Natural Language",
            "year": 2015
        },
        {
            "authors": [
                "Hwee Tou Ng",
                "Siew Mei Wu",
                "Ted Briscoe",
                "Christian Hadiwinoto",
                "Raymond Hendy Susanto",
                "Christopher Bryant."
            ],
            "title": "The CoNLL-2014 shared task on grammatical error correction",
            "venue": "Proceedings of the Eighteenth Conference on Computational Natu-",
            "year": 2014
        },
        {
            "authors": [
                "Hwee Tou Ng",
                "Siew Mei Wu",
                "Yuanbin Wu",
                "Christian Hadiwinoto",
                "Joel Tetreault."
            ],
            "title": "The CoNLL2013 shared task on grammatical error correction",
            "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared",
            "year": 2013
        },
        {
            "authors": [
                "Ossama Obeid",
                "Nasser Zalmout",
                "Salam Khalifa",
                "Dima Taji",
                "Mai Oudah",
                "Bashar Alhafni",
                "Go Inoue",
                "Fadhl Eryani",
                "Alexander Erdmann",
                "Nizar Habash."
            ],
            "title": "CAMeL tools: An open source python toolkit for Arabic natural language processing",
            "venue": "Proceedings",
            "year": 2020
        },
        {
            "authors": [
                "Franz Josef Och",
                "Hermann Ney."
            ],
            "title": "A systematic comparison of various statistical alignment models",
            "venue": "Computational Linguistics, 29(1):19\u201351.",
            "year": 2003
        },
        {
            "authors": [
                "Kostiantyn Omelianchuk",
                "Vitaliy Atrasevych",
                "Artem Chernodub",
                "Oleksandr Skurzhanskyi."
            ],
            "title": "GECToR \u2013 grammatical error correction: Tag, not rewrite",
            "venue": "Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational",
            "year": 2020
        },
        {
            "authors": [
                "Marek Rei",
                "Helen Yannakoudakis."
            ],
            "title": "Compositional sequence labeling models for error detection in learner writing",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1181\u20131191,",
            "year": 2016
        },
        {
            "authors": [
                "Sascha Rothe",
                "Jonathan Mallinson",
                "Eric Malmi",
                "Sebastian Krause",
                "Aliaksei Severyn."
            ],
            "title": "A simple recipe for multilingual grammatical error correction",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th",
            "year": 2021
        },
        {
            "authors": [
                "Alla Rozovskaya",
                "Houda Bouamor",
                "Nizar Habash",
                "Wajdi Zaghouani",
                "Ossama Obeid",
                "Behrang Mohit."
            ],
            "title": "The second QALB shared task on automatic text correction for Arabic",
            "venue": "Proceedings of the Second Workshop on Arabic Natural Language Pro-",
            "year": 2015
        },
        {
            "authors": [
                "Alla Rozovskaya",
                "Nizar Habash",
                "Ramy Eskander",
                "Noura Farra",
                "Wael Salloum."
            ],
            "title": "The Columbia system in the QALB-2014 shared task on Arabic error correction",
            "venue": "Proceedings of the EMNLP 2014",
            "year": 2014
        },
        {
            "authors": [
                "Alla Rozovskaya",
                "Dan Roth."
            ],
            "title": "Joint learning and inference for grammatical error correction",
            "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 791\u2013 802, Seattle, Washington, USA. Association for Com-",
            "year": 2013
        },
        {
            "authors": [
                "Ali Safaya",
                "Moutasem Abdullatif",
                "Deniz Yuret."
            ],
            "title": "KUISAIL at SemEval-2020 task 12: BERTCNN for offensive speech identification in social media",
            "venue": "Proceedings of the Fourteenth Workshop on Semantic Evaluation, pages 2054\u20132059, Barcelona",
            "year": 2020
        },
        {
            "authors": [
                "Aiman Solyman",
                "Zhenyu Wang",
                "Qian Tao",
                "Arafat Abdulgader Mohammed Elhag",
                "Rui Zhang",
                "Zeinab Mahmoud"
            ],
            "title": "Automatic Arabic grammatical error correction based on expectationmaximization routing and target-bidirectional",
            "year": 2022
        },
        {
            "authors": [
                "Aiman Solyman",
                "Marco Zappatore",
                "Wang Zhenyu",
                "Zeinab Mahmoud",
                "Ali Alfatemi",
                "Ashraf Osman Ibrahim",
                "Lubna Abdelkareim Gabralla."
            ],
            "title": "Optimizing the impact of data augmentation for lowresource grammatical error correction",
            "venue": "Journal of",
            "year": 2023
        },
        {
            "authors": [
                "Aiman Solyman",
                "Wang Zhenyu",
                "Tao Qian",
                "Arafat Abdulgader Mohammed Elhag",
                "Muhammad Toseef",
                "Zeinab Aleibeid."
            ],
            "title": "Synthetic data with neural machine translation for automatic correction in Arabic grammar",
            "venue": "Egyptian Informatics Journal,",
            "year": 2021
        },
        {
            "authors": [
                "Felix Stahlberg",
                "Shankar Kumar."
            ],
            "title": "Seq2Edits: Sequence transduction using span-level edit operations",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5147\u20135159, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Milan Straka",
                "Jakub N\u00e1plava",
                "Jana Strakov\u00e1."
            ],
            "title": "Character transformations for non-autoregressive GEC tagging",
            "venue": "Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021), pages 417\u2013422, Online. Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Dima Taji",
                "Jamila El Gizuli",
                "Nizar Habash."
            ],
            "title": "An Arabic dependency treebank in the travel domain",
            "venue": "Proceedings of the Workshop on OpenSource Arabic Corpora and Processing Tools (OSACT), Miyazaki, Japan.",
            "year": 2018
        },
        {
            "authors": [
                "Joel R. Tetreault",
                "Martin Chodorow."
            ],
            "title": "The ups and downs of preposition error detection in ESL writing",
            "venue": "Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 865\u2013872, Manchester, UK. Coling 2008",
            "year": 2008
        },
        {
            "authors": [
                "Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Watson",
                "Nasser Zalmout",
                "Nizar Habash."
            ],
            "title": "Utilizing character and word embeddings for text normalization with sequence-to-sequence models",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u2019emi Louf",
                "Morgan Funtowicz",
                "Jamie Brew"
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "year": 2019
        },
        {
            "authors": [
                "Ziang Xie",
                "Anand Avati",
                "Naveen Arivazhagan",
                "Dan Jurafsky",
                "Andrew Y. Ng"
            ],
            "title": "Neural language correction with character-based attention",
            "year": 2016
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in Neural Information Processing Systems, volume 32. Curran Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Helen Yannakoudakis",
                "Ted Briscoe",
                "Ben Medlock."
            ],
            "title": "A new dataset and method for automatically grading ESOL texts",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2011
        },
        {
            "authors": [
                "Zheng Yuan",
                "Ted Briscoe."
            ],
            "title": "Grammatical error correction using neural machine translation",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2016
        },
        {
            "authors": [
                "Zheng Yuan",
                "Christopher Bryant."
            ],
            "title": "Documentlevel grammatical error correction",
            "venue": "Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, pages 75\u201384, Online. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Zheng Yuan",
                "Felix Stahlberg",
                "Marek Rei",
                "Bill Byrne",
                "Helen Yannakoudakis."
            ],
            "title": "Neural and FSTbased approaches to grammatical error correction",
            "venue": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applica-",
            "year": 2019
        },
        {
            "authors": [
                "Zheng Yuan",
                "Shiva Taslimipoor",
                "Christopher Davis",
                "Christopher Bryant."
            ],
            "title": "Multi-class grammatical error detection for correction: A tale of two systems",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Wajdi Zaghouani",
                "Dana Awad"
            ],
            "title": "Toward an Arabic punctuated corpus: Annotation guidelines",
            "year": 2016
        },
        {
            "authors": [
                "Wajdi Zaghouani",
                "Nizar Habash",
                "Houda Bouamor",
                "Alla Rozovskaya",
                "Behrang Mohit",
                "Abeer Heider",
                "Kemal Oflazer."
            ],
            "title": "Correction annotation for non-native arabic texts: Guidelines and corpus",
            "venue": "Proceedings of the Linguistic Annotation Workshop",
            "year": 2015
        },
        {
            "authors": [
                "Wajdi Zaghouani",
                "Behrang Mohit",
                "Nizar Habash",
                "Ossama Obeid",
                "Nadi Tomeh",
                "Alla Rozovskaya",
                "Noura Farra",
                "Sarah Alkuhlani",
                "Kemal Oflazer."
            ],
            "title": "Large scale Arabic error annotation: Guidelines and framework",
            "venue": "Proceedings of the Ninth International",
            "year": 2014
        },
        {
            "authors": [
                "Yue Zhang",
                "Bo Zhang",
                "Zhenghua Li",
                "Zuyi Bao",
                "Chen Li",
                "Min Zhang."
            ],
            "title": "SynGEC: Syntax-enhanced grammatical error correction with a tailored GECoriented parser",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Wei Zhao",
                "Liang Wang",
                "Kewei Shen",
                "Ruoyu Jia",
                "Jingming Liu."
            ],
            "title": "Improving grammatical error correction via pre-training a copy-augmented architecture with unlabeled data",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "English grammatical error correction (GEC) has witnessed significant progress in recent years due to increased research efforts and the organization of several shared tasks (Ng et al., 2013, 2014; Bryant et al., 2019). Most state-of-the-art (SOTA) GEC systems borrow modeling ideas from neural machine translation (MT) to translate from erroneous to corrected texts. In contrast, grammatical error detection (GED), which focuses on locating and identifying errors in text, is usually treated as a sequence labeling task. Both tasks have evident pedagogical benefits to native (L1) and foreign (L2) language teachers and students. Also, modeling GED information explicitly within GEC systems yields better results in English (Yuan et al., 2021).\nWhen it comes to morphologically rich languages, GEC and GED have not received as much\n1https://github.com/CAMeL-Lab/arabic-gec\nattention, largely due to the lack of datasets and standardized error type annotations. Specifically for Arabic, the focus on GEC started with the QALB-2014 (Mohit et al., 2014) and QALB-2015 (Rozovskaya et al., 2015) shared tasks; however, recent sequence-to-sequence (Seq2Seq) modeling advances have not been explored much in Arabic GEC. Moreover, multi-class Arabic GED has not been investigated due to the lack of error type information in Arabic GEC datasets. In this paper, we try to address these challenges. Our main contributions are as follows:\n1. We are the first to benchmark newly developed pretrained Seq2Seq models on Arabic GEC.\n2. We tackle the task of Arabic GED by introducing word-level GED labels for existing Arabic GEC datasets, and present the first results on multi-class Arabic GED.\n3. We systematically show that using GED information in GEC models improves performance across GEC datasets in different domains.\n4. We leverage contextual morphological preprocessing in improving GEC performance.\n5. We achieve SOTA results on two (L1 and L2) previously published Arabic GEC datasets. We also establish a strong benchmark on a recently created L1 Arabic GEC dataset."
        },
        {
            "heading": "2 Related Work",
            "text": "GEC Approaches Early efforts focused on building feature-based machine learning (ML) classifiers to fix common error types (Chodorow et al., 2007; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011; Kochmar et al., 2012; Rozovskaya and Roth, 2013; Farra et al., 2014). Such models required feature engineering and lacked the ability to correct all error types simultaneously.\nReformulating GEC as a monolingual MT task alleviated these issues, first with statistical MT ap-\nproaches (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014, 2016) and then neural MT approaches (Yuan and Briscoe, 2016; Xie et al., 2016; Junczys-Dowmunt et al., 2018; Watson et al., 2018), with Transformer-based models being the most dominant (Yuan et al., 2019; Zhao et al., 2019; Grundkiewicz et al., 2019; Katsumata and Komachi, 2020; Yuan and Bryant, 2021).\nMore recently, edit-based models have been proposed to solve GEC (Awasthi et al., 2019; Malmi et al., 2019; Stahlberg and Kumar, 2020; Mallinson et al., 2020; Omelianchuk et al., 2020; Straka et al., 2021; Mallinson et al., 2022; Mesham et al., 2023). While Seq2Seq models generate corrections to erroneous input, edit-based models generate a sequence of corrective edit operations. Edit-based models add explainability to GEC and improve inference time efficiency. However, they generally require human engineering to define the size and scope of the edit operations (Bryant et al., 2023).\nGED Approaches Rei and Yannakoudakis (2016) presented the first GED results using a neural approach framing GED as a binary (correct/incorrect) sequence tagging problem. Others used pretrained language models (PLMs) such as BERT (Devlin et al., 2019), ELECTRA (Clark et al., 2020), and XLNeT (Yang et al., 2019) to improve binary GED (Bell et al., 2019; Kaneko and Komachi, 2019; Yuan et al., 2021; Rothe et al., 2021). Zhao et al. (2019) and Yuan et al. (2019) demonstrated that combining GED and GEC yields improved results: they used multi-task learning to add token-level and sentence-level GED as auxiliary tasks when training for GEC. Similarly, Yuan et al. (2021) showed that binary and multi-class GED improves GEC.\nArabic GEC and GED The Qatar Arabic Language Bank (QALB) project (Zaghouani et al., 2014, 2015) organized the first Arabic GEC shared tasks: QALB-2014 (L1) (Mohit et al., 2014) and QALB-2015 (L1 and L2) (Rozovskaya et al., 2015). Recently, Habash and Palfreyman (2022) created the ZAEBUC corpus, a new L1 Arabic GEC corpus of essays written by university students. We report on all of these sets.\nArabic GEC modeling efforts ranged from feature-based ML classifiers to statistical MT models (Rozovskaya et al., 2014; Bougares and Bouamor, 2015; Nawar, 2015). Watson et al. (2018) introduced the first character-level Seq2Seq\nmodel and achieved SOTA results on the L1 Arabic GEC data used in the QALB-2014 and 2015 shared tasks. Recently, vanilla Transformers (Vaswani et al., 2017) were explored for synthetic data generation to improve L1 Arabic GEC and were tested on the L1 data of the QALB-2014 and 2015 shared tasks (Solyman et al., 2021, 2022, 2023). To the best of our knowledge, the last QALB-2015 L2 reported results were presented in the shared task itself. We compare our systems against the best previously developed models whenever feasible.\nA number of researchers reported on Arabic binary GED. Habash and Roth (2011) used featureengineered SVM classifiers to detect Arabic handwriting recognition errors. Alkhatib et al. (2020) and Madi and Al-Khalifa (2020) used LSTM-based classifiers. None of them used any of the publicly available GEC datasets mentioned above to train and test their systems. In our work, we explore multi-class GED by obtaining error type annotations from ARETA (Belkebir and Habash, 2021), an automatic error type annotation tool for MSA. To our knowledge, we are the first to report on Arabic multi-class GED. We report on publicly available data to enable future comparisons."
        },
        {
            "heading": "3 Background",
            "text": ""
        },
        {
            "heading": "3.1 Arabic Linguistic Facts",
            "text": "Modern Standard Arabic (MSA) is the official form of Arabic primarily used in education and media across the Arab world. MSA coexists in a diglossic (Ferguson, 1959) relationship with local Arabic dialects that are used for daily interactions. When native speakers write in MSA, there is frequent code-mixing with the dialects in terms of phonological, morphological, and lexical choices (Habash et al., 2008). In this paper, we focus on MSA GEC. While its orthography is standardized, written Arabic suffers many orthographic inconsistencies even\nin professionally written news articles (Buckwalter, 2004; Habash et al., 2012). For example, hamzated Alifs ( @ \u00c2, @ A\u030c)2 are commonly confused with the\nun-hamzated letter ( @ A), and the word-final letters \u00f8\ny and \u00f8 \u00fd are often used interchangeably. These errors affect 11% of all words (4.5 errors per sentence) in the Penn Arabic Treebank (Habash, 2010). Additionally, the use of punctuation in Arabic is very inconsistent, and omitting punctuation marks is very frequent (Awad, 2013; Zaghouani and Awad, 2016). Punctuation errors constitute \u223c40% of errors in the QALB-2014 GEC shared task. This is ten times higher than punctuation errors found in the English data used in the CoNLL-2013 GEC shared task (Ng et al., 2013). Arabic has a large vocabulary size resulting from its rich morphology, which inflects for gender, number, person, case, state, mood, voice, and aspect, and cliticizes numerous particles and pronouns. Arabic\u2019s diglossia, orthographic inconsistencies, and morphological richness pose major challenges to GEC models."
        },
        {
            "heading": "3.2 Arabic GEC Data",
            "text": "We report on three publicly available Arabic GEC datasets. The first two come from the QALB2014 (Mohit et al., 2014) and QALB-2015 (Rozovskaya et al., 2015) shared tasks. The third is the newly created ZAEBUC dataset (Habash and Palfreyman, 2022). None of them were manually annotated for specific error types. Table 1 presents a summary of the dataset statistics. Detailed dataset statistics are presented in Appendix B.\nQALB-2014 consists of native/L1 user comments from the Aljazeera news website, whereas QALB-2015 consists of essays written by Arabic L2 learners with various levels of proficiency. Both datasets have publicly available training (Train), development (Dev), and test (Test) splits. The ZAEBUC dataset comprises essays written by native Arabic speakers, which were manually corrected and annotated for writing proficiency using the Common European Framework of Reference (CEFR) (Council of Europe, 2001). Since the ZAEBUC dataset did not have standard splits, we randomly split it into Train (70%), Dev (15%), and Test (15%), while keeping a balanced distribution of CEFR levels.\nThe three sets vary in a number of dimensions: domain, level, number of words, percentage of erroneous words, and types of errors. Appendix C\n2Arabic HSB transliteration (Habash et al., 2007).\npresents automatic error type distributions over the training portions of the three datasets. Orthographic errors are more common in the L1 datasets (QALB-2014 and ZAEBUC) compared to the L2 dataset (QALB-2015). In contrast, morphological, syntactic, and semantic errors are more common in QALB-2015. Punctuation errors are more common in QALB-2014 and QALB-2015 compared with ZAEBUC."
        },
        {
            "heading": "3.3 Metrics for GEC and GED",
            "text": "GEC systems are most commonly evaluated using reference-based metrics such as the MaxMatch (M2) scorer (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017), and GLEU (Napoles et al., 2015), among other reference-based and referenceless metrics (Felice and Briscoe, 2015; Napoles et al., 2016; Asano et al., 2017; Choshen et al., 2020; Maeda et al., 2022). In this work, we use the M2 scorer because it is language agnostic and was the main evaluation metric used in previous work on Arabic GEC. The M2 scorer compares hypothesis edits made by a GEC system against annotated reference edits and calculates the precision (P), recall (R), and F0.5. In terms of GED, we follow previous work (Bell et al., 2019; Kaneko and Komachi, 2019; Yuan et al., 2021) and use macro precision (P), recall (R), and F0.5 for evaluation. We also report accuracy."
        },
        {
            "heading": "4 Arabic Grammatical Error Detection",
            "text": "Most of the work on GED has focused on English (\u00a72), where error type annotations are provided manually (Yannakoudakis et al., 2011; Dahlmeier et al., 2013) or obtained automatically using an error type annotation tool such as ERRANT (Bryant et al., 2017). However, when it comes to morphologically rich languages such as Arabic, GED remains a challenge. This is largely due to the lack of manually annotated data and standardized error type frameworks. In this work, we treat GED as a mutli-class sequence labeling task. We present a method to automatically obtain error type annotations by extracting edits from parallel erroneous and corrected sentences and then passing them to an Arabic error type annotation tool. To the best of our knowledge, this is the first work that explores multi-class GED in Arabic."
        },
        {
            "heading": "4.1 Edit Extraction",
            "text": "Before automatically labeling each erroneous sentence token, we need to align the erroneous and\ncorrected sentence pairs to locate the positions of all edits so as to map errors to corrections. This step is usually referred to as edit extraction in GEC literature (Bryant et al., 2017).\nWe first obtain character-level alignment between the erroneous and corrected sentence pair by computing the weighted Levenshtein edit distance (Levenshtein, 1966) for each pair of tokens in the two sentences. The output of this alignment is a sequence of token-level edit operations representing the minimum number of insertions, deletions, and replacements needed to transform one token into another. Each of these operations involves one token at most belonging to either sentence. However, some errors may involve more than one single edit operation. To capture multi-token edits, we extend the alignment to cover merges and splits by implementing an iterative algorithm that greedily merges or splits adjacent tokens such that the overall cumulative edit distance is minimized."
        },
        {
            "heading": "4.2 Error Type Annotation",
            "text": "Next, we pass the extracted edits to an automatic annotation tool to label them with specific error types. We use ARETA, an automatic error type annotation tool for MSA (Belkebir and Habash, 2021). Internally, ARETA is built using a combination of rule-based components and an Arabic morphological analyzer (Taji et al., 2018; Obeid et al., 2020). It uses the error taxonomy of the Arabic Learner Corpus (ALC) (Alfaifi and Atwell, 2012; Alfaifi\net al., 2013) which defines seven error classes covering orthography (O), morphology (M), syntax (X), semantics (S), punctuation (P), merges, and splits. The error classes are further differentiated into 32 error tags that can be assigned individually or in combination.\nARETA comes with its own alignment algorithm that extracts edits, however, it does not handle many-to-one and many-to-many edit operations (Belkebir and Habash, 2021). We replace ARETA\u2019s internal alignment algorithm with ours to increase the coverage of error typing. Using our edit extraction algorithm with ARETA enables us to automatically annotate single-token and multi-token edits with various error types. Appendix C presents the error types obtained from ARETA by using our alignment over the three GEC datasets we use.\nTo demonstrate the effectiveness of our alignment algorithm, we compare our algorithm to the alignments generated by the M2 scorer, a standard Levenshtein edit distance, and ARETA. Table 2 presents the evaluation results of the alignment algorithms against the manual gold alignments of the\nQALB-2014 and QALB-2015 Dev sets in terms of precision (P), recall (R), and alignment error rate (AER) (Mihalcea and Pedersen, 2003; Och and Ney, 2003). Results show that our alignment algorithm is superior across all metrics.\nFigure 1 presents an example of the different alignments generated by the algorithms we evaluated. The M2 scorer\u2019s alignment over-clusters multiple edits into a single edit (words 6\u201313). This is not ideal, particularly because the M2 scorer does not count partial matches during the evaluation, which leads to underestimating the models\u2019 performances (Felice and Briscoe, 2015). A standard Levenshtein alignment does not handle merges correctly, e.g., words 8 and 9 in the erroneous sentence are aligned to words 9 and 10 in the corrected version. Among the drawbacks of ARETA\u2019s alignment is that it does not handle merges, e.g., erroneous words 8 and 9 are aligned with corrected words 9 and 10, respectively."
        },
        {
            "heading": "5 Arabic Grammatical Error Correction",
            "text": "Recently developed GEC models rely on Transformer-based architectures, from standard Seq2Seq models to edit-based systems built on top of Transformer encoders. Given Arabic\u2019s morphological richness and the relatively small size of available data, we explore different GEC models, from morphological analyzers and rule-based systems to pretrained Seq2Seq models. Primarily, we are interested in exploring modeling approaches to address the following two questions:\n\u2022 RQ1: Does morphological preprocessing improve GEC in Arabic?\n\u2022 RQ2: Does modeling GED explicitly improve GEC in Arabic?\nMorphological Disambiguation (Morph) We use the current SOTA MSA morphological analyzer and disambiguator from CAMeL Tools (Inoue et al., 2022; Obeid et al., 2020). Given an input sentence, the analyzer generates a set of potential analyses for each word and the disambiguator selects the optimal analysis in context. The analyses include minimal spelling corrections for common errors, diacritizations, POS tags, and lemmas. We use the dediacritized spellings as the corrections.\nMaximum Likelihood Estimation (MLE) We exploit our alignment algorithm to build a simple lookup model to map erroneous words to their corrections. We implement this model as a bigram\nmaximum likelihood estimator over the training data: P (ci|wi, wi\u22121, ei); where wi and wi\u22121 are the erroneous word (or phrases in case of a merge error) and its bigram context, ei is the error type of wi, and ci is the correction of wi. During inference, we pick the correction that maximizes the MLE probability. If the bigram context (wi and wi\u22121) was not observed during training, we backoff to a unigram. If the erroneous input word was not observed in training, we pass it to the output.\nChatGPT Given the rising interest in using large language models (LLMs) for a variety of NLP tasks, we benchmark ChatGPT (GPT-3.5) on the task of Arabic GEC. We follow the setup presented by Fang et al. (2023) on English GEC. To the best of our knowledge, we are the first to present ChatGPT results on Arabic GEC. The experimental setup along with the used prompts are presented in Appendix A.\nSeq2Seq with GED Models We experiment with two newly developed pretrained Arabic Transformer-based Seq2Seq models: AraBART (Kamal Eddine et al., 2022) (pretrained on 24GB of MSA data mostly in the news domain), and AraT5 (Nagoudi et al., 2022) (pretrained on 256GB of both MSA and Twitter data).\nWe extend the Seq2Seq models we use to incorporate token-level GED information during training and inference. Specifically, we feed predicted GED tags as auxiliary input to the Seq2Seq models. We add an embedding layer to the encoders of AraBART and AraT5 right after their corresponding token embedding layers, allowing us to learn representations for the auxiliary GED input. The GED embeddings have the same dimensions as the positional and token embeddings, so all three embeddings can be summed before they are passed to the multi-head attention layers in the encoders.\nOur approach is similar to what was done by Yuan et al. (2021), but it is much simpler as it reduces the model\u2019s size and complexity by not introducing an additional encoder to process GED input. Since the training data we use is relatively small, not drastically increasing the size of AraBART and AraT5 becomes important not to hinder training."
        },
        {
            "heading": "6 Experiments",
            "text": ""
        },
        {
            "heading": "6.1 Arabic Grammatical Error Detection",
            "text": "We build word-level GED classifiers using Transformer-based PLMs. From the many avail-\nable Arabic monolingual BERT models (Antoun et al., 2020; Abdul-Mageed et al., 2021; Lan et al., 2020; Safaya et al., 2020; Abdelali et al., 2021), we chose to use CAMeLBERT MSA (Inoue et al., 2021), as it was pretrained on the largest MSA dataset to date.\nIn our GED modeling experiments, we project multi-token error type annotations to single-token labels. In the case of a Merge error (many-to-one), we label the first token as Merge-B (Merge beginning) and all subsequent tokens as Merge-I (Merge inside). For all other multi-token error types, we repeat the same label for each token. We further label all deletion errors with a single Delete tag. To reduce the output space of the error tags, we only model the 14 most frequent error combinations (appearing more than 100 times). We ignore unknown errors when we compute the loss during training; however, we penalize the models for missing them in the evaluation. Since the majority of insertion errors are related to missing punctuation marks rather than missing words (see Appendix C), and due to inconsistent punctuation error annotations (Mohit et al., 2014), we exclude insertion errors from our GED modeling and evaluation. We leave the investigation of insertion errors to future work. The full GED output space we model consists of 43 error tags (43-Class).\nWe take advantage of the modularity of the ARETA error tags to conduct multi-class GED experiments, reducing the 43 error tags to their corresponding 13 main error categories as well as to a binary space (correct/incorrect). The statistics of the error tags we model across all datasets are in Appendix D. Figure 1 shows an example of error types at different granularity levels. Table 3 presents the GED granularity results. Unsurprisingly, all numbers go up when we model fewer error types. However, modeling more error types does not significantly worsen the performance in terms of error detection accuracy. It seems that all systems are capable of detecting comparable\nnumbers of errors despite the number of classes, but the verbose systems struggle with detecting the specific class labels."
        },
        {
            "heading": "6.2 Arabic Grammatical Error Correction",
            "text": "We explore different variants of the abovementioned Seq2Seq models. For each model, we study the effects of applying morphological preprocessing (+Morph), providing GED tags as auxiliary input (+GED), or both (+Morph+GED). Applying morphological preprocessing simply means correcting the erroneous input using the morphological disambiguator before training and inference.\nTo increase the robustness of the models that take GED tags as auxiliary input, we use predicted (not gold) GED tags when we train the GEC systems. For each dataset, we run its respective GED model on the same training data it was trained on and we pick the predictions of the worst checkpoint. During inference, we resolve merge and delete errors before feeding erroneous sentences to the model. This experimental setup yields the best performance across all GEC models.\nTo ensure fair comparison to previous work on Arabic GEC, we follow the same constraints that were introduced in the QALB-2014 and QALB2015 shared tasks: systems tested on QALB-2014 are only allowed to use the QALB-2014 training data, whereas systems tested on QALB-2015 are allowed to use the QALB-2014 and QALB-2015 training data. For ZAEBUC, we train our systems on the combinations of the three training datasets. We report our results in terms of precision (P), recall (R), F1, and F0.5. F1 was the official metric used in the QALB-2014 and QALB-2015 shared tasks. However, we follow the most recent work on GEC and use F0.5 (weighing precision twice as much as recall) as our main evaluation metric.\nWe use Hugging Face\u2019s Transformers (Wolf et al., 2019) to build our GED and GEC models. The hyperparameters we used are detailed in Appendix A."
        },
        {
            "heading": "7 Results",
            "text": "Table 4 presents the results on the Dev sets.\nBaselines The Morph system which did not use any training data constitutes a solid baseline for mostly addressing the noise in Arabic spelling. The MLE system claims the highest precision of all compared systems, but it suffers from low recall as expected. ChatGPT has the highest recall among the baselines, but with lower precision. A sample of 100 ChatGPT mismatches reveals that 37% are due to mostly acceptable punctuation choices and 25% are valid paraphrases or re-orderings; however, 38% are grammatically or lexically incorrect.\nSeq2Seq Models AraT5 and AraBART outperform previous work on QALB-2014 and QALB2015, with AraBART being the better model on average.\nDoes morphological preprocessing improve Arabic GEC? Across all models (MLE, AraT5, and AraBART), training and testing on morphologically preprocessed text improves the performance, except for MLE+Morph on QALB-2015 where there is no change in F0.5.\nDoes GED help Arabic GEC? We start off by using the most fine-grained GED model (43- Class) to exploit the full effect of the ARETA GED tags and to guide our choice between AraBART and AraT5. Using GED as an auxiliary input in both AraT5 and AraBART improves the results across all three Dev sets, with AraBART+GED demonstrating superior performance compared to the other models, on average. Applying morphological preprocessing as well as using GED as an auxiliary input yields the best performance across the three Dev sets, except for QALB2015 in the case of AraT5+Morph+GED. Overall, AraBART+Morph+GED is the best performer on average in terms of F0.5. The improvements using GED with GEC systems are mostly due to recall. An error comparison between AraBART and the AraBART+Morph+GED model (Appendix E) shows improved performance on the majority of the error types.\nTo study the effect of GED granularity on GEC, we train two additional AraBART+Morph+GED models with 13-Class and 2-Class GED tags. The results in Table 5 show that 13-Class GED was best in QALB-2014 and ZAEBUC, whereas 43-Class\nGED was best in QALB-2015 in terms of F0.5. However, in terms of precision and recall, GED models with different granularity behave differently across the three Dev sets. On average, using any GED granularity improves over AraBART, with 13-Class GED yielding the best results, although it is only 0.1 higher than 43-Class GED in terms of F0.5. For completeness, we further estimate an oracle upper bound by using gold GED tags with different granularity. The results (in Table 5) show that using GED with different granularity improves the results considerably. This indicates that GED is providing the GEC system with additional information; however, the main bottleneck is the GED prediction reliability as opposed to GED granularity. Improving GED predictions will most likely lead to better GEC results.\nTest Results Since the best-performing models on the three Dev sets benefit from different GED granularity when used with AraBART+Morph, we present the results on the Test sets using all different GED granularity models. The results of using AraBART and its variants on the Test sets are presented in Table 6. On QALB-2014, using\nMorph, GED, or both improves the results over AraBART, except for 2-Class GED. AraBART+43Class GED is the best performer (0.3 increase in F0.5, although not statistically significant).3 It is worth noting that AraBART+Morph achieves the highest recall on QALB-2014 (2.7 increase over AraBART and statistically significant at p < 0.05). For QALB-2015-L1, using GED by itself across all granularity did not improve over AraBART, but when combined with Morph, the 43-Class GED model yields the best performance in F0.5 (0.6 increase statistically significant at p < 0.05). When it comes to QALB-2015-L2, Morph does not help, but using GED alone improves the results over AraBART, with 43-Class and 13-Class GED being the best (0.4 increase). Lastly, in ZAEBUC, Morph does not help, but using 13-Class GED by itself improves over AraBART (0.4 increase). Overall, all the improvements we observe are attributed to recall, which is consistent with the Dev results.\nFollowing the QALB-2015 shared task (Rozovskaya et al., 2015) reporting of no-punctuation\n3Statistical significance was done using a two-sided approximate randomization test.\nresults due to observed inconsistencies in the references (Mohit et al., 2014), we present results on the Test sets without punctuation errors in Table 7. The results are consistent with those with punctuation, indicating that GED and morphological preprocessing yield improvements compared to using AraBART by itself across all Test sets. The score increase among all reported metrics when removing punctuation, specifically in the L1 data, indicates that punctuation presents a challenge for GEC models and needs further investigation both in terms of data creation and modeling approaches.\nAnalyzing the Test Results Table 8 presents the average absolute changes in precision and recall over the Test sets when introducing Morph, GED, or both. Adding Morph alone or GED alone improves recall (up to 0.8 in the case of Morph) and slightly hurts precision. When using both Morph and GED, we observe significant improvements in recall with an average of 1.5 but with higher drops of precision with an average of \u22120.7."
        },
        {
            "heading": "8 Conclusion and Future Work",
            "text": "We presented the first results on Arabic GEC using Transformer-based pretrained Seq2Seq models. We also presented the first results on multi-class Arabic GED. We showed that using GED information as an auxiliary input in GEC models improves GEC performance across three datasets. Further, we investigated the use of contextual morphological preprocessing in aiding GEC systems. Our models achieve SOTA results on two Arabic GEC shared tasks datasets and establish a strong benchmark on a recently created dataset.\nIn future work, we plan to explore other GED and GEC modeling approaches, including the use of syntactic models (Li et al., 2022; Zhang et al., 2022). We plan to work more on insertions, punctuation, and infrequent error combinations. We also plan to work on GEC for Arabic dialects, i.e., the conventional orthography of dialectal Arabic normalization (Habash et al., 2018; Eskander et al., 2013; Eryani et al., 2020).\nLimitations\nAlthough using GED information as an auxiliary input improves GEC performance, our GED systems are limited as they can only predict error types for up to 512 subwords since they are built by fine-tuning CAMeLBERT. We also acknowledge\nthe limitation of excluding insertion errors when modeling GED. Furthermore, our GEC systems could benefit from employing a copying mechanism (Zhao et al., 2019; Yuan et al., 2019), particularly because of the limited training data available in Arabic GEC. Moreover, the dataset sizes of QALB-2015-L2 and ZAEBUC are too small to allow us to test for statistical significance."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank Ted Briscoe for helpful discussions and constructive feedback. We acknowledge the support of the High Performance Computing Center at New York University Abu Dhabi. Finally, we wish to thank the anonymous reviewers at EMNLP 2023 for their feedback."
        },
        {
            "heading": "A Detailed Experimental Setup",
            "text": "Grammatical Error Detection Our GED models were fine-tuned for 10 epochs using a learning rate of 5e-5, a batch size of 32, and a seed of 42. At the end of the fine-tuning, we pick the best checkpoint based on the performance on the Dev sets.\nGrammatical Error Correction When using AraBART, we fine-tune the models for 10 epochs by using a learning rate of 5e-5, a batch size of 32, a maximum sequence length of 1024, and a seed of 42. For AraT5, we fine-tune the models for 30 epochs by using a learning rate of 1e-4 and the rest of the hyperparameters are the same as the ones used in AraBART. During inference, we use beam search with a beam width of 5 for all models. At the end of the fine-tuning, we pick the best checkpoint based on the performance on the Dev sets by using the M2 scorer. The M2 scorer suffers from extreme running times in cases where the generated outputs differ significantly from the input. To mitigate this bottleneck, we extend the M2 scorer by introducing a time limit for each sentence during evaluation. If the evaluation of a single generated sentence surpasses this limit, we pass the input sentence to the output without modifications. We use this extended version of the M2 scorer when reporting our results on the Dev sets. When reporting our results on the Test sets, we use the M2 scorer release that is provided by the QALB shared task. We make our extended version of the M2 scorer publicly available.\nChatGPT We start with prompting ChatGPT with a 3-shot prompt. Our exact prompt is the following:\n\"Please identify and correct any spelling and grammar mistakes in the following sentence indicated by <input> INPUT </input> tag. You need to comprehend the sentence as a whole before gradually identifying and correcting any errors while keeping the original sentence structure unchanged as much as possible.\nAfterward, output the corrected version directly without any explanations. Here are some in-context examples:\n(1), <input> SRC-1 </input>: <output> TGT-1 </output>. (2), <input> SRC-2 </input>: <output> TGT-2 </output>. (3), <input> SRC-3 </input>: <output> TGT-3 </output>. Please feel free to refer to these examples. Remember to format your corrected output results with the tag <output> Your Corrected Version </output>. Please start: <input> INPUT </input>\""
        },
        {
            "heading": "B Datasets Statistics",
            "text": ""
        },
        {
            "heading": "C Error Types Statistics",
            "text": ""
        },
        {
            "heading": "D GED Granularity Data Statistics",
            "text": ""
        },
        {
            "heading": "E Error Analysis on Error Types",
            "text": ""
        }
    ],
    "title": "Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation",
    "year": 2023
}