{
    "abstractText": "A salient characteristic of pre-trained language models (PTLMs) is a remarkable improvement in their generalization capability and emergence of new capabilities with increasing model capacity and pre-training dataset size. Consequently, we are witnessing the development of enormous models pushing the stateof-the-art. It is, however, imperative to realize that this inevitably leads to prohibitively long training times, extortionate computing costs, and a detrimental environmental impact. Significant efforts are underway to make PTLM training more efficient through innovations in model architectures, training pipelines, and loss function design, with scant attention being paid to optimizing the utility of training data. The key question that we ask is whether it is possible to train PTLMs by employing only highly informative subsets of the training data while maintaining downstream performance? Building upon the recent progress in informative data subset selection, we show how we can employ submodular optimization to select highly representative subsets of the training corpora and demonstrate that the proposed framework can be applied to efficiently train multiple PTLMs (BERT, BioBERT, GPT-2) using only a fraction of data. Further, we perform a rigorous empirical evaluation to show that the resulting models achieve up to \u223c 99% of the performance of the fully-trained models. We made our framework publicly available at https://github.com/Efficient-AI/ingenious.",
    "authors": [
        {
            "affiliations": [],
            "name": "Krishnateja Killamsetty"
        },
        {
            "affiliations": [],
            "name": "Sumit Bhatia"
        },
        {
            "affiliations": [],
            "name": "Milan Aggarwal"
        },
        {
            "affiliations": [],
            "name": "Ganesh Ramakrishnan"
        },
        {
            "affiliations": [],
            "name": "Rishabh Iyer"
        },
        {
            "affiliations": [],
            "name": "Balaji Krishnamurthy"
        }
    ],
    "id": "SP:975fc4bf3a16bc967e0ced810cf8e2d9a65ea488",
    "references": [
        {
            "authors": [
                "Akiko Aizawa."
            ],
            "title": "An information-theoretic perspective of tf\u2013idf measures",
            "venue": "Information Processing & Management, 39(1):45\u201365.",
            "year": 2003
        },
        {
            "authors": [
                "Badr AlKhamissi",
                "Millicent Li",
                "Asli Celikyilmaz",
                "Mona Diab",
                "Marjan Ghazvininejad"
            ],
            "title": "A review on language models as knowledge bases",
            "year": 2022
        },
        {
            "authors": [
                "Francis Bach."
            ],
            "title": "Learning with submodular functions: A convex optimization perspective",
            "venue": "Foundations and Trends\u00ae in Machine Learning, 6(2-3):145\u2013373.",
            "year": 2013
        },
        {
            "authors": [
                "Francis Bach."
            ],
            "title": "Submodular functions: from discrete to continuous domains",
            "venue": "Mathematical Programming, 175(1):419\u2013459.",
            "year": 2019
        },
        {
            "authors": [
                "Jeff Bilmes."
            ],
            "title": "Submodularity in machine learning and artificial intelligence",
            "venue": "Vighnesh Birodkar, Hossein Mobahi, and Samy Bengio. 2019. Semantic redundancies in imageclassification datasets: The 10% you don\u2019t need.",
            "year": 2022
        },
        {
            "authors": [
                "ArXiv",
                "abs/1901.11409. Yonatan Bitton",
                "Michael Elhadad",
                "Gabriel Stanovsky",
                "Roy Schwartz"
            ],
            "title": "Data efficient masked language modeling for vision and language. In Findings of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Sigler",
                "Mateusz Litwin",
                "Scott Gray",
                "Benjamin Chess",
                "Jack Clark",
                "Christopher Berner",
                "Sam McCandlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei."
            ],
            "title": "Language models are few-shot learners",
            "venue": "Trevor Campbell and Tamara Broderick. 2018.",
            "year": 2020
        },
        {
            "authors": [
                "Carbin."
            ],
            "title": "The lottery ticket hypothesis for pretrained bert networks",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages 15834\u2013 15846. Curran Associates, Inc. Alexandre de Br\u00e9bisson and Pascal Vincent. 2016. An",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-",
            "year": 2019
        },
        {
            "authors": [
                "Cham. Satoru Fujishige"
            ],
            "title": "Submodular functions and optimization",
            "venue": "Elsevier. Jonas Geiping and Tom Goldstein",
            "year": 2005
        },
        {
            "authors": [
                "Mitchell Gordon",
                "Kevin Duh",
                "Nicholas Andrews."
            ],
            "title": "Compressing BERT: Studying the effects of weight pruning on transfer learning",
            "venue": "Proceedings of the 5th Workshop on Representation Learning for NLP, pages 143\u2013155, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Linguistics. Yaru Hao",
                "Li Dong",
                "Furu Wei",
                "Ke Xu."
            ],
            "title": "Visualizing and understanding the effectiveness of BERT",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
            "year": 2019
        },
        {
            "authors": [
                "Hong Kong",
                "China"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 2004
        },
        {
            "authors": [
                "John Hewitt",
                "Christopher D. Manning"
            ],
            "title": "coresets for k-means and k-median clustering",
            "venue": "In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing,",
            "year": 2019
        },
        {
            "authors": [
                "Linguistics. Rishabh Iyer",
                "Jeffrey Bilmes"
            ],
            "title": "A memoization framework for scaling submodular optimization to large scale problems",
            "venue": "In The 22nd International Conference on Artificial Intelligence and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "2340\u20132349. PMLR. Rishabh Iyer",
                "Ninad Khargoankar",
                "Jeff Bilmes",
                "Himanshu Asanani"
            ],
            "title": "Submodular combinatorial information measures with applications in machine learning",
            "venue": "In Algorithmic Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "PMLR. Rishabh Krishnan Iyer"
            ],
            "title": "Submodular optimization and machine learning: Theoretical results, unifying and scalable algorithms, and applications",
            "venue": "Ph.D. thesis",
            "year": 2015
        },
        {
            "authors": [
                "Peter Izsak",
                "Moshe Berchansky",
                "Omer Levy."
            ],
            "title": "How to train BERT with an academic budget",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10644\u201310652, Online and Punta Cana, Dominican Republic. Associ-",
            "year": 2021
        },
        {
            "authors": [
                "Ganesh Jawahar",
                "Beno\u00eet Sagot",
                "Djam\u00e9 Seddah"
            ],
            "title": "What does BERT learn about the structure of language",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Florence"
            ],
            "title": "TinyBERT: Distilling BERT for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jeremy Kahn"
            ],
            "title": "A.I.\u2019s carbon footprint is big, but easy to reduce, Google researchers say. https://fortun e.com/2021/04/21/ai-carbon-footprint-reduce-env ironmental-impact-of-tech-google-research-study",
            "year": 2021
        },
        {
            "authors": [
                "Online",
                "Krishnateja Killamsetty",
                "Harsha Kokel",
                "Rishabh K Iyer"
            ],
            "title": "ORIENT: Submodular mutual information measures for data subset selection under distribution shift",
            "year": 2022
        },
        {
            "authors": [
                "Jivat Kaur",
                "Sumit Bhatia",
                "Milan Aggarwal",
                "Rachit Bansal",
                "Balaji Krishnamurthy"
            ],
            "title": "LM-CORE: Language models with contextually relevant external knowledge",
            "venue": "In Findings of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Vishal Kaushal",
                "Ganesh Ramakrishnan",
                "Rishabh Iyer."
            ],
            "title": "Submodlib: A submodular optimization library",
            "venue": "arXiv preprint arXiv:2202.10680. Krishnateja Killamsetty, Guttu Sai Abhishek, Aakriti Lnu, Ganesh Ramakrishnan, Alexandre V. Ev-",
            "year": 2022
        },
        {
            "authors": [
                "fimievski",
                "Lucian Popa",
                "Rishabh K Iyer"
            ],
            "title": "AUTOMATA: Gradient based data subset selection for compute-efficient hyper-parameter tuning",
            "venue": "In ThirtySixth Conference on Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Krishnateja Killamsetty",
                "Durga S",
                "Ganesh Ramakrishnan",
                "Abir De",
                "Rishabh Iyer."
            ],
            "title": "Grad-match: Gradient matching based data subset selection for efficient deep model training",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "PMLR. Krishnateja Killamsetty",
                "Durga Sivasubramanian",
                "Ganesh Ramakrishnan",
                "Rishabh Iyer"
            ],
            "title": "2021b. Glister: Generalization based data subset selec",
            "venue": "Proceedings of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Jeff Bilmes"
            ],
            "title": "Submodularity for data selection in machine translation",
            "year": 2014
        },
        {
            "authors": [
                "Suraj Kothawade",
                "Nathan Beck",
                "Krishnateja Killamsetty",
                "Rishabh Iyer."
            ],
            "title": "Similar: Submodular information measures based active learning in realistic scenarios",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 18685\u201318697. Curran",
            "year": 2021
        },
        {
            "authors": [
                "Suraj Kothawade",
                "Jiten Girdhar",
                "Chandrashekhar Lavania",
                "Rishabh Iyer."
            ],
            "title": "Deep submodular networks for extractive data summarization",
            "venue": "arXiv preprint arXiv:2010.08593.",
            "year": 2020
        },
        {
            "authors": [
                "Olga Kovaleva",
                "Alexey Romanov",
                "Anna Rogers",
                "Anna Rumshisky."
            ],
            "title": "Revealing the dark secrets of BERT",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural",
            "year": 2019
        },
        {
            "authors": [
                "Andreas Krause",
                "Daniel Golovin."
            ],
            "title": "Submodular function maximization",
            "venue": "Tractability, 3:71\u2013104.",
            "year": 2014
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang."
            ],
            "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
            "venue": "Bioinformatics, 36(4):1234\u20131240.",
            "year": 2020
        },
        {
            "authors": [
                "Conglong Li",
                "Minjia Zhang",
                "Yuxiong He"
            ],
            "title": "Curriculum learning: A regularization method for efficient and stable billion-scale GPT model pre-training",
            "year": 2022
        },
        {
            "authors": [
                "I Loshchilov",
                "F Hutter."
            ],
            "title": "Online batch selection for faster training of neural networks",
            "venue": "International Conference on Learning Representations (ICLR) 2016 Workshop Track.",
            "year": 2016
        },
        {
            "authors": [
                "Ayush Maheshwari",
                "Oishik Chatterjee",
                "Krishnateja Killamsetty",
                "Ganesh Ramakrishnan",
                "Rishabh Iyer."
            ],
            "title": "Semi-supervised data programming with subset selection",
            "venue": "arXiv preprint arXiv:2008.09887.",
            "year": 2020
        },
        {
            "authors": [
                "Michel Minoux."
            ],
            "title": "Accelerated greedy algorithms for maximizing submodular set functions",
            "venue": "Optimization techniques, pages 234\u2013243. Springer.",
            "year": 1978
        },
        {
            "authors": [
                "Baharan Mirzasoleiman",
                "Ashwinkumar Badanidiyuru",
                "Amin Karbasi",
                "Jan Vondr\u00e1k",
                "Andreas Krause."
            ],
            "title": "Lazier than lazy greedy",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 29.",
            "year": 2015
        },
        {
            "authors": [
                "Baharan Mirzasoleiman",
                "Jeff Bilmes",
                "Jure Leskovec."
            ],
            "title": "Coresets for data-efficient training of machine learning models",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Re-",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Mittal",
                "Durga Sivasubramanian",
                "Rishabh Iyer",
                "Preethi Jyothi",
                "Ganesh Ramakrishnan."
            ],
            "title": "Partitioned gradient matching-based data subset selection for compute-efficient robust asr training",
            "venue": "Findings of",
            "year": 2022
        },
        {
            "authors": [
                "Koichi Nagatsuka",
                "Clifford Broni-Bediako",
                "Masayasu Atsumi"
            ],
            "title": "Ctr-bert: Cost-effective knowledge distillation for billion-parameter teacher models. In NeurIPS Efficient Natural Language and Speech Processing",
            "year": 2021
        },
        {
            "authors": [
                "George L Nemhauser",
                "Laurence A Wolsey",
                "Marshall L Fisher."
            ],
            "title": "An analysis of approximations for maximizing submodular set functions\u2014i",
            "venue": "Mathematical programming, 14(1):265\u2013294. Mansheej Paul, Surya Ganguli, and Gintare Karolina",
            "year": 1978
        },
        {
            "authors": [
                "Dziugaite."
            ],
            "title": "Deep learning on a data diet: Finding important examples early in training",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 20596\u201320607. Curran Associates, Inc. Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel,",
            "year": 2021
        },
        {
            "authors": [
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural",
            "year": 2019
        },
        {
            "authors": [
                "Hong Kong",
                "China"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "Language Processing (EMNLP-IJCNLP),",
            "year": 2022
        },
        {
            "authors": [
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Anna Rogers",
                "Olga Kovaleva",
                "Anna Rumshisky"
            ],
            "title": "Foundations and Trends\u00ae",
            "year": 2020
        },
        {
            "authors": [
                "Sa\u00efd Salhi."
            ],
            "title": "Discrete location theory",
            "venue": "Journal of the Operational Research Society, 42:1124\u20131125. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint",
            "year": 1991
        },
        {
            "authors": [
                "arXiv:1910.01108. Timo Schick",
                "Hinrich Sch\u00fctze"
            ],
            "title": "It\u2019s not just size that matters: Small language models are also fewshot learners",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Roy Schwartz",
                "Jesse Dodge",
                "Noah A Smith",
                "Oren Etzioni"
            ],
            "title": "Computational Linguistics: Human Language Technologies, pages 2339\u20132352",
            "venue": "Green ai. Communications of the ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Or Sharir",
                "Barak Peleg",
                "Yoav Shoham"
            ],
            "title": "The cost of training nlp models: A concise overview",
            "year": 2020
        },
        {
            "authors": [
                "Matthew E. Peters",
                "Iz Beltagy."
            ],
            "title": "Staged training for transformer language models",
            "venue": "International Conference on Machine Learning, ICML 2022, 17-",
            "year": 2022
        },
        {
            "authors": [
                "Hong-Jie Dai",
                "Feng Liu",
                "Yifei Chen",
                "Chengjie Sun",
                "Sophia Katrenko",
                "Pieter Adriaans",
                "Christian Blaschke",
                "Rafael Torres",
                "Mariana Neves",
                "Preslav Nakov",
                "Anna Divoli",
                "Manuel Ma\u00f1a-L\u00f3pez",
                "Jacinto Mata",
                "W. John Wilbur"
            ],
            "title": "Overview of BioCreative II",
            "year": 2008
        },
        {
            "authors": [
                "jay Korthikanti"
            ],
            "title": "Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, a large-scale generative language model",
            "venue": "arXiv preprint arXiv:2201.11990",
            "year": 2022
        },
        {
            "authors": [
                "Surya Ganguli",
                "Ari S. Morcos."
            ],
            "title": "Beyond neural scaling laws: beating power law scaling via data pruning",
            "venue": "Thirty-Sixth Conference on Neural Information Processing Systems. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,",
            "year": 2022
        },
        {
            "authors": [
                "Abu Awal Md Shoeb",
                "Abubakar Abid",
                "Adam Fisch",
                "Adam R Brown",
                "Adam Santoro",
                "Aditya Gupta",
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615",
            "year": 2022
        },
        {
            "authors": [
                "Olivier Taboureau",
                "Sonny Kim Nielsen",
                "Karine Audouze",
                "Nils Weinhold",
                "Daniel Edsg\u00e4rd",
                "Francisco S. Roque",
                "Irene Kouskoumvekaki",
                "Alina Bora",
                "Ramona Curpan",
                "Thomas Sk\u00f8t Jensen",
                "S\u00f8ren Brunak",
                "Tudor I. Oprea"
            ],
            "title": "ChemProt: a disease",
            "year": 2011
        },
        {
            "authors": [
                "Ehsan Tohidi",
                "Rouhollah Amiri",
                "Mario Coutino",
                "David Gesbert",
                "Geert Leus",
                "Amin Karbasi."
            ],
            "title": "Submodularity in action: From machine learning to signal processing applications",
            "venue": "IEEE Signal Processing Magazine, 37(5):120\u2013133.",
            "year": 2020
        },
        {
            "authors": [
                "Mariya Toneva",
                "Alessandro Sordoni",
                "Remi Tachet des Combes",
                "Adam Trischler",
                "Yoshua Bengio",
                "Geoffrey J Gordon."
            ],
            "title": "An empirical study of example forgetting during deep neural network learning",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Kai Wei",
                "Rishabh Iyer",
                "Jeff Bilmes."
            ],
            "title": "Submodularity in data subset selection and active learning",
            "venue": "International Conference on Machine Learning, pages 1954\u20131963. PMLR.",
            "year": 2015
        },
        {
            "authors": [
                "Kai Wei",
                "Yuzong Liu",
                "Katrin Kirchhoff",
                "Chris Bartels",
                "Jeff Bilmes."
            ],
            "title": "Submodular subset selection for large-scale speech training data",
            "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3311\u20133315. IEEE.",
            "year": 2014
        },
        {
            "authors": [
                "Kai Wei",
                "Yuzong Liu",
                "Katrin Kirchhoff",
                "Jeff Bilmes."
            ],
            "title": "Unsupervised submodular subset selection for speech data",
            "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4107\u20134111. IEEE.",
            "year": 2014
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Ruslan Salakhutdinov",
                "Quoc V. Le"
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "year": 2020
        },
        {
            "authors": [
                "Xingcheng Yao",
                "Yanan Zheng",
                "Xiaocong Yang",
                "Zhilin Yang."
            ],
            "title": "NLP from scratch without largescale pretraining: A simple and efficient framework",
            "venue": "Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Ma-",
            "year": 2022
        },
        {
            "authors": [
                "Ofir Zafrir",
                "Ariel Larey",
                "Guy Boudoukh",
                "Haihao Shen",
                "Moshe Wasserblat."
            ],
            "title": "Prune once for all: Sparse pre-trained language models",
            "venue": "arXiv preprint arXiv:2111.05754.",
            "year": 2021
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "der the BSD license"
            ],
            "title": "HuggingFace is available under Apache 2.0 license. For submodular optimization, we use a library called SUBMODLIB (Kaushal et al., 2022)",
            "year": 2022
        },
        {
            "authors": [],
            "title": "Formally, a function f is submodular (Fujishige, 2005; Bilmes, 2022) if for x \u2208 U",
            "venue": "f(A\u222a x)\u2212 f(A) \u2265 f(B \u222a x)\u2212 f(B),",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Pre-trained language models (PTLMs) (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2020; Brown et al., 2020; Raffel et al., 2020) have revolutionized the field of natural language processing (NLP), becoming the default choice for a wide array of NLP tasks. The versatility of PTLMs,\n\u2217Work done during internship at Media and Data Science Research (MDSR) Lab, Adobe Inc.\nhowever, is accompanied by significant costs. For instance, it costs an estimated $12 million to train GPT-3 (Brown et al., 2020) with roughly 1.2 million pounds of CO2 emissions (Kahn, 2021). Megatron-Turing NLG (Smith et al., 2022) is a 530 billion parameter PTLM, which is thrice the size of GPT-3 and is trained on 4480 A100 GPUs and yields close to 1% performance improvements over GPT-3. By continually increasing the size of PTLMs and pre-training corpora to improve generalization ability, significant additional resources and energy are consumed, resulting in dire environmental consequences (Sharir et al., 2020). Further, such large-scale resource utilization and the costs associated with PTLMs create an uneven playing field for small organizations and universities, which operate with significant resource constraints. Hence, a crucial step towards developing responsible, fair, and GreenAI (Schwartz et al., 2020) involves minimizing inefficiencies and costs of training these models.\nSignificant efforts toward improving the efficiency of PTLMs have ventured in directions such as optimizing the model architecture (Chen et al., 2020; Gordon et al., 2020; Zafrir et al., 2021), modifications to the training pipeline (Izsak et al., 2021; Shen et al., 2022) and task (Schick and Sch\u00fctze, 2021), sample efficient masking techniques for improved convergence (Bitton et al., 2021) and leveraging contextual knowledge to reduce model size (Kaur et al., 2022). In this work, driven by the observation that the scale of the pre-training corpus contributes significantly to the training costs of PTLMs, we explore the feasibility of training PTLMs using highly informative subsets of the corpus. Recent studies have demonstrated the feasibility of informative data subset selection for efficient deep model training for images (Mirzasoleiman et al., 2020; Killamsetty et al., 2021a,b,c; Pooladzandi et al., 2022) in both supervised and\nsemi-supervised settings. In light of this, the key question we attempt to answer is: Can we efficiently pre-train language models using highly informative subsets of the training corpus without compromising performance?\nThe first step in answering the above question is identifying informative (or representative) subsets of the underlying training corpus such that they maximize the representation of the remaining samples in the corpus. Intuitively, given a set of sentences, the subsequent addition of sentences similar to existing sentences in the set yields diminishing returns. More information gains can be achieved by adding diverse, dissimilar sentences. While the classical subset selection problem is NP-hard, we can leverage the diminishing gains property of submodular functions (Fujishige, 2005) and frame subset selection as a submodular maximization problem. Several recent works (Wei et al., 2015; Mirzasoleiman et al., 2020; Kothawade et al., 2021; Karanam et al., 2022; Maheshwari et al., 2020) have formulated the subset selection problem as that of maximizing a submodular objective. However, applying existing subset selection frameworks to PTLMs is nontrivial given the scale of corpora typically used for pre-training (e.g., Wikipedia and Common Crawl consisting of hundreds of millions of sequences and billions of tokens). Most of the existing methods rely on per-sample gradients, which are expensive to compute, and to the best of our knowledge, none of the previous works have considered subset selection for such large datasets.\nOur contributions: We propose the informative data subset selection task for efficient pre-training of PTLMs and present INGENIOUS, a framework for subset selection using submodular optimization (Section 3). We show how to overcome the scalability challenge for typical large-scale pretraining corpora and employ scalable sentence feature encoders to obtain individual data sample features relevant for subset selection. We also employ various engineering techniques to scalably select subsets from large-scale datasets (Section 3). We use INGENIOUS to pre-train BERT and GPT-2 and evaluate the performance of the resulting models on downstream tasks (Section 4). A rigorous empirical evaluation reveals that the models pre-trained with INGENIOUS retain upto \u2248 99% performance of the models pre-trained using the\nfull dataset. Figure 1 summarizes the cost-savings vs performance trade-off achieved by INGENIOUS for BERT pre-training. We also present thorough ablation studies revealing the impact of various design choices and parameters involved. We also evaluate the models trained by INGENIOUS in terms of their knowledge retention capabilities and show how INGENIOUS can be used to accelerate pre-training of domain-specific language models such as BioBERT (Section 4.4). Finally, we discuss the inferences that could be drawn from our work, limitations of our proposed framework and lay out directions for further improvement (Section 5)."
        },
        {
            "heading": "2 Related Work",
            "text": "Knowledge distillation and pruning based methods (Sanh et al., 2019; Jiao et al., 2020; Muhamed et al., 2021) pre-train a smaller variant of PTLMs (such as BERT) with lesser capacity using the full model as teacher network. Even though lighter versions such as DistilBERT (Sanh et al., 2019) retain \u2248 97% of the performance with up to 60% faster inference, the PTLM still needs to be completely pre-trained initially to be able to distill the lighter version. Thus, the efficiency gains are restricted only to the fine-tuning and inference. Other methods prune the architecture through forcing the weights with lesser magnitude to zero value during pre-training (Chen et al., 2020; Gordon et al., 2020) as well as during finetuning (Zafrir et al., 2021).\nModel architecture and training task optimizations: Schick and Sch\u00fctze (2021) have shown that smaller PTLMs can achieve better performance by formulating the task input in cloze style. Izsak et al. (2021) proposed to optimize BERT pretraining through multiple optimizations related to data, model size, and optimizer choice. Shen et al. (2022) proposed a staged training mechanism where they start with training a relatively smaller model, which is then used for initializing the full capacity model at a later stage. Yao et al. (2022) identify relevant samples from the pretraining corpus based on their similarity with the task-specific dataset to train task-specific PTLM followed by fine-tuning, thus inherently suffering from the limitation of pre-training separate models for every downstream task.\nCurriculum learning based methods employ the sequence length of training samples as a proxy for hardness.Typically, shorter (easier) sequences are presented in the initial stages of pre-training followed by longer (harder) sequences at later stages (Nagatsuka et al., 2021; Li et al., 2022). However, such methods have been shown to perform well only in limited configurations with respect to the choice of language models, stage of pre-training, etc..\nHardware optimizations for PTLM Training: The suite of Open Pre-Trained Transformers (OPT) (Zhang et al., 2022) require 1/7th of the carbon footprint for pre-training when compared to popular PTLMs such as GPT-3 (Brown et al., 2020) while achieving comparable few-shot generalization. OPTs leverage extensive data and tensor parallelism with high-memory GPUs (supporting large batch sizes), which are usually not easily accessible and can lead to exorbitant costs.\nNoticeably different from the aforementioned works, we explore making PTLM training more efficient by utilizing highly informative subsets of the training data. Consequently, our proposal effectively complements other optimization methods that target aspects such as model architecture and hardware enhancements."
        },
        {
            "heading": "3 The INGENIOUS Framework",
            "text": "We now present INGENIOUS - an informative data subset selection framework for pre-training language models. We summarize the training pipeline in Figure 2. We first describe the nota-\ntion to formulate the problem, followed by details of different steps involved in the framework."
        },
        {
            "heading": "3.1 Notation",
            "text": "We denote the unlabeled dataset for pre-training by U = {xj}nj=1, consisting of n data points each corresponding to a varying length of sequence of symbols {si}mi=1 (these symbols could be words or character sequences such as sub-words). Let S \u2286 U be the subset of the unlabeled dataset on which the language model is trained. Let the language model be parameterized by \u03b8. We subscript the changing variables such as model parameters \u03b8, subset S with the timestep t to denote their specific values at that timestep."
        },
        {
            "heading": "3.2 Problem Formulation",
            "text": "In its most general form, subset selection is defined as St = argmax\nS\u2286U f(S) (1)\nwhere the subset St \u2286 U at step t is selected such that it maximizes the function f .\nWhile the above general subset selection problem is NP-Hard, the problem becomes approximable in case the function f is submodular in nature (Fujishige, 2005). A set function f : 2U \u2212\u2192 R is submodular if for x \u2208 U , f(A \u222a x) \u2212 f(A) \u2265 f(B \u222a x) \u2212 f(B), \u2200A \u2286 B \u2286 U and x /\u2208 B. We pose the data subset selection problem as a submodular maximization problem since it allows for easier optimization by employing different approximations (Nemhauser et al., 1978; Iyer and Bilmes, 2019). In order to choose a suitable submodular function, one must understand the characteristics of the subsets that are crucial for the end-goal \u2013 efficient learning in our case. Previous works in computer vision have demonstrated that commonly used vision datasets contain many redundancies, and eliminating such redundant data samples does not affect the model\u2019s performance (Birodkar et al., 2019; Toneva et al., 2019; Paul et al., 2021; Sorscher et al., 2022). Further, one can achieve faster model training by using highly informative and representative data subsets (Kaushal et al., 2019; Mirzasoleiman et al., 2020; Sorscher et al., 2022). Please refer to Appendix B for more related work on submodularity based subset selection. Building upon the learnings from computer vision research, our primary requirement for the selected subset is that it should faithfully represent the training data and have min-\nimal redundancy within itself."
        },
        {
            "heading": "3.3 Overview of Approach",
            "text": "In order to select a representative subset as discussed above, we use Facility Location (Salhi, 1991; Krause and Golovin, 2014), a commonlyused submodular function closely related to kmedoid clustering which is defined as\nfFL(A) = \u2211 i\u2208U max j\u2208A Kij (2)\nwhere A is the subset being evaluated, K is a pairwise similarity matrix and Kij is the similarity between the ith and jth samples. Thus, our subset selection problem can be represented as:\nSt = argmax S\u2286U :|S|=k fFL(S) (3)\nHere, k represents the size of the subset S. We would like to clarify that Equation (3) enables us to choose diverse samples such that each represents other samples in the corpus, instead of selection of similar samples. The optimization problem in Equation (3) is an instance of cardinality-constrained monotone submodular maximization for which an approximate solution can be obtained by incrementally building the subset from scratch using algorithms such as Naive Greedy (Nemhauser et al., 1978), Lazy Greedy (Minoux, 1978), Stochastic Greedy (Mirzasoleiman et al., 2015), Lazier-than\nLazy-Greedy (Mirzasoleiman et al., 2015). We use the Lazier-than-Lazy Greedy optimizer as it is the most computationally efficient, along with memoization (Iyer and Bilmes, 2019).\nThe facility location function utilizes a pairwise similarity kernel K (of size |U|\u00d7 |U|) between the data samples in U to select representative subsets. To estimate the kernel values, we compute the cosine similarity between the feature representations of data samples obtained using the LM itself. To ensure that the extracted representations are meaningful during the initial phase, we warm start the model for W training steps as suggested by Killamsetty et al. (2021a,c) (step A in Figure 2). Further, to ensure that LM sees diverse data samples, we probabilistically sample data points based on submodular ordering obtained from running the greedy algorithm(steps B and C in Figure 2) and update the subset after every Rth iteration (step D in Figure 2) .\nThis re-sampling procedure is repeated till the predetermined number of steps. Algorithm 1 summarises the steps involved and in the following section, we describe the details of each step."
        },
        {
            "heading": "3.4 Methodology Details",
            "text": "Feature Encoders for Similarity Computation: The selection of optimal representative subsets requires a similarity kernel that captures the intrinsic relationships between data samples. We ex-\nAlgorithm 1: Pre-Training using INGENIOUS\nInput: Training dataset: U , Initial model parameters: \u03b80, Total no of training steps: T , Training steps interval for subset selection: R, Number of steps for warmstart phase: W , Size of the subset: k, Learning rates: {\u03b1t}t=T\u22121t=0 Set t = 0 optimizer = AdamW() *** Warmstart Phase *** repeat\nCompute batches Ub = ((xb, yb); b \u2208 (1 \u00b7 \u00b7 \u00b7B)) from U for b = 1 to B do\nif t \u2265 W then break\nCompute mask mt on Ub \u03b8t+1 = optimizer.step() t = t + 1\nuntil until t \u2265 W *** Subset Selection *** greedyIdxs, gains = argmax|S|\u2264|U|fFL(S,U, \u03b8t) probabilities = TaylorSoftmax(gains) St \u223csample(greedyIdxs, probabilities, k) repeat\nCompute batches Stb = ((xb, yb); b \u2208 (1 \u00b7 \u00b7 \u00b7B)) from St for b = 1 to B do if t \u2265 T then\nbreak\nCompute mask mt on Stb \u03b8t+1 = optimizer.step() t = t + 1 if (t%R == 0) then\nSt+1 \u223csample(greedyIdxs, probabilities, k) break\nelse St+1 = St\nuntil until t \u2265 T *** Evaluate trained model on validation set *** eval = evaluate (\u03b8T ,V) return eval, \u03b8T\nplore dense and sparse feature encoders for obtaining the feature representation of text samples in U . As a dense feature encoder for text samples, we use the intermediate representations as obtained from the LM that is currently being trained. We compute the representations of an input sequence by averaging the output embeddings of the constituent tokens. A question then arises on which layer of the underlying model should be used for obtaining this representation since different layers encode different types of information (Rogers et al., 2020). Another possibility is to use sparse representations such as TF-IDF (Aizawa, 2003) owing to its success at capturing statistically important lexical features (Robertson et al., 2009). We study the effect of using sparse feature representations (i.e., TF-IDF) and dense feature representations obtained from different layers of LM in Section 4.3. Our experiments revealed that dense feature encoders yield the best results.\nSubmodular Greedy Ordering based Data Selection: After deciding on the choice of similarity\nkernel, we now describe how to select the subsets (steps B and C in Figure 2) as defined by Equation (3). Approximate submodular maximization algorithms such as LazierthanLazy Greedy start with an empty subset and incrementally add data points one by one till the size of the subset equals the budget k set by us. If S represents subset selected so far, and e represents the next locally optimal data sample to be added, the submodular gain value of e is defined as f(S \u222a e) \u2212 f(S). While running the algorithm, we initially set the budget as the size of the entire data(say M ) in order to obtain and store the submodular gain (step B2 in Figure 2) of each data sample at the time of their addition.\nThe key idea here is to use the submodular gains associated with each data sample as an importance score; convert them to a probability distribution by using the second order Taylor-softmax operation (de Br\u00e9bisson and Vincent, 2016) (step C in Figure 2) and then sample a subset of desired size(say k) from the above distribution. Given gains vector {g1, g2, \u00b7 \u00b7 \u00b7 , gM}, Taylor-softmax operation over the vector for converting it to probability distribution P can be specified as P def={\n1+gi+0.5g 2 i\u2211M\nj=1 1+gj+0.5g 2 j }M i=1 .\nUsing the probability distribution P for sampling ensures that samples which have high importance score associated with them are selected with greater probability. However, it also allows the LM to explore the samples with low importance score during training to prevent overfitting. We reuse this probability distribution to sample new subsets of size k every R steps by sampling k points without replacement (step D in Figure 2).\nRecall that we require a similarity kernel of size |U| \u00d7 |U|, hence the memory required for storing the similarity kernels is practically infeasible. We now describe how we scale INGENIOUS to handle size of the pre-training datasets used for LMs.\nPartitioning based Efficient Subset Selection: To minimize the memory consumption, instead of constructing a probability distribution over the entire unlabeled set directly, we first partition (step B1 in Figure 2) the unlabeled set into NP random blocks of equal sizes (i.e., partition size is |U|NP ) and construct a probability distribution Pi over each data block Upi : |U p i | = |U| NP . We then use the con-\nstructed probability distributions Pi over each data block Upi to sample a subset of size k/NP from the data block without replacement. We compute the final subset using subsets from each partition as follows:\nSt = NP\u22c3 i=1 sample (Upi , Pi, k NP ) (4)\nThe partitioning of the unlabeled set allows us to get away with constructing similarity kernels of size |U|NP \u00d7 |U| NP\n, thereby reducing the similarity kernel memory usage by around NP 2 times. We discuss the effect of the partition size in Appendix K. In order to maximize the utilization of available resources, we can construct probability distributions over each block in the data partition in parallel. As in recent work (Mittal et al., 2022), partitioned facility location can be shown as a lower bound of the original objective function,i.e., facility location that is being maximized. It should be noted that memory utilization also increases with the number of parallel processes. For example, when NPP subsets are selected from partitions in parallel, the memory usage due to similarity kernel is of the order O(NPP |U| 2\nN2P ). In our experiments,\nwe set NPP = 100 processes."
        },
        {
            "heading": "4 Experiments and Results",
            "text": "We use BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019) and a domain-specific version of BERT - BioBERT (Lee et al., 2020) as the underlying LMs. Specifically, we use BERTBase(110M) and GPT2-Small(124M). For BERT, we use English Wikipedia in conjunction with BooksCorpus as the pre-training corpora and employ MLM and NSP tasks for pre-training following details in the work of Devlin et al. (2019). We perform pre-training using a batch size of 1024 for 1,000,000 steps in the case of vanilla-BERT. We perform ablations over data subset sizes and number of pre-training steps for INGENIOUS enabled pre-training and find a subset size of 25% (Appendix J) with 250,000 pre-training steps (25%) as an optimal choice. We set the value of R to 25000 steps. We refer the reader to Appendix G for further implementation details. For INGENIOUS enabled pre-training of BioBERT and GPT-2, we discuss the implementation details and experimental results in Sections 4.4 and 4.5, respectively."
        },
        {
            "heading": "4.1 INGENIOUS for BERT Pre-training",
            "text": "We consider two leagues of pre-trained models, viz., (i) BERT pre-trained on subsets selected through INGENIOUS and (ii) vanilla BERT pretrained fully up to 1 million steps. We contrast these by fine-tuning each on the commonly used GLUE benchmark (Wang et al., 2019) and report the performances of each. Further, we compare INGENIOUS against three baselines - B1) Random Selection: which is obtained by pretraining BERT on a randomly sampled subset of the same size as that selected by INGENIOUS; B2) Early Stopping: BERT pre-training stopped at 250K steps as checkpoint for evaluation; B3) Loss Based Sampling (Loshchilov and Hutter, 2016): which is obtained by pre-training BERT on a subset, of the same size as those selected by INGENIOUS, sampled from a probability distribution that is constructed by ranking the losses in descending order and allocating the high rank (high loss) samples greater probability than low rank (low loss) samples. We would like to emphasise that we choose the baselines B1 and B3 owing to their relevance to making LM pre-training efficient with respect to data optimization. Table 1 reports the GLUE score averaged over 20 runs on the dev sets obtained after 250K pre-training steps for the pre-trained models obtained by different methods.\nWe observe that despite using only a subset of training data and being trained only for 250K steps, INGENIOUS achieves 98.6% performance of the vanilla fully pre-trained BERT. Further, INGENIOUS achieves statistically significant improvements over the three baselines (B1, B2, and B3). INGENIOUS also outperforms baseline B3, which prioritizes training the BERT model on samples with a high loss rate. Prioritizing high-loss samples may likely result in overfitting, which may explain the poor fine-tuning performance of baseline B3 on GLUE tasks compared to baseline B2. Therefore, INGENIOUS selects informative subsets that not only help improve BERT pretraining convergence but also help retain its generalization capabilities. Further, we observe that extended training of INGENIOUS till 400K steps yields 99.1% performance of the vanilla BERT. We would like to highlight that most of the downstream task performance achieved by an PTLM is due to the initial stages of pre-training with most of the later pre-training resulting in up to \u223c 1% improvement (Smith et al., 2022). In this context, INGENIOUS helps in achieving later-stage performance gains relatively earlier. Finally, we would like to highlight that INGENIOUS performs significantly better compared to the baselines on\nthe CoLA task (in Table 1) which is deemed to be most difficult (Geiping and Goldstein, 2022) in the GLUE benchmark. This implies that the subsets selected by INGENIOUS are able to capture the important and highly informative signals from the underlying data resulting in robust performance on challenging tasks as well.\nFurther, to compare different methods at different stages of pre-training, we obtain corresponding checkpoints and fine-tune on GLUE tasks. For this particular setting, we present a comparison of vanilla BERT pre-training against INGENIOUS in Figure 3. We plot the downstream performance for all the methods and it can be seen that INGENIOUS shows better performance than all the baselines at 250K steps of pre-training and thereafter, beyond 250K steps, the trend continues consistently (Figure 3 - top). Also, pre-training through informative subsets enables BERT to achieve a performance level at 250K steps which the vanilla pretraining achieves only after over 350K iterations. Similarly, for any given pre-training cost, INGENIOUS yields a better GLUE score than the baselines (Figure 3 - bottom). Further we observe that INGENIOUS consistenly outperforms the baselines even when extended to 1 million steps(maximum number of training steps prescribed by Devlin et al. (2019) for vanilla BERT pre-training) as shown in Figure 4.\nEffectiveness of Importance Sampling: We also evaluated a variant where the samples are selected greedily based on submodular ranking instead of importance sampling over submodular gains. In contrast to the 81.57 achieved by INGENIOUS, it achieved an Avg. GLUE score of 80.5 after 250K pre-training steps, highlighting the effectiveness\nof importance sampling."
        },
        {
            "heading": "4.2 Knowledge Retention with INGENIOUS",
            "text": "Large PTLMs, when trained on a sufficiently large corpus, stores various types of knowledge implicitly in their parameters (AlKhamissi et al., 2022). Since INGENIOUS uses only a subset of the whole data for pre-training, it is natural for it to contain lesser knowledge in its parameters but how does it compare with vanilla BERT pretraining and other baseline when it comes to knowledge retention? To answer this question, we use LAMA benchmark (Petroni et al., 2019), a probe designed to analyze factual knowledge present in PTLMs. LAMA is derived from four distinct types of knowledge sources - Google-RE, T-REx, ConceptNet, and SQuAD \u2013 from which cloze sentences are created using facts contained in the respective knowledge sources. The PTLM has to predict the fact tokens in place of the mask tokens in cloze sentences. In Table 2, we summarize the results. We note that INGENIOUS suffers minimal loss in knowledge retention with respect to fully pre-trained vanilla BERT on all tasks. Further, the decrease in performance is less as compared to the baselines (for most tasks) which suffer a more severe decrease in performance. Intuitively, we attribute this to the ability of INGENIOUS to select highly informative subsets from the corpus while excluding the redundant information."
        },
        {
            "heading": "4.3 Effect of Embedding Representations",
            "text": "Different BERT layers have been shown to capture different information - lower layers capture word order (Rogers et al., 2020), middle capture syntactic information (Hewitt and Manning, 2019; Jawahar et al., 2019) and the later layers capture task-specific information (Kovaleva et al., 2019; Hao et al., 2019). We vary the layers - (3, 6, 9 and 12) used to obtain features for subset selection\nEmbeddings Representation Avg. GLUE Score\nand report the performance on GLUE in Table 3. We observe that layer 9 features yield the best results. Further, in Table 3, we compare the effect of using TF-IDF as sample representations and contrast them against dense features (BERT Layer-9). We observe that dense embeddings perform better than shallow TF-IDF features. We also report effect of subset size and number of partitions in Appendices J and K."
        },
        {
            "heading": "4.4 INGENIOUS for Domain-Specific PTLM - BioBERT",
            "text": "We evaluate the performance of Bio-BERT (Lee et al., 2020) pre-trained on subsets selected through INGENIOUS and compare it with vanilla Bio-BERT by fine-tuning it on biomedical datasets for the Named Entity Recognition (NER) and Relation Extraction (RE) tasks. For vanilla BioBERT, we start with a pre-trained BERT model and further pre-train it on the PubMed abstracts dataset for 200,000 steps(as recommended by the original study). Please refer to Appendix I for further implementation details. We present the performance convergence plots of vanilla Bio-BERT vs. training time using INGENIOUS with a subset size of 25% in Figure 5. It shows that during initial\nstages of pre-training, INGENIOUS performs similar to vanilla since the LM is still learning representations, however once better representations for subset selection are learned, INGENIOUS achieves faster convergence than vanilla w.r.t pre-training time and achieves the best accuracy around 1.4x faster."
        },
        {
            "heading": "4.5 INGENIOUS for GPT-2 Pre-training",
            "text": "We also pre-train GPT-2 (Radford et al., 2019) using INGENIOUS. We estimate the mean accuracy for GLUE fine-tuning (averaged over 20 runs) and zero-shot accuracy on BBQ Lite generative task. Please refer to Appendix H for implementation details. We plot the performance (see Figure 6) obtained for the above benchmarks against checkpoints at different pre-training stages (steps). Figure 6 - left and right shows that INGENIOUS performs consistently better than vanilla GPT-2 pretraining on GLUE and BBQ Lite respectively at different stages of pre-training indicating better convergence."
        },
        {
            "heading": "5 Conclusions",
            "text": "We presented INGENIOUS, a framework for efficient pre-training of language models using highly informative data subsets, and presented a submodular optimization based algorithm. We described how it can be scaled for language models and showed its effectiveness using rigorous empirical evaluation. Our future work will explore exploiting external knowledge bases to identify and reduce redundancies in the corpus and to study multi-modal training where redundant information can be spread across different modalities."
        },
        {
            "heading": "6 Limitations",
            "text": "In terms of limitations, the submodular maximization based on estimation of pairwise sample sim-\nilarity can be potentially constrained by memory limitations and might require high CPU RAM capacity. Further, we do acknowledge that our experiments are performed on relatively smaller PTLMs compared to GPT-3, OPT or PaLM owing to resource limitations. We have tried our best to perform extensive experiments and perform ablation studies to inform our design choices within our resource constraints."
        },
        {
            "heading": "7 Ethical Considerations",
            "text": "We believe that INGENIOUS has a significant positive impact on society since it makes pre-training of LMs compute efficient, thereby reducing CO2 emissions and energy costs. Nonetheless, the INGENIOUS framework is susceptible to biases and toxic words within the pre-training corpora as it relies on standard pre-training datasets. An exciting future direction of this research is to investigate whether we could use targeted subset selection to filter out toxic words, as well as phrases that promote cultural stereotypes and biases from the pre-training corpora before LM pre-training."
        },
        {
            "heading": "8 Acknowledgments and Disclosure of Funding",
            "text": "This work is supported by an Adobe Data Science Award, and by the National Science Foundation under Grant No. IIS-2106937 awarded to Rishabh Iyer. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Adobe or the National Science Foundation. Ganesh Ramakrishnan is grateful to the Adobe Award and Institute Chair Professorship Award at IIT Bombay for supporting this work."
        },
        {
            "heading": "A Code, Software, and Licenses",
            "text": "The data and code for INGENIOUS is available at the following url: https://github.com/Efficient-AI/ ingenious. We release the code repositories of INGENIOUS with an MIT license, which is available for everybody to use freely.\nAll the code is developed using open-source HuggingFace for training LMs with PyTorch as the underlying framework. PyTorch is available under the BSD license. HuggingFace is available under Apache 2.0 license. For submodular optimization, we use a library called SUBMODLIB (Kaushal et al., 2022), which is freely available at https://github.com/decile-team/submodlib which is available under the MIT license."
        },
        {
            "heading": "B Additional Background and Related Work",
            "text": "Submodular Functions: Let U denote the unlabeled set of n data points U = {1, 2, 3, ..., n} and a set function f : 2U \u2212\u2192 R. Formally, a function f is submodular (Fujishige, 2005; Bilmes, 2022) if for x \u2208 U , f(A\u222a x)\u2212 f(A) \u2265 f(B \u222a x)\u2212 f(B), \u2200A \u2286 B \u2286 U and x /\u2208 B. For a set A \u2286 U , f(A) provides a real-valued score for A. A function f is said to be monotone if f(A) \u2264 f(B) whenever A \u2286 B. Further, f is supermodular if \u2212f is submodular, modular if it is both, and normalized if f(\u03d5) = 0. Submodularity occurs naturally in various real-world applications (Tohidi et al., 2020; Bach, 2013, 2019; Iyer, 2015) and a number of combinatorial functions, such as facility location, set cover, log determinant, graph cut, etc. (Iyer et al., 2021; Iyer and Bilmes, 2019; Kothawade et al., 2020, 2021; Karanam et al., 2022) are inherently submodular in nature. Submodularity is particularly attractive due to the constant factor 1 \u2212 1e (Nemhauser et al., 1978) approximation for cardinality-constrained submodular maximization, allowing us to solve various combinatorial optimization problems, which are often NPHard in nature. Several recent works (Wei et al., 2014a, 2015; Mirzasoleiman et al., 2020; Killamsetty et al., 2021b,a,c, 2022; Kothawade et al., 2021; Karanam et al., 2022) have formulated the subset selection objective as a submodular maximization problem. Furthermore, variants of the greedy algorithm (Mirzasoleiman et al., 2015; Iyer and Bilmes, 2019) that can maximize a submod-\nular function in near-linear time have been proposed.\nSubmodular Data Subset Selection: Submodular optimization has been successfully employed for data subset selection in various applications such as speech recognition (Wei et al., 2014b,a; Mittal et al., 2022), machine translation (Kirchhoff and Bilmes, 2014), active-learning (Wei et al., 2015; Kothawade et al., 2021), efficient deep learning (Kaushal et al., 2019; Killamsetty et al., 2022; Pooladzandi et al., 2022). Another active area of research is selecting representative subsets of data, also known as coresets (Feldman, 2020). A coreset is a weighted subset of data closely approximating certain desirable properties of the entire dataset (e.g., the loss function) (Feldman, 2020). Coreset selection has been shown to benefit a host of geometric problems such as k-means and k-median clustering (Har-Peled and Mazumdar, 2004) and, in recent times, has been used successfully for efficient bayesian inference (Campbell and Broderick, 2018) and improving training efficiency (Mirzasoleiman et al., 2020; Killamsetty et al., 2021a). Such informative data subset selection has shown remarkable promise for efficient and robust training of deep models (Killamsetty et al., 2021b,c). We direct the reader to a survey by Bilmes (2022) for a detailed review of submodularity and subset selection for ML."
        },
        {
            "heading": "C Datasets",
            "text": "For pre-training BERT, we use English Wikipedia, BooksCorpus datasets. English Wikipedia is \u223c20GiB of text containing 6,458,670 articles and BooksCorpus is \u223c5GiB of text containing 74,004,228 lines of text. For GPT-2, we use OpenWebText which is an open source replication of WebText dataset from OpenAI. OpenWebtext is around \u223c40GiB of text around 8,013,769 articles."
        },
        {
            "heading": "D Compute Infrastructure",
            "text": "All our pre-trainings were done on Google Cloud Platform (GCP) instances comprising of 8 NVIDIA A100-SXM4-40GB GPUs for BERT(bert-base-uncased: 110M parameters) and 16 NVIDIA A100-SXM4-40GB GPUs for GPT2(gpt2-small: 124M parameters). In each instance, there are 96 CPU cores with a total RAM of 680GiB. The costs are estimated using https: //cloud.google.com/products/calculator based on the time taken for training."
        },
        {
            "heading": "E Pre-Training performance of INGENIOUS for BERT",
            "text": "In Table 7, we show how validation-set losses change for vanilla BERT and INGENIOUS BERT over the course of pretraining."
        },
        {
            "heading": "F GLUE Task wise performance of INGENIOUS for BERT",
            "text": "We show task-wise performance on GLUE for BERT trained through INGENIOUS in Table 4. We compare against vanilla LM pre-training and baselines. We also report the standard deviation for each task along with the mean glue score."
        },
        {
            "heading": "G Further implementation details of pre-training BERT through",
            "text": "INGENIOUS\nWe use Adam optimizer (Kingma and Ba, 2014) with learning rate of 1e-4, \u03b21 = 0.9, \u03b22 = 0.99, L2 weight decay of 0.01. We warmstart the model for the first 80K training steps and subsequently train only on selected subsets. Training of all the models is performed on 8 NVIDIA A100-SXM440GB GPUs.\nH Implementation details of pre-training GPT-2 through INGENIOUS\nFor GPT-2, we use OpenWebtext(An open-source replication of WebText dataset from OpenAI) as the pre-training corpus and employ CLM task for pre-training following details in the work of Radford et al. (2019). We perform pre-training using a batch size of 256 (achieved using gradient accumulation of 2 steps) for 1,000,000 steps in case of vanilla GPT-2. With INGENIOUS, we pre-train GPT-2 for 250,000 steps. We set the value of R as 25K steps. We use Adam Optimizer (Kingma and Ba, 2014) with learning rate of 1e-4, \u03b21 = 0.9, \u03b22 = 0.99, L2 weight decay of 0.01. We warmstart the model the first 65K training steps and subsequently train only on selected subsets. Training of all the models is performed on 16 NVIDIA A100SXM4-40GB GPUs.\nI Implementation details of pre-training BioBERT through INGENIOUS\nWe use Adam optimizer (Kingma and Ba, 2014) with learning rate of 1e-4, \u03b21 = 0.9, \u03b22 = 0.99, L2 weight decay of 0.01. For Bio-BERT training using INGENIOUS, we use a subset size of 25% ,\nSubset Size (% of full dataset) Avg. GLUE Score\nNumber of partitions Avg. GLUE Score\nR value of 5000, no model warm-start, i.e., W = 0, and trained the Bio-BERT model for 200,000 steps."
        },
        {
            "heading": "J Subset size for efficiency gains",
            "text": "We study the effect of the size of the subset selected through INGENIOUS that is used for pretraining BERT. In Table 5, we analyse using the following values of subset sizes, viz., 10%, 15%, 20%, 25% and 30% and evaluate the fine-tuning performance on GLUE. While lower subset sizes (10-20%) result in inferior performance owing to the fact that the LM is shown less information, optimal performance is observed when 25% of the pre-training corpus is used, hence, we report corresponding results in Table 1."
        },
        {
            "heading": "K Partitions for efficient subset selection",
            "text": "As discussed in approach, we divide the pretraining dataset into partitions. In Table 6, we analyse the impact of performance on GLUE as the number of partitions is varied. Using fewest partitions (1500) is found to yield optimal performance. This aligns with the intuition that fewest partitions enable better subset selection since more samples are present in a single partition, allowing to select more representative samples overall."
        },
        {
            "heading": "L Few Examples of informative texts sampled by INGENIOUS",
            "text": "We summarize the three types of redundancies that we found in our analysis of selected subsets. More examples can be found at https://github.com/Effic ient-AI/ingenious.\n\u2022 Type 1: Same information conveyed by multiple sentences in different documents.\n\u2013 Sentence 1: \"separate sovereign countries but acted as a single bloc in foreign policy and security issues. the proposed union was being discussed by a joint scandinavian committee during the winter of 1948 \u2013 1949, but the cold war tension between the united states and the soviet union, and preparations for a western alliance that would result in the north atlantic treaty overshadowed the effort. when it became\" \u2013 Sentence 2: \"they would remain separate sovereign countries but act as a single block in foreign policy and security issues. the proposed union was discussed by a joint scandinavian committee during the winter of 1948 \u2013 1949, but in the end the cold war tension between the united states and the soviet union and preparations for a western alliance that would result in\"\nPre-Training Steps 100K 150K 200K 250K 300K 350K 400K 450K 500K 550K 600K 650K 700K 750K 800K 850K 900K 950K 1000K Vanilla BERT 2.29 2.07 1.99 1.92 1.88 1.86 1.83 1.8 1.79 1.78 1.76 1.74 1.73 1.73 1.71 1.7 1.69 1.69 1.68 INGENIOUS BERT 2.29 2.09 2.01 1.95 1.91 1.88 1.85 1.85 1.82 1.8 1.79 1.78 1.76 1.76 1.74 1.74 1.72 1.71 1.7"
        }
    ],
    "title": "INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models",
    "year": 2023
}