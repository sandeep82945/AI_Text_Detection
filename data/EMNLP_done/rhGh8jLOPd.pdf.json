{
    "abstractText": "Visual Question Answering (VQA) has been primarily studied through the lens of the English language. Yet, tackling VQA in other languages in the same manner would require a considerable amount of resources. In this paper, we propose scalable solutions to multilingual visual question answering (mVQA), on both data and modeling fronts. We first propose a translation-based framework to mVQA data generation that requires much less human annotation efforts than the conventional approach of directly collection questions and answers. Then, we apply our framework to the multilingual captions in the Crossmodal-3600 dataset and develop an efficient annotation protocol to create MaXM, a test-only VQA benchmark in 7 diverse languages. Finally, we develop a simple, lightweight, and effective approach as well as benchmark state-of-the-art English and multilingual VQA models. We hope that our benchmark encourages further research on mVQA.",
    "authors": [
        {
            "affiliations": [],
            "name": "Soravit Changpinyo"
        },
        {
            "affiliations": [],
            "name": "Linting Xue"
        },
        {
            "affiliations": [],
            "name": "Michal Yarom"
        },
        {
            "affiliations": [],
            "name": "Ashish V. Thapliyal"
        },
        {
            "affiliations": [],
            "name": "Idan Szpektor"
        },
        {
            "affiliations": [],
            "name": "Julien Amelot"
        },
        {
            "affiliations": [],
            "name": "Xi Chen"
        },
        {
            "affiliations": [],
            "name": "Radu Soricut"
        }
    ],
    "id": "SP:3aa0154693fde138c33455cca405df2f0a567969",
    "references": [
        {
            "authors": [
                "Pranav Aggarwal",
                "Ajinkya Kale."
            ],
            "title": "Towards zero-shot cross-lingual image retrieval",
            "venue": "arXiv preprint arXiv:2012.05107.",
            "year": 2020
        },
        {
            "authors": [
                "Aishwarya Agrawal",
                "Dhruv Batra",
                "Devi Parikh",
                "Aniruddha Kembhavi."
            ],
            "title": "Don\u2019t just assume; look and answer: Overcoming priors for visual question answering",
            "venue": "CVPR.",
            "year": 2018
        },
        {
            "authors": [
                "Harsh Agrawal",
                "Karan Desai",
                "Yufei Wang",
                "Xinlei Chen",
                "Rishabh Jain",
                "Mark Johnson",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee",
                "Peter Anderson."
            ],
            "title": "nocaps: novel object captioning at scale",
            "venue": "ICCV.",
            "year": 2019
        },
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katie Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198",
            "year": 2022
        },
        {
            "authors": [
                "Chris Alberti",
                "Daniel Andor",
                "Emily Pitler",
                "Jacob Devlin",
                "Michael Collins."
            ],
            "title": "Synthetic QA corpora generation with roundtrip consistency",
            "venue": "ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C. Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "VQA: Visual question answering",
            "venue": "ICCV.",
            "year": 2015
        },
        {
            "authors": [
                "Pratyay Banerjee",
                "Tejas Gokhale",
                "Yezhou Yang",
                "Chitta Baral."
            ],
            "title": "WeaQA: Weak supervision via captions for visual question answering",
            "venue": "Findings of ACL-IJCNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Lo\u00efc Barrault",
                "Fethi Bougares",
                "Lucia Specia",
                "Chiraag Lala",
                "Desmond Elliott",
                "Stella Frank."
            ],
            "title": "Findings of the third shared task on multimodal machine translation",
            "venue": "In",
            "year": 2018
        },
        {
            "authors": [
                "Ali Furkan Biten",
                "Ruben Tito",
                "Andres Mafla",
                "Lluis Gomez",
                "Mar\u00e7al Rusinol",
                "Ernest Valveny",
                "C.V. Jawahar",
                "Dimosthenis Karatzas."
            ],
            "title": "Scene text visual question answering",
            "venue": "ICCV.",
            "year": 2019
        },
        {
            "authors": [
                "James Bradbury",
                "Roy Frostig",
                "Peter Hawkins",
                "Matthew James Johnson",
                "Chris Leary",
                "Dougal Maclaurin",
                "George Necula",
                "Adam Paszke",
                "Jake VanderPlas",
                "Skye Wanderman-Milne",
                "Qiao Zhang"
            ],
            "title": "JAX: composable transformations of Python+NumPy programs",
            "year": 2018
        },
        {
            "authors": [
                "Emanuele Bugliarello",
                "Fangyu Liu",
                "Jonas Pfeiffer",
                "Siva Reddy",
                "Desmond Elliott",
                "Edoardo Maria Ponti",
                "Ivan Vuli\u0107."
            ],
            "title": "IGLUE: A benchmark for transfer learning across modalities, tasks, and languages",
            "venue": "ICML.",
            "year": 2022
        },
        {
            "authors": [
                "Soravit Changpinyo",
                "Doron Kukliansky",
                "Idan Szpektor",
                "Xi Chen",
                "Nan Ding",
                "Radu Soricut."
            ],
            "title": "All you may need for VQA are image captions",
            "venue": "NAACL.",
            "year": 2022
        },
        {
            "authors": [
                "Soravit Changpinyo",
                "Piyush Sharma",
                "Nan Ding",
                "Radu Soricut."
            ],
            "title": "Conceptual 12M: Pushing web-scale imagetext pre-training to recognize long-tail visual concepts",
            "venue": "CVPR.",
            "year": 2021
        },
        {
            "authors": [
                "Houlsby",
                "Radu Soricut."
            ],
            "title": "PaLI: A jointly-scaled multilingual language-image model",
            "venue": "ICLR.",
            "year": 2023
        },
        {
            "authors": [
                "Xinlei Chen",
                "Hao Fang",
                "Tsung-Yi Lin",
                "Ramakrishna Vedantam",
                "Saurabh Gupta",
                "Piotr Doll\u00e1r",
                "C. Lawrence Zitnick."
            ],
            "title": "Microsoft COCO Captions: Data collection and evaluation server",
            "venue": "arXiv preprint arXiv:1504.00325.",
            "year": 2015
        },
        {
            "authors": [
                "Jaemin Cho",
                "Jie Lei",
                "Hao Tan",
                "Mohit Bansal."
            ],
            "title": "Unifying vision-and-language tasks via text generation",
            "venue": "ICML.",
            "year": 2021
        },
        {
            "authors": [
                "Karan Desai",
                "Gaurav Kaul",
                "Zubin Aysola",
                "Justin Johnson."
            ],
            "title": "RedCaps: Web-curated image-text data created by the people, for the people",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition",
            "year": 2021
        },
        {
            "authors": [
                "Desmond Elliott",
                "Stella Frank",
                "Lo\u00efc Barrault",
                "Fethi Bougares",
                "Lucia Specia."
            ],
            "title": "Findings of the second shared task on multimodal machine translation and multilingual image description",
            "venue": "Proceedings of the Second Conference on Machine Translation.",
            "year": 2017
        },
        {
            "authors": [
                "Desmond Elliott",
                "Stella Frank",
                "Khalil Sima\u2019an",
                "Lucia Specia"
            ],
            "title": "Multi30K: Multilingual English-German image descriptions",
            "venue": "In Proceedings of the 5th Workshop on Vision and Language,",
            "year": 2016
        },
        {
            "authors": [
                "Yash Goyal",
                "Tejas Khot",
                "Douglas Summers-Stay",
                "Dhruv Batra",
                "Devi Parikh."
            ],
            "title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering",
            "venue": "CVPR.",
            "year": 2017
        },
        {
            "authors": [
                "Jiaxi Gu",
                "Xiaojun Meng",
                "Guansong Lu",
                "Lu Hou",
                "Minzhe Niu",
                "Hang Xu",
                "Xiaodan Liang",
                "Wei Zhang",
                "Xin Jiang",
                "Chunjing Xu."
            ],
            "title": "Wukong: 100 million large-scale chinese cross-modal pre-training dataset and a foundation framework",
            "venue": "arXiv preprint arXiv:2202.06767.",
            "year": 2022
        },
        {
            "authors": [
                "Danna Gurari",
                "Qing Li",
                "Abigale J. Stangl",
                "Anhong Guo",
                "Chi Lin",
                "Kristen Grauman",
                "Jiebo Luo",
                "Jeffrey P. Bigham."
            ],
            "title": "VizWiz Grand Challenge: Answering visual questions from blind people",
            "venue": "CVPR.",
            "year": 2018
        },
        {
            "authors": [
                "Or Honovich",
                "Leshem Choshen",
                "Roee Aharoni",
                "Ella Neeman",
                "Idan Szpektor",
                "Omri Abend."
            ],
            "title": "Q2: Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Drew A. Hudson",
                "Christopher D. Manning."
            ],
            "title": "GQA: A new dataset for real-world visual reasoning and compositional question answering",
            "venue": "CVPR.",
            "year": 2019
        },
        {
            "authors": [
                "Justin Johnson",
                "Bharath Hariharan",
                "Laurens Van Der Maaten",
                "Li Fei-Fei",
                "C. Lawrence Zitnick",
                "Ross Girshick."
            ],
            "title": "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning",
            "venue": "CVPR.",
            "year": 2017
        },
        {
            "authors": [
                "Melvin Johnson",
                "Mike Schuster",
                "Quoc V. Le",
                "Maxim Krikun",
                "Yonghui Wu",
                "Zhifeng Chen",
                "Nikhil Thorat",
                "Fernanda Vi\u00e9gas",
                "Martin Wattenberg",
                "Greg Corrado",
                "Macduff Hughes",
                "Jeffrey Dean"
            ],
            "title": "2017b. Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation",
            "year": 2017
        },
        {
            "authors": [
                "Kushal Kafle",
                "Christopher Kanan."
            ],
            "title": "An analysis of visual question answering algorithms",
            "venue": "ICCV.",
            "year": 2017
        },
        {
            "authors": [
                "Andrej Karpathy",
                "Li Fei-Fei."
            ],
            "title": "Deep visual-semantic alignments for generating image descriptions",
            "venue": "CVPR.",
            "year": 2015
        },
        {
            "authors": [
                "Cai",
                "Zheyun Feng",
                "Dhyanesh Narayanan",
                "Kevin Murphy."
            ],
            "title": "OpenImages: A public dataset for large-scale multi-label and multi-class image classification",
            "venue": "Dataset available from https://g.co/dataset/openimages.",
            "year": 2017
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Yuke Zhu",
                "Oliver Groth",
                "Justin Johnson",
                "Kenji Hata",
                "Joshua Kravitz",
                "Stephanie Chen",
                "Yannis Kalantidis",
                "Li-Jia Li",
                "David A. Shamma",
                "Michael Bernstein",
                "Li Fei-Fei"
            ],
            "title": "Visual Genome: Connecting language and vision using crowdsourced dense image annotations",
            "year": 2017
        },
        {
            "authors": [
                "Alexander Ku",
                "Peter Anderson",
                "Roma Patel",
                "Eugene Ie",
                "Jason Baldridge."
            ],
            "title": "Room-Across-Room: Multilingual vision-and-language navigation with dense spatiotemporal grounding",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Alina Kuznetsova",
                "Hassan Rom",
                "Neil Alldrin",
                "Jasper R.R. Uijlings",
                "Ivan Krasin",
                "Jordi Pont-Tuset",
                "Shahab Kamali",
                "Stefan Popov",
                "Matteo Malloci",
                "Tom Duerig",
                "Vittorio Ferrari"
            ],
            "title": "The open images dataset V4: unified image classification, object detection, and visual relationship",
            "year": 2020
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597.",
            "year": 2023
        },
        {
            "authors": [
                "Linjie Li",
                "Jie Lei",
                "Zhe Gan",
                "Jingjing Liu."
            ],
            "title": "Adversarial VQA: A new benchmark for evaluating the robustness of vqa models",
            "venue": "ICCV.",
            "year": 2021
        },
        {
            "authors": [
                "Xirong Li",
                "Chaoxi Xu",
                "Xiaoxu Wang",
                "Weiyu Lan",
                "Zhengxiong Jia",
                "Gang Yang",
                "Jieping Xu."
            ],
            "title": "COCO-CN for cross-lingual image tagging, captioning, and retrieval",
            "venue": "IEEE Transactions on Multimedia, 21(9):2347\u20132360.",
            "year": 2019
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out.",
            "year": 2004
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "Lubomir Bourdev",
                "Ross Girshick",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "C. Lawrence Zitnick",
                "Piotr Doll\u00e1r."
            ],
            "title": "Microsoft COCO: Common objects in context",
            "venue": "ECCV.",
            "year": 2014
        },
        {
            "authors": [
                "Fangyu Liu",
                "Emanuele Bugliarello",
                "Edoardo Maria Ponti",
                "Siva Reddy",
                "Nigel Collier",
                "Desmond Elliott."
            ],
            "title": "Visually grounded reasoning across languages and cultures",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Kenneth Marino",
                "Mohammad Rastegari",
                "Ali Farhadi",
                "Roozbeh Mottaghi."
            ],
            "title": "OK-VQA: A visual question answering benchmark requiring external knowledge",
            "venue": "CVPR.",
            "year": 2019
        },
        {
            "authors": [
                "Vicente Ordonez",
                "Girish Kulkarni",
                "Tamara Berg."
            ],
            "title": "Im2Text: Describing images using 1 million captioned photographs",
            "venue": "NIPS.",
            "year": 2011
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Gregor Geigle",
                "Aishwarya Kamath",
                "JanMartin O Steitz",
                "Stefan Roth",
                "Ivan Vuli\u0107",
                "Iryna Gurevych."
            ],
            "title": "xGQA: Cross-lingual visual question answering",
            "venue": "Findings of ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Jordi Pont-Tuset",
                "Jasper Uijlings",
                "Soravit Changpinyo",
                "Radu Soricut",
                "Vittorio Ferrari."
            ],
            "title": "Connecting vision and language with localized narratives",
            "venue": "ECCV.",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "JMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Know what you don\u2019t know: Unanswerable questions for SQuAD",
            "venue": "ACL.",
            "year": 2018
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "EMNLP.",
            "year": 2016
        },
        {
            "authors": [
                "Mengye Ren",
                "Ryan Kiros",
                "Richard Zemel."
            ],
            "title": "Exploring models and data for image question answering",
            "venue": "NIPS.",
            "year": 2015
        },
        {
            "authors": [
                "Teven Le Scao",
                "Stella Biderman",
                "Leo Gao",
                "Thomas Wolf",
                "Alexander M. Rush."
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "ICLR.",
            "year": 2022
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Richard Vencu",
                "Romain Beaumont",
                "Robert Kaczmarczyk",
                "Clayton Mullis",
                "Aarush Katta",
                "Theo Coombes",
                "Jenia Jitsev",
                "Aran Komatsuzaki."
            ],
            "title": "LAION-400M: Open dataset of CLIP-filtered 400 million image-text pairs",
            "venue": "arXiv preprint arXiv:2111.02114.",
            "year": 2021
        },
        {
            "authors": [
                "Dustin Schwenk",
                "Apoorv Khandelwal",
                "Christopher Clark",
                "Kenneth Marino",
                "Roozbeh Mottaghi."
            ],
            "title": "AOKVQA: A benchmark for visual question answering using world knowledge",
            "venue": "arXiv preprint arXiv:2206.01718.",
            "year": 2022
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut."
            ],
            "title": "Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "ACL.",
            "year": 2018
        },
        {
            "authors": [
                "Noam Shazeer",
                "Mitchell Stern."
            ],
            "title": "Adafactor: Adaptive learning rates with sublinear memory cost",
            "venue": "ICLR.",
            "year": 2018
        },
        {
            "authors": [
                "Sasha Sheng",
                "Amanpreet Singh",
                "Vedanuj Goswami",
                "Jose Alberto Lopez Magana",
                "Wojciech Galuba",
                "Devi Parikh",
                "Douwe Kiela."
            ],
            "title": "Human-adversarial visual question answering",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Amanpreet Singh",
                "Vivek Natarajan",
                "Meet Shah",
                "Yu Jiang",
                "Xinlei Chen",
                "Dhruv Batra",
                "Devi Parikh",
                "Marcus Rohrbach."
            ],
            "title": "Towards VQA models that can read",
            "venue": "CVPR.",
            "year": 2019
        },
        {
            "authors": [
                "Krishna Srinivasan",
                "Karthik Raman",
                "Jiecao Chen",
                "Michael Bendersky",
                "Marc Najork."
            ],
            "title": "WIT: Wikipedia-based image text dataset for multimodal multilingual machine learning",
            "venue": "SIGIR.",
            "year": 2021
        },
        {
            "authors": [
                "Ashish V. Thapliyal",
                "Jordi Pont-Tuset",
                "Xi Chen",
                "Radu Soricut."
            ],
            "title": "Crossmodal-3600: A massively multilingual multimodal evaluation dataset",
            "venue": "arXiv preprint arXiv:2205.12522.",
            "year": 2022
        },
        {
            "authors": [
                "Ramakrishna Vedantam",
                "C. Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "CIDEr: Consensus-based image description evaluation",
            "venue": "CVPR.",
            "year": 2015
        },
        {
            "authors": [
                "Jianfeng Wang",
                "Zhengyuan Yang",
                "Xiaowei Hu",
                "Linjie Li",
                "Kevin Lin",
                "Zhe Gan",
                "Zicheng Liu",
                "Ce Liu",
                "Lijuan Wang."
            ],
            "title": "GIT: A generative image-to-text transformer for vision and language",
            "venue": "arXiv preprint arXiv:2205.14100.",
            "year": 2022
        },
        {
            "authors": [
                "Peng Wang",
                "An Yang",
                "Rui Men",
                "Junyang Lin",
                "Shuai Bai",
                "Zhikang Li",
                "Jianxin Ma",
                "Chang Zhou",
                "Jingren Zhou",
                "Hongxia Yang."
            ],
            "title": "OFA: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework",
            "venue": "ICML.",
            "year": 2022
        },
        {
            "authors": [
                "Xin Wang",
                "Jiawei Wu",
                "Junkun Chen",
                "Lei Li",
                "Yuan-Fang Wang",
                "William Yang Wang."
            ],
            "title": "VaTeX: A large-scale, high-quality multilingual dataset for video-and-language research",
            "venue": "ICCV.",
            "year": 2019
        },
        {
            "authors": [
                "Zirui Wang",
                "Jiahui Yu",
                "Adams Wei Yu",
                "Zihang Dai",
                "Yulia Tsvetkov",
                "Yuan Cao."
            ],
            "title": "SimVLM: Simple visual language model pretraining with weak supervision",
            "venue": "ICLR.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "ICLR.",
            "year": 2022
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-totext transformer",
            "venue": "NAACL.",
            "year": 2021
        },
        {
            "authors": [
                "Antoine Yang",
                "Antoine Miech",
                "Josef Sivic",
                "Ivan Laptev",
                "Cordelia Schmid."
            ],
            "title": "Just ask: Learning to answer questions from millions of narrated videos",
            "venue": "ICCV.",
            "year": 2021
        },
        {
            "authors": [
                "Yuya Yoshikawa",
                "Yutaro Shigeto",
                "Akikazu Takeuchi."
            ],
            "title": "STAIR captions: Constructing a large-scale Japanese image caption dataset",
            "venue": "ACL.",
            "year": 2017
        },
        {
            "authors": [
                "Rowan Zellers",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Yejin Choi."
            ],
            "title": "From recognition to cognition: Visual commonsense reasoning",
            "venue": "CVPR.",
            "year": 2019
        },
        {
            "authors": [
                "Yuke Zhu",
                "Oliver Groth",
                "Michael Bernstein",
                "Fei-Fei Li."
            ],
            "title": "Visual7W: Grounded question answering in images",
            "venue": "CVPR.",
            "year": 2016
        },
        {
            "authors": [
                "Alayrac et al",
                "Wang"
            ],
            "title": "2022a), capable of open-ended VQA. We adopt this as a scalable and flexible modeling approach to mVQA as the language coverage increases",
            "venue": "In particular,",
            "year": 2022
        },
        {
            "authors": [
                "Wei"
            ],
            "title": "2022), where a task corresponds to VQA for a particular language. For the input question \u27e8question\u27e9 in language \u27e8lang\u27e9, we construct the prompt \u201cAnswer in \u27e8lang\u27e9: \u27e8question\u27e9",
            "year": 2022
        },
        {
            "authors": [
                "tor (Shazeer",
                "Stern"
            ],
            "title": "2018) with a \u03b21 of 0 and a second-moment exponential decay of 0.8. We use a linear warmup of 1K steps with a peak learning of learning rate of 1e-3 and inverse square-root decay. We set the ViT dropout rate to 0 and the",
            "year": 2018
        }
    ],
    "title": "MaXM: Towards Multilingual Visual Question Answering",
    "year": 2023
}