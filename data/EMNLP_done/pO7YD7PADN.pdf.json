{
    "abstractText": "Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model\u2019s predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Gustavo Gon\u00e7alves"
        },
        {
            "affiliations": [],
            "name": "Emma Strubell"
        }
    ],
    "id": "SP:a9b5aaeb0c076174d9f3258797ed2fa8615cda68",
    "references": [
        {
            "authors": [
                "Orevaoghene Ahia",
                "Julia Kreutzer",
                "Sara Hooker."
            ],
            "title": "The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation",
            "venue": "EMNLP (Findings), pages 3316\u2013 3333. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Arash Ahmadian",
                "Saurabh Dash",
                "Hongyu Chen",
                "Bharat Venkitesh",
                "Stephen Gou",
                "Phil Blunsom",
                "Ahmet \u00dcst\u00fcn",
                "Sara Hooker."
            ],
            "title": "Intriguing Properties of Quantization at Scale",
            "venue": "CoRR, abs/2305.19268.",
            "year": 2023
        },
        {
            "authors": [
                "Stella Biderman",
                "Hailey Schoelkopf",
                "Quentin Anthony",
                "Herbie Bradley",
                "Kyle O\u2019Brien",
                "Eric Hallahan",
                "Mohammad Aflah Khan",
                "Shivanshu Purohit",
                "USVSN Sai Prashanth",
                "Edward Raff",
                "Aviya Skowron",
                "Lintang Sutawika",
                "Oskar van der Wal"
            ],
            "title": "Pythia: A",
            "year": 2023
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Gilsinia Lopez",
                "Alexandra Olteanu",
                "Robert Sim",
                "Hanna M. Wallach."
            ],
            "title": "Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
            "venue": "ACL/IJCNLP (1), pages 1004\u20131015. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Tianlong Chen",
                "Jonathan Frankle",
                "Shiyu Chang",
                "Sijia Liu",
                "Yang Zhang",
                "Zhangyang Wang",
                "Michael Carbin."
            ],
            "title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks",
            "venue": "NeurIPS.",
            "year": 2020
        },
        {
            "authors": [
                "Pieter Delobelle",
                "Bettina Berendt."
            ],
            "title": "FairDistillation: Mitigating Stereotyping in Language Models",
            "venue": "ECML/PKDD (2), volume 13714 of Lecture Notes in Computer Science, pages 638\u2013654. Springer.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Maha Elbayad",
                "Jiatao Gu",
                "Edouard Grave",
                "Michael Auli."
            ],
            "title": "Depth-Adaptive Transformer",
            "venue": "ICLR. OpenReview.net.",
            "year": 2020
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima",
                "Shawn Presser",
                "Connor Leahy"
            ],
            "title": "The Pile: An 800GB Dataset of Diverse Text for Language",
            "year": 2021
        },
        {
            "authors": [
                "Michael Gira",
                "Ruisu Zhang",
                "Kangwook Lee."
            ],
            "title": "Debiasing Pre-Trained Language Models via Efficient Fine-Tuning",
            "venue": "LT-EDI, pages 59\u201369. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Marius Hessenthaler",
                "Emma Strubell",
                "Dirk Hovy",
                "Anne Lauscher."
            ],
            "title": "Bridging Fairness and Environmental Sustainability in Natural Language Processing",
            "venue": "EMNLP, pages 7817\u20137836. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Geoffrey E. Hinton",
                "Oriol Vinyals",
                "Jeffrey Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "NIPS Workshop on Deep Learning.",
            "year": 2015
        },
        {
            "authors": [
                "Sara Hooker",
                "Aaron Courville",
                "Gregory Clark",
                "Yann Dauphin",
                "Andrea Frome"
            ],
            "title": "What Do Compressed Deep Neural Networks Forget",
            "year": 2021
        },
        {
            "authors": [
                "Sara Hooker",
                "Nyalleng Moorosi",
                "Gregory Clark",
                "Samy Bengio",
                "Emily Denton."
            ],
            "title": "Characterising Bias in Compressed Models",
            "venue": "CoRR, abs/2010.03058.",
            "year": 2020
        },
        {
            "authors": [
                "Masahiro Kaneko",
                "Danushka Bollegala."
            ],
            "title": "Debiasing Pre-trained Contextualised Embeddings",
            "venue": "EACL, pages 1256\u20131266. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Masahiro Kaneko",
                "Danushka Bollegala."
            ],
            "title": "Unmasking the Mask - Evaluating Social Biases in Masked Language Models",
            "venue": "AAAI, pages 11954\u2013 11962. AAAI Press.",
            "year": 2022
        },
        {
            "authors": [
                "Masahiro Kaneko",
                "Danushka Bollegala",
                "Naoaki Okazaki."
            ],
            "title": "Debiasing Isn\u2019t Enough! - on the Effectiveness of Debiasing MLMs and Their Social Biases in Downstream Tasks",
            "venue": "COLING, pages 1299\u20131310. International Committee on Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Sneha Kudugunta",
                "Yanping Huang",
                "Ankur Bapna",
                "Maxim Krikun",
                "Dmitry Lepikhin",
                "Minh-Thang Luong",
                "Orhan Firat."
            ],
            "title": "Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference",
            "venue": "EMNLP (Findings), pages 3577\u20133599. Associa-",
            "year": 2021
        },
        {
            "authors": [
                "Keita Kurita",
                "Nidhi Vyas",
                "Ayush Pareek",
                "Alan W. Black",
                "Yulia Tsvetkov."
            ],
            "title": "Measuring Bias in Contextualized Word Representations",
            "venue": "CoRR, abs/1906.07337.",
            "year": 2019
        },
        {
            "authors": [
                "Faisal Ladhak",
                "Esin Durmus",
                "Mirac Suzgun",
                "Tianyi Zhang",
                "Dan Jurafsky",
                "Kathleen R. McKeown",
                "Tatsunori Hashimoto."
            ],
            "title": "When Do Pre-Training Biases Propagate to Downstream Tasks? A Case Study in Text Summarization",
            "venue": "EACL, pages 3198\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Paul Pu Liang",
                "Irene Mengze Li",
                "Emily Zheng",
                "Yao Chong Lim",
                "Ruslan Salakhutdinov",
                "LouisPhilippe Morency."
            ],
            "title": "Towards Debiasing Sentence Representations",
            "venue": "ACL, pages 5502\u20135515. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Lucas Liebenwein",
                "Cenk Baykal",
                "Brandon Carter",
                "David Gifford",
                "Daniela Rus."
            ],
            "title": "Lost in Pruning: The Effects of Pruning Neural Networks beyond Test Accuracy",
            "venue": "MLSys. mlsys.org.",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Nicholas Meade",
                "Elinor Poole-Dayan",
                "Siva Reddy."
            ],
            "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
            "venue": "ACL (1), pages 1878\u20131898. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Moin Nadeem",
                "Anna Bethke",
                "Siva Reddy."
            ],
            "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
            "venue": "ACL/IJCNLP (1), pages 5356\u2013 5371. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Nikita Nangia",
                "Clara Vania",
                "Rasika Bhalerao",
                "Samuel R. Bowman."
            ],
            "title": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
            "venue": "EMNLP (1), pages 1953\u20131967. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Kelechi Ogueji",
                "Orevaoghene Ahia",
                "Gbemileke Onilude",
                "Sebastian Gehrmann",
                "Sara Hooker",
                "Julia Kreutzer."
            ],
            "title": "Intriguing Properties of Compression on Multilingual Models",
            "venue": "EMNLP, pages 9092\u20139110. Association for Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Shauli Ravfogel",
                "Yanai Elazar",
                "Hila Gonen",
                "Michael Twiton",
                "Yoav Goldberg."
            ],
            "title": "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection",
            "venue": "ACL, pages 7237\u20137256. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter",
            "venue": "CoRR, abs/1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Timo Schick",
                "Sahana Udupa",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP",
            "venue": "Trans. Assoc. Comput. Linguistics, 9:1408\u20131424.",
            "year": 2021
        },
        {
            "authors": [
                "Kellie Webster",
                "Xuezhi Wang",
                "Ian Tenney",
                "Alex Beutel",
                "Emily Pitler",
                "Ellie Pavlick",
                "Jilin Chen",
                "Slav Petrov."
            ],
            "title": "Measuring and Reducing Gendered Correlations in Pre-trained Models",
            "venue": "CoRR, abs/2010.06032.",
            "year": 2020
        },
        {
            "authors": [
                "Canwen Xu",
                "Wangchunshu Zhou",
                "Tao Ge",
                "Ke Xu",
                "Julian J. McAuley",
                "Furu Wei."
            ],
            "title": "Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression",
            "venue": "EMNLP (1), pages 10653\u2013 10659. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Guangxuan Xu",
                "Qingyuan Hu."
            ],
            "title": "Can Model Compression Improve NLP Fairness",
            "venue": "CoRR, abs/2201.08542.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large Language Models (LLMs) are trained on large corpora using self-supervision, which allows models to consider vast amounts of unlabelled data, and learn language patterns through masking tasks (Devlin et al., 2019; Radford et al., 2019). However, self-supervision allows LLMs to pick up social biases contained in the training data. Which is amplified by larger models, more data, and longer training (Kaneko et al., 2022; Kaneko and Bollegala, 2022; Kurita et al., 2019; Delobelle and Berendt, 2022).\nSocial biases in LLMs are an ongoing problem that is propagated from pretraining to finetuning (Ladhak et al., 2023; Gira et al., 2022). Biased pretrained models are hard to fix, as retraining is\n1https://github.com/gsgoncalves/ EMNLP2023_llm_compression_and_social_ bias\nprohibitively expensive both financially and environmentally (Hessenthaler et al., 2022). At the same time, the compression of LLMs has been intensely studied. Pruning, quantization, and distillation are among the most common strategies to compress LLMs. Pruning reduces the parameters of a trained model by removing redundant connections while preserving equivalent performance to their original counterparts (Liebenwein et al., 2021; Ahia et al., 2021). Quantization reduces the precision of model weights and activations to improve efficiency while preserving performance (Ahmadian et al., 2023). Finally, knowledge distillation (Hinton et al., 2015) trains a smaller more efficient model based on a larger pre-trained model.\nWhile much research has been done on measuring and mitigating social bias in LLMs, and making LLMs smaller and more efficient, by using one or a combination of many compression methods (Xu et al., 2021), little research has been done regarding the interplay between social biases and LLM compression. Existing work has shown that pruning disproportionately impacts classification accuracy on low-frequency categories in computer vision models (Hooker et al., 2021), but that pruning transformer models can have a beneficial effect with respect to bias when modeling multilingual text (Hooker et al., 2020; Ogueji et al., 2022). Further, Xu and Hu (2022) have shown that compressing pretrained models improves model fairness by working as a regularizer against toxicity.\nUnlike previous work, our work focuses on the impacts of widely used quantization and distillation on the social biases exhibited by a variety of both encoder- and decoder-only LLMs. We focus on the effects of social bias over BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and Pythia LLMs (Biderman et al., 2023). We evaluate these models against Bias Bench (Meade et al., 2022), a compilation of three social bias datasets.\nIn our experimental results we demonstrate a cor-\nrelation between longer pretraining, larger models, and increased social bias, and show that quantization and distillation can reduce bias, demonstrating the potential for compression as a pragmatic approach for reducing social bias in LLMs."
        },
        {
            "heading": "2 Methodology",
            "text": "We were interested in understanding how dynamic Post-Training Quantization (PTQ) and distillation influence social bias contained in LLMs of different sizes, and along their pretraining. In dynamic PTQ, full-precision floating point model weights are statically mapped to lower precisions after training, with activations dynamically mapped from high to low precision during inference. To this end, in Section 2.1 we present the datasets of the Bias Bench benchmark (Meade et al., 2022) that enable us to evaluate three different language modeling tasks across the three social bias categories. In Section 2.2 we lay out the models we studied. We expand on the Bias Bench original evaluation by looking at the Large versions of the BERT and RoBERTa models, and the Pythia family of autoregressive models. The chosen models cover different language modeling tasks and span across a wide range of parameter sizes, thus providing a comprehensive view of the variations of social bias."
        },
        {
            "heading": "2.1 Measuring Bias",
            "text": "We use the Bias Bench benchmark for evaluating markers of social bias in LLMs. Bias Bench compiles three datasets, CrowS-Pairs (Nangia et al., 2020), StereoSet (SS) (Nadeem et al., 2021), and SEAT (Kaneko and Bollegala, 2021), for measuring intrinsic bias across three different identity categories: GENDER, RACE, and RELIGION. While the set of identities covered by this dataset is far from complete, it serves as a useful indicator as these models are encoding common social biases; however, the lack of bias indicated by this benchmark does not imply an overall lack of inappropriate bias in the model, for example with respect to other groups. We briefly describe each dataset below; refer to the original works for more detail.\nCrowS-Pairs is composed of pairs of minimally distant sentences that have been crowdsourced. A minimally distant sentence is defined as a small number of token swaps in a sentence, that carry different social bias interpretations. An unbiased\nmodel will pick an equal ratio of both stereotypical and anti-stereotypical choices, thus an optimal score for this dataset is a ratio of 50%.\nStereoSet is composed of crowdsourced samples. Each sample is composed of a masked context sentence, and a set of three candidate answers: 1) stereotypical, 2) anti-stereotypical, and 3) unrelated. Under the SS formulation, an unbiased model would give a balanced number of classifications of types 1) and 2), thus the optimal score is also 50%. The SS dataset also measures if we are changing the language modeling properties of our model. That is, if our model picks a high percentage of unrelated choices 3) it can be interpreted as losing its language capabilities. This is defined as the Language Model (LM) Score.\nSEAT evaluates biases in sentences. A SEAT task is defined by two sets of attribute sentences, and two other sets of target sentences. The objective of the task is to measure the distance of the sentence embeddings between the attribute and target sets to assess a preference between attributes and targets (bias). We provide more detail of this formulation in Appendix A.1."
        },
        {
            "heading": "2.2 Models",
            "text": "In this work, we focus on two popular methods for model compression: knowledge distillation and quantization. We choose these two methods given their competitive performance, wide deployment given the availability of distributions under the HuggingFace and Pytorch libraries, and the lack of understanding of the impact of these methods on social biases. We leave the study of more elaborate methods for improving model efficiency such as pruning (Chen et al., 2020), mixtures of experts (Kudugunta et al., 2021), and adaptive computation (Elbayad et al., 2020) to future work.\nSince model compression affects model size, we are particularly interested in understanding how pretrained model size impacts measures of social bias, and how that changes as a function of how well the model fits the data. We are also interested in investigating how the number of tokens observed during training impacts all of the above. We experiment with three different base LLMs: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and Pythia (Biderman et al., 2023), with uncompressed model sizes ranging from 70M parameters to 6.9B parameters. BERT and RoBERTa\nrepresent two similar sets of widely used and studied pretrained architectures, trained on different data with a small overlap. RoBERTa pretraining was done over 161 GB of text, which contained the 16GB used to train BERT, approximately a ten-fold increase. RoBERTa also trained for longer, with larger batch sizes which have shown to decrease the perplexity of the LLM (Liu et al., 2019).\nThe set of checkpoints released for the Pythia model family allows us to assess an even wider variety of model sizes and number of training tokens, including intermediate checkpoints saved during pretraining, so that we can observe how bias varies throughout pretraining. We used the models pretrained on the deduplicated version of The Pile (Gao et al., 2021) containing 768GB of text.\nKnowledge distillation (Hinton et al., 2015) is a popular technique for compressing the knowledge encoded in a larger teacher model into a smaller student model. In this work, we analyze DistilBERT (Sanh et al., 2019) and DistilRoBERTa2 distilled LMs. During training the student model minimizes the loss according to the predictions of\n2 https://huggingface.co/distilroberta-base\nthe teacher model (soft-targets) and the true labels (hard-targets) to better generalize to unseen data.\nQuantization compresses models by reducing the precision of their weights and activations during inference. We use the standard PyTorch implementation3 to apply dynamic PTQ over the linear layers of the transformer stack, from fp32 full-precision to quantized int8 precision. This work analyzes quantized BERT, RoBERTa, and Pythia models of a comprehensive range of sizes."
        },
        {
            "heading": "3 Results",
            "text": "Dynamic PTQ and distillation lower social bias. In Table 1 we analyze the effects of dynamic PTQ and distillation in the CrowS dataset, where BERT Base and RoBERTa Base are our baselines. To compare quantization and distillation, we add three debiasing baselines also referenced by Meade et al. (2022) that are competitive strategies to reduce bias. The INLP (Ravfogel et al., 2020) baseline consists of a linear classifier that learns to predict the target bias group given a set of context words, such as\n3 https://pytorch.org/tutorials/recipes/recipes/\ndynamic_quantization.html\n\u2019he/she\u2019. The Self-Debias baseline was proposed by Schick et al. (2021), and uses prompts to encourage models to generate toxic text and learns to give less weight to the generate toxic tokens. Self-Debias does not change the model\u2019s internal representation, thus it cannot be evaluated on the SEAT dataset.\nNotable trends in Table 1 are the reduction of social biases when applying dynamic PTQ and distillation, which can compete on average with the specifically designed debias methods. Additional results in in Appendix B also display similar trends. On the SS dataset in Table 4 we are also able to observe that the application of distillation provides remarkable decreases in social biases, at the great expense of LM score. However, dynamic PTQ shows a better trade-off in providing social bias reductions, while preserving LM score.\nOne model size does not fit all social biases. In Table 1 and the equivalent Tables in Appendix B we can see that social bias categories respond differently to model size, across the different datasets. While BERT Base/Large outperforms RoBERTa in GENDER, the best model for RACE and RELIGION varies across datasets. This can be explained by the different dataset tasks and the pretraining.\nIn Appendix B we show the social bias scores as\na function of the pretraining of the Pythia models in Figures 2 to 7, 9, 10 and 11. The BERT/RoBERTa Base and Large versions are roughly comparable with the 160M and 410M Pythia models. For the SS dataset, the 160M model is consistently less biased than the 410M model. However, this is not the case for the other two datasets where the 160M struggles in the RACE category while assessing the distance of sentence embeddings (SEAT); and in the RELIGION category while swapping minimally distant pairs (CrowS). This illustrates the difficulty of distinguishing between semantically close words, and shows the need for larger models pretrained for longer and on more data.\nLonger pretraining and larger models lead to more socially biased models. We study the effects of longer pretraining and larger models on social bias, by establishing the correlation of these variables in Figure 1. Here we can observe that as the model size increases so does the LM model score and social bias across the SS dataset. Moreover, later stages of pretraining have a higher LM model score, where the social bias score tends to be high. The application of dynamic PTQ shows a regularizer effect on all models.The Kendall Tau C across the models and categories shows a strong\ncorrelation between LM score and social bias. Statistical significant tests were performed using a one-sided t-test to evaluate the positive correlation.\nTables 2 and 3 show at what step, out of the 21 we tested, the best LM scores occur on the SS dataset. In Table 2 the best LM score increases monotonically with model size and so do the social biases. Interestingly, as the model size increases the best LM score appears after around 80% of the pretraining. In opposition, in Table 3, with dynamic PTQ the best LM score occurs around 20% of the pretraining and maintains the trend of higher LM score and social bias, albeit at lower scores than the original models. This shows an interesting possibility of early stopping depending on the deployment task of the LLM."
        },
        {
            "heading": "4 Limitations",
            "text": "While this work provides three different datasets, which have different views on social bias and allow for an indicative view of LLMs, they share some limitations that should be considered. The datasets SS and CrowS define an unbiased model as one that makes an equal amount of stereotypical and anti-stereotypical choices. While we agree that this makes a good definition of an impartial model it is a limited definition of an unbiased model. This has also been noted by Blodgett et al. (2021), showing that CrowS is slightly more robust than SS by taking \"extra steps to control for varying base rates between groups.\" (Blodgett et al., 2021). We should consider that these datasets depict mostly Western biases, and the dataset construction since it is based on assessors it is dependent on the assessor\u2019s views. Moreover, Blodgett et al. (2021) has also noted the existence of unbalanced stereotype pairs in SS and CrowS, and the fact that some samples in the dataset are not consensual stereotypes.\nAll datasets only explore three groups of biases: GENDER, RACE, and RELIGION, which are not by any means exhaustive representations of social bias. The experiments in this paper should be considered indicative of social bias and need to be further studied. Additionally, the GENDER category is defined as binary, which we acknowledge that does not reflect the timely social needs of LLMs, but can be extended to include non-binary examples by improving on existing datasets.\nWe benefited from access to a cluster with two AMD EPYC 7 662 64-Core Processors, where the quantized experiments ran for approximately 4\ndays. A CPU implementation was used given the quantization backends available in PyTorch. Experiments that did not require quantization ran using an NVIDIA A100 40GB GPU and took approximately 5 hours to run.\nEthics Statement\nWe reiterate that this work provides a limited Western view of Social bias focusing only on three main categories: GENDER, RACE, and RELIGION. Our work is further limited to a binary definition of GENDER, which we acknowledge that does not reflect the current society\u2019s needs. Moreover, we must also reiterate that these models need to be further studied and are not ready for production. The effects of quantization along pretraining should be considered as preliminary results."
        },
        {
            "heading": "5 Acknowledgments",
            "text": "This work has been partially funded by the FCT project NOVA LINCS Ref. UIDP/04516/2020, by the Amazon Science - TaskBot Prize Challenge and the CMU|Portugal projects iFetch Ref. LISBOA-01-0247-FEDER-045920 and GoLocal Ref. CMUP-ERI/TIC/0046/2014, and by the FCT Ph.D. scholarship grant Ref. SFRH/BD/140924/2018. We would like to acknowledge the NOVASearch group for providing compute resources for this work. Any opinions, findings, and conclusions in this paper are the authors\u2019 and do not necessarily reflect those of the sponsors."
        },
        {
            "heading": "A Details of Metric Calculation",
            "text": "A.1 SEAT The SEAT task shares the same task as WEAT task, which is defined by four word sets, two attribute sets, and two target sets. For example, to decide the presence of gender bias the two attribute sets are disjoint sets given by: 1) a masculine set of words, such as {\u2019man\u2019, \u2019boy\u2019, \u2019he\u2019, ...}, and 2) a set of feminine words {\u2019woman\u2019, \u2019girl\u2019, \u2019her\u2019, ...}. The target sets will characterize concepts such as \u2019sports\u2019 and \u2019culinary\u2019.\nWEAT evaluates how close are the attribute sets from the target sets to determine the existence of bias. Mathematically this is given by:\ns(A,B,X, Y ) = \u2211 x\u2208X s(x,A,B)\u2212 \u2211 y\u2208Y s(y,A,B)\n(1)\nWhere A and B represent the attribute sets, and X and Y are the target sets of words. The s function in Equation (1) denotes mean cosine similarity between the target word embeddings and the attribute word embeddings:\ns(w,A,B)= 1 |A| \u2211 a\u2208A cos(w, a)\u2212 1 |B| \u2211 b\u2208B cos(w, b).\n(2) The reported score of the benchmark (effect size)\nis given by:\nd = \u00b5({s(x,A,B)}x\u2208X)\u2212 \u00b5({s(y,A,B)}y\u2208Y )\n\u03c3({s(t,X, Y )}t\u2208A\u222aB) (3)\nWhere \u00b5 and \u03c3 are the mean and standard deviation respectively. Equation (3) is designed so that scores closer to zero indicate the smallest possible degree of bias. SEAT extends the previous formulation by considering the distance sentence embeddings instead of word embeddings.\nB Additional Plots and Tables\nTable 5: LM Scores vs. Biases on the SS dataset of the int8 models, at the same steps with the best LM Score for the original (full-precision) models (Table 2)\nTable 6: LM Scores vs. Biases on the SS dataset of the original (full-precision) models, at the same steps with the best LM Score for the int8 models (Table 3)"
        }
    ],
    "title": "Understanding the Effect of Model Compression on Social Bias in Large Language Models",
    "year": 2023
}