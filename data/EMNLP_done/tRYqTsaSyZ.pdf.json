{
    "abstractText": "Abstractive related work generation has attracted increasing attention in generating coherent related work that helps readers grasp the current research. However, most existing models ignore the inherent causality during related work generation, leading to spurious correlations which downgrade the models\u2019 generation quality and generalizability. In this study, we argue that causal intervention can address such limitations and improve the quality and coherence of generated related work. To this end, we propose a novel Causal Intervention Module for Related Work Generation (CaM) to effectively capture causalities in the generation process. Specifically, we first model the relations among the sentence order, document (reference) correlations, and transitional content in related work generation using a causal graph. Then, to implement causal interventions and mitigate the negative impact of spurious correlations, we use do-calculus to derive ordinary conditional probabilities and identify causal effects through CaM. Finally, we subtly fuse CaM with Transformer to obtain an end-to-end related work generation framework. Extensive experiments on two real-world datasets show that CaM can effectively promote the model to learn causal relations and thus produce related work of higher quality and coherence.ive related work generation has attracted increasing attention in generating coherent related work that helps readers grasp the current research. However, most existing models ignore the inherent causality during related work generation, leading to spurious correlations which downgrade the models\u2019 generation quality and generalizability. In this study, we argue that causal intervention can address such limitations and improve the quality and coherence of generated related work. To this end, we propose a novel Causal Intervention Module for Related Work Generation (CaM) to effectively capture causalities in the generation process. Specifically, we first model the relations among the sentence order, document (reference) correlations, and transitional content in related work generation using a causal graph. Then, to implement causal interventions and mitigate the negative impact of spurious correlations, we use do-calculus to derive ordinary conditional probabilities and identify causal effects through CaM. Finally, we subtly fuse CaM with Transformer to obtain an end-to-end related work generation framework. Extensive experiments on two real-world datasets show that CaM can effectively promote the model to learn causal relations and thus produce related work of higher quality and coherence.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiachang Liu"
        },
        {
            "affiliations": [],
            "name": "Qi Zhang"
        },
        {
            "affiliations": [],
            "name": "Chongyang Shi"
        },
        {
            "affiliations": [],
            "name": "Usman Naseem"
        },
        {
            "affiliations": [],
            "name": "Shoujin Wang"
        },
        {
            "affiliations": [],
            "name": "Liang Hu"
        },
        {
            "affiliations": [],
            "name": "Ivor W. Tsang"
        }
    ],
    "id": "SP:00a6bf0f50a15f589d681c0b133a2dd60c791c01",
    "references": [
        {
            "authors": [
                "Nitin Agarwal",
                "Ravi Shankar Reddy",
                "Kiran Gvr",
                "Carolyn Penstein Ros\u00e9."
            ],
            "title": "SciSumm: A multidocument summarization system for scientific articles",
            "venue": "Proceedings of the ACL-HLT 2011 System Demonstrations, pages 115\u2013120, Portland, Oregon.",
            "year": 2011
        },
        {
            "authors": [
                "Uchenna Akujuobi",
                "Xiangliang Zhang."
            ],
            "title": "Delve: A dataset-driven scholarly search and analysis system",
            "venue": "19(2):36\u201346.",
            "year": 2017
        },
        {
            "authors": [
                "Martin Arjovsky",
                "L\u00e9on Bottou",
                "Ishaan Gulrajani",
                "David Lopez-Paz"
            ],
            "title": "Invariant risk minimization",
            "year": 2019
        },
        {
            "authors": [
                "Peter C Austin."
            ],
            "title": "An introduction to propensity score methods for reducing the effects of confounding in observational studies",
            "venue": "Multivariate Behav Res, 46(3):399\u2013424.",
            "year": 2011
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv:2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Xiuying Chen",
                "Hind Alamro",
                "Mingzhe Li",
                "Shen Gao",
                "Xiangliang Zhang",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Capturing relations between scientific papers: An abstractive model for related work section generation",
            "venue": "Proceedings of the 59th Annual Meeting of the",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Fuli Feng",
                "Jizhi Zhang",
                "Xiangnan He",
                "Hanwang Zhang",
                "Tat-Seng Chua."
            ],
            "title": "Empowering language understanding with counterfactual reasoning",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2226\u20132236, Online.",
            "year": 2021
        },
        {
            "authors": [
                "Yubin Ge",
                "Ly Dinh",
                "Xiaofeng Liu",
                "Jinsong Su",
                "Ziyao Lu",
                "Ante Wang",
                "Jana Diesner."
            ],
            "title": "BACO: A background knowledge- and content-based framework for citing sentence generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Cong Duy Vu Hoang",
                "Min-Yen Kan."
            ],
            "title": "Towards automated related work summarization",
            "venue": "Coling 2010: Posters, pages 427\u2013435, Beijing, China. Coling 2010 Organizing Committee.",
            "year": 2010
        },
        {
            "authors": [
                "Yue Hu",
                "Xiaojun Wan."
            ],
            "title": "Automatic generation of related work sections in scientific papers: An optimization approach",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar. Association for",
            "year": 2014
        },
        {
            "authors": [
                "Xinyu Jiang",
                "Qi Zhang",
                "Chongyang Shi",
                "Kaiying Jiang",
                "Liang Hu",
                "Shoujin Wang."
            ],
            "title": "An ion exchange mechanism inspired story ending generator for different characters",
            "venue": "Machine Learning and Knowledge Discovery in Databases - European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Hanqi Jin",
                "Tianming Wang",
                "Xiaojun Wan."
            ],
            "title": "Multi-granularity interaction network for extractive and abstractive multi-document summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6244\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Wei Li",
                "Xinyan Xiao",
                "Jiachen Liu",
                "Hua Wu",
                "Haifeng Wang",
                "Junping Du."
            ],
            "title": "Leveraging graph to improve abstractive multi-document summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Xiangci Li",
                "Jessica Ouyang"
            ],
            "title": "Automatic related work generation: A meta study",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Yang Liu",
                "Mirella Lapata."
            ],
            "title": "Text summarization with pretrained encoders",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Kyle Lo",
                "Lucy Lu Wang",
                "Mark Neumann",
                "Rodney Kinney",
                "Daniel Weld."
            ],
            "title": "S2ORC: The semantic scholar open research corpus",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969\u20134983, Online. Asso-",
            "year": 2020
        },
        {
            "authors": [
                "Rada Mihalcea",
                "Paul Tarau."
            ],
            "title": "TextRank: Bringing order into text",
            "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404\u2013411, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Bowen Zhou",
                "Cicero Nogueira dos santos",
                "Caglar Gulcehre",
                "Bing Xiang"
            ],
            "title": "Abstractive text summarization using sequence-tosequence rnns and beyond",
            "year": 2016
        },
        {
            "authors": [
                "Judea Pearl."
            ],
            "title": "Causal inference in statistics: An overview",
            "venue": "Statistics Surveys, 3(none):96 \u2013 146.",
            "year": 2009
        },
        {
            "authors": [
                "Judea Pearl."
            ],
            "title": "Causality: Models, Reasoning and Inference",
            "venue": "Cambridge University Press.",
            "year": 2009
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "year": 2020
        },
        {
            "authors": [
                "Sascha Rothe",
                "Shashi Narayan",
                "Aliaksei Severyn."
            ],
            "title": "Leveraging pre-trained checkpoints for sequence generation tasks",
            "venue": "Transactions of the Association for Computational Linguistics, 8:264\u2013280.",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Pancheng Wang",
                "Shasha Li",
                "Haifang Zhou",
                "Jintao Tang",
                "Ting Wang."
            ],
            "title": "Toc-rwg: Explore the combination of topic model and citation information for automatic related work generation",
            "venue": "IEEE Access, 8:13043\u201313055.",
            "year": 2020
        },
        {
            "authors": [
                "Yichao Wang",
                "Huifeng Guo",
                "Bo Chen",
                "Weiwen Liu",
                "Zhirong Liu",
                "Qi Zhang",
                "Zhicheng He",
                "Hongkun Zheng",
                "Weiwei Yao",
                "Muyu Zhang",
                "Zhenhua Dong",
                "Ruiming Tang."
            ],
            "title": "Causalint: Causal inspired intervention for multi-scenario recommendation",
            "venue": "Pro-",
            "year": 2022
        },
        {
            "authors": [
                "Zhao Wang",
                "Aron Culotta."
            ],
            "title": "Identifying spurious correlations for robust text classification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3431\u20133440, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Zhenlei Wang",
                "Shiqi Shen",
                "Zhipeng Wang",
                "Bo Chen",
                "Xu Chen",
                "Ji-Rong Wen."
            ],
            "title": "Unbiased sequential recommendation with latent confounders",
            "venue": "WWW \u201922, page 2195\u20132204, New York, NY, USA. Association for Computing Machinery.",
            "year": 2022
        },
        {
            "authors": [
                "Xinyu Xing",
                "Xiaosheng Fan",
                "Xiaojun Wan."
            ],
            "title": "Automatic generation of citation texts in scholarly papers: A pilot study",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6181\u20136190, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Xinyu Xing",
                "Xiaosheng Fan",
                "Xiaojun Wan."
            ],
            "title": "Automatic generation of citation texts in scholarly papers: A pilot study",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6181\u20136190, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Yunqi Zhu",
                "Xuebing Yang",
                "Yuanyuan Wu",
                "Mingjin Zhu",
                "Wensheng Zhang."
            ],
            "title": "Differentiable n-gram objective on abstractive summarization",
            "venue": "Expert Systems with Applications, 215:119367.",
            "year": 2023
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "2022), in which they take the paper that related work belongs to as the target and employ a target-centered attention mechanism",
            "year": 2022
        },
        {
            "authors": [
                "ples. Wang",
                "Culotta"
            ],
            "title": "2020) propose a method for identifying spurious correlations in the text classification task. The method extracts the words with the highest relevance to the category and uses an estimator to determine whether the correlation",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "A comprehensive related work usually covers abundant reference papers, which costs authors plenty of time in reading and summarization and even forces authors to pursue ever-updating advanced work (Hu and Wan, 2014). Fortunately, the task of related work generation emerged and attracted increasing attention from the community of text summarization and content analysis in recent years (Chen et al., 2021, 2022). Related work generation can be considered as a variant of the multi-document\n\u2217 The authors contribute equally to this work. \u2020 Corresponding author.\nsummarization task (Li and Ouyang, 2022). Distinct from multi-document summarization, related work generation entails comparison after the summarization of a set of references and needs to sort out the similarities and differences between these references (Agarwal et al., 2011).\nRecently, various abstractive text generation methods have been proposed to generate related work based on the abstracts of references. For example, Xing et al. (2020a) used the context of citation and the abstract of each cited paper as the input to generate related work. Ge et al. (2021) encoded the citation network and used it as external knowledge to generate related work. Chen et al. (2022) proposed a target-aware related work generator that captures the relations between reference papers and the target paper through a target-centered attention mechanism. Equipped with well-designed encoding strategies, external knowledge, or novel training techniques, these studies have made promising progress in generating coherent related work.\nHowever, those models are inclined to explore and exploit spurious correlations, such as highfrequency word/phrase patterns, writing habits, or presentation skills, to build superficial shortcuts between reference papers and the related work of the target paper. Such spurious correlations may harm the quality of the generated related work, especially when distribution shift exists between the testing set and training set. This is because spurious correlations are different from genuine causal relations. They often do not intrinsically contribute to the related work generation and easily cause the robustness problem and impair the models\u2019 generalizability (Arjovsky et al., 2019).\nFigure 1 illustrates the difference between causality and spurious correlation. The phrases \"for example\" and \"later\" are often used to bridge two sentences in related work. Their usage may be attributed to writers\u2019 presentation habits about organizing sentence orders or the reference document\nrelations corresponding to the sentences. Ideally, a related work generation model is expected to learn the reference relation and distinguish it from the writing habits. However, previous generation models easily capture the superficial habitual sentence organization (a spurious correlation) instead of learning complex causal reference relations, especially when the habitual patterns frequently occur in the training set. In this case, the transitional phrases generated mainly based on writing habits are likely to be unsuitable and subsequently affect the content generation of related work during testing when the training and testing sets are not distributed uniformly.\nFortunately, causal intervention can effectively remove spurious correlations and focus on causal relations by intervening in the learning process. It not only observes the impact of the sentence order and document relation on generating transitional content, but also probes the impact of each possible order on the whole generation of related work, thereby removing the spurious correlations (Pearl, 2009a). Accordingly, causal intervention serving as an effective solution allows causal relations to exert a greater impact and instruct the model to produce the correct content.\nAccordingly, to address the aforementioned gaps in existing work for related work generation, we propose a novel Causal Intervention Module for Related Work Generation (CaM), which effectively removes spurious correlations by performing the causal intervention. Specifically, we first model the relations among sentence order, document relation, and transitional content in related work generation and identify the confounder that raises spurious correlations (see Figure 2). Then, we implement causal intervention that consists of three compo-\nnents: 1) Primitive Intervention cuts off the connection that induces spurious correlations in the causal graph by leveraging do-calculus and backdoor criterion (Pearl, 2009a), 2) Context-aware Remapping smoothens the distribution of intervened embeddings and injects contextual information, and 3) Optimal Intensity Learning learns the best intensity of overall intervention by controlling the output from different parts. Finally, we strategically fuse CaM with Transformer (Vaswani et al., 2017) to deliver an end-to-end causal related work generation model. Our main contributions are as follows:\n\u2022 To the best of our knowledge, this work is the first attempt to introduce causality theory into the related work generation task. \u2022 We propose a novel Causal Intervention Module for Related Work Generation (CaM) which utilizes causal intervention to mitigate the impact of spurious correlations. CaM is subtly fused with Transformer to derive an end-to-end causal related work generation model, enabling the propagation of intervened information. \u2022 Extensive experiments on two real-world benchmark datasets demonstrate that our proposed model can generate related works of high quality and verify the effectiveness and rationality of bringing causality theory into the related work generation task."
        },
        {
            "heading": "2 Problem Formulation",
            "text": "Given a set of reference papers D = {r1, ..., r|D|}, we assume the ground truth related work Y = (w1, w2, ..., wM ), where ri = (wi1, w i 2, ..., w i |ri|) denotes a single cited paper, wij is the j-th word in ri, and wj is the j-th word in related work Y . Generally, the related work generation task can be formulated as generating a related work section Y\u0302 = (w\u03021, w\u03022, ..., w\u0302M\u0302 ) based on the reference input D and minimizing the difference between Y and Y\u0302 . Considering that the abstract section is usually well-drafted to provide a concise paper summarization (Hu and Wan, 2014), we use the abstract section to represent each reference paper."
        },
        {
            "heading": "3 Methodology",
            "text": "We first analyze the causalities in related work generation, identify the confounder that raises spurious correlations, and use a causal graph to model these relations. Then, we implement CaM to enhance the quality of related work through causal intervention. Finally, we describe how CaM, as an intervention\nmodule, is integrated with Transformer to intervene in the entire generation process. The overall structure of our model is shown in Figure 3."
        },
        {
            "heading": "3.1 Causal Modeling for Related Work Generation",
            "text": "We believe that three aspects play significant roles in related work generation for better depicting the relations between different references, namely, sentence order c, document relation x, and transitional content y (illustrated in Figure 2). In many cases, sentence order is independent of the specified content and directly establishes relations with transitional content. For example, we tend to use \"firstly\" at the beginning and \"finally\" at the end while composing a paragraph, regardless of what exactly is in between. This relation corresponds to path c \u2192 y, and it should be preserved as a writing experience or habit. Meanwhile, there is a lot of transitional content that portrays the relations between referred papers based on the actual content, at this time, models need to analyze and use these relations. The corresponding path is x \u2192 y.\nThough ideally, sentence order and document relation can instruct the generation of transitional content based on practical writing needs, deep learning models are usually unable to trade off the influence of these two aspects correctly but prioritize sentence order. This can be attributed to the fact that sentence order information is easily accessible and learnable. In Figure 2, such relation corresponds to c \u2192 x \u2192 y. In this case, sentence order c is the confounder that raises a spurious correlation with transitional content y. Although performing well on the training set, once a data distribution shift exists between the test set and training set where the test set focuses more on document relations, the transitional content instructed by sentence order can be quite unreliable. To mitigate the impact of the spurious correlation, we need to cut off the path c \u2192 x, enabling the model to generate transitional content based on the correct and reliable causality\nof both c \u2192 y and x \u2192 y."
        },
        {
            "heading": "3.2 Causal Intervention Module for Related Work Generation",
            "text": "The proposed CaM contains three parts as shown in Figure 3: Primitive Intervention performs causal intervention and preliminarily removes the spurious correlations between sentence order and transitional content.Context-aware Remapping captures and fuses contextual information, facilitating the smoothing of the intervened embeddings. Optimal Intensity Learning learns the best intensity of holistic causal intervention."
        },
        {
            "heading": "3.2.1 Primitive Intervention",
            "text": "Based on the causal graph G shown in Figure 2, we first perform the following derivation using docalculus and backdoor criterion.\np(y|do(x)) = \u2211\nc p(y|do(x), c)p(c|do(x)) = \u2211 c p(y|x, c)p(c|do(x))\n= \u2211 c p(y|x, c)p(c) (1)\nIn short, the do-calculus is a mathematical representation of an intervention, and the backdoor criterion can help identify the causal effect of x on y (Pearl, 2009b). As a result, by taking into consideration the effect of each possible value of sentence order c on transitional content y, c stops affecting document relation x when using x to estimate y, which means path c \u2192 x is cut off (see the arrow-pointed graph in Figure 2). Next, we will explain how to estimate separately p(y|x, c) and p(c) using deep learning models and finally obtain p(y|do(x)).\nLet Eori \u2208 RM\u0302\u00d7d denote the input embeddings corresponding to M\u0302 -sized related work and Eitv \u2208 RM\u0302\u00d7d denote the output embeddings of Primitive Intervention. We first integrate the sentence order information into the input embeddings:\ne odr(j) i = Linear(e ori i \u2295 oj), eorii \u2208 Eori (2)\nwhere j = 1, ..., s, s is the total number of sentences in the generated related work and eodr(j)i denotes the order-enhanced embedding for the i-th word. We take oj = (lg (j + 1), \u00b7 \u00b7 \u00b7 , lg (j + 1)) with the same dimension as eori. The linear layer (i.e., Linear) further projects the concatenated embedding to eodr with the same dimension as eori. Accordingly, we have the estimation of p(y|x, c) := eodr. Then, we feed the subsequence\nEitv1:i\u22121 to a feed-forward network to predict the sentence position probability of the current decoding word:\nhi = Softmax(FFN(ReLU( \u2211i\u22121Eitv1:i\u22121)))\n(3)\nwhere each hji \u2208 hi denotes the probability. Thus, we estimate the sentence position probability of each decoding word p(c) := h. After obtaining the estimation of p(y|x, c) and p(c), the final embedding with primitive causal intervention is achieved:\neitvi = \u2211s j=1 e odr(j) i \u00d7 h j i , h j i \u2208 hi (4)\nwhere eodr(j)i \u00d7h j i multiplying sentence order probability with order-enhanced embeddings is exactly p(y|x, c)p(c) in Equation 1. Since most transitions are rendered by start words, our CaM intervenes only with these words, namely part of eitv \u2208 Eitv being equal to eori \u2208 Eori. For simplicity, we still use Eitv in the following."
        },
        {
            "heading": "3.2.2 Context-aware Remapping",
            "text": "Two problems may exist in Primitive Intervention: 1) The lack of learnable parameters may lead to the intervened embeddings and the original ones being apart and obstructs the subsequent decoding process. 2) Intervention in individual words may damage the context along with the order-enhanced embedding. To solve the two problems, we propose a Context-aware Remapping mechanism. First, we scan Eitv with a context window of fixed size nw:\nBi = WIN(E itv)\n= Eitvi:i+nw\u22121 (5)\nwhere WIN(\u00b7) returns a consecutive subsequence of Eitv at length nw. Then, we follow the process of Multi-head Attention Mechanism (Vaswani et al., 2017) to update the embeddings in Bi:\nBrmpi = MultiHead(Bi, Bi, Bi)\n= (ermpi , ..., e rmp i+nw\u22121)\n(6)\nEven though all embeddings in Bi are updated, we only keep the renewed ermpi+(nw/2) \u2208 B rmp i as the output, and leave the rest unchanged. Since WIN(\u00b7) scans the entire sequence step by step, every embedding will have the chance to update. The output is denoted as Ermp \u2208 RM\u0302\u00d7d."
        },
        {
            "heading": "3.2.3 Optimal Intensity Learning",
            "text": "There is no guarantee that causal intervention with maximum (unaltered) intensity will improve model performance, especially when combined with pretrained models (Brown et al., 2020; Lewis et al., 2020), as the intervention may conflict with the pre-training strategies. To guarantee performance, we propose the Optimal Intensity Learning.\nBy applying Primitive Intervention and Contextaware Remapping, we have three types of embeddings, Eori,Eitv, and Ermp. To figure out their respective importance to the final output, we derive the output intensity corresponding to each of them:\ngori = \u03c3(W ori \u00b7 eori) (7) gitv = \u03c3(W itv \u00b7 eori) (8)\ngrmp = \u03c3(W rmp \u00b7 eori) (9) cori, citv, crmp = fs([g ori, gitv, grmp]) (10)\nwhere \u03c3(\u00b7) is the sigmoid function, fs(\u00b7) is the softmax function. Combining cori, citv, crmp, we can obtain the optimal intervention intensity and the final word embedding set Eopm = (eopm1 , ..., e opm M\u0302 ) with causal intervention:\neopm = corieori + citveitv + crmpermp (11)"
        },
        {
            "heading": "3.3 Fusing CaM with Transformer",
            "text": "To derive an end-to-end causal generation model and ensure that the intervened information can be propagated, we choose to integrate CaM with Transformer (Vaswani et al., 2017). However, unlike the RNN-based models that generate words recurrently (Nallapati et al., 2016), the attention mechanism computes the embeddings of all words in parallel, while the intervention is performed on the sentence start words.\nTo tackle this challenge, we perform vocabulary mapping on word embeddings before intervention and compare the result with sentence start token [CLS] to obtain Mask:\nI = argmax[Linearvocab(E ori)] (12)\nMask = \u03b4(I, IDCLS) (13)\nI contains the vocabulary index of each word. \u03b4(\u00b7) compares the values of the two parameters, and returns 1 if the same, 0 otherwise. Mask indicates whether the word is a sentence start word. Therefore, Eopm can be calculated as:\nEopm = Eopm\u2299Mask+Eori\u2299(\u223c Mask) (14)\nThe \u2299 operation multiplies each embedding with the corresponding {0, 1} values, and \u223c denotes the inverse operation. Note that we omit Mask for conciseness in Section 3.2.3. Mask helps restore the non-sentence-start word embeddings and preserve the intervened sentence-start ones.\nAs illustrated in Figure 3, we put CaM between the Transformer layers in the decoder. The analysis of the amount and location settings will be discussed in detail in Section 4.6. The model is\ntrained to minimize the cross-entropy loss between the predicted Y\u0302 and the ground-truth Y , v is the vocabulary index for wi \u2208 Y :\nL = \u2212\u2211M\u0302 i log p v i (Y\u0302 ) (15)"
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "Following the settings in Chen et al. (2021, 2022), we adopt two publicly available datasets derived from the scholar corpora S2ORC (Lo et al., 2020) and Delve (Akujuobi and Zhang, 2017) respectively to evaluate our proposed method in related work generation. S2ORC consists of scientific papers from multiple domains, and Delve focuses on the computer domain. The datasets are summarized in Table 1, where the corresponding ratios of the training/validation/test pairs are detailed 1."
        },
        {
            "heading": "4.2 Settings",
            "text": "In our experiments, we incorporate CaM into the Transformer decoder (see Figure 3) and evaluate our model using the resultant encoder-decoder architecture. We utilize pre-trained weights from BERT (Devlin et al., 2019) for both the encoder and decoder of the architecture, as described in Rothe et al. (2020) 2. Also, when CaM is removed from the decoder in the following experiments, the remaining Transformer model we evaluate still employs pre-trained weights.\nIn the Transformer architecture we use, the dimension of word embedding is 768, both the number of attention heads and hidden layers in the encoder and decoder are 12, and the intermediate size is 3072. We implement our model with PyTorch on NVIDIA 3080Ti GPU. The maximum reference paper number is set to 5, i.e., |D| = 5. We select the first 440/|D| words in each reference paper abstract and concatenate them to obtain the model input sequence. The total number of sentences in target related work is set to 6, i.e., s = 6. We use beam search for decoding, with a beam size of 4 and a maximum decoding step of 200. We use SGD as the optimizer with a learning rate 1e \u2212 3. We use ROUGE-1, ROUGE-2, and ROUGE-L on F1 as the metrics (Lin, 2004; Jiang et al., 2022).\n1https://github.com/iriscxy/relatedworkgeneration 2https://huggingface.co/docs/transformers/main/en/model_\ndoc/encoder-decoder#transformers.EncoderDecoderModel"
        },
        {
            "heading": "4.2.1 Extractive Methods",
            "text": "(1) TextRank (Mihalcea and Tarau, 2004): A graph-based text ranking model that can be used in multi-document sentence extraction. (2) BertSumEXT (Liu and Lapata, 2019): An extractive summarization model that extends BERT by inserting multiple [CLS] tokens. (3) MGSum-ext (Jin et al., 2020): A multi-granularity network that jointly learns different semantic representations. Abstractive Methods: (1) TransformerABS (Vaswani et al., 2017): An abstractive summarization model based on Transformer. (2) BertSumABS (Liu and Lapata, 2019): An abstractive model based on BERT with a designed two-stage fine-tuning approach. (3) MGSum-abs (Jin et al., 2020): A multi-granularity interaction network that can be utilized for abstractive document summarization.(4) GS (Li et al., 2020): An abstractive summarization model that utilizes special graphs to encode documents to capture cross-document relations. (5) T5-base (Raffel et al., 2020): A text-totext generative language model that leverages transfer learning techniques. (6) BART-base (Lewis et al., 2020): A powerful sequence-to-sequence model that combines the benefits of autoregressive and denoising pretraining objectives. (7) Longformer (Beltagy et al., 2020): A transformer-based model that can efficiently process long-range dependencies in text. (8) RGG (Chen et al., 2021): An encoder-decoder model specifically tailored for related work generation, which constructs and refines the relation graph of reference papers. (9) NG-Abs (Zhu et al., 2023): A BART model that optimized jointly with the cross-entropy loss and the proposed differentiable N-gram objectives. (10)\nTAG (Chen et al., 2022): It takes the paper that related work belongs to as the target and employs a target-centered attention mechanism to generate related work."
        },
        {
            "heading": "4.3 Overall Performance",
            "text": "It can be found in Table 2 that abstractive models have attracted more attention in recent years and usually outperform extractive ones. Among the generative models, pretrained model T5 and BART achieve promising results in our task without additional design. Meanwhile, Longformer, which is good at handling long text input, also achieves\nfavorable results. However, the performance of these models is limited by the complexity of the academic content in the dataset.\nOur proposed CaM achieves the best performance on both datasets. Due to fusing CaM with Transformer, its large scale ensures that our model can still effectively capture document relations without additional modeling. Accordingly, CaM enables the model to obviate the impact of spurious correlations through causal intervention and promotes the model to learn more robust causalities to achieve the best performance."
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "To analyze the contribution of the different components of CaM, we separately control the use of Primitive Intervention (PI), Context-aware Remapping (RMP) and Optimal Intensity Learning (OPT). Figure 4 and Figure 5 show the performance comparison between different variants of CaM.\nFirst, we observe that Transformer already guarantees a desirable base performance. When only PI is used, the model generally shows a slight performance drop. PI+RMP outperforms RMP, showing the necessity of the PI and the effectiveness of RMP. PI+RMP+OPT achieves optimal results, indicating that OPT can effectively exploit the information across different representations."
        },
        {
            "heading": "4.5 Human Evaluation",
            "text": "We evaluate the quality of related work generated by the CaM, RRG, and Transformer from three perspectives (informativeness, coherence, and succinctness) by randomly selecting forty samples from S2ORC and rating the generated results by 15 PhD students. In the QA task, three PhD students posed three questions for each sample, ensuring that the answers existed in ground truth. Participants need to answer these questions after reading the generated text and we use accuracy as the metric. As table 3 shows, our method achieves the best in informativeness, coherence, and the QA task. However, succinctness is slightly lower than RRG, probably due to the output length limit."
        },
        {
            "heading": "4.6 Fusing Strategy Comparison",
            "text": "In our setting, the Transformer decoder consists of 12 layers, so there are multiple locations to fuse a different number of CaMs. For each scenario, CaMs are placed evenly among the Transformer decoder layers, and one will always be placed at the end of the entire model. The results of all cases are shown in Figure 6. It can be observed that the model performs best when the number of CaM is 4 both on S2ORC and Delve. With a small number of CaMs, the model may underperform the benchmark model and fail to achieve optimal performance due to the lack of sufficient continuous intervention. If there are too many CaMs, the distance between different CaMs will be too short, leaving an insufficient learning process for the entire fused model, and this might cause the CaMs to bring the noise."
        },
        {
            "heading": "4.7 Robustness Analysis",
            "text": ""
        },
        {
            "heading": "4.7.1 Testing with Reordered Samples",
            "text": "We randomly select 50 samples (15 from S2ORC and 35 from Delve) and manually rearrange the order of the cited papers and the order of their corresponding sentences in each sample.Transitional content in related work is also removed since the reordering damages the original logical relations.\nFigure 7 shows that CaM has better performance no matter whether the samples have been reordered or not. Regarding the reordered samples, the per-\nformance of Transformer decreases on all three metrics, but CaM only decreases on ROUGE-1 and ROUGE-2 at a much lower rate. Particularly, compared to Transformer, CaM makes improvement on ROUGE-L when tested with reordered samples. The result indicates that CaM is able to tackle the noise disturbance caused by reordering, and the generated content maintains better coherence."
        },
        {
            "heading": "4.7.2 Testing with Migrated Test Set",
            "text": "We train the models on Delve and test them on S2ORC, which is a challenging task and significant for robustness analysis. As expected, the performances of all models drop, but we can still obtain credible conclusions. Since CaM outperforms Transformer initially, simply comparing the ROUGE scores after migrating the test set is not informative. To this end, we use Relative Outperformance Rate (ROR) for evaluation:\nROR = (SCaM \u2212 STF)/STF (16)\nSCaM and STF are the ROUGE scores of CaM and Transformer, respectively. ROR computes the advantage of CaM over Transformer.\nFigure 8 reports that CaM outperforms Transformer regardless of migrating from Delve to S2ORC for testing. In addition, comparing the change of ROR, we observe that although migration brings performance drop, CaM not only maintains its advantage over Transformer but also enlarges it. The above two experiments demonstrate that the CaM effectively learns causalities to improve model robustness."
        },
        {
            "heading": "4.8 Causality Visualization",
            "text": "To visualize how causal intervention works in the generation process, we compare the related work generated by Transformer and CaM with a case study. Specifically, we map their cross attention\ncorresponding to \"however\" and \"the\" to the input content using different color shades (Figure 10) to explore what information these two words rely on. More details of the above two experiments can be found in Appendix B.\nWe picked out the words that \"however\" and \"the\" focused on the most and analyzed the implications of these words in the context of the input. The results are shown in Figure 9. It can be found that the words highlighted by CaM have their respective effects in the cited papers. When generating \"however\", the model aggregates this information, comparing the relations between the documents and producing the correct result. However, there is no obvious connection between the words focused on by Transformer, hence there is no clear decision process after combining the information, and the generated word \"the\" is simply a result obtained from learned experience and preference. Through causality visualization, it can be observed very concretely how CaM improves model performance by conducting causal intervention."
        },
        {
            "heading": "5 Conclusions",
            "text": "In this paper, we propose a Causal Intervention Module for Related Work Generation (CaM) to capture causalities in related work generation. We first model the relations in related work generation using a causal graph. The proposed CaM implements causal intervention and enables the model to capture causality. We subtly fuse CaM with Transformer to obtain an end-to-end model to integrate the intervened information throughout the generation process. Extensive experiments show the superiority of CaM and demonstrate our method\u2019s effectiveness.\nLimitations\nAlthough extensive experiments have demonstrated that CaM can effectively improve the performance of the generation model, as mentioned above, since the intervention occurs on the sentence start words, it is inconclusive that CaM can bring improvement if the generation of sentence start words is inaccurate. That is, if it is to be combined with small-scale models without any pre-trained knowledge, then the effectiveness of the model might not be ensured. This will also be a direction of improvement for our future work."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by the Fundamental Research Funds for the Central Universities."
        },
        {
            "heading": "A Related Work",
            "text": "A.1 Related Work Generation\nThe related work generation task can be viewed as a variant of the multi-document summarization task, and its methods can be categorized as extractive or abstractive. Most of the early studies use extractive methods. The work of Hoang and Kan (2010) is one of the first attempts. They propose a heuristic approach to generate general and specific content separately given a topic tree. Wang et al. (2020) train the model to extract cited text spans through a specific training set and use a greedy algorithm to select the most suitable candidate sentences to compose related work. Most recent studies focus on abstractive approaches. Xing et al. (2020b) use the citation context and the abstract of the cited papers together as inputs to generate citation text. Chen et al. (2021) construct a relation graph of the cited papers during the encoding process and update them iteratively. The relation graph is used as an auxiliary information for decoding. The most recent work is done by Chen et al. (2022), in which they take the paper that related work belongs to as the target and employ a target-centered attention mechanism to generate informative related work.\nA.2 Causal Intervention\nIn recent years, causality theory has attracted increasing attention in various domains. In the field of recommendation system, Wang et al. (2022a) use the causal graph to model multi-scenario recommendation and solve the problem of existing systems that may introduce unnecessary information from other scenarios.Wang et al. (2022b) propose a framework for sequential recommendation that can perceive data biases by reweighing training data and using inverse propensity scores(Austin, 2011). In the field of natural language processing, Feng et al. (2021) introduce counterfactual reasoning into the sentiment analysis task and leverage the knowledge of both factual and counterfactual samples. Wang and Culotta (2020) propose a method for identifying spurious correlations in the text classification task. The method extracts the words with the highest relevance to the category and uses an estimator to determine whether the correlation is a spurious correlation."
        },
        {
            "heading": "B Experiment Result for Causal Visualization",
            "text": "In this section, we will give an extra analysis of the experiments introduced in Section 4.8.\nB.1 Generated Related Work Comparison From Table 4, we can notice that CaM generates enriched content and its meaning is closer to ground truth compared to Transformer. Crucially, when pointing out the problems of previous approaches and presenting the new ones(sentence marked in green), CaM correctly generates \"however\" at the beginning of the sentence and the entire sentence has a more accurate expression, making the transitions more seamless. But Transformer only generates a very high-frequency word \"the\" at the same position. It can be perceived that in this process Transformer is not making effective decisions, but simply generating with preference and experience.\nB.2 Visualization Result Analysis on Full Text Figure 10 visualizes the cross attention of words \"however\" and \"the\" in CaM and Transformer. Different cited papers are split with vertical lines. The deeper blue color denotes the higher attention received by the input source word. Judging from the overall coloring situation, we can find that in CaM, there is more deep blue text, as well as more light-colored text. This means the information that \"however\" focuses on is more targeted and more important, and CaM is capable to produce correct content by accurately capturing document relations and avoid distractions from the confounder. In the result of Transformer, both light and deep blue text become less visible, and the coverage of normal blue increases greatly, indicating that \"the\" focuses on a wider range of information but lacks emphasis. It indicates that the decision process in Transformer is unclear and ineffective.\nDetailed analysis of the exact words they focus on and the decision process of the models is presented in Section 4.8."
        }
    ],
    "title": "Causal Intervention for Abstractive Related Work Generation",
    "year": 2023
}