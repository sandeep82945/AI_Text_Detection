{
    "abstractText": "Recently slot filling has witnessed great development thanks to deep learning and the availability of large-scale annotated data. However, it poses a critical challenge to handle a novel domain whose samples are never seen during training. The recognition performance might be greatly degraded due to severe domain shifts. Most prior works deal with this problem in a two-pass pipeline manner based on metric learning. In practice, these dominant pipeline models may be limited in computational efficiency and generalization capacity because of non-parallel inference and contextfree discrete label embeddings. To this end, we re-examine the typical metric-based methods, and propose a new adaptive end-to-end metric learning scheme for the challenging zero-shot slot filling. Considering simplicity, efficiency and generalizability, we present a cascade-style joint learning framework coupled with contextaware soft label representations and slot-level contrastive representation learning to mitigate the data and label shift problems effectively. Extensive experiments on public benchmarks demonstrate the superiority of the proposed approach over a series of competitive baselines.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuanjun Shi"
        },
        {
            "affiliations": [],
            "name": "Linzhi Wu"
        },
        {
            "affiliations": [],
            "name": "Minglai Shao"
        }
    ],
    "id": "SP:0989d1c5b8e16eb106c1bade0f1f2a72ced0ea16",
    "references": [
        {
            "authors": [
                "Ankur Bapna",
                "G\u00f6khan T\u00fcr",
                "Dilek Z. Hakkani-T\u00fcr",
                "Larry Heck."
            ],
            "title": "Towards zero-shot frame semantic parsing for domain scaling",
            "venue": "INTERSPEECH.",
            "year": 2017
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey E. Hinton."
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "Proceedings of ICML 2020, 13-18 July 2020, Virtual Event, volume 119, pages 1597\u20131607.",
            "year": 2020
        },
        {
            "authors": [
                "Alice Coucke",
                "Alaa Saade",
                "Adrien Ball",
                "Th\u00e9odore Bluche",
                "Alexandre Caulier",
                "David Leroy",
                "Cl\u00e9ment Doumouro",
                "Thibault Gisselbrecht",
                "Francesco Caltagirone",
                "Thibaut Lavril",
                "Ma\u00ebl Primet",
                "Joseph Dureau"
            ],
            "title": "Snips voice platform: an embedded",
            "year": 2018
        },
        {
            "authors": [
                "Sarkar Snigdha Sarathi Das",
                "Arzoo Katiyar",
                "Rebecca J. Passonneau",
                "Rui Zhang."
            ],
            "title": "Container: Fewshot named entity recognition via contrastive learning",
            "venue": "Proceedings of ACL, pages 6338\u20136353.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of NAACL-HLT, pages 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Xinya Du",
                "Luheng He",
                "Qi Li",
                "Dian Yu",
                "Panupong Pasupat",
                "Yuan Zhang."
            ],
            "title": "Qa-driven zero-shot slot filling with weak supervision pretraining",
            "venue": "Proceedings of ACL, Virtual Event, August 1-6, 2021, pages 654\u2013664. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Rashmi Gangadharaiah",
                "Balakrishnan Narayanaswamy."
            ],
            "title": "Joint multiple intent detection and slot labeling for goal-oriented dialog",
            "venue": "Proceedings of NAACL-HLT, pages 564\u2013569.",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "Simcse: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of EMNLP, pages 6894\u2013 6910. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Chih-Wen Goo",
                "Guang-Lai Gao",
                "Yun-Kai Hsu",
                "Chih-Li Huo",
                "Tsung-Chieh Chen",
                "Keng-Wei Hsu",
                "YunNung Chen."
            ],
            "title": "Slot-gated modeling for joint slot filling and intent prediction",
            "venue": "NAACL-HLT.",
            "year": 2018
        },
        {
            "authors": [
                "Keqing He",
                "Jinchao Zhang",
                "Yuanmeng Yan",
                "Weiran Xu",
                "Cheng Niu",
                "Jie Zhou."
            ],
            "title": "Contrastive zero-shot learning for cross-domain slot filling with adversarial attack",
            "venue": "COLING.",
            "year": 2020
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel."
            ],
            "title": "Gaussian error linear units (gelus)",
            "venue": "arXiv preprint arXiv:1606.08415.",
            "year": 2016
        },
        {
            "authors": [
                "Seong-Hwan Heo",
                "WonKee Lee",
                "Jong-Hyeok Lee."
            ],
            "title": "mcbert: Momentum contrastive learning with BERT for zero-shot slot filling",
            "venue": "Interspeech 2022, pages 1243\u20131247.",
            "year": 2022
        },
        {
            "authors": [
                "Chen Jia",
                "Liang Xiao",
                "Yue Zhang."
            ],
            "title": "Crossdomain NER using cross-domain language modeling",
            "venue": "Proceedings of ACL, pages 2464\u20132474.",
            "year": 2019
        },
        {
            "authors": [
                "Chen Jia",
                "Yue Zhang."
            ],
            "title": "Multi-cell compositional LSTM for NER domain adaptation",
            "venue": "Proceedings of ACL, pages 5906\u20135917.",
            "year": 2020
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "Chen Wang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan."
            ],
            "title": "Supervised contrastive learning",
            "venue": "Advances in NIPS.",
            "year": 2020
        },
        {
            "authors": [
                "Sungjin Lee",
                "Rahul Jha."
            ],
            "title": "Zero-shot adaptive transfer for conversational language understanding",
            "venue": "In AAAI, pages 6642\u20136649. AAAI Press.",
            "year": 2019
        },
        {
            "authors": [
                "Xiaoya Li",
                "Jingrong Feng",
                "Yuxian Meng",
                "Qinghong Han",
                "Fei Wu",
                "Jiwei Li."
            ],
            "title": "A unified MRC framework for named entity recognition",
            "venue": "Proceedings of ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Xuefeng Li",
                "Liwen Wang",
                "Guanting Dong",
                "Keqing He",
                "Jinzheng Zhao",
                "Hao Lei",
                "Jiachi Liu",
                "Weiran Xu."
            ],
            "title": "Generative zero-shot prompt learning for crossdomain slot filling with inverse prompting",
            "venue": "Findings of the ACL 2023, Toronto, Canada, July 9-14,",
            "year": 2023
        },
        {
            "authors": [
                "Jian Liu",
                "Mengshi Yu",
                "Yufeng Chen",
                "Jinan Xu."
            ],
            "title": "Cross-domain slot filling as machine reading comprehension: A new perspective",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:673\u2013685.",
            "year": 2022
        },
        {
            "authors": [
                "Shichen Liu",
                "Mingsheng Long",
                "Jianmin Wang",
                "Michael I. Jordan."
            ],
            "title": "Generalized zero-shot learning with deep calibration network",
            "venue": "NeurIPS.",
            "year": 2018
        },
        {
            "authors": [
                "Xingkun Liu",
                "Arash Eshghi",
                "Pawel Swietojanski",
                "Verena Rieser."
            ],
            "title": "Benchmarking natural language understanding services for building conversational agents",
            "venue": "IWSDS 2019, volume 714 of Lecture Notes in Electrical Engineering, pages 165\u2013183.",
            "year": 2019
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Michael Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv: Computation and Language.",
            "year": 2019
        },
        {
            "authors": [
                "Zihan Liu",
                "Genta Indra Winata",
                "Pascale Fung."
            ],
            "title": "Zero-resource cross-domain named entity recognition",
            "venue": "Proceedings of RepL4NLP@ACL, pages 1\u20136.",
            "year": 2020
        },
        {
            "authors": [
                "Zihan Liu",
                "Genta Indra Winata",
                "Peng Xu",
                "Pascale Fung."
            ],
            "title": "Coach: A coarse-to-fine approach for cross-domain slot filling",
            "venue": "Proceedings of ACL, pages 19\u201325. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Jie Ma",
                "Miguel Ballesteros",
                "Srikanth Doss",
                "Rishita Anubhai",
                "Sunil Mallya",
                "Yaser Al-Onaizan",
                "Dan Roth."
            ],
            "title": "Label semantics for few shot named entity recognition",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, Dublin,",
            "year": 2022
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher D. Manning."
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "Proceedings of EMNLP, pages 1532\u20131543. ACL.",
            "year": 2014
        },
        {
            "authors": [
                "Libo Qin",
                "Wanxiang Che",
                "Yangming Li",
                "Haoyang Wen",
                "Ting Liu."
            ],
            "title": "A stack-propagation framework with token-level intent detection for spoken language understanding",
            "venue": "EMNLP/IJCNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Libo Qin",
                "Fuxuan Wei",
                "Tianbao Xie",
                "Xiao Xu",
                "Wanxiang Che",
                "Ting Liu."
            ],
            "title": "Gl-gin: Fast and accurate non-autoregressive model for joint multiple intent detection and slot filling",
            "venue": "ACL/IJCNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Libo Qin",
                "Xiao Xu",
                "Wanxiang Che",
                "Ting Liu."
            ],
            "title": "Towards fine-grained transfer: An adaptive graphinteractive framework for joint multiple intent detection and slot filling",
            "venue": "Findings of EMNLP, pages 1807\u20131816.",
            "year": 2020
        },
        {
            "authors": [
                "Abhinav Rastogi",
                "Xiaoxue Zang",
                "Srinivas Sunkara",
                "Raghav Gupta",
                "Pranav Khaitan."
            ],
            "title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset",
            "venue": "AAAI 2020, pages 8689\u20138696. AAAI Press.",
            "year": 2020
        },
        {
            "authors": [
                "Erik Tjong Kim Sang",
                "Fien De Meulder."
            ],
            "title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition",
            "venue": "CoNLL.",
            "year": 2003
        },
        {
            "authors": [
                "Darsh J. Shah",
                "Raghav Gupta",
                "Amir A. Fayazi",
                "Dilek Hakkani-T\u00fcr."
            ],
            "title": "Robust zero-shot crossdomain slot filling with example values",
            "venue": "Proceedings of ACL, pages 5484\u20135490. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "A.B. Siddique",
                "Fuad Jamour",
                "Vagelis Hristidis."
            ],
            "title": "Linguistically-enriched and context-aware zero-shot slot filling",
            "venue": "Proceedings of the Web Conference 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Jake Snell",
                "Kevin Swersky",
                "Richard S. Zemel."
            ],
            "title": "Prototypical networks for few-shot learning",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,",
            "year": 2017
        },
        {
            "authors": [
                "Laurens van der Maaten",
                "Geoffrey E. Hinton."
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of Machine Learning Research, 9:2579\u20132605.",
            "year": 2008
        },
        {
            "authors": [
                "Andrew Viterbi."
            ],
            "title": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm",
            "venue": "IEEE transactions on Information Theory, 13(2):260\u2013269.",
            "year": 1967
        },
        {
            "authors": [
                "Liwen Wang",
                "Xuefeng Li",
                "Jiachi Liu",
                "Keqing He",
                "Yuanmeng Yan",
                "Weiran Xu."
            ],
            "title": "Bridge to target domain by prototypical contrastive learning and label confusion: Re-explore zero-shot learning for slot filling",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Di Wu",
                "Liang Ding",
                "Fan Lu",
                "J. Xie."
            ],
            "title": "Slotrefine: A fast non-autoregressive model for joint intent detection and slot filling",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Linzhi Wu",
                "Pengjun Xie",
                "Jie Zhou",
                "Meishan Zhang",
                "Chunping Ma",
                "Guangwei Xu",
                "Min Zhang."
            ],
            "title": "Robust self-augmentation for named entity recognition with meta reweighting",
            "venue": "Proceedings of NAACL 2022, Seattle, WA, United States, July 10-15,",
            "year": 2022
        },
        {
            "authors": [
                "Yuanmeng Yan",
                "Rumei Li",
                "Sirui Wang",
                "Fuzheng Zhang",
                "Wei Wu",
                "Weiran Xu."
            ],
            "title": "Consert: A contrastive framework for self-supervised sentence representation transfer",
            "venue": "Proceedings of ACL, pages 5065\u20135075. Association for Computational Linguis-",
            "year": 2021
        },
        {
            "authors": [
                "Mengshi Yu",
                "Jian Liu",
                "Yufeng Chen",
                "Jinan Xu",
                "Yujie Zhang."
            ],
            "title": "Cross-domain slot filling as machine reading comprehension",
            "venue": "IJCAI.",
            "year": 2021
        },
        {
            "authors": [
                "Xiaodong Zhang",
                "Houfeng Wang."
            ],
            "title": "A joint model of intent determination and slot filling for spoken language understanding",
            "venue": "IJCAI.",
            "year": 2016
        },
        {
            "authors": [
                "Su Zhu",
                "Zijian Zhao",
                "Rao Ma",
                "Kai Yu."
            ],
            "title": "Prior knowledge driven label embedding for slot filling in natural language understanding",
            "venue": "IEEE ACM Trans. Audio Speech Lang. Process., 28:1440\u20131451.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Slot filling, as an essential component widely exploited in task-oriented conversational systems, has attracted increasing attention recently (Zhang and Wang, 2016; Goo et al., 2018; Gangadharaiah and Narayanaswamy, 2019). It aims to identify a specific type (e.g., artist and playlist) for each slot entity from a given user utterance. Owing to the rapid development of deep neural networks and with help from large-scale annotated data, research on slot filling has made great progress with considerable performance improvement (Qin et al., 2019; Wu et al., 2020; Qin et al., 2020, 2021).\n\u2217Corresponding author. 1The source code is available at https://github.com/\nSwitchsyj/AdaE2ML-XSF.\nDespite the remarkable accomplishments, there are at least two potential challenges in realistic application scenarios. First is the data scarcity problem in specific target domains (e.g., Healthcare and E-commerce). The manually-annotated training data in these domains is probably unavailable, and even the unlabeled training data might be hard to acquire (Jia et al., 2019; Liu et al., 2020a). As a result, the performance of slot filling models may drop significantly due to extreme data distribution shifts. The second is the existence of label shifts (as shown in the example in Figure 1). The target domain may contain novel slot types unseen in the source-domain label space (Liu et al., 2018; Shah et al., 2019; Liu et al., 2020b; Wang et al., 2021), namely there is a mismatch between different domain label sets. This makes it difficult to apply the source models to completely unseen target domains that are unobservable during the training process.\nZero-shot domain generalization has been shown to be a feasible solution to bridge the gap of domain shifts with no access to data from the target domain. Recent dominating advances focus on the two-step pipeline fashion to learn the zero-shot model using the metric learning paradigms (Shah et al., 2019; Liu et al., 2020b; He et al., 2020; Wang et al., 2021; Siddique et al., 2021). Nevertheless, besides inefficient inference resulted from non-parallelization,\nthe generalization capability of these models may be limited due to lack of knowledge sharing between sub-modules, and context-independent discrete static label embeddings. Although the alternative question-answering (QA) based methods (Du et al., 2021; Yu et al., 2021; Liu et al., 2022) are able to achieve impressive results, they need to manually design and construct the questions/queries, essentially introducing detailed descriptive information about the slot types.\nIn this work, we revisit the metric-based zeroshot cross-domain slot filling under challenging domain (both data and label) shifts. We propose an adaptive end-to-end metric learning scheme to improve the efficiency and effectiveness of the zeroshot model in favor of practical applications. For one thing, we provide a cascade-style joint learning architecture well coupled with the slot boundary module and type matching module, allowing for knowledge sharing among the sub-modules and higher computational efficiency. Moreover, the soft label embeddings are adaptively learnt by capturing the correlation between slot labels and utterance. For another, since slot terms with same types tend to have the semantically similar contexts, we propose a slot-level contrastive learning scheme to enhance the slot discriminative representations within different domain context. Finally, to verify the effectiveness of the proposed method, we carry out extensive experiments on different benchmark datasets. The empirical studies show the superiority of our method, which achieves effective performance gains compared to several competitive baseline methods.\nOverall, the main contributions can be summarized as follows: (1) Compared with existing metric-based methods, we propose a more efficient and effective end-to-end scheme for zero-shot slot filling, and show our soft label embeddings perform much better than previous commonly-used static label representations. (2) We investigate the slotlevel contrastive learning to effectively improve generalization capacity for zero-shot slot filling. (3) By extensive experiments, we demonstrate the benefits of our model in comparison to the existing metric-based methods, and provide an insightful quantitative and qualitative analysis."
        },
        {
            "heading": "2 Methodology",
            "text": "In this section, we first declare the problem to be addressed about zero-shot slot filling, and then elab-\norate our solution to this problem."
        },
        {
            "heading": "2.1 Problem Statement",
            "text": "Suppose we have the source domain DS = {(xSi ,ySi )} NS i=1 with NS labeled samples from distribution PS , and the (testing) target domain DT = {(yTj )}Cj=1 with C slot types from target distribution P T . We define \u2126S as the label set of source domain DS , and \u2126T as the label set of target domain DT . \u2126sh = \u2126S \u2229 \u2126T denotes the common slot label set shared by DS and DT . In the zeroshot scenario, the label sets between different domains may be mismatching, thus \u2126sh \u2286 \u2126S and PS \u0338= P T . The goal is to learn a robust and generalizable zero-shot slot filling model that can be well adapted to novel domains with unknown testing distributions."
        },
        {
            "heading": "2.2 Overall Framework",
            "text": "In order to deal with variable slot types within an unknown domain, we discard the standard sequence labeling paradigm by cross-labeling (e.g., B-playlist, I-playlist). Instead, we adopt a cascade-style architecture coupled with the slot boundary module and typing module under a joint learning framework. The boundary module is used to detect whether the tokens in an utterance are slot terms or not by the CRF-based labeling method with BIO schema, while the typing module is used to match the most likely type for the corresponding slot term using the metric-based method. Since pretraining model is beneficial to learn general representations, we adopt the pre-trained BERT (Devlin et al., 2019) as our backbone encoder2. Figure 2 shows the overall framework, which is composed of several key components as follows:\nContext-aware Label Embedding Let c = [c1, \u00b7 \u00b7 \u00b7 , c|\u2126S |] (ci \u2208 \u2126S) denotes a slot label sequence consisting of all the elements of \u2126S . Given an input utterance sequence x = [x1, \u00b7 \u00b7 \u00b7 , xn] of n tokens with the corresponding ground-truth boundary label sequence ybd = [ybd1 , \u00b7 \u00b7 \u00b7 , ybdn ] (ybdi \u2208 {B, I, O}) and slot label sequence ysl = [ysl1 , \u00b7 \u00b7 \u00b7 , ysln ] (ysli \u2208 \u2126S), the slot label sequence acts as a prefix of the input utter-\n2Notice that we assume the BERT model is used as our encoder, but our method can also be integrated with other model architectures (e.g., RoBERTa (Liu et al., 2019b)).\nance, which is then encoded by BERT3:\n[rlabel; rutter] = BERT([c;x]), (1)\nwhere rlabel and rutter denote the fused contextual representations of the label and utterance sequence, respectively.\nFor each slot type, the slot label matrix is obtained by averaging over the representations of the slot label tokens. Unlike the conventional discrete and static label embeddings (Liu et al., 2020b; Siddique et al., 2021; Ma et al., 2022) that capture the semantics of each textual label separately, we attempt to build the label-utterance correlation, and the adaptive interaction between the slot labels and utterance tokens encourages the model to learn the context-aware soft label embeddings dynamically, which will be exploited as the supervision information for the metric learning.\nSlot Boundary Detection To determine the slot terms, we obtain the contextualized latent representations of the utterance through a single-layer BiLSTM,\nhutter = BiLSTM(rutter). (2)\nThen, a CRF layer is applied to the slot boundary decoding, aiming to model the boundary label de-\n3Considering the slot label sequence is not a natural language sentence linguistically, we remove the [SEP] token used to concatenate sentence pairs in BERT.\npendency. The negative log-likelihood objective function can be formulated as follows:\ne = Linear(hutter),\nscore(x,y) = n\u2211 i=1 (Tyi\u22121,yi + ei[yi]), Lbdy = \u2212 log p(ybd|x)\n= \u2212 log exp(score(x,y bd))\u2211\ny\u2032\u2208Yx exp(score(x,y \u2032))\n,\n(3)\nwhere e \u2208 Rn\u00d73 denotes the three-way emission vectors containing boundary information, T is the 3\u00d73 learnable label transition matrix, and Yx is the set of all possible boundary label sequences of utterance x. While inference, we employ the Viterbi algorithm (Viterbi, 1967) to find the best boundary label sequence.\nMetric-based Slot Typing Although slot boundary module can select the slot terms from an utterance, it fails to learn discriminative slot entities. Thus, we design a typing module to achieve it in parallel by semantic similarity matching between slot labels and utterance tokens.\nConcretely, we take advantage of the above boundary information to locate the slot entity tokens of the utterance. We specially exploit the soft-weighting boundary embedding vectors for enabling differentiable joint training, which are combined with the contextual utterance representations\nto obtain the boundary-enhanced representations:\nrbound = softmax(e) \u00b7Eb, u = Linear(Concat(rutter, rbound)),\n(4)\nwhere Eb \u2208 R3\u00d7db is a look-up table to store trainable boundary embeddings, and db indicates the embedding dimension. Meanwhile, the label embeddings are calculated by a bottleneck module consisting of an up-projection layer and a downprojection layer with a GELU (Hendrycks and Gimpel, 2016) nonlinearity:\nv = Linearup(GELU(Lineardw(rlabel))). (5)\nFurthermore, we leverage token-wise similarity matching between L2-normalized utterance representations and label embeddings. Since the slot entities are our major concern for predicting types, we ignore the non-entity tokens by mask provided by the boundary gold labels, resulting in the slot typing loss function defined as follows: Ltyp = \u2212 n\u2211\ni=1\n1[ybdi \u0338=O] log exp(\u27e8ui, sg(vi\u2217)\u27e9)\u2211|\u2126S | j=1 exp(\u27e8ui, sg(vj)\u27e9)\n\u2212 n\u2211\ni=1\n1[ybdi \u0338=O] log exp(\u27e8sg(ui),vi\u2217\u27e9)\u2211|\u2126S | j=1 exp(\u27e8sg(ui),vj\u27e9) ,\n(6)\nwhere \u27e8\u00b7, \u00b7\u27e9 measures the cosine similarity of two embeddings, sg(\u00b7) stands for the stop-gradient operation that does not affect the forward computation, i\u2217 indicates the index corresponding to the gold slot label ysli , and 1[ybdi \u0338=O] \u2208 {0, 1} is an indicator function, evaluating to 1 if ybdi is a non-O tag. Eq. 6 makes sure the label embeddings act as the supervision information (the 1st term) and meanwhile are progressively updated (the 2nd term).\nSlot-level Contrastive Learning Recent line of works have investigated the instance-level contrastive learning by template regularization (Shah et al., 2019; Liu et al., 2020b; He et al., 2020; Wang et al., 2021). As slots with the same types tend to have the semantically similar contexts, inspired by Das et al. (2022), we propose to use the slot-level contrastive learning to facilitate the discriminative slot representations that may contribute to adaptation robustness.4\n4Different from Das et al. (2022), we do not use the Gaussian embeddings produced by learnt Gaussian distribution parameters. There are two main reasons: one is to ensure the stable convergence of training, and the other is that the token representations may not follow normal distribution.\nMore specifically, we define a supervised contrastive objective by decreasing the similarities between different types of slot entities while increasing the similarities between the same ones. We just pay attention to the slot entities by masking out the parts with O boundary labels. Then, we gather in-batch positive pairs P+ with the same slot type and negative pairs P\u2212 with different ones:\ns = ReLU(Linear(rutter)),\nP+ = {(si, sj)|ysli = yslj , i \u0338= j}, P\u2212 = {(si, sj)|ysli \u0338= yslj , i \u0338= j},\n(7)\nwhere s denotes the projected point embeddings, and all example pairs are extracted from a minibatch. Furthermore, we adapt the NT-Xent loss (Chen et al., 2020) to achieve the slot-level discrimination, and the contrastive learning loss function can be formulated as:\nLctr = \u2212 log 1\n|P+| \u2211\n(si,sj)\u2208P+ exp(d(s i, sj)/\u03c4)\u2211\n(si,sj)\u2208P exp(d(s i, sj)/\u03c4)\n,\n(8)\nwhere P denotes P+ \u222a P\u2212, d(\u00b7, \u00b7) denotes the distance metric function (e.g., cosine similarity distance), and \u03c4 is a temperature parameter. We will investigate different kinds of metric functions in the following experiment section."
        },
        {
            "heading": "2.3 Training and Inference",
            "text": "During training, our overall framework is optimized end-to-end with min-batch. The final training objective is to minimize the sum of the all loss functions:\nL = Lbdy + Ltyp + Lctr, (9)\nwhere each part has been defined in the previous subsections. During inference, we have the slot type set of the target domain samples, and the testing slot labels constitute the label sequence, which is then concatenated with the utterance as the model input. The CRF decoder predicts the slot boundaries of the utterance, and the predicted slot type corresponds to the type with the highest-matching score. We take the non-O-labeled tokens as slot terms while the O-labeled tokens as the context."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Datasets and Settings",
            "text": "To evaluate the proposed method, we conduct the experiments on the SNIPS dataset for zero-\nshot settings (Coucke et al., 2018), which contains 39 slot types across seven different domains: AddToPlaylist (ATP), BookRestaurant (BR), GetWeather (GW), PlayMusic (PM), RateBook (RB), SearchCreativeWork (SCW) and SearchScreeningEvent (SSE). Following previous studies (Liu et al., 2020b; Siddique et al., 2021), we choose one of these domains as the target domain never used for training, and the remaining six domains are combined to form the source domain. Then, we split 500 samples in the target domain as the development set and the remainder are used for the test set. Moreover, we consider the case where the label space of the source and target domains are exactly the same, namely the zero-resource setting (Liu et al., 2020a) based on named entity recognition (NER) task. We train our model on the CoNLL-2003 (Sang and Meulder, 2003) dataset and evaluate on the CBS SciTech News dataset (Jia et al., 2019)."
        },
        {
            "heading": "3.2 Baselines",
            "text": "We compare our method with the following competitive baselines using the pre-trained BERT as encoder: (1) Coach. Liu et al. (2020b) propose a two-step pipeline matching framework assisted by template regularization; (2) PCLC.Wang et al. (2021) propose a prototypical contrastive learning method with label confusion; (3) LEONA. Siddique et al. (2021) propose to integrate linguistic knowledge (e.g., external NER and POS-tagging cues) into the basic framework.\nAlthough not our focused baselines, we also compare against the advanced generative baselines (Li et al., 2023) with T5-Large and QA-based methods (Du et al., 2021; Liu et al., 2022) that require manual efforts to convert slot type descriptions into sentential queries/questions, and process by means of the machine reading comprehension (MRC) ar-\nchitecture (Li et al., 2020)."
        },
        {
            "heading": "3.3 Implementation Details",
            "text": "We use the pre-trained uncased BERTBASE model5 as the backbone encoder. The dimension of the boundary embedding is set to 10. We use 0.1 dropout ratio for slot filling and 0.5 for NER. For the contrastive learning module, we use the cosine metric function and select the optimal temperature \u03c4 from 0.1 to 1. During training, the AdamW (Loshchilov and Hutter, 2019) optimizer with a mini-batch size 32 is applied to update all trainable parameters, and the initial learning rate is set to 2e-5 for BERT and 1e-3 for other modules. All the models are trained on NIVIDIA GeForce RTX 3090Ti GPUs for up to 30 epochs. The averaged F1-score over five runs is used to evaluate the performance. The best-performing model on the development set is used for testing."
        },
        {
            "heading": "3.4 Zero-Shot Slot Filling",
            "text": "As shown in Table 1, our method achieves more promising performance than previously proposed metric-based methods on various target domains, with an average about 5% improvements compared with the strong baseline LEONA. We attribute it to the fact that our proposed joint learning model make full use of the sub-modules, and the context-aware soft label embeddings provide better prototype representations. Moreover, we also observe that the slot-level contrastive learning plays an important role in improving adaptation performance. Our model with Slot-CL obtains consistent performance gains over almost all the target domains except for the SSE domain. We suspect that it may result from slot entity confusion. For example, for slot entities \u201ccinema\u201d and\n5https://huggingface.co/bert-base-uncased\n\u201ctheatre\u201d from SSE, they are usually annotated with object_location_type, but \u201ccinemas\u201d in \u201ccaribbean cinemas\u201d and \u201ctheatres\u201d in \u201cstar theatres\u201d are annotated with location_name, which is prone to be misled by the contrastive objective. Additionally, without introducing extra manual prior knowledge, our method achieves very competitive performance compared with the QA-based baselines."
        },
        {
            "heading": "3.5 Zero-Resource NER",
            "text": "In particular, we examine our method in the zeroresource NER setting. As presented in Table 2, our method is also adaptable to this scenario, and exceed or match the performance of previous competitive baselines. Meanwhile, the slot-level contrastive learning can yield effective performance improvements."
        },
        {
            "heading": "3.6 Ablation Study and Analysis",
            "text": "In order to better understand our method, we further present some quantitative and qualitative analyses that provides some insights into why our method works and where future work could potentially improve it.\nInference Speed One advantage of our framework is the efficient inference process benefiting from the well-parallelized design. We evaluate the speed by running the model one epoch on the BookRestaurant test data with batch size set to 32. Results in Table 3 show that our method achieves \u00d713.89 and \u00d77.06 speedup compared with the advanced metric-based method (i.e., LEONA) and QA-based method (i.e., SLMRC). This could be attributed to our batch-wise decoding in parallel. On the one hand, previous metric-based methods use the two-pass pipeline decoding process and instance-wise slot type prediction. On the other\nhand, the QA-based methods require introducing different queries for all candidate slot labels regarding each utterance, increasing the decoding latency of a single example.\nLabel-Utterance Interaction Here we examine how our model benefits from the label-utterance interaction. As presented in Table 4, the performance of our model drops significantly when eliminating the interaction from different aspects , justifying our design. Compared to the other degraded interaction strategies, the utterance-label interaction helps learn the context-aware label embeddings, namely the utterance provides the context cues for the slot labels. Furthermore, we also notice that interaction between slot labels also makes sense. When only let each slot label attend to itself and the utterance, we observe the performance drop probably due to the loss of discriminative information among different slot labels.\nEffect of Context-aware Label Embedding We study the effect of different types of label embeddings. Figure 3 shows the comparison results. We can see that the proposed context-aware soft label embedding outperforms other purely discrete or decoupled embeddings, including discrete BERT, decoupled BERT or GloVe (Pennington et al.,\n2014) embeddings. Interestingly, when fine-tuning, we find that BERTdis works slightly better than BERTdec, as it might be harmful to tune soft label embeddings without utterance contexts. Furthermore, we observe a significant improvement of our model when incorporating the GloVe static vectors, suggesting that richer label semantics can make a positive difference. Meanwhile, the discrete or decoupled label embeddings without fine-tuning may yield better results.\nMetric Loss for Contrastive Learning Here we explore several typical distance metric functions (including Cosine, MSE, Smooth L1, and KLdivergence) for the slot-level contrastive objective, and we also consider the influence of temperature \u03c4 . Figure 4 reveals that the temperature value directly affects the final performance. Also, it shows better results overall at around \u03c4 = 0.5 for each metric function we take. We select the cosine similarity function as our desired distance metric function, due to its relatively good performance.\nFew-Shot Setting To verify the effectiveness of our method in the few-shot setting where the target domain has a small amount of training examples, we conduct experiments in the 20-shot and 50-shot scenarios. In line with previous works, we take the first K examples in the development set for training named the K-Shot scenario and the remaining keeps for evaluation.\nTable 5 illustrates that our method achieves superior performance compared with other representative metric-based methods. However, we also notice that our method without the slot-level contrastive learning obtains limited absolute improvements as data size increase, indicating the slot-level contrastive learning performs better in this case.\nUnseen Slot Generalization Since label shift is a critical challenge in zero-shot learning, to verify the generalization capacity, we specifically test our method on the unseen target data. Following Liu et al. (2022), we split the dataset into the seen and unseen group, where we only evaluate on unseen slot entities during training in the unseenslot group, while evaluate on the whole utterance in the unseenuttr group. From Table 6, our method performs better than other metric-based baselines, showing the superiority of our method for unseen domain generalization.\nCross-Dataset Setting Considering slot labels and utterances may vary significantly across different datasets, we further evaluate the proposed method under the cross-dataset scenario, a more challenging setting. Here we introduce another popular slot filling dataset ATIS (Liu et al., 2019a). It is used for the target (source) domain data while the SNIPS for the source (target) domain data6, as shown in Table 7. The results confirm that our method still works well in this challenging setting.\nVisualization Figure 5 shows the visualization of normalized slot entity representations before similarity matching using t-SNE dimensionality reduction algorithm (van der Maaten and Hinton, 2008). Obviously, our method can better obtain well-gathered clusters when introducing the slotlevel contrastive learning, facilitating the discriminative entity representations."
        },
        {
            "heading": "4 Related Work",
            "text": "Zero-shot Slot Filling In recent years, zero-shot slot filling has received increasing attention. A dominating line of research is the metric-learning method, where the core idea is to learn a prototype representation for each category and classify\n6We ignore the evaluation on the SGD (Rastogi et al., 2020), which is a fairly large-scale dataset with extremely unbalanced label distributions.\ntest data based on their similarities with prototypes (Snell et al., 2017). For slot filling, the semantic embeddings of textual slot descriptions usually serve as the prototype representations (Bapna et al., 2017; Lee and Jha, 2019; Zhu et al., 2020). Shah et al. (2019) utilize both the slot description and a few examples of slot values to learn semantic representations of slots. Furthermore, various two-pass pipeline schemes are proposed by separating the slot filling task into two steps along with template regularization (Liu et al., 2020b), adversarial training (He et al., 2020), contrastive learning (Wang et al., 2021), linguistic prior knowledge (Siddique et al., 2021). However, these mostly utilize the context-free discrete label embeddings, and the two-pass fashion has potential limitations due to a lack of knowledge sharing between sub-modules as well as inefficient inference. These motivate us to exploit the context-aware label representations under an end-to-end joint learning framework.\nAnother line of research is the QA-based methods that borrow from question-answering systems, relying on manually well-designed queries. Du et al. (2021) use a set of slot-to-question generation strategies and pre-train on numerous synthetic QA pairs. Yu et al. (2021) and Liu et al. (2022) apply the MRC framework (Li et al., 2020) to overcome the domain shift problem. Heo et al. (2022) modify the MRC framework into sequence-labeling style by using each slot label as query. Li et al. (2023) introduce a generative framework using each slot label as prompt. In our work, we mainly focus on the metric-based method without intentionally introducing external knowledge with manual efforts.\nContrastive Learning The key idea is to learn discriminative feature representations by contrasting positive pairs against negative pairs. Namely, those with similar semantic meanings are pushed towards each other in the embedding space while those with different semantic meanings are pulled apart each other. Yan et al. (2021) and Gao et al. (2021) explore instance-level self-supervised contrastive learning where sample pairs are constructed by data augmentation. Khosla et al. (2020) further explore the supervised setting by contrasting the set of all instances from the same class against those from the other classes. Das et al. (2022) present a token-level supervised contrastive learning solution to deal with the few-shot NER task by means of Gaussian embeddings.\nPrevious studies for slot filling mainly focus on\ninstance-level contrastive learning, which may be sub-optimal for a fine-grained sequence labeling task. Inspired by supervised contrastive learning, we leverage a slot-level contrastive learning scheme for zero-shot slot filling to learn the discriminative representations for domain adaptation. For all existing slot entities within a mini-batch, we regard those with the same type as the positive example pairs and those with different type as negative ones."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we tackle the problem of generalized zero-shot slot filling by the proposed end-toend metric learning based scheme. We propose a cascade-style multi-task learning framework to efficiently detect the slot entity from a target domain utterance. The context-aware soft label embeddings are shown to be superior to the widely-used discrete ones. Regarding domain adaptation robustness, we propose a slot level contrastive learning scheme to facilitate the discriminative representations of slot entities. Extensive experiments across various domain datasets demonstrate the effectiveness of the proposed approach when handling unseen target domains. Our investigation also confirms that semantically richer label representations enable help further boost the recognition performance, which motivates us to further explore external knowledge enhanced soft label embeddings for advancing the metric-based method.\nLimitations\nAlthough our work makes a further progress in the challenging zero-shot slot filling, it is subject to several potential limitations. Firstly, since slot label sequence is used as the prefix of the utterance, this directly results in a long input sequence. Secondly, our method may be negatively affected by severe label ambiguity. There are some slot entities with rather similar semantics, leading to wrong slot type predictions. For example, \u201cbook a manadonese restaurant\u201d, the slot entity type of \u201cmanadonese\u201d is actually cuisine, but is easily identified as country. One major reason is that some utterances are relatively short and lack sufficient contextual cues. Thirdly, the recognition performance of metric-based methods may remain difficult to exceed that of advanced QA-based or generative methods due to the fact that the latter manually introduces detailed slot label description by well-designed queries or prompts."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Code Implementation Here we present the core pseudo code of the proposed method. class End2endSLUTagger(nn.Module):\ndef forward(bert_inp , num_type , lbl_bd , lbl_typ):\n# (bsz , seq_len , hsz) bert_repr = bert(* bert_inp) r_utter = bert_repr[:,num_type\n+1:] r_label = bert_repr [:,1: num_type +1] # label adapter in Eq.5 v = adapter(r_label) + r_label h_utter = lstm(r_utter) e = proj(h_utter) r_bound = matmul(softmax(e), bound_emb.weight) # (bsz , 1) l_bdy = crf(e, lbl_bd) u = proj(concat(token_repr , r_bound))\n# (bsz , seq_len) lbl_score = matmul(normalize(u), normalize(v).T.detach ()) l_sglbl = cross_entropy(sg_score\n, lbl_typ) l_sglbl *= lbl_bd.ne('O')\nutt_score = matmul(normalize(u). detach (), normalize(v).T) l_sgutt = cross_entropy( utt_score , lbl_typ)\nl_sgutt *= lbl_bd.ne('O') # (bsz , 1) l_typ = l_sglbl.sum(-1) +\nl_sgutt.sum(-1) # filter out paddings and non - slot tokens f_utt = proj(r_utter) filt_ids = lbl_typ != idx_O filt_emb = f_utt[filt_ids] filt_typ = lbl_typ[filt_ids] # repeat on the second dimension inter_emb = filt_emb.unsqueeze (1).repeat(1, num_slot , 1).view( num_slot*num_slot , -1) int_typ = filt_typ.unsqueeze (1). repeat(1, num_slot).view(num_slot* num_slot)\n# repeat on the first dimension rept_emb = filt_emb.unsqueeze (0)\n.repeat(num_slot , 1, 1).view( num_slot*num_slot , -1) rept_typ = filt_typ.unsqueeze (0) .repeat(num_slot , 1).view(num_slot* num_slot) sim_score = cosine_similarity( inter_emb , rept_emb)\n# view as (num_slot , num_slot) denom_mask = (inter_emb !=\nrept_emb) numer_mask = denom * (int_typ ==\nrept_typ) loss = softmax(sim_score) /\ntemperature # # calculate Slot -CL with Eq.8,\n(bsz , 1) l_ctr = -(loss*numer_mask).log()\n+ (loss*denom_mask).log() + num_mask.sum().log()\nl_ctr = l_ctr.mean()\nreturn l_bdy + l_typ + l_ctr\nListing 1: Pseudo code for our proposed method."
        }
    ],
    "title": "Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling",
    "year": 2023
}