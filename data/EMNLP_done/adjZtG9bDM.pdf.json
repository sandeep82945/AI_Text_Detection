{
    "abstractText": "Warning: This paper contains content and language that may be considered offensive to some readers. While biases disadvantaging African American Language (AAL) have been uncovered in models for tasks such as speech recognition and toxicity detection, there has been little investigation of these biases for language generation models like ChatGPT. We evaluate how well LLMs understand AAL in comparison to White Mainstream English (WME), the encouraged \"standard\" form of English taught in American classrooms. We measure large language model performance on two tasks: a counterpart generation task, where a model generates AAL given WME and vice versa, as well as a masked span prediction (MSP) task, where models predict a phrase hidden from their input. Using a novel dataset of AAL texts from a variety of regions and contexts, we present evidence of dialectal bias for six pre-trained LLMs through performance gaps on these tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nicholas Deas"
        },
        {
            "affiliations": [],
            "name": "Jessi Grieser"
        },
        {
            "affiliations": [],
            "name": "Shana Kleiner"
        },
        {
            "affiliations": [],
            "name": "Elsbeth Turcan"
        },
        {
            "affiliations": [],
            "name": "Kathleen McKeown"
        }
    ],
    "id": "SP:9b16ca04a16c044ce4913507ab54df11d04df460",
    "references": [
        {
            "authors": [
                "Afra Feyza Aky\u00fcrek",
                "Muhammed Yusuf Kocyigit",
                "Sejin Paik",
                "Derry Tanti Wijaya."
            ],
            "title": "Challenges in measuring bias via open-ended language generation",
            "venue": "Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP), pages",
            "year": 2022
        },
        {
            "authors": [
                "April Baker-Bell."
            ],
            "title": "Linguistic justice: Black language, literacy, identity, and pedagogy",
            "venue": "Routledge.",
            "year": 2020
        },
        {
            "authors": [
                "John Baugh."
            ],
            "title": "755SWB (Speaking while Black): Linguistic Profiling and Discrimination Based on Speech as a Surrogate for Race against Speakers of African American Vernacular English",
            "venue": "The Oxford Handbook of African American Language. Oxford",
            "year": 2015
        },
        {
            "authors": [
                "Emily M. Bender",
                "Batya Friedman."
            ],
            "title": "Data statements for natural language processing: Toward mitigating system bias and enabling better science",
            "venue": "Transactions of the Association for Computational Linguistics, 6:587\u2013604.",
            "year": 2018
        },
        {
            "authors": [
                "Emily M. Bender",
                "Timnit Gebru",
                "Angelina"
            ],
            "title": "McMillanMajor, and Shmargaret Shmitchell",
            "venue": "In Proceedings of the 2021 ACM Conference on Fairness, Accountability,",
            "year": 2021
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Solon Barocas",
                "Hal Daum\u00e9 III",
                "Hanna Wallach."
            ],
            "title": "Language (technology) is power: A critical survey of \u201cbias\u201d in NLP",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Lisa Green",
                "Brendan O\u2019Connor"
            ],
            "title": "Demographic dialectal variation in social media: A case study of African-American English",
            "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2016
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Brendan O\u2019Connor"
            ],
            "title": "Racial disparity in natural language processing: A case study of social media african-american english",
            "year": 2017
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Johnny Wei",
                "Brendan O\u2019Connor"
            ],
            "title": "Twitter Universal Dependency parsing for African-American and mainstream",
            "venue": "American English. In Proceedings of the 56th Annual Meeting of the Association",
            "year": 2018
        },
        {
            "authors": [
                "Staja \u201cStar\u201d Booker",
                "Chris Pasero",
                "Keela A. Herr"
            ],
            "title": "Practice recommendations for pain assessment by self-report with african american older adults",
            "venue": "Geriatric Nursing,",
            "year": 2015
        },
        {
            "authors": [
                "Alexander Braylan",
                "Omar Alonso",
                "Matthew Lease."
            ],
            "title": "Measuring annotator agreement generally across complex structured, multi-object, and freetext annotation tasks",
            "venue": "Proceedings of the ACM Web Conference 2022. ACM.",
            "year": 2022
        },
        {
            "authors": [
                "Emanuele Bugliarello",
                "Sabrina J. Mielke",
                "Antonios Anastasopoulos",
                "Ryan Cotterell",
                "Naoaki Okazaki."
            ],
            "title": "It\u2019s easier to translate out of English than into it: Measuring neural translation difficulty by crossmutual information",
            "venue": "Proceedings of the 58th An-",
            "year": 2020
        },
        {
            "authors": [
                "Kai-Wei Chang",
                "Vinodkumar Prabhakaran",
                "Vicente Ordonez."
            ],
            "title": "Bias and fairness in natural language processing",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on",
            "year": 2019
        },
        {
            "authors": [
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language models",
            "year": 2022
        },
        {
            "authors": [
                "Richard Delgado",
                "Jean Stefancic."
            ],
            "title": "Critical race theory: An introduction, volume 87",
            "venue": "NyU press.",
            "year": 2023
        },
        {
            "authors": [
                "Dorottya Demszky",
                "Devyani Sharma",
                "Jonathan Clark",
                "Vinodkumar Prabhakaran",
                "Jacob Eisenstein."
            ],
            "title": "Learning to recognize dialect features",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Jwala Dhamala",
                "Tony Sun",
                "Varun Kumar",
                "Satyapriya Krishna",
                "Yada Pruksachatkun",
                "Kai-Wei Chang",
                "Rahul Gupta."
            ],
            "title": "BOLD",
            "venue": "Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. ACM.",
            "year": 2021
        },
        {
            "authors": [
                "M Margaret Dolcini",
                "Jesse A Canchola",
                "Joseph A Catania",
                "Marissa M Song Mayeda",
                "Erin L Dietz",
                "Coral Cotto-Negr\u00f3n",
                "Vasudha Narayanan"
            ],
            "title": "National-level disparities in internet access among low-income and black and hispanic youth",
            "year": 2021
        },
        {
            "authors": [
                "Li Dong",
                "Nan Yang",
                "Wenhui Wang",
                "Furu Wei",
                "Xiaodong Liu",
                "Yu Wang",
                "Jianfeng Gao",
                "Ming Zhou",
                "Hsiao-Wuen Hon."
            ],
            "title": "Unified language model pre-training for natural language understanding and generation",
            "venue": "Advances in Neural Information Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Charlie Farrington",
                "Sharese King",
                "Mary Kohn."
            ],
            "title": "Sources of variation in the speech of african americans: Perspectives from sociophonetics",
            "venue": "WIREs Cognitive Science, 12(3).",
            "year": 2021
        },
        {
            "authors": [
                "Hila Gonen",
                "Srini Iyer",
                "Terra Blevins",
                "Noah A. Smith",
                "Luke Zettlemoyer"
            ],
            "title": "Demystifying prompts in language models via perplexity estimation",
            "year": 2022
        },
        {
            "authors": [
                "Lisa J. Green."
            ],
            "title": "African American English: A Linguistic Introduction",
            "venue": "Cambridge Univ. Press.",
            "year": 2009
        },
        {
            "authors": [
                "Jessica A Grieser."
            ],
            "title": "Toward understanding the nwords",
            "venue": "American Speech: A Quarterly of Linguistic Usage, 94(4):409\u2013419.",
            "year": 2019
        },
        {
            "authors": [
                "Jessica A Grieser."
            ],
            "title": "The Black side of the river: Race, language, and belonging in Washington, DC",
            "venue": "Georgetown University Press.",
            "year": 2022
        },
        {
            "authors": [
                "Sophie Groenwold",
                "Lily Ou",
                "Aesha Parekh",
                "Samhita Honnavalli",
                "Sharon Levy",
                "Diba Mirza",
                "William Yang Wang."
            ],
            "title": "Investigating AfricanAmerican Vernacular English in transformer-based text generation",
            "venue": "Proceedings of the 2020 Con-",
            "year": 2020
        },
        {
            "authors": [
                "Salima Harrat",
                "Karima Meftouh",
                "Kamel Smaili."
            ],
            "title": "Machine translation for arabic dialects (survey)",
            "venue": "Information Processing & Management, 56(2):262\u2013273. Advance Arabic Natural Language Processing (ANLP) and its Applications.",
            "year": 2019
        },
        {
            "authors": [
                "Linette N. Hinton",
                "Karen E. Pollock."
            ],
            "title": "Regional variations in the phonological characteristics of african american vernacular english",
            "venue": "World Englishes, 19(1):59\u201371.",
            "year": 2000
        },
        {
            "authors": [
                "I-Ching Hsu",
                "Jiun-De Yu."
            ],
            "title": "A medical chatbot using machine learning and natural language understanding",
            "venue": "Multimedia Tools and Applications, 81(17):23777\u201323799.",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Johnson",
                "Kevin Everson",
                "Vijay Ravi",
                "Anissa Gladney",
                "Mari Ostendorf",
                "Abeer Alwan."
            ],
            "title": "Automatic Dialect Density Estimation for African American English",
            "venue": "Proc. Interspeech 2022, pages 1283\u20131287.",
            "year": 2022
        },
        {
            "authors": [
                "Anna J\u00f8rgensen",
                "Dirk Hovy",
                "Anders S\u00f8gaard."
            ],
            "title": "Learning a POS tagger for AAVE-like language",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2016
        },
        {
            "authors": [
                "Tyler Kendall",
                "Charlie Farrington"
            ],
            "title": "The corpus of regional african american language",
            "year": 2021
        },
        {
            "authors": [
                "Svetlana Kiritchenko",
                "Saif Mohammad."
            ],
            "title": "Examining gender and race bias in two hundred sentiment analysis systems",
            "venue": "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 43\u201353, New Orleans, Louisiana.",
            "year": 2018
        },
        {
            "authors": [
                "Allison Koenecke",
                "Andrew Nam",
                "Emily Lake",
                "Joe Nudell",
                "Minnie Quartey",
                "Zion Mengesha",
                "Connor Toups",
                "John R. Rickford",
                "Dan Jurafsky",
                "Sharad Goel."
            ],
            "title": "Racial disparities in automated speech recognition",
            "venue": "Proceedings of the National",
            "year": 2020
        },
        {
            "authors": [
                "Nima Kordzadeh",
                "Maryam Ghasemaghaei."
            ],
            "title": "Algorithmic bias: review, synthesis, and future research directions",
            "venue": "European Journal of Information Systems, 31(3):388\u2013409.",
            "year": 2022
        },
        {
            "authors": [
                "Sonja L Lanehart."
            ],
            "title": "Sociocultural and historical contexts of African American English",
            "venue": "John Benjamins Publishing.",
            "year": 2001
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Lei Li",
                "Wei Liu",
                "Marina Litvak",
                "Natalia Vanetik",
                "Jiacheng Pei",
                "Yinan Liu",
                "Siya Qi"
            ],
            "title": "Subjective bias in abstractive summarization",
            "year": 2021
        },
        {
            "authors": [
                "Joshua L. Martin",
                "Kevin Tang."
            ],
            "title": "Understanding Racial Disparities in Automatic Speech Recognition: The Case of Habitual \u201cbe",
            "venue": "Proc. Interspeech 2020, pages 626\u2013630.",
            "year": 2020
        },
        {
            "authors": [
                "Tessa Masis",
                "Anissa Neal",
                "Lisa Green",
                "Brendan O\u2019Connor"
            ],
            "title": "Corpus-guided contrast sets for morphosyntactic feature detection in low-resource English varieties",
            "venue": "In Proceedings of the first workshop on NLP applications to field linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Zion Mengesha",
                "Courtney Heldreth",
                "Michal Lahav",
                "Juliana Sublewski",
                "Elyse Tuennerman"
            ],
            "title": "i don\u2019t think these devices are very culturally sensitive.\u201d\u2014impact of automated speech recognition errors on african americans",
            "year": 2021
        },
        {
            "authors": [
                "Josh Meyer",
                "Lindy Rauchenstein",
                "Joshua D. Eisenberg",
                "Nicholas Howell."
            ],
            "title": "Artie bias corpus: An open dataset for detecting demographic bias in speech applications",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Confer-",
            "year": 2020
        },
        {
            "authors": [
                "Marcyliena H. Morgan."
            ],
            "title": "7",
            "venue": "\u201cnuthin\u2019 but a g thang\u201d: Grammar and language ideology in hip hop identity. In Sociocultural and Historical Contexts of African American English.",
            "year": 2001
        },
        {
            "authors": [
                "Ziad Obermeyer",
                "Brian Powers",
                "Christine Vogeli",
                "Sendhil Mullainathan."
            ],
            "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
            "venue": "Science, 366(6464):447\u2013453.",
            "year": 2019
        },
        {
            "authors": [
                "Desmond U. Patton",
                "William R. Frey",
                "Kyle A. McGregor",
                "Fei-Tzin Lee",
                "Kathleen McKeown",
                "Emanuel Moss"
            ],
            "title": "Contextual analysis of social media: The promise and challenge of eliciting context in social media posts with natural language processing",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Jacquelyn Rahman."
            ],
            "title": "The n word: Its history and use in the african american community",
            "venue": "Journal of English Linguistics - J ENGL LINGUIST, 40:137\u2013 171.",
            "year": 2012
        },
        {
            "authors": [
                "Anthony Rios."
            ],
            "title": "Fuzze: Fuzzy fairness evaluation of offensive language classifiers on african-american english",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 34(01):881\u2013889.",
            "year": 2020
        },
        {
            "authors": [
                "Maggie Ronkin",
                "Helen E. Karn."
            ],
            "title": "Mock ebonics: Linguistic racism in parodies of ebonics on the internet",
            "venue": "Journal of Sociolinguistics, 3(3):360\u2013380.",
            "year": 1999
        },
        {
            "authors": [
                "Harrison Santiago",
                "Joshua Martin",
                "Sarah Moeller",
                "Kevin Tang."
            ],
            "title": "Disambiguation of morphosyntactic features of African American English \u2013 the case of habitual be",
            "venue": "Proceedings of the Second Workshop on Language Technology for Equality, Di-",
            "year": 2022
        },
        {
            "authors": [
                "Maarten Sap",
                "Swabha Swayamdipta",
                "Laura Vianna",
                "Xuhui Zhou",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "Annotators with attitudes: How annotator beliefs and identities bias toxic language detection",
            "venue": "Proceedings of the 2022 Conference of the North Amer-",
            "year": 2022
        },
        {
            "authors": [
                "Deven Santosh Shah",
                "H. Andrew Schwartz",
                "Dirk Hovy."
            ],
            "title": "Predictive biases in natural language processing models: A conceptual framework and overview",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Tianshu Shen",
                "Jiaru Li",
                "Mohamed Reda Bouadjenek",
                "Zheda Mai",
                "Scott Sanner"
            ],
            "title": "Unintended bias in language model-driven conversational recommendation",
            "year": 2022
        },
        {
            "authors": [
                "Emily Sheng",
                "Josh Arnold",
                "Zhou Yu",
                "Kai-Wei Chang",
                "Nanyun Peng"
            ],
            "title": "Revealing persona biases in dialogue systems",
            "year": 2021
        },
        {
            "authors": [
                "Emily Sheng",
                "Kai-Wei Chang",
                "Prem Natarajan",
                "Nanyun Peng."
            ],
            "title": "Towards Controllable Biases in Language Generation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3239\u20133254, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Emily Sheng",
                "Kai-Wei Chang",
                "Prem Natarajan",
                "Nanyun Peng."
            ],
            "title": "Societal biases in language generation: Progress and challenges",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Emily Sheng",
                "Kai-Wei Chang",
                "Premkumar Natarajan",
                "Nanyun Peng."
            ],
            "title": "The woman worked as a babysitter: On biases in language generation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Hiram Smith."
            ],
            "title": "Has nigga been reappropriated as a term of endearment? (a qualitative and quantitative analysis)",
            "venue": "American Speech, 94:420\u2013477.",
            "year": 2019
        },
        {
            "authors": [
                "Isabel Straw",
                "Chris Callison-Burch."
            ],
            "title": "Artificial intelligence in mental health and the biases of language based models",
            "venue": "PLOS ONE, 15(12):e0240376.",
            "year": 2020
        },
        {
            "authors": [
                "Yu Wan",
                "Baosong Yang",
                "Derek F. Wong",
                "Lidia S. Chao",
                "Haihua Du",
                "Ben C.H. Ao."
            ],
            "title": "Unsupervised neural dialect translation with commonality and diversity modeling",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):9130\u20139137.",
            "year": 2020
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing",
            "year": 2018
        },
        {
            "authors": [
                "Julie A. Washington",
                "Holly K. Craig",
                "Amy J. Kushmaul."
            ],
            "title": "Variable use of african american english across two language sampling contexts",
            "venue": "Journal of Speech, Language, and Hearing Research, 41(5):1115\u20131124.",
            "year": 1998
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi"
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "In International Conference on Learning Representations",
            "year": 2020
        },
        {
            "authors": [
                "Xuhui Zhou",
                "Maarten Sap",
                "Swabha Swayamdipta",
                "Yejin Choi",
                "Noah Smith."
            ],
            "title": "Challenges in automated debiasing for toxic language detection",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Caleb Ziems",
                "Jiaao Chen",
                "Camille Harris",
                "Jessica Anderson",
                "Diyi Yang."
            ],
            "title": "VALUE: Understanding dialect disparity in NLU",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Farrington",
                "TwitterAAE (Blodgett"
            ],
            "title": "2016), were originally created to study AAL and to study variation in AAL on social media",
            "year": 2016
        },
        {
            "authors": [
                "Braylan"
            ],
            "title": "2022. Annotator agreement is calcu",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "While biases disadvantaging African American Language (AAL) have been uncovered in models for tasks such as speech recognition and toxicity detection, there has been little investigation of these biases for language generation models like ChatGPT. We evaluate how well LLMs understand AAL in comparison to White Mainstream English (WME), the encouraged \"standard\" form of English taught in American classrooms. We measure large language model performance on two tasks: a counterpart generation task, where a model generates AAL given WME and vice versa, as well as a masked span prediction (MSP) task, where models predict a phrase hidden from their input. Using a novel dataset of AAL texts from a variety of regions and contexts, we present evidence of dialectal bias for six pre-trained LLMs through performance gaps on these tasks."
        },
        {
            "heading": "1 Introduction",
            "text": "Task-specific models proposed for speech recognition, toxicity detection, and language identification have previously been documented to present biases for certain language varieties, particularly for African American Language (AAL) (Sap et al., 2022; Koenecke et al., 2020; Meyer et al., 2020; Blodgett and O\u2019Connor, 2017). There has been little investigation, however, of the possible language variety biases in Large Language Models (LLMs) (Dong et al., 2019; Brown et al., 2020; Raffel et al., 2020), which have unified multiple tasks through language generation.\nWhile there are largely beneficial and socially relevant applications of LLMs, such as in alleviating barriers to mental health counseling1 and medical healthcare (Hsu and Yu, 2022) access, there is also potential for biased models to exacerbate existing societal inequalities (Kordzadeh and Ghasemaghaei, 2022; Chang et al., 2019; Bender et al., 2021). Past algorithms used in psychiatry and medicine have been shown to be racially biased, in some cases leading to, for example, underestimating patient risk and denial of care (Obermeyer et al., 2019; Straw and Callison-Burch, 2020). Furthermore, LLMs capable of understanding AAL and other language varieties also raise important ethical implications, such as enabling increased police surveillance of minority groups (see Patton et al. 2020 and section 8 for further discussion). Therefore, it is necessary to investigate the potential language variety biases of language generation models to both increase accessibility of applications with high social impact and also anticipate possible harms when deployed.\nMoreover, prior work (Grieser, 2022) has shown that African American speakers talking about racerelated issues use language in ways which may draw on morphosyntactic features of AAL in order to subtly foreground the race aspect of the discussion topic without explicit mention. Most training corpora include little representation of AAL (see further discussion in section 3), and even those that do can still fail to capture its significant regional and contextual variation (see Farrington et al. 2021 for examples). Without the ability to interpret these\n1https://www.x2ai.com/\nsubtler meanings of AAL, LLMs will undoubtedly exacerbate the misunderstandings which already take place between AAL speakers and other communities.\nGiven the lack of African American representation in LLMs and the possible harms to the AALspeaking community, we focus on LLMs\u2019 understanding of AAL to investigate biases. AAL is a language variety which follows consistent morphological, syntactic, and lexical patterns distinct from WME, such as the dropped copula (e.g., \"she at work\") and aspect markers (e.g., the habitual be: \"he be running\") (Lanehart, 2001; Green, 2009). We use Grieser (2022)\u2019s definition of AAL as the grammatically patterned variety of English used by many, but not all and not exclusively, African Americans in the United States. Following BakerBell (2020) and Alim and Smitherman (2012), we also use the definition of White Mainstream English (WME) as the dialect of English reflecting the linguistic norms of white Americans. While previous linguistic literature occasionally uses the terms \"Standard American English\" and \"African American Vernacular English,\" we employ AAL and WME instead to avoid the implication that AAL and other language varieties are \"non-standard\" and to more precisely identify the demographics of prototypical WME speakers, similarly to Baker-Bell (2020) and Alim and Smitherman (2012). Examples of AAL and WME are shown in Table 1.\nWe evaluate understanding of AAL by LLMs through production of language in each variety using automatic metrics and human judgments for two tasks: a counterpart generation task akin to dialect translation (Wan et al., 2020; Harrat et al., 2019) (see examples in Table 1) and a masked span prediction (MSP) task where models predict a phrase that was removed from their input, similar to Groenwold et al. (2020). We summarize our contributions as follows: (1) we evaluate six pre-trained,\nlarge language models on two language generation tasks: counterpart generation between language varieties and masked span prediction; (2) we use a novel dataset of AAL text from multiple contexts (social media, hip-hop lyrics, focus groups, and linguistic interviews) with human-annotated counterparts in WME; and (3) we document performance gaps showing that LLMs have more difficulty both interpreting and producing AAL compared to WME; our error analysis reveals patterns of AAL features that models have difficulty interpreting in addition to those that they can understand."
        },
        {
            "heading": "2 Background: Bias",
            "text": "In measuring AAL understanding, we identify evidence of bias through performance gaps and analysis of model behavior with each language variety. Following Blodgett et al. (2020), findings of bias could result in both allocational harms and representational harms posed by the evaluated models2.\nWhile LLMs are becoming more available and valuable resources, the models\u2019 lack of understanding of AAL limits their use by AAL speakers, and this disparity will only grow as the use of these models increases across social spheres. Our evaluation attempts to quantify these error disparities (Shah et al., 2020) by measuring models\u2019 understanding of AAL and WME texts. When LLMs do not perform equally well on different language varieties, the LLM itself as a resource becomes unfairly allocated, and speakers of minoritized language varieties like AAL are less able to leverage the benefits of LLMs. AAL speakers would be particularly unfairly impacted with applications in areas of health, including mental health.\nAdditionally, our evaluation includes a qualitative analysis of how AAL is currently understood and produced by LLMs. Prior sociolinguistic works discuss and study how attitudes toward African American speakers have formed linguistic prejudices against AAL (Baker-Bell, 2020; Baugh, 2015), as well as how stereotyped uses of AAL by non-AAL speakers can perpetuate racial divides (Ronkin and Karn, 1999). Stereotypical or offensive uses of AAL by LLMs thus reflect a representational harm to AAL speakers that can further\n2Allocational harms are reflected in the unfair distribution of resources and opportunities among social groups, while representational harms are reflected in disparate or harmful representations of a particular group (see Blodgett et al. (2020) for further discussion).\npromote these views. We advocate for approaches which carefully consider sociolinguistic variation in order to avoid generation of inappropriate speech across different settings."
        },
        {
            "heading": "3 Data",
            "text": "Biases in pre-trained language models can often be attributed to common training datasets. Training corpora for LLMs are typically drawn from internet sources, such as large book corpora, Wikipedia, outbound links from Reddit, and a filtered version of Common Crawl3 in the case of GPT-3 (Brown et al., 2020), which can severely under-represent the language of African Americans (Pew Research Center, 2018; Dolcini et al., 2021). Though few estimates of the presence of AAL in datasets exist, one study estimates that in the Colossal Cleaned Crawl Corpus (C4), only 0.07% of documents reflect AAL (Dodge et al., 2021). Beyond C4, African Americans are significantly underrepresented in data sources such as Wikipedia 4 (0.05%) and news articles (6%; Pew Research Center 2023), falling well below the national average. Additionally, as models learn from gold standard outputs provided by annotators, they learn to reflect the culture and values of the annotators as well."
        },
        {
            "heading": "3.1 Data Sources",
            "text": "There is significant variation in the use of features of AAL depending on, for example, the region or context of the speech or text (Washington et al., 1998; Hinton and Pollock, 2000). Accordingly, we collect a novel dataset of AAL from six different contexts. We draw texts from two existing datasets, the TwitterAAE corpus (Blodgett et al., 2016) and transcripts from the Corpus of Regional African American Language (CORAAL; Kendall and Farrington 2021), as well as four datasets collected specifically for this work: we collect all available posts and comments from r/BlackPeopleTwitter5 belonging to \"Country Club Threads\", which designates threads where only Black Redditors and other users of color may contribute 6. Given the influence of AAL on hip-hop music, we collect hip-hop lyrics from 27 songs, 3 from each of 9\n3http://commoncrawl.org/ 4https://meta.wikimedia.org/wiki/Community_\nInsights/Community_Insights_2021_Report/ Thriving_Movement\n5https://reddit.com/r/BlackPeopleTwitter 6To be verified as a person of color and allowed to contribute to Country Club Threads, users send in pictures of their forearm to reveal their skin tone.\nBlack artists from Morgan (2001) and Billboard\u2019s 2022 Top Hip-Hop Artists. Finally, we use the transcripts of 10 focus groups concerning grief and loss in the Harlem African American community and conducted as part of ongoing work by the authors to better understand the impacts of police brutality and other events on the grief experiences of African Americans. Following Bender and Friedman (2018), a data statement with further details is included in Appendix A.\n50 texts are sampled from each dataset, resulting in 300 candidate texts in total. We use a set of surface level and grammatical patterns to approximately weight each sample by the density of AAL-like language within the text (patterns are listed in Appendix B). 12 additional texts are also sampled from each dataset for fine-tuning."
        },
        {
            "heading": "3.2 Data Annotations",
            "text": "Our interdisciplinary team includes computer scientists, linguists, and social work scientists and thus, we could recruit knowledgeable annotators to construct semantically-equivalent re-writings of AAL texts into WME, referred to as counterparts. The four human annotators included 2 linguistics students, 1 computer science student, and 1 social work scientist, all of whom self-identify as AAL speakers and thus have knowledge of the linguistic and societal context of AAL and racial biases. These annotators were familiar with both AAL and WME, allowing them to provide accurate annotations and judgements of model generations in both language varieties. Annotators were asked to rewrite the AAL text in WME, ensuring that the counterparts conserve the original meaning and tone as closely as possible (see Appendix C.1).\nTo compute inter-annotator agreement, we asked each annotator to label the 72 additional texts, and they also shared a distinct 10% of the remainder of the dataset with each other annotator. We compute agreement using Krippendorff\u2019s alpha with Levenshtein distance (Braylan et al. 2022; see Appendix D for more details) showing 80% agreement (\u03b1 = .8000). After removing pairs from the dataset where annotators determined that no counterpart exists, the final dataset consists of 346 AAL-WME text pairs including the 72 additional texts. Dataset statistics are included in Table 2."
        },
        {
            "heading": "4 Methods",
            "text": "We evaluate multiple language generation models using two tasks. In the counterpart generation task, we evaluate models on producing near semantically-equivalent WME text given AAL text and vice versa to target LLMs\u2019 ability to interpret and understand AAL. A second task, masked span prediction, requires models to predict tokens to replace words and phrases hidden or masked from the input. This task resembles that of Groenwold et al. (2020), but spans vary in length and position. Much like BART pre-training (Lewis et al., 2020), span lengths are drawn from a Poisson distribution (\u03bb = 2) and span locations are sampled uniformly across words in the original text. We independently mask noun phrases, verb phrases, and random spans from the text for more fine-grained analysis.\nWhile our focus is on measuring model capabilities in interpreting AAL7, these generation tasks allow us to test whether the model understands the language well enough to produce it. It is not our goal to produce a LLM that can generate AAL within the context of downstream tasks (see section 8 for further discussion)."
        },
        {
            "heading": "4.1 Models",
            "text": "We consider six different models for the two tasks where applicable: GPT-3 (Brown et al., 2020);\n7In reference to model capabilities, \"interpretation\" and \"understanding\" refer to the ability of models to accurately encode the meaning and features of text in AAL and WME as opposed to cognitive notions of these terms.\nits chat-oriented successor, ChatGPT (GPT-3.5)8; GPT-4 (OpenAI, 2023), currently OpenAI\u2019s most advanced language model ; T5 (Raffel et al., 2020); its instruction-tuned variant, Flan-T5 (Chung et al., 2022); and BART (Lewis et al., 2020). Flan-T5, GPT-3, ChatGPT, and GPT-4 are evaluated on the counterpart generation task, while GPT-3, BART, and T5 are evaluated on the MSP task. We note that the GPT models besides GPT-3 were not included in the MSP task because token probabilities are not provided by the OpenAI API for chat-based models. An example of the instruction provided to GPT models is provided in Figure 1. Notably, the instruction text provided to GPT models uses \"African American Vernacular English\" and \"Standard American English\" because prompts with these terms were assigned lower perplexity than \"African American Language\" and \"White Mainstream English\" by all GPT models, and lower perplexity prompts have been shown to improve task performance (Gonen et al., 2022). Additionally, GPT models are simply asked to translate with no additional instructions in order to examine their natural tendency for tasks involving AAL text. We evaluate both Flan-T5 fine-tuned on the 72 additional texts, referred to as Flan-T5 (FT) in the results, and Flan-T5 without fine-tuning (with automatic metrics only). Additional modeling details and generation hyperparameters are included in Appendices E.1 and E.2."
        },
        {
            "heading": "4.2 Metrics",
            "text": "We use both automatic and human evaluation metrics for the counterpart generation task. As with most generation tasks, we first measure n-gram overlap of the model generations and gold standard reference texts and in our experiments, we utilize the Rouge metric. In addition, to account for the weaknesses of word-overlap measures, we also measure coverage of gold standard references with BERTScore (Zhang* et al., 2020) using the\n8https://openai.com/blog/chatgpt/\nmicrosoft/deberta-large-mnli checkpoint, because it is better correlated with human scores than other models9. Specifically, original AAL is the gold standard for model-generated AAL and human annotated WME counterparts are the gold standard for model-generated WME. In some experiments, Rouge-1, Rouge-L, and BERTScore are presented as gaps, where scores for generating WME are subtracted from those for generating AAL. Due to the tendency of models to avoid toxic outputs and neutralize text, we also consider the percentage of toxic terms removed when transitioning from model inputs in one language variety to outputs in the other. Toxicity scores are derived as the number of words categorized as offensive in the word list of Zhou et al. (2021), and percent change between inputs and outputs are calculated as (Toxin\u2212Toxout)Toxin .\nHuman evaluation is also conducted on the generated counterparts. The same linguistics student, computer science student, and social work scientists involved in creating the dataset of aligned counterparts were also asked to judge model generations. As a baseline, human-generated counterparts are included in the human evaluation. 100 WME and AAL texts along with their generated\n9https://docs.google.com/spreadsheets/d/ 1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/ edit\nand annotated counterparts are randomly sampled from the dataset for human evaluation. We ensure that annotators do not rate human or modelgenerated counterparts for which they initially generated the WME counterpart. All annotators are asked to rate each assigned counterpart using 5- point Likert scales on the dimensions below.\nHuman-likeness (Human-like) measures whether the annotator believes that the text was generated by a human or language model. Linguistic Match (Dialect) measures how well the language of the counterpart is consistent with the intended English variety (i.e., AAL or WME). Meaning Preservation (Meaning) measures how accurately the counterpart conveys the meaning of the original text. And finally, Tone Preservation (Tone) measures how accurately the counterpart conveys the tone or other aspects beyond meaning of the original text. Additional details on the judgment instructions are included in Appendix C.2.\nIn the masked span prediction task, span predictions are evaluated using automated metrics: model perplexity of the reference span, and the entropy of the model\u2019s top 5 most probable spans. With the exception of GPT-3, experiments are repeated 5 times, randomly sampling spans to mask in each trial. Metrics are reported as the percent change in perplexity between WME and AAL."
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 Are AAL and WME Metrics Comparable?",
            "text": "Studies such as Bugliarello et al. (2020) might suggest that in translation-like tasks, it is invalid to compare results from automatic metrics, such as BLEU, cross-lingually because: (1) different languages may use different numbers of words to convey the same meaning, and (2) models for different languages utilize different tokenization schemes.\nThough we emphasize that AAL and WME are language varieties of English rather than distinct languages, a similar argument may be made that their Rouge scores are not directly comparable. However, the counterpart generation task setting does not suffer from either of the aforementioned weaknesses. To show this, we calculate differences in the number of words and 1-gram Type-Token Ratio for AAL and WME text pairs in our dataset.\nAs shown in Table 3, the total number of words in the AAL and WME texts are similar, and we find that the lengths of each pair of texts differ by less than 1/10th of a word (0.095) on average. Bugliarello et al. (2020) also finds that among metrics studied, translation difficulty is most correlated with the Type-Token Ratio (TTR) of the target language. Table 3 shows that the difference in the 1-gram TTR between AAL and WME is not statistically significant. Finally, as the same models are applied to both AAL and WME texts, the tokenization schemes are also identical. Therefore, the identified weaknesses of cross-lingual comparison do not apply to our results."
        },
        {
            "heading": "5.2 Counterpart Generation",
            "text": "Figure 2 shows results using automatic coverage metrics on counterpart generations in AAL and WME. Rouge-1, Rouge-L and BERTScore (the coverage scores) for model output are computed over the generated AAL or WME in comparison to the corresponding gold standards. We note that the\nmodels consistently perform better when generating WME, indicating that it is harder for models to reproduce similar content and wording as the gold standard when generating AAL. ChatGPT is the worst model for producing WME from AAL, and ChatGPT and GPT-3 are nearly equally bad at producing AAL from WME. Flan-T5 (FT) does best for both language varieties, likely due to the fact that Flan-T5 (FT) was directly fine-tuned for the task. Flan-T5 without fine-tuning performs comparatively with slightly lower coverage scores in both directions. We also compute the coverage scores between the original AAL text from the dataset and the human annotated counterparts in WME, labeled as \"Human\" in Figure 2. Models tend to generate counterparts with lower coverage scores than the text input to the model, which reflects the alternative language variety. This suggests that it is difficult for models to generate counterparts in either direction.\nFigure 3 shows human judgments of modelgenerated WME and model-generated AAL. With the exception of Flan-T5 (FT), we see that modelgenerated WME is judged as more human-like and closer to the intended language variety than model-generated AAL. These results confirm findings from automatic metrics, showing that models more easily generate WME than AAL. In contrast, for meaning and tone, the reverse is true, indicating that models generate WME that does not match the meaning of the original AAL. The difference between scores on AAL and WME were significant on all metrics for at least two of the models as determined by a two-tailed t-test of the means (see * in Figure 3 for models with significant differences). We see also that the drop in meaning and tone scores from judgments on human WME is larger than the drop in human-like and dialect scores on WME. These observations suggest that models have a hard time interpreting AAL.\nToxicity scores (Figure 4) show that models tend to remove toxic words when generating both AAL and WME10. To test whether removal of toxic words contributed to the inability to preserve meaning, we computed both coverage scores and human evaluation scores on two subsets of the data: \"toxic\" texts (at least one term categorized as offensive) and \"non-toxic\" texts (no terms categorized as offensive). We display these results as the difference in coverage scores and in human judgment scores between model generated AAL and WME as shown in Figure 5 and Figure 6. Positive scores indicate that AAL performs better. Here we see that human judgments on meaning and tone show that generated WME is worse than generated AAL for both toxic and non-toxic subsets. Thus, differences in use of toxic words between input and output cannot be the sole cause for lower scores on meaning and tone. This confirms that models have difficulty interpreting features of AAL. We note furthermore\n10Models are developed to avoid generating toxic or offensive language, so the trend of neutralizing input texts in any dialect is expected. There are notable differences, however, in the extent to which this neutralization occurs. The results show that a significantly higher proportion of toxic language is removed when generating WME from AAL than in the reverse direction.\nthat gaps in coverage are consistently larger for the non-toxic subsets of the data, demonstrating that the use of profanity and toxic language are also not the primary cause of gaps in coverage metrics."
        },
        {
            "heading": "5.3 Masked Span Prediction",
            "text": "For MSP, we measure both perplexity and the entropy of generating a masked span in either AAL or WME. A lower perplexity score indicates it is easier for the model to determine the missing phrase, while a lower entropy score indicates the model places high probability in its top predictions. Figure 7 shows the differences in perplexity and entropy between masked span prediction for modelgenerated WME and for model-generated AAL.\nNegative percent changes in perplexity indicate that it is easier for models to predict spans in WME than AAL, while for entropy, indicate that models place higher probability in their top predictions for WME than for AAL sentences.\n6 Discussion: How well do models interpret AAL?\nWe discussed earlier how Figure 3 and Figure 6 demonstrate that models have difficulty interpreting AAL when generating WME. Figure 7 supports this finding as well, as models generally output higher perplexities for masked spans in AAL compared to aligned WME.\nThe largest gaps in perplexity between the two language varieties is assigned to masked verb phrases. One set of distinct features characterizing AAL are verbal aspects which do not occur in WME such as the future gone (e.g., I\u2019m gone do it later), so this result may suggest that models struggle with the use of AAL-specific aspects in particular over other AAL features. A similar trend is found in the entropy metric, suggesting that AAL\ntext also lowers model confidence in their own predictions. These results for AAL support similar findings by Groenwold et al. (2020) for GPT-2 in an auto-completion setting.\nManual inspection revealed more fine-grained patterns of model behavior within aspectual verbs and other AAL features. Models seem to correctly interpret some specific features of AAL, namely: the use of ain\u2019t, double negation, and habitual be.\nExamples of misinterpretation, however, are shown in Table 4 illustrating difficulty with several other aspects of AAL. Several mistakes involve lexical interpretation, such as in example 1, where the model is not able to interpret the meaning of \"he faded\" as \"he\u2019s high\" and example 2 where the model inserts shorty apparently intending the mean-\ning \"youth\" instead of its more common meaning of \"girlfriend\". The models also struggle with features that appear the same as in WME, but have slightly different meanings in AAL. These include remote past been (example 2), which is incorrectly interpreted as past perfect (have been), and existential it (example 3), which in WME is closest in meaning to \"there\" as in \"there are ...\" and is not correctly interpreted by any model.\nWe also include an example where GPT-4 misinterprets the phrase \"a nigga\" as referencing another person, when in the provided context, the use most closely resembles referencing oneself. The word nigga, is one of the N-words, a set of lexical items well-documented by linguists as being misunderstood by non-native speaker in terms of their syntactic and semantic complexity (Rahman, 2012; Grieser, 2019; Smith, 2019). In particular, while the model removes the word in generating the WME counterpart, it does not correctly understand the use of the N-word as referencing the subject. Without this understanding, it is probable that models will both misinterpret the words as toxic and use them in ways that are considered offensive to the AAL-speaking community.\nIn additional analysis of counterpart generations, we examined model performance gaps in each subset of the AAL dataset. Among subsets of the data, gaps between Rouge-1 metrics for AAL and WME counterparts vary significantly. GPT-4, for example, presents the largest performance gap for the TwitterAAE corpus (Blodgett and O\u2019Connor, 2017), and the smallest gaps for the hip-hop and focus group subsets as shown in Figure 8. Manual inspection reveals that this aligns with the trends in AAL-use among the subsets as well: distinct features of AAL appear to be more frequent in the TwitterAAE dataset, while AAL features in the focus group transcripts appear to be more sparse. This pattern may be due to the makeup and context of the focus groups, as most participants were college-educated, working professionals and selected to specifically describe grief experiences, possibly affecting the use of AAL in the discussions. These results may suggest, as would be expected, that higher density of AAL features leads to larger performance gaps."
        },
        {
            "heading": "7 Related Work",
            "text": "While few have specifically focused on bias against AAL in language generation, related work has ex-\ntensively investigated societal biases in language tasks. One large-scale study (Ziems et al., 2022) investigates performance on the standard GLUE benchmark (Wang et al., 2018) using a synthetically constructed AAL dataset of GLUE for fine-tuning. They show performance drops on a small humanwritten AAL test set unless the Roberta model is fine-tuned.\nRacial Bias in Generation. Mitigating and evaluating social biases in language generation models is a challenging problem due to the apparent tradeoffs between task performance and bias mitigation, the many possible sources of bias, and the variety of biases and perspectives to examine (Sheng et al., 2021b; Aky\u00fcrek et al., 2022). A number of studies have proposed bias evaluation measures, often using prompts crafted to reveal biased associations of, for example, occupation and gender (i.e., \"The [man/woman] worked as a ...\") (Sheng et al., 2020, 2019; Kiritchenko and Mohammad, 2018; Dhamala et al., 2021; Shen et al., 2022) and in other cases, graph representations to detect subjective bias in summarization (Li et al., 2021) and personas for dialogue generation (Sheng et al., 2021a). However, the bias measurements in many of these approaches are not directly applicable to language in a natural setting, where the real-life harmful impacts of bias in language generation would be more prevalent.\nAAL Feature Extraction. Past work makes progress in lowering performance gaps between AAL and WME by focusing on linguistic feature extraction tasks. Given that some features of AAL such as the aspectual verbs (i.e., habitual be, remote past been) do not have equivalent meanings and functions in WME (Green, 2009), standard partof-speech (POS) taggers and dependency parsers cannot maintain performance for AAL text. Studies have attempted to lessen this gap by creating a POS tagger specifically for AAL through domain adaptation (J\u00f8rgensen et al., 2016) and a dependency parser for AAL in Tweets (Blodgett et al., 2018). Beyond these tasks, considerable attention has been given to developing tools for features specific to AAL and other language varieties, such as detecting dialect-specific constructions (Masis et al., 2022; Demszky et al., 2021; Santiago et al., 2022; Johnson et al., 2022) to aid in bias mitigation strategies.\nAAL in Language Tasks. Bias has also been measured specifically with respect to AAL in down-\nstream, user-facing tasks. With the phonological differences between AAL and WME, automatic speech recognition (ASR) systems have shown large performance drops when transcribing speech from African American speakers (Koenecke et al., 2020; Martin and Tang, 2020; Mengesha et al., 2021). Toxicity detection and offensive language classification models have also been evaluated and have shown a higher probability of incorrectly labeling AAL text as toxic or offensive when compared to WME text (Zhou et al., 2021; Rios, 2020; Sap et al., 2022). Most closely related to this work, one study evaluated bias against AAL in transformer generation models, showing that in a sentence auto-completion setting, GPT-2 generates AAL text with more negative sentiment than in aligned WME texts (Groenwold et al., 2020). Further investigation of both a larger set of language generation models as well as a broader set of generation tasks would provide a clearer picture of model biases against AAL."
        },
        {
            "heading": "8 Conclusion",
            "text": "We demonstrate through investigation of two tasks, counterpart generation and masked span prediction, that current LLMs have difficulty both generating and interpreting AAL. Our results show that LLMs do better matching the wording of gold standard references when generating WME than when generating AAL, as measured by Rouge and BERTScore. Human evaluation shows that LLM output is more likely to be judged as human-like and to match the input dialect when generating WME than AAL. Notably, however, LLMs show difficulty in generating WME that matches the meaning and tone of the gold standard, indicating difficulty in interpreting AAL. Our results suggest that more work is needed in order to develop LLMs that can appropriately interact with and understand AAL speakers, a capability that is important as LLMs are increasingly deployed in socially impactful contexts (e.g., medical, crisis)."
        },
        {
            "heading": "Limitations",
            "text": "We acknowledge a few limitations accompanying the evaluation of biases in LLMs. While our analysis is primarily restricted to intrinsic evaluation of model biases, users primarily interact with LLMs in a chat-based interface such as with ChatGPT, or use the model for specific tasks such as question answering. This approach was chosen to analyze\nbiases that would be present across all tasks involving AAL. Performance gaps and biases analyzed in a task-specific setting, however, may yield different trends than presented in this paper, and we leave this investigation to future work.\nAdditionally, AAL exhibits significant variation by region, context, speaker characteristics, and many other variables. We attempt to more comprehensively reflect real AAL use by drawing text from multiple sources and contexts, but are ultimately limited by the data available. For example, while CORAAL reflects natural AAL speech, it is limited to a select set of regions (e.g., New York, Georgia, North Carolina, Washington DC), and while the Twitter and Reddit AAL subsets may span many regions, they are also influenced by the linguistic features of social media. Similar biases may also exist in other underrepresented varieties of English such as Mexican American English, Indian English, or Appalachian English. Due to the availability of data, we focus on AAL, but given texts in other varieties, this work could be extended to examine biases regarding these and other language varieties.\nFinally, evaluation metrics relying on trained models or lexicons, such as BERTScore and toxicity measures, may also inherently encode biases concerning AAL text. Rather than using a model to measure toxicity, we instead use a lexicon of offensive terms provided in Zhou et al. (2021) and used to measure lexical biases in toxicity models. Given that analyzing performance gaps relies on accurate and unbiased measures of model performance, future work may give attention to developing unbiased language generation metrics."
        },
        {
            "heading": "Ethics Statement",
            "text": "We recognize that identifying potential bias against AAL in LLMs should also include a critically reflexive analysis of the consequences if language models are better at understanding language varieties specific to marginalized communities such as AAL, and the extent to which that impacts those speakers. In prior research, Patton et al. (2020) have noted that decisions made by researchers engaged in qualitative analysis of data through language processing should understand the context of the data and how algorithmic systems will transform behavior for individual, community, and system-level audiences. Critical Race Theory posits that racism exists across language\npractices and interactions (Delgado and Stefancic, 2023). Without these considerations, LLMs capable of understanding AAL could inadvertently be harmful in contexts where African Americans continue to be surveilled (e.g., social media analysis for policing).\nDespite this, including African American representation in language models could potentially benefit AAL speakers in socially impactful areas, such as mental health and healthcare (e.g., patient notes that fully present the pain African American patients are experiencing in the emergency room, Booker et al. 2015). Considering both the potential for misuse of the data as well as the potential for social good, we will make the Tweet IDs and other collected data available to those that have signed an MOU indicating that they will use the data for research purposes only, limiting research to improved interpretation of AAL in the context of an application for social good. In the MOU, applicants must include their intended use of the data and sign an ethics agreement."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by grant IIS2106666 from the National Science Foundation, National Science Foundation Graduate Research Fellowship DGE-2036197, the Columbia University Provost Diversity Fellowship, and the Columbia School of Engineering and Applied Sciences Presidential Fellowship. Any opinion, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. We thank the anonymous reviewers and the following people for providing feedback on an earlier draft: Tuhin Chakrabarty, Esin Durmus, Fei-Tzin Lee, Smaranda Muresan, and Melanie Subbiah. We also thank Mayowa Fageyinbo, Gideon Kortenhoven, Kendall Lowe, and Tajh Martin for providing annotations."
        },
        {
            "heading": "A Data Statement",
            "text": "We provide details about our dataset in the following data statement. Much of the dataset is drawn from existing datasets that lack data statements, and in those cases, we include what information we can."
        },
        {
            "heading": "A.1 Curation Rationale",
            "text": "The dataset was collected in order to study the robustness of LLMs to features of AAL. The data is composed of AAL-usage in a variety of regions and contexts to capture the variation in the use of and density of features. In order to better ensure the included texts reflect AAL, we sample texts\nfrom social media, sociolinguistic interviews, focus groups, and hip-hop lyrics and weight the probability of sampling a text using a small set of known AAL morphosyntactic features. The datasets that were previously collected, CORAAL Kendall and Farrington 2021 and TwitterAAE (Blodgett et al., 2016), were originally created to study AAL and to study variation in AAL on social media respectively. For all texts in the dataset, we also collect human-annotated counterparts in WME to provide a baseline for model evaluations."
        },
        {
            "heading": "A.2 Language Variety",
            "text": "All texts included in the dataset are in English (enUS) as spoken or written by African Americans in the United States with a majority of texts reflecting linguistic features of AAL. Some texts notably contain no features of AAL and reflect WME."
        },
        {
            "heading": "A.3 Speaker Demographics",
            "text": "Most speakers included in the dataset are African American. The r/BPT texts were restricted to users who have been verified as African American, CORAAL and focus group transcripts were originally interviews with African Americans, and hip-hop lyrics were restricted to African American artists. The TwitterAAE dataset is not guaranteed to be entirely African American speakers, but the texts are primarily aligned with AAL and have a high probability of being produced by AAL speakers. Other demographics such as age and gender are unknown."
        },
        {
            "heading": "A.4 Annotator Demographics",
            "text": "While all AAL texts in the dataset reflect natural usage of AAL, the WME counterparts in the dataset are annotated. We recruited 4 human annotators to generate WME counterparts for each text. All annotators self-identify as African American, self identify as AAL speakers, and are native English speakers. Additionally, the 4 annotators are undergraduate and graduate students aged 20-28, 2 of whom were graduate students in sociolinguistics. All annotators were compensated at a rate between $18 and $27 per hour depending the annotator\u2019s university and whether they were an undergraduate or graduate student."
        },
        {
            "heading": "A.5 Speech Situation",
            "text": "Speech situations vary among the 6 datasets we compose. The r/BPT posts, r/BPT comments,\nand TwitterAAE subsets are all originally typewritten text, intended for a broad audience, and are drawn from asynchronous online interactions. The CORAAL and focus group transcript subsets are originally spoken and later transcribed, intended for others in their respective conversations, and are drawn from synchronous in-person interactions. Finally, the hip-hop lyrics subset are both spoken and written, intended for a broad audience of hiphop listeners, and are likely repeatedly changed and edited before released. r/BPT comments and posts are sampled from the origin of the subreddit in October 2015, CORAAL transcripts are sampled from interviews between 1888 and 2005, hip-hop lyrics are drawn from songs released in 2022, focus groups were conducted between February and November 2022, and the time range of the TwitterAAE dataset is unknown to the authors."
        },
        {
            "heading": "A.6 Text Characteristics",
            "text": "Among the data subsets, the focus group transcripts are the most topically focused. All focus groups primarily included discussion surrounding the experiences and responses to grief in the Harlem community, focusing on experiences due to daily stressors, the death of loved ones, police shootings, and the COVID-19 pandemic. In the r/BPT posts and r/BPT comments subsets, texts were typically written in response to a tweet by an African American Twitter user, ranging from political commentary to discussion of the experience of African Americans in the United States. The hip-hop lyrics subset is not topically focused, but includes texts that follow specific rhyming patterns and meters. The remaining subsets of the data (TwitterAAE, CORAAL) span a variety of topics and structures."
        },
        {
            "heading": "B AAL Search Patterns",
            "text": "To better ensure our dataset includes use of AAL features, we use a set of regex and grammar-based search patterns as part of the sampling procedure. Regex patterns for AAL features are listed below.\nThe set also includes grammar-based patterns using the spacy POS tagger to detect the use of habitual be, completive done (or \"dun\", \"dne\"), future\ngone (or \"gne\", \"gon\"), and remote past been (or \"bin\"). Each of these features are detected by the use of each term (or their variants) if they are not preceded by another auxiliary verb or preposition in the clause (i.e., \"He be eating\" contains a use of habitual be, but \"Should he be eating?\" does not because the auxiliary verb \"should\" precedes \"be\" in the clause). While standard POS taggers could potentially underperform on AAL, there were no AAL-specific POS taggers available at the time of dataset collection to our knowledge."
        },
        {
            "heading": "C Annotation Procedure",
            "text": ""
        },
        {
            "heading": "C.1 Counterpart Annotations",
            "text": "Annotators were asked to provide a semanticallyequivalent rewriting (or counterpart) of a given text from the AAL dataset in WME. The specific set of guidelines provided to annotators were:\n1. Change alternative spellings (i.e., \"shoulda\" for \"should\u2019ve\") 2. Maintain usernames, hashtags, and URLs if present 3. Ignore emojis unless speakers of WME may use them differently 4. Conserve and un-censor profanity 5. Avoid unnecessary changes 6. Use your best judgement in special cases\nAs noted, annotators had the option to label a text as \"Not Interpretable\" if it lacks a reasonable counterpart in WME."
        },
        {
            "heading": "C.2 Counterpart Judgment Instructions",
            "text": "In judging counterparts, annotators were provided with an original text from the dataset in either WME or AAL and a model-generated (or humanannotated) counterpart. Notably, judgments were assigned ensuring that no annotator received a text they were involved in creating the counterpart for. Additionally, annotators were not given definitions or guidelines for terms such as \"meaning\" or \"tone\" to avoid biasing judgements and to encourage annotators to use their own interpretation of the terms. The questions asked of annotators are as follows:\n1. Human-likeness: Is the interpretation more likely generated by a human or language model? (1) Very Likely a Model, (2) Likely a Model, (3) Neutral, (4) Likely a Human, (5) Very Likely a Human\n2. Linguistic Match: How well does the interpretation resemble AAL? (1) Completely Unlike AAL, (2) Unlike AAL, (3) Neutral, (4) Like AAL, (5) Completely Like AAL 3. Meaning Preservation: How well does the interpretation reflect the meaning of the original text? (1) Completely innacurate, (2) Inaccurate, (3) Neutral, (4) Accurate, (5) Completely Accurate 4. Tone Preservation: How well does the counterpart accurately reflect the tone of the original text? (1) Completely innacurate, (2) Inaccurate, (3) Neutral, (4) Accurate, (5) Completely Accurate\nFor judgment samples that involved judging WME counterparts, questions and response options that refer to \"AAL\" were changed accordingly."
        },
        {
            "heading": "D Annotator Agreement Calculation",
            "text": "Because the task provided to annotators required generating text, we use Levenshtein distance and Krippendorf\u2019s Alpha to calculate annotator agreement based on the general form as described in Braylan et al. 2022. Annotator agreement is calculated with the following formula:\n\u03b1 = 1\u2212 D\u0302o D\u0302e\n(1)\nwhere D\u0302o represents the observed distance between annotations, D\u0302e represents the expected distance, and Levenshtein distance is used as the distance function D(a, b). Expected distance between two annotators is calculated by randomly shuffling one set of annotations and calculating the average Levenshtein distance between the randomized pairs."
        },
        {
            "heading": "E Model and Experiment Details",
            "text": ""
        },
        {
            "heading": "E.1 Checkpoints and Training",
            "text": "For GPT-3, ChatGPT, and GPT-4, we use the textdavinci-003, gpt-3.5-turbo, and gpt-4 checkpoints respectively . For the T5 model variants, we use the t5-large and google/flan-t5-large checkpoint. FlanT5 is fine-tuned using a learning rate of 3e-5 for 5 epochs across the full set of 72 additional texts. Finally, for BART we use the facebook/bart-large checkpoint."
        },
        {
            "heading": "E.2 Generation Hyperparameters",
            "text": "For all GPT-family models, we use the default temperature of 0.7 in generations. For the BART and T5 model variants, we use a beam width of 3, temperature of 1, and a no_repeat_ngram_size of 3."
        },
        {
            "heading": "F Full Counterpart Results",
            "text": "Table 6 and Table 7 show the raw automatic metric and human judgement scores for the counterpart generation task respectively. Table 8 shows the percentage of toxic terms removed by the model when generating counterparts in AAL or WME. Finally, Table 9 and Table 10 present the raw automatic metric and human judgments scores on the toxic (at least one term categorized as offensive) and nontoxic (no terms categorized as offensive) subsets of the corpus."
        },
        {
            "heading": "G Full MSP Results",
            "text": "Table 11 presents the raw perplexity and entropy scores in the Masked Span Prediction task."
        },
        {
            "heading": "H Additional Counterpart Generation Examples",
            "text": "The remaining appendices, Tables 12-19, provide further examples from the counterpart generation task. Examples are drawn randomly from subsets where the total score given to one of the models evaluated exceeds the ratings of the original annotated counterpart and where the total score of a model is lower."
        }
    ],
    "title": "Evaluation of African American Language Bias in Natural Language Generation",
    "year": 2023
}