{
    "abstractText": "Understanding sentence meanings and updating information states appropriately across time\u2014what we call \u201csituational understanding\u201d (SU)\u2014is a critical ability for human-like AI agents. SU is essential in particular for chat models, such as ChatGPT, to enable consistent, coherent, and effective dialogue between humans and AI. Previous works have identified certain SU limitations in non-chatbot Large Language models (LLMs), but the extent and causes of these limitations are not well understood, and capabilities of current chat-based models in this domain have not been explored. In this work we tackle these questions, proposing a novel synthetic environment for SU testing which allows us to do controlled and systematic testing of SU in chat-oriented models, through assessment of models\u2019 ability to track and enumerate environment states. Our environment also allows for close analysis of dynamics of model performance, to better understand underlying causes for performance patterns. We apply our test to ChatGPT, the stateof-the-art chatbot, and find that despite the fundamental simplicity of the task, the model\u2019s performance reflects an inability to retain correct environment states across time. Our follow-up analyses suggest that performance degradation is largely because ChatGPT has non-persistent in-context memory (although it can access the full dialogue history) and it is susceptible to hallucinated updates\u2014including updates that artificially inflate accuracies. Our findings suggest overall that ChatGPT is not currently equipped for robust tracking of situation states, and that trust in the impressive dialogue performance of ChatGPT comes with risks. We release the codebase for reproducing our test environment, as well as all prompts and API responses from ChatGPT, at https://github.com/ yangalan123/SituationalTesting.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chenghao Yang"
        },
        {
            "affiliations": [],
            "name": "Allyson Ettinger"
        }
    ],
    "id": "SP:3a507a3e11a18f08d474aae22acc52951601ad6a",
    "references": [
        {
            "authors": [
                "Jacob Andreas."
            ],
            "title": "Language models as agent models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5769\u20135779, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models",
            "venue": "ArXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Matthew Hausknecht",
                "Layla El Asri",
                "Mahmoud Adada"
            ],
            "title": "Textworld: A learning environment for text-based games",
            "venue": "In Computer Games: 7th Workshop, CGW 2018,",
            "year": 2019
        },
        {
            "authors": [
                "I. Heim",
                "A. Kratzer."
            ],
            "title": "Semantics in Generative Grammar",
            "venue": "Blackwell Textbooks in Linguistics. Wiley.",
            "year": 1998
        },
        {
            "authors": [
                "Matthew Henderson",
                "Blaise Thomson",
                "Jason D. Williams."
            ],
            "title": "The second dialog state tracking challenge",
            "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 263\u2013272, Philadelphia,",
            "year": 2014
        },
        {
            "authors": [
                "Md Mosharaf Hossain",
                "Antonios Anastasopoulos",
                "Eduardo Blanco",
                "Alexis Palmer."
            ],
            "title": "It\u2019s not a non-issue: Negation as a source of error in machine translation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Hans Kamp",
                "Josef Van Genabith",
                "Uwe Reyle."
            ],
            "title": "Discourse representation theory",
            "venue": "Handbook of Philosophical Logic: Volume 15, pages 125\u2013394.",
            "year": 2011
        },
        {
            "authors": [
                "Lauri Karttunen."
            ],
            "title": "Discourse referents",
            "venue": "International Conference on Computational Linguistics COLING 1969: Preprint No. 69: Collection of Abstracts of Papers, S\u00e5nga S\u00e4by, Sweden.",
            "year": 1969
        },
        {
            "authors": [
                "Najoung Kim",
                "Sebastian Schuster."
            ],
            "title": "Entity tracking in language models",
            "venue": "Proc. of ACL, Location of the conference. Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Henry Ku\u010dera",
                "Winthrop Nelson Francis."
            ],
            "title": "Computational analysis of present-day American English",
            "venue": "Brown university press.",
            "year": 1967
        },
        {
            "authors": [
                "David Lewis."
            ],
            "title": "General semantics",
            "venue": "Montague grammar, pages 1\u201350. Elsevier.",
            "year": 1976
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Belinda Z. Li",
                "Maxwell Nye",
                "Jacob Andreas."
            ],
            "title": "Implicit representations of meaning in neural language models",
            "venue": "Proc. of ACL, pages 1813\u20131827, Online. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Belinda Z Li",
                "Maxwell Nye",
                "Jacob Andreas."
            ],
            "title": "Language modeling with latent situations",
            "venue": "ArXiv preprint, abs/2212.10012.",
            "year": 2022
        },
        {
            "authors": [
                "Percy Liang",
                "Rishi Bommasani",
                "Tony Lee",
                "Dimitris Tsipras",
                "Dilara Soylu",
                "Michihiro Yasunaga",
                "Yian Zhang",
                "Deepak Narayanan",
                "Yuhuai Wu",
                "Ananya Kumar"
            ],
            "title": "Holistic evaluation of language models",
            "venue": "ArXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Reginald Long",
                "Panupong Pasupat",
                "Percy Liang."
            ],
            "title": "Simpler context-dependent logical forms via model projections",
            "venue": "Proc. of ACL, pages 1456\u2013 1465, Berlin, Germany. Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Tom McCoy",
                "Ellie Pavlick",
                "Tal Linzen."
            ],
            "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "venue": "Proc. of ACL, pages 3428\u20133448, Florence, Italy. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Introducing ChatGPT",
            "venue": "Accessed: 202304-24.",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "ArXiv preprint, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Abhilasha Ravichander",
                "Matt Gardner",
                "Ana Marasovic."
            ],
            "title": "CONDAQA: A contrastive reading comprehension dataset for reasoning about negation",
            "venue": "Proc. of EMNLP, pages 8729\u20138755, Abu Dhabi, United Arab Emirates. Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Tongshuang Wu",
                "Carlos Guestrin",
                "Sameer Singh."
            ],
            "title": "Beyond accuracy: Behavioral testing of NLP models with CheckList",
            "venue": "Proc. of ACL, pages 4902\u20134912, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Freda Shi",
                "Xinyun Chen",
                "Kanishka Misra",
                "Nathan Scales",
                "David Dohan",
                "Ed H Chi",
                "Nathanael Sch\u00e4rli",
                "Denny Zhou."
            ],
            "title": "Large language models can be easily distracted by irrelevant context",
            "venue": "International Conference on Machine Learning, pages",
            "year": 2023
        },
        {
            "authors": [
                "Kurt Shuster",
                "Jing Xu",
                "Mojtaba Komeili",
                "Da Ju",
                "Eric Michael Smith",
                "Stephen Roller",
                "Megan Ung",
                "Moya Chen",
                "Kushal Arora",
                "Joshua Lane"
            ],
            "title": "Blenderbot 3: A deployed conversational agent that continually learns to responsibly engage",
            "year": 2022
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Yikang Shen",
                "Qinhong Zhou",
                "Hongxin Zhang",
                "Zhenfang Chen",
                "David Cox",
                "Yiming Yang",
                "Chuang Gan."
            ],
            "title": "Principle-driven selfalignment of language models from scratch with minimal human supervision",
            "venue": "ArXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Mirac Suzgun",
                "Nathan Scales",
                "Nathanael Sch\u00e4rli",
                "Sebastian Gehrmann",
                "Yi Tay",
                "Hyung Won Chung",
                "Aakanksha Chowdhery",
                "Quoc V Le",
                "Ed H Chi",
                "Denny Zhou"
            ],
            "title": "Challenging BIG-Bench tasks and whether chain-of-thought",
            "year": 2022
        },
        {
            "authors": [
                "Shubham Toshniwal",
                "Sam Wiseman",
                "Karen Livescu",
                "Kevin Gimpel."
            ],
            "title": "Chess as a testbed for language model state tracking",
            "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI, 2022, Thirty-Fourth Conference on Innovative Applications",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Frank Veltman."
            ],
            "title": "Defaults in update semantics",
            "venue": "Journal of philosophical logic, 25(3):221\u2013261.",
            "year": 1996
        },
        {
            "authors": [
                "Jason Weston",
                "Antoine Bordes",
                "Sumit Chopra",
                "Tom\u00e1s Mikolov."
            ],
            "title": "Towards AI-Complete question answering: A set of prerequisite toy tasks",
            "venue": "Proc. of ICLR.",
            "year": 2016
        },
        {
            "authors": [
                "Jason D Williams",
                "Matthew Henderson",
                "Antoine Raux",
                "Blaise Thomson",
                "Alan Black",
                "Deepak Ramachandran."
            ],
            "title": "The dialog state tracking challenge series",
            "venue": "AI Magazine, 35(4):121\u2013124.",
            "year": 2014
        },
        {
            "authors": [
                "Chien-Sheng Wu",
                "Andrea Madotto",
                "Ehsan HosseiniAsl",
                "Caiming Xiong",
                "Richard Socher",
                "Pascale Fung."
            ],
            "title": "Transferable multi-domain state generator for task-oriented dialogue systems",
            "venue": "Proc. of ACL, pages 808\u2013819, Florence, Italy. Association for",
            "year": 2019
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric. P Xing",
                "Hao Zhang",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "title": "Judging LLM-as-a-judge with MT-Bench and chatbot arena",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Understanding the meaning of language inputs and their impact on information states is essen-\ntial for building a communicative human-like AI agent (e.g., ChatGPT (OpenAI, 2022)). This capability requires an agent to know truth conditions (Lewis, 1976; Heim and Kratzer, 1998) of a given input, and how that input updates the context over time (Veltman, 1996). For instance, the natural language input \u201cOpen BOX4, obtain KEY-1\u201d in a game environment should assert updates \u201cOPENED(BOX-4) = True\u201d and \u201cOBTAINED(KEY-1) = True\u201d in the agent\u2019s representation of the environment state. We describe this ability as situational understanding (SU), as the agent needs to ground language in situations and understand situational changes.\nRecent research indicates that despite the tremendous success of Large Language Models (LLMs) (e.g., GPT-3 (Brown et al., 2020)), these models still fail to understand situational changes, and cannot serve as human-like agents to accomplish realworld tasks. For example, evidence suggests that models fail to detect sarcasm expressed in underlying context (Suzgun et al., 2022), infer incorrect logic states when context changes in games (Li et al., 2022) and fail to track entity state changes in a discourse (Kim and Schuster, 2023). While these works have shown important limitations in LLMs, it remains unclear why these models show these limitations, and there is less work that shows the extent to which these limitations persist in more recent, chat-trained models like ChatGPT.\nIn this work we seek to shed light on both of these questions. To test the situation tracking ability of ChatGPT and other chat models, we design a synthetic environment for controlled testing of models\u2019 ability to follow instructions and maintain consistent understanding of situation states. We design a multi-stage testing framework, and include various controls to avoid models relying on mappings memorized from pre-training rather than doing real-time state tracking. Our environment allows for flexible testing under different conditions,\nas well as controlled follow-up analyses to enable closer examination of causal factors.\nWe apply our tests primarily to ChatGPT, which we find to outperform other models fine-tuned on chats. Our results show that ChatGPT\u2019s performance reflects a failure to retain coherent and correct environment states across time, despite the simplicity of the task and the fact that ChatGPT has access to the full dialogue history in its input window. Our follow-up analyses suggest that a major contributor to this degradation is failure to retain prior states in memory, as well as susceptibility to spurious hallucinated updates (which can also contribute to accidentally correct outputs). Our findings suggest overall that ChatGPT lacks the ability to track situation state changes robustly."
        },
        {
            "heading": "2 Related work",
            "text": "Situational understanding ability is essential for building agent-like real-world intelligent systems. In comprehensive benchmarks, Big-Bench-Hard (BBH, Suzgun et al. (2022)) finds that for GPTfamily models and PaLM 540B models (Chowdhery et al., 2022), even equipped with the stateof-the-art Chain-of-Thought prompting, still fail on tasks that require situational information (e.g., to detect sarcasm). HELM (Liang et al., 2022) also points out LLMs can lack state tracking ability based on evaluation results for 30 models on bAbI (Weston et al., 2016) environment.\nIn synthetic and controlled environments, Li et al. (2021); Andreas (2022); Li et al. (2022) does probing on BART (Lewis et al., 2020), T5 (Raffel et al., 2020) and GPT-3 using the SCONE (Long et al., 2016) and TextGame (C\u00f4t\u00e9 et al., 2019) datasets, and show that these models lack the ability to track and infer information states. Kim and Schuster (2023) re-analyze the results in Li et al. (2021) and find that GPT-3/3.5 and Flan-T5 (Chung et al., 2022) cannot track entity state changes in a discourse. Toshniwal et al. (2022) instruct GPT2 (Radford et al., 2019) to play chess and find it difficult to track board states per move.\nWe build on these previous works in two primary ways. First, these works mainly use LLMs as feature extractors (e.g., train probe models over intermediate representations or final-layer representations), and none of them discuss whether situational understanding limitations still exist in the recent powerful and widely used ChatGPT system. We investigate here whether ChatGPT has the crit-\nical underlying situational understanding to generate coherent, consistent and effective dialogue (Karttunen, 1969; Kamp et al., 2011; Wu et al., 2019). Second, tests used in previous work are susceptible to interference from confounds present when fine-tuning linear probes, or shortcuts in the testing environment that can be utilized to bypass situational understanding tests (Kim and Schuster, 2023). We aim to address some of these concerns by introducing a number of additional controls in our tests. Finally, we also carry out a more in-depth exploration of model update dynamics, to better understand causes for patterns of performance."
        },
        {
            "heading": "3 Building a Situational Testing Environment",
            "text": "What we want to test is models\u2019 ability to process situational changes conveyed by language inputs, and to maintain internal representations of the corresponding situation states. To test this, we use a synthetic box-moving environment like TextGame (Li et al., 2021), where we have full access to underlying game states, but eliminate complicated reward pursuing and route branching. Having this kind of full-information synthetic environment is helpful to test models\u2019 evolving understanding of environment states as the input describes progressively more situational changes."
        },
        {
            "heading": "3.1 Environment Setup",
            "text": "The environment includes two basic components:\n1. Instructions. Instructions directed to an agent, defining the agent\u2019s quest and providing information about the environment. As we will describe below, this component also sometimes contains explanations of the meanings of non-language functors and arguments to be used in the output.\n2. Steps and queries. Descriptions of steps taken by the agent, followed by a query that prompts for enumeration of all relevant environment states after the action is taken. Queries and answers to the queries take the format of sets of logical representations of states with corresponding truth values.\nIn our tests, the input provided to the model includes the task instructions, along with n fewshot Step-Query-Answer examples to demonstrate the task. In the zero-th step, we define the step\nas \u201cDo nothing\u201d and provide in the Answer the full set of correct truth values as an initialization. The n few-shot Step-Query-Answer examples are then followed by a series of Steps without QueryAnswer components, followed by a Test Step and Test Query that the model is expected to complete with the full enumeration of environment states that hold after that step is taken. An example of our environment format, to be further explained below, is shown in Example 1.\n1 Instructions: As an agent, you need to find the way to go out of this quest.\nCurrently, there are several boxes in front of you and there is a key inside each box. You can use only one of these keys to open the door and finish this quest. There are 5 boxes and 5 keys here. Boxes are identified as jqC\u2212X and Keys are identified as bsS\u2212X. NvSWxzvJb(jqC\u22122)=True means that jqC\u22122 has been opened. B(bsS\u22123)=True means that bsS\u22123 has been obtained. NvSWxzvJb (jqC\u22122)=False means that jqC\u22122 has not been opened. B(bsS\u22123)=False means that bsS\u22123 has not been obtained.\n2 Step\u22120: Initialization. Do nothing. 3 Question: NvSWxzvJb(jqC\u22120)=?...B(bsS\u22120)=?B( bsS\u22121)=?... 4 Answer: NvSWxzvJb(jqC\u22120)=False...B(bsS\u22120)= False... 5 Step\u22121: Open jqC\u22123 and retrieve bsS\u22122. 6 Question: NvSWxzvJb(jqC\u22120)=?...\nExample 1: Running example of our test environment.\nFor the experiments below, we set the number of boxes and keys to 10. This corresponds to a total of 20 environment states (boxes and keys have separate states) to be enumerated after each step. We sample the number of steps randomly from a uniform distribution U(1, 10). We then keep instructions almost entirely the same across samples, except that in the Synthetic Language settings (see Section 3.2) the state predicates are defined randomly and therefore vary between samples (e.g., in one instance we use NvSWxzvJb(jqC-0)=True to refer to the box jqC-0 having been opened, while in another instance we use Abc(bb-0)=True to represent the box bb-0 having been opened)."
        },
        {
            "heading": "3.2 Robustness Checks for State Tracking",
            "text": "Synthetic language When testing models\u2019 ability to map to logical representations of environment states, a concern with using language-based logical symbols (such as OPENED(BOX-4)) is that the models may be able to leverage pre-training on similar language-based logical symbols, or simply copy from the input language describing the\nactions, without needing to convert to abstract situation states. To control for this possibility, in addition to using natural language (NL) functors and arguments for our environment states, we also adopt settings in which synthetic language (SL) is used to specify the functors and arguments of the environment states. This allows us to disentangle our target task from pattern memorization and copying capabilities, better ensuring that models must rely on a combination of the instructions and the changes caused by the actions taken. To build synthetic functors and arguments, we use randomly selected ASCII characters. The length for each functor or argument is a random sample from U(1, 10). In the instructions, we include explanations of the meanings of these synthetic functors and arguments. An example of our synthetic language setting is shown in Example 1 (example instructions from our NL setting can be seen in Example 2 and Example 3). To counteract randomness effects and reduce any bias from specific synthetic language, for each test case (an instance), we generate a different set of synthetic functors and arguments.\nCounterintuitive instructions To further control for potential memorization of mappings between language and logical states from pre-training, we include one additional manipulation involving what we call counterintuitive instruction. In counterintuitive instruction settings, the instructions define mappings that reverse the standard usage of logical statements and truth values. The two versions we use are counterintuitive output format (Example 2), and counterintuitive language instruction (Example 3). These manipulations draw on the tradition of negation testing (McCoy et al., 2019; Ribeiro et al., 2020; Hossain et al., 2020; Ravichander et al., 2022), and allow us to further disentangle pre-training memorization from understanding of our particular instructions.1"
        },
        {
            "heading": "3.3 Evaluation Metrics",
            "text": "We mainly evaluate models\u2019 success in our synthetic environment by measuring two metrics: State-EM, and Step-EM.\nState-EM is the proportion of all predicted 1There are other possible perturbations that we can do, such as adding a distractor at each step (Appendix A) or applying synthetic language only on functors / only on arguments (Appendix B). For simplicity, we omit the discussion of other perturbation types and only focus on the two robustness checks explained in this section."
        },
        {
            "heading": "8 Instructions: ... OPENED(BOX\u22123)=True",
            "text": "states that match the expected states. This metric is useful to check to what extent the model develops correct partial understanding in response to situational changes.\nState-EM = #(Matched States) #(Queried States)\n(1)\nStep-EM is a stricter metric than State-EM. It is the proportion of steps for which the full set of predicted states at that step have an exact match with the expected states, including all the truth values and the number of predicted states. This allows us to check whether the model can maintain consistent and coherent understanding over situational changes. This metric is also important given the sparsity of updates at each step, to ensure that models cannot achieve strong performance simply by copying previous states.\nStep-EM = { 1 if Matched States=Ground Truth States=Predicted States 0 otherwise\n(2) We illustrate the computation of these two metrics for the following simplified case (2 boxes and keys, synthetic language, no counterintuitive instructions):\n9 [Instructions and some previous steps] 10 Question: NvSWxzvJb(jqC\u22120)=? B(bsS\u22120)=? NvSWxzvJb(jqC\u22121)=? B(bsS\u22121)=? 11 Correct answer: NvSWxzvJb(jqC\u22120)=True, B(\nbsS\u22120)=False, NvSWxzvJb(jqC\u22121)=False, B (bsS\u22121)=False\n12 Model Answer: NvSWxzvJb(jqC\u22120)=True, B(bsS \u22120)=True, NvSWxzvJb(jqC\u22121)=False, B(bsS \u22121)=False\nExample 4: Example for Metrics Computation State-EM in this case is 3/4 = 75% (only 3 out of 4 states are correct), while Step-EM is 0 because the step contains a incorrect state.\nWhen computing these metrics, we find that at times the generated output does not strictly follow the given format from the in-context samples. Therefore, we use regular expressions to extract the\ntruth values and corresponding states from model outputs. Details can be found in Appendix D.\nExistence of Shortcuts It is clear that simple string automata plus a status tracking table should already be sufficient to solve every instance of our tasks. However, the simplicity of the task is part of its value: if models have a basic capacity to track and maintain environment states, this task should be straightforward. Nonetheless, we will see in the experiments below that ChatGPT still struggles to solve these tasks reliably, despite the existence of such simple solutions, indicating the presence of fundamental limitations in this class of capability."
        },
        {
            "heading": "3.4 Discussion: Synthetic Environment as Simulation for Real-World Application",
            "text": "At its core, our synthetic environment is a simplified simulation for real-world state-tracking tasks (usually in the form of slot-filling), a critical capability of dialogue systems / chatbots (Williams et al., 2014; Henderson et al., 2014; Wu et al., 2019). By prompting the model to update states, we are gradually giving the model more contextual information and testing whether ChatGPT, the state-of-the-art chatbot model, can closely follow users\u2019 prompts and keep track of the full interaction history.\nOur work has important potential implications as the usage of LLMs continues to proliferate. Instructing LLMs to remember many initial states, operate over synthetic languages, and keep track of interaction history can be seen as an important step in eventually teaching a highly-capable agent to follow social norms and policies. Our initial set of environment states is similar to establishing basic rules about dos-and-don\u2019ts at the beginning of human-AI conversations. The usage of synthetic languages is likewise similar to teaching AI agents about specific tones or styles of communication, terminologies, jargon, or perhaps even low-resource languages. Analyzing whether the model can keep\ntrack of environment states can allow us to draw conclusions about the model\u2019s ability to follow instructions or rules. In this sense, our work also has implications with respect to recent trends of Constitutional AI or Rule-based/Principle-driven models (Bai et al., 2022; OpenAI, 2023; Sun et al., 2023), in which human social rules (\u201cconstitution\u201d) are explicitly written at the beginning of a prompt to align AI agents\u2019 behavior to human values. Our initial environment states are like constitution policies that AI agents should obey. The steps and queries in our environments are reminiscent of a situation in which certain policies can be allowed to be updated with user permission. For example, initially an AI agent may be programmed to try its best to answer every question and disallow overly conservative responses like refuse-to-answer\u2014but under certain situations, the user could update with permission to the agent to refuse to answer for privacy, fairness or other social reasons.\nAs we will see in the experiments below, when more interactions occur, the model will gradually lose track of states, propagate errors, and even generate hallucinations, despite all updates falling within the input window. By design, a supercapable AI agent like ChatGPT should have the ability to read and use all information within the input window\u2014but our results suggest that this is not the case. Our research thus calls for further study, and for caution when implementing a Constitutional AI approach."
        },
        {
            "heading": "4 Experiments: testing model sensitivity to instructions and situation change",
            "text": "We use our environment to test ChatGPT in its ability to track situation changes, and we report the results below. We also compare against the performance of additional chat models, which we find to underperform ChatGPT. Those results can be found in Appendix E.\nWe try 2-shot, 3-shot and 5-shot settings (in which 2, 3 and 5 example steps with enumerated states are provided). We use 50 samples with the number of additional steps randomly sampled from {1, . . . , 10 \u2212 #(num-shots)} for each setting. We follow the official OpenAI cookbook2 to prepare the request and parse the response when interacting with ChatGPT API. Details are in Appendix C.\n2https://github.com/openai/ openai-cookbook/tree/main"
        },
        {
            "heading": "4.1 Results",
            "text": "Results for these tests on ChatGPT are shown in Table 1. We can see a number of patterns.\nFailures on Step-EM Under normal instructions, though the model achieves high State-EM (i.e., 90%), the Step-EM is generally much lower (up to 70% lower in 2-shot and 40 \u2212 50% lower in 5- shot). This indicates that although models are able to take advantage of the sparsity of the updates to get a majority of states correct, they are much less successful in accurately characterizing the entire environment state at a given step. As we will see in Section 5.2 State-EM may also be skewed by accidentally-correct states, and should in general be interpreted with some caution.\nChatGPT has limited capability to understand and follow counterintuitive instruction. From Table 1, we see that applying our counterintuitive instruction manipulation leads to significantly degraded performance. Especially on Step-EM, performance in most settings is 10 \u2212 20%, which is much worse than with normal instructions (> 20%, or even > 30% in most cases). This suggests that the model is indeed to some extent relying on standard language-to-logic mapping from pre-training, rather than fully understanding the instructions.\nDespite this inconsistency, the model still shows some success with the negation or the new rule with flipped truth values, as it still manages to achieve 70 \u2212 80% State-EM. Though the State-EM values should be taken with caution, these accuracies are substantially stronger than would be expected from random guessing (50%) or completely ignoring the rule (0%), suggesting some level of capability in making use of the counterintuitive instructions.\nEffects of synthetic language Use of synthetic language affects model performance, but not always in the predicted directions. Though performance is occasionally worse with synthetic language, it is more often better than with natural language. This suggests that the use of synthetic language may at times help the models to detach from unhelpful biases from pre-training, and rely more robustly on the in-context information. This follows the analysis presented in Liang et al. (2022).\nMore in-context samples do not necessarily help. We also see that even if we are already providing answers for approximately 50% of states (considering that we only have 10 boxes and 10 keys in the\nenvironment, and each step will change exactly two states permanently within the dialog), the model does not make improvement in most cases."
        },
        {
            "heading": "5 Analysis of model performance",
            "text": "In the previous section we tested the capacity of ChatGPT in tracking and enumerating states within our environment, and we found that the model showed clear limitations. In this section, we analyze model performance further, to better understand the source of these limitations."
        },
        {
            "heading": "5.1 Tracing Errors: State Tracking over Steps",
            "text": "To understand why ChatGPT shows relatively poor performance, a straightforward way is reusing the instances created in Section 4, but querying for the environment states after each intermediate step to see where the errors emerge. Specifically, rather than only querying after the final step, we make queries at all steps (excluding \u201cStep-0\u201d), including those within the in-context example window (for querying each step s in the in-context example window, in-context demonstrations of environment states are given only through step s \u2212 1). We evaluate State-EM and Step-EM at every step. We refer to this test as Intermediate State Probing.\nPotential confounder: state complexity When interpreting performance trajectory across increasing numbers of steps, a potential confounder is that performance may degrade simply because the set of environment states has become more complex, and not because there are too many steps of updates involved. To investigate this possibility, we also run a test in which we compress and skip k of the early steps, and initialize in the state that would have resulted from those steps. We then test the trajectory of model performance on the subsequent\nn steps. If performance after n steps in this setting is comparable to performance after k + n steps in the previous setting, this will suggest that the degradation is indeed due to the complexity of the environment states. If, however, performance after n steps in this setting is substantially better than performance at k + n steps in the prior analysis, this suggests that the degradation in performance is due to failure to handle the growing number of steps.\nThe experiment results for the Intermediate State Probing and Compressed Initialization Test are shown in Figure 2 and Table 2, respectively. From these results we make the following observations:\nDegraded performance over steps. We see in Figure 2 that with increasing number of situational changes, both State-EM and Step-EM degrade. This degradation is particularly true for Step-EM, which decreases dramatically as steps increase.\nState complexity does not explain the degradation. Additionally, we see in Table 2 that skipping steps and starting with more complex initialization leads to improved performance, indicating that the\ndegradation across steps is not attributable to state complexity alone.\nMore in-context demonstration mitigates degradation. In Section 4 we found that providing more in-context samples does not cause a significant improvement in the performance of the model at the final Test Query. With intermediate probing we gain a finer-grained picture of the impacts of number of in-context samples across steps. First, we see that when prompting with more incontext samples (5-shot vs 2-shot), there is less rapid degradation in model accuracy after the end of the in-context window. In the 2-shot (Figure 2a) case, at Step-3 (the second step preceded by a nondemonstration step), Step-EM quickly drops from 60% to 40%. By contrast, in 5-shot (Figure 2b), at Step-6, ChatGPT can still maintain a Step-EM value of 60%. This suggests that having more incontext samples does strengthen models\u2019 accuracy\nin tracking state changes\u2014but only temporarily or over a limited number of steps.\nThough we see that having more demonstrations can mitigate degradation after the demonstrations end, when we look within the in-context sample window itself, we see that on steps that directly follow in-context demonstrations (Step-1 for 2-shot, Step-1,2,3,4 for 5-shot), the model\u2019s performance does not monotonically increase in response to the accumulating demonstrations. A similar phenomenon is also discovered in other fewshot reasoning benchmark works (Suzgun et al., 2022; Liang et al., 2022), despite the fact that in traditional fine-tuning, usually, more training instances yield better generalization ability. This suggests that although adding more demonstrations can briefly mitigate loss of accuracy, it does not straightforwardly translate to gains in accuracy.\nInterim discussion These patterns of degradation over time occur, in both NL and SL settings, despite the fact that ChatGPT can read the full dialogue in its input window. This suggests that ChatGPT cannot effectively utilize the full information in its input window, and that claims about maximum input length capabilities (e.g., ChatGPT can model 4k tokens as introduced in the official announcement (OpenAI, 2022)) should be taken with a grain of salt."
        },
        {
            "heading": "5.2 Fine-grained analysis of update patterns",
            "text": "In the above section, we studied the trajectory of model performance as the number of steps increases, finding evidence that ChatGPT degrades in state tracking with increased number of steps. In this section, we do a finer-grained analysis of the update dynamics in these experiments, in order to examine more closely the causal factors leading to both erroneous and accurate predictions. For the purpose of this analysis, we define categories of state transitions, summarized in Figure 3. These categories allow us to analyze the relationships between states at the analyzed step and the corresponding prior state, both when updates should be made to those states and when they should not.\nThe experiment results are shown in Figure 4 (we only show 2-shot SL, for reasons of space).\nExamining first the patterns for states in which models make incorrect predictions, we see that the rise in errors is driven by states that should be untouched at that step. We see that as steps increase there are rapid increases in both Dirty Read (DR) transitions, where models retain a previous error, and Hallucinated Update (HU-IO) transitions, where models change a state from correct to incorrect despite there being no change to that state in the step description. These patterns indicate that the rise in errors over time can be attributed both to retention and propagation of errors from previous steps, but also to failures in retaining the memory of a previous step that should not change.\nExamining now the transitions associated with correct model predictions, we see that over time there is noteworthy decrease in Correct Update (CU) cases\u2014however, there is a much more dramatic decrease in Maintain Correctness (MC) cases, indicating that the model increasingly fails to retain the memory of previously correct states. Over time we also see, particularly in the case of counterintuitive instructions, a rise in Accidentally Correct (HU-AC) cases, in which the model switches from\nan incorrect state back to the correct state, despite the fact that no update to that state was described in the step. Both of these patterns are indicative of memory limitations and susceptibility to random noise in changes to enumerated states.\nThese results yield several conclusions:\nChatGPT has non-persistent in-context memory. A recurring observation above is that many of the model errors that increase over time can be attributed to limitations in retaining states in memory\u2014and in fact, some states marked as correct also reflect accidental correctness arising due to similar failures to retain prior states.\nStates can also be retained, but potentially by chance. In addition to memory retention failures, we also see propagation of errors between steps\u2014 which in theory is indicative of successful memory retention, by contrast to the retention failures cited above. However, considering the prevalence of hallucinated updates, and the limited options for state values, we can expect that at least some of these retained updates in fact occur by chance.\nCouterintuitive instruction exacerbates nonrobust behavior. As we mentioned above, the drop in correct CU updates is more dramatic\u2014 and the rise in spurious correct updates HU more substantial\u2014in the case of counterintuitive instructions. This suggests that the inability of the model to rely on memorized language-to-logic mappings generally reduces the model\u2019s ability to execute and maintain correct state updates."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose a novel synthetic testing environment for testing situational understanding capabilities, which we apply to test ChatGPT, the state-of-the-art chatbot. We instruct ChatGPT to process a series of sparse environment updates across time in a dialogue history. With careful environmental designs, we reduce the possibility of data contamination and other artifacts typically introduced by traditional probing methods. We find that despite the simplicity of our task, and even with ChatGPT having full access to the complete dialogue history within the input window, the model fails to retain coherent and correct environment states over time. Further analysis suggests that this failure is largely because ChatGPT does not have persistent in-context memory, and is susceptible to\nhallucinated updates. These findings indicate overall that ChatGPT does not have robust situational state tracking ability.\nOur proposed synthetic environment and the findings that it generates can have noteworthy realworld implications. First, it can diagnose the potential limitations of current chatbot systems in multi-round interactions. Second, our findings also reflect a potential problem for the model\u2019s ability to follow instructions and remain consistent with rules/norms established in its context, which is especially important for responsible AI safety and human-AI alignment research."
        },
        {
            "heading": "Acknowledgements",
            "text": "We are grateful for the insightful discussion with Chih-chan Tien (UChicago), Kanishika Misra (UT Austin), Hao Zhu (CMU) and Zhaofeng Wu (MIT) at the early stage of this work (names are not listed in particular order). We also thank the anonymous EMNLP reviewers and chairs for providing insightful and constructive feedback to make this work more solid.\nLimitations\nIn this work, we propose a controlled synthetic environment to investigate ChatGPT\u2019s situational understanding ability. While we believe our synthetic environment has important real-world implications, as we discussed in Section 3.4, for certain real-world applications our findings may not apply. Another limitation is that we only focus on the evaluation of ChatGPT as the state-of-the-art chatbot model (at least in terms of mainstream media coverage). There are other commercial chatbot models that could show stronger performance on our tasks, as they may have more complicated system designs (e.g., multi-module systems as in BlenderBot 3 (Shuster et al., 2022)) that could be better at dealing with multi-round dialogue history and extremely long inputs. As we do not have sufficient budget or open access to test many such systems, we leave a comprehensive benchmark evaluation of situational understanding ability for Chat-Tuned LLMs for future work. Our experiments closely follow the OpenAI official cookbook for interacting with ChatGPT, but it is possible that there could be more optimal prompts to fully unlock the capability of ChatGPT.\nThere are many other synthetic environments like TextWorld (C\u00f4t\u00e9 et al., 2019) that may be pro-\ngrammed to do situational testing as in our work (though it may not be easy to assert full controls), and it is would be interesting to establish whether in different environments we can still draw the same conclusions. Our work mainly focuses on our proposed environment as a case study, but we plan to extend our testing framework to other environments in the future."
        },
        {
            "heading": "A Distractor on Action",
            "text": "In addition to instruction understanding, the model should also learn to correctly understand and select key information every time a new step is taken. To examine the robustness of this capability, in addition to the inclusion of normal step descriptions, we also include distractor conditions (Shi et al., 2023), in which we add one randomly selected sentence from the Brown Corpus (Kuc\u030cera and Francis, 1967) as a distractor. An example of applying such distractor is shown in Example 5. The experiment results are shown in Table 3.\nFrom Table 3, we can see after adding distractors, the State-EM performance does not degrade as much as in counter-intuitive instructions, though Step-EM performance degrades a bit (but not consistently). These findings hold both in NL and synthetic languages. This suggests that ChatGPT does have the ability to understand the interaction happened at each step and can pick out useful information.\nWe also see that when adding distractors on actions, within the 2-shot condition, Step-EM in synthetic language environment is better than the one in NL environment. This is probably because the usage of synthetic language helps the model better distinguish useful information as they look very different from synthetic languages. However, when there are more in-context samples, the model will gradually learn to extract useful information at each step and the usage of synthetic language does not help that much."
        },
        {
            "heading": "B Partial Usage of Synthetic Languages",
            "text": "We can do a more fine-grained usage of synthetic langauges \u2013 we can choose to apply only on functors (e.g., \u201cOpened\u201d) or arguments (e.g., \u201cBOX-1\u201d). The full set of experiment results is shown in Table 4. We notice that the results are very similar to the full usage of synthetic languages so to save space, we move the results in Appendix.\nC Interaction with OpenAI API and Prompt Format\nAccording to the official OpenAI cookbook,3 to send requests to ChatGPT, it is advised to first add a \u201csystem message\u201d to ChatGPT (We use \u201cYou are a helpful assistant\u201d as this is one of the most used system messages). Then there are two typical ways 4 to send the prompt, if it contains in-context samples:\nTraditional Format Just put all your prompt contents in one-round interaction (as in prompting GPT-3-Davinci series models), as shown in Example 6:\n15 response = openai.ChatCompletion.create( 16 model=..., 17 messages=[ 18 {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, 19 {\"role\": \"user\", \"content\": \"[ Instruction + In\u2212Context Samples]\"}, 20 ], 21 ..., 22 )\nExample 6: Traditional Input Format\nFaked Multi-Round Format Another way is to synthesize a fake multi-round conversation and pretend the answers for in-context samples are generated by ChatGPT, as shown in Example 7:\n23 response = openai.ChatCompletion.create( 24 model=..., 25 messages=[ 26 {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, 27 {\"role\": \"user\", \"content\": \"[ Instruction]\"}, 28 {\"role\": \"user\", \"content\": \"[ Sample\u22121\u2212Input]\"}, 29 {\"role\": \"assistant\", \"content\": \"[\nSample\u22121\u2212Ground\u2212Truth\u2212Answer]\"},\n3https://github.com/openai/ openai-cookbook/blob/main/examples/How_ to_format_inputs_to_ChatGPT_models.ipynb\n4As of the time the project is initialized. OpenAI may change a bit in the documentation and examples in the GitHub repository.\n30 {\"role\": \"user\", \"content\": \"[ Sample\u22122\u2212Input]\"}, 31 {\"role\": \"assistant\", \"content\": \"[ Sample\u22122\u2212Ground\u2212Truth\u2212Answer]\"}, 32 ... 33 ], 34 ..., 35 )\nExample 7: Faked Multi-Round Input Format\nWe do a prior study on experimenting with these two formats and find they give pretty similar StepEM and State-EM performance in fully NL and fully Synthetic Language settings (usually the performance difference is less than 5%). But faked multi-round format would insert many more tokens (leading to a higher cost) in the requests and can make the OpenAI server reject to respond, leading to low response rates. Therefore, to save budgets, in the main text of the paper, we only report the results using the traditional format. Another reason is that to see how much our findings on ChatGPT can generalize to other models, we also replicate some of our experiments when comparing with other models in Appendix E. Some models there may not support faked multi-round input format.\nFor response parsing, we just follow the instruction to parse the JSON-style response described in the same notebook and obtain the model output. For decoding parameters, we just follow default OpenAI API setting."
        },
        {
            "heading": "D Post-Processing to Extract Answers from OpenAI API Responses",
            "text": "We note that ChatGPT can return answers not strictly follow the given format. So we do a lightweight postprocessing mainly using regular expressions to make sure we parse ChatGPT results appropriately. Specifically, we first clean out the space and newline characters at the beginning and end of the answer. We then use the following regular expression to match all logical statements and extract groups of functors, arguments and truth values:\n36 ([a\u2212zA\u2212Z0\u22129]+)\\(([a\u2212zA\u2212Z0\u22129]+\u2212\\d)\\)=( True|true|False|false)"
        },
        {
            "heading": "E Comparison with other models",
            "text": "To clarify whether our observations can be generalized to other popular models, we choose one strong open-sourced competitor chatbot (according to ChatArena (Zheng et al., 2023)): Vicuna-13b\u2014 and we also compare against the performance of GPT-3-davinci-003 (it is believed that ChatGPT is a variant fine-tuned over GPT-3).\nComparison between Vicuna-13B On ChatArena (Zheng et al., 2023), Vicuna13B (Chiang et al., 2023) is voted to be the most close-to-ChatGPT chat models based on LLAMA (Touvron et al., 2023). We feed the same input to Vicuna and find the outputs are hard to parse and often incomplete. Even when there are no counter-intuitive instructions and we give 5 in-context samples, the best observed State-EM (in SL environment) is only 32%. Compared with ChatGPT (94%), there seems a big gap on the capability of situational understanding for open-source models.\nComparison between GPT-3.5 and ChatGPT Due to the budget limit and context length limit of GPT-3.5, we only compare ChatGPT performance with GPT-3.5 (Davinci-003) on 2-shot. The experiment results are in Table 5. We can see Davinci-003 achieves a similar performance as ChatGPT."
        }
    ],
    "title": "Can You Follow Me? Testing Situational Understanding in ChatGPT",
    "year": 2023
}