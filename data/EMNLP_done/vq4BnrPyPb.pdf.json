{
    "abstractText": "Research on neural networks has focused on understanding a single model trained on a single dataset. However, relatively little is known about the relationships between different models, particularly those trained or tested on different datasets. We address this by studying how the weight space and the underlying loss landscape of different models are interconnected. Specifically, we demonstrate that finetuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa \u2013 that any model that resides anywhere in those regions also exhibits high performance. Notably, we show that language models that have been finetuned on the same dataset form a tight cluster in the weight space, while models finetuned on different datasets from the same underlying task form a looser cluster. Moreover, traversing around the region between the models leads to new models that perform comparably or even better than models obtained via finetuning, even on tasks that the original models were not finetuned on. Our findings provide insight into the relationships between models, demonstrating that a model positioned between two similar models can acquire the knowledge of both. We leverage this and design a method for selecting a better model for efficient finetuning. Specifically, we show that starting from the center of the region is as effective, if not more, than using the pretrained model in 11 out of 12 datasets, resulting in an average accuracy improvement of 3.06.",
    "authors": [
        {
            "affiliations": [],
            "name": "Almog Gueta"
        },
        {
            "affiliations": [],
            "name": "Elad Venezian"
        },
        {
            "affiliations": [],
            "name": "Colin Raffel"
        }
    ],
    "id": "SP:ed5574e59c51654bfd998acb1fbbccb52da4cc33",
    "references": [
        {
            "authors": [
                "S.K. Ainsworth",
                "J. Hayase",
                "S. Srinivasa"
            ],
            "title": "Git re-basin: Merging models modulo permutation symmetries",
            "venue": "arXiv preprint arXiv:2209.04836,",
            "year": 2022
        },
        {
            "authors": [
                "R. Bar-Haim",
                "I. Dagan",
                "B. Dolan",
                "L. Ferro",
                "D. Giampiccolo",
                "B. Magnini"
            ],
            "title": "The second pascal recognising textual entailment challenge",
            "year": 2006
        },
        {
            "authors": [
                "F. Barbieri",
                "J. Camacho-Collados",
                "F. Ronzano",
                "L. Espinosa-Anke",
                "M. Ballesteros",
                "V. Basile",
                "V. Patti",
                "H. Saggion"
            ],
            "title": "SemEval-2018 Task 2: Multilingual Emoji Prediction",
            "venue": "In Proceedings of the 12th International Workshop on Semantic Evaluation",
            "year": 2018
        },
        {
            "authors": [
                "F. Barbieri",
                "J. Camacho-Collados",
                "L. Espinosa Anke",
                "L. Neves"
            ],
            "title": "TweetEval: Unified benchmark and comparative evaluation for tweet classification",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "V. Basile",
                "C. Bosco",
                "E. Fersini",
                "D. Nozza",
                "V. Patti",
                "F.M. Rangel Pardo",
                "P. Rosso",
                "M. Sanguinetti"
            ],
            "title": "SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter",
            "venue": "In Proceedings of the 13th International Workshop",
            "year": 2019
        },
        {
            "authors": [
                "L. Bentivogli",
                "P. Clark",
                "I. Dagan",
                "D. Giampiccolo"
            ],
            "title": "The sixth pascal recognizing textual entailment challenge",
            "venue": "In TAC,",
            "year": 2009
        },
        {
            "authors": [
                "G. Benton",
                "W. Maddox",
                "S. Lotfi",
                "A.G.G. Wilson"
            ],
            "title": "Loss surface simplexes for mode connecting volumes and fast ensembling",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Camburu",
                "O.-M",
                "T. Rockt\u00e4schel",
                "T. Lukasiewicz",
                "P. Blunsom"
            ],
            "title": "e-snli: Natural language inference with natural language explanations",
            "venue": "In NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "D.M. Cer",
                "M.T. Diab",
                "E. Agirre",
                "I. Lopez-Gazpio",
                "L. Specia"
            ],
            "title": "Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
            "venue": "In International Workshop on Semantic Evaluation,",
            "year": 2017
        },
        {
            "authors": [
                "A.M. Chen",
                "Lu",
                "H.-m",
                "R. Hecht-Nielsen"
            ],
            "title": "On the geometry of feedforward neural network error surfaces",
            "venue": "Neural Computation,",
            "year": 1993
        },
        {
            "authors": [
                "L. Choshen",
                "E. Venezian",
                "S. Don-Yehia",
                "N. Slonim",
                "Y. Katz"
            ],
            "title": "Where to start? analyzing the potential value of intermediate models",
            "venue": "arXiv preprint arXiv:2211.00107,",
            "year": 2022
        },
        {
            "authors": [
                "L. Choshen",
                "E. Venezian",
                "N. Slonim",
                "Y. Katz"
            ],
            "title": "Fusing finetuned models for better pretraining",
            "venue": "arXiv preprint arXiv:2204.03044,",
            "year": 2022
        },
        {
            "authors": [
                "C. Clark",
                "K. Lee",
                "Chang",
                "M.-W",
                "T. Kwiatkowski",
                "M. Collins",
                "K. Toutanova"
            ],
            "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Compu-",
            "year": 2019
        },
        {
            "authors": [
                "I. Dagan",
                "O. Glickman",
                "B. Magnini"
            ],
            "title": "The pascal recognising textual entailment challenge",
            "venue": "In MLCW,",
            "year": 2005
        },
        {
            "authors": [
                "de Marneffe",
                "M.-C",
                "M. Simons",
                "J. Tonhauser"
            ],
            "title": "The CommitmentBank: Investigating projection in naturally occurring discourse. 2019",
            "venue": "To appear in Proceedings of Sinn und Bedeutung 23. Data",
            "year": 2019
        },
        {
            "authors": [
                "W.B. Dolan",
                "C. Brockett"
            ],
            "title": "Automatically constructing a corpus of sentential paraphrases",
            "venue": "In Proceedings of the Third International Workshop on Paraphrasing (IWP2005),",
            "year": 2005
        },
        {
            "authors": [
                "S. Don-Yehiya",
                "E. Venezian",
                "C. Raffel",
                "N. Slonim",
                "Y. Katz",
                "L. Choshen"
            ],
            "title": "Cold fusion: Collaborative descent for distributed multitask finetuning",
            "year": 2022
        },
        {
            "authors": [
                "Y. Elazar",
                "N. Kassner",
                "S. Ravfogel",
                "A. Feder",
                "A. Ravichander",
                "M. Mosbach",
                "Y. Belinkov",
                "H. Sch\u00fctze",
                "Y. Goldberg"
            ],
            "title": "Measuring causal effects of data statistics on language model\u2019s \u2018factual",
            "year": 2022
        },
        {
            "authors": [
                "R. Entezari",
                "H. Sedghi",
                "O. Saukh",
                "B. Neyshabur"
            ],
            "title": "The role of permutation invariance in linear mode connectivity of neural networks",
            "venue": "arXiv preprint arXiv:2110.06296,",
            "year": 2021
        },
        {
            "authors": [
                "T. Garipov",
                "P. Izmailov",
                "D. Podoprikhin",
                "D.P. Vetrov",
                "A.G. Wilson"
            ],
            "title": "Loss surfaces, mode connectivity, and fast ensembling of dnns",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "D. Giampiccolo",
                "B. Magnini",
                "I. Dagan",
                "W.B. Dolan"
            ],
            "title": "The third pascal recognizing textual entailment challenge",
            "venue": "In ACL-PASCAL@ACL,",
            "year": 2007
        },
        {
            "authors": [
                "R. He",
                "J. McAuley"
            ],
            "title": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
            "venue": "In proceedings of the 25th international conference on world wide web,",
            "year": 2016
        },
        {
            "authors": [
                "R. Hecht-Nielsen"
            ],
            "title": "On the algebraic structure of feedforward network weight spaces",
            "venue": "Advanced Neural Computers,",
            "year": 1990
        },
        {
            "authors": [
                "G. Ilharco",
                "M.T. Ribeiro",
                "M. Wortsman",
                "S. Gururangan",
                "L. Schmidt",
                "H. Hajishirzi",
                "A. Farhadi"
            ],
            "title": "Editing models with task arithmetic",
            "venue": "arXiv preprint arXiv:2212.04089,",
            "year": 2022
        },
        {
            "authors": [
                "K. Jordan",
                "H. Sedghi",
                "O. Saukh",
                "R. Entezari",
                "B. Neyshabur"
            ],
            "title": "Repair: Renormalizing permuted activations for interpolation repair",
            "venue": "arXiv preprint arXiv:2211.08403,",
            "year": 2022
        },
        {
            "authors": [
                "J. Juneja",
                "R. Bansal",
                "K. Cho",
                "J. Sedoc",
                "N. Saphra"
            ],
            "title": "Linear connectivity reveals generalization strategies",
            "venue": "arXiv preprint arXiv:2205.12411,",
            "year": 2022
        },
        {
            "authors": [
                "S. Kornblith",
                "M. Norouzi",
                "H. Lee",
                "G. Hinton"
            ],
            "title": "Similarity of neural network representations revisited",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "H.J. Levesque",
                "E. Davis",
                "L. Morgenstern"
            ],
            "title": "The winograd schema challenge",
            "venue": "In KR,",
            "year": 2011
        },
        {
            "authors": [
                "M. Li",
                "S. Gururangan",
                "T. Dettmers",
                "M. Lewis",
                "T. Althoff",
                "N.A. Smith",
                "L. Zettlemoyer"
            ],
            "title": "Branch-trainmerge: Embarrassingly parallel training of expert language models",
            "venue": "arXiv preprint arXiv:2208.03306,",
            "year": 2022
        },
        {
            "authors": [
                "X. Li",
                "D. Roth"
            ],
            "title": "Learning question classifiers",
            "venue": "In COLING 2002: The 19th International Conference on Computational Linguistics,",
            "year": 2002
        },
        {
            "authors": [
                "Y. Liu",
                "M. Ott",
                "N. Goyal",
                "J. Du",
                "M. Joshi",
                "D. Chen",
                "O. Levy",
                "M. Lewis",
                "L. Zettlemoyer",
                "Stoyanov",
                "V. Roberta"
            ],
            "title": "A robustly optimized bert pretraining approach",
            "year": 1907
        },
        {
            "authors": [
                "P. Lu",
                "I. Kobyzev",
                "M. Rezagholizadeh",
                "A. Rashid",
                "A. Ghodsi",
                "P. Langlais"
            ],
            "title": "Improving generalization of pre-trained language models via stochastic weight averaging",
            "venue": "arXiv preprint arXiv:2212.05956,",
            "year": 2022
        },
        {
            "authors": [
                "P. Malo",
                "A. Sinha",
                "P. Korhonen",
                "J. Wallenius",
                "P. Takala"
            ],
            "title": "Good debt or bad debt: Detecting semantic orientations in economic texts",
            "venue": "Journal of the Association for Information Science and Technology,",
            "year": 2014
        },
        {
            "authors": [
                "M. Matena",
                "C. Raffel"
            ],
            "title": "Merging models with fisher-weighted averaging",
            "venue": "arXiv preprint arXiv:2111.09832,",
            "year": 2021
        },
        {
            "authors": [
                "B. McMahan",
                "E. Moore",
                "D. Ramage",
                "S. Hampson",
                "B.A. y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2017
        },
        {
            "authors": [
                "W. Merrill",
                "V. Ramanujan",
                "Y. Goldberg",
                "R. Schwartz",
                "N.A. Smith"
            ],
            "title": "Parameter norm growth during training of transformers",
            "year": 2020
        },
        {
            "authors": [
                "S.I. Mirzadeh",
                "M. Farajtabar",
                "D. Gorur",
                "R. Pascanu",
                "H. Ghasemzadeh"
            ],
            "title": "Linear mode connectivity in multitask and continual learning",
            "venue": "arXiv preprint arXiv:2010.04495,",
            "year": 2020
        },
        {
            "authors": [
                "S.M. Mohammad",
                "F. Bravo-Marquez"
            ],
            "title": "Emotion intensities in tweets. In Proceedings of the sixth joint conference on lexical and computational semantics (*Sem), Vancouver, Canada, 2017",
            "year": 2017
        },
        {
            "authors": [
                "July"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "doi: 10.18653/v1/2020.acl-main.441. URL https: //aclanthology.org/2020.acl-main.441.",
            "year": 2020
        },
        {
            "authors": [
                "Michigan",
                "June"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "doi: 10.3115/1219840.1219855. URL https://aclanthology.org/P05-1015.",
            "year": 2005
        },
        {
            "authors": [
                "J. Phang",
                "T. F\u00e9vry",
                "S.R. Bowman"
            ],
            "title": "Sentence encoders on stilts: Supplementary training on intermediate labeled-data",
            "venue": "tasks. ArXiv,",
            "year": 2018
        },
        {
            "authors": [
                "A. Poliak",
                "A. Haldar",
                "R. Rudinger",
                "J.E. Hu",
                "E. Pavlick",
                "A.S. White",
                "B.V. Durme"
            ],
            "title": "Collecting diverse natural language inference problems for sentence representation evaluation",
            "venue": "In Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Qin",
                "C. Qian",
                "J. Yi",
                "W. Chen",
                "Y. Lin",
                "X. Han",
                "Z. Liu",
                "M. Sun",
                "J. Zhou"
            ],
            "title": "Exploring mode connectivity for pre-trained language models",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Dhabi",
                "United Arab Emirates",
                "December"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "URL https: //aclanthology.org/2022.emnlp-main.451.",
            "year": 2022
        },
        {
            "authors": [
                "P. Rajpurkar",
                "J. Zhang",
                "K. Lopyrev",
                "P. Liang"
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2016
        },
        {
            "authors": [
                "A. Ram\u2019e",
                "K. Ahuja",
                "J. Zhang",
                "M. Cord",
                "L. Bottou",
                "D. Lopez-Paz"
            ],
            "title": "Recycling diverse models for out-of-distribution",
            "venue": "generalization. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "M. Roemmele",
                "C.A. Bejan",
                "A.S. Gordon"
            ],
            "title": "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
            "venue": "In 2011 AAAI Spring Symposium Series,",
            "year": 2011
        },
        {
            "authors": [
                "S. Rosenthal",
                "N. Farra",
                "P. Nakov"
            ],
            "title": "SemEval-2017 task 4: Sentiment analysis in Twitter",
            "venue": "In Proceedings of the 11th International Workshop on Semantic Evaluation",
            "year": 2017
        },
        {
            "authors": [
                "K.R. Scherer",
                "H.G. Wallbott"
            ],
            "title": "Evidence for universality and cultural variation of differential emotion response patterning",
            "venue": "Journal of personality and social psychology,",
            "year": 1994
        },
        {
            "authors": [
                "E. Sheng",
                "D. Uthus"
            ],
            "title": "Investigating societal biases in a poetry composition system",
            "venue": "In Proceedings of the Second Workshop on Gender Bias in Natural Language Processing,",
            "year": 2020
        },
        {
            "authors": [
                "R. Socher",
                "A. Perelygin",
                "J. Wu",
                "J. Chuang",
                "C.D. Manning",
                "A. Ng",
                "C. Potts"
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2013
        },
        {
            "authors": [
                "A. Talman",
                "H. Celikkanat",
                "S. Virpioja",
                "M. Heinonen",
                "J. Tiedemann"
            ],
            "title": "Uncertainty-aware natural language inference with stochastic weight averaging",
            "venue": "arXiv preprint arXiv:2304.04726,",
            "year": 2023
        },
        {
            "authors": [
                "A. Toledo",
                "E. Venezian",
                "N. Slonim"
            ],
            "title": "Revisiting sequential information bottleneck: New implementation and evaluation",
            "year": 2022
        },
        {
            "authors": [
                "L. Van der Maaten",
                "G. Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research,",
            "year": 2008
        },
        {
            "authors": [
                "C. Van Hee",
                "E. Lefever",
                "V. Hoste"
            ],
            "title": "SemEval2018 task 3: Irony detection in English tweets",
            "venue": "In Proceedings of The 12th International Workshop on Semantic Evaluation,",
            "year": 2018
        },
        {
            "authors": [
                "A. Wang",
                "Y. Pruksachatkun",
                "N. Nangia",
                "A. Singh",
                "J. Michael",
                "F. Hill",
                "O. Levy",
                "S.R. Bowman"
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "A. Warstadt",
                "A. Singh",
                "S.R. Bowman"
            ],
            "title": "Neural network acceptability judgments",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "A. Williams",
                "N. Nangia",
                "S. Bowman"
            ],
            "title": "A broadcoverage challenge corpus for sentence understanding through inference",
            "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2018
        },
        {
            "authors": [
                "M. Wortsman",
                "S. Gururangan",
                "S. Li",
                "A. Farhadi",
                "L. Schmidt",
                "M. Rabbat",
                "A.S. Morcos"
            ],
            "title": "lo-fi: distributed fine-tuning without communication",
            "venue": "arXiv preprint arXiv:2210.11948,",
            "year": 2022
        },
        {
            "authors": [
                "P. Yadav",
                "D. Tam",
                "L. Choshen",
                "C. Raffel",
                "M. Bansal"
            ],
            "title": "Resolving interference when merging models",
            "venue": "ArXiv, abs/2306.01708,",
            "year": 2023
        },
        {
            "authors": [
                "M. Zampieri",
                "S. Malmasi",
                "P. Nakov",
                "S. Rosenthal",
                "N. Farra",
                "R. Kumar"
            ],
            "title": "Predicting the Type and Target of Offensive Posts in Social Media",
            "venue": "In Proceedings of NAACL,",
            "year": 2019
        },
        {
            "authors": [
                "X. Zhang",
                "J. Zhao",
                "Y. LeCun"
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Marquez"
            ],
            "title": "Emoji (Barbieri et al., 2018)",
            "venue": "Irony (Van Hee et al.,",
            "year": 2018
        },
        {
            "authors": [
                "Elazar"
            ],
            "title": "We finetune each one on the same datasets, from the General family",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Specifically, we demonstrate that finetuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa \u2013 that any model that resides anywhere in those regions also exhibits high performance. Notably, we show that language models that have been finetuned on the same dataset form a tight cluster in the weight space, while models finetuned on different datasets from the same underlying task form a looser cluster. Moreover, traversing around the region between the models leads to new models that perform comparably or even better than models obtained via finetuning, even on tasks that the original models were not finetuned on.\nOur findings provide insight into the relationships between models, demonstrating that a model positioned between two similar models can acquire the knowledge of both. We leverage this and design a method for selecting a better model for efficient finetuning. Specifically, we show that starting from the center of the region is as effective, if not more, than using the pretrained model in 11 out of 12 datasets, resulting in an average accuracy improvement of 3.06."
        },
        {
            "heading": "1 Introduction",
            "text": "Models that share the same architecture but differ in their weights can have dramatically different capabilities. As an example, finetuned variants of a pretrained model all share an architecture, yet they are specialized for different tasks. This study\n\u2217Research done during internship in IBM Research.\nexplores the relationship between the weights of different finetuned models and the capabilities they exhibit. We analyze the weight space, where each model is represented by a weight vector \u03b8 \u2208 Rn. For simplicity, we refer to both a point in weight space and the neural network itself as a \u201cmodel\u201d.\nWe find that distance characterizes models\u2019 knowledge and similarity. Particularly, after finetuning a pretrained model on similar datasets, the resulting models are close to each other in weight space (\u00a72.3). Throughout the paper, we consider 3 granularities (\u00a73.1), showing that (i) models finetuned on the same data are closer to each other than to other models; (ii) models finetuned on the same task also cluster together; and (iii) models finetuned on general language tasks are not spread arbitrarily around the pretrained model, but fall in a constrained region in space.\nWe find that different finetuning runs on the same data tend to converge on similar points in weight space rather than dispersed points. Loosely, those points embed the necessary knowledge to perform\nthe task. This leads to the hypothesis that other points in the proximity of finetuned models might also perform the task well. Notably, such points in weight space might not necessarily be reached via finetuning, but rather via spatial transformations. Indeed, we replicate the finding (Entezari et al., 2021, c.f. \u00a78) that models finetuned on the same dataset are linearly connected, i.e., points on the line between the two models attain similar or even lower loss (\u00a75.1). We expand this finding to the convex hull between the finetuned models (\u00a75.2), suggesting that knowledge is shared across the region in space. That is, finetuned models define a connected basin of low loss, and every point within it performs well. To show this, we test models sampled from the region and find they even outperform the models achieved by finetuning. Moreover, we replicate the findings in all the aforementioned granularities: regions per dataset, task, and in general. For each, we observe a low loss across datasets, beyond the loss the individual models optimized. Furthermore, we show in \u00a76 that these regions are relatively tight, in the sense that extrapolating (rather than interpolating) can quickly produce a poorly performing model.\nOur empirical findings have intriguing implications, suggesting, for example, that the best models may not lie at the edges of the region, but rather closer to its center, while finetuning often yields models at the edge of the region. Motivated by these findings, we demonstrate in \u00a77 that a model created by averaging the weights of finetuned models from the same region outperforms the pretrained model on a variety of tasks after subsequent finetuning, even on tasks that the original finetuned models were not trained on.\nOverall, our work contributes to the growing body of knowledge about the loss landscape, finding connectivity in a whole bounded region rather than mere linear connectivity, finding connectivity between models not trained on the same task, and finding connectivity in generalization, evaluating models on multiple losses. We also provide initial context to empirical findings about fusing models. We discuss the relations to previous works in \u00a78."
        },
        {
            "heading": "2 Experimental Setup",
            "text": "We conduct two main types of experiments. In one we train models with different characteristics (e.g., dataset or task, see \u00a73.1) and examine their representation in weight space using clustering. In\nthe second experiment type, we compare losses of one group of models to another. Below, we describe the datasets (\u00a72.1), settings (\u00a72.2), and granularity levels of comparison between models (\u00a73.1)."
        },
        {
            "heading": "2.1 Datasets",
            "text": "We finetune and evaluate models on 36 datasets. Those datasets can be categorized into a few families: natural language inference (NLI), Sentiment analysis and Topic classification tasks, Twitter domain, and a collection of general datasets that covers a wide range of capabilities. We chose classification datasets for reliable evaluation. The details of each dataset family are found in App. A. We mostly rely on the MNLI (Williams et al., 2018b) dataset, the NLI family, and the General group, as case studies, and elaborate on them below:\nGeneral dataset family contains 12 text classification datasets from GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019), excluding testonly (AX-b (Wang et al., 2019), AX-g (Poliak et al., 2018)) and regression (STS-B (Cer et al., 2017)) datasets. We further exclude WSC (Levesque et al., 2012) and CoPA (Roemmele et al., 2011) which are small and therefore produce unstable results (e.g., finetuning results were sometimes lower than pretrained model results). The datasets consist of a wide range of classification tasks, from sentiment analysis to linguistic acceptability to NLI.\nNLI family is composed of 6 natural language inference (NLI) datasets: MNLI (Williams et al., 2018a), QNLI Rajpurkar et al. 2016, RTE (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), WNLI (Levesque et al., 2011), ESNLI (Camburu et al., 2018), and adversarial NLI (Nie et al., 2020)."
        },
        {
            "heading": "2.2 Training Approaches",
            "text": "We experiment with RoBERTa-base (Liu et al., 2019) as our base pretrained model, except in App. B where we analyze different pretrained models. For finetuning, we follow the standard hyperparameters (Liu et al., 2019), with a larger batch size of 256 and a learning rate of 5e\u2212 5. Most experiments analyze 5 different seeds, and the samedataset clustering 20 seeds (\u00a73.1). Those seeds control randomly initialized weights in the classification head as well as data shuffling."
        },
        {
            "heading": "2.3 Clustering Approach",
            "text": "In the clustering experiments, we qualitatively explore whether models trained on similar data end up close together in weight space. We experimented with various distance metrics and clustering algorithms. While many metrics worked well, we found that subtracting the pretrained weight values from the finetuned values (referred to as \u201ctask vectors\u201d by Ilharco et al. (2022)) and measuring distance via cosine similarity was conceptually simple, cheap to compute, and provided qualitatively reasonable results compared to more sophisticated methods (Kornblith et al., 2019; Toledo et al., 2022). We also tested Euclidean distance but it did not produce clear clusters. This is likely caused by the weights\u2019 norm growth during training (Merrill et al., 2020) that is unrelated to the data at hand (\u00a7C). This can also explain questions that were previously left open (Qin et al., 2022). As a clustering algorithm, we use Spectral Clustering with as many clusters as datasets or dataset families (Pedregosa et al., 2011). For visualization, we project the 120M dimensional weight vectors into 2 dimensions using t-SNE (Van der Maaten & Hinton, 2008)."
        },
        {
            "heading": "3 Methodology: Comparing Models",
            "text": "In this work, we compare models that share an architecture, but were trained on different data. To do so, we investigate the space of weights \u03c9 \u2208 Rd where each model has a weight vector and each point in space represents a model. We adopt the typical perspective that the model f\u03b8 consists of a representation encoder f\u03c9 followed by a taskspecific classifier f\u03d5, i.e. f\u03b8 = f\u03d5 \u25e6 f\u03c9 := f\u03d5,\u03c9 (Choshen et al., 2022a; Ram\u2019e et al., 2022).\nIdeally, we would compare finetuned models by\ntheir loss. Unfortunately, the loss is often incomparable across datasets or tasks. Hence, we compare by preserving each encoder, and fitting a classification head to each model for each target dataset.\nSpecifically, to calculate the loss of a model we perform the following: First, we remove any existing masked language modeling layers or classification heads and replace them with a new randomly initialized classification head. This leaves the rest of the weights i.e., the encoder f\u03c9, fixed. We then perform linear probing, i.e., we train only the new classification head on a desired target data xtrain and its labels ytrain. Lastly, we pass the test data xtest through the model (including the classifier f\u03d5 on top of it) and report the loss with respect to the labels ytest. Formally, for the model f\u03d5,\u03c9 and loss function l, we report the generalized loss lg(\u03c9) = l(f\u03d5,\u03c9(xtest), ytest) where f\u03d5 = argmin\u03d5 l(f\u03d5,\u03c9(xtrain), ytrain). This approach has a desirable trait: When considering the task on which the model was originally finetuned, our loss lg is equal to the original finetuning loss l. Furthermore, since fitting a linear classifier given a fixed representation is a convex optimization problem, we observe similar results across runs.\nThe generalized loss lg enables comparing models finetuned on different datasets. It is hence undesirable to test only on one of the datasets. We thus consider a loss on a dataset, but also the average loss on a family of datasets. For example, the average loss across all entailment datasets rather than the loss on a particular dataset."
        },
        {
            "heading": "3.1 Levels of Granularity",
            "text": "To study the relationship between weights of similarly trained models, we experiment with 3 levels\nof granularity for dataset similarity. At each level, we analyze models finetuned on source datasets sharing some traits. In each level\u2019s setting, we define an interior group (hereafter In) of datasets that share a trait as well as an exterior group (hereafter Ex) of models not sharing the trait. By default, we report on each group the average loss over all source datasets used for finetuning In models.\nSame-Dataset. In the most specific case, models are similar if they were finetuned on the same dataset. Interior models are finetuned on MNLI (Williams et al., 2018a) and Ex on the rest of the General datasets. We report the loss over MNLI.\nSame-Task. At this broader granularity, we consider the group of models trained on the same task. In that case, In contains models finetuned on NLI datasets and Ex contains models finetuned on all other datasets. We report loss over all NLI datasets, except for ANLI which is not intended for such test purposes. ANLI is made with adversarial examples that cause misclassifications for NLI-trained models. In initial trials, it showed similar trends, but we omit it from the test for good practice.\nGeneral. In the most general case, we consider any model finetuned on any of the General datasets as In. This leaves little to consider as exterior, so we construct Ex by perturbing the pretrained model\u2019s weights in a random direction. We apply a perturbation whose norm is equal to the average task vector norm of In models. Since there is no clear prior to sampling a random direction in space, we aim for a prior that prefers points in the weight space that represent \"reasonable\" networks. We use Xavier initialization (Glorot & Bengio, 2010) to define such a prior. The prior is an i.i.d. Gaussian distribution over each weight with zero mean and where variance depends on the layer characteristics. This choice reduces the probability of sampling networks with exploding or vanishing outputs, which would stand as a weak baseline."
        },
        {
            "heading": "4 Analysis in Weight Space",
            "text": "We start our analysis by showing that the models trained on similar data fall into the same region in weight space - i.e., they are clustered together. We leave the inverse claim (i.e. showing that models within the cluster obtain a lower loss than the models outside the cluster) to \u00a75.1 and \u00a75.2.\nSpecifically, we find (see Fig. 2) that finetuning on similar data results in closer weight space mod-\nels compared to models that have been trained on different datasets or tasks. Notably, despite the fact that neural networks implement highly non-linear functions, finetuning similarity is expressed in the Euclidean space of their weights. Moreover, we show in App. \u00a7C that the direction in space is determined by the type of training data and not by its amount. In App. B, we show that this proximity is contingent on starting from the same base model.\nSimilarity Per Dataset. In the simplest case, for each dataset in the General group, we finetune models with 20 random seeds and cluster the resulting 280 models into 12 clusters. As seen in Fig. 2(a), for the most part, models finetuned on the same dataset are clustered together. Accordingly, the overall clustering accuracy is 98%, with all but 3 clusters perfectly matched.\nSimilarity Per Task. In this experiment, we show that models finetuned on datasets from the same task are also close in weight space (we discuss same-domain proximity in App. D). As explained in \u00a72.1 we have dataset families for 3 tasks: NLI, Topic, and Sentiment. For each dataset in each family, We finetuned models with 5 random seeds. Then, we cluster all models into 3 clusters. As seen in Fig. 2(b), models that were finetuned on the same task family are closer to each other and are clustered together (clustering accuracy of 90%). We report the F1 Score per group in App. D.\nSimilarity in General. Unlike datasets or tasks, we can not create multiple distinct general groups and can not expect multiple clusters to occur. Therefore, we do not present clustering for this granularity level. However, we can still infer that this general region does not encompass the whole space around the pretrained model, and has a superior loss in general (see \u00a75.2)."
        },
        {
            "heading": "4.1 Cause: Data Type, not Size",
            "text": "Supposedly, a confounding factor may explain the above results, wherein the finetuned model moves more with more data. To test this, we finetune models on sub-samples with different sample sizes (200, 400, 800, 1.6K, 3K). For consistency, we take only the 9 datasets from General family that contain at least 3K training samples. We then cluster the finetuned models into k clusters, with k the number of datasets or the number of dataset sizes.\nThe resulting clusters (App. C) are clustered by data type, not by the amount of data, similar to\nFig. 2. Choosing k to be the number of data-sizes does not cluster by data size either. We conclude that the observed similarity comes from the nature of the data, and not from the size of a given dataset."
        },
        {
            "heading": "5 Loss in the Region between Models",
            "text": "In \u00a74, we claim that models trained on similar data converge near each other, but is this area to which they converge meaningful? In this section, we show that models falling in the entire region around these clusters correspond to performant models.\nThe models we analyzed so far were the outcome of a gradient-based optimization process searching for the minimum loss. The locality we observed in weight space indicates that the points found through this procedure are concentrated in relatively small regions. We hypothesize that a whole region of low losses (corresponding to performant models) exists between the separate points found during finetuning. For example, the \"NLI region\" contains MNLI, SNLI and QNLI models but also other points that reflect models that might not have been found through gradient-based optimization on a specific dataset but exhibit the general abilities needed to perform natural language inference.\nWe test this hypothesis by interpolating pairs of similarly trained models and show in \u00a7 5.1 that the points between the models perform comparably to or even better than the original finetuned models. This suggests that indeed there are regions in weight space where all points encode the knowledge or behaviour required for a particular task. We expand this claim in \u00a75.2 and show that the whole region that lies between these models (their convex hull) corresponds to models that perform well."
        },
        {
            "heading": "5.1 Interpolation: Lines Between Model Pairs",
            "text": "In this experiment, we consider the points in weight space between pairs of finetuned models. Given a pair of models, we shift from one model to the other by linearly interpolating between their weights, i.e., we take the model\u2019s weights \u03c91, \u03c92 \u2208 Rd, and consider weighted sums of their weights: \u03c91 \u2217 \u03b1 + \u03c92 \u2217 (1 \u2212 \u03b1). where \u03b1 \u2208 [0, 1]. We then evaluate each interpolated model both on the datasets the original models were finetuned on, and on additional datasets unseen by the models. We interpolate pairs of different models finetuned on the same dataset, or on two different datasets. We report the average losses produced by repeating the experiment with finetuning using different seeds.\nResults ( Fig. 3) show that interpolated models perform comparably or even better than the models they are created from. We present further results testing the groups on different losses in App. \u00a7E and find performance is often best somewhere in the interpolation between the two models. We now elaborate on each granularity level separately.\nInterpolation Per Dataset. We interpolate 5 finetuned models on the MNLI dataset (resulting in a total of 10 pairs) and evaluate on MNLI. We report an analogous experiment with SST2 in App. \u00a7E. Figure 3(a) shows that the interpolated models perform well on average and even outperform the original models they are created from. Similar results were found in other settings (e.g.; Wortsman et al., 2022b) and we discuss those works in \u00a78.\nInterpolation Per Task. We interpolate 5 models finetuned on MNLI with 5 models finetuned on ESNLI, both from the NLI task, resulting in 25 pairs, and evaluate on all NLI test datasets. We replicate the results of the previous experiment and find the interpolated models are performant on all targets on average, as can be seen in Fig. 3(b).\nInterpolation In General. We interpolate 5 models finetuned on MNLI with 5 models finetuned on SST2, both from the General family, resulting in 25 pairs and evaluate on all General datasets as targets. Fig. 3(c) shows improved performance in this extended group and better performance in the interpolated models than in the finetuned ones."
        },
        {
            "heading": "5.2 Comparison between Region losses",
            "text": "Thus far, we showed that models on the line between model pairs perform well. We now extend the analysis to show that models in the whole region between similar models perform well. However, visualizing or searching a whole multidimensional region (the convex hull) is not feasible. Instead, we sample models in the region and show they outperform their external counterparts.\nLet In be a group of models and In\u2019 be the convex hull between all the models in In, making each model in In\u2019 a weighted average of models in In:\u2211|In|\ni=0 \u03b1i\u00b7\u03c9i where \u2211|In|\ni=0 \u03b1i = 1 and \u03c9i \u2208 In. Practically, as In\u2019 is infinite, we estimate it by sampling |In| models uniformly from the region they convey.\nWe note that weighted averaging in this manner was shown to be practical and work well in many scenarios, either in efficient finetuning (\u00a77 Yadav\net al., 2023) or in full finetuning (Choshen et al., 2022b; Matena & Raffel, 2021, c.f. \u00a78).\nWe define a metric to compare two groups of models. Given In models group and the exterior models group Ex, we calculate PB as the probability that an In model outperforms an Ex one:\nPB = E i\u2208In,j\u2208Ex 1{lg(\u03c9i) \u2264 lg(\u03c9j)}.\nPB can also be applied to In\u2019 and Ex.\nAs a loss function, we take the average loss over the source datasets used to create the In models.\nTesting models from In and In\u2019 groups, we find they indeed outperform Ex models on the tasks the In models were trained on. We find this is true in all granularity levels \u2013 models in the dataset region are better than other models, and more broadly any finetuned model is better than models that have been randomly shifted by the same distance from the pretrained model. Moreover, we again find (as in \u00a75.1) that In\u2019 is even better than the In. In addition to the bottom-line metric PB, we depict the loss distributions across those models in Fig. 4.\nLoss Per Dataset. We test the performance of models between models finetuned on a dataset. We consider the case where In is the group of finetuned models on MNLI and Ex is the group of finetuned models on General datasets. Both groups are evaluated on the MNLI dataset. We find PB is 100% for In, meaning that all MNLI models outperform on MNLI than all the rest of the models. More surprising is that the same is true for In\u2019, PB of 100% \u2013 all the models between MNLI models are better than Ex. In fact, in 88% of the times In\u2019 models are also better than In \u2013 i.e. models finetuned on MNLI!\nLoss Per Task. We compare models from a task region with models from other regions. Here, In are the models finetuned on NLI task and Ex on the rest of the datasets described in \u00a72.1. Both groups are evaluated on all NLI test datasets. NLI In group models are better in PB = 75.3% of the cases, and the In\u2019 models in 100%. In\u2019 is also better than In with PB = 96.7%.\nLoss In General. We define In to be finetuned models on General datasets and Ex to be random models as defined in \u00a73.1. Both are evaluated on\nthe General datasets. We find again that In is better than Ex (PB = 89.8%) but worse than In\u2019 (PB = 90%) which is also better than Ex (PB = 100%).\nTo conclude, we consistently see that the region between finetuned models not only provide models that are better than the baseline but also provides models that are better than the finetuned models defining the edges of region."
        },
        {
            "heading": "6 Region Edges",
            "text": "Above, we have shown that there are spacial regions that specify learnt generalizations. We now look for the boundaries of those regions, where loss is no longer similarly low. To do that we traverse in the opposite way to the interpolation. We also test the edges going from the center of the region to other directions in App. F."
        },
        {
            "heading": "6.1 Extrapolation: Lines Between Models",
            "text": "In Section 5.1, we took pairs of models and found that the linear path between them passes through a region of low loss. We now continue on this path and check how far in the opposite directions (i.e. away from the model being interpolated to) do we need to move in order for the loss to rise. We reproduce the interpolations settings of \u00a75.1, but apply linear extrapolation, i.e., test \u03b1 values out of range [0,1]. We make 10 steps in logarithmic advances from 1 to 32 and similarly from 0 to -31.\nFigure 5 depicts the results for the Same-Dataset granularity level. We provide more detailed results in App. G. We find that for all granularity levels extrapolation rapidly reaches bad performance. This implies the converged models are near the edge of the loss basin. We further observe that the region has a relatively flat base and steep cliffs, implying that the regions we find are small basins and not e.g. a subspace. In a sense, we discover a bounded region that characterizes the loss region (of e.g., MNLI dataset) where the models within have a low loss and the models beyond have a high loss."
        },
        {
            "heading": "7 Practical Takes",
            "text": "Our work has several practical implications. First, we observed (\u00a75.2) that models inside the region (In\u2019) are often superior to the finetuned models defining the region (In). Practically, one can average models from the same region and cautiously expect the resulting model to perform better. This model can be used without further finetuning, in\nthe Same-Dataset region, as has indeed been used in practice (c.f. \u00a78; Wortsman et al., 2022b,a).\nWe provide another implication of our findings. If indeed models in In\u2019 share partial information with models from In, this aggregated information may be general and useful for other tasks. In practice, there are two common uses for a trained model, either for the immediate classification of unseen examples or as a starting point for further training. We focus on the later use as a low loss directly indicates it could be useful in that setting.\nWe hypothesize that points in the region could be better for finetuning than finetuning the pretrained model itself. As there are endless possibilities of points in the region with no preference to any specific, we practically pick the centroid of the region, i.e., the average between models in In. The centroid point is equally influenced by each model defining the region, and without further information may be stronger than arbitrary points in the region (see App. \u00a7E), but also be suboptimal (see \u00a75.1, App. \u00a7E).\nFor subsequent training, we employ parameterefficient finetuning. Specifically, BitFit (Ben Zaken et al., 2022), one of the most parameter-efficient methods, which has been shown to attain strong performance. Changing only a small subset of the weights reduces the complex effects of training dynamics and eases attributing improvements to the initialization weights. We avoid giving an unfair advantage to our method and for each target dataset choose the centroid of all models excluding ones finetuned on the target dataset itself.\nWe find (Fig. 6 and App. H) that starting from\nthe centroid results in a better performing model than starting from a pretrained model, by 4.04% on average. The centroid is better in almost all cases, outperforming the pretrained in 9 cases, matching the results in 2, and underperforming in 1 case.\nEfficient finetuning is especially interesting in the scenario of scarce data (App. H). We hence replicate the results in a few-shot scenario limiting the training examples to 1K. The general trend is replicated, only that improvements reach as high as 34% improvement and above 10.66% on average."
        },
        {
            "heading": "8 Explaining previous results",
            "text": "A great deal of prior work considered the connectivity between models, i.e. whether the path in weight space between two networks has a low loss throughout. Early work demonstrated that models trained on the same dataset have such a path but that the path is not necessarily linear (Garipov et al., 2018; Frankle et al., 2020). This non-linearity was often explained by the fact that networks can represent the same function after their weights are permuted (Ainsworth et al., 2022; Jordan et al., 2022; Chen et al., 1993; Hecht-Nielsen, 1990). Taking into account these symmetries and/or using the same initialization was then shown to produce a linear path of low loss (McMahan et al., 2017; Entezari et al., 2021). Benton et al. (2021) even considered simplexes of low loss, rather than linear paths. In addition, Mirzadeh et al. (2020) showed that multitask learning converges to a point with low loss for both tasks, and in parallel work Qin et al. (2022) showed that the minima are connected for two datasets of the same task. We generalize those notions in the context of finetuned models. Specifically, we confirm that indeed there is a linear path between two models, but further that there is a whole region with low loss through which the\nlinear path moves. Intriguingly, we have observed that these low-loss regions are unique for each specific dataset or task, whereas Juneja et al. (2022) has reported the existence of multiple basins per each. We also generalize this finding to models that were not trained on the same data and are tested on different data. Qin et al. (2022) is the only work we know to compare models trained on different tasks. However, they report random chance performance in this case. To enable meaningful model comparison, we proposed the generalized loss (\u00a73).\nOur results also support and provide some preliminary explanations of recent practical findings. Some works show that starting from a finetuned model helps when finetuning on a different target dataset (Choshen et al., 2022a; Phang et al., 2018), which may be related to the fact that the initial finetuning stage moves the model into the general \"language\" region (or, even better, the region of space corresponding to the target task). Moreover, a growing literature has shown improvements from averaging two or more finetuned models. Some of those average models trained on the same dataset (Wortsman et al., 2022b,a), which we show picks a model from inside the dataset region. Others show that averages between models can improve models from tasks that they were not trained on (Choshen et al., 2022b; Matena & Raffel, 2021), which agrees with our more general findings. Ilharco et al. (2022) further suggests that some attributes can be added to the model by moving in certain directions in the weight space. In parallel work, Ram\u2019e et al. (2022) considers two finetuning stages before averaging. Lu et al. (2022) and Talman et al. (2023) propose optimization methods featuring Stochastic Weight Averaging (SWA). Our results may indicate that the success of such methods may be partly attributed to its tendency to fall within a region, rather than on its borders. More recent work considers iterative model averaging, where in each iteration multiple models are trained in parallel from the same initial point and then averaged to aggregate their knowledge. Such a procedure has been demonstrated both for self-supervised pretraining (Li et al., 2022) and as a supervised pretraining, similar to a massively multitask learning scenario (Don-Yehiya et al., 2022). Future work could focus on understanding how those processes move through the weight space and whether they move to areas of loss space outside of the region corresponding to a single iteration of averaging finetuned models."
        },
        {
            "heading": "9 Conclusion and Discussion",
            "text": "Combining all of our results together conveys a consistent message: There are regions in weight space corresponding to good performance on a dataset, a task, or in general. From \u00a72.3 we can conclude that performant models are centered in certain areas (or more specifically basins) in weight space. We find in \u00a75.1 that these form one basin rather than multiple nearby points falling into multiple basins and, in \u00a75.2, that this basin is a convex region and not simply a line between two points. Finally, the extrapolations in \u00a76 show those areas do not exceed far beyond the finetuned models. Moreover, our results suggest that models found via finetuning typically lie on the boundaries of these regions and are often suboptimal, prompting future work in exploring the limitations of gradient-based training."
        },
        {
            "heading": "10 Limitations",
            "text": "We discuss limitations where relevant throughout the work, but also provide this section for general discussion of limitations.\nOur work was only evaluated on finetuning a pretrained model, and hence may not hold in general when randomly initializing. They also focused on English classification data.\nWhile our results were very robust when referring to tasks, we did not find many groups of datasets of distinct domains to test on and got mixed results in those aspects. We discuss the results in App. D.\nThe scope of our experiments is broad in some aspects it is less so in others. While our experiments included thousands of finetuned models, trained on 36 datasets and also evaluated on 36 datasets. We did not replicate it on many pretrained models as well."
        },
        {
            "heading": "A Dataset List",
            "text": "Most datasets could be downloaded from huggingface datasets. We explicitly state the download link when relevant. As we used groups of datasets we report here the full list of datasets they contain.\nGeneral: CoLA (Warstadt et al., 2019), SST2 (Socher et al., 2013), MRPC (Dolan & Brockett, 2005), QQP (data.quora.com/First-Quora-Dataset-Release-Question-Pairs), MNLI (Williams et al., 2018a), QNLI Rajpurkar et al. 2016, RTE (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), WNLI (Levesque et al., 2011) BoolQ (Clark et al., 2019), CB (de Marneffe et al., 2019), CoPA (Roemmele et al., 2011), MULTIRC (Khashabi et al., 2018), WIC (Pilehvar & Camacho-Collados, 2019)\nNLI datasets: MNLI (Williams et al., 2018a), QNLI Rajpurkar et al. 2016, RTE (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), WNLI (Levesque et al., 2011), ESNLI (Camburu et al., 2018), adversarial NLI (Nie et al., 2020).\nTwitter domain datasets (collected by TweetEval (Barbieri et al., 2020)) EmoInt (Mohammad & BravoMarquez, 2017), Emoji (Barbieri et al., 2018), Irony (Van Hee et al., 2018), OffenseEval (Zampieri et al., 2019), HatEval (Basile et al., 2019), Sentiment Analysis (Rosenthal et al., 2017)\nSentiment Analysis: Poem Sentiment (Sheng & Uthus, 2020), IMDB (Maas et al., 2011), Rotten Tomatoes (Pang & Lee, 2005), SST 5bins (Socher et al., 2013), SST2 (Socher et al., 2013), Amazon reviews (He & McAuley, 2016) ,Financial Phrasebank (Malo et al., 2014)\nTopic Classification: AG news (Zhang et al., 2015), ISEAR (Scherer & Wallbott, 1994), Yahoo answers (Zhang et al., 2015), DBpedia (Zhang et al., 2015), 20 newsgroup (Zhang et al., 2015), TREC in both fine-grained and coarse-grained labels (Li & Roth, 2002)"
        },
        {
            "heading": "B Similarity Per Dataset, when Starting from different Pretrained Models",
            "text": "After seeing in \u00a72.3 the repeated behavior on several granularity levels, we were curious whether we could receive the same behavior on a larger granularity level - models starting from different pretrained RoBERTa models, and finetuned on the same datasets. In this experiment, we employ two pretrained RoBERTa models, the original RoBERTa-base and the re-implementation of RoBERTa-base created by Elazar et al. (2022). We finetune each one on the same datasets, from the General family. Results show that the models get clustered according to the pretrained model they were created from, regardless to the finetuning they went through. This might arise from the low distances moved from the initialization, pretraining changes the model\u2019s weights much more than finetuning. Therefore, since we start from different pretrained models, the resulted finetuned models are more similar to the pretrained model they started from.\nAs the results on both pretrained models are comparable, we deduce that there is not one unique basin or region for each ability, but many. However, around a starting point it seems there are distinct regions within reach."
        },
        {
            "heading": "C Cause: Data Type, not Size",
            "text": "We provide the clustering and visualize with t-SNE in Fig. 7. We see that the clustering and the data type agree in all but one of the cases.\nWe provide in Fig. 8 a detailed view of the similarities between each pair of models by dataset and amount of data seen in training. We find that with relatively little data, the direction in space is already determined, i.e., similar datasets go to similar direction even with limited amount of data."
        },
        {
            "heading": "D Similarity Per Task and Domain",
            "text": "As noted in 2.1, the datasets we use can be separated into specific four dataset families in addition to the general group: NLI, Sentiment analysis, Topic, and Twitter. while the first three are characterized by their task, the last group is characterized by the domain of the dataset it contained. As one can see in Fig. 9 and 1 although the clustering shows good separation between task groups, it struggles to separate the Twitter domain group models from the other groups. Separating the space into 4 clusters and labeling them in a\n1-to-1 mapping to maximize accuracy, we find 31 f-score on the Twitter cluster and 62,71,1 on the Topic, Sentiment and NLI groups respectively.\nA possible explanation may be that the domain feature is orthogonal to the task feature, in the sense that some datasets should be assigned to two groups at the same time (for example TweetEval Sentiment Analysis (Rosenthal et al., 2017) is part of the Twitter domain group, as well as the Sentiment analysis task group). This gives place to two competing hypotheses that we leave unanswered. Either the regions of domains overlap with regions of tasks; or, even if less likely, domains are not separated into regions in space, unlike tasks.\nE Interpolation Between Models\nWe provide a more comprehensive interpolation experiment. In it we show the interpolation between pairs of models and report the loss of each of the datasets used to create the pair of models, as well as the average reported in the main paper.\nIn Fig. 10, one can see not only the interpolation between models in In, but interpolation between the centroids. We take the average of all the models in one group from which we interpolate (e.g., all MNLI models) and set it as a centroid. We then repeat it on the other group and interpolate between those centroids instead of interpolating between actual finetuned models. We find that although now we are interpolating between two points that were both not the outcome of traditional ways of optimization, we find comparable and often even lower losses than before. This also motivates the practical experiments reported in \u00a77."
        },
        {
            "heading": "F Loss Region Outside of Models in Other Directions",
            "text": "After seeing that we can reach outside of regions by performing linear-extrapolation, we test the performance of models when we move away to different directions. To test it, we start with several models of the same region, calculate their centroid by averaging their weights, and gradually move away from this centroid according to the same procedure as in section 3.1. We move away from the centroid towards one of two directions: towards the origin of the axis, or towards random directions. We evaluate on the same datasets the In models were finetuned on.\nFigure 11 shows the results for the first and third granularity levels. A detailed analysis for each level follows.\nOutside of the Dataset Region. We compare the performance of three types of models: finetuned models on MNLI, models moving from the centroid of MNLI models to the origin, and models moving from it to random directions.\nResults show that when the distance of the generated models from the centroid is similar to the distance of the finetuned models (radius \u2264 1), the generated models perform as well as the finetuned models, meaning we are still inside the MNLI region and all models share the knowledge needed for the MNLI target task. It also implies the directions in which finetuned models vary are not special, most changes around the center are equally acceptable.\nWhen the distance increases and we move farther away from the centroid, the performance of the randomly generated models decreases significantly, indicating the end of the region. A surprising finding is that this happens on random directions, but not when going towards the origin. The performance in that case is similar to the performance of the finetuned models, even when the models are farther from the centroid then the finetuned models. While we did not expect this phenomenon or have an explanation to it, we report it as an avenue for future work.\nOutside of the Finetuning Region. We compare the performance of three types of models: finetuned models on datasets from the General family, models starting from the centroid of those models towards the origin or towards random directions. Each time, we evaluate all above models on a single dataset from the General family, separating the performance of the model finetuned on the target dataset (called source model), to the rest of finetuned models (called non-source models), resulting in total of four types of models in the comparison, including the two types of generated models starting from the centroid.\nWe average the performance of each type on all target datasets we evaluate on, and show the results in Figure 11(b). We can see that the source model outperforms all other models. For small distances from the centroid, the non-source models underperform the generated models, and for large distances it outperform the generated models going towards random directions. The generated models going towards the origin outperform the two above types of models, for all distances. These results suggest that when staying close enough to the centroid, roaming from the centroid to different directions might be superior to a finetuned model on a different dataset. However, when distancing far from the centroid, finetuned models on other datasets then the target dataset perform better than generated models going towards random directions, since the last are probably outside of the region. Worth noticing, the standard deviation of the last is meaningfully larger than the rest of the models, and of the one of generated models in the Dataset granularity level."
        },
        {
            "heading": "G Extrapolation Between Models",
            "text": "Fig. 12 presents the same behaviour for all three granularity levels- extrapolation rapidly reaches bad performances.\nWe provide a more comprehensive extrapolation experiment showing each time the extrapolation with the loss of each of the datasets used to create the pair of models, and the average reported in the main paper. We find (see Fig. 13(b)) that despite all of our datasets called and framed as natural language inference, WNLI (Levesque et al., 2011) behaves differently and might be considered not strictly a part of the region. This may also explain the long tail in Fig. 4(b)."
        },
        {
            "heading": "H Efficient Finetuning",
            "text": "We provide in this section the full results of efficiently finetuning. We provide the full results of the regular finetuning in Table 2 and the few-shot setting in Table 3 and Fig. 14."
        }
    ],
    "title": "Knowledge is a Region in Weight Space for Finetuned Language Models",
    "year": 2023
}