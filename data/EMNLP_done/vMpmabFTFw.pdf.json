{
    "abstractText": "Recent studies have shown that sequence-tosequence (seq2seq) models struggle with compositional generalization (CG), i.e., the ability to systematically generalize to unseen compositions of seen components. There is mounting evidence that one of the reasons hindering CG is the representation of the encoder uppermost layer is entangled, i.e., the syntactic and semantic representations of sequences are entangled. However, we consider that the previously identified representation entanglement problem is not comprehensive enough. Additionally, we hypothesize that the source keys and values representations passing into different decoder layers are also entangled. Starting from this intuition, we propose COMPOSITION (Compose Syntactic and Semantic Representations), an extension to seq2seq models which learns to compose representations of different encoder layers dynamically for different tasks, since recent studies reveal that the bottom layers of the Transformer encoder contain more syntactic information and the top ones contain more semantic information. Specifically, we introduce a composed layer between the encoder and decoder to compose different encoder layers\u2019 representations to generate specific keys and values passing into different decoder layers. COMPOSITION achieves competitive results on two comprehensive and realistic benchmarks, which empirically demonstrates the effectiveness of our proposal. Codes are available at https://github. com/thinkaboutzero/COMPOSITION.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lei Lin"
        },
        {
            "affiliations": [],
            "name": "Shuangtao Li"
        },
        {
            "affiliations": [],
            "name": "Yafang Zheng"
        },
        {
            "affiliations": [],
            "name": "Biao Fu"
        },
        {
            "affiliations": [],
            "name": "Shan Liu"
        },
        {
            "affiliations": [],
            "name": "Yidong Chen"
        },
        {
            "affiliations": [],
            "name": "Xiaodong Shi"
        }
    ],
    "id": "SP:3a4bf6c6aeb7a26f9efbed17acc60d5d5a9898b2",
    "references": [
        {
            "authors": [
                "Lasha Abzianidze",
                "Johannes Bjerva",
                "Kilian Evang",
                "Hessel Haagsma",
                "Rik van Noord",
                "Pierre Ludmann",
                "DucDuy Nguyen",
                "Johan Bos"
            ],
            "title": "The parallel meaning bank: Towards a multilingual corpus of translations annotated with compositional meaning",
            "year": 2017
        },
        {
            "authors": [
                "Ekin Aky\u00fcrek",
                "Afra Feyza Aky\u00fcrek",
                "Jacob Andreas."
            ],
            "title": "Learning to recombine and resample data for compositional generalization",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Ekin Akyurek",
                "Jacob Andreas."
            ],
            "title": "Lexicon learning for few shot sequence modeling",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Andreas."
            ],
            "title": "Good-enough compositional data augmentation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7556\u20137566, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference",
            "year": 2015
        },
        {
            "authors": [
                "Ankur Bapna",
                "Mia Chen",
                "Orhan Firat",
                "Yuan Cao",
                "Yonghui Wu"
            ],
            "title": "Training deeper neural machine",
            "year": 2018
        },
        {
            "authors": [
                "Francesco Cazzaro",
                "Davide Locatelli",
                "Ariadna Quattoni",
                "Xavier Carreras."
            ],
            "title": "Translate first reorder later: Leveraging monotonicity in semantic parsing",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6,",
            "year": 2023
        },
        {
            "authors": [
                "Rahma Chaabouni",
                "Roberto Dess\u00ec",
                "Eugene Kharitonov."
            ],
            "title": "Can transformers jump around right in natural language? assessing performance transfer from SCAN",
            "venue": "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpret-",
            "year": 2021
        },
        {
            "authors": [
                "Yong Cheng",
                "Lu Jiang",
                "Wolfgang Macherey",
                "Jacob Eisenstein."
            ],
            "title": "AdvAug: Robust adversarial augmentation for neural machine translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Henry Conklin",
                "Bailin Wang",
                "Kenny Smith",
                "Ivan Titov."
            ],
            "title": "Meta-learning to compositionally generalize",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural",
            "year": 2021
        },
        {
            "authors": [
                "Verna Dankers",
                "Elia Bruni",
                "Dieuwke Hupkes."
            ],
            "title": "The paradox of the compositionality of natural language: A neural machine translation case study",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2022
        },
        {
            "authors": [
                "Mostafa Dehghani",
                "Stephan Gouws",
                "Oriol Vinyals",
                "Jakob Uszkoreit",
                "Lukasz Kaiser."
            ],
            "title": "Universal transformers",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Tobias Domhan."
            ],
            "title": "How much attention do you need? a granular analysis of neural machine translation architectures",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1799\u20131808.",
            "year": 2018
        },
        {
            "authors": [
                "Li Dong",
                "Mirella Lapata."
            ],
            "title": "Language to logical form with neural attention",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Zi-Yi Dou",
                "Zhaopeng Tu",
                "Xing Wang",
                "Shuming Shi",
                "Tong Zhang."
            ],
            "title": "Exploiting deep representations for neural machine translation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4253\u20134262, Brussels,",
            "year": 2018
        },
        {
            "authors": [
                "Zi-Yi Dou",
                "Zhaopeng Tu",
                "Xing Wang",
                "Longyue Wang",
                "Shuming Shi",
                "Tong Zhang."
            ],
            "title": "Dynamic layer aggregation for neural machine translation with routing-by-agreement",
            "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The",
            "year": 2019
        },
        {
            "authors": [
                "Jerry A Fodor",
                "Zenon W Pylyshyn."
            ],
            "title": "Connectionism and cognitive architecture: A critical analysis",
            "venue": "Cognition.",
            "year": 1988
        },
        {
            "authors": [
                "Markus Freitag",
                "Yaser Al-Onaizan."
            ],
            "title": "Beam search strategies for neural machine translation",
            "venue": "Proceedings of the First Workshop on Neural Machine Translation. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Karlis Freivalds",
                "Emils Ozolins",
                "Agris Sostaks."
            ],
            "title": "Neural shuffle-exchange networks - sequence processing in o(n log n) time",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Furrer",
                "Marc van Zee",
                "Nathan Scales",
                "Nathanael Sch\u00e4rli."
            ],
            "title": "Compositional generalization in semantic parsing: Pre-training vs",
            "venue": "specialized architectures. arXiv preprint arXiv:2007.08970.",
            "year": 2020
        },
        {
            "authors": [
                "Demi Guo",
                "Yoon Kim",
                "Alexander Rush."
            ],
            "title": "Sequence-level mixed sample data augmentation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Yinuo Guo",
                "Zeqi Lin",
                "Jian-Guang Lou",
                "Dongmei Zhang."
            ],
            "title": "Hierarchical poset decoding for compositional generalization in language",
            "venue": "Advances in Neural Information Processing Systems. Curran Associates, Inc.",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu He",
                "Xu Tan",
                "Yingce Xia",
                "Di He",
                "Tao Qin",
                "Zhibo Chen",
                "Tie-Yan Liu."
            ],
            "title": "Layer-wise coordination between encoder and decoder for neural machine translation",
            "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Spanbased semantic parsing for compositional generalization",
            "venue": "Proceedings of the 59th Annual Meeting",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Herzig",
                "Peter Shaw",
                "Ming-Wei Chang",
                "Kelvin Guu",
                "Panupong Pasupat",
                "Yuan Zhang."
            ],
            "title": "Unlocking compositional generalization in pre-trained models using intermediate representations",
            "venue": "arXiv.",
            "year": 2021
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural Comput.",
            "year": 1997
        },
        {
            "authors": [
                "Yichen Jiang",
                "Mohit Bansal."
            ],
            "title": "Inducing transformer\u2019s compositional generalization ability via auxiliary sequence prediction tasks",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Vir-",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Brenden Lake",
                "Marco Baroni."
            ],
            "title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
            "venue": "International conference on machine learning. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "Brenden M. Lake."
            ],
            "title": "Compositional generalization through meta sequence-to-sequence learning",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14,",
            "year": 2019
        },
        {
            "authors": [
                "Brenden M Lake",
                "Tomer D Ullman",
                "Joshua B Tenenbaum",
                "Samuel J Gershman."
            ],
            "title": "Building machines that learn and think like people",
            "venue": "Behavioral and brain sciences.",
            "year": 2017
        },
        {
            "authors": [
                "Bei Li",
                "Ziyang Wang",
                "Hui Liu",
                "Yufan Jiang",
                "Quan Du",
                "Tong Xiao",
                "Huizhen Wang",
                "Jingbo Zhu."
            ],
            "title": "Shallow-to-deep training for neural machine translation",
            "venue": "arXiv preprint arXiv:2010.03737.",
            "year": 2020
        },
        {
            "authors": [
                "Qing Li",
                "Yixin Zhu",
                "Yitao Liang",
                "Ying Nian Wu",
                "Song-Chun Zhu",
                "Siyuan Huang."
            ],
            "title": "Neuralsymbolic recursive machine for systematic generalization",
            "venue": "CoRR, abs/2210.01603.",
            "year": 2022
        },
        {
            "authors": [
                "Yafu Li",
                "Yongjing Yin",
                "Yulong Chen",
                "Yue Zhang."
            ],
            "title": "On compositional generalization of neural machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Yuanpeng Li",
                "Liang Zhao",
                "Jianyu Wang",
                "Joel Hestness."
            ],
            "title": "Compositional generalization for primitive substitutions",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference",
            "year": 2019
        },
        {
            "authors": [
                "Zhaoyi Li",
                "Ying Wei",
                "Defu Lian."
            ],
            "title": "Learning to substitute spans towards improving compositional generalization",
            "venue": "arXiv preprint arXiv:2306.02840.",
            "year": 2023
        },
        {
            "authors": [
                "Lei Lin",
                "Shuangtao Li",
                "Xiaodong Shi."
            ],
            "title": "Leapt: Learning adaptive prefix-to-prefix translation for simultaneous machine translation",
            "venue": "ICASSP 2023 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135.",
            "year": 2023
        },
        {
            "authors": [
                "Chenyao Liu",
                "Shengnan An",
                "Zeqi Lin",
                "Qian Liu",
                "Bei Chen",
                "Jian-Guang Lou",
                "Lijie Wen",
                "Nanning Zheng",
                "Dongmei Zhang."
            ],
            "title": "Learning algebraic recombination for compositional generalization",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Fenglin Liu",
                "Xuancheng Ren",
                "Guangxiang Zhao",
                "Xu Sun",
                "Liangyou Li."
            ],
            "title": "Layer-wise crossview decoding for sequence-to-sequence learning",
            "venue": "ArXiv preprint, abs/2005.08081.",
            "year": 2020
        },
        {
            "authors": [
                "Qian Liu",
                "Shengnan An",
                "Jian-Guang Lou",
                "Bei Chen",
                "Zeqi Lin",
                "Yan Gao",
                "Bin Zhou",
                "Nanning Zheng",
                "Dongmei Zhang."
            ],
            "title": "Compositional generalization by learning analytical expressions",
            "venue": "Advances in Neural Information Processing Systems 33: An-",
            "year": 2020
        },
        {
            "authors": [
                "Qian Liu",
                "Shengnan An",
                "Jian-Guang Lou",
                "Bei Chen",
                "Zeqi Lin",
                "Yan Gao",
                "Bin Zhou",
                "Nanning Zheng",
                "Dongmei Zhang."
            ],
            "title": "Compositional generalization by learning analytical expressions",
            "venue": "Advances in Neural Information Processing Systems 33: An-",
            "year": 2020
        },
        {
            "authors": [
                "Shan Liu",
                "Yafang Zheng",
                "Lei Lin",
                "Yidong Chen",
                "Xiaodong Shi."
            ],
            "title": "A novel pos-guided data augmentation method for sign language gloss translation",
            "venue": "CCF International Conference on Natural Language Processing and Chinese Computing, pages",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Sarthak Mittal",
                "Sharath Chandra Raparthy",
                "Irina Rish",
                "Yoshua Bengio",
                "Guillaume Lajoie."
            ],
            "title": "Compositional attention: Disentangling search and retrieval",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-",
            "year": 2022
        },
        {
            "authors": [
                "Maxwell I. Nye",
                "Armando Solar-Lezama",
                "Josh Tenenbaum",
                "Brenden M. Lake."
            ],
            "title": "Learning compositional rules via neural program synthesis",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Process-",
            "year": 2020
        },
        {
            "authors": [
                "Myle Ott",
                "Sergey Edunov",
                "Alexei Baevski",
                "Angela Fan",
                "Sam Gross",
                "Nathan Ng",
                "David Grangier",
                "Michael Auli."
            ],
            "title": "fairseq: A fast, extensible toolkit for sequence modeling",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 - November 1, 2018. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.",
            "year": 2020
        },
        {
            "authors": [
                "Alessandro Raganato",
                "J\u00f6rg Tiedemann"
            ],
            "title": "An analysis of encoder representations in transformerbased machine translation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
            "year": 2018
        },
        {
            "authors": [
                "Vikas Raunak",
                "Vaibhav Kumar",
                "Florian Metze."
            ],
            "title": "On compositionality in neural machine translation",
            "venue": "ArXiv preprint.",
            "year": 2019
        },
        {
            "authors": [
                "Laura Ruis",
                "Brenden M. Lake."
            ],
            "title": "Improving systematic generalization through modularity and augmentation",
            "venue": "CoRR, abs/2202.10745.",
            "year": 2022
        },
        {
            "authors": [
                "Jake Russin",
                "Jason Jo",
                "Randall C O\u2019Reilly",
                "Yoshua Bengio"
            ],
            "title": "Compositional generalization in a deep seq2seq model by separating syntax and semantics. arXiv preprint arXiv:1904.09708",
            "year": 2019
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for",
            "year": 2016
        },
        {
            "authors": [
                "Yanyao Shen",
                "Xu Tan",
                "Di He",
                "Tao Qin",
                "Tie-Yan Liu."
            ],
            "title": "Dense information flow for neural machine translation",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2018
        },
        {
            "authors": [
                "David R. So",
                "Quoc V. Le",
                "Chen Liang."
            ],
            "title": "The evolved transformer",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA.",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V Le."
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "Advances in neural information processing systems.",
            "year": 2014
        },
        {
            "authors": [
                "Tristan Thrush."
            ],
            "title": "Compositional neural machine translation by removing the lexicon from syntax",
            "venue": "arXiv preprint arXiv:2002.08899.",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems.",
            "year": 2017
        },
        {
            "authors": [
                "Elena Voita",
                "Rico Sennrich",
                "Ivan Titov."
            ],
            "title": "The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Qiang Wang",
                "Bei Li",
                "Tong Xiao",
                "Jingbo Zhu",
                "Changliang Li",
                "Derek F. Wong",
                "Lidia S. Chao."
            ],
            "title": "Learning deep transformer models for machine translation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Qiang Wang",
                "Fuxue Li",
                "Tong Xiao",
                "Yanyang Li",
                "Yinqiao Li",
                "Jingbo Zhu."
            ],
            "title": "Multi-layer representation fusion for neural machine translation",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 3015\u20133026, Santa Fe,",
            "year": 2018
        },
        {
            "authors": [
                "Weiwen Xu",
                "Ai Ti Aw",
                "Yang Ding",
                "Kui Wu",
                "Shafiq Joty."
            ],
            "title": "Addressing the vulnerability of NMT in input perturbations",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the",
            "year": 2021
        },
        {
            "authors": [
                "Jingfeng Yang",
                "Le Zhang",
                "Diyi Yang."
            ],
            "title": "SUBS: subtree substitution for compositional semantic parsing",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo-",
            "year": 2022
        },
        {
            "authors": [
                "Yuekun Yao",
                "Alexander Koller."
            ],
            "title": "Structural generalization is hard for sequence-to-sequence models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December",
            "year": 2022
        },
        {
            "authors": [
                "Yongjing Yin",
                "Yafu Li",
                "Fandong Meng",
                "Jie Zhou",
                "Yue Zhang."
            ],
            "title": "Categorizing semantic representations for neural machine translation",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 5227\u20135239. International",
            "year": 2022
        },
        {
            "authors": [
                "Yongjing Yin",
                "Jiali Zeng",
                "Yafu Li",
                "Fandong Meng",
                "Jie Zhou",
                "Yue Zhang."
            ],
            "title": "Consistency regularization training for compositional generalization",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2023
        },
        {
            "authors": [
                "Hao Zheng",
                "Mirella Lapata."
            ],
            "title": "Compositional generalization via semantic tagging",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pages 1022\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Hao Zheng",
                "Mirella Lapata."
            ],
            "title": "Disentangled sequence to sequence learning for compositional generalization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Hao Zheng",
                "Mirella Lapata."
            ],
            "title": "Realworld compositional generalization with disentangled sequence-to-sequence learning",
            "venue": "arXiv preprint arXiv:2212.05982.",
            "year": 2022
        },
        {
            "authors": [
                "Yafang Zheng",
                "Lei Lin",
                "Zhaohong Lai",
                "Binling Wang",
                "Shan Liu",
                "Biao Fu",
                "Wenhao Rao",
                "Peigen Ye",
                "Yidong Chen",
                "Xiaodong Shi."
            ],
            "title": "Layer-wise representation fusion for compositional generalization",
            "venue": "arXiv preprint arXiv:2307.10799.",
            "year": 2023
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Claire Cui",
                "Olivier Bousquet",
                "Quoc V Le",
                "Ed H. Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "The",
            "year": 2023
        },
        {
            "authors": [
                "Li"
            ],
            "title": "We train all COMPOSITION models from scratch. For CFQ, we use the base RoBERTa with 12 encoder layers, which is combined with a Transformer decoder that has 2 decoder layers with",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Recent studies have shown that sequence-tosequence (seq2seq) models struggle with compositional generalization (CG), i.e., the ability to systematically generalize to unseen compositions of seen components. There is mounting evidence that one of the reasons hindering CG is the representation of the encoder uppermost layer is entangled, i.e., the syntactic and semantic representations of sequences are entangled. However, we consider that the previously identified representation entanglement problem is not comprehensive enough. Additionally, we hypothesize that the source keys and values representations passing into different decoder layers are also entangled. Starting from this intuition, we propose COMPOSITION (Compose Syntactic and Semantic Representations), an extension to seq2seq models which learns to compose representations of different encoder layers dynamically for different tasks, since recent studies reveal that the bottom layers of the Transformer encoder contain more syntactic information and the top ones contain more semantic information. Specifically, we introduce a composed layer between the encoder and decoder to compose different encoder layers\u2019 representations to generate specific keys and values passing into different decoder layers. COMPOSITION achieves competitive results on two comprehensive and realistic benchmarks, which empirically demonstrates the effectiveness of our proposal. Codes are available at https://github. com/thinkaboutzero/COMPOSITION."
        },
        {
            "heading": "1 Introduction",
            "text": "A crucial property of human language learning is its compositional generalization (CG) \u2014 the algebraic ability to understand and produce a potentially infinite number of novel combinations from known components (Fodor and Pylyshyn, 1988; Lake et al.,\n\u2217Equal Contribution. \u2020Corresponding Author.\nKnown Components\nSemantic Syntactic lost the dog\nhe liked \u4ed6\n\u4e22\u5931\nI got it !\nPattern 2.3: V + DET + N + MOD\n\u559c\u6b22 ta xihuan\n\u4e86 \u72d7 diushi le gou\nHow?\nNovel Compound lost the dog he liked\nSemantic + Syntactic gou \u4e22\u5931 diushi de \u4e86 \u4ed6 \u559c\u6b22 \u7684 \u72d7 le ta xihuan\nFigure 1: Examples from CoGnition (Li et al., 2021) show the workflow of how humans exhibit CG. Suppose interpreters know the translation: [\u4e22\u5931\u4e86\u72d7] for \u201clost the dog\u201d and [\u4ed6\u559c\u6b22] for \u201che liked\u201d (semantic information). When they first encounter \u201clost the dog he liked\u201d, they can correctly translate [\u4e22\u5931\u4e86\u4ed6\u559c\u6b22\u7684 \u72d7] instead of [\u4e22\u5931\u4e86\u72d7\u4ed6\u559c\u6b22] depending on Pattern 2.3 (syntactic information).\n2017). For example, if a person knows \u201cthe doctor watches a movie\u201d [\u533b\u751f\u770b\u7535\u5f71]1 and \u201cthe lawyer\u201d [\u5f8b\u5e08], then it is natural for the person to know the translation of \u201cthe lawyer watches a movie\u201d is [\u5f8b\u5e08\u770b\u7535\u5f71] even though they have never seen it before. Such nature is beneficial for generalizing to new compositions of previously observed elements, which is often required in real-world scenarios.\nDespite astonishing successes across a broad range of natural language understanding and generation tasks (Sutskever et al., 2014; Dong and Lapata, 2016; Vaswani et al., 2017), neural network models, in particular the very popular sequence-tosequence (seq2seq) architecture, are argued difficult to capture the compositional structure of human language (Lake and Baroni, 2018; Keysers et al., 2020; Li et al., 2021). A key reason for failure on CG is different semantic factors (e.g., lexical meaning and syntactic patterns) required by CG are entangled, which was proved explicitly or implicitly to exist in the representation of the encoder uppermost layer (encoder entanglement\n1The sentence in \u201c[]\u201d denotes the Chinese translation.\nproblem) by previous studies (Li et al., 2019; Raunak et al., 2019; Russin et al., 2019; Liu et al., 2020b, 2021; Jiang and Bansal, 2021; Zheng and Lapata, 2022a; Yin et al., 2022; Ruis and Lake, 2022; Li et al., 2022; Cazzaro et al., 2023). In other words, the syntactic and semantic representations of sequences are entangled.\nIn order to alleviate the encoder entanglement problem, one line of research on CG mainly concentrate on improving the encoder representation or separating the learning of syntax and semantics which adopt similar approaches to humans\u2019 strategies for CG (see Figure 1). Specifically, several works either produce two separate syntactic and semantic representations, and then compose them (Li et al., 2019; Russin et al., 2019; Jiang and Bansal, 2021) or design external modules, and then employ a multi-stage generation process (Liu et al., 2020b, 2021; Ruis and Lake, 2022; Li et al., 2022; Cazzaro et al., 2023). Moreover, some studies explore bag-of-words pre-training (Raunak et al., 2019), newly decoded target context (Zheng and Lapata, 2022a,b) or prototypes of token representations over the training set (Yin et al., 2022) to improve the encoder representation. Furthermore, we hypothesize that the source keys and values representations passing into different decoder layers are also entangled (keys, values entanglement problem), not just the representation of the encoder uppermost layer. We will further illustrate it in Section 5.1.\nTherefore, one natural question can be raised: how to alleviate keys, values entanglement problem. As a remedy, we examine CG from a new perspective to solve it, i.e., utilizing different encoder layers\u2019 information. We conduct preliminary analysis provided in Appendix A, and conclude that the bottom layers of the Transformer encoder contain more syntactic information and the top ones contain more semantic information. Inspired by this, we collect representations outputted by each encoder layer instead of separating the learning of syntax and semantics. So one intuitive solution to solve keys, values entanglement problem is to learn different and specific combinations of syntactic and semantic information (i.e., representations outputted by each encoder layer) for keys and values of different decoder layers. We argue that an effective composition is to provide different combinations for different tasks and a specific combination for a particular task. For example, the model can learn preference of layers in different levels\nof the encoder for different tasks (i.e., For A task, the information at encoder layer 0 may be more important, however, for B task, the information at encoder layer 5 may be more important). Additionally, the model can select which encoder layer of information is most suitable for itself (that is, which encoder layer of information is the most important) for a particular task. Inspired by that, we propose the composed layer (learnable scalars or vectors) to generate different specific source keys and values passing into different decoder layers for different particular tasks, since we argue that the learned scalars or vectors (i.e., different dynamic composition modes) by the model itself during training process can be dynamically adjusted for different particular tasks, and provide a way to learn preference of layers in different levels of the encoder for a particular task. Putting everything together, we propose COMPOSITION (Compose Syntactic and Semantic Representations), an extension to seq2seq models that learns to compose the syntactic and semantic representations of sequences dynamically for different tasks. COMPOSITION is simple yet effective, and mostly applicable to any seq2seq models without any dataset or task-specific modification.\nExperimental results on CFQ (Keysers et al., 2020) (semantic parsing) and CoGnition (Li et al., 2021) (machine translation, MT) empirically show that our method can improve generalization performance, outperforming competitive baselines and other techniques. Notably, COMPOSITION achieves 19.2% and 50.2% (about 32%, 20% relative improvements) for instance-level and aggregate-level error reduction rates on CoGnition. Extensive analyses demonstrate that composing the syntactic and semantic representations of sequences dynamically for different tasks leads to better generalization results."
        },
        {
            "heading": "2 Related Work",
            "text": "Compositional Generalization. After realizing existing neural models still struggle in scenarios requiring CG (Lake and Baroni, 2018; Keysers et al., 2020; Li et al., 2021), there have been various studies attempt to improve the model\u2019s ability of CG, including data augmentation (Andreas, 2020; Aky\u00fcrek et al., 2021; Yang et al., 2022; Li et al., 2023), modifications on model architecture (Li et al., 2019; Russin et al., 2019; Nye et al., 2020; Liu et al., 2020c, 2021; Zheng and Lapata, 2021;\nHerzig and Berant, 2021; Chaabouni et al., 2021; Mittal et al., 2022; Zheng et al., 2023), intermediate representations (Furrer et al., 2020; Herzig et al., 2021), meta-learning (Lake, 2019; Conklin et al., 2021), explorations on pre-trained language models (Furrer et al., 2020; Zhou et al., 2023), auxiliary objectives (Jiang and Bansal, 2021; Yin et al., 2023), two representations (Li et al., 2019; Russin et al., 2019; Jiang and Bansal, 2021) and enriching the encoder representation (Raunak et al., 2019; Zheng and Lapata, 2022a,b; Yin et al., 2022; Yao and Koller, 2022). One line of research exploring how to alleviate the encoder entanglement problem has attracted much attention. Our work is in line with it, but we examine CG from a new perspective, i.e., utilizing different encoder layers\u2019 information.\nNeural Machine Translation. Recently, CG and robustness of Neural Machine Translation (NMT) have gained much attention from the research community (Cheng et al., 2020; Xu et al., 2021; Lake and Baroni, 2018; Li et al., 2021), including pre-training (Raunak et al., 2019), data augmentation (Guo et al., 2020a), datasets (Li et al., 2021), and enriching semantic information at tokenlevel (Thrush, 2020; Akyurek and Andreas, 2021; Zheng and Lapata, 2022a,b; Yin et al., 2022). Noteworthily, Dankers et al. (2022) argue that MT is a suitable and relevant testing ground to test CG in natural language. Different from them, we introduce a composed layer to compose different encoder layers\u2019 information dynamically, which is inspired by previous studies about analyzing Transformer (Raganato et al., 2018; Voita et al., 2019).\nEncoder Layer Fusion. Encoder layer fusion (EncoderFusion) is a technique to fuse all the encoder layers (instead of the uppermost layer) for seq2seq models, which has been proven beneficial, such as layer attention (Bapna et al., 2018; Shen et al., 2018; Wang et al., 2019), layer aggregation (Dou et al., 2018; Wang et al., 2018; Dou et al., 2019), and layer-wise coordination (He et al., 2018; Liu et al., 2020a). However, other studies show that exploiting low-layer encoder representations fails to improve model performance (Domhan, 2018). The essence of different EncoderFusion works is to explore different ways to combine information from different encoder layers. Our approach is essentially the same as EncoderFusion work, which explores different ways to combine information from different encoder layers, however, we propose a new way to combine them. Meanwhile, we\nconsider that there are also three distinct differences. Firstly, our method exploits information from all encoder sub-layers and generates specific keys, values passing into different decoder layers while they do not. Secondly, our method shows the effectiveness of utilizing low-layer encoder representations while they have the opposite view (see Appendix D). Thirdly, we do not share the same motivation or task. Their work focuses on how to transform information across layers in deep neural network scenarios for seq2seq tasks. Our motivation is to compose the syntactic and semantic representations of sequences dynamically for CG."
        },
        {
            "heading": "3 Methodology",
            "text": "We adopt the Transformer architecture (Vaswani et al., 2017) to clarify our method, however, our proposed method is mostly applicable to any seq2seq models. In the following, we first introduce the Transformer baseline (Section 3.1), and then our proposed COMPOSITION (Section 3.2)."
        },
        {
            "heading": "3.1 Transformer",
            "text": "The Transformer (Vaswani et al., 2017) is designed for sequence to sequence tasks which adopts an encoder-decoder architecture. The multi-layer encoder summarizes a source sequence into a contextualized representation and another multi-layer decoder produces the target sequence conditioned on the encoded representation.\nFormally, given a sequence of source sentence X = {x1, . . . , xS} and a sequence of target sentence Y = {y1, . . . , yT }, where S, T denote the number of source and target tokens, respectively. D = {(X,Y ), . . .} denotes a training corpus, V denotes the vocabulary of D, and \u03b8 denotes parameters of the Transformer model. The model aims to estimate the conditional probability p(y1, . . . , yT |x1, . . . , xS):\np(Y |X; \u03b8) = T+1\u220f t=1 p(yt|y<t, X; \u03b8), (1)\nwhere t is the index of each time step, y<t denotes a prefix of Y and each factor p(yt|X, y1, . . . , yt\u22121; \u03b8) is defined as a softmax distribution of V .\nDuring training, the model are generally optimized with the cross-entropy (CE) loss, which is calculated as follows:\nLCE(\u03b8) = \u2212 T+1\u2211 t=1 log p(yt|y<t, X; \u03b8). (2)\nTransformer. The bright yellow block in the middle denotes the composed layer introduced in Section 3.2. The red line denotes that we collect representations of the same positions for the rest encoder layers.\nDuring inference, the model predicts the probabilities of target tokens in an auto-regressive mode and generates target sentences using a heuristic search algorithm, such as beam search (Freitag and Al-Onaizan, 2017)."
        },
        {
            "heading": "3.2 COMPOSITION",
            "text": "Our proposed COMPOSITION extends the Transformer by introducing a composed layer between the encoder and decoder. Figure 2 shows the overall architecture of our approach."
        },
        {
            "heading": "3.2.1 Composed Layer",
            "text": "The composed layer is a list consisting of 2N learnable vectors due to 2N source keys, values passing into N decoder layers, where each vector involves 2M learnable scalars or vectors. M,N denote the number of encoder and decoder layers respectively."
        },
        {
            "heading": "3.2.2 Dynamic Combination",
            "text": "Here, we describe how to use the composed layer to compose collected representations dynamically for generating specific keys and values representations passing into different decoder layers. Let fSelf\u2212Attention and fFeed\u2212Forward denote a Transformer self-attention sub-layer and feed-forward sub-layer respectively. The embedding layer of\nthe Transformer encoder first maps X to embeddings H0, and then H0 are fed into a Transformer self-attention sub-layer and feed-forward sub-layer to generate HSA1 \u2208 Rd\u00d7S , HFF1 \u2208 Rd\u00d7S respectively, where d denotes the hidden size. Next, each subsequent encoder layer takes the previous layer\u2019s output as input. The overall process is as follows:\nHSA1 = fSelf\u2212Attention(H0), (3)\nHFF1 = fFeed\u2212Forward(H SA 1 ), (4)\nHSAi = fSelf\u2212Attention(H FF i\u22121), (5)\nHFFi = fFeed\u2212Forward(H SA i\u22121), (6)\nwhere 2 \u2264 i \u2264 M denote i-th encoder layer. Therefore, we can collect representations outputted by each encoder sub-layer Hcollect = {HSA1 , HFF1 , . . . ,HSAM , HFFM }. The keys and values of multi-head attention module of decoder layer l are defined to be:\nH lkey = 2M\u2211 i=1 wikH i collect, (7)\nH lvalue = 2M\u2211 i=1 wivH i collect, (8)\nwhere wik \u2208 R, wiv \u2208 R are learnable scalars or vectors and mutually different (e.g. wik \u0338= wiv, wik \u0338= w j k and w i v \u0338= w j v), which weight each collected source representation in a dynamic linear manner. Eq. 7 and 8 provide a way to learn preference of sub-layers in different levels of the encoder."
        },
        {
            "heading": "4 Experiments",
            "text": "We mainly evaluate COMPOSITION on two comprehensive and realistic benchmarks for measuring CG, including CFQ (Keysers et al., 2020) and CoGnition (Li et al., 2021)."
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Datasets. CoGnition is a recently released realistic English \u2192 Chinese (En\u2192Zh) translation dataset, which is used to systematically evaluate CG in MT scenarios. It consists of a training set of 196,246 sentence pairs, a validation set and a test set of 10,000 samples. In particular, it also has a dedicated synthetic test set (i.e., CG-test set) consisting of 10,800 sentences containing novel compounds, so that the ratio of compounds that are correctly translated can be computed to evaluate the model\u2019s\nability of CG directly. CFQ is automatically generated from a set of rules in a way that precisely tracks which rules (atoms) and rule combinations (compounds) of each example. In this way, we can generate three splits with maximum compound divergence (MCD) while guaranteeing a small atom divergence between train and test sets, where large compound divergence denotes the test set involves more examples with unseen syntactic structures. We evaluate our method on all three splits. Each split dataset consists of a training set of 95,743, a validation set and a test set of 11,968 examples. Figure 3 shows examples of them. Data Preprocess. We follow the same settings of Li et al. (2021) and Keysers et al. (2020) to preprocess CoGnition and CFQ datasets separately. For CoGnition, we use an open-source Chinese tokenizer2 to preprocess Chinese and apply Moses tokenizer3 to preprocess English, which is the same in Lin et al. (2023) and Liu et al. (2023). We employ byte-pair encoding (BPE) (Sennrich et al., 2016) for Chinese with 3,000 merge operations, generating a vocabulary of 5,500 subwords. We do not apply BPE for English due to the small vocabulary (i.e., 2000). For CFQ, we use the GPT2-BPE\n2https://github.com/fxsjy/jieba 3https://github.com/moses-smt/mosesdecoder/\nblob/master/scripts/tokenizer/tokenizer.perl\ntokenizer4 to preprocess source and target English text. Setup. For CoGnition and CFQ, we follow the same experimental settings and configurations of Li et al. (2021) and Zheng and Lapata (2022a) repspectively. We implement all comparison models and COMPOSITION with an open source Fairseq toolkit (Ott et al., 2019). More details are provided in Appendix B. Evaluation Metrics. For CoGnition, we use compound translation error rate (CTER (Li et al., 2021)) to measure the model\u2019s ability of CG. Specifically, instance-level CTER denotes the ratio of samples where the novel compounds are translated incorrectly, and aggregate-level CTER denotes the ratio of samples where the novel compounds suffer at least one incorrect translation when aggregating all 5 contexts. To calculate CTER, Li et al. (2021) manually construct a dictionary for all the atoms based on the training set, since each atom contains different translations. We also report characterlevel BLEU scores (Papineni et al., 2002) using SacreBLEU (Post, 2018) as a supplement. For CFQ, we use exact match accuracy to evaluate model performance, where natural language utterances are mapped to meaning representations."
        },
        {
            "heading": "4.2 Model Settings",
            "text": "Machine Translation. We compare our method with previous competitive systems: (1) Transformer (Vaswani et al., 2017): first proposes a new\n4https://github.com/facebookresearch/fairseq/ blob/main/examples/roberta/multiprocessing_bpe_ encoder.py\nencoder-decoder architecture based solely on attention mechanisms; (2) Transformer-Rela: only replaces sinusoidal (absolute) positional embedding with a relative one; (3) Transformer-Small: only decreases the number of encoder layers and decoder layers to 4, 4 respectively; (4) Transformer-Deep: only increases the number of encoder layers to 8; (5) Bow (Raunak et al., 2019): uses bag-of-words pre-training to improve the representation of the encoder upmost layer; (6) SeqMix (Guo et al., 2020a): synthesizes examples to encourage compositional behavior; (7) Dangle (Zheng and Lapata, 2022a): adaptively re-encodes (at each time step) the source input to disentangle the representation of the encoder upmost layer;5 (8) Proto-Transformer (Yin et al., 2022): integrates prototypes of token representations over the training set into the source encoding to achieve the goal of categorization; (9) Transformer+CReg (Yin et al., 2023): promotes representation consistency across samples and prediction consistency for a single sample; (10) RDanglesep (Zheng and Lapata, 2022b): disentangles their representations and only re-encode keys periodically, at some interval; (11) DLCL (Wang et al., 2019): proposes an approach based on dynamic linear combination of layers (DLCL), and is one of the very popular EnocderFusion work. Our method is built on top of (1)-(4), i.e., COMPOSITION, COMPOSITION-Rela, COMPOSITIONSmall and COMPOSITION-Deep. We also provide reasons for experiments on CoGnition without language models (see Appendix E). Semantic Parsing. We compare our method with previous competitive systems: (1) LSTM + atten-\n5We use the same variant reported by Zheng and Lapata (2022a) (i.e., Dangle-EncDec (abs)) with sinusoidal (absolute) positional embedding.\ntion: introduces attention mechanism (Bahdanau et al., 2015) in LSTM (Hochreiter and Schmidhuber, 1997); (2) Transformer (Vaswani et al., 2017); (3) Universal Transformer (Dehghani et al., 2019): combines the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs; (4) Evolved Transformer (So et al., 2019): uses wide depth-wise separable convolutions in the early layers of both the encoder and decoder; (5) CGPS (Li et al., 2019): leverages prior knowledge of compositionality with two representations, and adds entropy regularization to the encodings; (6) NSEN (Freivalds et al., 2019): is derived from the Shuffle-Exchange network; (7) T5-11B (Raffel et al., 2020): treats every natural language processing task as a text-to-text problem, and is therefore suitable for the semantic parsing tasks. T5-11B is a T5 model with 11B parameters finetuned on CFQ; (8) T5-11B-mod (Furrer et al., 2020): shows that using masked language model (MLM) pre-training together with an intermediate representation leads to significant improvements in performance; (9) RoBERTa (Liu et al., 2019): makes use of the RoBERTa-base model as the encoder and the randomly initialized Transformer decoder trained from scratch, where we use the same experimental settings of (Zheng and Lapata, 2022a); (10) HPD (Guo et al., 2020b): proposes a novel hierarchical partially ordered set (poset) decoding paradigm, which consists of three components: sketch prediction, primitive prediction, and traversal path prediction; (11) Dangle (Zheng and Lapata, 2022a); (12) RoBERTa+CReg (Yin et al., 2023); (13) COMPOSITION: builds on (9) with our method."
        },
        {
            "heading": "4.3 Results on CoGnition",
            "text": "The main results on CoGnition are shown in Table 1. We observe that: (1) COMPOSITION gives 20.4% CTERInst and 52.0% CTERAggr, with a\nsignificant improvement of 8.0% and 10.9% accordingly compared to the Transformer. Moreover, COMPOSITION significantly outperforms most baseline models under the almost same parameter settings,6 indicating composing the syntactic and semantic information of sequences dynamically for a particular task is more beneficial to CG. Although Transformer+CReg achieves slightly better performance and contains fewer parameters, it is more complex and costly compared with COMPOSITION; (2) COMPOSITION, COMPOSITIONRela, COMPOSITION-Small and COMPOSITIONDeep can deliver various performance improvements, demonstrating the general effectiveness of our method; (3) COMPOSITION-Deep performs better than Bow, Dangle and Proto-Transformer, indicating that focusing on alleviating the encoder entanglement problem only can achieve part of goals of CG as mentioned in Section 1. Compared to SeqMix, the improvement of COMPOSITION is more significant (2.3% vs 10.9% CTERAggr). SeqMix utilizes linear interpolation in the input embedding space to reduce representation sparsity, and we suppose that the samples synthesized randomly may be unreasonable and harmful to model training; (4) It can be seen that Transformer is even slightly better than DLCL, indicating DLCL and COMPOSITION do not share the same motivation or scenario."
        },
        {
            "heading": "4.4 Results on CFQ",
            "text": "The main results on CFQ are presented in Table 2. We observe that: (1) RoBERTa is comparable to T511B, T5-11B-mod and outperforms other baseline systems without pre-training except HPD, indicating that pre-training indeed benefits CFQ; (2) COM-\n6We implement our approach based on Transformer-Deep for a fair comparison with Proto-Transformer.\nPOSITION substantially boosts the performance of RoBERTa (43.4 \u2192 59.4), about 37% relative improvements, and is in fact superior to T5-11B and T5-11B-mod. It also outperforms other baseline systems without pre-training except HPD. This result demonstrates that pre-training as a solution to CG also has limitations, and also indicates that COMPOSITION is complementary to pre-trained models; (3) HPD performs better than Dangle, RoBERTa+CReg and COMPOSITION, achieving 67.3 exact match accuracy, which is highly optimized for the CFQ dataset. On the contrary, COMPOSITION, RoBERTa+CReg and Dangle are generally applicable to any seq2seq models for solving any seq2seq tasks including MT, as mentioned in Section 4.3. However, compared with competitive performance on CoGnition, the improvements brought by COMPOSITION is relatively moderate, and even worse than Dangle. The underlying reason is related to a recent finding that compositionality in natural language is much more complex than the rigid, arithmetic-like operations (Li et al., 2021; Zheng and Lapata, 2022a; Dankers et al., 2022). MT is paradigmatically close to the tasks typically considered for testing compositionality in natural language, while our approach is more suitable for dealing with such scenarios."
        },
        {
            "heading": "5 Analysis",
            "text": "In this section, we conduct in-depth analyses of COMPOSITION to provide a comprehensive understanding of the individual contributions of each component. For all experiments, we train a COMPOSITION (6-6 encoder and decoder layers) instead of other experimental settings on the CoGnition dataset, unless otherwise specified."
        },
        {
            "heading": "5.1 Effects of Specific Keys and Values of Different Decoder Layers",
            "text": "As mentioned in Section 1 and 3.2, we hypothesize that keys, values entanglement problem ex-\nists.7 It is clear that our hypothesized keys, values entanglement problem is an extension to encoder entanglement problem. We show curiosity about whether this problem exists, and COMPOSITION can alleviate it. In this experiment, we investigate its influence on CoGnition. As shown in Table 3, we observe certain improvements (-5.8% and -8.0% CTERInst, -7.8% and -10.9% CTERAggr) when separately alleviating the encoder or keys, values entanglement problem.8 It suggests that our method can alleviate both problems separately, and learning to compose information of different encoder layers dynamically can improve CG performance. Furthermore, the improvement brought from alleviating keys, values entanglement problem is more significant than that brought from alleviating encoder entanglement problem (52.0% vs 55.1% CTERAggr), demonstrating the reasonableness of keys, values entanglement problem.\nTo further illustrate the reasonableness of keys, values entanglement problem and understand how COMPOSITION alleviates it, we visualize the learned composition weights of COMPOSITION after normalized.9 Specifically, we train COMPOSITION on CoGnition and then extract W ik,W i v (see Section 3.2.2) to visualize them. Ideally, each key or value of different decoder layers should pay different attention weights to different encoder layers\u2019 information. As shown in Figure 4, the learned composition weights (after normalized) are mutually distinct for keys and values of different decoder layers, which implies COMPOSITION learns different dynamic composition modes for keys and values of every decoder layer respectively. In addition, it also indicates the reasonableness of keys, values entanglement problem we proposed, since keys and\n7It is noteworthy that the representation of the encoder upmost layer serves as the same key and value passing into every decoder layer in the Transformer.\n8We use one or 2N learnable vectors to generate one or 2N representations passing into N decoder layers.\n9We only use representations outputted by Eq. 6 for brevity.\nvalues of different decoder layers utilize more than just the information of the encoder topmost layer. More importantly, it also emphasizes our method provides an effective composition of syntactic and semantic information, i.e., a specific combination for a particular task. To further demonstrate it, we also provide a toy experiment in Appendix C."
        },
        {
            "heading": "5.2 Effects of Composing Information of Encoder Layers or Sub-layers",
            "text": "As mentioned in Section 3, the Transformer encoder layer consists of two sub-layers. We assume that sub-layers may contain language information in different aspects, which may produce better generalization results. Therefore, we are curious about whether composing different encoder layers\u2019 or sub-layers\u2019 information is more beneficial to CG. In this experiment, we investigate its influence on CoGnition. Specifically, we train COMPOSITION to compose representations outputted by either Eq. 5 or 6 or a combination of both dynamically. Results are presented in Table 4. We observe certain improvements (-6.2% and -5.8% CTERInst) when separately composing SA- and FF-level representations, where SA and FF denote representations outputted by Eq. 5 and 6 respectively. Furthermore, the combination of both them brings further improvement (-8.0% CTERInst), which illustrates that the information in different encoder sub-layers is complementary and has cumulative gains. It also suggests that syntactic and semantic information brought by SA or FF is similar, but slightly different (Li et al., 2020), and can improve generalization performance respectively. It can be seen that the results of COMPOSITION-SA and COMPOSITIONFF presented in Table 4 are basically the same, and the improvements brought by the combination of both them is relatively moderate."
        },
        {
            "heading": "5.3 Effects on Compositional Generalization",
            "text": "Compound Length and Context Length. Longer compounds have more complex semantic information and longer contexts are harder to comprehend, making them more difficult to generalize (Li et al., 2021). We classify the test samples by compound length and context length, and calculate the CTERInst. In Figure 5, we can observe that COMPOSITION generalizes better as the compound and context grows longer compared to Transformer. In particular, COMPOSITION gives a lower CTER by 11.0% over samples when the context length is more longer than 13 tokens. It suggests that our approach can better captures the compositional structure of human language. Complex Modifier. The postpositive modifier atom (MOD) is used to enrich the information of its preceding word (e.g., he liked in the phrase lost the dog he liked), which is challenging to translate due to word reordering from English to Chinese. We divide the test samples into two groups according to compounds with (w/) or without (wo/) MOD. In Figure 6, we observe that the advantage of COMPOSITION grows larger in translating the compounds with MOD, demonstrating its superiority in processing complex semantic composition. Case Study. We present 3 source examples containing a novel compound the waiter he liked with MOD and 4 atoms, and their translations in Table 5. For all samples, correct translations denote that the novel compounds are translated correctly. COMPOSITION correctly translates the novel compounds across different contexts for all samples, while Transformer suffers from omitting different atoms. For example, the translation of the waiter is omitted in the first example, he liked is omitted in the second example and he is omitted in the third example. Our results not only contain the correct compound translations but also achieve better translation quality, while Transformer makes errors on unseen compositions, confirming the necessity of\ncomposing the syntactic and semantic representations of sequences dynamically."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we examine CG from a new perspective, i.e., utilizing different encoder layers\u2019 information. Specifically, we propose an extension to seq2seq models which composes different encoder layers\u2019 representations dynamically to generate specific keys and values passing into different decoder layers. Experiments on CoGnition and CFQ have shown the effectiveness of our proposal on CG without any dataset or task-specific modification. To our knowledge, we are the first to point out a new representation entanglement problem and investigate how to utilize information of different encoder layers benefits CG, achieving promising results on two realistic benchmarks. We hope the work and perspective presented in this paper can inspire future related work on CG.\nLimitations\nThere are two limitations of our approach. Firstly, compared with competitive performance on CoGnition, the improvements brought by COMPOSITION on CFQ is relatively moderate, and even worse than some competitive methods. Hence, COMPOSITION is more suitable for tasks typically considered for testing compositionality in natural language. We strongly recommend researchers pay more attention to tasks evaluating compositionality on natural language. Meanwhile, we regard that designing a more general method that can improve generalization performance in both synthetic and natural scenarios is a promising direction to explore in the future. Secondly, our method is mostly applicable to any seq2seq models which adopt an encoder-decoder architecture instead of encoderonly or decoder-only architecture. However, the methodology of the proposed COMPOSITION is still rather general to any seq2seq models which\nadopt any architecture, since we can use the randomly initialized encoder or decoder to constitute the encoder-decoder architecture."
        },
        {
            "heading": "Acknowledgement",
            "text": "We thank all the anonymous reviewers for their insightful and valuable comments. This work is supported by National key R&D Program of China (Grant no.2022ZD0116101), the Key Support Project of NSFC-Liaoning Joint Foundation (Grant no. U1908216), and the Project of Research and Development for Neural Machine Translation Models between Cantonese and Mandarin (No. WT135-76)."
        },
        {
            "heading": "A Preliminary Analysis",
            "text": "In this section, we analyze the amount of syntactic and semantic information captured by different encoder layers in the Transformer under MT scenarios. We aim at analyzing the representations learned by different encoder layers of different models through probing the encoder as input representation for various prediction tasks. We measure the importance of input features for various tasks by evaluating the ability of the decoder. Specifically, we use a fixed encoder representation as input and two different tasks, i.e., Part-of-Speech (POS) tagging, and Semantic tagging, to evaluate the syntactic and semantic information contained in different encoder layers respectively. The reason is that we assume if the input representation effectively captures a property (syntactic or semantic information), then the decoder can easily predict that property.\nTo explore the precise effects of information captured by different encoder layers, we train the Transformer on the WMT18 English \u2192 Chinese (EnZh, rich-resource), English \u2192 Estonian (EnEt,\nlow-resource)10 by following the same settings of Raganato et al. (2018).11 After training the MT models, we freeze the encoder parameters, and only train one decoder layer12 for each task, since we expect the decoder should not have overly significant impact on the model\u2019s performance of different tasks. We then analyze the amount of syntactic and semantic information in different encoder layers via evaluating the different encoder layers\u2019 performance of corresponding task. We use the Universal Dependencies English Web Treebank v2.0 (Zeman et al., 2017) for POS tagging (syntactic task) and the annotated data from the Parallel Meaning Bank (PMB) (Abzianidze et al., 2017) for Semantic tagging (semantic task).13 We use precision to evaluate model performance.\nResults on POS tagging and Semantic tagging are presented in Figure 7 and 8 respectively. We observe that:\n\u2022 For EnEt and EnZh, the performance tends to decrease as the number of layers increase.\n\u2022 For EnEt and EnZh, the performance tends to increase as the number of layers increase.\nTherefore, we can conclude that the bottom layers of the Transformer encoder contain more\n10We use two datasets with different sizes to analyze information captured by different encoder layers across models with different translation quality and target language.\n11The provided datasets are freely available at https:// www.statmt.org/wmt18/translation-task.html.\n12We follow the same settings of Raganato et al. (2018) and adopt one attention head and one feed-forward sub-layer to consitute the decoder layer.\n13We follow the same data preprocess process of Zeman et al. (2017) and Abzianidze et al. (2017).\nsyntactic information and the top ones contain more semantic information, and the information encoded by each encoder layer transforms from syntactic to semantic as the number of layers increase."
        },
        {
            "heading": "B Experimental Settings",
            "text": "For CoGnition, we set hidden size to 512 and feedforward dimension to 1,024. The number of encoder and decoder layers are 6, 6 and the number of attention heads are 4. The model parameters are optimized by Adam (Kingma and Ba, 2015), with \u03b21 = 0.9, \u03b22 = 0.98. The learning rate is set to 5e-4 and the number of warm-steps is 4000. We set max tokens as 8,192 tokens for iteration. We use one GeForce GTX 2080Ti for training with 100,000 steps and decoding. We report the average performance over 6 random seeds provided in Li et al. (2021). We train all COMPOSITION models from scratch. For CFQ, we use the base RoBERTa with 12 encoder layers, which is combined with a Transformer decoder that has 2 decoder layers with hidden size 256 and feed-forward dimension 512. We use a separate target vocabulary. The number of attention heads are 8. The model parameters are optimized by Adam (Kingma and Ba, 2015), with \u03b21 = 0.9, \u03b22 = 0.98. The learning rate is set to 1e-4 and the number of warm-steps is 4000. We set max tokens as 4,096 tokens for iteration. We use one GeForce GTX 2080Ti for training with 45,000 steps and decoding. We report the average performance over 3 random seeds provided in Zheng and Lapata (2022a). We train COMPOSITION built on top of RoBERTa with full parameter fine-tuning."
        },
        {
            "heading": "C Effects of the Effective Composition",
            "text": "As mentioned in Section 3, we introduce the composed layer between the encoder and decoder to compose different encoder sub-layers\u2019 information dynamically to generate specific keys and values passing into different decoder layers. We show curiosity about whether the composed layer can fuse all encoder sub-layers\u2019 information effectively.\nTherefore, we conduct a toy experiment on CoGnition. Specifically, all encoder sub-layers\u2019 information is accumulated to serve as the same key and value passing into every decoder layer (called Transformer-accu),14 rather than composing them dynamically like we do. Results are listed in Table 6. Transformer-accu even fails to train. It suggests that even if the syntactic and semantic information of sequences is considered, the inappropriate combinations will instead bring noise to significantly affect the model\u2019s CG performance."
        },
        {
            "heading": "D Effects of Representations from Low-layer Encoder",
            "text": "To verify the low-layer encoder representations are also essential to our approach, we only evaluate our approach on CoGnition with the collected encoder representations of the top three layers. Results are presented in Table 7. We can observe that only composing the representations of the top three encoder layers leads to a sharp drop in performance (27.0% vs 20.4% CTERInst), but still outperforms the Transformer baseline (27.0% vs 28.4% CTERInst). It further demonstrates the distinct difference between our method and the findings introduced by previous studies on EncoderFusion. It also reflects our starting point is correct, i.e., exploring how to compose syntactic and semantic information. It can be seen that COMPOSITION\u2019s performance is dramatically reduced given only semantic information (the last three encoder layers\u2019 information)."
        },
        {
            "heading": "E Reasons for Experiments on CoGnition without Language Models",
            "text": "We do not conduct experiments on CoGnition with language models for two reasons. First, CoGnition is constructed to test CG performance in MT scenarios with simple sentence pairs (see Figure 3), however, language models are trained on\n14{y0, ..., yl} are the output of the encoder layers 0 \u223c l. The input of keys and values of decoder layer i is xi = y0 + \u00b7 \u00b7 \u00b7+ yi\u22121, where 0 < i < L.\nvast amounts of multilingual sentences or bilingual sentence pairs. It is contrary to the compositional generalization task itself, since we can not guarantee that every sentence in the test set is a novel combination from known components for language models. Second, it is unfair to compare large language models with systems without pre-training. We strongly recommend researchers pay more attention to conduct experiments on CoGniton without language models."
        }
    ],
    "title": "Learning to Compose Representations of Different Encoder Layers towards Improving Compositional Generalization",
    "year": 2023
}