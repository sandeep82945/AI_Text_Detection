{
    "abstractText": "Text image machine translation (TIMT) which translates source language text images into target language texts has attracted intensive attention in recent years. Although the endto-end TIMT model directly generates target translation from encoded text image features with an efficient architecture, it lacks the recognized source language information resulting in a decrease in translation performance. In this paper, we propose a novel Cross-modal Cross-lingual Interactive Model (CCIM) to incorporate source language information by synchronously generating source language and target language results through an interactive attention mechanism between two language decoders. Extensive experimental results have shown the interactive decoder significantly outperforms end-to-end TIMT models and has faster decoding speed with smaller model size than cascade models. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Cong Ma"
        },
        {
            "affiliations": [],
            "name": "Yaping Zhang"
        },
        {
            "affiliations": [],
            "name": "Mei Tu"
        },
        {
            "affiliations": [],
            "name": "Yang Zhao"
        },
        {
            "affiliations": [],
            "name": "Yu Zhou"
        },
        {
            "affiliations": [],
            "name": "Chengqing Zong"
        },
        {
            "affiliations": [],
            "name": "Zhongke Fanyu"
        }
    ],
    "id": "SP:efc07c222d75c1a6631fa2ede0453aba966a05ae",
    "references": [
        {
            "authors": [
                "Haithem Afli",
                "Andy Way."
            ],
            "title": "Integrating optical character recognition and machine translation of historical documents",
            "venue": "Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities, LT4DH@COLING, pages",
            "year": 2016
        },
        {
            "authors": [
                "Lei Jimmy Ba",
                "Jamie Ryan Kiros",
                "Geoffrey E. Hinton."
            ],
            "title": "Layer normalization",
            "venue": "arXiv preprint arXiv:1607.06450.",
            "year": 2016
        },
        {
            "authors": [
                "Jeonghun Baek",
                "Geewook Kim",
                "Junyeop Lee",
                "Sungrae Park",
                "Dongyoon Han",
                "Sangdoo Yun",
                "Seong Joon Oh",
                "Hwalsuk Lee"
            ],
            "title": "What is wrong with scene text recognition model comparisons? dataset and model analysis",
            "year": 2019
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference",
            "year": 2015
        },
        {
            "authors": [
                "Jinying Chen",
                "Huaigu Cao",
                "Premkumar Natarajan."
            ],
            "title": "Integrating natural language processing with image document analysis: what we learned from two real-world applications",
            "venue": "Int. J. Document Anal. Recognit., 18(3):235\u2013247.",
            "year": 2015
        },
        {
            "authors": [
                "Zhuo Chen",
                "Fei Yin",
                "Xu-Yao Zhang",
                "Qing Yang",
                "Cheng-Lin Liu."
            ],
            "title": "Cross-lingual text image recognition via multi-task sequence to sequence learning",
            "venue": "25th International Conference on Pattern Recognition (ICPR), pages 3122\u20133129.",
            "year": 2020
        },
        {
            "authors": [
                "Jun Du",
                "Qiang Huo",
                "Lei Sun",
                "Jian Sun."
            ],
            "title": "Snap and translate using windows phone",
            "venue": "2011 International Conference on Document Analysis and Recognition (ICDAR), pages 809\u2013813. IEEE Computer Society.",
            "year": 2011
        },
        {
            "authors": [
                "Jonas Gehring",
                "Michael Auli",
                "David Grangier",
                "Yann N. Dauphin."
            ],
            "title": "A convolutional encoder model for neural machine translation",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver,",
            "year": 2017
        },
        {
            "authors": [
                "Jonas Gehring",
                "Michael Auli",
                "David Grangier",
                "Denis Yarats",
                "Yann N. Dauphin."
            ],
            "title": "Convolutional sequence to sequence learning",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August",
            "year": 2017
        },
        {
            "authors": [
                "Xavier Glorot",
                "Yoshua Bengio"
            ],
            "title": "Understanding the difficulty of training deep feedforward neural",
            "year": 2010
        },
        {
            "authors": [
                "Hao He",
                "Qian Wang",
                "Zhipeng Yu",
                "Yang Zhao",
                "Jiajun Zhang",
                "Chengqing Zong."
            ],
            "title": "Synchronous interactive decoding for multilingual neural machine translation",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI), pages 12981\u201312988.",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "Ryota Hinami",
                "Shonosuke Ishiwatari",
                "Kazuhiko Yasuda",
                "Yusuke Matsui."
            ],
            "title": "Towards fully automated manga translation",
            "venue": "The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI).",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations (ICLR).",
            "year": 2015
        },
        {
            "authors": [
                "Yuchen Liu",
                "Jiajun Zhang",
                "Hao Xiong",
                "Long Zhou",
                "Zhongjun He",
                "Hua Wu",
                "Haifeng Wang",
                "Chengqing Zong."
            ],
            "title": "Synchronous speech recognition and speech-to-text translation with interactive decoding",
            "venue": "The Thirty-Fourth AAAI Conference on",
            "year": 2020
        },
        {
            "authors": [
                "Ziyao Lu",
                "Xiang Li",
                "Yang Liu",
                "Chulun Zhou",
                "Jianwei Cui",
                "Bin Wang",
                "Min Zhang",
                "Jinsong Su."
            ],
            "title": "Exploring multi-stage information interactions for multi-source neural machine translation",
            "venue": "IEEE ACM Trans. Audio Speech Lang. Process., 30:562\u2013570.",
            "year": 2022
        },
        {
            "authors": [
                "Cong Ma",
                "Xu Han",
                "Linghui Wu",
                "Yaping Zhang",
                "Yang Zhao",
                "Yu Zhou",
                "Chengqing Zong."
            ],
            "title": "Modal contrastive learning based end-to-end text image machine translation",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, pages 1\u201313.",
            "year": 2023
        },
        {
            "authors": [
                "Cong Ma",
                "Yaping Zhang",
                "Mei Tu",
                "Xu Han",
                "Linghui Wu",
                "Yang Zhao",
                "Yu Zhou."
            ],
            "title": "Improving end-to-end text image translation from the auxiliary text translation task",
            "venue": "2022 26th International Conference on Pattern Recognition (ICPR), pages 1664\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Cong Ma",
                "Yaping Zhang",
                "Mei Tu",
                "Yang Zhao",
                "Yu Zhou",
                "Chengqing Zong."
            ],
            "title": "E2timt: Efficient and effective modal adapter for text image machine translation",
            "venue": "Document Analysis and Recognition ICDAR 2023, pages 70\u201388, Cham. Springer Nature",
            "year": 2023
        },
        {
            "authors": [
                "Cong Ma",
                "Yaping Zhang",
                "Mei Tu",
                "Yang Zhao",
                "Yu Zhou",
                "Chengqing Zong."
            ],
            "title": "Multi-teacher knowledge distillation for end-to-end text image machine translation",
            "venue": "Document Analysis and Recognition ICDAR 2023, pages 484\u2013501, Cham. Springer Nature",
            "year": 2023
        },
        {
            "authors": [
                "Elman Mansimov",
                "Mitchell Stern",
                "Mia Chen",
                "Orhan Firat",
                "Jakob Uszkoreit",
                "Puneet Jain."
            ],
            "title": "Towards end-to-end in-image neural machine translation",
            "venue": "Proceedings of the First International Workshop on Natural Language Processing Beyond Text, pages",
            "year": 2020
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "K. Chandra Shekar",
                "Marilyn Cross",
                "Vignesh Vasudevan."
            ],
            "title": "Optical character recognition and neural machine translation using deep learning techniques",
            "venue": "Innovations in Computer Science and Engineering.",
            "year": 2021
        },
        {
            "authors": [
                "Baoguang Shi",
                "Xiang Bai",
                "Cong Yao."
            ],
            "title": "An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 39(11):2298\u20132304.",
            "year": 2017
        },
        {
            "authors": [
                "Baoguang Shi",
                "Xinggang Wang",
                "Pengyuan Lyu",
                "Cong Yao",
                "Xiang Bai."
            ],
            "title": "Robust scene text recognition with automatic rectification",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30,",
            "year": 2016
        },
        {
            "authors": [
                "Tonghua Su",
                "Shuchen Liu",
                "Shengjie Zhou."
            ],
            "title": "Rtnet: An end-to-end method for handwritten text image translation",
            "venue": "16th International Conference on Document Analysis and Recognition (ICDAR), pages 99\u2013113.",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V. Le."
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Mon-",
            "year": 2014
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, pages 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "Yining Wang",
                "Jiajun Zhang",
                "Long Zhou",
                "Yuchen Liu",
                "Chengqing Zong."
            ],
            "title": "Synchronously generating two languages with interactive decoding",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th",
            "year": 2019
        },
        {
            "authors": [
                "Weijia Xu",
                "Yuwei Yin",
                "Shuming Ma",
                "Dongdong Zhang",
                "Haoyang Huang."
            ],
            "title": "Improving multilingual neural machine translation with auxiliary source languages",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event /",
            "year": 2021
        },
        {
            "authors": [
                "Yaping Zhang",
                "Shuai Nie",
                "Shan Liang",
                "Wenju Liu."
            ],
            "title": "Robust text image recognition via adversarial sequence-to-sequence domain adaptation",
            "venue": "IEEE Trans. Image Process., 30:3922\u20133933.",
            "year": 2021
        },
        {
            "authors": [
                "Yaping Zhang",
                "Shuai Nie",
                "Wenju Liu",
                "Xing Xu",
                "Dongxiang Zhang",
                "Heng Tao Shen."
            ],
            "title": "Sequenceto-sequence domain adaptation network for robust text image recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Zhao",
                "Jiajun Zhang",
                "Yu Zhou",
                "Chengqing Zong."
            ],
            "title": "Knowledge graphs enhanced neural machine translation",
            "venue": "Proceedings of the TwentyNinth International Joint Conference on Artificial Intelligence, IJCAI 2020, pages 4039\u20134045. ijcai.org.",
            "year": 2020
        },
        {
            "authors": [
                "Yang Zhao",
                "Jiajun Zhang",
                "Chengqing Zong."
            ],
            "title": "Transformer: A general framework from machine translation to others",
            "venue": "Mach. Intell. Res., 20(4):514\u2013 538.",
            "year": 2023
        },
        {
            "authors": [
                "Yang Zhao",
                "Jiajun Zhang",
                "Chengqing Zong",
                "Zhongjun He",
                "Hua Wu."
            ],
            "title": "Addressing the undertranslation problem from the entropy perspective",
            "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Ap-",
            "year": 2019
        },
        {
            "authors": [
                "Long Zhou",
                "Jiajun Zhang",
                "Chengqing Zong."
            ],
            "title": "Synchronous bidirectional neural machine translation",
            "venue": "Trans. Assoc. Comput. Linguistics, pages 91\u2013105.",
            "year": 2019
        },
        {
            "authors": [
                "Long Zhou",
                "Jiajun Zhang",
                "Chengqing Zong",
                "Heng Yu."
            ],
            "title": "Sequence generation: From both sides to the middle",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI), pages 5471\u20135477.",
            "year": 2019
        },
        {
            "authors": [
                "Barret Zoph",
                "Kevin Knight."
            ],
            "title": "Multi-source neural translation",
            "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Text image machine translation (TIMT) aims at translating text in images from the source language into the target language, which has been widely used in various applications such as photo translation, scene text translation, digital document translation, and so on. Existing research on TIMT is mainly divided into two categories of methods: cascade method and end-to-end method. Cascade method (Hinami et al., 2021; Shekar et al., 2021; Afli and Way, 2016; Chen et al., 2015; Du et al., 2011) takes a text image recognition (TIR) model for source language text recognition (Baek et al.; Shi et al., 2017, 2016; Zhang et al., 2021, 2019) and then translates them into target language texts with a machine translation (MT) model (Vaswani et al., 2017; Gehring et al., 2017a,b; Johnson et al., 2017; Bahdanau et al., 2015; Sutskever et al., 2014; Zhao et al., 2019, 2020). To explicitly recognize the\n\u2217Corresponding author. 1https://github.com/EriCongMa/CCIM\nsource language embedded in text images, the cascade model combines TIR models and MT models for the TIMT task. However, two individual models in the cascade frame have double parameters and the decoding speed is slow. Meanwhile, errors in the TIR model are further propagated in the MT model leading to performance decrease. The end-to-end method directly translates the source language text image into target language through a unified encoder-decoder architecture, which is more parameter-efficient than cascade models with faster decoding speed (Ma et al., 2022; Su et al., 2021; Mansimov et al., 2020; Chen et al., 2020; Ma et al., 2023a,b,c).\nHowever, the performance of end-to-end models is limited because the translation process lacks explicit source language guidance from recognition texts. An intuitive solution is to incorporate the recognition history into the translation decoder to offer more efficient guidance. Recently, multisource interaction has been studied to incorporate effective information into target model (Lu et al., 2022; Xu et al., 2021; He et al., 2021; Liu et al., 2020; Zhou et al., 2019a,b; Wang et al., 2019; Zoph and Knight, 2016). Although multi-source interaction is vital to enhance the encoding capacity of TIMT model through attending recognition information explicitly, it has not been explored yet.\nTo address the above issues, we propose a novel Cross-modal Cross-lingual Interactive Model (CCIM) for TIMT, which effectively incorporates source language recognition information into the TIMT decoder through interactive attention. The interactive decoder has two decoding modules, one for source language and the other one for target language generation. A cross-lingual interactive attention mechanism is introduced to bridge the two language decoders. When generating translation results, the target language decoder not only receives the hidden states from the encoder and previous decoded translation history but also attends to the\nMono-lingual Cross-Modal Attention\nCross-lingual Cross-Modal Attention\nFeed Forward Layer\n!\"\nCross-lingual Interactive Attention\n#$, \u2026 , #\"'$\nFeed Forward Layer\n#\"\n!$, \u2026 , !\"'$ Translation History Recognition History\nMulti-Head Self-Attention\nSource Language Decoder\nInteractive Attention\nEncoder-Decoder Cross-Attention\n\u2026\n\"#,\u2026 , \"%&# Recognition History\n+ Interactive Attention\nTarget Language Decoder\nMulti-Head Self-Attention\nEncoder-Decoder Cross-Attention\n\u2026\n'#,\u2026 , '%&# Translation History\n+\nMulti-Head Self-Attention\nSource Language Decoder\nInteractive Attention\nEncoder-Decoder Cross-Attention\n\"#, \u2026 , \"%&# Recognition History\nMulti-Head Self-Attention\nTarget Language Decoder\nInteractive Attention\nEncoder-Decoder Cross-Attention\n'#, \u2026 , '%&# Translation History\n\u2026 \u2026 (a) Weighted Interactive Attention\n(b) Hierarchical Interactive Attention\n(b) Weighted Cross-lingual Interactive Attention\nMulti-Head Self-Attention\nSource Language Decoder\nInteractive Attention\nEncoder-Decoder Cross-Attention\n\u2026\n\"#,\u2026 , \"%&# Recognition History\n+ Interactive Attention\nTarget Language Decoder\nMulti-Head Self-Attention\nEncoder-Decoder Cross-Attention\n\u2026\n'#,\u2026 , '%&# Translation History\n+\nMulti-H ad Self-Attention\nSource Language Decoder\nInteractive Attention\nEncoder-Decoder Cross-Attention\n\"#, \u2026 , \"%&# Recognition History\nMulti-Head Self-Attention\nTarget Language Decoder\nInteractive Attention\nEnc der-D coder Cross-Attention\n'#, \u2026 , '%&# Translation History\n\u2026 \u2026 (a) Weighted Interactive Attention\n(b) Hierarchical Interactive Attention(c) Hierarchical Cross-lingual Interactive Attention (a) Architecture of Cross-modal Cross-lingual Interactive Model for Text Image Translation\nConvolutional Neural Network Modules\nMulti-Head Self-Attention\nFeed Forward Layer\nImage Encoder\nTranslation Result Recognition Result\nInteractive Decoder\nRecognition History\nRecognition History\nTranslation History\nTranslation His ory\nFigure 1: (a) illustrates the proposed cross-modal cross-lingual interactive model (CCIM). (b) illustrates the weighted cross-lingual interactive attention module. (c) illustrates the hierarchical cross-lingual interactive attention module.\ndecoded recognition history. Our contributions are summarized as follows:\n\u2022 We propose a novel cross-modal cross-lingual interactive model (CCIM) for the TIMT task, which effectively enhances the translation decoder by incorporating recognition features.\n\u2022 Weighted and hierarchical interactive decoding strategies have been studied to validate the effectiveness of interactive generation.\n\u2022 Experimental results on three evaluation datasets have revealed the CCIM improves the translation quality of end-to-end TIMT models and outperforms cascade models with fewer parameters and faster decoding speed."
        },
        {
            "heading": "2 Methodology",
            "text": ""
        },
        {
            "heading": "2.1 Cross-modal Cross-lingual Interactive Model",
            "text": "The proposed CCIM model consists of an image encoder and an interactive decoder. As shown in Figure 1 (a), the image encoder first extracts image features given the source language text image, then two decoders are utilized for text image recognition and translation synchronously.\nFor image encoding, a convolutional neural network is utilized to extract image representation through multi-layer convolution and pooling operations (He et al., 2016). While for multi-head attention (MHA), the model collects information from different positions to update the hidden state of the current position (Vaswani et al., 2017):\nMHA(Q,K, V ) = Concat(head1, ..., headh)WO where headi = Attention(QW iQ,KW i K , V W i V ) (1)\nwhere W iQ,W i K and W i V represent query, key, and value projection matrices for head i, respectively. WO denotes the output projection matrix.\nSelf-attention (SA) The interactive decoder first calculates self-attention hidden states for both source language X and target language Y given the same query, key, and value:\nHSAX = MHA(X,X,X) HSAY = MHA(Y, Y, Y ) (2)\nThen, cross-lingual interactive-attention (IA) hidden states are calculated through two language decoders. Two types of IA mechanisms are utilized to recognize and translate synchronously:\nWeighted Interactive Attention (WIA) As shown in Figure 1 (b), the self-attention and interactive attention are calculated separately and then weighted summation:\nHWIAX = H SA X + \u03bb\u00d7 MHA(X,Y, Y ) HWIAY = H SA Y + \u03bb\u00d7 MHA(Y,X,X)\n(3)\nwhere the query of WIA is from the corresponding language decoding history. Key and value are from the other language history.\nHierarchical Interactive Attention (HIA) To fuse self- and interactive-attention together, a hierarchical calculation mechanism is introduced to\nobtain interactive information through serial computing as shown in Figure 1 (c):\nHHIAX = MHA(H SA X , H SA Y , H SA Y ) HHIAY = MHA(H SA Y , H SA X , H SA X )\n(4)\nwhere the query is from the inner-lingual selfattention results, while the key and value are from the other language self-attention features.\nEncoder-Decoder Cross-Attention (CA) Crosslingual interactive-attention hidden states are fed into the encoder-decoder cross-attention mechanism to further incorporate encoder features into the decoder as in (Vaswani et al., 2017).\nHCAX = MHA(H IA X , HI , HI) HCAY = MHA(H IA Y , HI , HI)\n(5)\nwhere HI represents the hidden states from the image encoder. H IAX , H IAY can be WIA or HIA for source and target languages. HCAX , HCAY denote the output of the encoder-decoder cross-attention module for source and target language, respectively.\nThe hidden states from the encoder-decoder cross-attention mechanism are then further encoded by the feedforward layer to obtain the interactive decoder layer outputs. Notes that the residual connections (He et al., 2016) and layer normalization (Ba et al., 2016) in standard transformer decoder are also utilized after self-attention, interactive attention, encoder-decoder cross attention and feedforward modules in interactive decoder (Zhao et al., 2023), which are not drawn in Figure 1 for simplification."
        },
        {
            "heading": "2.2 Loss Functions for Optimization",
            "text": "Since the interactive decoder has two decoders for the source language and target language respectively, TIR and TIMT tasks are optimized synchronously by multi-task learning. The training dataset contains triple paired samples as D = {Ii, Xi, Y i}|D|i , where Ii is the i-th source language text image, Xi is the i-th source language texts and Y i is the corresponding translated target\nlanguage texts. The model is updated by optimizing both TIR and TIMT loss functions:\nL = LTIR + LTIMT\nLTIR = \u2212 |D|\u2211 i M\u2211 j logP (xij |Ii, xi<j , yi<j)\nLTIMT = \u2212 |D|\u2211 i N\u2211 j logP (yij |Ii, xi<j , yi<j)\n(6)\nwhere x<j and y<j denote the recognition and translation history. M and N represent the token length of the source language and target language. Note that the interactive decoder has three attention modules (self-attention, interactive attention from the other task, and encoder-decoder attention), the decoder generates tokens given the condition of both text image, recognition history, and translation history. Thus the interactive decoder has the potential to generate more accurate translation results."
        },
        {
            "heading": "2.3 Training and Inference",
            "text": "For each decoding step during training, the teacherforcing decoding strategy is utilized to train the parameters in the decoder in a parallel computing way and each position in the decoder can attend to all positions in the decoder up to and including that position through the attention mask. During inference, the decoder generates both source and target language tokens by tokens in an autoregressive way. For each step, the two sub-branches of the interactive decoder can attend encoder features, recognition history features, and translation history features, and predict both source and target language at the current step."
        },
        {
            "heading": "3 Experiments and Results",
            "text": ""
        },
        {
            "heading": "3.1 Datasets",
            "text": "The experiments have been conducted on a public TIMT corpus released by (Ma et al., 2022). The training set contains one million triple-paired samples of source language images, source language\ntexts, and target language translation pairs for each translation direction. The source language text images in the training dataset are synthesized by using bilingual text sentences. To validate the generalization of models, one synthetic test set and two real-word (subtitle and street-view) test sets are utilized to evaluate the translation performance. The statistics of the dataset are shown in Table 1."
        },
        {
            "heading": "3.2 Experimental Settings",
            "text": "Image encoder in CCIM utilizes the same configuration in (Ma et al., 2022). The source language and target language decoder are 6-layer transformer decoder with 512-dimensional hidden sizes as in (Vaswani et al., 2017; Zhao et al., 2023). The maximum sentence length for English, German, and Chinese are set to 80, 80, and 40 respectively. The preprocessed image height is set to 32 and the channel is 3. To align the length of the image feature and text feature, preprocessed image width is resized to 320, 320, and 160. The batch size is set to 64, and the training step is 300,000. All models are initialized with Xavier initiation method (Glorot and Bengio, 2010) and optimized with Adam optimizer (Kingma and Ba, 2015) on a single NVIDIA V100 GPU. Sacre-BLEU2 (Papineni et al., 2002) is utilized for evaluation metric."
        },
        {
            "heading": "3.3 Baseline Models",
            "text": "\u2022 CLTIR model is a vanilla multi-task learning\nbased TIMT model with auxiliary TIR task training (Chen et al., 2020).\n\u2022 RTNet bridges the TIR encoder and MT decoder through a feature transformer, which is also trained with TIR task (Su et al., 2021).\n\u2022 MTETIMT is a machine translation enhanced TIMT model, which is trained with both auxiliary TIR and MT tasks (Ma et al., 2022).\n2https://github.com/mjpost/sacrebleu"
        },
        {
            "heading": "3.4 Comparison with Different End-to-End TIMT Models",
            "text": "Table 2 shows the main results on three evaluation domains. As shown in Table 1, CCIM outperforms the existing best multi-task based MTETIT by 0.21 BLEU scores on average. Meanwhile, CCIM improves the translation performance on real-world domains by 0.12 BLEU scores on average, indicating the good generalization of our proposed method. Furthermore, CCIM can generate source language and target language synchronously, which can meet the requirement of both recognition and translation tasks in practical applications."
        },
        {
            "heading": "3.5 Model Size and Decoding Speed",
            "text": "The Cascade model deploys TIR and MT models, leading to parameter redundancy and decoding delay. With an end-to-end architecture, CCIM outperforms the cascade model with fewer parameters and faster decoding speed as shown in Table 3. Specifically, CCIM decreases around 24.62% parameters and achieves 1.64x acceleration compared with the cascade model. Meanwhile, CCIM significantly outperforms the cascade model by 1.75 BLEU scores, which effectively alleviates the error propagation problem in the cascade model."
        },
        {
            "heading": "3.6 Comparison of Different Interactive Attention Types",
            "text": "To validate the effectiveness of interactive attention, an ablation study of replacing key and value\nin interactive attention with random samples noise vector has been implemented. Experimental results in Table 4 show that random noise replaced interactive attention generates a poor translation, especially for weighted interactive attention. We attribute that weighted interactive attention incorporates noise signals through weighted summation, which severely disturbs the information flow. Furthermore, hierarchical interactive attention outperforms weighted interactive attention, which reveals that flexible calculation of hierarchical architecture is better than vanilla summation operation."
        },
        {
            "heading": "3.7 Case Study of CCIM Model",
            "text": "Fig. 2 shows an example of TIMT generated by end-to-end and CCIM models. Although the endto-end model translates the general meaning of the sentence, it ignores the meaning of \u2019double-check\u2019 in the source language text image. Since there is no interaction during decoding, the multi-task based model also ignores this meaning. CCIM successfully translated this word through interactive attention with the source language decoder, indicating CCIM can effectively alleviate the problem of lacking source language information in vanilla end-to-end TIMT models.\n3.8 Wait-k Strategy for CCIM The wait-k strategy is commonly employed in speech translation, aiming at generating better translation given more recognition history. To validate the wait-k strategy in the TIMT task, we also conducted corresponding experiments as shown in Table 5. From the experimental result, the wait-k\nstrategy makes the recognition task decode first, enabling the translation task to access more source language information for improved translation quality. While the wait-k strategy enhances translation quality, it does introduce some latency increase. The CCIM model achieves the best translation performance when k = 4 in our experiments."
        },
        {
            "heading": "4 Conclusion",
            "text": "This paper proposes a novel interactive decoder based end-to-end TIMT model, which explicitly incorporates recognized hidden states into the translation process. Through the interactive attention mechanism, recognition and translation results are generated synchronously and mutually enhanced. By making full use of the source language recognition information, CCIM outperforms existing endto-end and multi-task based TIMT models on both synthetic and real-world evaluation sets. Furthermore, with the end-to-end architecture, CCIM has fewer parameters and faster decoding speed than cascade models. Ablation study of different interactive attention types shows hierarchical interactive attention has stronger interactive ability across recognition and translation tasks. In the future, we will explore more interactive methods for end-toend text image machine translation."
        },
        {
            "heading": "5 Acknowledgements",
            "text": "This work has been supported by the National Natural Science Foundation of China (NSFC) grants 62106265."
        },
        {
            "heading": "6 Limitations",
            "text": "Our method is now designed for text line images, which need preprocessing of text detection in images. In the future, we will consider optimizing the text detection and translation in images jointly to increase the scalability of our work."
        }
    ],
    "title": "CCIM: Cross-modal Cross-lingual Interactive Image Translation",
    "year": 2023
}