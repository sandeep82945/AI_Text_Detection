{
    "abstractText": "The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial validation of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a representative large language model, and the results show that the existing successful models exhibit a frustrating degradation, with a maximum drop of 23.43 F1 score. Our resources and code are available at https://github.com/qijimrc/ROBUST.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ji Qi"
        },
        {
            "affiliations": [],
            "name": "Chuchun Zhang"
        },
        {
            "affiliations": [],
            "name": "Xiaozhi Wang"
        },
        {
            "affiliations": [],
            "name": "Kaisheng Zeng"
        },
        {
            "affiliations": [],
            "name": "Jifan Yu"
        },
        {
            "affiliations": [],
            "name": "Jinxin Liu"
        },
        {
            "affiliations": [],
            "name": "Lei Hou"
        },
        {
            "affiliations": [],
            "name": "Juanzi Li"
        },
        {
            "affiliations": [],
            "name": "Bin Xu"
        }
    ],
    "id": "SP:41caa26fab3ef2b473d5250765e594f71eb6e7ae",
    "references": [
        {
            "authors": [
                "Gabor Angeli",
                "Melvin Jose Johnson Premkumar",
                "Christopher D Manning."
            ],
            "title": "Leveraging linguistic structure for open domain information extraction",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th",
            "year": 2015
        },
        {
            "authors": [
                "Michele Banko",
                "Michael J Cafarella",
                "Stephen Soderland",
                "Matt Broadhead",
                "Oren Etzioni."
            ],
            "title": "Open information extraction from the web",
            "venue": "IN IJCAI.",
            "year": 2007
        },
        {
            "authors": [
                "Sangnie Bhardwaj",
                "Samarth Aggarwal",
                "Mausam Mausam."
            ],
            "title": "Carb: A crowdsourced benchmark for open ie",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference",
            "year": 2019
        },
        {
            "authors": [
                "Huajun Chen",
                "Ning Hu",
                "Guilin Qi",
                "Haofen Wang",
                "Zhen Bi",
                "Jie Li",
                "Fan Yang."
            ],
            "title": "Openkg chain: A blockchain infrastructure for open knowledge graphs",
            "venue": "Data Intelligence, 3(2):205\u2013227.",
            "year": 2021
        },
        {
            "authors": [
                "Janara Christensen",
                "Stephen Soderland",
                "Oren Etzioni."
            ],
            "title": "An analysis of open information extraction based on semantic role labeling",
            "venue": "Proceedings of the sixth international conference on Knowledge capture, pages 113\u2013120.",
            "year": 2011
        },
        {
            "authors": [
                "Michael Collins",
                "Nigel Duffy."
            ],
            "title": "Convolution kernels for natural language",
            "venue": "Advances in neural information processing systems, 14.",
            "year": 2001
        },
        {
            "authors": [
                "Lei Cui",
                "Furu Wei",
                "Ming Zhou."
            ],
            "title": "Neural open information extraction",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 407\u2013 413.",
            "year": 2018
        },
        {
            "authors": [
                "Luciano Del Corro",
                "Rainer Gemulla."
            ],
            "title": "Clausie: clause-based open information extraction",
            "venue": "Proceedings of the 22nd international conference on World Wide Web, pages 355\u2013366.",
            "year": 2013
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Kuicai Dong",
                "Zhao Yilin",
                "Aixin Sun",
                "Jung-jae Kim",
                "Xiaoli Li."
            ],
            "title": "Docoie: A document-level contextaware dataset for openie",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2377\u20132389.",
            "year": 2021
        },
        {
            "authors": [
                "Nicholas FitzGerald",
                "Julian Michael",
                "Luheng He",
                "Luke Zettlemoyer."
            ],
            "title": "Large-scale QA-SRL parsing",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2051\u20132060, Melbourne,",
            "year": 2018
        },
        {
            "authors": [
                "Kiril Gashteovski",
                "Mingying Yu",
                "Bhushan Kotnis",
                "Carolin Lawrence",
                "Mathias Niepert",
                "Goran Glava\u0161."
            ],
            "title": "BenchIE: A framework for multi-faceted factbased open information extraction evaluation",
            "venue": "In",
            "year": 2022
        },
        {
            "authors": [
                "Luheng He",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "Question-answer driven semantic role labeling: Using natural language to annotate natural language",
            "venue": "Proceedings of the 2015 conference on empirical methods in natural language processing, pages",
            "year": 2015
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Wei Xu",
                "Jun Araki",
                "Graham Neubig."
            ],
            "title": "Generalizing natural language analysis through span-relation representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2120\u20132133.",
            "year": 2020
        },
        {
            "authors": [
                "Keshav Kolluru",
                "Vaibhav Adlakha",
                "Samarth Aggarwal",
                "Soumen Chakrabarti"
            ],
            "title": "2020a. Openie6: Iterative grid labeling and coordination analysis for open information extraction",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Keshav Kolluru",
                "Samarth Aggarwal",
                "Vipul Rathore",
                "Soumen Chakrabarti"
            ],
            "title": "2020b. Imojie: Iterative memory-based joint open information extraction",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "William Lechelle",
                "Fabrizio Gotti",
                "Phillippe Langlais."
            ],
            "title": "WiRe57 : A fine-grained benchmark for open information extraction",
            "venue": "Proceedings of the 13th Linguistic Annotation Workshop, pages 6\u201315, Florence, Italy. Association for Computational Linguis-",
            "year": 2019
        },
        {
            "authors": [
                "Ruibo Liu",
                "Guoqing Zheng",
                "Shashank Gupta",
                "Radhika Gaonkar",
                "Chongyang Gao",
                "Soroush Vosoughi",
                "Milad Shokouhi",
                "Ahmed Hassan Awadallah."
            ],
            "title": "Knowledge infused decoding",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Harinder Pal"
            ],
            "title": "Demonyms and compound relational nouns in nominal open ie",
            "venue": "Proceedings of the 5th Workshop on Automated Knowledge Base Construction, pages 35\u201339.",
            "year": 2016
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Kevin Pei",
                "Ishan Jindal",
                "Kevin Chen-Chuan Chang",
                "Chengxiang Zhai",
                "Yunyao Li."
            ],
            "title": "When to use what: An in-depth comparative empirical analysis of openie systems for downstream applications",
            "venue": "arXiv preprint arXiv:2211.08228.",
            "year": 2022
        },
        {
            "authors": [
                "Ji Qi",
                "Yuxiang Chen",
                "Lei Hou",
                "Juanzi Li",
                "Bin Xu."
            ],
            "title": "Syntactically robust training on partiallyobserved data for open information extraction",
            "venue": "arXiv preprint arXiv: 2301.06841.",
            "year": 2023
        },
        {
            "authors": [
                "Youngbin Ro",
                "Yukyung Lee",
                "Pilsung Kang."
            ],
            "title": "Multi\u02c62OIE: Multilingual open information extraction based on multi-head attention with BERT",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1107\u20131117, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Arpita Roy",
                "Youngja Park",
                "Taesung Lee",
                "Shimei Pan."
            ],
            "title": "Supervising unsupervised open information extraction models",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference",
            "year": 2019
        },
        {
            "authors": [
                "Swarnadeep Saha",
                "Harinder Pal"
            ],
            "title": "Bootstrapping for numerical open ie",
            "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Swarnadeep Saha"
            ],
            "title": "Open information extraction from conjunctive sentences",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 2288\u20132299.",
            "year": 2018
        },
        {
            "authors": [
                "Michael Schmitz",
                "Stephen Soderland",
                "Robert Bart",
                "Oren Etzioni"
            ],
            "title": "Open language learning for information extraction. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural lan",
            "year": 2012
        },
        {
            "authors": [
                "Jacob Solawetz",
                "Stefan Larson."
            ],
            "title": "Lsoie: A large-scale dataset for supervised open information extraction",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2595\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Gabriel Stanovsky",
                "Ido Dagan."
            ],
            "title": "Creating a large benchmark for open information extraction",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2300\u20132305.",
            "year": 2016
        },
        {
            "authors": [
                "Gabriel Stanovsky",
                "Julian Michael",
                "Luke Zettlemoyer",
                "Ido Dagan."
            ],
            "title": "Supervised open information extraction",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2018
        },
        {
            "authors": [
                "Jiao Sun",
                "Xuezhe Ma",
                "Nanyun Peng."
            ],
            "title": "Aesop: Paraphrase generation with adaptive syntactic control",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2021
        },
        {
            "authors": [
                "Mingming Sun",
                "Xu Li",
                "Xin Wang",
                "Miao Fan",
                "Yue Feng",
                "Ping Li."
            ],
            "title": "Logician: a unified end-to-end neural approach for open-domain information extraction",
            "venue": "Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining,",
            "year": 2018
        },
        {
            "authors": [
                "John Wieting",
                "Kevin Gimpel."
            ],
            "title": "ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2018
        },
        {
            "authors": [
                "Junlang Zhan",
                "Hai Zhao."
            ],
            "title": "Span model for open information extraction on accurate corpus",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.",
            "year": 2020
        },
        {
            "authors": [
                "Victor Zhong",
                "Weijia Shi",
                "Wen-tau Yih",
                "Luke Zettlemoyer."
            ],
            "title": "Romqa: A benchmark for robust, multievidence, multi-answer question answering",
            "venue": "arXiv preprint arXiv:2210.14353.",
            "year": 2022
        },
        {
            "authors": [
                "Jianing Zhou",
                "Suma Bhat."
            ],
            "title": "Paraphrase generation: A survey of the state of the art",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Open Information Extraction (OpenIE) aims to extract n-ary knowledge tuples {(a1, p, a2, ..., an)} consisting of n arguments and one predicate from the natural text in a domain-independent manner, which has been served as the backbone to benefit NLP applications for many years (Liu et al., 2021; Pei et al., 2022; Chen et al., 2021).\nDue to its structural flexibility, the evaluation of OpenIE is a nontrivial problem, which in turn drives the advancement of the task. Early studies (Stanovsky and Dagan, 2016; Zhan and Zhao, 2020) measure the performance of extractions\n\u2217Corresponding author: xubin@tsinghua.edu.cn\nbased on the lexical matching of syntactic heads between elements. To tackle the overly lenient metric, subsequent approaches (Lechelle et al., 2019; Bhardwaj et al., 2019; Gashteovski et al., 2022) propose to use of exact matching between tokens for delicate evaluation. Among these benchmarks, CaRB (Bhardwaj et al., 2019) adopts the all-pair matching table to compute the tuple match scores between extractions, which has been considered the de facto standard for evaluation. Research including these efforts has been devoted to evaluating the pairwise matching correctness between model extractions and golden facts on a sentence.\nHowever, the conventional evaluation benchmarks do not measure the robustness of models in the realistic open-world scenario, where the syntactic and expressive forms may vary under the same knowledge meaning (Qi et al., 2023). As shown in Figure 1, while the three sentences s1, s2, s3 contain the same structured knowledge (a1, p, a2, a3), the state-of-the-art model OpenIE6 successfully extracts facts (in green color) on sentence s1, but fails to predict arguments (in red color) on the other sentences due to the syntactic and expressive drifts. In this example, the sentence s1 comes from CaRB which has a similar syntactic distribution to the training set, and existing benchmarks can only eval-\nuate models on this limited target attributing it the commendable scores (46.4/33.3), rather than on the other world samples. For accurate and faithful evaluation, we should measure the performance of models on sentences with various syntactic and expressive distributions under the same knowledge meaning (Zhong et al., 2022).\nNevertheless, it is not trivial to construct a benchmark that satisfies the aforementioned conditions of encompassing both knowledge invariance and distributional shift. First, manual annotation of parallel texts to maintain the same knowledge meaning with different syntactic and expressive forms may result in either too trivial or artificial. Second, it is difficult to build a metric that measures the robustness as well as be compatible with existing benchmarks (e.g., (Bhardwaj et al., 2019; Gashteovski et al., 2022)) to ensure comparability.\nOn the other hand, natural language paraphrasing is defined as producing sentences with different surface forms (syntactic and lexical) by conveying the same semantic meaning (Zhou and Bhat, 2021). Going beyond the pairwise correctness comparison, can we evaluate the robustness of models based on reliable paraphrases equipped with syntactic and expressive transformations?\nIn this paper, we introduce ROBUST, a Robust OpenIE Benchmark with Ubiquitous Syntactic Transformations, aiming to evaluate the robustness of OpenIE models. ROBUST is a large-scale human-annotated benchmark consisting of 1, 272 robustness testing cliques, where each clique contains sentences with different syntactic and expressive variations while conveying the same underlying knowledge meaning, for a total of 4,971 sentences and 16,191 knowledge extractions. To obtain each clique, we first adopt a syntactically controllable paraphraser with diversified syntactic sampling and expressive filtering strategies to generate paraphrases for each sentence in CaRB. We then design a two-stage annotation pipeline to perform sentence correction and knowledge extraction for each individual paraphrase in cliques based on human experts. This data paradigm enables evaluation to go beyond pairwise matching to clique-wise comparisons. Upon the testbed structure, we calculate the robustness scores with respect to the worst performance within a clique and further analyze the performance variances on all cliques. This metric fairly reflects the robustness of models to distributional drifts and is also compatible with existing\nbenchmarks calculated at one sentence magnitude. To explore the robustness of existing models, we implement typical OpenIE systems published in the past decade. The experimental results show a dramatic degradation in model performance on ROBUST, with an average drop of 18 percentage points in F1 scores, indicating that the robustness of existing successful models is far from satisfactory. We then further analyze the correlation between the variances of the model performance and the divergences of the syntactic distances on the cliques. The results find that the variance grows as the syntactic distance increases, and models behaved with similar variance on most of the cliques also demonstrate the inner consistency of our benchmark. In addition, we also evaluate the a representative large language model ChatGPT1 for OpenIE. Experimental results demonstrate that ChatGPT achieves a remarkable performance that is compatible with the state-of-the-art model on CaRB (F1 score of 0.516 under the 10-shot setting), yet it still exhibits the robustness issue on ROBUST (F1 score of 0.275 under the 10-shot setting)."
        },
        {
            "heading": "2 The ROBUST Benchmark",
            "text": "In this section, we describe the details of the benchmark construction. The benchmark consists of cliques based on syntactically diverse paraphrase generation and human annotation to unsure the knowledge invariance and distributional shift, where both the syntactic transformations sampled from real world and the human experience guarantee the naturalness. We also provide details of annotations and strategies in the Appendix A.1 and A.2."
        },
        {
            "heading": "2.1 Data Preparation",
            "text": "Paraphrase Generation. Considering the compatibility with previous benchmarks, we build our benchmark based on CaRB (Bhardwaj et al., 2019), which contains 1,272 sentences2 of general domain originated from OIE2016 (Stanovsky and Dagan, 2016) with high-quality n-tuples annotations. To build sufficient paraphrases, we adopt AESOP (Sun et al., 2021), a syntactically controllable paraphrasing model generating paraphrases by specifying pruned target syntactic trees that can be sampled diversely. The model used in our work is trained on a parallel annotated data with two-level target\n1https://chat.openai.com/ 2We remove 10 sentences that do not have extractions from\nthe original data.\nsyntactic trees. During generation, we first collect a set of constituency parse pairs {(TPsi , T P ti )} pruned at height 3 from the ParaNMT-50M (Wieting and Gimpel, 2018). And then for each sentence s with its constituency parse tree T , we obtain 2 most similar parses {(T \u2032Psi , T \u2032P s2 )} by calculating weighted ROUGE scores between parse strings and select 5 top-ranked parses from {TPti } for each T \u2032Psi by a sampling with the distribution of TPt \u223c (#T \u2032Psi ,T P t )\u2211\nj #(T \u2032P si ,T \u2032Ptj ) . We thus generate 10 syntac-\ntically varying paraphrases for each sentence.\nDiversified Expressive Filtering. Though different syntactic trees are specified in the paraphrase generation, we find that there are still similar expressions in the generated sentences. Therefore, we further filter the paraphrases with a heuristic search strategy to maintain the most diverse ones. For each clique composed of multiple sentence nodes, including an original sentence and multiple paraphrases, we first calculate the BLEU scores (Papineni et al., 2002) between all pairs of nodes. We then repeat the following simple strategy on paraphrase nodes until reaching the maximum acceptable number to eliminate homogeneity: (1) find the pair of nodes with the largest score in the current clique; (2) remove a node if its length is less than 2/3 of the original sentence, otherwise remove the node with the highest sum of scores with all other nodes. As depicted in Figure 1, the remaining sentences s2 and s3 exhibit distinct syntactic structures and expressive forms compared to the original sentence s1. The detailed process with an example is shown in Appendix A.2.2."
        },
        {
            "heading": "2.2 Annotation",
            "text": "For each paraphrase within a clique, we further design a two-stage annotation pipeline based on human experts to perform sentence correction and structured knowledge extraction. All annotators undergo training with tutorials to pass a final examination, and our batch-wise sampling validation ensure an annotation accuracy of over 90%. Detailed annotation including annotators, platforms, and quality checking can be found in Appendix A.1. Paraphrase Annotation. While automatically generated paraphrases present syntactic and expressive variants, the correctness of the sentences cannot be fully guaranteed. To ensure the quality of the sentences, we perform a thorough paraphrase annotation with three types of corrections:\n\u2022 Grammar Correcting: Correct grammatical mistakes in sentences to ensure the fluency.\n\u2022 Phrase Replacing: Replace the incorrect phrases in sentences to ensure the correctness.\n\u2022 Sentence Rewriting: Rewrite the entire sentence if it has a semantic difference from the original sentence.\nAll operations are required to preserve both the distinctiveness of the annotation from the original sentence and their semantic equivalence. Based on this paradigm, all paraphrases are guaranteed to differ from the original sentence in expression, while retaining the same semantic meaning. As shown in Figure 2, the three sentences in the 1st column exhibit different syntactic and expressive forms. A detailed process is available in Appendix A.1.1.\nKnowledge Annotation. In the second stage, we leverage human experts to annotate N-ary knowledge tuples on the paraphrases finished in the first stage. We design a guideline involving an iterative process to instruct annotators in extracting all possible facts from a sentence. By referring to the annotation of CaRB, in each iteration, we also divide the task of annotating into three steps: (1) recognizing the predicate, (2) finding the arguments for that predicate, and (3) optionally obtaining the time and location arguments for the tuple if possible.\nIn particular, we distribute the complete clique to individual annotators to obtain extractions with the same structured knowledge meaning. This annotation process ensures the characteristics in CaRB (i.e. Completeness, Assertedness, Informativeness, and Atomicity) while maintaining consistency with the underlying knowledge. As illustrated in the fourth column of Figure 2, the extractions from different sentences correspond to the same underlying knowledge. Detailed annotation process is available in Appendix A.1.2."
        },
        {
            "heading": "3 Data Analysis",
            "text": "To understand the general characteristics of ROBUST, we provide quantitative statistics at different granularities in comparison to previous benchmarks. In contrast to the traditional analysis on words and sentences, we further investigate the syntactic phenomena on cliques to explain the robustness evaluation."
        },
        {
            "heading": "3.1 Data Statistics",
            "text": "Table 1 shows the quantitative statistics of ROBUST and representative OpenIE benchmarks, including OIE2016 (Stanovsky and Dagan, 2016), Re-OIE2016 (Zhan and Zhao, 2020), CaRB (Bhardwaj et al., 2019) and BenchIE (Gashteovski et al., 2022). In comparison with the conventional dataset, ROBUST provides the largest number of humanannotated high-quality sentences. Meanwhile,\nbased on the annotation paradigm, ROBUST raises a new data structure, the clique, which establishes the interconnection of sentences with underlying knowledge. The average number of sentences per clique is 3.877.\nIn addition, we find that previous benchmarks completely originate from OIE2016 based on Wiki and newswires, potentially leading to distribution bias to similar training corpus, especially for pretrained language models (e.g. BERT (Devlin et al., 2019)) trained on the general corpora. ROBUST mitigates this bias by extending syntactic and expressive distributions to realistic scenarios. We further compute the vocabulary sizes for CaRB and ROBUST, resulting in 7648 and 7981, respectively, demonstrating that our natural annotations do not introduce many rare words."
        },
        {
            "heading": "3.2 Syntactic Analysis",
            "text": "The proposed benchmark measures the robustness of models on the drifts of linguistic observations. Therefore, the syntactic divergence in the clique is the key to ensuring robustness evaluation. We provide a thorough syntactic analysis of cliques to investigate the divergence. Metrics of Syntactic Correlation. In order to analyze the syntactic divergence in the cliques, we need a metric to measure the syntactic correlation between two sentences. A fast and effective algorithm is the HWS distance proposed in (Qi et al., 2023), which calculates the syntactic tree distance between two sentences based on a hierarchically weighted matching strategy, where smaller weights imply a greater focus on the comparison of skeletons. The value domain of this is [0, 1], where 1 indicates the farthest distance. However, we find that their method may lead to the overcounting\nproblem for repeated consecutive spans 3. We revise the original algorithm to solve the problem while maintaining efficiency. The details of the revised algorithm are shown in Appendix A.2.1 for ease of use.\nWe additionally implement the algorithm of Convolutional Tree Kernel (CTK) similarity proposed in (Collins and Duffy, 2001) to fairly illustrate the syntactic phenomenon. In contrast to distance, it measures the similarity between a pair of tree structures by counting the number of tree fragments in common. The value domain of this algorithm is also [0, 1], where 1 means the maximum similarity. Intra-clique Syntactically Analysis. To exhaustively investigate the syntactic divergence on the cliques, we calculate the average syntactic distance/similarity in each individual clique based on the algorithms described above. The result is shown in Figure 3, where the horizontal axis and vertical axis are the output and the discounting weights of two algorithms, respectively.\nOverall, we observe that the values of syntactic distance and syntactic similarity are mainly scattered between [0.6, 0.9] and [0.0, 0.7], respectively, indicating that most of the cliques exhibit significant syntactic discrepancies. Another notable observation is that the distribution of the HWS scatter representing the distance is closer to 1 as the discount weight decreases, suggesting that the differences in syntactic skeletons are more significant in ROBUST. Inter-cliques Syntactically Analysis. Going be-\n3For two strings s1s3s4 and s1s2s1 with consecutive span s1 in common (e.g, SVPNP and SVPNPVP), the resulting distance may increase with the repetition of span s1.\nyond the individual clique, we further explore the syntactic divergence over all cliques. As shown in Figure 4, we average the mean of clique-wise syntactic distance/similarity on all cliques, based on the linearly increased discounting weights. We find that the average similarity of syntactic trees on ROBUST decreases rapidly as the discounted weight of the algorithm increases. Considering that increasing the weights implies a reduced focus on the low-level tree fragments, this result suggests that ROBUST involves prominent variability in the high-level skeleton of syntactic trees."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we explore the robustness of existing successful OpenIE systems and further analyze the impact of different model architectures on robustness. We first introduce the proposed ROBUST metric, which calculates the robustness performance on a clique, and then extensively evaluate six typical models from three major categories and a large language model ChatGPT. Furthermore, based on the clique structure, we analyze the correlation between the variances of the model performance and the syntactic divergences in cliques."
        },
        {
            "heading": "4.1 Evaluation Metrics",
            "text": "The existing widely used CaRB scorer computes pairwise matching scores based on extractions on a sentence. Though accurate, it has rather limitations. We extend this scorer on cliques to calculate the robustness scores. The CaRB Metric. To evaluate the correctness of system tuples, CaRB first creates an all-pair matching table, with each column as a system tuple and each row as a gold tuple, and computes precision and recall scores in each cell. Then, it calculates the overall recall R by averaging the maximum values of all rows and the overall precision P by averaging the one-to-one precisions between system tuples and gold tuples in the order of the best match score to the worst. Finally, the overall F1 is computed with R and P . The ROBUST Metric. An OpenIE system is considered robust if it behaves consistently on sentences with the same underlying knowledge meaning but differing syntactic and expressive variations, indicating the preservation of knowledge invariance. Therefore, we naturally calculate the robustness scores of a model on each clique.\nGiven a clique including k sentences C =\n{s1, ..., sk} in ROBUST, we first calculate the P/R/F1 scores of the model on each sentence, and then select the scores from the sentence with the worst F1 as the ultimate robustness scores P robust/Rrobust/F robust1 . As mentioned above, we can compute the pair-wise P/R/F1 scores based on the CaRB scorer.\nIt is noteworthy that the ROBUST evaluation metric is compatible with existing benchmarks because we calculate on the order of magnitude of one sentence, and we can directly compare our robustness scores with CaRB and others."
        },
        {
            "heading": "4.2 Evaluation Models",
            "text": "To exhaustively evaluate the robustness of existing paradigms, we select six typical OpenIE approaches from 3 categories. (1) Rule-based models, which adopt linguistic patterns to identify knowledge facts, including OpenIE4 (Christensen et al., 2011), ClauseIE (Del Corro and Gemulla, 2013), and OpenIE5 (Saha et al., 2017, 2018). (2) Independent NN-based models, that train neural networks from scratch with designed architecture, including RnnOIE (Stanovsky et al., 2018) and SpanOIE (Zhan and Zhao, 2020). (3) PLMbased models, that rely on a pre-trained language model usually trained on a large-scale text corpus, including OpenIE6 (Kolluru et al., 2020a) which introduces a novel iterative grid labeling architecture, which treats OpenIE as a 2-D grid labeling task to produce extractions gradually based on BERT.\nWe also evaluate the OpenIE performance of ChatGPT. We use the python API interface of gpt-\n3.5-turbo version4 for all experiments. We perform few-shot experiments with manually constructed prompts and sampled demonstrations for CaRB and ROBUST benchmarks. The prompt template is available in Appendix A.3."
        },
        {
            "heading": "4.3 Major Results",
            "text": ""
        },
        {
            "heading": "4.3.1 Results on Typical OIE Models",
            "text": "We run the source code of all baselines on both CaRB and ROBUST and compute the average scores across all samples. All results are shown in Table 2. Note that although the ROBUST scores are calculated in a different environment than CaRB, it still offers a fair comparison due to the calculation manner. Based on the result, we can see that current successive OpenIE systems experience a considerable performance decline on ROBUST across the board. Compared with CaRB, the average degradation for precision, recall, and the F1 score is 20%, 15%, and 18%, respectively. This observation suggests that research on the robustness of existing OpenIE models still needs to be completed, as overly idealized evaluations encourage models to match fixed expressions strictly.\nWith the concrete comparison of model architectures, we find that the SpanOIE model demonstrates a relatively small decrease in all three scores compared to other models, indicating its robustness to syntactic transformations. This result suggests that the extraction strategy of enumerating geometric spans is, to some extent, independent of syntactic drift, making it less susceptible to sentence form transformations.\n4the experimental period is 2023.04.01\u20132023.05.16."
        },
        {
            "heading": "4.3.2 Results on ChatGPT",
            "text": "We evaluate ChatGPT\u2019s OpenIE capability on CaRB and ROBUST. We randomly select 1/3/5/10 demonstrations from CaRB, and prompt ChatGPT to extract knowledge tuples by incorporating these demonstrations. We exclude sentences that belong to the same clique as the demonstrations during extraction. The result shows that ChatGPT exhibits impressive capability on CaRB, attaining a 51.6 F1 score in 10-shot setting, comparable to the supervised state-of-the-art model OpenIE6. However, it still faces the robustness problem, as evidenced by a decline in the F robust1 score to 27.5 on ROBUST in the same setting.\nWe also investigate the impact of ChatGPT\u2019s performance on the diversity of demonstrations. We first randomly select 100 pairs of cliques {(Ci, Cj)|(Ci = (s1i , s2i , ...)}100 from ROBUST. For each sentence in clique Ci, we prompt ChatGPT by specifying 1/2/3/4 demonstrations from clique Cj . We then calculate the CaRB F1 score for each sentence (shown in blue), the average CaRB F1 score for all sentence (s1i , s 2 i , ...) (shown in orange), and the ROBUST F robust score on all sen-\ntence in clique Cj (shown in green). The results in Figure 6b show that the correctness and robustness of ChatGPT can be improved by giving more diversified demonstrations."
        },
        {
            "heading": "4.4 Detailed Analysis",
            "text": "In this section, we investigate the coherence among cliques in ROBUST, as well as the variations in model performance across different cliques. Is the evaluation of model performance consistent across cliques? It is necessary to investigate whether our evaluation of the model is consistent across the majority of cliques in order to explore the internal consistency of our data samples. Based on the main results, we calculate the F1 score variance in each clique for three representative models, RnnOIE, SpanOIE, and OpenIE6. The distribution of the number of cliques based on variance is depicted in Figure 5a. We find that the majority of cliques exhibit relatively slight variances, indicating a high degree of consistency among robustness cliques. In addition, we sample 11 subsets of interval 100 in ROBUST and calculate the Person\u2019s Correlation Coefficient between the average F robust of OpenIE6 on each subset and the number of cliques of each subset. This result is \u22120.1480, indicating a weak correlation between these two factors. How does the syntactic divergence affect the performance of models? Benefiting from the data structure of ROBUST, we can further investigate the effect of syntactic divergence on the performance of models. Concretely, for each clique, we calculate the average HWS/CTK values between all pairs of sentences and the variance of F1 across all sentences. The result is shown in Figure 55. The results indicate a general trend where the variance of model performance decreases with increasing\n5We divide all samples into five intervals and calculate the average variance to avoid abnormal values.\nsyntactic divergence. Based on the main experiment results, which indicate low performance of models on the overall benchmark, the observed degradation implies a consistent trend of poorer model performance in more open scenarios."
        },
        {
            "heading": "5 Related Work",
            "text": "OpenIE Approaches. The OpenIE task was first proposed by (Banko et al., 2007) and is a fundamental NLP task. Earlier models focused on statistical or rule-based methods to handle this task (Christensen et al., 2011; Schmitz et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Pal et al., 2016; Saha et al., 2017, 2018). Recently, with the rapid development of deep representation learning, many supervised neural models have been proposed for OpenIE. These approaches could be roughly classified into two lines: 1.Sequence Labeling-based models. RnnOIE (Stanovsky et al., 2018) applies a BiLSTM transducer, extending deep Semantic Role Labeling models to extract tuples. SenseOIE (Roy et al., 2019) leverages an ensemble of multiple unsupervised OpenIE systems\u2019 outputs and the lexical and syntactic information to improve performance. SpanRel (Jiang et al., 2020) represents the OpenIE task in a single format consisting of spans and relations between spans. SpanOIE (Zhan and Zhao, 2020) predicts the candidate relation spans and classifies all possible spans of the sentence as subject or object for each span. Multi2OIE (Ro et al., 2020) first predicts all relational arguments by BERT and then predicts the subject and object arguments associated with each relation using multi-headed attention. OpenIE6 (Kolluru et al., 2020a) provides an iterative grid labeling architecture, which treats OpenIE as a 2-D grid labeling task. 2.Sequence Generative models. Neural Open IE (Cui et al., 2018) and Logician (Sun et al., 2018) generate OpenIE extractions by a seq2seq paradigm. IMoJIE (Kolluru et al., 2020b) leverages a BERT-based encoder and generates the next extraction which is fully conditioned on the extractions produced so far. OpenIE Benchmarks. Several benchmark datasets have been proposed to evaluate existing OpenIE approaches. OIE2016 (Stanovsky and Dagan, 2016) developed a method to create a large-scale OpenIE dataset using QA-SRL annotations (He et al., 2015) which was found to be noisy with missing extractions. After that, CaRB (Bhardwaj et al., 2019) and Re-OIE2016 (Zhan and\nZhao, 2020) re-annotated the corpus to improve the dataset\u2019s quality for more accurate evaluation. Wire57 (Lechelle et al., 2019) provided high-quality expert annotations, but the size is too small to serve as a comprehensive test dataset with only 57 sentences. DocOIE (Dong et al., 2021) argued that in reality a sentence usually exists as part of a document rather than standalone; the contextual information can help models understand it better and annotate a document-level OpenIE dataset. LSOIE (Solawetz and Larson, 2021) was built by converting the QA-SRL BANK 2.0 dataset (FitzGerald et al., 2018) to OpenIE which had a significant improvement over previous work in terms of data quantity. BenchIE (Gashteovski et al., 2022) created a fact-based benchmark and framework for multi-faceted comprehensive evaluation of OpenIE models in the multi-lingual setting.\nDespite the widespread interest in these benchmarks and the related OpenIE approaches provides promising results. However, the traditional peer-topeer matching-based evaluation can not measure the robustness of those approaches, where the syntax and expression may be various with underlying meaning (Qi et al., 2023). This work significantly fills the gap between traditional metrics and missed robustness evaluation for OpenIE and calls for more efforts in this research area."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "In this work, we propose ROBUST, a large-scale human-annotated OpenIE benchmark consisting of 1272 robustness testing cliques, where each clique contains sentences with different syntactic and expressive variations while conveying the same underlying knowledge meaning. We introduce our methodology for constructing the benchmark, including a syntactically and expressively diverse paraphrase generation, and a two-stage manual annotation. A comprehensive analysis is then performed to demonstrate the consistency of the proposed data with the real world. We finally perform extensive experiments on existing successive models as well as a representative large language model, and the results show that the robustness of existing methods is far from satisfied. The further detailed analysis demonstrates the substantial internal coherence of our benchmark, providing inspiration for the future development of robustness benchmarks."
        },
        {
            "heading": "Acknowledgement",
            "text": "We sincerely appreciate the three reviewers from ACL2023, who provided thorough reviews and suggestions during the initial submission of this work. This work is supported by the National Natural Science Foundation of China (No. 62277033). It also got partial support from National Engineering Laboratory for Cyberlearning and Intelligent Technology, and Beijing Key Lab of Networked Multimedia. This work is also supported by a grant from the Institute for Guo Qiang, Tsinghua University (2019GQB0003) and the NSFC Youth Project (62006136)."
        },
        {
            "heading": "7 Limitations",
            "text": "We have presented a dataset with metrics to evaluate the robustness of OpenIE models in this paper. However, there are still several limitations that need to be improved in further study. First, there are a few studies exploring the pre-trained language models to perform zero-shot information extraction with advantages. To the lack of open source code, we have not explored the robustness performance of these zero-shot models. Second, we think the robustness problem generally exists in the NLP community, we remain the extensive study of robustness examination for more domains and models in future works."
        },
        {
            "heading": "8 Ethic Consideration",
            "text": "There are two major considerations for conducting the evaluation of our proposed new benchmark. First, the source sentences are selected as same as CaRB, the original dev and test splits of OIE2016 in the open domain source of Wall Street Journal text and Wikipedia. All these data files are leveraged for the research purpose, and the result will be publicly available. Second, the annotators in this research are paid a salary higher than the market average and further allowed to choose flexible working time for human rights. For data utilization, we will make all annotation results publicly available under the CC BY-SA 4.0 license (free for research use)."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Annotation Details",
            "text": "We have the following detailed annotation information. Who: For Task1 and Task2, we employed two separate annotation teams consisting of 6 and 9 students respectively, who are all majoring in CS at universities. We ensured their professionalism through the tutorials and a final examination. Where: As both tasks are easy to read and write for annotators, we distributed the data directly without using a special annotation platform. Quality: We adopted a batched iterative annotation and evaluation process to ensure that the sampling accuracy is above 90%. License: We will release all annotation results under the CC BY-SA 4.0 license (free for research use)."
        },
        {
            "heading": "A.1.1 Paraphrase Annotation Process",
            "text": "The goal of paraphrase annotation is to correct the automatically generated sentences from the models based on human intelligence. Overall, we adopt an iterative step of combining human annotation paired with expert evaluation to ensure accuracy and efficiency. In each iteration, at least three human workers who are fluent in English reading and writing annotate a batch of samples, and then two domain experts will check the annotation results on a random sample of 40% of the batch. The batch annotations will be accepted until the validation accuracy is greater than 90%. For the annotation of each paraphrase, the annotators are asked to correct the sentence with syntactic, phrasal, or semanticdifferent mistakes against the original sentence."
        },
        {
            "heading": "A.1.2 N-tuples Annotation Process",
            "text": "We leverage the same iterative annotation strategy with the paraphrase annotation for OpenIE N-tuples annotation. In particular, we design an annotation flowchart for the workers according to the similar process in CaRB, by dividing the task into 4 steps: (1) identifying the relation, (2) identifying the arguments for that relation, and (3) optionally identifying the location and time attributes for the tuple. The same validation meaner with the paraphrase annotation is adopted to reach each acceptable annotation batch.\nAlgorithm 1 HWS Distance Input: Constituency parses T1, T2 of sentences\ns1, s2, pruning height h, discount factor \u03b1 Output: Syntactic distance d between s1, s2\n1: Get trees T h1 , T h 2 pruned at height h, and their\nlevel-order traversal sequences q1, q2 2: Initialize total length and count l = 0;m = 0 3: A[i][1] = 1 if q1[i] == q2[1], i = 1, ..., q1.len 4: A[1][j] = 1 if q1[1] == q2[j], j = 1, ..., q2.len 5: J[1] = 1 if q1[i] == q2[1], i = 1, ..., q1.len 6: I[1] = 1 if q1[1] == q2[j], j = 1, ..., q2.len 7: for i = 2 \u2192 q1.len do 8: for j = 2 \u2192 q2.len do 9: if q1[i] == q2[j] then 10: if A[i\u2212 1][j\u2212 1] \u2265 I[i] && A[i\u2212 1][j \u2212 1] \u2265 J [j] then 11: A[i][j] = A[i\u2212 1][j \u2212 1] + 1 12: I[i] = A[i\u2212 1][j \u2212 1] + 1 13: J [i] = A[i\u2212 1][j \u2212 1] + 1 14: end if 15: else 16: A[i][j] = 0 17: if A[i\u2212 1][j \u2212 1] > 1 then 18: l = l + A[i\u2212 1][j \u2212 1]\u00d7 \u03b1m 19: m++ 20: end if 21: end if 22: end for 23: end for 24: if A[i][j] \u2265 1 then 25: l = l + A[i][j]\u00d7 \u03b1m 26: end if 27: Return 1\u2212 l/min(q1.len, q2.len)"
        },
        {
            "heading": "A.2 Algorithms Details",
            "text": ""
        },
        {
            "heading": "A.2.1 Hierarchically Weighted Syntactic Distance Algorithm (Revised)",
            "text": "The revised Hierarchically Weighted Syntactic Distance Algorithm (HWS distance) is shown in algorithm 1. We fix the over-counting problem for repeated consecutive spans while preserving the efficiency with the same time complexity in the original work (Qi et al., 2023)."
        },
        {
            "heading": "A.2.2 Diversified Filtering Process",
            "text": "We perform diversified filtering based on BLEU scores between all pairs of sentences in each set of generated paraphrases to maintain the most diverse paraphrases. For example, given the generated paraphrases following:\nAs shown in Figure 7, we first calculate the BLEU scores between all pairs of paraphrases (shown on the edges). We then find the two sentences p1, p4 with the maximum BLEU score. Because the lengths of these two sentences are larger than 2/3 of the original sentence, we then calculate the summation of scores from each of them to all other sentences which results sum(p1, p/1) = 136.9 and sum(p1, p/1) = 158.7, and remove the sentence p4 that has larger summation score. We repeat the strategy above to remove the sentence p1 and obtain 3 expressively diverse paraphrases."
        },
        {
            "heading": "A.3 Prompts and Analysis for ChatGPT",
            "text": ""
        },
        {
            "heading": "A.3.1 Prompt Design",
            "text": "We create a prompt template for the task of OpenIE to query the ChatGPT. An example of a 1-shot prompt is shown in Figure 8, where the highlighted demonstration and the variable <sentence> can be replaced with specified examples."
        },
        {
            "heading": "A.3.2 Performance with Syntactic Correlations",
            "text": "In this section, we further investigate the correlation between the model performance and syntactic distance of demonstrations and questions for the ChatGPT model. We first randomly sample a set of 100 pairs of cliques {(Ci1, Ci2)|i = 1, ..., 100}\nin ROBUST. Then for each pair, we select all examples in clique Ci1 as demonstrations and select all sentences in Ci2 as questions to calculate the F robust1 -score. For syntactic correlations, we first calculate the averaged value ai between question i and all sentences in C1 and further calculate the average on (a1, a2, ...) as the final correlation on current clique-pairs. We divide the scores into several intervals and compute the average value in each corresponding interval to avoid abnormal values. The results based on both implementations of HWS distance and Tree Kernel similarity as the syntactic correlation are shown in Figure 9.\nIn the left figure of the result, we can see that the F robust1 -score of the model gradually increases as the average syntactic similarity of the two cliques increases. The same observation is also shown in the right figure with the averaged syntactic distance between two cliques. These results suggest that ChatGPT is sensitive to the syntactic distribution between questions and demonstrations and that giving demonstrations with similar syntactic distribution enhances the effectiveness of ChatGPT."
        },
        {
            "heading": "A.4 Error Analysis for OIE Systems",
            "text": "We conduct error analysis for three typical OpenIE models OpenIE4, SpanOIE, and OpenIE6 on a robustness clique. The model predictions with the CaRB and ROBUST scores are shown in Table 4.\nFirst, we can see that the sentences in the clique exhibit a significant syntactic and expressive divergence. It implies that the constructed data source satisfies the expectation. Second, we find all sentences in the clique have more than one extraction, while the OpenIE4 and OpenIE6 models predict the extractions insufficiently, which causes a lower recall. On the other hand, the SpanOIE model outputs predictions by enumerating all possible geometric spans, which build sufficient outputs regardless of syntactic features. This architecture offers SpanOIE a consistent performance."
        }
    ],
    "title": "Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction",
    "year": 2023
}