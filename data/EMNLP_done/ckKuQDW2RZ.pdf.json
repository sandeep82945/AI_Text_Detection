{
    "abstractText": "Mathematical understanding and reasoning are crucial tasks for assessing the capabilities of artificial intelligence (AI). However, existing benchmarks either require just a few steps of reasoning, or only contain a small amount of data in one specific topic, making it hard to analyse AI\u2019s behaviour with reference to different problems within a specific topic in detail. In this work, we propose CONIC10K, a challenging math problem dataset on conic sections in Chinese senior high school education. Our dataset contains various problems with different reasoning depths, while only the knowledge from conic sections is required. Since the dataset only involves a narrow range of knowledge, it is easy to separately analyse the knowledge a model possesses and the reasoning ability it has. For each problem, we provide a high-quality formal representation, the reasoning steps, and the final solution. Experiments show that existing large language models, including GPT-4, exhibit weak performance on complex reasoning. We hope that our findings could inspire more advanced techniques for precise natural language understanding and reasoning. Our dataset and codes are available at https://github.com/whyNLP/Conic10K.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haoyi Wu"
        },
        {
            "affiliations": [],
            "name": "Wenyang Hui"
        },
        {
            "affiliations": [],
            "name": "Yezeng Chen"
        },
        {
            "affiliations": [],
            "name": "Weiqi Wu"
        },
        {
            "affiliations": [],
            "name": "Kewei Tu"
        },
        {
            "affiliations": [],
            "name": "Yi Zhou"
        }
    ],
    "id": "SP:b9451d0fdd1b7b83ec1f12038ad214b06651f569",
    "references": [
        {
            "authors": [
                "Aida Amini",
                "Saadia Gabriel",
                "Shanchuan Lin",
                "Rik Koncel-Kedziorski",
                "Yejin Choi",
                "Hannaneh Hajishirzi."
            ],
            "title": "MathQA: Towards interpretable math word problem solving with operation-based formalisms",
            "venue": "Proceedings of the 2019 Conference",
            "year": 2019
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Tim Dettmers",
                "Mike Lewis",
                "Sam Shleifer",
                "Luke Zettlemoyer."
            ],
            "title": "8-bit optimizers via block-wise quantization",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.",
            "year": 2022
        },
        {
            "authors": [
                "Zhengxiao Du",
                "Yujie Qian",
                "Xiao Liu",
                "Ming Ding",
                "Jiezhong Qiu",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "GLM: General language model pretraining with autoregressive blank infilling",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Saurav Kadavath",
                "Akul Arora",
                "Steven Basart",
                "Eric Tang",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring mathematical problem solving with the MATH dataset",
            "venue": "Proceedings of the Neural Information Processing Systems Track",
            "year": 2021
        },
        {
            "authors": [
                "Mark Hopkins",
                "Ronan Le Bras",
                "Cristian PetrescuPrahova",
                "Gabriel Stanovsky",
                "Hannaneh Hajishirzi",
                "Rik Koncel-Kedziorski."
            ],
            "title": "SemEval-2019 task 10: Math question answering",
            "venue": "Proceedings of the 13th International Workshop on Semantic Evalua-",
            "year": 2019
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen."
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Tom Kwiatkowski",
                "Eunsol Choi",
                "Yoav Artzi",
                "Luke Zettlemoyer."
            ],
            "title": "Scaling semantic parsers with on-the-fly ontology matching",
            "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1545\u20131556, Seattle,",
            "year": 2013
        },
        {
            "authors": [
                "Guillaume Lample",
                "Fran\u00e7ois Charton."
            ],
            "title": "Deep learning for symbolic mathematics",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.",
            "year": 2020
        },
        {
            "authors": [
                "Hunter Lightman",
                "Vineet Kosaraju",
                "Yura Burda",
                "Harri Edwards",
                "Bowen Baker",
                "Teddy Lee",
                "Jan Leike",
                "John Schulman",
                "Ilya Sutskever",
                "Karl Cobbe."
            ],
            "title": "Let\u2019s verify step by step",
            "venue": "arXiv preprint arXiv:2305.20050.",
            "year": 2023
        },
        {
            "authors": [
                "Wang Ling",
                "Dani Yogatama",
                "Chris Dyer",
                "Phil Blunsom."
            ],
            "title": "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
            "year": 2017
        },
        {
            "authors": [
                "Pan Lu",
                "Ran Gong",
                "Shibiao Jiang",
                "Liang Qiu",
                "Siyuan Huang",
                "Xiaodan Liang",
                "Song-Chun Zhu."
            ],
            "title": "Inter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning",
            "venue": "Proceedings of the 59th Annual Meeting of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Pan Lu",
                "Liang Qiu",
                "Wenhao Yu",
                "Sean Welleck",
                "Kai-Wei Chang."
            ],
            "title": "A survey of deep learning for mathematical reasoning",
            "venue": "arXiv preprint arXiv:2212.10535.",
            "year": 2022
        },
        {
            "authors": [
                "Sewon Min",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Hannaneh Hajishirzi."
            ],
            "title": "MetaICL: Learning to learn in context",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Matthew Finlayson",
                "Pan Lu",
                "Leonard Tang",
                "Sean Welleck",
                "Chitta Baral",
                "Tanmay Rajpurohit",
                "Oyvind Tafjord",
                "Ashish Sabharwal",
                "Peter Clark",
                "Ashwin Kalyan."
            ],
            "title": "LILA: A unified benchmark for mathematical reasoning",
            "venue": "Proceedings of",
            "year": 2022
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Arindam Mitra",
                "Neeraj Varshney",
                "Bhavdeep Sachdeva",
                "Peter Clark",
                "Chitta Baral",
                "Ashwin Kalyan."
            ],
            "title": "NumGLUE: A suite of fundamental yet challenging mathematical reasoning tasks",
            "venue": "Proceedings of the 60th Annual Meeting of",
            "year": 2022
        },
        {
            "authors": [
                "banie",
                "Zaid Alyafeai",
                "Albert Webson",
                "Edward Raff",
                "Colin Raffel"
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "CoRR, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Arkil Patel",
                "Satwik Bhattamishra",
                "Navin Goyal"
            ],
            "title": "Are NLP models really able to solve simple math word problems",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Guilherme Penedo",
                "Quentin Malartic",
                "Daniel Hesslow",
                "Ruxandra Cojocaru",
                "Alessandro Cappelli",
                "Hamza Alobeidli",
                "Baptiste Pannier",
                "Ebtesam Almazrouei",
                "Julien Launay"
            ],
            "title": "The RefinedWeb dataset for Falcon LLM: outperforming curated corpora",
            "year": 2023
        },
        {
            "authors": [
                "Christopher Klamm",
                "Colin Leong",
                "Daniel van Strien",
                "David Ifeoluwa Adelani"
            ],
            "title": "BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100",
            "year": 2022
        },
        {
            "authors": [
                "J\u00f6rg Tiedemann",
                "Santhosh Thottingal."
            ],
            "title": "OPUSMT \u2013 building open translation services for the world",
            "venue": "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation, pages 479\u2013480, Lisboa, Portugal. European Associa-",
            "year": 2020
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V. Le",
                "Ed H. Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "The Eleventh International Conference",
            "year": 2023
        },
        {
            "authors": [
                "Yan Wang",
                "Xiaojiang Liu",
                "Shuming Shi."
            ],
            "title": "Deep neural solver for math word problems",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 845\u2013854,",
            "year": 2017
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "The Tenth International Conference on Learning Representa-",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V. Le",
                "Denny Zhou."
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Ping Yang",
                "Junjie Wang",
                "Ruyi Gan",
                "Xinyu Zhu",
                "Lin Zhang",
                "Ziwei Wu",
                "Xinyu Gao",
                "Jiaxing Zhang",
                "Tetsuya Sakai."
            ],
            "title": "Zero-shot learners for natural language understanding via a unified multiple choice perspective",
            "venue": "Proceedings of the 2022 Conference",
            "year": 2022
        },
        {
            "authors": [
                "Xiaotian Zhang",
                "Chunyang Li",
                "Yi Zong",
                "Zhengyu Ying",
                "Liang He",
                "Xipeng Qiu."
            ],
            "title": "Evaluating the performance of large language models on GAOKAO benchmark",
            "venue": "CoRR, abs/2305.12474.",
            "year": 2023
        },
        {
            "authors": [
                "Wei Zhao",
                "Mingyue Shang",
                "Yang Liu",
                "Liang Wang",
                "Jingming Liu."
            ],
            "title": "Ape210k: A large-scale and template-rich dataset of math word problems",
            "venue": "CoRR, abs/2009.11506.",
            "year": 2020
        },
        {
            "authors": [
                "Yi Zhou."
            ],
            "title": "From first-order logic to assertional logic",
            "venue": "Artificial General Intelligence: 10th International Conference, AGI 2017, Melbourne, VIC, Australia, August 15-18, 2017, Proceedings, pages 87\u201397, Cham. Springer International Publishing.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Mathematical understanding and reasoning ability is an important component of human intelligence. Such an ability is the foundation of data analysis, financial applications and scientific research. Though there have been lots of studies\n3 Equal Contribution. \u2663 Corresponding Authors. \u2020 Work completed while the author was at ShanghaiTech\nUniversity.\n(Lample and Charton, 2020; Wei et al., 2022b), mathematical reasoning are far from being solved by existing methods (Lu et al., 2022), even with symbolic reasoners (Hopkins et al., 2019) and large language models (LLMs) (Lightman et al., 2023). To evaluate and analyse the mathematical ability, various datasets and benchmarks have been proposed in recent years (Zhao et al., 2020; Hendrycks et al., 2021; Mishra et al., 2022b,a). However, these datasets or benchmarks often suffer from the following problems: (1) The problems can be solved with only a few reasoning steps, so language models may rely on shallow heuristics to achieve high performance (Patel et al., 2021); (2) The dataset covers a wide range of topics and hence there is only a small amount of data for each topic, which makes it hard to distinguish whether the model fails because of a lack of background information, or due to weak reasoning ability.\nTo address the above issues, we propose CONIC10K, an open-ended math problem dataset on conic sections in Chinese senior high school education. This dataset contains 10,861 carefully annotated problems, each one has a formal representation, the corresponding text spans, the answer, and natural language rationales. Figure 1 shows an example problem in our dataset. To evaluate the mathematical understanding and reasoning ability, we perform two different tasks on existing LLMs: semantic parsing and mathematical question answering (mathQA). Semantic parsing assesses a language model\u2019s ability to understand mathematics. The model is required to translate the math problem in natural language into its formal meaning representations. MathQA jointly evaluates the language model\u2019s ability of mathematical understanding and reasoning. The model needs to gen-\nerate the answers to questions. Since the topic of CONIC10K is restricted to conic sections, the knowledge required to solve different problems is the same, while the only difference is the difficulty in reasoning. Therefore, if the model is able to solve simple problems but not hard ones, we are assured that the failure lies in the lack of ability in mathematical reasoning.\nOur experiments show that current models obtain good performance in semantic parsing. However, in mathQA, these models are far from being satisfactory. When performing zero-shot chain of thought (CoT) (Wei et al., 2022b) prompting, the best model GPT-4 (OpenAI, 2023) can only achieve 15.5% accuracy using human evaluation. When finetuning is further applied, the best model ChatGLM-6b (Du et al., 2022) still obtains a poor accuracy of 22.5% under human evaluation. When we translate the problems into English and apply zero-shot CoT to reason in English, the accuracy of GPT-4 is 26.0%, which is still far below the performance of human experts at 57.5% with a 3-minute time limit for each problem. This shows that the poor performance is not due to the language being used but to a deficiency in reasoning ability. Therefore, we believe the mathematical reasoning ability of language models is still limited despite their huge success in natural language understanding.\nWe conclude our contributions as follows: 1) We propose CONIC10K, a challenging math problem dataset on conic sections in Chinese senior high school education, with high-quality annotations of formal representations; 2) We perform experiments to inspect the mathematical understanding and reasoning ability of LLMs separately; 3) We give detailed analysis on the model behaviour and conduct comprehensive case studies. We hope that our work could help the community to better analyse LLMs in mathematical understanding and reasoning and inspire more advanced techniques to enhance the mathematical reasoning ability of LLMs."
        },
        {
            "heading": "2 Related Work",
            "text": "There has been a wide range of datasets on math problems in the literature. MATHQA (Amini et al., 2019) and GSM8K (Cobbe et al., 2021) are math word problem datasets. They focus on open-domain understanding, where the objective is to extract a single equation based on the information about quantities in the problem, rather than mathematical reasoning. Similarly, Math23K\n(Wang et al., 2017) and Ape210K (Zhao et al., 2020) are popular datasets about Chinese math word problems with open-domain scenarios and simple reasoning steps. Geometry3K (Lu et al., 2021) is a geometry problem-solving dataset that provides formal representations, but the dataset size is small and the problems do not require complex reasoning. AQuA (Ling et al., 2017), NumGLUE (Mishra et al., 2022b) and Lila (Mishra et al., 2022a) are large-scale datasets of various math problems. They have been used as benchmarks in solving math word problems and mathematical reasoning tasks, but we find that these datasets require only a few reasoning steps. MATH (Hendrycks et al., 2021) is the one with the longest reasoning steps among these datasets. It has been used as a standard benchmark in recent work of LLMs (Lewkowycz et al., 2022; Lightman et al., 2023). However, while it covers a wide range of problems, it contains limited data in each specific topic, making it hard to analyse the model behavior in detail with reference to one topic. It also does not provide any formal representations. Our proposed CONIC10K contains problems of long reasoning steps using closed-domain knowledge and has highquality annotations with formal representations. A detailed comparison between the aforementioned datasets and CONIC10K is shown in Table 1."
        },
        {
            "heading": "3 Dataset",
            "text": ""
        },
        {
            "heading": "3.1 Formal Representation",
            "text": "We design a formal representation that avoids ambiguity and is close to natural language. Specifically, our representation is built upon Assertional Logic (Zhou, 2017). Assertional Logic (AL) is a powerful knowledge representation that is more expressive than first-order logic while easier to read and write for humans. In this work, we use a variant of AL with three components: declarations, facts and queries. Declarations define individuals with their types (e.g. G:Ellipse). Facts are assertions that describe the conditions in the problem (e.g. Focus(G)={F1, F2}). Queries are the terms that represent the goal of the problem (e.g. Range(Eccentricity(G))). See more details in Appendix A."
        },
        {
            "heading": "3.2 Dataset Format",
            "text": "An example is presented in Figure 1. For each question, we give 1) the question text in natural language with math formulas in LATEX, 2) the ra-"
        },
        {
            "heading": "G: Ellipse",
            "text": ""
        },
        {
            "heading": "F1, F2: Point",
            "text": "tionale in natural language, 3) the answer to the question, 4) the formal representation and 5) the text span corresponding to each sentence in the formal representation."
        },
        {
            "heading": "3.3 Dataset Construction",
            "text": ""
        },
        {
            "heading": "3.3.1 Data Collection",
            "text": "To construct the dataset, we first collect approximately 20,000 open-ended problems about conic sections from two websites that focus on Chinese high school education in image format. Each problem image contains the problem text, rationale, and answer. Then, we use mathpix1 to convert these images into text. Since our dataset is focused on conic sections, we filter out problems that involve knowledge from other topics such as sequences and solid geometry. After that, we remove duplicated\n1https://mathpix.com/\nproblem using fuzzy matching. After the above process is finished, the size of the dataset is reduced from around 20,000 to approximately 14,000."
        },
        {
            "heading": "3.3.2 Annotation",
            "text": "To ensure the correctness of the data and avoid ambiguities, we apply strict quality control during the annotation process2. The complete process is as follows:\nInitiation We first build a small dataset with hundreds of samples, write the annotation guidelines and design a rule-based AI assistant for annotation. The rule-based AI assistant is able to recognize LATEX math expressions and complete simple formal representations, which greatly accelerates the annotation process and reduces annotation errors.\n2See Appendix A.3 for more details.\nVerification We select the annotators from a group of candidates by their performance on the small dataset. These annotators are provided with annotation guidelines along with hundreds of samples. Annotators with the best performance will take part in the rest of the annotation process.\nAnnotation We ask the annotators to further filter out problems about other topics, write the formal representation, select the corresponding text spans and fix the incorrectly recognized problem texts and answers. Each problem is annotated by two annotators, and then validated by another validator with an automated tool for comparison. We also randomly check 3% of the annotations. This process takes 4 months in total.\nFinalization After the annotation is finished, we train a language model3 through 5-fold crossvalidation, manually check the inconsistency between model predictions and the annotated formal representations, and fix the errors in annotations. This helps us correct another 2% of the data. Then we randomly split the dataset into train, validation, and test sets with the ratio 7.5:1:2. The train set size is 7,758, the validation set size is 1,035, and the test set size is 2,068. We proceed to the evaluation of LLMs with this split."
        },
        {
            "heading": "3.4 Dataset Statistics",
            "text": "Table 2 presents the basic statistics about CONIC10K. The problems in our dataset tend to be long and complex. Besides these metrics, we also estimate the number of reasoning steps by the\n3We finetune the OPUS-mt-zh-en (Tiedemann and Thottingal, 2020). It is a machine translation model that translates Chinese into English.\nminimum number of rules required to get enough information to obtain an answer. Since the process of applying rules is subjective, we ask two graduate students to individually annotate the rules used to solve the problems. We uniformly sampled 30 problems from each of the datasets listed in Table 1 and ask the two students to annotate the reasoning steps. Results show that CONIC10K is the dataset with the second largest number of reasoning steps. The distribution of reasoning steps in CONIC10K is depicted in Figure 2. We show additional dataset statistics in Appendix B.\nTo facilitate model analysis, we divide the answers into 6 categories as described in Table 3. Figure 3 shows the distribution on these categories."
        },
        {
            "heading": "4 Experiments",
            "text": "This section describes our experiments to evaluate the mathematical understanding and reasoning abilities of various models."
        },
        {
            "heading": "4.1 Tasks",
            "text": "Based on data provided by CONIC10K, we introduce two tasks: semantic parsing and mathQA.\nSemantic parsing requires a model to translate math problems in natural language into formal representations, while mathQA needs a model to give correct solutions to math problems. The semantic parsing task aims solely at assessing the model\u2019s ability to understand mathematics, and the mathQA task jointly evaluates the model\u2019s ability of mathematical understanding and reasoning."
        },
        {
            "heading": "4.2 Models",
            "text": "We evaluated the performance of several popular pretrained models on the above two tasks. The models used for evaluation are as listed in Table 4."
        },
        {
            "heading": "4.3 Evaluation Details",
            "text": "Due to limited computation resources, we conducted full finetuning on models with size of less than 4B. For models around 7B, we performed parameter efficient finetuning using LoRA (Hu et al., 2022) and 8-bit quantization (Dettmers et al., 2022). We also apply zero-shot CoT inference without finetuning for models with sizes between 7B and 13B. The models evaluated in zero-shot CoT setting all have undergone instruction tuning or RLHF in their respective pretraining process. When finetuning, we use instruction tuning (Wei et al., 2022a) to train the models. The instructions are architecturespecific and task-specific, as depicted in Table 5.\nWhen finetuning language models, we use the following hyperparameter settings. We use AdamW as the optimizer. The learning rate is selected from {8e\u22125, 2e\u22125}, with a linear learning rate decay. For models using LoRA, we set target modules to q, k, v for Falcon-7b and to q, v for other models. The LoRA rank is set to 16 for models with size around 7B. To ensure a similar number of trainable parameters, we set the LoRA rank to 24 for Bloomz-3b and 32 for Bloomz-1b7. We use greedy decoding in all generations.\nIn zero-shot CoT inference for mathQA, we use the same prompt as GAOKAO-Benchmark (Zhang et al., 2023) to instruct the models to give an answer together with a rationale. In MathQA, we also experiment with in-context learning (Min et al., 2022), which adds in-context demonstrations of the task in the prompt, and self-consistency (Wang et al., 2023), which conducts majority voting on the sampled results on GPT-3.5-turbo. In semantic parsing, however, the formal representation is unknown to the above models. Since it requires more than 3,000 tokens to explain the syntax and semantics of each component in the formal language, which is out of the context length limit of most models listed above, we do not evaluate the performance of zero-shot CoT in semantic parsing.\nIn addition to the methods mentioned above, we also evaluate the following two methods in mathQA as a reference: (1) Guessing \u20182\u2019: Predicting the most frequent answer in the train set, which is \u20182\u2019. (2) Human Experts: We randomly select 20 problems from the test set and ask two graduate students to answer. Each problem has a 3-minute time limit. We report the average accuracy of these two students."
        },
        {
            "heading": "4.4 Metrics",
            "text": ""
        },
        {
            "heading": "4.4.1 Semantic Parsing",
            "text": "For semantic parsing, we evaluate the model predictions by micro-F1, macro-F1 and accuracy. The accuracy is the proportion of the problems that have a one-to-one match between all sentences in the prediction and the ground truth. Micro-F1 (mi-F1) and macro-F1 (ma-F1) are defined as follows:\nmi-F1 = 2 \u00b7 pr p+ r , (1)\nma-F1 = \u2211n\ni=1 F1i n\n(2)\nwhere n is the total number of problems, p = # of all matched sentences # of all predicted sentences is the overall precision, r = # of all matched sentences\n# of all gold sentences is the overall recall, F1i is the F1 score of problem i.\nTo compute the metric, we need to find the number of matched sentences between the prediction and ground truth. Since the formal representation is insensitive to individual naming, we enumerate all possible individual name mappings between prediction and ground truth and select the mapping\n5https://chat.openai.com/, we use GPT-3.5-turbo-0314 version.\n6we use GPT-4-0314 version.\nthat achieves the maximum number of matched sentences. We optimize the evaluation script by only considering individuals with the same type so that the evaluation time on the validation set and test set is acceptable."
        },
        {
            "heading": "4.4.2 MathQA",
            "text": "In mathQA, since it is nontrivial to automatically determine whether two answers are the same (e.g., 1/ \u221a 2 vs. \u221a 2/2, x \u2212 y = 0 vs. x = y, and 3x + 4y = 5 vs. 35x + 4 5y \u2212 1 = 0), we rely on human evaluation to determine the correctness of model answers."
        },
        {
            "heading": "5 Results and Discussions",
            "text": "In this section, we introduce and explain the results of the experiments. The main results of semantic parsing and mathQA are shown in Table 6 and Table 7 respectively."
        },
        {
            "heading": "5.1 Semantic Parsing",
            "text": "Language models show good ability of understanding on math problems after proper training.\nThe best model mT5-xl can successfully translate 84.6% of the problems into formal representations. For the problems it fails to accurately translate, the predictions only differ from the ground truth in minor details. The F1 score and accuracy from Bloomz family and Falcon-7b are much lower than other models. The performance of finetuned instruction tuned models is consistently better than that of finetuned base models.\nModels pretrained on code show strong ability in learning syntax. Models except for the mT5 family have been pretrained on code. The syntax error rates of these models are on average lower than that of the mT5 family, even though their F1\nscore and accuracy may be lower than the mT5 family. Since the formal representation resembles programming languages in syntax, pretraining on code may be able to help model to learn the syntax of formal representations more easily.\nIncreasing model size effectively improves model\u2019s performance in semantic parsing. From the results of the model families mT5, mT0 and Bloomz, we find that increasing the model size from the smallest to largest in our experiment can significantly improve the accuracy by at least 7.4%."
        },
        {
            "heading": "5.2 MathQA",
            "text": "Language models generally show poor performance on mathQA in CONIC10K. Under the zeroshot CoT setting, most models achieve an accuracy close to 0. Even after finetuning, the accuracy of the best model is still significantly lower than that of human experts by 35.0%.\nSimple problems under finetuning setting may not be simple under zero-shot CoT setting. Most models finetuned on CONIC10K have the best performance in Simple Numbers among the answer categories. However, when it comes to zeroshot CoT setting, GPT-4 and GPT-3.5-turbo obtain best accuracy in Coordinate. One possible reason is that after sufficient training on CONIC10K, the model can develop a shallow understanding of the task (Patel et al., 2021), including the frequent answers of a specific kind of questions. Since Simple Numbers are simpler in form and have fewer potential answers compared to Coordinates, being familiar with the answer distribution can effectively increase the probability to hit the correct answer. However, in zero-shot CoT setting, the model is unaware of these distributions, so it has no advantage in difficult problems that have simple answers.\nThe accuracy is close to 0 in zero-shot CoT. Under the zero-shot CoT setting, Bloomz-7b1 and Falcon-7b-inst show extremely poor performances with 0 accuracy in all problems. These models tend to generate repetitive patterns, and in most cases fail to give an answer. Other models except for GPT-4 generate text that looks like a valid rationale, but the majority of reasoning steps are incorrect. They often produce hallucinations in premises and rules, and derive wrong results. In Table 9, even with in-context demonstrations or majority voting, the performances are still low. We showcase some failing cases in Table 10.\nThe scaling law is less clear compared to semantic parsing. Though we observe that increasing the model size continuously and effectively improves model performance in semantic parsing, such a phenomenon disappears in mathQA tasks. In mT5 and mT0 series, large models do not necessarily outperform small models. Similar observations have been made in MATH (Hendrycks et al., 2021) where the authors find that accuracy on math problems increases only modestly with model size.\nChinese-oriented language models have better performance in mathQA in CONIC10K. In the zero-shot CoT setting, the two Chinese-oriented models, Ziya-13b and ChatGLM-6b, achieve the best performance below GPT-3.5-turbo. In the finetuning using LoRA setting, ChatGLM-6b achieve an accuracy of 22.5% and outperform other models by a large margin.\nTranslating problems into English does not make the performance of GPT-4 on par with human experts in mathQA. We translate the problems into English and evaluate GPT-4 in zeroshot CoT setting to determine whether the poor performance is due to language or long reasoning steps. The results in Table 8 show the performance is significantly improved from 15.5% to 26.0% by translating the problems into English. However, this accuracy is still low compared to 57.5% from human experts. Therefore, the primary challenge of mathQA in CONIC10K still lies in how to do mathematical reasoning correctly."
        },
        {
            "heading": "5.3 Case Study",
            "text": "We inspect and analyse both success and failure cases in the experiment, which leads us to some interesting findings.\nLLMs have limited ability in understanding long LATEX expressions. 9.7% of the incorrect predictions from mT0-xl are due to errors in translating simple but long LATEX expressions. Common failures include missing terms, flipped signs and incorrect copies. For example, the LATEX expression in the problem is x^2+y^2+2\\sqrt{2}x-4\\sqrt{2}y+10-r^2=0, but the translated sentence becomes -4*sqrt(2)*y +2*sqrt(2)*x+x^2+y^2+2=-r^2. In this example, we observe both a flipped sign and an incorrect constant. We do not observe similar errors in relatively short LATEX expressions.\nModels can hardly find shortcuts in reasoning in mathQA. We observe that models usually employ naive approaches to solve problems and fail to find shortcut solutions, which leads to more complicated computation and longer reasoning steps. The additional reasoning steps and computation make the models more likely to make mistakes during reasoning. Some examples of naive solutions from GPT-4 and the corresponding shortcut solutions are listed in Table 11 and 12.\nGPT-4 and GPT-3.5-turbo probably lack knowledge about certain concepts. When asked problems about focal distance, GPT-4 and GPT-3.5turbo keep giving incorrect answers and often give a value that is half of the ground truth. Based on these observations, we suspect that these two models lack knowledge about focal distance. We ask GPT-4 and GPT-3.5-turbo to explain what focal distance is in both Chinese and English, and they\nkeep defining it as the distance between the center of an ellipse or hyperbola and one of its foci instead of the correct definition, the distance between the two foci. A probable reason is that \u2018focal distance\u2019 is not a commonly used term within the English corpus, making the models unlikely to obtain correct knowledge about it."
        },
        {
            "heading": "6 Conclusion",
            "text": "We present CONIC10K, a math problem understanding and reasoning benchmark. It provides problems that require complex reasoning, while only involving knowledge about conic sections in Chinese senior high school education. We test popular LLMs on both semantic parsing and math question answering, inspecting model performance and behaviours. Results show that existing LLMs, including GPT-4, have poor performance in mathematical reasoning, while most models could achieve good performance in mathematical understanding (but not perfect yet). We analyse the model predictions in detail and find LLMs tend to hallucinate in reasoning, often fail to find the shortcuts solution, and may lack the knowledge to solve problems. We hope our dataset, CONIC10K, can help to discover the weaknesses of LLMs in mathematical understanding and reasoning and inspire more advanced techniques to enhance the mathematical reasoning ability of LLMs.\nLimitations\nCONIC10K is a dataset with high-quality formal representation annotations, but there are still some limitations:\n\u2022 We design the formal representation to be accurate, unambiguous and close to natural language, but such representation is not commonly used and does not fit any existing symbolic reasoners. The conclusion may not apply to other formal representations such as propositional logic and first-order logic, or rationales like executable programs.\n\u2022 In conic sections, the commonly used mathematical reasoning strategies could be limited. For example, our problems may require solving simultaneous equations systems, but not likely mathematical inductions. Therefore, our dataset cannot evaluate some reasoning strategies such as mathematical induction.\nEthics Statement\nCONIC10K is a dataset that requires massive data sources and heavy annotation. We claim that our work is free of ethical risks from the following perspectives:\nData Source The problems in CONIC10K are collected from two websites that do not limit the usage of data for education and research purpose. We strictly follow the term of use and manually check all the data to avoid inappropriate information in the annotation stage.\nAnnotation We hire a group of 14 annotators for formal representation annotation and sign a contract to prescribe the rights from both sides. We clearly state the purpose of our study and the future data use. These annotators are well-paid for their work. The authors take responsibility to maintain the annotation website, provide necessary documents, answer questions from the annotators and clean up the data."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Natural Science Foundation of China (61976139, 62250057), and by Shanghai Frontiers Science Center of Human-centered Artificial Intelligence and MoE Key Lab of Intelligent Perception and Human-Machine Collaboration (ShanghaiTech University)."
        },
        {
            "heading": "A Formal Representation",
            "text": "A.1 The Assertional Logic\nAssertional Logic (AL) (Zhou, 2017) is a formal representation where all kinds of knowledge are formalized by equality assertions. It builds upon the equality properties and the set theory. AL representations are human-friendly and it has been proved that the expressiveness of AL is stronger than first-order logic (or kth-order logic for any k \u2265 1).\nHere, we briefly introduce the syntax of AL. Given a specific domain, the syntactic structure of AL is composed of three components: individuals, concepts and operators. Individuals represent objects in the domain, concepts represent groups of objects and operators represent relationships and connections among individuals and concepts. Operators are similar to functions and predicates in first-order logic (FOL), but they could accept highorder constructs (concept, concept of concepts), which leads to the strong expressiveness of AL.\nAn assertion is of the form a = b, where a, b are two terms (individuals, either atomic or compound). The knowledge base of AL is just a set of assertions.\nA.2 Our Representation\nWe apply AL as our formal representation because of its strong readability. Our principle is that the formal representation should 1) avoid ambiguity. The formal representation should resolve the ambiguity in natural language and with the information inside the annotations, it should be possible to work out the solution by hand; 2) close to natural language. It should be able to represent the problem without rephrasing it; 3) simple and clear. Designing a representation with thousands of operators is definitely expressive and powerful, but it sacrifices the strength of logic and fails to extract common knowledge underneath.\nTherefore, we apply only 94 operators and 20 concepts (see Table 2) to represent all the problems in the dataset. To better accommodate the natural language, we also designed 3 pseudo operators: OneOf, WhenMin, WhenMax. These operators do not fit the semantics of AL, but greatly simplify the representation and are closer to natural language. Also, it is trivial to convert these operators to terms in AL.\nThere also has been evidence showing that rephrasing significantly impacts learning (Kwiatkowski et al., 2013). To avoid rephrasing, we write detailed documents for the annotators, ask them to raise questions when they are not confident and frequently check the data during annotation.\nWe design our representation in three components: declarations, facts and queries.\nDeclarations The declarations define individuals with their types. It has the format of var: type, where var is an individual and type is a concept. These sentences are a special representation of the\nassertion Is(var, type) = True. For simplicity, we allow defining multiple individuals in one sentence, with commas separating different individuals.\nFacts The facts are assertions that describe the conditions in the problem. For clarity, we allow the use of syntactic sugar, which includes <,\u2264, > ,\u2265,+,\u2212,\u00d7,\u00f7, ab. That is, a sentence could be an inequality such as a > b, which indicates an assertion (a > b) = True.\nQueries The queries are the terms that represent the target of the problem. They ought to be an assertion with the left-hand-side(LHS) the query term and the right-hand-side(RHS) an unknown individual in AL, but we use the simplest format during the annotation.\nA.3 Annotation Quality Control\nOur previous study shows that the annotation of formal language is extremely hard for humans. It is difficult for an experienced annotator to reach an accuracy above 50%. As a result, we employ multiple measures to control the dataset quality, including:\n1. We provide a rule-based AI assistant to complete most of the annotations with high precision.\n2. We only hire annotators with the highest performance on the small dataset we built in advance.\n3. During the annotation, we ask the annotators to raise questions whenever they are not confident about how to annotate. We provide detailed documents and dedicated help to ensure the correctness of the annotation.\n4. In addition to formal representations, we ask the annotators to annotate the text spans. We find it helps to increase the annotation accuracy.\n5. Each problem will be annotated by two annotators individually, then passed to another validator. We design a web UI which could automatically compare two annotations and extract the difference. The validator will determine which one is correct, or a third annotation is required.\n6. Every time the annotators finish 1000 annotations, we randomly sample 10 problems for additional checks. After all the annotations were finished, we randomly sample 200 problems for additional checks. In the additional check, we independently annotated the sampled problems, and then compare them with the existing annotation. We ask the annotators to do a thorough check if the accuracy is below 80%7.\n7. We provide competitive payments (> 150k CNY in total, \u2248 20k USD) to the annotators. We allow adequate time for the annotation process.\n8. After the annotation is finished, we finetune a zh-en translation model for further validation. We split the whole dataset into five random splits of the same size. Then, we pick four of them to finetune the model and collect predictions for the last split. We manually check all the problems whose prediction does not match the annotation. We repeat this process five times and obtain the final dataset."
        },
        {
            "heading": "B Additional Dataset Statistics",
            "text": "We show the frequency of keywords in Figure 4, and the distribution of question length in Figure 5. In question length, we count all latex commands such as \\frac, \\leftarrow as one token."
        },
        {
            "heading": "C Additional Results",
            "text": "We showcase some interesting outputs from models. Models including Bloomz-7b and ChatGLM-6b often generate redundant text and hallucinate when reasoning. We provide some samples in Table 10. GPT-4 shows a strong ability to perform reasoning\n7In the final check, about 99% of the 200 sampled formal representation annotations (without considering text spans) pass the check.\nin natural language, but its solution often fails to find the shortcuts designed in the problems. Such a solution often requires complicated calculations. Due to its limited ability in arithmetic calculation, the final answer from GPT-4 is likely to be incorrect. We provide some samples in Table 11 and 12.\nProblem Rationale & Solution\nThe equation of the line that passes through the point M(3,\u22121) and bisects the chord of the hyperbola x 2\n4 \u2212 y 2 = 1 is?\nNaive solution by GPT-4: First, we need to find two symmetric points on the hyperbola. Let these points be A(x1, y1) and B(x2, y2), which are symmetric with respect to the point M(3,\u22121). According to the properties of symmetric points, we have:\nx1 + x2 = 2 \u00b7 3 = 6\ny1 + y2 = 2 \u00b7 (\u22121) = \u22122\nSince points A and B lie on the hyperbola, they satisfy the equation of the hyperbola:\nx21 4 \u2212 y21 = 1 x22 4 \u2212 y22 = 1\nSubstituting y1 = \u22122\u2212 y2 into the first equation, we get:\nx21 4 \u2212 (\u22122\u2212 y2)2 = 1\nSubstituting x1 = 6\u2212 x2 into the above equation, we get:\n(6\u2212 x2)2\n4 \u2212 (\u22122\u2212 y2)2 = 1\nThis is a system of linear equations in terms of x2 and y2. By solving for x2 and y2, we can obtain the coordinates of points A and B. Solving, we find x2 = 6\u00b1 \u221a 2 2 and y2 = \u2213 \u221a 2 \u2212 1. Therefore, A ( 6+ \u221a 2 2 ,\u2212 \u221a 2\u2212 1 ) and\nB ( 6\u2212 \u221a 2 2 , \u221a 2\u2212 1 ) . Next, we need to find the equation of the line passing through points A and B. Let the equation of the line be y = kx+ b. Substituting the coordinates of points A and B into the equation, we obtain two equations:\n\u2212 \u221a 2\u2212 1 = k \u00b7 6 +\n\u221a 2\n2 + b\n\u221a 2\u2212 1 = k \u00b7 6\u2212\n\u221a 2\n2 + b\nBy subtracting the two equations to eliminate b, we get:\n\u22122 \u221a 2 = k \u00b7 \u221a 2\nSolving for k, we find k = \u22122. Substituting this value into the first equation, we get b = 5. Therefore, the equation of the line passing through point M(3,\u22121) and bisecting the chord of the hyperbola is y = \u22122x+ 5 .\nReference shortcut solution: Let the points intersected with the chord be A(x1, y1) and B(x2, y2). Then, they satisfy the equation of the hyperbola:\nx21 4 \u2212 y21 = 1 x22 4 \u2212 y22 = 1\nSubtract the two equations:\nx21 \u2212 x22 4 \u2212 (y21 \u2212 y22) = 0\n(x1 \u2212 x2)(x1 + x2) 4 \u2212 (y1 \u2212 y2)(y1 + y2) = 0\nSince M(3,\u22121) is the midpoint of the chord, x1 + x2 = 6, y1 + y2 = \u22122. Substitute this into the above equation:\n6(x1 \u2212 x2) 4 + 2(y1 \u2212 y2) = 0\nThen, we have the slope of the chord:"
        }
    ],
    "title": "CONIC10K: A Challenging Math Problem Understanding and Reasoning Dataset",
    "year": 2023
}