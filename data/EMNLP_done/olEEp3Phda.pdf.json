{
    "abstractText": "Speaker identification in novel dialogues can be widely applied to various downstream tasks, such as producing multi-speaker audiobooks and converting novels into scripts. However, existing state-of-the-art methods are limited to handling explicit narrative patterns like \"Tom said, \u2019...\u2019\", unable to thoroughly understand long-range contexts and to deal with complex cases. To this end, we propose a framework named SPC, which identifies implicit speakers in novels via symbolization, prompt, and classification. First, SPC symbolizes the mentions of candidate speakers to construct a unified label set. Then, by inserting a prompt we reformulate speaker identification as a classification task to minimize the gap between the training objectives of speaker identification and the pre-training task. Two auxiliary tasks are also introduced in SPC to enhance long-range context understanding. Experimental results show that SPC outperforms previous methods by a large margin of 4.8% accuracy on the web novel collection, which reduces 47% of speaker identification errors, and also outperforms the emerging ChatGPT. In addition, SPC is more accurate in implicit speaker identification cases that require long-range context semantic understanding.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yue Chen"
        },
        {
            "affiliations": [],
            "name": "Tian-Wei He"
        },
        {
            "affiliations": [],
            "name": "Hong-Bin Zhou"
        },
        {
            "affiliations": [],
            "name": "Jia-Chen Gu"
        },
        {
            "affiliations": [],
            "name": "Heng Lu"
        },
        {
            "affiliations": [],
            "name": "Zhen-Hua Ling"
        }
    ],
    "id": "SP:8025ee89d652ad51867277a20b83aa7674490b4b",
    "references": [
        {
            "authors": [
                "Wanxiang Che",
                "Yunlong Feng",
                "Libo Qin",
                "Ting Liu."
            ],
            "title": "N-LTP: an open-source neural language technology platform for chinese",
            "venue": "EMNLP (Demos), pages 42\u201349. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Danqi Chen",
                "Christopher D. Manning."
            ],
            "title": "A fast and accurate dependency parser using neural networks",
            "venue": "EMNLP, pages 740\u2013750. ACL.",
            "year": 2014
        },
        {
            "authors": [
                "Jia-Xiang Chen",
                "Zhen-Hua Ling",
                "Li-Rong Dai"
            ],
            "title": "A chinese dataset for identifying speakers",
            "year": 2019
        },
        {
            "authors": [
                "Yue Chen",
                "Zhen-Hua Ling",
                "Qing-Feng Liu."
            ],
            "title": "A neural-network-based approach to identifying speakers in novels",
            "venue": "Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30",
            "year": 2021
        },
        {
            "authors": [
                "Junyoung Chung",
                "Caglar Gulcehre",
                "Kyunghyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "venue": "NIPS 2014 Workshop on Deep Learning, December 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Carolina Cuesta-L\u00e1zaro",
                "Animesh Prasad",
                "Trevor Wood."
            ],
            "title": "What does the sea say to the shore? A BERT based DST style approach for speaker to dialogue attribution in novels",
            "venue": "ACL (1), pages 5820\u20135829. Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Ziqing Yang."
            ],
            "title": "Pre-training with whole word masking for chinese BERT",
            "venue": "IEEE ACM Trans. Audio Speech Lang. Process., 29:3504\u20133514.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT (1), pages 4171\u2013 4186. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "David K. Elson",
                "Kathleen R. McKeown."
            ],
            "title": "Automatic attribution of quoted speech in literary narrative",
            "venue": "AAAI, pages 1013\u20131019. AAAI Press.",
            "year": 2010
        },
        {
            "authors": [
                "Kevin Glass",
                "Shaun Bangay."
            ],
            "title": "A naive saliencebased method for speaker identification in fiction books",
            "venue": "Proceedings of the 18th Annual Symposium of the Pattern Recognition Association of South Africa (PRASA\u201907), pages 1\u20136.",
            "year": 2007
        },
        {
            "authors": [
                "Hua He",
                "Denilson Barbosa",
                "Grzegorz Kondrak."
            ],
            "title": "Identification of speakers in novels",
            "venue": "ACL (1), pages 1312\u20131320. The Association for Computer Linguistics.",
            "year": 2013
        },
        {
            "authors": [
                "Yuxiang Jia",
                "Huayi Dou",
                "Shuai Cao",
                "Hongying Zan."
            ],
            "title": "Speaker identification and its application to social network construction for chinese novels",
            "venue": "Int. J. Asian Lang. Process., 30(4):2050018:1\u20132050018:18.",
            "year": 2020
        },
        {
            "authors": [
                "Yoon Kim."
            ],
            "title": "Convolutional neural networks for sentence classification",
            "venue": "EMNLP, pages 1746\u2013 1751. ACL.",
            "year": 2014
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "ICLR (Poster).",
            "year": 2015
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Grace Muzny",
                "Michael Fang",
                "Angel X. Chang",
                "Dan Jurafsky."
            ],
            "title": "A two-stage sieve approach for quote attribution",
            "venue": "EACL (1), pages 460\u2013470. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Welinder",
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Junjie Pan",
                "Lin Wu",
                "Xiang Yin",
                "Pengfei Wu",
                "Chenchang Xu",
                "Zejun Ma."
            ],
            "title": "A chapter-wise understanding system for text-to-speech in chinese novels",
            "venue": "ICASSP, pages 6069\u20136073. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Ajay Patel",
                "Bryan Li",
                "Mohammad Sadegh Rasooli",
                "Noah Constant",
                "Colin Raffel",
                "Chris CallisonBurch."
            ],
            "title": "Bidirectional language models are also few-shot learners",
            "venue": "ICLR. OpenReview.net.",
            "year": 2023
        },
        {
            "authors": [
                "Yisi Sang",
                "Xiangyang Mou",
                "Mo Yu",
                "Dakuo Wang",
                "Jing Li",
                "Jeffrey M. Stanton."
            ],
            "title": "MBTI personality prediction for fictional characters using movie scripts",
            "venue": "EMNLP (Findings), pages 6715\u2013 6724. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "venue": "CoRR, abs/1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
            "venue": "EACL, pages 255\u2013269. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Matteo Soo",
                "Ya-Chi Yang",
                "Von-Wun Soo."
            ],
            "title": "Automatic conversion of a chinese fairy story into a script - A preliminary report and proposal",
            "venue": "TAAI, pages 1\u20136. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Dian Yu",
                "Ben Zhou",
                "Dong Yu."
            ],
            "title": "End-toend chinese speaker identification",
            "venue": "NAACL-HLT, pages 2274\u20132285. Association for Computational Linguistics.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Speaker identification in novel dialogues, also known as dialogue attribution (Muzny et al., 2017; Cuesta-L\u00e1zaro et al., 2022), aims at identifying the speaking characters of utterances in fiction texts (Glass and Bangay, 2007; Elson and McKeown, 2010). It is an important task for various downstream applications like automatically assigning appropriate voices to utterances in audiobook production (Pan et al., 2021) and novel-to-script conversion (Soo et al., 2019). As dialogues serve as\n\u2217Work done during the internship at Ximalaya. \u2020Corresponding author.\nthe major interaction between characters in novels, automatic identification of speakers can also be useful for novel-based knowledge mining tasks such as social network extraction (Jia et al., 2020) and personality profiling of the characters (Sang et al., 2022).\nTable 1 shows an example randomly sampled from the Chinese novel World of Plainness. For utterances U1, U2, U3, and U5, their speakers can be easily determined by recognizing the explicit narrative patterns like \"Said Wu Zhongping\" in the previous or following sentence. U4 is an exception that does not fall into this explicit pattern. To correctly predict the speaker of U4 requires an understanding of the conversation flow. Although many speaker identification cases can be solved by recognizing narrative patterns, many complex examples still call for a deep understanding of the surrounding context. We refer to these complex examples as implicit speaker identification cases. They pose difficulties for existing speaker identification algorithms.\nMost recent approaches for speaker identification (Chen et al., 2021; Cuesta-L\u00e1zaro et al., 2022; Yu et al., 2022) are based on pre-trained language models (PLMs) like BERT (Devlin et al., 2019). PLM-based methods enjoy the advantage of the PLM\u2019s internal linguistic and commonsense knowledge obtained from pre-training. However, two main downsides should be highlighted for these methods. On the one hand, some methods truncate a context into shorter textual segments before feeding them to PLMs (Chen et al., 2021; Cuesta-L\u00e1zaro et al., 2022). Intuitively, this approach inevitably introduces a bias to focus on short and local texts, and to identify speakers by recognizing explicit narrative patterns usually in the local context of the utterance. It may fail in implicit speaker identification cases when such patterns are not available and long-range semantic understanding is indispensable. On the other hand, some methods adopt an end-to-end setting (Yu et al., 2022) that sometimes extracts uninformative speakers like personal pronouns. Besides, they only perform mention-level speaker identification in which two extracted aliases of the same character won\u2019t be taken as the same speaker. In recent months, large language models (LLMs) have become the most exciting progress in the NLP community. Although LLMs show impressive zero-shot/few-shot capabilities in many benchmarks (Patel et al., 2023; Ouyang et al., 2022; Chung et al., 2022), how well they work for speaker identification remains unknown.\nDrawing inspiration from the successful application of prompt learning and pattern-exploiting training in various tasks like sentiment analysis (Patel et al., 2023; Schick and Sch\u00fctze, 2021), we propose a framework to identify speakers in novels via symbolization, prompt, and classification (SPC). SPC first symbolizes the mentions of candidate speakers to construct a unified label set for speaker classification. Then it inserts a prompt to introduce a placeholder for generating a feature for the speaker classifier. This approach minimizes the gap between the training objectives of speaker identification and the pre-training task of masked language modeling, and helps leverage the internal knowledge of PLMs. In previous studies, the interutterance correlation in conversations was shown to be useful for speaker identification in sequential utterances (He et al., 2013; Muzny et al., 2017; Chen et al., 2021). SPC also introduces auxil-\niary character classification tasks to incorporate supervision signals from speaker identification of neighborhood utterances. In this way, SPC can learn to capture the implicit speaker indication in a long-range context.\nTo measure the effectiveness of the proposed method and to test its speaker identification ability, SPC is evaluated on four benchmarks for speaker identification in novels, specifically, the web novel collection, World of Plainness, Jin-Yong novels, and CSI dataset. Compared to the previous studies that conduct experiments on merely 1-18 labeled books (Yu et al., 2022), the web novel collection dataset contains 230 labeled web novels. This dataset enables us to fully supervise the neural models and to analyze their performance at different training data scales. Experimental results show that the proposed method outperforms the best-performing baseline model by a large margin of 4.8% accuracy on the web novel collection. Besides, this paper presents a comparison with the most popular ChatGPT1, and results indicate that SPC outperforms it in two benchmarks. To facilitate others to reproduce our results, we have released our source code 2.\nTo sum up, our contributions in this paper are three-fold: (1) We propose SPC, a novel framework for implicit speaker identification in novels via symbolization, prompt, and classification. (2) The proposed method outperforms existing methods on four benchmarks for speaker identification in novels, and shows superior cross-domain performance after being trained on sufficient labeled data. (3) We evaluate ChatGPT on two benchmarks and present a comparison with the proposed method."
        },
        {
            "heading": "2 Related Work",
            "text": "In recent years, deep neural networks have shown great superiority in all kinds of NLP tasks (Kim, 2014; Chen and Manning, 2014), including textbased speaker identification. Candidate Scoring Network (CSN) (Chen et al., 2021) is the first deep learning approach developed for speaker identification and outperforms the manual featurebased methods by a significant margin. For each candidate speaker of an utterance, CSN first encodes the shortest text fragment which covers both the utterance and a mention of the\n1https://chat.openai.com/ 2https://github.com/YueChenkkk/SPC-Novel-Speaker-\nIdentification\ncandidate speaker with a BERT. Then the features for the speaker classifier are extracted from the output of BERT. In this way, the model learns to identify speakers by recognizing superficial narrative patterns instead of understanding the context. Cuesta-L\u00e1zaro et al. (2022) migrates a dialogue state tracking style architecture to speaker identification. It encodes each sentence separately with a Distill-BERT (Sanh et al., 2019) before modeling cross-sentence interaction with a Gated Recurrent Unit (Chung et al., 2014). Then a matching score is calculated between each character and each utterance. However, this method still just utilizes the PLM to model local texts and results in poor performance. Yu et al. (2022) adopts an end-to-end setting that directly locates the span of the speaker in the context. It feeds the concatenation of the utterance and its context to a RoBERTa (Liu et al., 2019; Cui et al., 2021), after which the start and end tokens are predicted on the output hidden states. Yet under end-to-end setting, only mention-level speaker identification is performed, in which two extracted aliases of the same character are taken as two different speakers.\nPrevious studies have shown the great advantage of applying deep-learning methods to speaker identification in novel dialogues, but these methods either are limited to recognizing superficial patterns or only identify speakers at mention level. Our study proposes a method to identify speakers based on context understanding and improves the performance on implicit speaker identification problems when long-range semantic information is needed. The proposed method outperforms other competitors given sufficient labeled data, and is more efficient in data utilization."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Task Definition",
            "text": "Before we dive into the details of our proposed approaches, it would be necessary to declare the basic settings for the task of novel-based speaker identification. The sentences in the novel have been split and each sentence is identified as either an utterance or a narrative. The name and aliases of the speaking characters in the novel have been collected in advance. The occurrences of the speaking characters in the novel are referred to as mentions. For the target utterance whose speaker we intend to identify, a selected context that covers the target utterance is extracted and denoted as\nctx = {x\u2212N1 , ..., x\u22121, u, x1, ..., xN2}. u denotes the target utterance and x\u2212N1 , ..., x\u22121, x1, ..., xN2 denote the N1 + N2 sentences surrounding u. The speaker of the target utterance is assumed to be mentioned in the selected context, while the exceptions (should be rare) are discarded from the dataset. 3 Assume that m candidate speakers are located in ctx by matching the text in ctx to the speakers\u2019 names."
        },
        {
            "heading": "3.2 Framework Introduction",
            "text": "Figure 1 shows the framework of the proposed SPC. SPC takes the selected context as input and generates the likelihood of character classification as its output. First, the mentioned characters in the input context are replaced with symbols. Then, a prompt with a placeholder (the [MASK]) is inserted to the right of the target utterance. After that, placeholders of auxiliary tasks are introduced into the context. At last, the PLM encodes the processed context and classifies the characters at each placeholder."
        },
        {
            "heading": "3.3 Character Symbolization",
            "text": "Character symbolization unifies the candidate speaker sets in different contexts, after which speaker identification can be formulated as a classification task. We assign a unique symbol Cj(j = 1, ...,m) to each candidate character mentioned in ctx, and replace the mentions of each character with its corresponding symbol in ctx. Note that this mapping from characters to symbols is only used for the specific selected context rather than for the whole novel, so as to reduce the number of classification labels. The character symbols form a local candidate set CS = {C1, ..., Cm}. Let M be the maximum number of candidate characters. C1, ..., CM have been added to the special token vocabulary of the PLM in advance. The embeddings of these special tokens are randomly initialized and will be jointly optimized with other model parameters."
        },
        {
            "heading": "3.4 Prompt Insertion",
            "text": "Next, we insert into the selected context a prompt p=\"[prefix] ___ [postfix]\", right after the target utterance. In the inserted prompt, \"___\" is the placeholder we aim to classify, while [prefix] and [postfix] are manually crafted strings on both sides of the placeholder. In practice, we choose\n3In practice, we use a 21-sentence window. 97.2% of utterances in the web novel collection are included.\n\"\uff08\" and \"\u8bf4\u4e86\u8fd9\u53e5\u8bdd\uff09\" for [prefix] and [postfix] respectively, which combine to mean \"(___ said these words)\" in English. The [MASK] token in the PLM\u2019s vocabulary is used for the placeholder."
        },
        {
            "heading": "3.5 Speaker Classification based on a PLM",
            "text": "Then we feed the processed context to a PLM which has been pre-trained on masked language modeling (MLM) to classify the missing character at the placeholder. Specifically, we utilize the pre-trained MLM head of the language model to produce a scalar score sj(j = 1, ...,m) for each candidate character Cj :\nh \u2032 = LayerNorm(GELU(W Th + bT )), (1)\ns =WDh \u2032 + bD, (2)\nh \u2208 Rd is the output hidden state of the PLM corresponding to the placeholder, where d is the hidden size of the PLM. W T \u2208 Rd\u00d7d and bT \u2208 Rd are the weight and bias of the linear transform layer. WD \u2208 RM\u00d7d and bD \u2208 RM are the weight and\nbias of the decoder layer. W T and bT are pretrained along with the PLM, while WD and bD are randomly initialized. s = [s1, .., sM ] is the output score vector in which the first m scores correspond to the m candidate in CS.\nAt the training stage, as our goal is to assign the correct speaker a higher score than other candidates, we choose margin ranking loss to instruct the optimization. For example, assume the correct speaker of the target utterance is Cj , then Cj is paired with each candidate in CS\\Cj . The speaker identification loss of u is calculated on the scores of each candidate pair, as:\nL(u, ctx) (3)\n= 1\nm\u2212 1 m\u2211 k=1,k \u0338=j max{sk \u2212 sj +mgn, 0},\nwhere mgn is the ideal margin between the scores of the two candidates. At the inference stage, the candidate assigned the highest score is identified as the speaker."
        },
        {
            "heading": "3.6 Auxiliary Character Classification",
            "text": "Based on the classification task form, we designed two auxiliary character classification tasks: 1) neighborhood utterances speaker classification (NUSC) to utilize the speaker alternation pattern between adjacent utterances, and 2) masked mention classification (MMC) to alleviate the excessive reliance on neighborhood explicit narrative patterns. NUSC teaches the model to classify the speakers of the neighborhood utterances of the target utterance. In this way, the model learns to utilize the dependency between the speakers of neighborhood utterances. MMC randomly masks the character mentions in the neighborhood sentences of the target utterance and quizzes the model on classifying the masked characters. It corrupts the explicit narrative patterns like \"Tom said\" which usually exists in the neighborhood sentence of the utterance and guides the model to utilize long-range semantic information.\nWe relabel the target utterance as ui and its previous and following utterance as ui\u22121 and ui+1. To perform NUSC, the prompt p is also inserted after ui\u22121 and ui+1, as long as these two utterances and their speakers are covered in ctx. Then, the model classifies the speakers for ui\u22121 and ui+1 as described in Section 3.5. The loss introduced by NUSC is the average speaker classification loss of the two neighborhood utterances:\nLNUSC = 1\n2 L(ui\u22121, ctx) +\n1 2 L(ui+1, ctx). (4)\nFor MMC, we randomly mask the character mentions by a probability in the previous and following sentences of the target utterance (i.e. x\u22121 and x1), provided the masked characters are mentioned somewhere other than these two sentences. The model is required to predict the masked characters. Let the indexes of the masked characters be MC, then we have the average classification loss of each masked mention as:\nLMMC = 1 |MC| \u2211\nj\u2208MC Lmgn(j), (5)\nin which Lmgn is the same margin ranking loss function as is shown in Eq.(3).\nAfter incorporating NUSC and MMC into the training objective, the loss function turns into:\nLACC = L(ui, ctx)+\u03b1LNUSC + \u03b2LMMC , (6)\nwhere \u03b1 and \u03b2 control the strength of supervision signals from NUSC and MMC respectively. At\nthe inference stage, we do not mask the character mentions in x1 and x\u22121 to retain the explicit narrative patterns. The prompt is inserted after ui\u22121 and ui+1 at the inference stage to keep traininginference consistency."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We conducted experiments on four Chinese speaker identification datasets which are the web novel collection (WN), World of Plainness (WP) (Chen et al., 2019) 4, Jin-Yong novels (JY) (Jia et al., 2020) 5, and end-to-end Chinese Speaker Identification dataset (CSI) (Yu et al., 2022) 6 respectively. WN is a large internal speaker identification dataset. Its annotation details can be referred to in Appendix D. WN and CSI both consist of web novels of various genres and writing styles, while WP and JY are printed literature with more normative writings. WP and JY can serve as cross-domain evaluation datasets for WN. As no test data was provided in CSI, we use the development set of CSI as the test data and randomly sampled 10% of instances from the training set for validation. The number of novels and utterances in each subset of each dataset is shown in Table 2. There are no overlapped novels in the different subsets of WN.\nFor clarity, each subset of a dataset is named as \"dataset-subset\", e.g., \"WN-train\" for the training set of WN. To evaluate the impact of a smaller training set, we also sampled 5 novels from WNtrain to make a smaller training set with about 31k utterances. To distinguish between the whole training set and the sampled one, we referred to them as WN-large and WN-small respectively."
        },
        {
            "heading": "4.2 Baselines",
            "text": "We compared SPC to the following baselines. CSN (Chen et al., 2021) feeds a candidate-specific textual segment to the PLM and then outputs the likelihood of the corresponding candidate. A revision algorithm based on speaker alternation pattern is adopted to revise the speaker identification results in two-party dialogues. DST_SI (Cuesta-L\u00e1zaro et al., 2022) encodes each sentence separately with the PLM before\n4https://github.com/YueChenkkk/Chinese-DatasetSpeaker-Identification\n5https://github.com/huayi-dou/The-speakeridentification-corpus-of-Jin-Yong-novels\n6https://github.com/yudiandoris/csi\nmodeling cross-sentence interaction with a GRU. The embeddings of the character mention and the utterance are dotted to obtain the likelihood. A CRF is employed to model the dependency between speakers of neighborhood utterances. Due to limited GPU memory, we only use a base model for DST_SI. E2E_SI (Yu et al., 2022) feeds the concatenation of the utterance and its context to the PLM, after which the start and end tokens are predicted on the output hidden states. GPT-3.5-turbo was based on GPT-3 (Patel et al., 2023) and aligned to human preference by reinforcement learning with human feedback (Ouyang et al., 2022). We prompted the model with the format \"{context}#{utterance}#The speaker of this utterance is:\". We used a tolerant metric that the response was considered correct if the true speaker\u2019s name is a sub-string of the response.7 We only evaluated this baseline on WP and JY due to the expensive API costs."
        },
        {
            "heading": "4.3 Implementation Details",
            "text": "As context selection plays a critical part in speaker identification, we detail the context selection procedures for the methods we implemented. For SPC, we selected a 21-sentence context window surrounding the target utterance, which corresponds to N1 = N2 = 10 in Section 3.1. If the context window exceeds the PLM\u2019s length limit (512 tokens for RoBERTa), we would truncate the context window to fit the input length requirement. Since DST_SI is not open source, we implemented it ourselves. We followed their paper and segment conversations by restricting the number of intervening narratives\n7We also tried prompting with multiple choices of candidate names but the performance degraded drastically.\nbetween utterances to 1. We further included the previous and following 10 sentences of each conversation, and limited the maximum number of involved sentences to 30. More details can be referred to Appendix A."
        },
        {
            "heading": "4.4 Overall Results",
            "text": "We tuned and evaluated the models on the same dataset (in-domain), or tuned the models on WN and evaluated them on WP and JY (cross-domain). Note that although we compared zero-shot GPT3.5-turbo to other cross-domain results, it hadn\u2019t been tuned on any data. The released CSI dataset masked 10% of tokens due to copyright issues, so we collected E2E_SI\u2019s performance on the masked CSI from the GitHub page of CSI. Validation/Test accuracies are shown in Table 3. We will mainly discuss the test results and leave the validation results for reference.\nFirst, we can conclude from the table that RoBERTa-large performed better than RoBERTabase and BERT-base for the same method. Regardless of the specific PLM, the comparative relationship between different methods remains constant. So we mainly focus on the performance of different methods based on RoBERTa-large. SPC based on RoBERTa-large consistently performed better than or comparable to all non-LLM baselines in both in-domain evaluation and cross-domain evaluation. In the in-domain evaluation of WN, SPC outperformed the best opponent CSN by 4.8% and 3.9% trained on WN-large and WN-small, achieving overall accuracies of 94.6% and 90.0%. These are remarkable improvements as the errors are reduced by 47% and 28%. As WN-test includes 70 web novels of various genres, we believe it reflects general performance on web novels. In the in-domain evaluation on WP which is the only dataset evaluated in CSN paper (Chen et al., 2021), SPC still outperformed CSN by 1.0%. We observed MMC might cause serious overfitting for very small datasets like WP-train, so we didn\u2019t adopt MMC for WP-train. In cross-domain evaluations, SPC also consistently outperformed all non-LLM baselines, which shows its better generalizability to novels of unseen domains.\nAlthough GPT-3.5-turbo underperformed WNlarge tuned SPC, its zero-shot performance is still remarkable. In comparison, GPT-3.5-turbo has a much large number of parameters and benefits from its vast and general pre-training corpus, while\nSPC excelled by effectively tuning on an adequate domain-specific corpus. It\u2019s worth mentioning that the response of GPT-3.5-turbo may contain more than one name, e.g., \"Jin-bo\u2019s sister, Xiu\" and \"Runye\u2019s husband (Xiang-qian)\". These responses may fool our evaluation criterion, as the response is only required to cover the true speaker."
        },
        {
            "heading": "4.5 Ablation study",
            "text": "We conducted ablation studies based on SPC to investigate the effect of each module, with results shown in Table 4.\nWe first removed ACC from SPC. As can be seen in the table, removing ACC dropped the evaluation accuracies by 0.8% and 0.6% on WN, by 0.2% on JY, and by 3.7% on WP. This indicates that the auxiliary character classification tasks are beneficial for improving speaker identification performance. 8 Only the NUSC task was applied to training on WP-train, and it contributed a lot to the performance on WP-test. We think it\u2019s because the writing of WP is more normative than the writing of novels in WN. The sequential utterances in WP\n8We also conducted pilot experiments to use more neighborhood utterances for NUSC but gained no improvement. See details in Appendix B.2.\nusually obey the speaker alternation pattern, which can be easily learned and utilized.\nWe further ablated prompting. To this end, we did not insert the prompt but extracted the CSNstyle features from the output of PLM to produce the likelihood of each candidate speaker. After ablating the prompt-based architecture, the performance of models trained on WN-large decreased by 0.6%, whereas those on WN-small and WP-train decreased drastically by 4.8% and 11.9%. It shows that prompting is helpful for boosting performance in a low-resource setting and verifies our starting point for developing this approach. Prompting closes the gap between the training objectives of speaker identification and the pre-training task, which can help the PLM understand the task and leverage its internal knowledge. JY is the exception in which performance did not degrade after this ablation, although its training set only contains 15k samples. We believe this is because JY is too easy and lacks challenging cases to discriminate between different ablations.\nTo gain insight into how the performance of SPC and its ablated methods varies with the scale of training data, we trained them on varying numbers of novels sampled from WN-large and evaluated their performance on WN-test. To facilitate comparison, we performed a similar evaluation on CSN. As is shown in Figure 2, every method achieved better accuracy with more training utterances. SPC consistently outperformed the other methods on all evaluated training data scales. It\u2019s observed that ACC brought almost constant improvements as the training data grew. While the promptbased architecture was more effective in low-\ndata scenarios and the improvement became less significant as training utterances increased.\nThe deeper ablated SPC (SPC w/o. ACC & prompt) initially under-performed CSN, but it overtook CSN when the training data reached about 150k utterances. In comparison, CSN didn\u2019t benefit much from the increment of training data. As a possible explanation, the deeper ablated method using a longer context window (445.5 tokens avg.) gradually learned to handle implicit speaker identification cases that require long-range context understanding, after being trained on more data. However, CSN using a short context window (131.3 tokens avg.) still failed in these cases. It\u2019s also revealed that generally more data is needed to train models that take a longer context window as input. However, with prompting and ACC, SPC overcomes this difficulty and learns to identify speakers in a long context window with a much smaller data requirement. As is shown in Figure 2, SPC took merely 31k utterances to reach the accuracy of 90%, while about 180k utterances were needed by its deeper ablated method to gain the same performance."
        },
        {
            "heading": "4.6 Implicit Speaker Identification Analysis",
            "text": "In this section, we\u2019re going to find out whether our proposed method shows a stronger ability to identify implicit speakers than other methods using a shorter context window, like CSN.\nIntuitively, if the speaker\u2019s mention is close to the utterance, then the speaker of this utterance can probably be identified by recognizing explicit narra-\ntive patterns like \"Tom said\". On the contrary, utterances with distant speaker mentions are typically implicit speaker identification cases and require context understanding. We calculated the speaker identification accuracies of the utterances in WNtest, categorizing them based on the proximity of each utterance to the closest mention of its speaker. The comparison between SPC and CSN is shown in Figure 3. The term \"sentence distance\" refers to the absolute value of the utterance\u2019s sentence index minus the index of the sentence where the speaker\u2019s closest mention is found.\nIt can be seen from the figure that, as the sentence distance increased, both SPC and CSN identified speakers more inaccurately. Initially, at sentence distance = 1, both models performed comparably and achieved accuracies above 95%. However, when sentence distance came to 2, the identification accuracy of CSN drastically dropped to 74%. Utterances with sentence distance > 1 can be regarded as implicit speaker identification cases, so CSN is not good at identifying the speakers of these utterances. While SPC still maintained over 80% until sentence distance reached 5 and consistently outperformed CSN by 8.4% to 13.4% for sentence distance greater than 1. Thus it\u2019s verified that SPC has a better understanding of the context compared to CSN, and thus can better deal with implicit speakers. The proportion of test utterances at each sentence distance is also shown in Figure 3. 81.1% of the utterances are situated at sentence distance = 1, elucidating the reason behind CSN\u2019s commendable overall performance\ndespite its incapacity to handle implicit speakers. We also conducted a case study on the implicit speaker identification cases in WN-test, with a few translated examples provided in Appendix C. SPC did perform better than CSN in many challenging speaker identification cases."
        },
        {
            "heading": "5 Conclusions",
            "text": "In this paper, we propose SPC, an effective framework for implicit speaker identification in novels via symbolization, prompt, and classification. Experimental results show SPC\u2019s superiority on four speaker identification benchmarks and its remarkable cross-domain capability. Furthermore, SPC significantly outperforms the strongest competitor CSN in implicit speaker identification cases that require deeper context understanding. We also evaluate ChatGPT on two speaker identification benchmarks and present a comparison with SPC. In the future, we hope to harness LLMs with longer input length limits to further improve speaker identification performance.\nLimitations\nAlthough SPC has proved its effectiveness in novel-based speaker identification, we consider two aspects that can be further improved. First, we only implemented SPC on small base models containing less than 1 billion parameters. In light of the swift progress in LLMs, investigating the full potential of these advanced LLMs holds significant value and promise for future advancements. Second, in real-world applications, our approach operates on the output of a character name extraction module which might produce incorrect results. Thus, it\u2019s worth studying how to improve the robustness of the speaker identification approach and make it less vulnerable to errors in the character names."
        },
        {
            "heading": "B Experiment Settings Discussion",
            "text": "B.1 Using Different Prompts We tried different prompt templates to see how they affect speaker identification performance. As can be seen from Table 7, different prompt templates don\u2019t affect the performance much, but using a empty prompt would hurt the performance. This indicates SPC\u2019s insensitivity to prompt selection.\nB.2 Using N-neighborhood Utterances We conducted pilot experiments on using different numbers of neighborhood utterances for neighborhood utterance speaker classification (NUSC). We trained the models on WN-small, showing their validation performance in Figure 4. It\u2019s clear that using 1-neighborhood utterances for NUSC (our setting for SPC) brings some improvements, compared to 0-neighborhood (not applying NUSC). However, extending to more neighborhood utterances does not bring further improvements. A possible explanation is that the dependency between neighborhood utterances mostly lies in adjacent utterances, instead of distant ones."
        },
        {
            "heading": "C Case Study",
            "text": "Figure 5 shows three translated examples from WNtest, with speaker identification results of different\nmethods listed at the bottom of each example. They are all implicit speaker identification examples without explicit narrative patterns. We observed SPC did much better than CSN in solving these challenging speaker identification cases that require deeper context understanding. Example 3 shows that the auxiliary character classification tasks (ACC) help the model to better capture the interutterance dependency."
        },
        {
            "heading": "D Web Novel Collection Details",
            "text": "The web novel collection was annotated on 230 web novels by a group of Chinese native annotators. The utterances in the novels were first identified based on quotes. Then, the annotators were instructed to mark the name of the speaker in the neighborhood context of each utterance, and the names of other characters were also marked if found.\nCopyrights of the web novels belong to their respective proprietors. The authors are allowed to use the data for research purposes and should follow the principle of fair use. The data annotators are a group of employed professional data annotators each at least with a bachelor\u2019s degree. Their wages are in line with local regulations. Before the novels were handed to the annotators, they had been reviewed by a group of content reviewers to filter any offensive information, including violence, terror, abuse, etc. The annotation instruction informed the annotators of the potential use of the annotated data for speaker identification research."
        }
    ],
    "title": "Symbolization, Prompt, and Classification: A Framework for Implicit Speaker Identification in Novels",
    "year": 2023
}