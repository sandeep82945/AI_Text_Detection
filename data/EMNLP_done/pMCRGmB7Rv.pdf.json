{
    "abstractText": "The ability to automatically generate accurate protocols for scientific experiments would represent a major step towards the automation of science. Large Language Models (LLMs) have impressive capabilities on a wide range of tasks, such as question answering and the generation of coherent text and code. However, LLMs can struggle with multi-step problems and longterm planning, which are crucial for designing scientific experiments. Moreover, evaluation of the accuracy of scientific protocols is challenging, because experiments can be described correctly in many different ways, require expert knowledge to evaluate, and cannot usually be executed automatically. Here we present an automatic evaluation framework for the task of planning experimental protocols, and we introduce BIOPROT1: a dataset of biology protocols with corresponding pseudocode representations. To measure performance on generating scientific protocols, we use an LLM to convert a natural language protocol into pseudocode, and then evaluate an LLM\u2019s ability to reconstruct the pseudocode from a high-level description and a list of admissible pseudocode functions. We evaluate GPT-3 and GPT-4 on this task and explore their robustness. We externally validate the utility of pseudocode representations of text by generating accurate novel protocols using retrieved pseudocode, and we run a generated protocol successfully in our biological laboratory. Our framework is extensible to the evaluation and improvement of language model planning abilities in other areas of science or other areas that lack automatic evaluation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Odhran O\u2019Donoghue"
        },
        {
            "affiliations": [],
            "name": "Aleksandar Shtedritski"
        },
        {
            "affiliations": [],
            "name": "John Ginger"
        },
        {
            "affiliations": [],
            "name": "Ralph Abboud"
        },
        {
            "affiliations": [],
            "name": "Ali Essa Ghareeb"
        },
        {
            "affiliations": [],
            "name": "Samuel G Rodriques"
        }
    ],
    "id": "SP:dfce031535fbec4753ab4a025cbfa9d65b8f1a0e",
    "references": [
        {
            "authors": [
                "Michael Ahn",
                "Anthony Brohan",
                "Noah Brown",
                "Yevgen Chebotar",
                "Omar Cortes",
                "Byron David",
                "Chelsea Finn",
                "Keerthana Gopalakrishnan",
                "Karol Hausman",
                "Alex Herzog"
            ],
            "title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "year": 2022
        },
        {
            "authors": [
                "Fan Bai",
                "Alan Ritter",
                "Peter Madrid",
                "Dayne Freitag",
                "John Niekrasz."
            ],
            "title": "SynKB: Semantic search for synthetic procedures",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages",
            "year": 2022
        },
        {
            "authors": [
                "Iz Beltagy",
                "Kyle Lo",
                "Arman Cohan."
            ],
            "title": "SciBERT: A pretrained language model for scientific text",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Manik Bhandari",
                "Pranav Narayan Gour",
                "Atabak Ashfaq",
                "Pengfei Liu",
                "Graham Neubig."
            ],
            "title": "Reevaluating evaluation in text summarization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Daniil A Boiko",
                "Robert MacKnight",
                "Gabe Gomes."
            ],
            "title": "Emergent autonomous scientific research capabilities of large language models",
            "venue": "arXiv preprint arXiv:2304.05332.",
            "year": 2023
        },
        {
            "authors": [
                "Andres M Bran",
                "Sam Cox",
                "Andrew D White",
                "Philippe Schwaller."
            ],
            "title": "Chemcrow: Augmenting large-language models with chemistry tools",
            "venue": "arXiv preprint arXiv:2304.05376.",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Debadutta Dash",
                "Rahul Thapa",
                "Juan M Banda",
                "Akshay Swaminathan",
                "Morgan Cheatham",
                "Mehr Kashyap",
                "Nikesh Kotecha",
                "Jonathan H Chen",
                "Saurabh Gombar",
                "Lance Downing"
            ],
            "title": "Evaluation of gpt-3.5 and gpt-4 for supporting",
            "year": 2023
        },
        {
            "authors": [
                "Danny Driess",
                "Fei Xia",
                "Mehdi SM Sajjadi",
                "Corey Lynch",
                "Aakanksha Chowdhery",
                "Brian Ichter",
                "Ayzaan Wahid",
                "Jonathan Tompson",
                "Quan Vuong",
                "Tianhe Yu"
            ],
            "title": "Palm-e: An embodied multimodal language model",
            "venue": "arXiv preprint arXiv:2303.03378",
            "year": 2023
        },
        {
            "authors": [
                "Yu Gu",
                "Robert Tinn",
                "Hao Cheng",
                "Michael Lucas",
                "Naoto Usuyama",
                "Xiaodong Liu",
                "Tristan Naumann",
                "Jianfeng Gao",
                "Hoifung Poon."
            ],
            "title": "Domain-specific language model pretraining for biomedical natural language processing",
            "venue": "ACM Transactions on Computing",
            "year": 2021
        },
        {
            "authors": [
                "Taicheng Guo",
                "Kehan Guo",
                "Bozhao Nan",
                "Zhenwen Liang",
                "Zhichun Guo",
                "Nitesh V. Chawla",
                "Olaf Wiest",
                "Xiangliang Zhang"
            ],
            "title": "What indeed can gpt models do in chemistry",
            "year": 2023
        },
        {
            "authors": [
                "Stefan Hegselmann",
                "Alejandro Buendia",
                "Hunter Lang",
                "Monica Agrawal",
                "Xiaoyi Jiang",
                "David Sontag."
            ],
            "title": "Tabllm: Few-shot classification of tabular data with large language models",
            "venue": "International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andy Zou",
                "Mantas Mazeika",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "arXiv preprint arXiv:2009.03300.",
            "year": 2020
        },
        {
            "authors": [
                "Wenlong Huang",
                "Pieter Abbeel",
                "Deepak Pathak",
                "Igor Mordatch."
            ],
            "title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "venue": "International Conference on Machine Learning, pages 9118\u20139147. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Sergio Jim\u00e9nez",
                "Javier Segovia-Aguas",
                "Anders Jonsson."
            ],
            "title": "A review of generalized planning",
            "venue": "The Knowledge Engineering Review, 34:e5.",
            "year": 2019
        },
        {
            "authors": [
                "Di Jin",
                "Eileen Pan",
                "Nassim Oufattole",
                "Wei-Hung Weng",
                "Hanyi Fang",
                "Peter Szolovits."
            ],
            "title": "What disease does this patient have? a large-scale open domain question answering dataset from medical exams",
            "venue": "Applied Sciences, 11(14):6421.",
            "year": 2021
        },
        {
            "authors": [
                "Qiao Jin",
                "Bhuwan Dhingra",
                "Zhengping Liu",
                "William Cohen",
                "Xinghua Lu."
            ],
            "title": "PubMedQA: A dataset for biomedical research question answering",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang."
            ],
            "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
            "venue": "Bioinformatics, 36(4):1234\u20131240.",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Lewis",
                "Myle Ott",
                "Jingfei Du",
                "Veselin Stoyanov."
            ],
            "title": "Pretrained language models for biomedical and clinical tasks: Understanding and extending the state-of-the-art",
            "venue": "Proceedings of the 3rd Clinical Natural Language Processing Workshop, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Zhan Ling",
                "Yunhao Fang",
                "Xuanlin Li",
                "Zhiao Huang",
                "Mingu Lee",
                "Roland Memisevic",
                "Hao Su."
            ],
            "title": "Deductive verification of chain-of-thought reasoning",
            "venue": "arXiv preprint arXiv:2306.03872.",
            "year": 2023
        },
        {
            "authors": [
                "Bo Liu",
                "Yuqian Jiang",
                "Xiaohan Zhang",
                "Qiang Liu",
                "Shiqi Zhang",
                "Joydeep Biswas",
                "Peter Stone."
            ],
            "title": "Llm+ p: Empowering large language models with optimal planning proficiency",
            "venue": "arXiv preprint arXiv:2304.11477.",
            "year": 2023
        },
        {
            "authors": [
                "Hanmeng Liu",
                "Ruoxi Ning",
                "Zhiyang Teng",
                "Jian Liu",
                "Qiji Zhou",
                "Yue Zhang"
            ],
            "title": "Evaluating the logical reasoning ability of chatgpt and gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "Shengchao Liu",
                "Jiongxiao Wang",
                "Yijin Yang",
                "Chengpeng Wang",
                "Ling Liu",
                "Hongyu Guo",
                "Chaowei Xiao."
            ],
            "title": "Chatgpt-powered conversational drug editing using retrieval and domain feedback",
            "venue": "arXiv preprint arXiv:2305.18090.",
            "year": 2023
        },
        {
            "authors": [
                "Renqian Luo",
                "Liai Sun",
                "Yingce Xia",
                "Tao Qin",
                "Sheng Zhang",
                "Hoifung Poon",
                "Tie-Yan Liu."
            ],
            "title": "Biogpt: generative pre-trained transformer for biomedical text generation and mining",
            "venue": "Briefings in Bioinformatics, 23(6).",
            "year": 2022
        },
        {
            "authors": [
                "Aman Madaan",
                "Niket Tandon",
                "Prakhar Gupta",
                "Skyler Hallinan",
                "Luyu Gao",
                "Sarah Wiegreffe",
                "Uri Alon",
                "Nouha Dziri",
                "Shrimai Prabhumoye",
                "Yiming Yang"
            ],
            "title": "Self-refine: Iterative refinement with self-feedback",
            "venue": "arXiv preprint arXiv:2303.17651",
            "year": 2023
        },
        {
            "authors": [
                "Yao Mu",
                "Qinglong Zhang",
                "Mengkang Hu",
                "Wenhai Wang",
                "Mingyu Ding",
                "Jun Jin",
                "Bin Wang",
                "Jifeng Dai",
                "Yu Qiao",
                "Ping Luo."
            ],
            "title": "Embodiedgpt: Visionlanguage pre-training via embodied chain of thought",
            "venue": "arXiv preprint arXiv:2305.15021.",
            "year": 2023
        },
        {
            "authors": [
                "Subhabrata Mukherjee",
                "Arindam Mitra",
                "Ganesh Jawahar",
                "Sahaj Agarwal",
                "Hamid Palangi",
                "Ahmed Awadallah."
            ],
            "title": "Orca: Progressive learning from complex explanation traces of gpt-4",
            "venue": "arXiv preprint arXiv:2306.02707.",
            "year": 2023
        },
        {
            "authors": [
                "Liangming Pan",
                "Alon Albalak",
                "Xinyi Wang",
                "William Yang Wang"
            ],
            "title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "year": 2023
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Baolin Peng",
                "Chunyuan Li",
                "Pengcheng He",
                "Michel Galley",
                "Jianfeng Gao."
            ],
            "title": "Instruction tuning with gpt-4",
            "venue": "arXiv preprint arXiv:2304.03277.",
            "year": 2023
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom."
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "venue": "arXiv preprint arXiv:2302.04761.",
            "year": 2023
        },
        {
            "authors": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Dongsheng Li",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
            "venue": "arXiv preprint arXiv:2303.17580.",
            "year": 2023
        },
        {
            "authors": [
                "Hoo-Chang Shin",
                "Yang Zhang",
                "Evelina Bakhturina",
                "Raul Puri",
                "Mostofa Patwary",
                "Mohammad Shoeybi",
                "Raghav Mani."
            ],
            "title": "Biomegatron: Larger biomedical domain language model",
            "venue": "arXiv preprint arXiv:2010.06060.",
            "year": 2020
        },
        {
            "authors": [
                "Tom Silver",
                "Soham Dan",
                "Kavitha Srinivas",
                "Joshua B Tenenbaum",
                "Leslie Pack Kaelbling",
                "Michael Katz."
            ],
            "title": "Generalized planning in pddl domains with pretrained large language models",
            "venue": "arXiv preprint arXiv:2305.11014.",
            "year": 2023
        },
        {
            "authors": [
                "Mujeen Sung",
                "Jinhyuk Lee",
                "Sean Yi",
                "Minji Jeon",
                "Sungdong Kim",
                "Jaewoo Kang"
            ],
            "title": "Can language models be biomedical knowledge bases? arXiv preprint arXiv:2109.07154",
            "year": 2021
        },
        {
            "authors": [
                "Ronen Tamari",
                "Fan Bai",
                "Alan Ritter",
                "Gabriel Stanovsky."
            ],
            "title": "Process-level representation of scientific protocols with interactive annotation",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Ross Taylor",
                "Marcin Kardas",
                "Guillem Cucurull",
                "Thomas Scialom",
                "Anthony Hartshorn",
                "Elvis Saravia",
                "Andrew Poulton",
                "Viktor Kerkez",
                "Robert Stojnic."
            ],
            "title": "Galactica: A large language model for science",
            "venue": "arXiv preprint arXiv:2211.09085.",
            "year": 2022
        },
        {
            "authors": [
                "Leonid Teytelman",
                "Alexei Stoliartchouk",
                "Lori Kindler",
                "Bonnie L Hurwitz."
            ],
            "title": "Protocols",
            "venue": "io: virtual communities for protocol development and discussion. PLoS Biology, 14(8):e1002538.",
            "year": 2016
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Guanzhi Wang",
                "Yuqi Xie",
                "Yunfan Jiang",
                "Ajay Mandlekar",
                "Chaowei Xiao",
                "Yuke Zhu",
                "Linxi Fan",
                "Anima Anandkumar."
            ],
            "title": "Voyager: An open-ended embodied agent with large language models",
            "venue": "arXiv preprint arXiv:2305.16291.",
            "year": 2023
        },
        {
            "authors": [
                "Peiyi Wang",
                "Lei Li",
                "Liang Chen",
                "Dawei Zhu",
                "Binghuai Lin",
                "Yunbo Cao",
                "Qi Liu",
                "Tianyu Liu",
                "Zhifang Sui."
            ],
            "title": "Large language models are not fair evaluators",
            "venue": "arXiv preprint arXiv:2305.17926.",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Zhenqin Wu",
                "Bharath Ramsundar",
                "Evan N. Feinberg",
                "Joseph Gomes",
                "Caleb Geniesse",
                "Aneesh S. Pappu",
                "Karl Leswing",
                "Vijay Pande"
            ],
            "title": "Moleculenet: A benchmark for molecular machine learning",
            "year": 2017
        },
        {
            "authors": [
                "Shunyu Yao",
                "Dian Yu",
                "Jeffrey Zhao",
                "Izhak Shafran",
                "Thomas L Griffiths",
                "Yuan Cao",
                "Karthik Narasimhan."
            ],
            "title": "Tree of thoughts: Deliberate problem solving with large language models",
            "venue": "arXiv preprint arXiv:2305.10601.",
            "year": 2023
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "arXiv preprint arXiv:1904.09675.",
            "year": 2019
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Hai Zhao",
                "George Karypis",
                "Alex Smola."
            ],
            "title": "Multimodal chain-of-thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2302.00923.",
            "year": 2023
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "arXiv preprint",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Traditional manual methods for research in biology are time-consuming, labour-intensive, and highly prone to human error. Robotic laboratory automation has the potential to increase accuracy, repro-\n1The dataset and code for evaluation are available at https://github.com/bioplanner/bioplanner\nducibility, and scalability, contributing to more scientific breakthroughs and a faster transition from research to real-world applications.\nOne important step towards automation of biology research is the automated generation of a laboratory protocol (Accurate step-by-step instructions on how to complete an experiment to accomplish a specific goal) which can subsequently be converted into robot code. LLMs have significant latent scientific knowledge and thus may be able to formulate accurate scientific protocols, which has been demonstrated for the field of chemistry (Bran et al., 2023; Boiko et al., 2023). However, to-date there has not been any clear way to evaluate the accuracy of a generated scientific protocol, except by manual evaluation. Without established evaluation metrics, progress in the field of automating science remains challenging.\nEvaluating laboratory protocols is difficult for\ntwo reasons. Firstly, protocols are very sensitive to tiny details, and slight variations in instructions can lead to significantly different outcomes. When comparing generated protocols against ground truths, metrics that rely on n-gram overlaps such as BLEU (Papineni et al., 2002) or contextual embeddings such as BERTScore (Zhang et al., 2019) might not capture small differences, such as the order of actions, or relation between substances (Bhandari et al., 2020). Secondly, the same protocol can be described correctly at various levels of granularity. The same technique (e.g. sequencing library preparation) can be described by a single line or multiple paragraphs. This variability in granularity makes it difficult to evaluate the accuracy of LLM-generated protocols.\nWe here present an automated approach to evaluating the ability of a language model to write biological protocols. Our approach is inspired by robotic planning, in which a closed set of admissible actions is provided to a controller agent (Jim\u00e9nez et al., 2019; Ahn et al., 2022; Huang et al., 2022). We use GPT-4 to automatically convert a written protocol into pseudocode using a protocolspecific set of pseudofunctions that is generated by the model (see Figure 1). Here, a \u201cteacher\u201d model generates the admissible action set and correct answer in terms of step-by-step pseudocode. Having access to this privileged information, we can then evaluate the performance of a \u201cstudent\u201d, that has to solve the task from scratch. In this way, we then evaluate the ability of language models to generate a protocol when presented only with the appropriate pseudocode functions and a short description of the protocol. In effect, our approach allows us to automatically convert the process of writing a scientific protocol into a series of multiple-choice questions (i.e., pick a pseudofunction from a provided set), which can be evaluated much more robustly than natural language generation. This paradigm allows us to rapidly measure the protocol knowledge of GPT-3.5 and GPT-4 with minimal human intervention, and can serve as a general approach for evaluating and improving long-horizon planning in open-ended tasks in the future.\nTo this end, we also introduce a novel dataset, BIOPROT, of publicly available biology laboratory protocols, containing instructions in both free text and protocol-specific pseudocode. The dataset has been reviewed by domain experts and allows evaluation of model performance several different tasks,\nsuch as next-step prediction, or full protocol generation. We further show the utility of this dataset by automatically designing and successfully executing a lab experiment using GPT-4 and the action space defined using BIOPROT.\nIn summary, we make the following contributions: (i) We propose evaluating protocol generation on pseudocode rather than free text instructions; (ii) We introduce the BIOPROT dataset, a manually audited dataset of open-access biology protocols; (iii) We evaluate the ability of GPT-4 to accurately convert natural language protocols into pseudocode; (iv) We define a suite of tasks and metrics for evaluating protocol generation; (v) We evaluate several LLMs on our tasks to provide objective measures of these models\u2019 ability to generate biological experiments; (vi) We automatically generate a biology experiment and successfully execute it in a lab."
        },
        {
            "heading": "2 Related Works",
            "text": "LLMs for Natural Sciences Using LLM for scientific tasks such as entity extraction in biological documents (Tamari et al., 2021) or retrieval of chemical reaction procedures (Bai et al., 2022) is a natural use case of such models. Work such as SciBERT, BioGPT, Galactica and others have also shown the utility of pretraining an LLM on a corpus of biomedical (Gu et al., 2021; Lewis et al., 2020; Luo et al., 2022; Lee et al., 2020; Shin et al., 2020) or general scientific text (Beltagy et al., 2019; Taylor et al., 2022). More recently, pre-trained generalist LLMs such as GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023) have shown to be capable of tasks such as searching for chemical compounds similar to a given one (OpenAI, 2023) or drug editing (Liu et al., 2023c). Furthermore, a GPT-4 agent augmented with tools has been shown to be capable of synthesis planning and drug discovery (Bran et al., 2023) or planning reactions and executing them on a robotic platform (Boiko et al., 2023).\nTask Decomposition LLMs trained on nexttoken prediction can struggle with more complex logical reasoning in na\u00efve setups (Liu et al., 2023b). However, decomposing complex tasks into subtasks in frameworks such as Chain-of-Thought reasoning (Wei et al., 2022; Zhang et al., 2023), and its variants such as Least-to-Most (Zhou et al., 2022) and Tree of Thought reasoning (Yao et al., 2023) improves performance in multi-step reasoning problems. In addition to test-time improve-\nments, LLMs also improve in performance when trained on step-by-step reasoning data generated by larger LLMs (Mukherjee et al., 2023; Mu et al., 2023). Task decomposition has also been combined with self-verification through deductive reasoning to improve step-by-step reasoning accuracy (Ling et al., 2023). Here, we approach task decomposition from another angle - we first ask the model to define a discrete set of actions needed to complete a task, and then how to compose them.\nPlanning Closely related to task decomposition is planning. LLMs have been successful at planning in simulated and real embodied space, both through the use of restricted action space (Ahn et al., 2022; Driess et al., 2023), function/tool search (Wang et al., 2023a; Schick et al., 2023; Shen et al., 2023; Bran et al., 2023; Boiko et al., 2023) and translation of plans into admissible action space (Huang et al., 2022). Planning models have been explicitly combined with Chain-ofThought reasoning for performance improvement (Mu et al., 2023; Shen et al., 2023). LLM planners can also learn to create their own training curriculum and refine their function use (Wang et al., 2023a). LLM-based planning and reasoning can benefit from writing problems in a machinereadable language such as Planning Domain Definition Language (PDDL) and symbolic logic (Pan et al., 2023; Silver et al., 2023). Furthermore, interactions with simulators and debuggers can be used to improve both plans (Liu et al., 2023a) and value functions that determine the appropriateness of action calls (Ahn et al., 2022; Driess et al., 2023; Mu et al., 2023). Our work extends recent work in planning through the automated generation of admissible action spaces and consequent evaluation without the need for a simulation environment.\nEvaluating LLM Scientists Evaluating LLMs on scientific tasks is limited to QA benchmarks for measuring general science knowledge (Hendrycks et al., 2020), or specialist knowledge such as chemistry (Guo et al., 2023; Wu et al., 2017), biomedical science (Sung et al., 2021) or medicine (Jin et al., 2019, 2021). However, evaluating an LLM\u2019s performance on more open-ended tasks, such as healthcare support (Dash et al., 2023) or chemical synthesis planning (Bran et al., 2023) is done manually. To the best of our knowledge, we are the first to approach automatic evaluation of LLMs on open-ended problems in science.\nAutomatic Evaluation of LLMs While evaluation of the performance of an LLM in games (Wang et al., 2023a) or planning in PDDL domains (Silver et al., 2023) can be done automatically, many works rely on self-evaluation, where GPT-4 is used as an evaluator (Bubeck et al., 2023; Bran et al., 2023; Chiang et al., 2023; Peng et al., 2023; Zhou et al., 2023). However, these have been found to contradict human evaluation (Bran et al., 2023) or be systematically biased (Wang et al., 2023b), where the order of the provided responses affects the predicted ranking. In comparison to these works, we use an LLM to generate pseudo-ground truth data on an easy task, in which the model consistently performs well at, which we use to evaluate on a more difficult task with real-world implications."
        },
        {
            "heading": "3 The BIOPROT dataset",
            "text": "Here we describe the BIOPROT dataset - a collection of publicly available protocols that are used to evaluate the performance of LLMs on protocol generation on a large range of topics in biology. We discuss the contents of the dataset (Section 3.1), creating a set of admissible actions and translating the protocol steps (Section 3.2), manual verification of the data (Section 3.3), and the tasks that can be approached with it (Section 4). The dataset can be found in the Supplementary Materials. This approach can be used to generate pseudocode datasets in any domain that has step-by-step instructional data."
        },
        {
            "heading": "3.1 A Dataset of Protocols for Biology",
            "text": "We collect publicly available protocols from Protocols.io (Teytelman et al., 2016), a platform for developing and sharing reproducible methods. This database contains over 9,000 public protocols of different scientific areas and complexity. Each protocol consists of (i) a title, (ii) a description, and (iii) step-by-step instructions. We automatically and manually filter the protocols, in order to obtain a set of protocols that are related to biology, can be reproduced, and are of sufficient difficulty. For further details about the filtering, refer to the Supplementary Materials. In Table 1 we present a summary of the collected protocols."
        },
        {
            "heading": "3.2 Translating Protocols to Pseudocode",
            "text": "As discussed in Section 1, evaluation of planning problems is difficult in natural text, and prior works opt for manual evaluation (Bran et al., 2023; Boiko\net al., 2023). To this end, we \u201ctranslate\u201d the free text protocols into pseudocode using GPT-4 (see Figure 2). We task GPT-4 to (i) define a set of pseudofunctions that suffice to execute the protocol, and (ii) convert the protocol steps into pseudocode using only the provided pseudofunctions.\nWe make use of a one-shot example prompt, and an automatic feedback loop (Liu et al., 2023a) that provides error signals if: the generated code is not valid Python pseudocode; no pseudofunctions are defined; the pseudocode or pseudofunctions do not have arguments; any numerical parameters in the pseudocode do not have units. Finally, GPT4 is prompted to check for errors or omissions in the pseudofunctions and pseudocode. Information about our generated pseudocode is summarized in Table 2."
        },
        {
            "heading": "3.3 Manual Verification",
            "text": "We manually reviewed the generated pseudofunctions and pseudocode for accuracy. Original protocols and generated ground-truth pseudocode were assessed line-by-line by a competent laboratory scientist. They confirmed (i) whether the original natural language protocol made sense, (ii) whether the title and description sufficiently described the protocol so that a competent scientist could attempt to complete it without the protocol, and (iii) whether the pseudocode was accurate. Finally, edits were made to the generated pseudocode as necessary. We show a breakdown of the edits made in Table 3.\nOverall, 59 of the 100 protocols were found to be completely accurate requiring no edits. However, many protocols that did require edits only required minor edits. The most common errors found were missing units for numbers, which in most cases would not prevent a competent scientist from completing a protocol. The more impactful errors found were most commonly (1) missing details which would allow one to successfully complete a step of the protocol (these were usually highly verbose steps which explained a detailed technical method for manipulating a sample) and (2) not explaining the composition of a material used in the protocol (e.g. a buffer).\nThe corrected protocols are made available as the BIOPROT dataset. Even without human editing, LLMs with error-checking loops can be used to create a largely accurate dataset for biology protocol pseudocode, thus enabling self-evaluation."
        },
        {
            "heading": "3.4 Machine-generated Descriptions",
            "text": "For some of our downstream tasks, it is necessary to have high-quality descriptions of protocols that give a sense of what the protocol steps should include. However, protocol descriptions in Protocols.io are not always suitable for this purpose. To this end, we also generated descriptions of protocols that provided a high-level overview of the protocols\u2019 objective (the prompt for this is seen in the Supplementary Materials). We include both our machine-generated descriptions and the original descriptions in our dataset."
        },
        {
            "heading": "4 Metrics and evaluation",
            "text": "Using the BIOPROT dataset, we evaluate an LLM\u2019s capabilities to reason about and generate scientific protocols on several different tasks.\nNext Step Prediction Given a protocol title, description, an admissible set of pseudofunctions, and partially completed pseudocode, we evaluate the model\u2019s ability to correctly identify the pseudofunction corresponding to the next step in the protocol. We evaluate the correctness of both the predicted function and the function arguments.\nFor function-level accuracy, we report the percentage of the number of correct function assignments\naccuracy = 1\nN N\u2211 n=1 1[fpredi = f GT i ],\nwhere fpred and fGT are the predicted and groundtruth functions, respectively, and N is the number of steps in the protocol.\nDuring generation, the model is prompted to name each function argument and provide the argument parameters. To evaluate accuracy of the arguments, we first check whether the function argument names is correct. For that purpose, we compute precision and recall of the arguments\u2019 names. For correct function arguments, we consider the accuracy of the argument value using the BLEU metric (Papineni et al., 2002). Additionally, we encode the predicted and ground truth argument values, apredi and a GT i , respectively, with SciBERT (Beltagy et al., 2019) sentence encoder E to get the SciBERTscore:\nSciBERTscore = 1\nN N\u2211 i=0 \u27e8E(apredi ), E(aGTi )\u27e9 \u2225E(apredi )\u2225\u2225E(aGTi )\u2225 ,\nwhich is the average cosine similarity between predicted and ground truth argument values for all N steps. This metric is inspired by BERTScore (Zhang et al., 2019), but we use a SciBERT encoder as it is better suited to the scientific domain. We only compute argument-level metrics for correctly predicted functions, as not to penalize the model twice for wrong function predictions.\nProtocol Generation Given a protocol title, description, and an admissible set of pseudofunctions, the model is tasked to generate corresponding pseudocode. We again evaluate the correctness of predicted functions and their corresponding arguments. This is a more difficult task than the previous one, as the model needs to plan the entire execution of the protocol. For function-level evaluation, we need to measure (i) if the correct functions were called, and (ii) if they were used in the correct order. For the former, we report precision and recall of function calls, where we take into account repeated calls of the same function. For evaluating whether the functions are used in the correct order, we use the Levenshtein distance Ld between the predicted and ground-truth sequence of functions. The Levenshtein distance is originally a string edit distance that measures the number of insertions, deletions, or substitutions to make one word into another. We consider each function call as a separate symbol, which incurs a cost of 1 for being added, deleted, or substituted. We report a normalized Levenshtein\ndistance Ldn Ldn =\nLd N ,\nwhere N is the number of functions in the groundtruth pseudocode.\nIn addition, we evaluate the predicted function arguments. We use the same metrics as described under \"Next Step Prediction\".\nFunction Retrieval Our approach has the potential to allow novel protocols to be assembled from steps provided in existing protocols in the dataset, if the model is able to correctly identify which steps are needed for any given protocol. Thus, given a protocol title and description, and a set of pseudofunctions, we evaluate the models\u2019 ability to correctly identify which of the provided functions are needed to execute the protocol. In this task, we provide the model with a set of pseudofunctions consisting of the ground-truth pseudofunctions for the given protocol, and pseudofunctions drawn from several (i) random or (ii) nearest neighbour protocols. Providing functions from nearest neighbour protocols is more difficult, as they are likely to be more similar to the correct functions. We measure the precision and recall of retrieved functions."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Implementation details",
            "text": "We explore the performance of GPT-3.5 and GPT-4 from the OpenAI API. Where we find nearest neighbors, we use an embedding index of all protocols\u2019 descriptions using text-embedding-ada-002 embeddings, unless stated otherwise. We show the prompts we use in the Supplementary Material.\nFor each of the tasks listed in Section 4, we evaluate the models in several settings:\n\u2022 Shuffled: the model can be provided either with functions in the order in which they are\ngenerated, or randomly shuffled. The functions tend to be defined in the order they appear in the original protocol, and that serves as a signal to the model we evaluate. By randomly shuffling the input functions, we make the task more difficult.\n\u2022 Feedback: The model has access to an error loop that can detect undefined functions and Python syntax errors. Such feedback loops have been found to be beneficial in PDDL planning (Silver et al., 2023) and reasoning (Madaan et al., 2023)."
        },
        {
            "heading": "5.2 Results",
            "text": "Next step prediction We show results on next step prediction in Table 4. We see that GPT-4 consistently outperforms GPT-3.5 in both the prediction of the correct next step, whereas GPT-3.5 performs better at predicting function arguments. We note there is a drop in performance when the input functions are shuffled, likely because if not shuffled, the functions appear roughly in the order as they should be called as they were sequentially generated by the LLM.\nProtocol generation We show results on full protocol generation in Table 5. We observe the biggest gap in the Levenshtein distance score metric, where GPT-4 significantly outperforms GPT-3.5. Meanwhile, GPT-4 and GPT-3.5 show similar precision and recall of used functions. This suggests that while both have a similar ability to use the correct functions, GPT-4 performs better at using the right order. We also observe that shuffling the input functions consistently leads to a drop in performance.\nFunction retrieval We show retrieval results in Table 6. We see that GPT-4 outperforms GPT-3.5 on this task. However, the results on this task appear generally poor. One possible reason for the\npoor performance is that the correct answer may sometimes be ambiguous. For example, Mix and MixSubstance are semantically identical, but have different syntax, and the model would be penalized for selecting a function not from the query protocol. This effect would explain why performance using the\u201c\"nearest\u201d neighbours is worse than performance when using \u201crandom\u201d protocols."
        },
        {
            "heading": "5.3 Using GPT-4 as an evaluator",
            "text": "We use GPT-4 as an evaluator, where given (i) a protocol description, (ii) admissible pseudofunctions, (iii) ground-truth pseudocode (generated as described in Section 3.2), and (iv) predicted pseudocode, the model is prompted to predict which one of (iii) or (iv) better matches the protocol description (i). We report the rate at which the predicted pseudocode was preferred in Table 8. In general, GPT-4 only performs slightly above chance in identifying the ground truth protocol, versus LLM generations, although it is unclear whether this is because the machine-generated protocols are largely correct, or because GPT-4 is unable to distinguish correct from incorrect protocols. Note that prior works (Bran et al., 2023) found that a GPT evaluator tends to prefer longer and more coherent, but not necessarily more correct generations."
        },
        {
            "heading": "5.4 Using GPT-4-Generated Descriptions",
            "text": "For some protocols, we observe that the detail present in the protocol description does not suffice to enable protocol reconstruction. To this end, we use GPT-4 to generate a short pseudo description given the protocol steps in natural text. We present results on next step generation and full protocol generation in Figure 8. We see a small increase in performance, which is expected, as the summarygenerating model can include more detail (however, the pseudo descriptions are shorter \u2013 see Table 2)."
        },
        {
            "heading": "5.5 Real-World Validation",
            "text": "Finally, to validate that BIOPROT can be used to generate accurate novel protocols, we devised a setup for end-to-end protocol creation. To do this we opted to build an LLM agent with access to tools, such that it can retrieve protocols that contain relevant pseudofunctions, and use their pseudofunctions to generate new pseudocode. Note that for good performance in this real-world validation task, the LLM needs to be able to (1) find relevant psueodofunctions from other protocols, and (2) generate correct pseudocode, both of which are tasks we build metrics for. Details are as follows: we created a Toolformer-like (Schick et al., 2023) chain-of-thought LLM agent (Wei et al., 2022) with access to a tool for searching for protocols in the BIOPROT database. This agent used the GPT-4 LLM. We prompted the agent to retrieve protocols relevant to generating a new target protocol. We extracted the pseudofunctions from the retrieved protocols and then prompted the agent to generate a new protocol using only the retrieved pseudofunctions. We used this setup to create two experiments using GPT-4: (1) culturing a single colony of E.coli bacteria overnight and making a glycerol stock with\nthe suspension (a form of cryopreservation for longterm storage), and (2) culturing Symbiodinum (a large genus of dinoflagellates endosymbiontic to cnidarians that may help corals survive in warming oceans), extracting its DNA, and then running the DNA on an agarose gel."
        },
        {
            "heading": "5.6 Real-World Validation Results",
            "text": "The model generated two new protocols using pseudofunctions from our database. Both of these protocols were reviewed by a scientist and were determined to be accurate and sufficient for a competent lab scientist to follow. We opted to complete the first protocol using E.coli as we did not have Symbiodinium available in the laboratory. We validated the first protocol by implementing it in the lab with the instructions and parameter values provided by the model. The protocol ran successfully: the cells remained viable after storage at -80 \u00b0C, as evidenced by subsequent culture on nutrient agar (see Figure 3). The methods and prompts used to generate these experiments, as well as the agent chain-of-thought reasoning, can be found in the Appendix."
        },
        {
            "heading": "6 Conclusion",
            "text": "We have introduced a method for automatic evaluation of LLMs on open-ended planning problems, such as those found in experimental sciences, and a dataset of such planning problems in biology laboratory protocols. We then defined a suite of tasks and evaluation metrics that can be used to measure performance and help drive progress in the field. We evaluate GPT-3.5 and GPT-4 on these tasks and find that there is more to be desired in terms of performance. Finally, we show an application of our dataset and framework, where an LLM generates a protocol that is successfully executed in a laboratory."
        },
        {
            "heading": "7 Limitations",
            "text": "Use of paid API The GPT-4 and GPT-3.5 models we use are not open-sourced and can have significant costs for large-scale experiments. In total, we used approximately $1000 for API calls. Further work should explore the performance of opensourced LLMs.\nAdditional scientific fields Our work is focused on biology, but could be extended to other fields such as chemistry and materials science. Future works should explore extending the dataset and framework.\nMisuse There is a risk of misuse, where adversaries could use our framework or dataset to inform the synthesis of harmful compounds. We have taken care to ensure the protocols in BIOPROT contain no protocols that can easily be used for such purposes. Continued research on aligning LLMs and restriction of dangerous outputs is important to minimize risk. We hope that our approach of using pseudofunctions may in the future allow for easier programmatic evaluation of outputs, and easier detection of the generation of hazardous substances."
        },
        {
            "heading": "A Dataset Filtering",
            "text": "We filter Protocols.io protocols such that (i) they can be parsed and provided to an LLM, and (ii) they are sufficiently challenging to serve as an evaluation set. We performed the following automatic and manual filtering:\nAutomatic filtering Protocols were automatically removed if they:\n\u2022 Do not contain a description\n\u2022 Contain linked files that could not be parsed\n\u2022 Contain images that could not be parsed by text-only models\n\u2022 Contain tables (standard LLMs can sometimes struggle with table-based data representations without few-shot examples (Hegselmann et al., 2023))\n\u2022 Consist of fewer than three steps (such protocols were insufficiently complex to demonstrate multi-step planning)\nManual filtering Following automatic filtering, protocols were manually removed if they:\n\u2022 Were not relevant to biology\n\u2022 Were considered poorly written to the extent that a human could not accurately replicate the protocol"
        },
        {
            "heading": "B Prompts",
            "text": "B.1 Main experiments We show the prompts we use for generating pseudofunctions and pseudocode in Figure 6, predicting pseudocode in Figure 7, summarising protocol steps in Figure 8. We show the error messages we use in the feedback loops in Figure 9. Figure 12, and the resulting generated protocol used in our real-world experiment is found in Figure 13\nB.2 Lab experiments Here we show the prompts we use in Section 5 of the paper. Figure 10 is the prompt provided to the CoT agent. Figure 11 shows the Langchain output form the agent. Figure 12 shows the prompt that contains the retrieved pseudofunctions. Finally, Figure 13 shows the pseudocode that was given to a biologist to execute in a laboratory."
        },
        {
            "heading": "C Qualitative Evaluation",
            "text": "We show qualitative results for protocol id 145 from BIOPROT. For further qualitative examples, please refer to the BIOPROT dataset.\nTitle Ethanol precipitation of nucleic acids (Eppendorf tubes)\nDescription Nucleic acid precipitation is used to concentrate and/or purify nucleic acids. The below protocol is based on the fact that nucleic acids are less soluble in alcohol than in more polar water. Addition of salt further decreases solubility by competing for water dipoles; as does low temperature. Please see the OpenWetWare website for more details.\nSteps\n1. Add 1/10 volume of 3M sodium acetate, pH 5.2 or 1/2 volume of 5M ammonium acetate. reagents\n2. Add 2-3 volumes of 100% Ethanol.\n3. Mix and freeze overnight in -20. NOTES In general, the time you need to incubate in the freezer depends on how much nucleic acid you have, how big it is and the volume it is in. My general protocol is to freeze for 20 min to 1 hr at -80C. This seems to work well for most things, but you may want to freeze longer if you have only a small concentration of nucleic acid or if it is small in size(<15 nucleotides). (Kathleen) NOTES If you are in a hurry, you can also dip you epi shortly into liquid nitrogen. If you added enough ethanol, the mix won\u2019t freeze. Careful with isopropanol - it freezes more quickly. This works well for me and saves me a lengthy incubation in the fridge. (Jasu)\n4. Spin at full speed in a standard microcentrifuge at 4 degrees for 30 minutes. 1800s\n5. Decant (or carefully pipet off) the supernatant.\n6. Dry the pellet. NOTES For this you can air dry (tubes open, 15 min) or dry in a speedvac. DNA and RNA (if you don\u2019t have RNases in your sample) are typically hearty\nModel Neighbourhood Precision Recall\nLLama2-7B Nearest 26.1 57.5 LLama2-7B Random 28.1 56.3\nGenerated Pseudocode and Pseudofunctions We show the generated pseudocode and pseudofunctions, which we use as ground truth, in Figure 4\nPredicted Protocol We show the predicted protocol in Figure 5"
        },
        {
            "heading": "D LLama evaluation",
            "text": "To benchmark performance on open-source models, we also conducted a run of our experimental\nevaluation tasks on Llama-2 (Touvron et al., 2023). We evaluate the 7B model and report performance on protocol generation and function retrieval in Table 9 and Table 10, respectively. We found that Llama-2 significantly underperforms GPT-3.5 and GPT-4 models in function selection. As part of our evaluation on Llama-2 we observe that, when using feedback, the model is distracted and does not attempt to re-write code. Iterative feedback appears to be a process that is effective for GPT models and not Llama models, and this observation is consistent with prior work (Madaan et al., 2023). We also ran Llama-2 on the next step prediction task, but we found that the model was unable to complete this task. The model would typically produce text that states an intent to complete the pseudocode rather than writing the actual next pseudocode line. This difference in behaviour is likely due to a difference in training regimes between GPT models and Lllama-2, but given the lack of documentation around the training of GPT models, the precise nature of this difference is unknown."
        },
        {
            "heading": "E Dataset and Evaluation",
            "text": "The BIOPROT dataset and evaluation metrics from this paper can be found at https://github.com/bioplanner/bioplanner"
        },
        {
            "heading": "F Human Benchmarking",
            "text": "While we believe our metric is internally useful for comparing the performance of LLM models and approaches, we wanted to assess how our tasks used relate to human performance. To this end, we performed a human evaluation of next-step prediction tasks and function selection tasks. We worked with an undergraduate biomedical sciences student and asked them to complete the next step prediction task and the function selection task. The student had access to internet search and an unlimited amount of time to answer questions. With the next step prediction task we provided shuffled\nfunctions, and with the function selection task, we used random distractor functions. For the function selection task, Human Precision was 87.5%, and human Recall was 0.84%, (n=20), indicating a significant increase in performance over GPT-4. GPT-4 performance is potentially weaker than human performance in the function selection task due to the large number of nearest-neighbour functions in the context window acting as distractors from the task instructions. For the next step prediction task human accuracy was 54.8% (n=32), with Precision and Recall or arguments being 97% and 95% respectively. This performance is roughly comparable to GPT-4 in the shuffled function setting."
        }
    ],
    "title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology",
    "year": 2023
}