{
    "abstractText": "While pre-trained language models (PLMs) have shown evidence of acquiring vast amounts of knowledge, it remains unclear how much of this parametric knowledge is actually usable in performing downstream tasks. We propose a systematic framework to measure parametric knowledge utilization in PLMs. Our framework first extracts knowledge from a PLM\u2019s parameters and subsequently constructs a downstream task around this extracted knowledge. Performance on this task thus depends exclusively on utilizing the model\u2019s possessed knowledge, avoiding confounding factors like insufficient signal. Employing this framework, we study factual knowledge of PLMs and measure utilization across 125M to 13B parameter PLMs. We observe that: (1) PLMs exhibit two gaps in acquired vs. utilized knowledge, (2) they show limited robustness in utilizing knowledge under distribution shifts, and (3) larger models close the acquired knowledge gap but the utilized knowledge gap remains.",
    "authors": [
        {
            "affiliations": [],
            "name": "Amirhossein Kazemnejad"
        },
        {
            "affiliations": [],
            "name": "Mehdi Rezagholizadeh"
        },
        {
            "affiliations": [],
            "name": "Prasanna Parthasarathi"
        },
        {
            "affiliations": [],
            "name": "Sarath Chandar"
        }
    ],
    "id": "SP:0b3f0374c7859670da1ab9a5e50472a331148102",
    "references": [
        {
            "authors": [
                "Sid Black",
                "Leo Gao",
                "Phil Wang",
                "Connor Leahy",
                "Stella Biderman."
            ],
            "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow",
            "venue": "Zenodo.",
            "year": 2021
        },
        {
            "authors": [
                "Terra Blevins",
                "Hila Gonen",
                "Luke Zettlemoyer."
            ],
            "title": "Prompting language models for linguistic structure",
            "venue": "ArXiv preprint, abs/2211.07830.",
            "year": 2022
        },
        {
            "authors": [
                "Tom B. Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "year": 2020
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman",
                "Alex Ray",
                "Raul Puri"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Yunxuan Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma",
                "Albert Webson",
                "Shixiang Shane Gu"
            ],
            "title": "Scaling instruction-finetuned language models",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Shibo Hao",
                "Bowen Tan",
                "Kaiwen Tang",
                "Bin Ni",
                "Hengzhe Zhang",
                "Eric P Xing",
                "Zhiting Hu."
            ],
            "title": "Bertnet: Harvesting knowledge graphs from pretrained language models",
            "venue": "ArXiv preprint, abs/2206.14268.",
            "year": 2022
        },
        {
            "authors": [
                "Ves Stoyanov"
            ],
            "title": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
            "year": 2023
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Frank F. Xu",
                "Jun Araki",
                "Graham Neubig"
            ],
            "title": "How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423\u2013438",
            "year": 2020
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Artur Kulmizev",
                "Joakim Nivre."
            ],
            "title": "Schr\u00f6dinger\u2019s tree - on syntax and neural language models",
            "venue": "ArXiv preprint, abs/2110.08887.",
            "year": 2021
        },
        {
            "authors": [
                "Joel Lamy-Poirier",
                "Jo\u00e3o Monteiro",
                "Oleh Shliazhko",
                "Nicolas Gontier"
            ],
            "title": "Starcoder: may the source be with you! ArXiv preprint, abs/2305.06161",
            "year": 2023
        },
        {
            "authors": [
                "Yanyang Li",
                "Jianqiao Zhao",
                "Michael Lyu",
                "Liwei Wang."
            ],
            "title": "Eliciting knowledge from large pre-trained models for unsupervised knowledgegrounded conversation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Liu",
                "Alisa Liu",
                "Ximing Lu",
                "Sean Welleck",
                "Peter West",
                "Ronan Le Bras",
                "Yejin Choi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Generated knowledge prompting for commonsense reasoning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "ArXiv preprint, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zihan Liu",
                "Mostofa Patwary",
                "Ryan Prenger",
                "Shrimai Prabhumoye",
                "Wei Ping",
                "Mohammad Shoeybi",
                "Bryan Catanzaro."
            ],
            "title": "Multi-stage prompting for knowledgeable dialogue generation",
            "venue": "Findings of the Association for Computational Linguistics: ACL",
            "year": 2022
        },
        {
            "authors": [
                "Dublin",
                "Ireland"
            ],
            "title": "Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Charles Lovering",
                "Rohan Jha",
                "Tal Linzen",
                "Ellie Pavlick."
            ],
            "title": "Predicting inductive biases of pretrained models",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.",
            "year": 2021
        },
        {
            "authors": [
                "Tom McCoy",
                "Ellie Pavlick",
                "Tal Linzen."
            ],
            "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428\u20133448, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Amil Merchant",
                "Elahe Rahimtoroghi",
                "Ellie Pavlick",
                "Ian Tenney"
            ],
            "title": "What happens to BERT embeddings during fine-tuning",
            "venue": "In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,",
            "year": 2020
        },
        {
            "authors": [
                "Benjamin Newman",
                "Prafulla Kumar Choubey",
                "Nazneen Rajani."
            ],
            "title": "P-adapters: Robustly extracting factual information from language models with diverse prompts",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Rodrigo Nogueira",
                "Kyunghyun Cho."
            ],
            "title": "Passage re-ranking with bert",
            "venue": "ArXiv preprint, abs/1901.04085.",
            "year": 2020
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "ArXiv preprint, abs/2203.02155.",
            "year": 2022
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Chengwei Qin",
                "Aston Zhang",
                "Zhuosheng Zhang",
                "Jiaao Chen",
                "Michihiro Yasunaga",
                "Diyi Yang"
            ],
            "title": "Is chatgpt a general-purpose natural language processing task solver? ArXiv preprint, abs/2302.06476",
            "year": 2023
        },
        {
            "authors": [
                "Guanghui Qin",
                "Jason Eisner."
            ],
            "title": "Learning how to ask: Querying LMs with mixtures of soft prompts",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Adam Roberts",
                "Colin Raffel",
                "Noam Shazeer"
            ],
            "title": "How much knowledge can you pack into the parameters of a language model",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Taylor Shin",
                "Yasaman Razeghi",
                "Robert L. Logan IV",
                "Eric Wallace",
                "Sameer Singh."
            ],
            "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Ian Tenney",
                "Dipanjan Das",
                "Ellie Pavlick."
            ],
            "title": "BERT rediscovers the classical NLP pipeline",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593\u2013 4601, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Ian Tenney",
                "Patrick Xia",
                "Berlin Chen",
                "Alex Wang",
                "Adam Poliak",
                "R. Thomas McCoy",
                "Najoung Kim",
                "Benjamin Van Durme",
                "Samuel R. Bowman",
                "Dipanjan Das",
                "Ellie Pavlick"
            ],
            "title": "What do you learn from context? probing for sentence structure",
            "year": 2019
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals."
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "ArXiv preprint, abs/1807.03748.",
            "year": 2019
        },
        {
            "authors": [
                "Cunxiang Wang",
                "Pai Liu",
                "Yue Zhang"
            ],
            "title": "Can generative pre-trained language models serve as knowledge bases for closed-book QA",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Alex Warstadt",
                "Yian Zhang",
                "Xiaocheng Li",
                "Haokun Liu",
                "Samuel R. Bowman."
            ],
            "title": "Learning which features matter: RoBERTa acquires a preference for linguistic generalizations (eventually)",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods",
            "year": 2020
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "The Tenth International Conference on Learning Representa-",
            "year": 2022
        },
        {
            "authors": [
                "Wenhao Yu",
                "Dan Iter",
                "Shuohang Wang",
                "Yichong Xu",
                "Mingxuan Ju",
                "Soumya Sanyal",
                "Chenguang Zhu",
                "Michael Zeng",
                "Meng Jiang."
            ],
            "title": "Generate rather than retrieve: Large language models are strong context generators",
            "venue": "The Eleventh Inter-",
            "year": 2023
        },
        {
            "authors": [
                "Wang",
                "Luke Zettlemoyer."
            ],
            "title": "Opt: Open pretrained transformer language models",
            "venue": "ArXiv preprint, abs/2205.01068.",
            "year": 2022
        },
        {
            "authors": [
                "Xuhui Zhou",
                "Yue Zhang",
                "Leyang Cui",
                "Dandan Huang."
            ],
            "title": "Evaluating commonsense in pretrained language models",
            "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial",
            "year": 2020
        },
        {
            "authors": [
                "Yichu Zhou",
                "Vivek Srikumar."
            ],
            "title": "A closer look at how fine-tuning changes BERT",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1046\u20131061, Dublin, Ireland. Association for",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent research has demonstrated that language models pre-trained on vast amounts of internet data acquire a broad range of knowledge about linguistic structures (Tenney et al., 2019b; Blevins et al., 2022), encyclopedic relations (Petroni et al., 2019; Hao et al., 2022), levels of commonsense (Zhou et al., 2020; Liu et al., 2022a) , and even coding and reasoning rules (Chen et al., 2021; Wei et al., 2022b). Recent studies on behavioral parametric probing and prompting (Jiang et al., 2020; Qin and Eisner, 2021; Brown et al., 2020) has demonstrated that such knowledge, collectively referred to as \u201cparametric knowledge,\u201d resides reliably within a subset of trained parameters in pre-trained models (PLMs). Importantly, this knowledge can be identified without additional finetuning. For instance,\n\u2020 Equal advising.\nknowledge (what the model knows). Gap 2 exists in how much of this knowledge can actually be utilized in downstream tasks (the usable knowledge). We find that although the first gap mostly shrinks, the second remains as we increase the model\u2019s size.\ngiven the prompt \u201cThe capital of France is\u201d, a PLM can be queried to complete the input and extract the fact \u201cParis\u201d.\nA common assumption about parametric knowledge is that if the model poses a certain type of knowledge, it utilizes it when performing downstream tasks related to that knowledge. For example, if a model knows about X and Y (such that X and Y are similar), and is taught to perform a task on X , the convention is that the model generalizes the application of the task on Y and all other similar knowledge. Such is the foundation for the recent interest in instruction tuning (Wei et al., 2022a; Chung et al., 2022), and the SFT-RLHF pipeline (Ouyang et al., 2022). In this paradigm, LLMs are finetuned to learn how to follow instructions on a few tasks the model is capable of and are subsequently expected to generalize and follow instructions for novel tasks by utilizing their pre-training knowledge (residing in their parameters).\nHowever, it is not clear to what extent this as-\nsumption holds in practice, giving rise to a central question: how much of parametric knowledge will get applied in downstream tasks? If the causal link between \"identifiable knowledge\" and its practical application in downstream tasks is not established (Kulmizev and Nivre, 2021), the mere presence of knowledge within a model\u2019s parameters does not necessarily guarantee its utilization in such tasks. This raises questions about the assertion of pretrained language models (PLMs) as differentiable knowledge bases (Hao et al., 2022) and their overall capabilities. For instance, as demonstrated by Qin et al. (2023), ChatGPT\u2019s performance lags behind its foundational model, GPT-3.5, in areas including commonsense and logical reasoning tasks.\nPrevious studies have investigated this question within linguistic domains and have demonstrated that although PLMs have the capacity to encode linguistic knowledge, they may not effectively employ it in downstream tasks. For example, McCoy et al. (2019) illustrates that PLMs employ syntactic heuristics to solve NLI even though they are able to represent proper linguistic hierarchies (Tenney et al., 2019a), even after finetuning (Merchant et al., 2020; Zhou and Srikumar, 2022). Warstadt et al. (2020) provide evidence that RoBERTa requires data inoculation or pre-training with extensive data in order to effectively utilize its hierarchical linguistic knowledge. In a more recent study, Lover-\ning et al. (2021) demonstrate that the quantity of \u201cevidence\u201d presented in the finetuning dataset influences the features that PLMs rely on during the finetuning process. Specifically, the model may resort to lexical heuristics when the finetuning signal toward linguistic features is insufficient.\nIn this work, we are interested in a more general sense of knowledge and propose XTRAEVAL \u2014 EXTRACT, TRAIN, AND EVALUATE \u2014 to systematically measure how much of parametric knowledge is utilized in downstream tasks. XTRAEVAL sidesteps potential confounders (such as shortcuts or insufficient signal) that arise from the nature of arbitrary crowd-sourced tasks used in prior work by carefully creating the downstream task from the model\u2019s own knowledge. Specifically, given a pre-trained language model, our framework first identifies and extracts knowledge residing in its parameters. Subsequently, using the extracted knowledge, we construct a downstream task on which we finetune the model. Finally, we measure knowledge utilization based on its performance on the downstream task. By constructing the task based on the model\u2019s pre-existing knowledge, we ensure that (1) the model is evaluated solely on its possessed knowledge, avoiding penalties for lacking information and (2) successful task completion relies explicitly on utilizing the model\u2019s parametric knowledge, eliminating the insufficient training sig-\nnal issue and dataset shortcuts. In this paper, we provide the first instantiation of this paradigm based on encyclopedic knowledge facts and conduct an extensive study to measure knowledge utilization of PLMs across a wide range of parametric scales (ranging from 125M to 13B). We observe the following:\n\u2022 PLMs show two different but equally important gaps: (1) The gap in the acquired knowledge and (2) and the gap in parametric knowledge that can be actively applied to downstream tasks (Section 3).\n\u2022 PLMs are not robust to finetuning distribution shifts, and failure to utilize knowledge worsens with such shifts, further questioning their generalization capabilities (Section 4).\n\u2022 Although scaling the number of parameters helps to close the first gap, the second still remains in larger sizes (Section 5).\nIn the next sections, we first describe our framework and its instantiation in detail (Section 2), and finally present our experimental results in Sections 3 to 5."
        },
        {
            "heading": "2 Framework",
            "text": ""
        },
        {
            "heading": "2.1 EXTRACT, TRAIN, AND EVALUATE",
            "text": "Principles The primary objective of our evaluation framework is to measure how much of the knowledge present in the model\u2019s parameters is actually usable in downstream tasks. Ideally, downstream tasks must be designed in a way that solely attributes any success to the model\u2019s knowledge being used, while ensuring that failure in performing the task is not due to a lack of pre-training knowledge. The Paradigm To this end, we propose EXTRACT, TRAIN, AND EVALUATE , which consists of three main steps:\nStep 1. Given a pre-trained model M\u03b8 with parameters \u03b8 and a diagnostic dataset D (e.g. a set of encyclopedic facts or coding problems), we first extract and identify parametric knowledge as a set of data instances x \u2208 D the model can solve without further training (zero-shot). We denote such a set as D\u03b8, a realization of M\u03b8\u2019s parametric knowledge w.r.t D.\nStep 2. We construct a downstream task K around the model\u2019s own knowledge D\u03b8 (e.g. fact\nretrieval or following instructions in coding) such that the model can only solve the task by utilizing the knowledge identified in the first step. More formally, we create K\u03b8train and K\u03b8test as the nonoverlapping train and test sets of downstream task K, where the model learns the task from K\u03b8train.\nStep 3. Finally, the performance on the test set K\u03b8test is used as a measure of the model\u2019s ability to utilize its knowledge.\nConstructing the downstream task based on the model\u2019s knowledge ensures that the model is not evaluated on the knowledge it did not acquire during pre-training. Also, the I.I.D. nature of this paradigm (i.e. the model is only exposed to inputs it is already familiar with) allows us to measure whether the model can utilize its knowledge at all."
        },
        {
            "heading": "2.2 Encyclopedic Knowledge",
            "text": "Factual parametric knowledge as in encyclopedic facts is well-studied in PLMs (Petroni et al., 2019; Jiang et al., 2020) and allows for an objective and systematic evaluation of our framework (Figure 2). Therefore, in this paper, we instantiate XTRAEVAL to measure the utilization of parametric knowledge concerning encyclopedic facts. In this case, the diagnostic dataset D is a set of encyclopedic facts D = {\u27e8h, r, t\u27e9i}ni=1 acquired from an off-the-shelf knowledge base (e.g. Wikipedia). Each fact xi \u2208 D is a tuple of the form \u27e8head, relation, tail\u27e9, such as \u27e8Barack Obama,GraduatedFrom,Harvard\u27e9.\nIn the extraction phase, a pre-trained model M\u03b8 has to zero-shot predict the tail entity t given the head entity h and the relation r. We use softprompting (Qin and Eisner, 2021) to obtain the model\u2019s predictions, as it enhances prediction consistency compared to discrete prompts, particularly for moderate-sized models. The extracted knowledge D\u03b8 \u2282 D is the subset of tuples the model can predict correctly.\nOur downstream task K is a standard document retrieval task (Karpukhin et al., 2020). Given a query q, the model retrieves the relevant document from a set of candidates. We construct K\u03b8 from the extracted knowledge in D\u03b8 by converting each fact x \u2208 D\u03b8 into a retrieval instance k \u2208 K\u03b8. This conditions the downstream task on the model\u2019s knowledge. The conversion generates a query q by removing the tail entity t from x. It then generates relevant and irrelevant documents using a stochastic generator\nd \u223c P(d | H = h,R = r, T = t), (1)\nwhere d depends on the head entity h, relation r, and tail entity t. The document generator, P(d | \u00b7), selects a template at random and fills in the blanks with the input entities. If H , R, or T are missing, the generator chooses a random entity from D\u03b8 to complete the input. Specifically, we generate relevant document d+ by sampling from P(d | \u00b7) with gold entities in x as input, and create irrelevant documents d\u2212 by omitting one or more entities. Each k comprises a tuple (q, {d+, d\u22121 , . . . , d\u2212m}), where m is the number of irrelevant documents.\nWe partition D\u03b8 randomly (60%-40%) to generate K\u03b8train and K\u03b8test, which serve as the training and test sets for the downstream task, respectively. We finetune the model on K\u03b8train in cross-encoder setup (Nogueira and Cho, 2020) with the InfoNCE objective (van den Oord et al., 2019):\nL(k) = \u2212 log exp(sim(q, d +))\u2211\nd\u2208{d+,d\u22121 ,...,d \u2212 m} exp(sim(q, d))\n.\nThe similarity score sim(., ) is computed as\nsim(q, d) = h(M\u03b8([CLS]; q; d)),\nwhere h is a randomly initialized value head that takes the representation of the [CLS] token (or the\nlast token for decoder-only models) and outputs a scalar as the similarity measure (Figure A.1). Finally, we evaluate the model on K\u03b8test by measuring its accuracy in retrieving the relevant document d+ among {d+, d\u22121 , . . . , d\u2212m} for a given query q. The task design ensures that the association between knowledge query qi and gold fact document d+i relies solely on the parametric knowledge represented by xi \u2208 D\u03b8 This is because other variables, like text overlap, are randomly sampled from the same distribution for both query and documents.\nThus, the model can only solve the task by utilizing its internal knowledge. Finetuning on K\u03b8train should only trigger the utilization of the parametric knowledge.\nTraining The document generator P(d | \u00b7) can generate various types of documents for each fact x \u2208 D\u03b8. Please refer to Table 1 for a list of all the types. For training, we use three types for negative documents d\u2212\u2019s with uniform weights: (h, r, \u00b7), (\u00b7, r, t), and (h, \u00b7, t) as they are the hardest ones since they only differ in one entity from the query. To keep the GPU memory usage under control, we sample four documents per each type (refer to Section 3.1 for the effect of the number of negatives on the results), which results in a total of m = 12 negatives. We resample the documents on each epoch to avoid overfitting and use a validation set to choose the best checkpoint. Also, we keep the learning rate low and use no weight decay to prevent any forgetting. We use three seeds for the extraction phase, three seeds for splitting D\u03b8 into train and test, and three seeds for finetuning on the downstream task, which results in 27 different runs per each model.\nInference During inference, the model must identify the gold document d+ amidst distractor documents d\u2212\u2019s. To ascertain that the model genuinely recognizes the correct answer, we employ a varied assortment of distractors. Initially, we use document type (h, r, \u00b7), ensuring all non-gold tails are included. Subsequently, we utilize the remaining non-gold document types listed in Table 1 as distractors, sampling 50 documents for each type. Lastly, we also sample 50 irrelevant but factually correct documents from the test set to assess the model\u2019s sensitivity to factual accuracy. We evaluate pre-trained models across various families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and\nBERT (Devlin et al., 2019). Unless otherwise stated, we use the base size (125M) of these models. We investigate the scaling behavior of OPT and LLaMa (Touvron et al., 2023) in Section 5. We initialize the diagnostic dataset D from LAMA (Petroni et al., 2019), which has 34K facts over 40 relations. Our results are reported over 1134 finetuning runs (Refer to Appendix A for detailed hyperparameters.)"
        },
        {
            "heading": "3 Evaluating the Knowledge Utilization",
            "text": "We separately report the fraction of facts (D) that can be extracted and the downstream performance of models in Figure 3.\nFirst, we find that, on par with previous work (Qin and Eisner, 2021), there is a significant gap in the encyclopedic facts the models can correctly predict and the entire facts present in the diagnostic dataset D (Figure 3a). Note that one can arbitrarily increase the number of correctly predicted by considering a prediction as correct if the gold entity is among the model\u2019s top-k predictions. However, we only consider k = 1 to only focus on the facts that the model can confidently predict. Nonetheless, we find that BERT and RoBERTa extract slightly more encyclopedic facts than GPT-Neo and OPT.\nCritically, all models demonstrate a pronounced gap in downstream task performance, or knowledge utilization, (Figure 3b). This unexpected outcome occurs despite the downstream task being seemingly simple since (1) models are trained and evaluated on examples based on their accurate encyclopedic knowledge predictions, and (2) both K\u03b8train and K\u03b8test are sampled from the same distributions (I.I.D), so the models only encounter seen entities. Notably, OPT and GPT-Neo manage to outperform\nBERT and RoBERTa by a small margin. This finding suggests that models struggle to utilize their entire parametric knowledge in downstream tasks. In the next sections, we investigate the potential causes of this gap."
        },
        {
            "heading": "3.1 Role of Downstream Training Data",
            "text": "The effect of initial knowledge D\u03b8 As we utilize D\u03b8 to create the downstream task, examining the impact of its size (|D\u03b8|) on knowledge utilization is crucial. If consistent behavior is observed for different sizes, it implies that the utilization gap does not stem from the amount of initial knowledge and must be rooted in inductive biases (e.g., the model or finetuning process), allowing us to measure and compare utilization with different initial knowledge.\nTo measure such effect, for each model, we first compute D\u03b8, and then instead of directly using it for K\u03b8, we sub-sample smaller sets of it at various\nfractions and construct the downstream task using each sub-sampled D\u03b8. In Figure 4.a, we observe the knowledge utilization is fairly consistent (at least for fractions > 0.4) across different sizes of D\u03b8 for all models. Larger fractions seem to have less variance as well. This suggests that the utilization performance is intrinsic to the downstream knowledge transfer rather than the initial knowledge residing in the model.\nThe effect of the number of negatives The model learns to apply its parametric knowledge by optimizing the retrieval objective. To ensure the training signal, produced by contrastive loss on K\u03b8train, is strong, we vary the number of negative documents for creating K\u03b8train. If the training signal is weak, we expect knowledge utilization to improve with more negatives.\nTo answer this question, we follow the same setup as described in Section 2 and increase the number of negative documents sampled per type from 4 to 10. We also consider reducing it to 2 negatives per type to better understand its effectiveness. We keep the initial knowledge D\u03b8 fixed.\nFigure 4.b summarizes our findings. Knowledge utilization remains the same for all models as we increase the number of negatives. This pattern is observed even with as few as two negatives per type. This suggests that the training signal is strong enough across the board and the gap in knowledge utilization is not rooted in the training objective."
        },
        {
            "heading": "3.2 Gap 1 vs. Gap 2",
            "text": "Findings in Section 3.1 shows that the gap in knowledge utilization (i.e. accuracy on K\u03b8test) does not depend on the size of D\u03b8 and is fairly consistent across different number of negatives. Moreover, we find that the variation across the random splitting of D\u03b8 to create train and test sets of the downstream task is negligible.\nThe robustness to such design choices allows us to define Usable Knowledge, which basically indicates the portion of facts from D that the model can actually utilize in the downstream task. We compute this metric by multiplying the accuracy on K\u03b8test by the fraction of correctly predicted facts in D. We report the results in Figure 5.\nThese results clearly demonstrate that there exist two gaps in the models\u2019 knowledge. Gap 1 is in how many facts the model knows after pre-training. Gap 2 is in how many of facts the model knows can be truly utilized in downstream tasks. Indeed,\nD\u03b8 (what the model knows). Gap 2 exists in how many of the known facts the model can actually utilize in downstream tasks (the usable knowledge).\nwe see that although RoBERTa manages to extract more facts than GPT-Neo, due to Gap 2, it performs the same as GPT-Neo in downstream tasks."
        },
        {
            "heading": "4 Robustness of Knowledge Utilization",
            "text": "We intentionally design the downstream task K\u03b8 to be straightforward and free of any distributional shift as we want to measure the maximum knowledge utilization of the model. However, in realworld applications, it is likely that the model encounter samples that are different from the training distribution. In this section, we investigate the robustness of knowledge application in the presence of such distributional shifts."
        },
        {
            "heading": "4.1 Non-I.I.D. K\u03b8train and K\u03b8test",
            "text": "Recall that we randomly divide D\u03b8 into two sets as the data source for the creation of K\u03b8train and K\u03b8test. In this experiment, however, we split D\u03b8 such that the relation types (r) in K\u03b8train and K\u03b8test are disjoint. Specifically, we randomly select 60% of the relations and their corresponding facts for K\u03b8train and the rest for K\u03b8test. We repeat this process over three seeds to create three different splits. We still follow the same procedure for converting knowledge triples to document retrieval examples as explained in Section 2. In this way, we ensure we don\u2019t change the task\u2019s nature, i.e. the model still needs to apply its parametric knowledge to solve the task, but the distributional shift between K\u03b8train and K\u03b8test can represent real-world scenarios. If the model learns to systematically apply its knowledge, we expect its downstream performance to be similar to or close to the I.I.D. setting (Section 3).\nWe observe downstream task performance drops\nsignificantly for all models when evaluated OOD (Figure 6). This indicates the models cannot use their knowledge on examples with unseen relation types, though all relations and facts originate in D\u03b8. Thus, knowledge usage in downstream tasks is sensitive to distribution shifts, suggesting failure to apply pre-training knowledge may be more severe in real-world applications."
        },
        {
            "heading": "5 Effect of Scaling law On The Gaps",
            "text": "Recent NLP success has come from scaling up pretraining model parameters (Brown et al., 2020). With larger models and increased compute, capabilities such as in-context learning and chain-ofthought reasoning emerge (Wei et al., 2022b). The expanded capacity allows these models to absorb more knowledge from pre-training data, improving their usefulness as knowledge sources. However, it remains uncertain if scaling boosts the proportion of pre-training knowledge applicable to downstream tasks. Ideally, we like to see a narrowing gap in pre-training knowledge alongside superior knowledge utilization.\nTo investigate this, we evaluate XTRAEVAL on increasing sizes of OPT and LLaMa models. Specifically, at each scale, we first extract the model\u2019s parametric knowledge and then create the downstream task based on it using the same procedure as described in Section 2. Figure 1 reports the results of this experiment.\nFirst, we confirm that a greater fraction of knowledge triples in D can be identified in larger models,\n125M 350M 1.3B 2.7B 7B 13B\nModel Size (log scale)\n0\n0.25\n0.50\n0.75\n1\nF ra\nct io\nn of\nE nc\nyc lo\np ed\nic F\nac ts\nIdeal gap\nModel Family\nOPT LLaMa\nGap Type\nGap 1 Gap 2\n125M 350M 1.3B 2.7B 7B 13B\nModel Size (log scale)\nIdeal gap\nModel Family\nOPT LLaMa\nGap Type\nGap 1 Gap 2\n125M 350M 1.3B 2.7B 7B 13B\nModel Size (log scale)\n0\n0.25\n0.50\n0.75\n1\nF ra\nct io\nn of\nE nc\nyc lo\np ed\nic F\nac ts\nIdeal gap\nModel Family\nOPT LLaMa\nGap Type\nGap 1 Gap 2\nFigure 7: Gaps in parametric knowledge Knowledge gaps directly compute across different model sizes. Specifically, we use 1 \u2212 (Accuracy on D\u03b8) for Gap 1 and (Accuracy on D\u03b8)\u00d7(1\u2212downstream accuracy) for Gap 2\u2020.\nsuggesting they acquire more knowledge from pretraining data. Secondly, we find that the gap between identifiable and usable knowledge persists in larger models, and their ability to apply knowledge in downstream tasks does not improve with scaling. Figure 7 illustrates these gaps directly, demonstrating that while Gap 1 decreases in larger models, Gap 2 remains relatively unchanged.\nThe results suggest that while PLMs, even at small scales, pose considerable knowledge, extracting an equivalent amount of usable knowledge necessitates much larger models. For instance, OPT-125M accurately predicts 34% of encyclopedic facts, but only OPT-13B (approximately 100\u00d7 larger) can reliably apply the same volume in downstream tasks. Enhanced pre-training routines, including the use of more data or higher quality data, can bolster knowledge acquisition, as is clearly demonstrated by LLaMa models. Notably, LLaMa-7B significantly outperforms OPT-13B. While LLaMa models possess a greater amount of usable knowledge due to superior initial knowledge, a gap in knowledge utilization remains discernible (Figure 7)."
        },
        {
            "heading": "6 Discussion",
            "text": "Lately, pre-trained language models with chatbot interfaces have increasingly been served as knowledge bases (Ouyang et al., 2022). These chatbots typically employ the model\u2019s parametric\n\u2020We conducted the experiments for OPT-6.7B multiple times, and observed a dip in performance in all runs. We suspect that this consistent decline may be attributed to issues that arose during the pre-training phase.\nknowledge to respond to queries and offer information. Our study examines the dependability of this knowledge and its impact on downstream task performance. We discover that, regardless of inductive biases, PLMs face difficulty utilizing their full knowledge in downstream tasks (Section 3). This unreliability of parametric knowledge could constrain the concept of \u201cPLMs as differentiable knowledge bases.\u201d\nAdditionally, our findings show that the utilization gap persists even with scaling (Section 5). Notably, while models at each scale capture more knowledge from pre-training data, obtaining the same amount of usable knowledge requires significantly larger models. This exposes a potential constraint in the recent trend of adopting mid-sized PLMs (Li et al., 2023).\nLastly, we discover that knowledge utilization depends on the peculiarities of finetuning data for downstream tasks. Specifically, as seen in Section 4, PLMs struggle to apply their knowledge to relation types not encountered during finetuning, even if they accurately predicted such facts in step 1. This generalization gap could highlight challenges within the recent SFT-RLHF paradigm (Ouyang et al., 2022). For instance, the model may only adhere to instructions and excel at tasks resembling the finetuning data. Consequently, it might be necessary to meticulously craft finetuning data to activate and utilize all aspects of parametric knowledge in downstream tasks. However, it requires elaborate studies to establish the systematic issues in knowledge application beyond encyclopedic knowledge like procedural and task knowledge."
        },
        {
            "heading": "7 Related Work",
            "text": "Parametric Knowledge Petroni et al. (2019) constructed a probing dataset to measure the factual knowledge present in PLMs. They showed that many encyclopedic facts can be extracted without further training of the model and proposed PLMs as a new type of knowledge base, which can be trained on the unstructured text and queried using natural language. Follow-up work improves the methods for probing and extracting world knowledge from PLMs (Jiang et al., 2020; Shin et al., 2020; Qin and Eisner, 2021; Newman et al., 2022). Apart from encyclopedic facts, studies have explored PLMs\u2019 parametric knowledge in other areas, such as linguistic structures (Tenney et al., 2019b; Blevins et al., 2022), and commonsense (Zhou et al., 2020;\nLiu et al., 2022a). Recently, the emergent abilities of LLMs have shown that they acquire skills like coding (Chen et al., 2021), reasoning (Chowdhery et al., 2022), and in-context learning (Brown et al., 2020), in addition to the previously mentioned knowledge.\nUsing the Parametric Knowledge Roberts et al. (2020) finetune a pre-trained T5 model for question answering in a closed-book setting and showed that it can perform on par or better than models that use explicit knowledge bases. Wang et al. (2021) made a similar observation for the BART model. More recently, PLMs are being used to generate facts and documents for knowledge-intensive tasks (Li et al., 2022; Liu et al., 2022b; Yu et al., 2023). In this paradigm, in order to answer factual questions, instead of retrieving relevant documents, the model has to first generate the facts and then answer the question with those facts as context. This paradigm shows that the models may not be able to use their parametric knowledge on their own and need explicit grounding to be able to use it. Furthermore, there is a plethora of work that investigates whether the model employs its linguistic knowledge when solving downstream language understanding tasks. McCoy et al. (2019) shows that RoBERTa does not use its linguistic knowledge for solving NLI. Instead, it relies on shallow heuristics. Lovering et al. (2021)\u2019s observation aligns with this finding and shows the training data used for the downstream task needs to have enough evidence to trigger the model\u2019s linguistic knowledge. In our work, we use a more general notation of parametric knowledge and investigate utilization in cases where sufficient evidence is present in the finetuning data."
        },
        {
            "heading": "8 Conclusion",
            "text": "In this study, we presented EXTRACT, TRAIN, AND EVALUATE (XTRAEVAL ), a framework designed to assess the parametric knowledge of pretrained language models. Employing XTRAEVAL we identified a previously unnoticed gap in what models know and how much of it they can actually use. Our findings reveal that this gap exists not only in smaller models but also persists in larger ones. Additionally, we demonstrate that a distributional shift in finetuning data can result in even larger gaps between the model\u2019s knowledge and its practical application in downstream tasks."
        },
        {
            "heading": "Limitations",
            "text": "Although XTRAEVAL is agnostic to the specific type of parametric knowledge, our work primarily focuses on encyclopedic facts as one type of world knowledge that PLMs can acquire. It is plausible that similar results would hold for other knowledge types, however, further work is needed for a precise investigation.\nWhile there are various downstream tasks that could be evaluated, we primarily focus on document retrieval as it allows us to systematically demonstrate the key issue of knowledge application that we aim to highlight. We also acknowledge that our study was limited to a few model families and parameter scales due to compute constraints. However, our evaluation protocol is model agnostic, enabling future work to explore this phenomenon on other tasks and with different models."
        },
        {
            "heading": "A Training Details",
            "text": ""
        },
        {
            "heading": "A.1 Knowledge Extraction",
            "text": "We adopt the same procedure as outlined by Qin and Eisner (2021) for extracting knowledge facts from a frozen PLM. Specifically, we utilize softprompts instead of discrete prompts. We insert three soft prompts before and after the head entity and allocate distinct soft-prompts for each relation type. We then train them using the training set provided by Qin and Eisner (2021) and employ a validation set to select the best checkpoint. The hyperparameters used in this stage, borrowed from Qin and Eisner (2021), are summarized in Table A.1."
        },
        {
            "heading": "A.2 Finetuning",
            "text": "For fine-tuning the models, we follow a straightforward procedure, training the models in a cross-encoder setup (Figure A.1). The hyperparameters used for fine-tuning are listed in Table A.2. In initial experiments, we tried lr \u2208 1\u00d7 10\u22125, 3\u00d7 10\u22125, 5\u00d7 10\u22125, but we did not find any significant difference between them for all models. Thus, we opted to use the same learning rate for all models."
        },
        {
            "heading": "A.3 Dataset Details",
            "text": "We employ the LAMA dataset (Petroni et al., 2019) as our diagnostic set, consisting of 34,039 facts. The training and validation sets for soft-prompt training, provided by Qin and Eisner (2021), contain 29,029 and 7,255 triples, respectively. The size of the downstream dataset varies from one model instance to another, as we construct the downstream task based on its extracted knowledge. The size of such datasets can be easily calculated by multiplying the number of facts in D by the knowledge\nextraction accuracy obtained by the model."
        },
        {
            "heading": "A.4 Scaling Experiment Details",
            "text": "We adhere to the same procedure as other models in the scaling experiments of OPT and LLaMa. For larger models, we only increase the batch size following Iyer et al. (2023) to ensure better finetuning stability. Due to the extensive computational and cost requirements of fully fine-tuning the 7B and 13B models, we limit the number of seeds for these variants. Specifically, we trained four seeds for LLaMa-7B, two for OPT-13B, and one for LLaMa-13B."
        },
        {
            "heading": "A.5 Computational Resources",
            "text": "For the experiments in this study, we exclusively use NVIDIA V100-32GB GPUs. Models with \u2264 350M parameters were trained on a single GPU. For knowledge extraction, we utilized parallel training implemented by DeepSpeed for larger parameter sizes. Table A.4 displays the number of GPUs used and the approximate duration of a single training run."
        },
        {
            "heading": "A.6 Reproducibility",
            "text": "For the experiments in this study, we exclusively use NVIDIA V100-32GB GPUs. Models with \u2264 350M parameters were trained on a single GPU. For knowledge extraction, we utilized parallel train-\ning implemented by DeepSpeed for larger parameter sizes. Table A.4 displays the number of GPUs used and the approximate duration of a single training run."
        },
        {
            "heading": "B Full Results",
            "text": ""
        },
        {
            "heading": "B.1 Detailed IID Results",
            "text": "We present the comprehensive results of our experiments from Section 3 in Tables B.5 to B.8."
        },
        {
            "heading": "B.2 Detailed OOD Results",
            "text": "Table B.9 provides a detailed account of the experiment results in Section 4."
        },
        {
            "heading": "B.3 Detailed Scaling Results",
            "text": "The detailed results of Section 5 are presented in Tables B.10 and B.11."
        }
    ],
    "title": "Measuring the Knowledge Acquisition-Utilization Gap in Pre-trained Language Models",
    "year": 2023
}