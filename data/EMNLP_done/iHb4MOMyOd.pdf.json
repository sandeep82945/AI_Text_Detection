{
    "abstractText": "Pre-trained sentence representations are crucial for identifying significant sentences in unsupervised document extractive summarization. However, the traditional two-step paradigm of pre-training and sentence-ranking, creates a gap due to differing optimization objectives. To address this issue, we argue that utilizing pre-trained embeddings derived from a process specifically designed to optimize cohensive and distinctive sentence representations helps rank significant sentences. To do so, we propose a novel graph pre-training autoencoder to obtain sentence embeddings by explicitly modelling intra-sentential distinctive features and inter-sentential cohesive features through sentence-word bipartite graphs. These pre-trained sentence representations are then utilized in a graph-based ranking algorithm for unsupervised summarization. Our method produces predominant performance for unsupervised summarization frameworks by providing summary-worthy sentence representations. It surpasses heavy BERTor RoBERTa-based sentence representations in downstream tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Qianren Mao"
        },
        {
            "affiliations": [],
            "name": "Shaobo Zhao"
        },
        {
            "affiliations": [],
            "name": "Jiarui Li"
        },
        {
            "affiliations": [],
            "name": "Xiaolei Gu"
        },
        {
            "affiliations": [],
            "name": "Shizhu He"
        },
        {
            "affiliations": [],
            "name": "Bo Li"
        },
        {
            "affiliations": [],
            "name": "Jianxin Li"
        }
    ],
    "id": "SP:5d820c46648f9f9a633e99f588dd0d715d2f809d",
    "references": [
        {
            "authors": [
                "Joan Bruna",
                "Wojciech Zaremba",
                "Arthur Szlam",
                "Yann LeCun."
            ],
            "title": "Spectral networks and locally connected networks on graphs",
            "venue": "2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference",
            "year": 2014
        },
        {
            "authors": [
                "Ming Chen",
                "Zhewei Wei",
                "Zengfeng Huang",
                "Bolin Ding",
                "Yaliang Li."
            ],
            "title": "Simple and deep graph convolutional networks",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML, volume 119 of Proceedings of Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT, pages 4171\u20134186. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Yue Dong",
                "Andrei Romascanu",
                "Jackie Chi Kit Cheung."
            ],
            "title": "Discourse-aware unsupervised summarization for long scientific documents",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "G\u00fcnes Erkan",
                "Dragomir R. Radev."
            ],
            "title": "Lexrank: Graph-based lexical centrality as salience in text summarization",
            "venue": "J. Artif. Intell. Res., 22:457\u2013479.",
            "year": 2004
        },
        {
            "authors": [
                "Alexander R. Fabbri",
                "Irene Li",
                "Tianwei She",
                "Suyi Li",
                "Dragomir R. Radev."
            ],
            "title": "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model",
            "venue": "Proceedings of the 57th",
            "year": 2019
        },
        {
            "authors": [
                "Max Grusky",
                "Mor Naaman",
                "Yoav Artzi"
            ],
            "title": "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
            "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2018
        },
        {
            "authors": [
                "Karl Moritz Hermann",
                "Tom\u00e1s Kocisk\u00fd",
                "Edward Grefenstette",
                "Lasse Espeholt",
                "Will Kay",
                "Mustafa Suleyman",
                "Phil Blunsom."
            ],
            "title": "Teaching machines to read and comprehend",
            "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neu-",
            "year": 2015
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long short-term memory",
            "venue": "Neural Comput., 9(8):1735\u2013 1780.",
            "year": 1997
        },
        {
            "authors": [
                "Ruipeng Jia",
                "Yanan Cao",
                "Hengzhu Tang",
                "Fang Fang",
                "Cong Cao",
                "Shi Wang."
            ],
            "title": "Neural extractive summarization with hierarchical attentive heterogeneous graph network",
            "venue": "EMNLP, pages 3622\u20133631. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling."
            ],
            "title": "Variational graph auto-encoders",
            "venue": "CoRR, abs/1611.07308.",
            "year": 2016
        },
        {
            "authors": [
                "Ryan Kiros",
                "Yukun Zhu",
                "Ruslan Salakhutdinov",
                "Richard S. Zemel",
                "Raquel Urtasun",
                "Antonio Torralba",
                "Sanja Fidler."
            ],
            "title": "Skip-thought vectors",
            "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Pro-",
            "year": 2015
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Yoshua Bengio",
                "Patrick Haffner."
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proc. IEEE, 86(11):2278\u2013 2324.",
            "year": 1998
        },
        {
            "authors": [
                "Wei Li",
                "Xinyan Xiao",
                "Jiachen Liu",
                "Hua Wu",
                "Haifeng Wang",
                "Junping Du."
            ],
            "title": "Leveraging graph to improve abstractive multi-document summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL, pages",
            "year": 2020
        },
        {
            "authors": [
                "Xinnian Liang",
                "Shuangzhi Wu",
                "Mu Li",
                "Zhoujun Li."
            ],
            "title": "Improving unsupervised extractive summarization with facet-aware modeling",
            "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP, volume ACL/IJCNLP 2021 of Find-",
            "year": 2021
        },
        {
            "authors": [
                "Jingzhou Liu",
                "Dominic J.D. Hughes",
                "Yiming Yang."
            ],
            "title": "Unsupervised extractive text summarization with distance-augmented sentence graphs",
            "venue": "SIGIR \u201921: The 44th International ACM SIGIR Conference",
            "year": 2021
        },
        {
            "authors": [
                "Yang Liu",
                "Mirella Lapata."
            ],
            "title": "Text summarization with pretrained encoders",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Qianren Mao",
                "Jianxin Li",
                "JiaZheng Wang",
                "Xi Li",
                "Peng Hao",
                "Lihong Wang",
                "Zheng Wang."
            ],
            "title": "Explicitly modeling importance and coherence for timeline summarization",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP,",
            "year": 2022
        },
        {
            "authors": [
                "Rada Mihalcea",
                "Paul Tarau."
            ],
            "title": "Textrank: Bringing order into text",
            "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 404\u2013411. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Feifei Zhai",
                "Bowen Zhou."
            ],
            "title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
            "venue": "AAAI, pages 3075\u20133081. AAAI Press.",
            "year": 2017
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B. Cohen",
                "Mirella Lapata."
            ],
            "title": "Ranking sentences for extractive summarization with reinforcement learning",
            "venue": "NAACL-HLT, Volume 1 (Long Papers), pages 1747\u20131759. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Ani Nenkova",
                "Kathleen R. McKeown."
            ],
            "title": "Automatic summarization",
            "venue": "Found. Trends Inf. Retr., 5(23):103\u2013233.",
            "year": 2011
        },
        {
            "authors": [
                "Evan Sandhaus."
            ],
            "title": "The new york times annotated corpus",
            "venue": "Linguistic Data Consortium, Philadelphia, 6(12):e26752.",
            "year": 2008
        },
        {
            "authors": [
                "Abigail See",
                "Peter J. Liu",
                "Christopher D. Manning."
            ],
            "title": "Get to the point: Summarization with pointer-generator networks",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL, Volume 1: Long Papers,",
            "year": 2017
        },
        {
            "authors": [
                "Danqing Wang",
                "Pengfei Liu",
                "Yining Zheng",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Heterogeneous graph neural networks for extractive document summarization",
            "venue": "ACL, pages 6209\u20136219. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Hong Wang",
                "Xin Wang",
                "Wenhan Xiong",
                "Mo Yu",
                "Xiaoxiao Guo",
                "Shiyu Chang",
                "William Yang Wang"
            ],
            "title": "Self-supervised learning for contextualized",
            "year": 2019
        },
        {
            "authors": [
                "Wen Xiao",
                "Giuseppe Carenini."
            ],
            "title": "Extractive summarization of long documents by combining global and local context",
            "venue": "EMNLP-IJCNLP, pages 3009\u20133019. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Jiacheng Xu",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu."
            ],
            "title": "Discourse-aware neural extractive model for text summarization",
            "venue": "CoRR, abs/1910.14142.",
            "year": 2019
        },
        {
            "authors": [
                "Keyulu Xu",
                "Chengtao Li",
                "Yonglong Tian",
                "Tomohiro Sonobe",
                "Ken-ichi Kawarabayashi",
                "Stefanie Jegelka."
            ],
            "title": "Representation learning on graphs with jumping knowledge networks",
            "venue": "Proceedings of the 35th International Conference on Machine",
            "year": 2018
        },
        {
            "authors": [
                "Shusheng Xu",
                "Xingxing Zhang",
                "Yi Wu",
                "Furu Wei",
                "Ming Zhou."
            ],
            "title": "Unsupervised extractive summarization by pre-training hierarchical transformers",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 Novem-",
            "year": 2020
        },
        {
            "authors": [
                "Michihiro Yasunaga",
                "Rui Zhang",
                "Kshitijh Meelu",
                "Ayush Pareek",
                "Krishnan Srinivasan",
                "Dragomir R. Radev."
            ],
            "title": "Graph-based neural multi-document summarization",
            "venue": "CoNLL, pages 452\u2013462. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Wenpeng Yin",
                "Yulong Pei."
            ],
            "title": "Optimizing sentence modeling and selection for document summarization",
            "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI, pages 1383\u20131389. AAAI Press.",
            "year": 2015
        },
        {
            "authors": [
                "Hao Zheng",
                "Mirella Lapata."
            ],
            "title": "Sentence centrality revisited for unsupervised summarization",
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL, pages 6236\u2013 6247. ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Ming Zhong",
                "Pengfei Liu",
                "Danqing Wang",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Searching for effective neural extractive summarization: What works and what\u2019s next",
            "venue": "ACL , Volume 1: Long Papers, pages 1049\u20131058. Association for Computational Linguis-",
            "year": 2019
        },
        {
            "authors": [
                "Qingyu Zhou",
                "Nan Yang",
                "Furu Wei",
                "Shaohan Huang",
                "Ming Zhou",
                "Tiejun Zhao."
            ],
            "title": "Neural document summarization by jointly learning to score and select sentences",
            "venue": "ACL , Volume 1: Long Papers, pages 654\u2013663. Association for Computational Lin-",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Unsupervised document summarization involves generating a shorter version of a document while preserving its essential content (Nenkova and McKeown, 2011). It typically involves two steps: pretraining to learn sentence representations and sentence ranking using sentence embeddings to select the most relevant sentences within a document.\nMost research focuses on graph-based sentence ranking methods, such as TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004), to identify the significant sentence by utilizing topological relations. Continual improvements have been demonstrated by several attempts (Narayan et al., 2018; Zhou et al., 2018; Wang et al., 2019; Xiao and Carenini, 2019; Wang\n\u2217 Jianxin Li is the corresponding author.\net al., 2020), modelling graph-based ranking methods through a global view of the document.\nFor unsupervised document summarization, learning semantic sentence embeddings is crucial, alongside the sentence ranking paradigm. Textual pre-training models like skip-thought model (Kiros et al., 2015), TF-IDF, and BERT (Devlin et al., 2019) generate sentential embeddings, enabling extractive systems to produce summaries that capture the document\u2019s central meaning (Yasunaga et al., 2017; Xu et al., 2019; Jia et al., 2020; Wang et al., 2020). By combining sentence representations generated from pre-trained language models, prominent performances have been achieved with graph-based sentence ranking methods (Zheng and Lapata, 2019; Liang et al., 2021; Liu et al., 2021).\nDespite the effectiveness of graph-based ranking methods that incorporate pre-trained sentential embeddings, there are some underexplored issues. Firstly, a significant gap exists between the twostep paradigm of textual pre-training and sentence graph-ranking, as the optimization objectives diverge in these two steps. The pre-trained framework is primarily designed to represent sentences with universal embeddings rather than summaryworthy features. By relying solely on the universal embeddings, the nuanced contextual information of the document may be overlooked, resulting in sub-\noptimal summaries. Secondly, the existing graph formulation (e.g., GCNs (Bruna et al., 2014)) only encodes distinctive sentences but not necessarily cohensive ones, which may limit the extraction of summary-worthy sentences.\nIn summarization, cohensive sentences reveal how much the summary represents a document, and distinctive sentences involve how much complementary information should be included in a summary. To exemplify how these sentence features come from words, we analyze a sentenceword bipartite graph as depicted in Figure 1.\n\u2022 The connections Sa\u2212w1, Sb\u2212w2, Sc\u2212w4 capture intra-sentential information, where the unique word nodes w1 =Bejing,w2 = fan,w4 = shocked contribute distinctive features to their respective sentence nodes Sa, Sb, Sc.\n\u2022 The connections Sa\u2212w0, Sb\u2212w0, Sb\u2212w3, Sc\u2212w3 capture inter-sentential information, where the shared word nodes w0 = Argentina,w3 =Messi contains cohensive features for their connected sentence nodes Sa, Sb, Sc.\nClearly, a sentence\u2019s unique features come from its individual word nodes, while its cohensive features come from shared word nodes with other sentences. Based on this observation, we argue that optimizing cohensive and distinctive sentence representations during pre-training is ultimately beneficial for ranking significant sentences in downstream extractive summarization. To achieve this, we propose a novel graph pre-training paradigm using a sentence-word bipartite graph with graph convolutional auto-encoder (termed as Bi-GAE1) to learn sentential representations.\nIn detail, we pre-train the bipartite graph by predicting the word-sentence edge centrality score in self-supervision. Intuitively, more unique nodes imply smaller edge weights, as they are not shared with other nodes. Conversely, when there are more shared nodes, their edge weights tend to be greater. We present a novel method for bipartite graph encoding, involving the concatenation of an inter-sentential GCNinter and an intra-sentential GCNintra. These two GCNs allocate two encoding channels for aggregating inter-sentential cohesive features and intra-sentential distinctive features during pre-training. Ultimately, the pre-trained sen-\n1Code and data available at: https://github.com/ OpenSUM/BiGAE.\ntence node representations are utilized for downstream extractive summarization.\nOur pre-trained sentence representations obtain superior performance in both single document summarization on the CNN/DailyMail dataset (Hermann et al., 2015) and multiple document summarization on the Multi-News dataset (Sandhaus, 2008) within salient extractive summarization frameworks. i) To our knowledge, we are the first to introduce the bipartite word-sentence graph pretraining method and pioneer bipartite graph pretrained sentence representations in unsupervised extractive summarization. ii) Our pre-trained sentence representation excels in downstream tasks using the same summarization backbones, surpassing heavy BERT- or RoBERTa-based representations and highlighting its superior performance."
        },
        {
            "heading": "2 Background & Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Sentence Ranking Summarization",
            "text": "Traditional extractive summarization methods are mostly unsupervised (Yin and Pei, 2015; Nallapati et al., 2017; Zheng and Lapata, 2019; Zhong et al., 2019; Mao et al., 2022). Among them, graphbased sentential ranking methods are widely used. Two popular algorithms for single-document summarization are unsupervised LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004), estimating the centrality score of each sentence node among the textual context nodes.\nIn contrast to LexRank and TextRank constructing an undirected sentence graph, the model of PacSum (Zheng and Lapata, 2019) builds a directed graph. Its sentence centrality is computed by aggregating its incoming and outgoing edge weights:\nCentrality(si)= \u03bb1 \u2211 j<i ei, j + \u03bb2 \u2211 j>i ei, j, (1)\nwhere hyper-parameters \u03bb1, \u03bb2 are different weights for forwardand backward-looking directed edges and \u03bb1 + \u03bb2 = 1. ei, j is the weights of the edges ei, j \u2208 E and is computed using word co-occurrence statistics, such as the similarity score. Building upon the achievements of PacSum(Zheng and Lapata, 2019), recent models such as FAR (Liang et al., 2021) and DASG (Liu et al., 2021) have aimed to improve extractive summarization by integrating centrality algorithms. These models primarily focus on seeking central sentences based on semantic facets (Liang et al., 2021) or sentence positions (Liu et al., 2021)."
        },
        {
            "heading": "2.2 Sentential Pre-training",
            "text": "PLM\u2019s pre-training, such as BERT and GPT, is crucial for identifying meaningful sentences in downstream summarization tasks. The previously mentioned graph-based summarization methods, such as PacSum(Zheng and Lapata, 2019), FAR(Liang et al., 2021), and DASG (Liu et al., 2021) utilize pre-trained BERT representations for sentence ranking. STAS (Xu et al., 2020) takes a different approach by pre-training a Transformer-based LM to estimate sentence importance. However, STAS is not plug-and-play and requires a separate pretraining model for each downstream task.\nDespite the success of the aforementioned unsupervised extractive summarization methods, it still maintains a gap between the PLMs\u2019 pre-training and the downstream sentence ranking methods. Additionally, low-quality representations can result in incomplete or less informative summaries, negatively affecting their quality. Pre-training models typically produce generic semantic representations instead of generating summary-worthy representations, which can result in suboptimal performance in unsupervised summarization tasks."
        },
        {
            "heading": "3 Methodology",
            "text": "In what follows, we describe our pre-training model Bi-GAE (as shorthand for Bipartite Graph Pre-training with Graph Convolutional AutoEncoders ) used for unsupervised extractive summarization. We will introduce bipartite graph encoding and the pre-training procedure using our Bi-GAE. Ultimately, we will utilize the pre-trained sentence representations for the downstream unsupervised summarization."
        },
        {
            "heading": "3.1 Document as a Bipartite Graph",
            "text": "Formally, We denote the constructed bipartite wordsentence graph G = {V,A,E,X}, where V = Vw \u222a Vs. Here, Vw denotes |Vw| = n unique words of the document and Vs corresponds to the |Vs| = m sentences in the document. A ={ e11, ..., ei, j, ..., enm } defines the adjacency relationships among nodes, and ei, j \u2208 {0, 1}n\u00d7m indicates the edge weight from source node i to target node j. X \u2208 R(n+m)\u00d7d, is termed as a matrix containing the representation of all nodes. The node representations will be iteratively updated by aggregating summary-worthy features (intra-sentential and inter-sentential messages) between word and sentence nodes via the bipartite graph autoencoder."
        },
        {
            "heading": "3.2 Bipartite Graph Pre-training",
            "text": "We reform the original VGAE (Kipf and Welling, 2016) pre-training framework by optimizing edge weight prediction in bipartite graphs. The pretraining optimizer learns to fit the matrices between the input weighted adjacency matrix and the reconstructed adjacency matrix in a typical way of self-supervised learning. By integrating an intrasentential GCNintra and an inter-sentential GCNinter in the VGAE (Kipf and Welling, 2016) selfsupervised framework, our pre-training method enables effective aggregation of intra-sentential and inter-sentential information, allowing for the representation of high-level summary-worthy features in the bipartite graph pre-training. Bipartite Graph Initializers. Let Xw \u2208 R(n)\u00d7dw and Xs \u2208 R(m)\u00d7ds represent the input feature matrix of the word and sentence nodes respectively, where dw and ds are the dimension of word embedding vector and sentence representation vector respectively. We first use convolutional neural networks (CNN) (LeCun et al., 1998) with different kernel sizes to capture the local n-gram feature for each sentence S Ci and then use the bidirectional long short-term memory (BiLSTM) (Hochreiter and Schmidhuber, 1997) layer to get the sentencelevel feature S Li . The concatenation of the CNN local feature and the BiLSTM global feature is used as the sentence node initialized feature XS i = [S Ci ; S L i ]. The initialized representations are used as inputs to the graph autoencoder module. Bipartite Graph Encoder. To model summaryworthy representations, we encode the bipartite graph by a concatenation of an intra-sentential GCNintra and an inter-sentential GCNinter, in which two GCNs assign two encoding channels for aggregating intra-sentential distinctive features and inter-sentential cohesive features. The GCNintra (H0 = X,Aweight,\u0398), can be seen as a form of message passing to aggregate intra-sentential distinctive features. The first GCNintra layer generates a lower-dimensional feature matrix. Its node-wise formulation is given by:\nhintraj = \u0398 \u22a4 \u2211\ni\u2208N(u)\u222a{ j} 1\u221a d\u0303id\u0303 j ei, jhintrai , (2)\nwhere ei, j \u2208 Aweight denotes the edge weight from source node i to target node j. Here we use the betweenness centrality 2 as the edge\n2https://networkx.org/documentation/latest/ reference/algorithms/centrality.html\nweights. The betweenness centrality of an edge is the sum of fractions of the shortest paths passing through it. The first GCNintra layer makes features of neighbour nodes with fewer association relationships aggregated and enlarged and outputs a lower-dimensional feature matrix H. Then, the second GCNintra layer generates \u00b5intra = GCN\u00b5 ( Hintra,Aweight ) and log(\u03c3intra)2 =\nGCN\u03c3 ( Hintra,Aweight ) .\nThe GCNinter (H0 = X,Aweight,\u0398) can be seen as a form of message passing to aggregate intersentential cohensive features:\nhinterj = \u0398 \u22a4 \u2211\ni\u2208N(u)\u222a{ j} \u221a d\u0303i\u221a d\u0303 j ei, jhinteri . (3)\nThe graph convolution operator GCNinter will aggregate neighbour node features with more association relationships aggregated and enlarged. Analogously, we can obtain \u00b5inter and log(\u03c3inter)2, which are parameterized by the two-layer GCNinter.\nThen we can generate the latent variable Z as output of bipartite graph encoder by sampling from GCNinter and GCNintra and then concatenating sampled two latent variables Zinter and Zintra:\nq(Zinter ||Zintra) = N\u220f\ni=1\nq(zinteri ) N\u220f\ni=1\nq(zintrai ), (4)\nwhere q(zinteri ) and q(z intra i ) are from two GCNs, satisfying independent distribution conditions. Here,\nq(zinteri ) = N ( zinteri |\u00b5interi , diag((\u03c3interi )2) ) , (5)\nq(zintrai ) = N ( zintrai |\u00b5intrai , diag((\u03c3intrai )2) ) . (6)\nGenerative Decoder. Our generative decoder is given by an inner product between latent variables Z. The output of our decoder is a reconstructed adjacency matrix A\u0302, which is defined as follows:\np(A\u0302|Z) = N\u220f\ni=1 N\u220f j=1 p(Ai, j|ziz j), (7)\nwhere p(Ai, j|ziz j) = \u03c3(z\u22a4i z j), and Ai, j are the elements of A\u0302. \u03c3(\u00b7) is the logistic sigmoid function. Edge Weights Prediction as the Pre-training Objective. We use edge weight reconstruction as the training objective to optimize our pre-trained BiGAE. Specifically, the pre-training optimizer learns to fit the matrices between the input weighted adjacency matrix Aweight and the reconstructed adjacency matrix A\u0302weight.\nL = MSE(p(A\u0302weight|Z),Aweight))\u2212KL(q(Z)||p(Z), (8) The loss function of the bipartite graph pre-training has two parts. The first part is MSE loss which measures how well the pre-training model reconstructs the structure of the bipartite graph. KL works as a regularizer in original VGAE, and p(Z) = N(0, 1) is a Gaussian prior."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "During the graph pre-training, the Bi-GAE is optimized by the prediction of edge weights in a selfsupervised manner. Subsequently, we utilize the\npre-trained sentence representations of Bi-GAE to replace those used in state-of-the-art unsupervised summarization backbones. This allows us to assess the effectiveness of the pre-trained sentence representation in downstream tasks."
        },
        {
            "heading": "4.1 Downstream Tasks and Datasets",
            "text": "We evaluate our approach on two summarization datasets: the CNN/DailyMail (Hermann et al., 2015) dataset and the Multi-news (Fabbri et al., 2019) dataset. The CNN/DailyMail comprises articles from CNN and Daily Mail news websites, summarized by their associated highlights. We follow the standard splits and preprocessing steps used in baselines (See et al., 2017; Liu and Lapata, 2019; Zheng and Lapata, 2019; Xu et al., 2020; Liang et al., 2021), and the resulting dataset contains 287,226 articles for training, 13,368 for validation, and 11,490 for the test. The Multinews is a large-scale multi-document summarization (MDS) dataset and comes from a diverse set of news sources. It contains 44,972 articles for training, 5,622 for validation, and 5,622 for testing. Referring to prior works (Fabbri et al., 2019; Liu et al., 2021), we create sentence discourse graphs for each document and cluster them, with each cluster yielding a summary sentence."
        },
        {
            "heading": "4.2 Pre-training Datasets",
            "text": "We construct a bipartite graph with word and sentence nodes, determining edge weights through graph centrality. The centrality-based weights denoted as Aweight serve as inputs for the Bi-GAE model. During pre-training, we use MSE loss to measure the average squared difference between the predicted edge values A\u0302weight and the true values input Aweight, as it indicates more minor errors between the predicted and true values. We conveniently utilize training datasets without their summarization labels as the corpus to pre-train sentence representations by our Bi-GAE."
        },
        {
            "heading": "4.3 Backbones of Summarization Approaches",
            "text": "There are several simple unsupervised summarization extraction frameworks, including TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004), as well as more robust graph-based ranking methods such as PacSum (Zheng and Lapata, 2019), FAR (Liang et al., 2021), DASG (Liu et al., 2021). Graph-based ranking methods take sentence representations as input, using the algorithm of graph-based sentence\ncentrality ranking for sentence selection. We now introduce extractive summarization backbones.\n\u2022 TextRank and LexRank utilize PageRank to calculate node centrality based on a Markov chain model recursively.\n\u2022 PacSum (Zheng and Lapata, 2019) constructs graphs with directed edges. The rationale behind this approach is that the centrality of two nodes is influenced by their relative position in the document, as illustrated by Equation 15.\n\u2022 DASG (Liu et al., 2021) selects sentences for summarization based on the similarities and relative distances among neighbouring sentences. It incorporates a graph edge weighting scheme to Equation 15, using a coefficient that maps a pair of sentence indices to a value calculated by their relative distance.\n\u2022 FAR (Liang et al., 2021) modifies Equation 15 by applying a facet-aware centrality-based ranking model to filter out insignificant sentences. FAR also incorporates a similarity constraint between candidate summary representation and document representation to ensure the selected sentences are semantically related to the entire text, thereby facilitating summarization.\nThe main distinction among the extractive frameworks mentioned above lies in their centrality algorithms. A comprehensive comparison of these algorithms can be found in Appendix 8."
        },
        {
            "heading": "4.4 Compared Sentence Embeddings",
            "text": "We evaluate three sentence representations for computing sentence centrality. The first compared sentence embedding employs a TF-IDF-based approach, where each vector dimension is calculated based on the term frequency (TF) of the word in the sentence and the inverse document frequency (IDF) of the word across the entire corpus of documents. The second representation is based on the Skip-thought model (Kiros et al., 2015), an encoder-decoder model trained on surrounding sentences using a sentence-level distributional hypothesis (Kiros et al., 2015). We utilize the publicly available skip-thought model3 to obtain sentence representations. The third approach relies on BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019) to generate sentence embeddings.\n3https://github.com/ryankiros/skip-thoughts"
        },
        {
            "heading": "4.5 Implementation Details and Metrics",
            "text": "In pre-training the Bi-GAE, we choose the best model and hyper-parameters based on their performance on the validation set. Appendix 8.2 provides detailed information on the hyper-parameters used during the Bi-GAE pre-training procedure. For finetuning the unsupervised extractive summarization frameworks, there are a few hyper-parameters to be tuned for computing centrality scores. The main hyper-parameters for the extractive summarization frameworks are listed in Appendix 8.3. We have kept the remaining hyper-parameters in the backbones of summarization frameworks unchanged."
        },
        {
            "heading": "5 Results and Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Single-Document Experiments",
            "text": "Our results on the CNN/Daily Mail are summarized in Table 1. The Oracle upper bound extracts gold standard summaries by greedily selecting sen-\ntences that optimize the mean of ROUGE-1 and ROUGE-2 scores. The results indicate the following: (i) Our pre-trained sentence representation, which incorporates TextRank and LexRank extractive frameworks, yields prominent improvements in ROUGE-1/2/L performances. (ii) Our model acquires intra-sentential distinctive and intersentential cohensive features of sentences via pretraining on sentence-word bipartite graphs, aiding graph-based ranking for unsupervised summarization. (iii) Our pre-trained sentence representation outperforms all other robust sentence representation methods across all summarization frameworks.\nIn contrast, sentence representations initialized with BERT or RoBERTa perform poorly in TextRank and LexRank frameworks. This could be attributed to the collapse of BERT-derived sentence representations, resulting in high similarity scores for all sentences and thus failing to leverage the potential centrality in TextRank and LexRank. However, our methods surpass BERT and RoBERTa in\nthe FAR and DASG summarization frameworks, showcasing the effectiveness of sentence representations pre-trained by our graph auto-encoders."
        },
        {
            "heading": "5.2 Multi-Document Experiments",
            "text": "Table 2 shows the comparison of Multi-news summarization. Given that all frameworks employing our pre-trained representations outperform the First-3 baseline, our approach effectively mitigates position bias (Dong et al., 2021). This bias often results in incomplete summaries that neglect essential information located in the middle of the document. The results demonstrate two key findings: (i) Our method adeptly captures essential summary-worthy sentences, thereby consolidating the process of sentence clustering and, in turn, improving extractive accuracy. (ii) The embedded, intra-sentential distinctive features and inter-sentential cohensive features are crucial in ranking significant sentences across multiple documents."
        },
        {
            "heading": "5.3 Component-wise Analysis",
            "text": "To comprehend how modelling intra-sentential features and inter-sentential features contribute to sentence-word bipartite graphs, we conducted an ablation study on the CNN/DailyMail dataset. As shown in Table 3, we can observe that the Bi-GAE model equipped solely with GCNinter or solely with GCNintra performs well. When combined with both, Bi-GAE yields the best results across all metrics. This highlights the importance of incorporating intra-sentential and inter-sentential features for effective summarization. Combining the two GCNs leads to complementary effects, enhancing the model\u2019s overall performance. On the contrary, using only GCNinter or GCNintra individually results in poor performance, as it fails to capture either the semantically cohensive or the distinctive\ncontent of the document."
        },
        {
            "heading": "5.4 Effects of Pre-training Datasets",
            "text": "To evaluate the impact of different pre-training datasets, we test summarization frameworks using two types of representations pre-trained on distinct corpora. In Table 4 and Table 5, we can observe pre-training on the Multi-news dataset showed minimal performance degradation or limited changes in CNN/DailyMail summarization, and vice versa \u2014 the similarity between the two news corpora leads to consistent results in downstream tasks."
        },
        {
            "heading": "5.5 Density Estimation of Summarization",
            "text": "There are three measures - density, coverage, and compression - introduced by Grusky et al. (2018) and Fabbri et al. (2019) to assess the extractive nature of an extractive summarization dataset. In this paper, we adopt these measures to evaluate the quality of extracted summaries, as illustrated in\nFigure 3. The coverage (x-axis) measure assesses the degree to which a summary is derived from the original text. The density (y-axis) measures the extent to which a summary can be described as a series of extractions. Compression c, on the other hand, refers to the word ratio between two texts - Text A and Text B. Higher compression pose a challenge as it necessitates capturing the essential\naspects of the reference text with precision. For detailed mathematical definitions of these evaluation measures, please refer to Appendix 8.6.\nWe utilize three measures that quantify the level of text overlap between (i) the Oracle summary and the manual summary (subfigures (a) and (e)), (ii) the summary extracted by the BERT-based DASG and the manual summary (subfigure (b) and (f)), (iii) the summary extracted by our Bi-GAE based DASG and the manual summary (subfigure (c) and (g)), and (iv) the summary extracted by our Bi-GAE based DASG and the Oracle (subfigure (d) and (h)). These measures are plotted using kernel density estimation in Figure 3. Among them, subfigure (a) displays the comparison between the Oracle summary compared to the manual summary, which serves as the upper bound for the density and coverage distributions of extractive compression score in extractive summarization. Subfigure (e) shows this score in the multi-news dataset.\nComparing the extractive summary of our BiGAE based DASG (DASG integrated by the sentence representation of our Bi-GAE) and the extractive Oracle summary in subfigures (a), (b), and (c), we have observed variability in copied word percentages for diverse sentence extraction in CNN/DailyMail. A lower score on the x-axis\nsuggests a greater inclination of the model to extract fragments (novel words) that differ from standard sentences. Our model also outperforms the BERT-based DASG in compression score (0.6522) to compare subfigures (b) and (c). Regarding the yaxis (fragment density) in subfigure (d), our model shows variability in the average length of copied sequences to the Oracle summary, suggesting varying styles of word sequence arrangement. These advantages persist in the multi-news dataset."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we introduce a pre-training process that optimizes summary-worthy representations for extractive summarization. Our approach employs graph pre-training autoencoders to learn intrasentential and inter-sentential features on sentenceword bipartite graphs, resulting in pre-trained embeddings useful for extractive summarization. Our model is easily incorporated into existing unsupervised summarization models and outperforms salient BERT-based and RoBERTa-based summarization methods with predominant ROUGE-1/2/L score gains. Future work involves exploring the potential of our pre-trained sentential representations for other unsupervised extractive summarization tasks and text-mining applications."
        },
        {
            "heading": "7 Limitations",
            "text": "We emphasize the importance of pre-trained sentence representations in learning meaningful representations for summarization. In our approach, we pre-train the sentence-word bipartite graph by predicting the edge betweenness score in a selfsupervised manner. Exploring alternative centrality scores (such as TF-IDF score or current-flow betweenness for edges) as optimization objectives for MSE loss would be a viable option.\nAdditionally, we seek to validate the effectiveness of the sentence representations learned from Bi-GAE in other unsupervised summarization backbones and tasks."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by the National Natural Science Foundation of China (No.U20B2053 and No.62376270)."
        },
        {
            "heading": "8 Appendix",
            "text": ""
        },
        {
            "heading": "8.1 Details about Centrality Algorithms",
            "text": "The key idea of graph-based ranking is to calculate the centrality score of each sentence (or vertex) described in section 2.1. In this section, we give the differences in centrality algorithms among several salient summarization backbone models.\nThe PacSum (Zheng and Lapata, 2019) method enhances the centrality of two nodes in sentence graphs, considering how their relative positions in a document influence their importance.\nCentrality(si)= \u03bb1 \u2211 j<i ei, j + \u03bb2 \u2211 j>i ei, j, (9)\nwhere hyper-parameters \u03bb1, \u03bb2 are different weights for forward and backward-looking directed edges and \u03bb1 + \u03bb2 = 1. ei, j is the normalized similarity score.\nThe FAR (Liang et al., 2021) approach enhances the centrality of two nodes with distance constraints in sentence graphs by considering how their relative positions in a document influence their importance.\nCentrality(si)=\u03bb1 \u2211 j<i Max((ei, j \u2212 \u03f5), 0)\n+ \u03bb2 \u2211 j>i Max((ei, j \u2212 \u03f5), 0), (10)\nwhere \u03f5 = \u03b2 \u00b7 ( max(ei, j) \u2212 min(ei, j) ) . For s1, the threshold \u03f5 can be seen as a diameter, s1 is the centre. \u03b2 is a hyper-parameter to control the scale of diameter.\nThe DASG (Liu et al., 2021) method enhances the centrality of two nodes in sentence graphs by taking into account their relative position and semantic facets within a document.\nCentrality(si)=\u03bb+\u230a j\u2212i m +1 \u230b\u2211 j<i ei, j\n+ \u03bb\u2212\u230a i\u2212 j m +1 \u230b\u2211 j>i ei, j, (11)\nwhere \u03bb+1 , ..., \u03bb + k and \u03bb \u2212 1 , ..., \u03bb \u2212 k are fixed hyperparameters and k is set to be 3 empirically.\n8.2 Hyper-parameters in Bi-GAE pre-training\nWe mainly use PyTorch Geometric, PYG 4 to implement Bi-GAE. More specifically, we limit the\n4https://github.com/pyg-team/pytorch_ geometric\nvocabulary to 50,000 and initialize tokens with 300- dimensional GloVe 840B embeddings5. We filter stop words and punctuations when creating word nodes and truncate the input document to a maximum length of 50 sentences. To eliminate the noisy common words, we remove 10% of the vocabulary with low TF-IDF values over the whole dataset. We initialize sentence nodes with ds = 150. We use a batch size of 8 during pre-training and apply the Adam optimizer with a learning rate of 5e-5 for CNN/DailyMail and 2e-5 for Multi-News. The dropout is 0.1. The pre-training model is trained for 210,000 steps, and the warm-up step is set to 8000. Attempts made to invoke certain model interfaces in PYG have revealed that using JKNET (Xu et al., 2018) and GCNII (Chen et al., 2020) as the encoder backbone in the pre-training process results in performance for downstream tasks that are essentially indistinguishable from those of GCN."
        },
        {
            "heading": "8.3 Hyper-parameters in Summarization",
            "text": "We begin by using Stanford NLP 6 to split sentences and preprocess the dataset. The source text has a maximum sentence length of 512, while the summary is limited to a maximum sentence length of 140. During the tuning process for extractive summarization, we fine-tune the parameters related to the centrality algorithm within a narrow range of [-1.0, 2.0]. Table 6 presents the optimal hyperparameters for each extractive summarization backbones, utilizing our Bi-GAE pre-trained sentence representations. For the CNN/DailyMail dataset, we select the top-3 sentences for the summarization based on the average length of the Oracle humanwritten summaries, whereas, for Multi-New, we choose the top-9 sentences."
        },
        {
            "heading": "8.4 Sentence Similarity Computation",
            "text": "The crucial aspect of the unsupervised graph rank method in downstream tasks lies in the calculation of similarity between two sentences. In this regard, we examine two methods for calculating similarity, both of which draw inspiration from the similarity calculation approach utilized in PacSum(Zheng and Lapata, 2019). The first one can employ a pair-wise dot product to compute an unnormalized similarity matrix E\u0304i j = v\u22a4i v j, and the second one is cosine similarity E\u0304i j = cos(vi, v j). The final normalized\n5https://nlp.stanford.edu/projects/glove/ 6https://github.com/stanfordnlp/CoreNLP\nsimilarity matrix E is defined as: E\u0303i j = E\u0304i j \u2212 [ minE\u0304 + \u03b2(maxE\u0304 \u2212 minE\u0304) ] , (12)\nwhere E\u0303i j is designed to mitigate the influence of absolute values and instead emphasize the relative contributions of different similarity scores. The hyper-parameter \u03b2 \u2208 [0, 1] controls the threshold below which the similarity score of E\u0303i j is set to 0.\nFigure 7 and Figure 8 illustrate the testing results of models using two similarities. Through empirical analysis, we have discovered that the pair-wise dot product yields better performance in most cases on CNN/Dailymail summarization and Multi-news summarization. This finding aligns with the results reported in PacSum(Zheng and Lapata, 2019).\n8.5 Bi-GAE Pre-training Validation We meticulously fine-tune a multitude of parameters in our process. For the pre-training of the CNN/Daily Mail corpus, we find that the optimal learning rate for our model is 5e-5, with a batch size of 8. Similarly, in the pre-training of the Multinews corpus, we find that the optimal learning rate is 2e-5 while maintaining a batch size of 8.\nTo assess the pre-training performances, we conduct accuracy tests of the edge weight prediction on\nthe verification set. As shown in Figure 4, our findings indicate that the optimal prediction accuracy for both corpora typically ranges between 0.60 and 0.65. Based on these observations, we formulated the following hypothesis: when there are more unique nodes, their edge weights should be smaller since they are not shared by other nodes. Conversely, when there are more shared nodes, their edge weights should be greater. The improvement in performance on downstream tasks validates the soundness of our hypothesis."
        },
        {
            "heading": "8.6 Characterizing Summarization Strategies",
            "text": "As shown in Figure 3, each box is a normalized bivariate density plot of extractive fragment coverage (x-axis) and density (y-axis), and the top left corner of each plot shows the median compression ratio c between text A and text B. Fragment Coverage Extractive fragment coverage is the percentage of words in the summary that are from the source article, measuring the extent to which a summary is derivative of a text:\nCOVERAGE(A, B) = 1|B| \u2211\nf\u2208F(A,B) | f | , (13)\nwhere F (A, B) is the set of shared sequences of\ntokens in A and B and is identified as extractive in a greedy manner. For example, a summary (text B) with 10 words that 7 words are the same as its article (text A) and include 3 new words will have COVERAGE(A, B) =0.7. Fragment Density The density measure quantifies the average length of the extractive fragment to which each word in the text belongs.\nDENS ITY(A, B) = 1|B| \u2211\nf\u2208F(A,B) | f |2 . (14)\nFor instance, a summary (text B) might contain many individual words from the article (text A) and therefore have high coverage. For instance, a summary might contain many individual words from the article and therefore have high coverage. For an article (text A) with a 10-word summary (text B) made of two extractive fragments of lengths 3 and 4 would have COVERAGE(A, S) = 0.7 and DENS ITY(A, B) =2.5. Compression Ratio The compression ratio c is defined as the word ratio between the article and summary:\nCOMPRES S ION(A, B) = |A||B| . (15)\nSummarizing with higher compression is challenging as it requires capturing more precisely the critical aspects of the article text.\nAmong our settings about the above metrics, we have expanded the comparison between summary\ntext and article text to include: the comparison between extracted summary and manual summary, the comparison between the extractive Oracle and the manual summary, or the comparison between extracted summary and Oracle summary."
        }
    ],
    "title": "Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders",
    "year": 2023
}