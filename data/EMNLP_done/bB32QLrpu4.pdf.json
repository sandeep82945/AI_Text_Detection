{
    "abstractText": "The automatic Brain CT reports generation can improve the efficiency and accuracy of diagnosing cranial diseases. However, current methods are limited by 1) coarse-grained supervision: the training data in image-text format lacks detailed supervision for recognizing subtle abnormalities, and 2) coupled cross-modal alignment: visual-textual alignment may be inevitably coupled in a coarse-grained manner, resulting in tangled feature representation for report generation. In this paper, we propose a novel Pathological Graph-driven Cross-modal Alignment (PGCA) model for accurate and robust Brain CT report generation. Our approach effectively decouples the cross-modal alignment by constructing a Pathological Graph to learn finegrained visual cues and align them with textual words. This graph comprises heterogeneous nodes representing essential pathological attributes (i.e., tissue and lesion) connected by intraand inter-attribute edges with prior domain knowledge. Through carefully designed graph embedding and updating modules, our model refines the visual features of subtle tissues and lesions and aligns them with textual words using contrastive learning. Extensive experimental results confirm the viability of our method. We believe that our PGCA model holds the potential to greatly enhance the automatic generation of Brain CT reports and ultimately contribute to improved cranial disease diagnosis.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yanzhao Shi"
        },
        {
            "affiliations": [],
            "name": "Junzhong Ji"
        },
        {
            "affiliations": [],
            "name": "Xiaodan Zhang"
        },
        {
            "affiliations": [],
            "name": "Liangqiong Qu"
        },
        {
            "affiliations": [],
            "name": "Ying Liu"
        }
    ],
    "id": "SP:646596b92427004234f35fd2a62ebf3f732f4084",
    "references": [
        {
            "authors": [
                "Peter Anderson",
                "Xiaodong He",
                "Chris Buehler",
                "Damien Teney",
                "Mark Johnson",
                "Stephen Gould",
                "Lei Zhang."
            ],
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "venue": "2018 IEEE Conference on Computer Vision and Pattern",
            "year": 2018
        },
        {
            "authors": [
                "Adrian Brady",
                "Riste\u00e1rd \u00d3 Laoide",
                "Peter McCarthy",
                "Ronan McDermott."
            ],
            "title": "Discrepancy and error in radiology: concepts, causes and consequences",
            "venue": "The Ulster medical journal, 81(1):3.",
            "year": 2012
        },
        {
            "authors": [
                "Zhihong Chen",
                "Yaling Shen",
                "Yan Song",
                "Xiang Wan."
            ],
            "title": "Cross-modal memory networks for radiology report generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Zhihong Chen",
                "Yan Song",
                "Tsung-Hui Chang",
                "Xiang Wan."
            ],
            "title": "Generating radiology reports via memory-driven transformer",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, Novem-",
            "year": 2020
        },
        {
            "authors": [
                "Sasank Chilamkurthy",
                "Rohit Ghosh",
                "Swetha Tanamala",
                "Mustafa Biviji",
                "Norbert G. Campeau",
                "Vasantha Kumar Venugopal",
                "Vidur Mahajan",
                "Pooja Rao",
                "Prashant Warier"
            ],
            "title": "Development and validation of deep learning algorithms for detection",
            "year": 2018
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei."
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami,",
            "year": 2009
        },
        {
            "authors": [
                "Colette M Griffin",
                "Declan T Chard",
                "Geoff JM Parker",
                "Gareth J Barker",
                "Alan J Thompson",
                "David H Miller."
            ],
            "title": "The relationship between lesion and normal appearing brain tissue abnormalities in early relapsing remitting multiple sclerosis",
            "venue": "Journal of",
            "year": 2002
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "Baoyu Jing",
                "Pengtao Xie",
                "Eric P. Xing."
            ],
            "title": "On the automatic generation of medical imaging reports",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Jonathan Krause",
                "Justin Johnson",
                "Ranjay Krishna",
                "Li Fei-Fei."
            ],
            "title": "A hierarchical approach for generating descriptive image paragraphs",
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26,",
            "year": 2017
        },
        {
            "authors": [
                "Alon Lavie",
                "Abhaya Agarwal."
            ],
            "title": "METEOR: an automatic metric for MT evaluation with high levels of correlation with human judgments",
            "venue": "Proceedings of the Second Workshop on Statistical Machine Translation, WMT@ACL 2007, Prague, Czech Re-",
            "year": 2007
        },
        {
            "authors": [
                "Christy Y. Li",
                "Xiaodan Liang",
                "Zhiting Hu",
                "Eric P. Xing."
            ],
            "title": "Knowledge-driven encode, retrieve, paraphrase for medical image report generation",
            "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Ap-",
            "year": 2019
        },
        {
            "authors": [
                "Mingjie Li",
                "Wenjia Cai",
                "Karin Verspoor",
                "Shirui Pan",
                "Xiaodan Liang",
                "Xiaojun Chang."
            ],
            "title": "Crossmodal clinical graph transformer for ophthalmic report generation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Mingjie Li",
                "Bingqian Lin",
                "Zicong Chen",
                "Haokun Lin",
                "Xiaodan Liang",
                "Xiaojun Chang."
            ],
            "title": "Dynamic graph enhanced contrastive learning for chest x-ray report generation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 3334\u20133343.",
            "year": 2023
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Fenglin Liu",
                "Xian Wu",
                "Shen Ge",
                "Wei Fan",
                "Yuexian Zou."
            ],
            "title": "Exploring and distilling posterior and prior knowledge for radiology report generation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Han Qin",
                "Yan Song."
            ],
            "title": "Reinforced cross-modal alignment for radiology report generation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 448\u2013458.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Xiao Song",
                "Xiaodan Zhang",
                "Junzhong Ji",
                "Ying Liu",
                "Pengxu Wei."
            ],
            "title": "Cross-modal contrastive attention model for medical report generation",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Repub-",
            "year": 2022
        },
        {
            "authors": [
                "A\u00e4ron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals."
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748.",
            "year": 2018
        },
        {
            "authors": [
                "Ramakrishna Vedantam",
                "C. Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "Cider: Consensus-based image description evaluation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 4566\u20134575.",
            "year": 2015
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Alexander Toshev",
                "Samy Bengio",
                "Dumitru Erhan."
            ],
            "title": "Show and tell: A neural image caption generator",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 3156\u20133164.",
            "year": 2015
        },
        {
            "authors": [
                "Jun Wang",
                "Abhir Bhalerao",
                "Yulan He."
            ],
            "title": "Crossmodal prototype driven network for radiology report generation",
            "venue": "Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XXXV, pages 563\u2013579.",
            "year": 2022
        },
        {
            "authors": [
                "Xiaosong Wang",
                "Yifan Peng",
                "Le Lu",
                "Zhiyong Lu",
                "Ronald M. Summers."
            ],
            "title": "Tienet: Text-image embedding network for common thorax disease classification and reporting in chest x-rays",
            "venue": "2018 IEEE Conference on Computer Vision and Pattern Recog-",
            "year": 2018
        },
        {
            "authors": [
                "Kelvin Xu",
                "Jimmy Ba",
                "Ryan Kiros",
                "Kyunghyun Cho",
                "Aaron C. Courville",
                "Ruslan Salakhutdinov",
                "Richard S. Zemel",
                "Yoshua Bengio."
            ],
            "title": "Show, attend and tell: Neural image caption generation with visual attention",
            "venue": "Proceedings of the 32nd International",
            "year": 2015
        },
        {
            "authors": [
                "Yuan Xue",
                "Tao Xu",
                "L. Rodney Long",
                "Zhiyun Xue",
                "Sameer K. Antani",
                "George R. Thoma",
                "Xiaolei Huang."
            ],
            "title": "Multimodal recurrent model with attention for automated radiology report generation",
            "venue": "Medical Image Computing and Computer Assisted",
            "year": 2018
        },
        {
            "authors": [
                "An Yan",
                "Zexue He",
                "Xing Lu",
                "Jiang Du",
                "Eric Y. Chang",
                "Amilcare Gentili",
                "Julian J. McAuley",
                "Chun-Nan Hsu."
            ],
            "title": "Weakly supervised contrastive learning for chest x-ray report generation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2021
        },
        {
            "authors": [
                "Shuxin Yang",
                "Xian Wu",
                "Shen Ge",
                "Zhuozhao Zheng",
                "S. Kevin Zhou",
                "Li Xiao."
            ],
            "title": "Radiology report generation with a learned knowledge base and multimodal alignment",
            "venue": "Medical Image Anal., 86:102798.",
            "year": 2023
        },
        {
            "authors": [
                "Sisi Yang",
                "Junzhong Ji",
                "Xiaodan Zhang",
                "Ying Liu",
                "Zheng Wang."
            ],
            "title": "Weakly guided hierarchical encoder-decoder network for brain ct report generation",
            "venue": "IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2021, Houston, TX,",
            "year": 2021
        },
        {
            "authors": [
                "Yixiao Zhang",
                "Xiaosong Wang",
                "Ziyue Xu",
                "Qihang Yu",
                "Alan L. Yuille",
                "Daguang Xu."
            ],
            "title": "When radiology report generation meets knowledge graph",
            "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innova-",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Writing diagnostic reports for Brain CT imaging with multiple scans is widely applied in medicine, to summarize the findings of cranial diseases. Nonetheless, this traditional clinical practice could be time-consuming and error-prone for radiologists due to some subjective factors (e.g. fatigue and distraction) (Brady et al., 2012). A computer-aided\n\u2217Corresponding Authors\nreporting system aims to automatically generate accurate reports, which has the potential to lighten the workload of physicians and economize insufficient clinical resources in populated areas.\nWith the advent of deep neural networks in image captioning (Vinyals et al., 2015; Anderson et al., 2018), medical report generation (MRG) methods are increasingly ramping up (Jing et al., 2018; Wang et al., 2018; Li et al., 2019; Yang et al., 2021; Yan et al., 2021; Chen et al., 2021; Song et al., 2022; Qin and Song, 2022; Yang et al., 2023; Li et al., 2023). Different from image captioning, MRG task focuses on subtle yet crucial medical terminologies, with the report length typically 4-6 times longer than those of nature object captions. This prompts MRG models to refine the dedicated consistency of salient pathological features between visual and textual modalities. To achieve this, recent studies have employed various cross-modal alignment mechanisms (Liu et al., 2021; Wang et al., 2022; Li et al., 2023), leveraging specific knowledge to effectively improve report generation.\nDespite the promising achievements, previous methods are still thwarted by the following two concerns. 1) Coarse-grained supervision: In clinical practice, brain findings are often characterized by some subtle yet vital pathological elements that belong to different attributes (i.e. tissues and lesions)(Griffin et al., 2002), see Figure 1. However, medical images and long reports are always treated as coarse-grained supervisory signals in the mainstream methods. Although few recent studies have explored the learning of fine-grained signals (Liu et al., 2021; Wang et al., 2022) in chest X-ray samples, it is unsuitable for Brain CT which encompasses more sophisticated pathology patterns. Thus, how to leverage the intrinsic fine-grained cranial knowledge to recognize subtle abnormalities remains an open question. 2) Coupled crossmodal alignment: Current cross-modal alignment methods in MRG inevitably tend to couple visualtextual representation during coarse-grained alignment, which is manifested as highly-similar attention maps (Yang et al., 2021) or tangled semantic representations in instance-level contrastive learning (Yan et al., 2021), resulting in inadequate feature learning.\nIn this work, we propose a novel Pathological Graph-driven Cross-modal Alignment model, named PGCA, which introduces a detailed Pathological Graph (PG) to seamlessly capture domain knowledge in samples, and explicitly extract finegrained semantic correspondence between visual regions and diagnostic texts for accurate Brain CT report generation. Specifically, the PG contains heterogeneous nodes of tissue and lesion attributes, which are connected by edges with prior knowledge. As shown in Figure 1, we decompose PG into two fixed Tissue Graph and Lesion Graph with intra-attribute knowledge (relations in each attribute), and one dynamic Tissue-Lesion Graph with inter-attribute knowledge (correspondences between two attributes for each case). Then, we respectively incorporate Tissue and Lesion Graphs into an attribute-oriented graph convolution network for dedicated node features learning, which is jointly updated via an Intra-Attribute Classification (IAC) loss module. While the Tissue-Lesion graph is updated via an Inter-Correlation Alignment (ICA) loss module, which facilitates learning the complex tissue-lesion connections, in turn, benefits node feature representation. Finally, a crossmodal Contrastive Learning (CL) module is pro-\nposed to reconcile the learned visual features with their corresponding word embeddings in the report, to further boost the visual encoder and textual decoder in report generation.\nIn sum, our main contributions contain:\n1. We propose a novel framework to seamlessly capture detailed domain knowledge from Pathological Graph, and explicitly align fine-grained visual and textual features of pathology, which improves the encoder and decoder of Brain CT report generation by sharing feature learning layers.\n2. We, for the first time, introduce the idea of decoupling cranial tissues and lesions via Pathological Graph into the medical report generation area, which is capable of handling finegrained alignment between long-text report and multiple scans.\n3. We comprehensively validate our model on the BCT-CHR dataset. The experimental results indicate that our method surpasses previous arts in medical report generation."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Medical Report Generation With Knowledge Graph",
            "text": "To mimic the expert knowledge of radiologists, the using of Knowledge Graph (KG) can endow the MRG model with better capabilities of abnormal recognition and has gained increasing research interest, which can be summarized into three types. The first type focuses on stressing clinical terminologies (e.g. abnormalities and diseases). Li et al. (2019) extracts a set of chest medical terminologies from the MIMIC-CXR dataset, which are regarded as graph nodes. The edges linking nodes are assigned with attention weights to depict latent relations, which may be affected by error-updated cases. The second type is to build a universal graph that contains lesion nodes with stable edges acquired by prior knowledge (Zhang et al., 2020; Liu et al., 2021). Such relationships are stored in the adjacency matrix and learned by graph convolution networks. As an extension of this graph, Li et al. (2023) proposes to dynamically add characterized nodes for each sample, but still limits to represent another detailed attribute, i.e. pathological tissues. The last type utilizes NLP-rule based methods to extract detailed triplets from the training corpus\nand build a clinical graph (Li et al., 2022). The sample-related triplets are restored to learn specific knowledge, though the triplets may be wrong extracted.\nOur pathological graph seamlessly combines the advantages of the last two types. We first separately construct two fixed tissue and lesion graphs as the way in the second type for dedicated pathology representing, and then extract structured triplets for each case to depict fine-grained relations between paired attributes."
        },
        {
            "heading": "2.2 Cross-modal Feature Alignment",
            "text": "Learning medical semantics across visual and textual modalities is essential for MRG model to generate logical and accurate reports. Thus, the crossmodal feature alignment has attracted growing interest in recent studies, which can be roughly divided into three stages. In the early stage, various attention mechanisms (Jing et al., 2018; Wang et al., 2018; Xue et al., 2018; Yang et al., 2021) are adopted to extract abnormal visual features and guide the generation of diagnostic texts. However, the attention map updated by naive cross-entropy loss could not sufficiently represent the complex cross-modal patterns. For the second stage, Chen et al. (2021) introduces the memory vector to restore multi-modal relations, which is further extended with reinforcement learning (Qin and Song, 2022) and class-related prototypes (Wang et al., 2022). Since the visual and textual features across different samples are highly similar, it is still challenging to capture the essence of abnormal clues. Parallel to the second stage, the incorporation of contrastive learning can effectively distinguish similar features, which paves the next phase. Yan et al. (2021); Yang et al. (2023) utilize instance-level contrast (i.e. image-report pairs from same cases are positives, otherwise are negatives) in MRG models to enhance the consistency of multi-modal features. Li et al. (2023) further improves the visual representation with graph features and benefits the report generation. Our PGCA follows the third stage. Different from previous methods, we refine complex visual features into dedicated node features, which are contrasted with related medical word features for fine-grained feature alignment."
        },
        {
            "heading": "3 Methodology",
            "text": "As shown in Figure 2, our framework contains two parallel branches, namely Brain CT report gener-\nation and Pathological Graph-driven Cross-modal Alignment (PGCA), which interact by the shared visual and textual embedding layers."
        },
        {
            "heading": "3.1 Brain CT Report Generation",
            "text": "Given the input scans S = {s1, ..., sN}, where N denotes the number of scans in each sample, the target of this branch is to generate a diagnostic report Y = {y1, ..., yM} with M words. The report generation model follows the traditional encoder-decoder pipeline. Firstly, we apply the ResNet101 (He et al., 2016) to extract visual features of S that contain global features F = {f1, ..., fN} \u2208 RN\u00d7d (N = 24, d = 2048) and grid features G = {g1, ..., gN} \u2208 RN\u00d7H\u00d7d (H = 196). Then, F and G are embedded by a visual encoder, resulting in global and spatial visual feature embeddings Vf and Vg, respectively. Finally, visual features V = {Vf , Vg} is used to generate long reports in the decoder, which contains a textual embedding layer and a language model with keywords-driven interactive recurrent network (Yang et al., 2021). We train the parameters \u03b8 by minimizing the cross-entropy loss, which can be expressed as:\nLg = \u2212 M\u2211 m log p\u03b8(xm|V, x1:m\u22121), (1)\nwhere p(xm|V, x1:m\u22121) denotes the predicted probability for the m-th word based on visual features V and previous word embeddings x1, x2, ..., xm\u22121."
        },
        {
            "heading": "3.2 Pathological Graph-driven Cross-modal Alignment",
            "text": "To boost the report generation, we propose to learn fine-grained visual-textual representations to improve the encoder and decoder by co-training with PGCA, which contains graph construction, graph embedding and updating, and cross-modal contrastive learning."
        },
        {
            "heading": "3.2.1 Pathological Graph Construction",
            "text": "The knowledge graph is widely adopted to represent the relationship of medical entities. Different from previous Chest report generation methods (Zhang et al., 2020; Liu et al., 2021) with only 1-2 images and fewer pathological entities for diagnosis, generating Brain CT reports from multiple scans meets more challenges. Regularly, elements to be reported from scans contain some key\nbrain tissues and their associated lesions, which are depicted as fundamental brain findings (Griffin et al., 2002). Motivated by this, we propose to organize a Pathological Graph (PG) to cover clinically important elements in Brain CT samples. We first obtain a set of critical tissues and lesions, denoted as T = {t1, ..., tNt}(Nt = 19) and L = {l1, ..., lNl}(Nl = 13) respectively, where Nt and Nl are the numbers of selected tissues and lesions , based on the knowledge of experts and the word frequency in training corpus. These elements serve as the nodes in our PG. Given the heterogeneous nature of the nodes (i.e., tissue and lesion attributes), we proceed to link them using edges derived from various prior knowledge, such as relations in tissues, relations in lesions, and relations between tissues and lesions, as shown in Figure 1. This process effectively partitions the PG into three distinct sub-components: the Tissue Graph, Lesion Graph, and Tissue-Lesion Graph. Next, we mainly introduce the building of these three components.\n1) Tissue Graph: Clinically, physicians often divide Brain CT scans into 8 layers to facilitate the diagnosis of different brain tissues. Following this diagnostic pattern, we enhance the correlation of tissues that existed in the same layer by prior medical knowledge (more details see Appendix A.1), and then define the Tissue Graph G(T ) = (V(T ), E(T )) with Nt clinically essential tissue nodes and a global node, where V(T ), E(T ) denote the nodes and edges connecting them.\n2) Lesion Graph: Following the conclusion that lesions diagnosed in the same tissue are more related (Zhang et al., 2020), we extract adequate\n<tissue,lesion> pairs from each sentence in the training corpus with artificial refining, and thereby relations in lesions can be summarized with tissues as a bridge (more details see Appendix A.2). Then, we define the Lesion Graph G(L) = (V(L), E(L)) with Nl lesion nodes and a global node, where V(L), E(L) denote the nodes and edges, respectively.\n3) Tissue-Lesion Graph: Since Tissue and Lesion Graphs are fixed to learn stable semantics, Tissue-Lesion Graph is designed to depict detailed attributes\u2019 interconnections for each case. We modify the extracted <tissue,lesion> pairs into <tissue,relation,lesion> triplets, where the relation \u2208 {0, 1} denotes whether the tissue is paired with the lesion. For each tissue node v(T )i \u2208 GT , its associated lesions can be denoted as a local Lesion Graph G(Li) \u2208 G(L), and we represent the Tissue-Lesion Graph as (G(T ), {G(Li)}) to store the inter-attribute correlations for each instance.\nAs an aggregation of three subgraphs, the PG encapsulates a wealth of medical knowledge. Next, we will explore how to represent this knowledge within the model."
        },
        {
            "heading": "3.2.2 Pathological Graph Embedding",
            "text": "Different from previous single-attribute learning methods (Zhang et al., 2020; Liu et al., 2021), we adopt an attribute-oriented graph convolution network in parallel for both Tissue and Lesion Graphs. Specifically, we first initialize the graph features by spatial visual features Vg as (Zhang et al., 2020) to initially map each pathological node vi to different spatial regions, resulting in initialized tissue graph feature T 0f and lesion graph feature L 0 f . More\ndetails are in Appendix A.4. Then, our graph convolution network follows Zhang et al. (2020); Liu et al. (2021) to propagate two graph features as:\nT l+1f = update1(T l f ,msge1(T l f , AT )), (2) Ll+1f = update2(L l f ,msge2(L l f , AL)), (3)\nwhere T lf , L l f respectively denote the tissue and lesion graph features in the l-th layer, and AT ,AL are the normalized Laplacian of adjacency matrixes according to Tissue and Lesion Graph. msge1, msge2 are feature aggregation functions based on the adjacency matrix, while update1 and update2 propagate node features in PG.\nAfter 2-layer propagation, we obtain the tissue graph embedding Tf = {Tf 1, ..., TfNt} \u2208 RNt\u00d7N\u00d7dg and lesion graph embedding Lf = {Lf 1, ..., LfNl} \u2208 R\nNl\u00d7N\u00d7dg , where Tf i \u2208 RN\u00d7dg and Lf i \u2208 R\nN\u00d7dg respectively denote the i-th tissue and lesion node embedding. Afterward, we use a global average pooling on Tf and Lf to converge the information of N scans and obtain the generalized node features of tissues Ta \u2208 RNt\u00d7dg and lesions La \u2208 RNl\u00d7dg ."
        },
        {
            "heading": "3.2.3 Pathological Graph Updating",
            "text": "Since the tissues and lesions are sparse and semantically correlated in Brain CT, node features in PG are supposed to be updated with the following concerns: 1) Concrete semantics of pathology groundings; 2) Tissue-lesion relations. Note that the updating is only the tuning of graph node features and does not involve any new connections within the tissue and lesion subgraphs.\nTo accurately represent the medical semantics of each node, we introduce an Intra-Attribute classification (IAC) module to dynamically update the node features. Separate IAC modules are employed for tissue and lesion nodes. Here, we take the tissue nodes as an example to illustrate IAC. We design a classification head, which takes Ta as input, to predict the tissue labels C\u0302t \u2208 RNt . We then use BCE loss to optimize the IAC module as:\nLIACt = \u2212 Nt\u2211 i W iIACt [Ct i log C\u0302t i\n+(1\u2212 Cti) log(1\u2212 C\u0302t i )],\n(4)\nwhere Ct \u2208 RNt denotes the ground tissue labels extracted from reports, WIACt is a weight matrix for tissue node updating, and i denotes the current\ntissue index. Following this way, we can acquire the fine-grained pathology-grounded node features for both tissues and lesions.\nWhile IAC captures the semantics of single attribute nodes, learning the inter-attribute relations between tissues and lesions is also essential, as our task is to generate reports with complex cranial findings rather than a simple classification. Therefore, we design an Inter-Correlation Alignment (ICA) module to match the correct tissue-lesion pairs via BCE loss. Specifically, we first extract the triplets in Tissue-Lesion Graph to depict tissuelesion relations in a current case. As such, each sample is equipped with an attribute alignment matrix Ma \u2208 {0, 1}Nt\u00d7Nl as the ground-truth label, where the matrix element eij = 1 denotes the i-th tissue is associated with the j-th lesion in this sample. Afterward, we use dedicated node features (Ta and La) to predict the inter-correlation matrix as:\nM\u0302a = Sigmoid((TaL T a )W T a + ba) \u2208 RNt\u00d7Nl ,\n(5) where Wa,ba are parameters. The mutual information between two attributes is finally preserved by minimizing the following ICA loss:\nLICA = \u2212 Nt\u2211 i Nl\u2211 j WICA i j [Ma i j log M\u0302a i j\n+(1\u2212Maij) log(1\u2212 M\u0302a i j)],\n(6)\nwhere WICA is a learnable weight matrix, and Maij , M\u0302a i j denote the (i-th,j-th) value in the ground-truth and predicted attribute matrix, respectively."
        },
        {
            "heading": "3.2.4 Cross-modal Contrastive Learning",
            "text": "Considering that detailed medical information is significant to find subtle pathological clues that depict critical medical conditions, the goal of our Contrastive Learning (CL) module is to represent the alignment of fine-grained pathological semantics from visual and textual modalities. To achieve this, we regard the nodes learned by IAC and ICA as fine-grained visual semantics since they comprehensively considered the intra- and inter-attribute knowledge and focused on salient pathological regions. Then, we extract textual features of corresponding words in the report via the shared textual embedding layers in textual decoder and map each node feature to its matched textual embeddings by contrastive learning. Specifically, we use tissue nodes for example. Given the j-th sample with N\nscans and a report, we first project node features and associated word embeddings into a latent space, resulting in Vtj = {Vt (1) j , Vt (2) j , ..., Vt (Ntj ) j } and Rtj = {Rt (1) j , Rt (2) j , ..., Rt (Ntj ) j }, where Vt (i) j \u2208 Rdg , Rt (i) j \u2208 Rdg denote the projected visual and textual features, and Ntj is the number of tissues reported in the j-th sample. We then use the loss function similar to temperature-normalized InfoNCE (van den Oord et al., 2018) loss for visualtextual alignment:\nLCLt = \u2212 Ntj\u2211 u=1 log exp( s(Vt (u) j ,Rt (u) j ) \u03c4 ) Ntj\u2211\nv=1,u\u0338=v\nexp( s(Vt\n(u) j ,Rt (v) j )\n\u03c4 )\n,\n(7) where s(.) measures the cosine similarity between cross-modal vectors, and \u03c4 is the temperature hyperparameter. Following the same process of tissue semantic alignment, we can also obtain the lesion contrastive learning loss LCLl ."
        },
        {
            "heading": "3.3 Overall objective function",
            "text": "Finally, our model is optimized by the total loss regarding the report generation branch and PGCA branch, which is defined as:\nL = Lg + \u03bb1LIAC + \u03bb2LICA + \u03bb3LCL, (8)\nwhere LIAC and LCL are calculated by the sum of tissue and lesion aspects. \u03bb1, \u03bb2, and \u03bb3 are the coefficients to balance the total loss."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "We evaluate our model on the Brain CT report generation dataset BCT-CHR (Yang et al., 2021).\nThere are 49,152 CT scans and 2048 Chinese reports from 2048 anonymous samples, and each sample includes 24 scans over multiple pathological layers for various abnormal detection and a paired patient report. Following Yang et al. (2021), the dataset is split by 7 : 2 : 1 for training, testing, and validation, respectively. When tokenizing reports, the words with less than 2 occurrences are dropped, counting to 798 words in the vocabulary. Notably, the English translations are only for better understanding and are not used in training."
        },
        {
            "heading": "4.2 Evaluation Metrics",
            "text": "We fully evaluate the performance on the NLG (Natural Language Generation) and CE (Clinical Evaluation) metrics. NLG metrics contains BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), ROUGE-L (Lin, 2004) and CIDEr (Vedantam et al., 2015), which are denoted as B1, B2, B3, B4, M, RG, and C. Then, according to the pathological knowledge obtained by radiologists, we use 24 keywords (\u201cbasal ganglia\", \u201cedema\", \u201clateral ventricle\", etc.) to evaluate the CE metrics: P (Precision), R (Recall), and F1 score."
        },
        {
            "heading": "4.3 Implementation Details",
            "text": "We set the scan number N = 24 for each sample and reshape the size of scans to 512 \u00d7 512. The scan features are extracted by ResNet101 (He et al., 2016), which is pretrained on ImageNet (Deng et al., 2009) and fine-tuned on CQ500 dataset (Chilamkurthy et al., 2018). The hyperparameters are tuned via the validation set. Empirically, the loss coefficients {\u03bb1, \u03bb2, \u03bb3}, temperature value \u03c4 and graph node dimension dg are set to {0.3, 0.001, 0.2}, 0.4 and 512, respectively. We\ntrain the model with Adam optimizer (Kingma and Ba, 2015) on an NVIDIA RTX 3090 GPU with a batch size of 4 for 60 epochs. The learning rate is initially set to 4e-4 in the first 30 epochs, and we decrease it by a 0.8 rate per 3 epochs after that."
        },
        {
            "heading": "4.4 Results and Discussion",
            "text": ""
        },
        {
            "heading": "4.4.1 Comparison Study",
            "text": "Besides the only Brain CT report generation method WGAM (Yang et al., 2021), we also reproduce some SOTA models in image captioning (Show-Tell (Vinyals et al., 2015), Soft ATT (Xu et al., 2015), HRNN (Krause et al., 2017), UpDown (Anderson et al., 2018)) and chest X-ray Report generation (MRMA (Xue et al., 2018), R2Gen (Chen et al., 2020), WCL (Yan et al., 2021), R2Gen-CMN (Chen et al., 2021), CMMRL (Qin and Song, 2022), XProNet (Wang et al., 2022)) on BCT-CHR dataset for comprehensive comparisons.\nAs shown in Table 1, our method outperforms the competitors on almost all evaluation metrics. Specifically, the negative results in Show-Tell indicate that complex medical semantics may not be fully captured without an efficient cross-modal interaction. Benefit from the cross-modal attention (Soft ATT, Up-Down, and WGAM) and instancelevel contrastive learning (WCL), the models gain improving results. Unexpectedly, by simply using memory vectors without domain knowledge, R2Gen-CMN and CMMRL perform poorly on most metrics, while XProNet adds class-related knowledge and gets better results. This reflects the effect of knowledge and the difficulties of our task. WGAM benefits from weakly-guided attention to capture visual features of Brain CT and achieves higher BLEU scores. Despite these successes, detailed visual and textual features are not sufficiently learned in MRG systems, which fails to generate accurate Brain CT reports. With the use of pathologi-\ncal graph and fine-grained cross-modal alignment, our PGCA achieves the best performance in contrast with previous arts. Especially, compared with the only work WGAM for Brain CT report generation, PGCA gains remarkable improvement in B3 (20.4% \u2192 21.6%), RG (35.1% \u2192 36.5%), and F1 (55.9% \u2192 57.2%), which justifies that PGCA does not confuse the learning of some essential pathological details, but generating sentences with more accurate topics. It is worth noting that PGCA is slightly inferior on CIDEr, which is more sensitive to word frequency. The reason may be that our graph contains some high-frequency terms, leading to more occurrences of them in generated reports and slightly decreasing the CIDEr."
        },
        {
            "heading": "4.4.2 Ablation Study",
            "text": "Table 2 summarizes the results of ablation studies to verify the contributions of graph knowledge injection and fine-grained contrastive learning. We remove the supervision of attention in WGAM as our Baseline, and then progressively add attribute components (tissue and lesion) and the module losses (LIAC , LICA and LCL), respectively denoting the incorporation of two types of attribute knowledge in pathological graph and the utilization of proposed modules (i.e. IAC, ICA, and CL).\nAs shown in Table 2, the advantages of adding tissue and lesion attributes can be well reflected by the improvement from Baseline to (b) and further to (d), which justifies our assumption for learning subtle tissues and lesions. The comparison between (b) and (c) also indicates the contributions brought by fine-grained contrastive learning. Note that, (a) denotes the Baseline model with the incorporation of CLIP loss (Radford et al., 2021) that directly uses image-text pairs for coarse-grained contrastive learning. We can observe that, only with fine-grained tissues for detailed feature align-\nment, (c) has already surpassed (a) by a large margin, and when the lesion attribute is considered, (e) gains further improvements. This indicates the effectiveness of our fine-grained cross-modal feature alignment idea in MRG. Compared with (d), (e) gains inferior results on METEOR, ROUGE and CIDEr, the reason may be that pathological semantics in two attributes are separately learned without a clinically important interaction. With the incorporation of ICA, our final model achieves the best performance. These results verify the capabilities of our modules to boost report generation.\nTo further confirm the module contributions for pathology identification, in Figure 3, we visualize the ROC curves for the classification of 10 essential\npathological elements summarized by experienced physicians. Since classification is only performed by our IAC module, the left chart shows the performance of the model only with IAC and the report generation branch. With the progressive addition of CL and ICA modules, the model gains better performance, which further indicates that CL and ICA can help enhance the node feature representation and boost the recognition of pathological elements. Besides, we also compare the classification performance with the classic ResNet101 (He et al., 2016) model. It is noticeable that our model outperforms the ResNet by a large margin, which not only indicates the effectiveness of our graph learning and cross-modal alignment methods but\nalso hints report generation and classification can boost each other.\nTo examine the impact of hyperparameters of loss (i.e., \u03bb1, \u03bb2 and \u03bb3 in Eq. 8) on our model, we provide parameter sensitivity experiments in Figure 4, where we fix two optimal hyperparameters and vary the third within a specific range. We can observe that too low or too high values of hyperparameters decrease the performance. The reason may be because the too low score of \u03bb1, \u03bb2, and \u03bb3 respectively lead to inadequate pathological semantic learning, inter-attribute matching, and visual-textual alignment, which degrades the feature representation. Too high \u03bb1, \u03bb2, and \u03bb3 may weaken the training of report generation-related parameters and cause sub-optimal results."
        },
        {
            "heading": "4.4.3 Qualitative Analysis",
            "text": "Figure 5 visualizes the generated reports from a qualitative perspective, we list the ground truth image-report pairs with pathological labels, predicted labels generated by IAC module, and the reports generated by Baseline, WGAM, and our PGCA. It is observed that Baseline and WGAM suffer from generating suboptimal reports with inaccurate cranial tissues and wrong-matched lesions. With the incorporation of our enriched graph\nknowledge and fine-grained cross-modal feature alignment, PGCA can not only recognize complex relations of pathologies and correctly predict labels, but also generate high-quality reports based on the understanding of pathological elements. We also visualize the ability to recognize fine-grained pathology semantics in Figure 6, the salient regions grounded by our IAC module are generally consistent with human-labeled boxes. These qualitative analyses further demonstrate the superiority of our PGCA model."
        },
        {
            "heading": "5 Conclusion",
            "text": "We propose a Pathological Graph-driven Crossmodal Alignment model for Brain CT report generation. First, we construct a Pathological Graph that incorporates detailed medical knowledge, allowing for the injection of this knowledge into dedicated node features through graph embedding and updating, thereby achieving fine-grained visual representations. Second, we align the learned node features with the corresponding word embeddings by crossmodal contrastive learning, to further boost report generation. Extensive experiments demonstrate that our model achieves superior performance in generating clinically accurate reports.\nLimitations\nThis paper is mainly toward Brain CT medical report generation and may not generalize well to other medical imaging, such as chest datasets (e.g. MIMIC-CXR and IU-Xray) and ophthalmology datasets (e.g. FFA-IR), without further adaption. Besides, we only focus on mining detailed tissue and lesion elements in brain findings and building fine-grained visual-textual alignment based\non them, which lacks the consideration of more specific details like tissue orientation or lesion size. In the future, explorations of how to incorporate useful medical knowledge with different types and granularity into the medical report generation model will substantially contribute to this field.\nEthics Statement\nThe medical images in the BCT-CHR dataset have undergone a thorough process of de-identification to protect participants\u2019 privacy. Additionally, we have obtained the required data permissions, ensuring our work meets ethical standards."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by the National Natural Science Foundation of China under Grant 61906007, 62276010, and 62306253, in part by the R&D Program of Beijing Municipal Education Commission under Grant KM202110005022 and KZ202210005009."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Details of Tissue Graph Construction In the Tissue Graph, nodes are defined as Nt tissue terms, which are summarized in two aspects: 1) The knowledge and expertise of experienced radiologists; 2) The frequency of tissue words in the training corpus. We present these tissues in Table 3. Then, with the given nodes, a rule-based algorithm is designed to assign the connecting edges via the prior domain knowledge, i.e. tissues in the same scan layer are more related, otherwise not. Inspired by Li et al. (2022), our algorithm contains the following three steps:\n1) Knowledge Formatting: We format the expert knowledge into <layer,tissue> pairs, where layer indicates a layer name (e.g. \u201ccanthus earline layer\", \u201csuprasellar cistern layer\", \u201cupper cerebral cortex layer\", etc.), and tissue denotes the tissue that can be observed in this layer.\n2) Dependency Parsing: With the formatted knowledge pairs, for each layer, we collect a set of paired tissues and then parse the dependency of these tissues into <tissue1,tissue2> pairs, where tissue1, tissue2 denotes two different tissues paired with the same layer. We traverse all the layers and finally obtain a large set of tissue dependency pairs.\n3) Entity Linking: A matrix Et0 \u2208 RNt\u00d7Nt is set to store the relation of Nt tissue nodes, which is initialized as an identity matrix. Then, we traverse the extracted tissue dependency pairs <tissue1,tissue2>, and add the weight by 0.2 to update the bidirectional edges of tissue1 and tissue2 for each time. If the edge between two different tissues reaches 0.8, we stop to add it. Finally, we add a column and a row with the weight of 1.0 to depict the fully connected edges of a global node, resulting in the edge matrix Et \u2208 R(Nt+1)\u00d7(Nt+1).\nIn total, the Tissue Graph contains 133 tissue dependency pairs, and more information are listed in Table 4.\nA.2 Details of Lesion Graph Construction We comprehensively combine the expert opinions and the word frequency in the training corpus to extract Nl lesion terms as our lesion nodes, which are presented in Table 5. Similar to the linking of tissue nodes, we use the same rule-based algorithm to build lesion edges. Different from the knowledge utilized in constructing the Tissue Graph, here we follow the assumption of Zhang et al. (2020): le-\nsions diagnosed in the same tissue are more related, otherwise not.\nIn Knowledge Formatting, we extract adequate <tissue,lesion> pairs from each sentence in the training corpus, and manually adjust the pairs to ensure correctness. In Dependency Parsing, we parse the dependencies of lesions into <lesion1,lesion2>, where lesion1, lesion2 denotes two different lesions paired with the same tissue. In Entity Linking, we store the relation of Nl lesion nodes in El0 \u2208 RNl\u00d7Nl , which is initialized as an identity matrix. We traverse the collected <lesion1,lesion2> pairs and add the weight by 0.1 to update the bidirectional edges of lesion1 and lesion2 for each time until the score reaches 0.9. Finally, a column and a row with the weight of 1.0 are added to represent the fully connected edges\nof one global node, resulting in the edge matrix El \u2208 R(Nl+1)\u00d7(Nl+1).\nIn total, our Lesion Graph includes 652 lesion dependency pairs, and more details are shown in Table 6.\nA.3 More Cases of Brain CT Report Generation\nMore cases of Brain CT report generation are presented in Figure 7. The correct tissues, lesions, and error-predicted pathological elements are marked in green, purple and red, respectively.\nA.4 Details of Graph Features Initialization\nTo map the graph features into pathology-related spatial regions, we follow the feature initialization method in Zhang et al. (2020) with some modifications. Given the spatial visual feature Vg \u2208 R(N\u2217H)\u00d7dg (N = 24, H = 196, dg = 512), where N , H ,dg denote the number of scans, grids,\nand channels, we duplicate it twice for initializing tissue and lesion graph features separately. We mainly take the tissue graph feature initialization for example.\nFirst, we transform the channel dimension of Vg into tissue number Nt as:\nVtg1 = Relu(VgW T t1 + bt1), (9) Vtg2 = Relu(Vtg1W T t2 + bt2), (10) Vtg3 = Relu(Vtg2W T t3 + bt3), (11)\nwhere Relu(.) denotes the ReLU activation function. Wt1 \u2208 R(N\u2217H)\u00d7512, Wt2 \u2208 R(N\u2217H)\u00d7128, Wt3 \u2208 R(N\u2217H)\u00d7Nt are learnable weights. bt1 , bt2 , bt3 are learnable biases. We use the transformation result Vtg3 \u2208 R\n(N\u2217H)\u00d7Nt to represent the graph information.\nThen, to bind the graph information Vtg3 with specific visual regions, we perform the multiplication of matrices Vg and Vtg3 along the dimension of spatial grids, resulting in Vtg \u2208 RN\u00d7H\u00d7Nt . In this way, each row of Vtg can depict one specific tissue region, and we represent it as one tissue node feature. Afterward, we conduct average pooling on the dimension of node number Nt, to obtain the global node feature Vtglobal \u2208 RN\u00d7H for the tissue graph, which contains general node representations. Finally, we concat Vtg and Vtglobal, and reshape it as the initialized tissue graph feature T 0f \u2208 R(Nt+1)\u00d7(N\u2217H).\nAs the same process, the lesion graph feature is initialized as L0f \u2208 R(Nl+1)\u00d7(N\u2217H), where Nl is the number of lesion nodes."
        }
    ],
    "title": "Granularity Matters: Pathological Graph-driven Cross-modal Alignment for Brain CT Report Generation",
    "year": 2023
}