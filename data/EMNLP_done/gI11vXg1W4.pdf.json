{
    "abstractText": "The Retrieval Question Answering (ReQA) task employs the retrieval-augmented framework, composed of a retriever and generator. The generator formulates the answer based on the documents retrieved by the retriever. Incorporating Large Language Models (LLMs) as generators is beneficial due to their advanced QA capabilities, but they are typically too large to be fine-tuned with budget constraints while some of them are only accessible via APIs. To tackle this issue and further improve ReQA performance, we propose a trainable Pluggable Reward-Driven Contextual Adapter (PRCA), keeping the generator as a black box. Positioned between the retriever and generator in a Pluggable manner, PRCA refines the retrieved information by operating in a tokenautoregressive strategy via maximizing rewards of the reinforcement learning phase. Our experiments validate PRCA\u2019s effectiveness in enhancing ReQA performance on three datasets by up to 20% improvement to fit black-box LLMs into existing frameworks, demonstrating its considerable potential in the LLMs era.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haoyan Yang"
        },
        {
            "affiliations": [],
            "name": "Zhitao Li"
        },
        {
            "affiliations": [],
            "name": "Yong Zhang"
        },
        {
            "affiliations": [],
            "name": "Jianzong Wang"
        },
        {
            "affiliations": [],
            "name": "Ning Cheng"
        },
        {
            "affiliations": [],
            "name": "Ming Li"
        },
        {
            "affiliations": [],
            "name": "Jing Xiao"
        }
    ],
    "id": "SP:cfa85d4a98b6612ab49dcd3a2bb7d16d6d33991d",
    "references": [
        {
            "authors": [
                "Vaibhav Adlakha",
                "Shehzaad Dhuliawala",
                "Kaheer Suleman",
                "Harm de Vries",
                "Siva Reddy."
            ],
            "title": "TopiOCQA: Open-domain conversational question answering with topic switching",
            "venue": "Transactions of the Association for Computational Linguistics, 10:468\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Chenxin An",
                "Ming Zhong",
                "Zhichao Geng",
                "Jianqiang Yang",
                "Xipeng Qiu."
            ],
            "title": "Retrievalsum: A retrieval enhanced framework for abstractive summarization",
            "venue": "CoRR, abs/2109.07943.",
            "year": 2021
        },
        {
            "authors": [
                "Danqi Chen",
                "Adam Fisch",
                "Jason Weston",
                "Antoine Bordes."
            ],
            "title": "Reading Wikipedia to answer opendomain questions",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870\u20131879,",
            "year": 2017
        },
        {
            "authors": [
                "Zhihong Chen",
                "Feng Jiang",
                "Junying Chen",
                "Tiannan Wang",
                "Fei Yu",
                "Guiming Chen",
                "Hongbo Zhang",
                "Juhao Liang",
                "Chen Zhang",
                "Zhiyi Zhang"
            ],
            "title": "Phoenix: Democratizing chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Zhengxiao Du",
                "Yujie Qian",
                "Xiao Liu",
                "Ming Ding",
                "Jiezhong Qiu",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "GLM: General language model pretraining with autoregressive blank infilling",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Fazzie",
                "FrankLeeeee",
                "BlueRum",
                "ver",
                "ofey",
                "Wenhao Chen",
                "Zangwei Zheng",
                "Xue Fuzhao"
            ],
            "title": "Colossalchat. https://github.com/hpcaitech/ ColossalAI/tree/main/applications/Chat",
            "year": 2023
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Mingwei Chang."
            ],
            "title": "Retrieval augmented language model pre-training",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "Gautier Izacard",
                "Mathilde Caron",
                "Lucas Hosseini",
                "Sebastian Riedel",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Edouard Grave."
            ],
            "title": "Unsupervised dense information retrieval with contrastive learning",
            "venue": "Transactions on Machine Learning Research, 08:1\u201321.",
            "year": 2022
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave."
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Gautier Izacard",
                "Patrick Lewis",
                "Maria Lomeli",
                "Lucas Hosseini",
                "Fabio Petroni",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Armand Joulin",
                "Sebastian Riedel",
                "Edouard Grave."
            ],
            "title": "Few-shot learning with retrieval augmented language models",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "2020a. BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Douwe Kiela"
            ],
            "title": "Retrieval-augmented generation for knowledge",
            "year": 2020
        },
        {
            "authors": [
                "Xinbei Ma",
                "Yeyun Gong",
                "Pengcheng He",
                "Hai Zhao",
                "Nan Duan."
            ],
            "title": "Query rewriting for retrievalaugmented large language models",
            "venue": "arXiv preprint arXiv:2305.14283.",
            "year": 2023
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "arXiv preprint arXiv:2302.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems, volume 35, pages 27730\u201327744. Curran Associates, Inc.",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoman Pan",
                "Kai Sun",
                "Dian Yu",
                "Jianshu Chen",
                "Heng Ji",
                "Claire Cardie",
                "Dong Yu."
            ],
            "title": "Improving question answering with external knowledge",
            "venue": "Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 27\u201337, Hong Kong,",
            "year": 2019
        },
        {
            "authors": [
                "Baolin Peng",
                "Chunyuan Li",
                "Pengcheng He",
                "Michel Galley",
                "Jianfeng Gao."
            ],
            "title": "Instruction tuning with gpt-4",
            "venue": "arXiv preprint arXiv:2304.03277.",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza."
            ],
            "title": "The probabilistic relevance framework: Bm25 and beyond",
            "venue": "Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389.",
            "year": 2009
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "John Schulman",
                "Philipp Moritz",
                "Sergey Levine",
                "Michael I. Jordan",
                "Pieter Abbeel."
            ],
            "title": "Highdimensional continuous control using generalized advantage estimation",
            "venue": "The Fourth International Conference on Learning Representations.",
            "year": 2016
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov."
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "CoRR, abs/1707.06347.",
            "year": 2017
        },
        {
            "authors": [
                "Weijia Shi",
                "Sewon Min",
                "Michihiro Yasunaga",
                "Minjoon Seo",
                "Rich James",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Wen-tau Yih."
            ],
            "title": "Replug: Retrievalaugmented black-box language models",
            "venue": "arXiv preprint arXiv:2301.12652.",
            "year": 2023
        },
        {
            "authors": [
                "Kurt Shuster",
                "Spencer Poff",
                "Moya Chen",
                "Douwe Kiela",
                "Jason Weston."
            ],
            "title": "Retrieval augmentation reduces hallucination in conversation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784\u20133803, Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Karen Sparck Jones."
            ],
            "title": "A statistical interpretation of term specificity and its application in retrieval",
            "venue": "Journal of Documentation, 28(1):11\u201321.",
            "year": 1972
        },
        {
            "authors": [
                "Nisan Stiennon",
                "Long Ouyang",
                "Jeffrey Wu",
                "Daniel Ziegler",
                "Ryan Lowe",
                "Chelsea Voss",
                "Alec Radford",
                "Dario Amodei",
                "Paul F Christiano."
            ],
            "title": "Learning to summarize with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D. Manning."
            ],
            "title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
            "venue": "Proceedings of the 2018 Conference on Empiri-",
            "year": 2018
        },
        {
            "authors": [
                "Qin",
                "Masahiro Tanaka",
                "Shuai Che",
                "Shuaiwen Leon Song",
                "Yuxiong He."
            ],
            "title": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales",
            "venue": "arXiv preprint arXiv:2308.01320.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Retrieval Question Answering (ReQA) tasks involve generating appropriate answers to given questions, utilizing relevant contextual documents. To achieve this, retrieval augmentation is employed (Chen et al., 2017; Pan et al., 2019; Izacard and Grave, 2021), and comprised of two key components: a retriever and a generator. The retriever\u2019s role is to retrieve relevant documents from a large corpus in response to the question, while the generator uses this contextual information to formulate accurate answers. Such systems alleviate the problem of hallucinations (Shuster et al., 2021), thereby enhancing the overall accuracy of the output.\n\u2020 The work was done when the first author was doing internship at Ping An Technology (Shenzhen) Co., Ltd., China.\n\u2217Corresponding author: Jianzong Wang.\nRecent advances in Large Language Models (LLMs) such as the generative pre-trained transformer (GPT) series (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023) have demonstrated remarkable potential, notably in their zero-shot and few-shot abilities within the realm of QA tasks. Owing to these capabilities, LLMs are excellent choices as generators within the retrievalaugmented framework. However, due to the vast parameters of LLMs, fine-tuning them becomes exceedingly difficult within a limited computation budget. Furthermore, certain LLMs such as GPT-4 (OpenAI, 2023) are closed-source, making it impossible to fine-tune them. To achieve optimal results on specific datasets, fine-tuning retrievalaugmented models becomes necessary (Guu et al., 2020; Lewis et al., 2020b; An et al., 2021). Previous attempts to integrate LLMs into the retrievalaugmented framework have met with partial suc-\ncess but also come with limitations. (Shi et al., 2023) utilized the logits from the final layer of the LLMs when calculating the loss function, which may not be available to certain powerful LLMs that served via APIs. (Ma et al., 2023) involved frequently invoking pricy LLMs and overlooked the impact of the input token length on the accuracy and effectiveness of the system.\nTo overcome these hurdles, we propose a trainable Pluggable Reward-driven Context Adapter (PRCA) that enables one to fine-tune the adapter instead of LLMs under the retrieval-augmented framework on specific datasets and achieve higher performance. Furthermore, PRCA distills the retrieved documents information guided by rewards from the generator through reinforcement learning. The distillation of retrieval information through PRCA reduces the length of text input to the generator and constructs a context of superior quality, which mitigates the hallucination issues during the answer generation. As shown in Figure 1, PRCA is placed between the retriever and the generator, forming a PRCA-based Paradigm where both the generator and the retriever remain frozen. In general, the introduction of the PRCA-based paradigm brings the following advantages:\nBlack-box LLMs Integration With the use of PRCA, LLMs can be treated as a black box integrated into the retrieval-augmented framework, eliminating the need for resource-intensive finetuning and restrictions on closed-nature models.\nRobustness PRCA serves as a pluggable adapter that is compatible with various retrievers and generators because PRCA-based paradigm keeps both the generator and retriever frozen.\nEfficiency The PRCA-based paradigm ensures the efficiency of the framework by reducing the text length inputted into the generator and can adapt to different retrieval corpus."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 The Potential of LLMs as Black-Box Models",
            "text": "LLMs have demonstrated remarkable capabilities in downstream QA tasks, even in scenarios with limited or no training data (Wei et al., 2022). This emergence capability enables them to efficiently tackle such tasks, making them potential candidates\nfor black-box models in inference. Furthermore, the non-open-source nature and large parameter size of these models further contribute to their inclination towards being perceived as black boxes.\nOn one hand, LLMs like GPT-4 (OpenAI, 2023) and PaLM (Scao et al., 2023) have showcased impressive performance in QA tasks. However, their closed source nature restricts access to these models, making API-based utilization the only feasible option, thereby categorizing them as black-box models.\nOn the other hand, training LLMs, exemplified by models like Bloom (Scao et al., 2022) and GLM130B (Zeng et al., 2023), impose substantial computational demands. Specifically, training Bloom took 3.5 months using 384 NVIDIA A100 80GB GPUs. Similarly, GLM-130B requires a two-month training period on a cluster of 96 DGX-A100 GPU servers. These resource requirements make it extremely challenging for the majority of researchers to deploy these models. Moreover, LLMs exhibit rapid development speeds. For instance, from LLaMA (Touvron et al., 2023) to Alpaca (Taori et al., 2023) and now Vicuna (Peng et al., 2023), the iterations are completed within a month. It is evident that the speed of training models lags behind the pace of model iterations. Consequentially, tuning small-size adapters for any sequenceto-sequence LLMs on downstream tasks could be a simpler and more efficient approach."
        },
        {
            "heading": "2.2 Retrieval-Augmented Framework",
            "text": "Various retrieval augmented ideas have been progressively developed and applied to improve the performance in the ReQA task.\nIn the initial stage of research, independent statistical similarity-base retrievers like TF-IDF (Sparck Jones, 1972) and BM25 (Robertson and Zaragoza, 2009) were used as fundamental retrieval engines. They helped in extracting the most relevant documents from the corpus for QA tasks (Chen et al., 2017; Izacard and Grave, 2021).\nThe concept of vectorization was subsequently introduced, where both questions and documents were represented as vectors, and vector similarity became a critical parameter for retrieval. This paradigm shift was led by methods such as dense retrieval, as embodied by DPR (Karpukhin et al., 2020). Models based on contrastive learning like SimCSE (Gao et al., 2021) and Contriver (Izacard et al., 2022a), along with sentence-level se-\nmantic models such as Sentence-BERT (Reimers and Gurevych, 2019), represented this era. These methods can be seen as pre-trained retrievers that boosted the effectiveness of the ReQA task.\nFurther development led to the fusion of retrieval and generation components within the ReQA frameworks. This was implemented in systems like REALM (Guu et al., 2020) and RAG (Lewis et al., 2020b), where retrievers were co-trained with generators, further refining the performance in the ReQA task.\nRecently, advanced approaches like Atlas (Izacard et al., 2022b) and RETRO (Borgeaud et al., 2022) have been introduced which could achieve performance comparable to large-scale models like Palm (Chowdhery et al., 2022) and GPT3 (Brown et al., 2020) with significantly fewer parameters."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Two-Stage Training for PRCA",
            "text": "PRCA is designed to take sequences composed of the given query and the Top-K relevant documents retrieved by the retriever. The purpose of PRCA is to distill this collection of results, presenting a concise and effective context to the generator, while keeping both the retriever and the generator frozen. This PRCA-based paradigm introduces two challenges: the effectiveness of the retrieval cannot be directly evaluated due to its heavy dependence on the responses generated by the generator, and learning the mapping relationship between the generator\u2019s outputs and the input sequence via backpropagation is obstructed due to the black-box generator. To tackle these issues, we propose a twostage training strategy for PRCA, as illustrated in Figure 2. In the contextual stage, supervised learning is employed to train PRCA, encouraging it to output context-rich extractions from the input text. During the reward-driven stage, the generator is treated as a reward model. The difference between the generated answer and the ground truth serves as a reward signal to further train PRCA. This process effectively optimizes the information distillation to be more beneficial for the generator to answer accurately."
        },
        {
            "heading": "3.2 Contextual Extraction Stage",
            "text": "In the contextual extraction stage, we train PRCA to extract textual information. Given an input text Sinput, PRCA generates an output sequence Cextracted, representing the context derived from the\ninput text. The objective of the training process is to minimize the discrepancy between Cextracted and the ground truth context Ctruth and the loss function is demonstrated as follows:\nmin \u03b8 L(\u03b8) = \u2212 1 N N\u2211 i=1 C (i) truth log(fPRCA(S (i) input; \u03b8))\n(1) where \u03b8 represents the parameters of PRCA\nIn the context extraction stage, PRCA is initialized from a BART-Large model pre-trained on CNN Daily Mail dataset (Lewis et al., 2020a)."
        },
        {
            "heading": "3.3 Reward-Driven Stage",
            "text": "In the reward-driven stage, the objective is to align the extracted context Cextracted from the previous stage with the downstream generator, ensuring that the text distilled by PRCA serves effectively to guide the generator\u2019s answering. Given the blackbox nature of the generator, a direct update of PRCA is not feasible. Therefore, we resort to reinforcement learning to optimize PRCA\u2019s parameters. Specifically, the generator offers rewards to guide the update of PRCA\u2019s parameter, targeting to improve answer quality. The reward is based on the ROUGE-L score between the generated answer O\nand the ground truth O\u2217. Meanwhile, it\u2019s vital that PRCA retains its skill of information extraction from long texts, as learned in the contextual extraction stage. Our objective is twofold: maximizing generator\u2019s reward and maintaining similarity between updated and original parameters of PRCA after contextual extraction training. Catering to the reward-driven training where policy actions manipulate sequence tokens, policy optimization, particularly via Proximal Policy Optimization (PPO) (Schulman et al., 2017; Stiennon et al., 2020), is the preferred method. However, when employing a black-box generator as a reward model, we identify certain limitations of using PPO.\nIn (2), we present the PPO\u2019s objective function J(\u03b8). This function strives to optimize the advantage, a value derived from the Generalized Advantage Estimation (GAE) (Schulman et al., 2016). The GAE leverages both \u03b3 and \u03bb as discounting factors, adjusting the estimated advantage based on the temporal difference \u03b4Vt+l, as depicted in (3). Here, Et[min(rt(\u03b8) \u00b7AGAEt , clip(rt(\u03b8), 1\u2212 \u03f5, 1 + \u03f5) \u00b7AGAEt )] captures the expected advantage. The clip function serves to prevent excessive policy updates by constraining the policy update step, ensuring stability in the learning process. The term \u03b2(V (st) \u2212 Rt)2 is a squared-error term between V (st) and Rt. This term seeks to minimize the difference between the predicted and actual value, ensuring accurate value predictions. However, the critic network V is usually initialized to have the same parameter as the reward model (Yao et al., 2023; Fazzie et al., 2023), which is inapplicable when the reward models are black-boxed. Additionally, the APIs from vendors usually have limited amount of return parameters which may cause the computation of Rt impossible.\nmax \u03b8\nJ(\u03b8) = Et[min(rt(\u03b8) \u00b7AGAEt ,\nclip(rt(\u03b8), 1\u2212 \u03f5, 1 + \u03f5) \u00b7AGAEt )] \u2212 \u03b2(V (st)\u2212Rt)2 (2)\nwhere rt(\u03b8) = \u03c0\u03b8(at|st)\n\u03c0\u03b8ori (at|st) is the ratio of the up-\ndated policy \u03c0\u03b8 to the original policy \u03c0\u03b8ori ; at represents the action (the next token); st is the state (the sequence of previous tokens); \u03f5 is the clipping parameter; V is a critic network; V (st) is the predicted value of state st; \u03b2 is a coefficient that weights the squared-error term; Rt is the expected return at time t.\nA GAE(\u03b3,\u03bb) t = T\u2211 l=0 (\u03b3\u03bb)l\u03b4Vt+l (3)\nwhere \u03b4Vt+l = Rt+l + \u03b3V (st+l+1) \u2212 V (st+l); \u03b3 and \u03bb as discounting and GAE parameters respectively.\nTo tackle this issue, we introduce a strategy to estimate Rt. In the PRCA, when the token \u27e8EOS\u27e9 is generated, we can obtain the reward REOS by comparing the generated answer against the ground truth. We consider it an accumulation of the reward Rt achieved at each time step t for the generated token. As for Rt, it serves as a target in J(\u03b8) to train the critic network V (s) for fitting, symbolizing the average reward of the current action, thereby assessing the advantage of the current policy. For each token, the greater the probability of generation, the more important this token is perceived by the current policy, so we consider its contribution to the total reward to be greater. Therefore, we regard the probability of generating each token as the weight of REOS , and the representation of Rt is given by the following:\nRt = REOS \u2217 e\u03c0\u03b8(at|st)\u2211K t=1 e \u03c0\u03b8(at|st) (4)\nREOS = ROUGE-L(O,O\u2217)\n\u2212 \u03b2 \u00b7DKL(\u03c0\u03b8||\u03c0\u03b8ori) (5)\nROUGE-L = LCS(X,Y )\nmax(|X|, |Y |) (6)\nwhere K is the number of tokens in one generated context, LCS(X,Y ) denotes the length of the longest common subsequence between sequence X and sequence Y , and |X| and |Y | denote the lengths of sequences X and Y , respectively.\nThis method mitigates the challenges associated with calculating Rt when interpreting the blackbox generator as a reward model. A substantial advantage it confers is the requirement of invoking the reward model only once for each context generation. Compared to the original PPO that employs the reward model for every token computation, our approach reduces the reward model usage to 1K , which is cost-effective especially when using LLMs as generators.\nTable 1: Overview of the data quantities used for training and testing across three benchmark datasets.\nDataset Train / Test # of Q # of C # of A\nSQuAD Train 87.6k 18.9k 87.6kTest 10.6k 2.1k 10.6k\nHotpotQA Train 90.4k 483.5k 90.4kTest 7.4k 66.5k 7.4k\nTopiQCQA Train 45.5k 45.5k 45.5kTest 2.5k 2.5k 2.5k"
        },
        {
            "heading": "4 Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We performed our experiments on three QA datasets: SQuAD (Rajpurkar et al., 2016), HotpotQA (Yang et al., 2018) and TopiOCQA (Adlakha et al., 2022). The complexity of three datasets increases sequentially: SQuAD is a dataset that matches questions, documents, and answers in a one-to-one manner. HotpotQA is a multi-hop QA dataset, requiring the synthesis of correct answers from multiple documents. TopiOCQA is a conversational QA dataset with topic switching.\nTo align these datasets with our ReQA task, we reconstructed all three datasets into the form of (Q,C,A), where Q and A denote the question and answer pair, and C represents a corpus composed of all the documents in the dataset respectively. In Table 1, we present the number of questions and answers employed in the PRCA training and testing phases for every dataset. Additionally, we provide the quantity of documents contained within each respective corpus."
        },
        {
            "heading": "4.2 Baseline Retrievers and Generators",
            "text": "We conducted experiments with five different retrievers, specifically BM25 (Robertson and Zaragoza, 2009), SentenceBert (Reimers and Gurevych, 2019), DPR (Karpukhin et al., 2020), SimCSE (Gao et al., 2021), and Contriver (Izacard et al., 2022a). We also utilized five generators which are T5-large (Raffel et al., 2020), Phoenix7B (Chen et al., 2023), Vicuna-7B (Peng et al., 2023), ChatGLM (Du et al., 2022) and GPT-3.5 1 to assess the effectiveness of PRCA. Note that both the retrievers and generators remain frozen through the experiment.\nBy pairing every retriever with each generator,\n1Our experiments were conducted with the default version of GPT-3.5-turbo and GPT-4 between May and June 2023 via https://openai.com.\nwe established a total of seventy-five baseline configurations on three datasets. For each configuration, we evaluated the performance with and without the application of PRCA and the difference serves as an indicator of the effectiveness of our proposed approach."
        },
        {
            "heading": "4.3 GPT-4 Assessment",
            "text": "Notably, we used GPT-4 for evaluation rather than traditional metrics like F1 and BLEU, as these metrics often misjudged semantically similar sentences. LLMs often output longer textual explanations for answers, even when the correct answer might be a word or two. Despite attempts to constrain answer lengths, the results weren\u2019t ideal. We then evaluated predictions using both manual methods and GPT-4 against golden answers. GPT-4\u2019s evaluations showed correctness rates of 96%, 93%, and 92% across three datasets, demonstrating its reliability and alignment with human judgment.\nSpecifically, the template for GPT-4 assessment is shown as follows. Finally, the accuracy rate of answering \u201cYes\u201d is counted as the evaluation metric.\nTemplate for GPT-4 Assessment\nPrompt: You are now an intelligent assessment assistant. Based on the question and the golden answer, judge whether the predicted answer correctly answers the question and give only a Yes or No. Question: Golden Answer: Predicted Answer:\nExpected Output: Yes / No"
        },
        {
            "heading": "4.4 Hyperparameter Configurations",
            "text": "To achieve optimal results in our PRCA training, careful selection of hyperparameters is pivotal. The\nconfiguration settings employed in our experiment are stated in Table 2."
        },
        {
            "heading": "5 Results and Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Overall Performance",
            "text": "As delineated in Table 3, among the seventy-five configurations, our experimental results suggest that the inclusion of PRCA improves performance in seventy-one configurations. On average, we observe an enhancement of 3%, 6%, and 9% on the SQuAD, HotpotQA, and TopiOCQA datasets, respectively. This demonstrates that PRCA possesses robustness and can enhance the performance of different combinations of retrievers and generators on the ReQA task. As illustrated in Figure 3, the improvements rendered by PRCA to the generators are significant across all three datasets. Particularly on the TopiOCQA dataset, the average improvement for generator Vicuna across five different retrievers reaches 14%. Notably, when SimSCE is the retriever, the enhancement offered by PRCA is 20%.\nIn Figure 3, we notice that the improvement to\ngenerator performance by PRCA across the three datasets is incremental, while the original performance of the generators across the three datasets is decremental without PRCA, correlating directly with the complexity of the datasets. This is because when faced with more complex issues, such as multi-hop questions in HotpotQA and topic transitions in multi-turn QA in TopiOCQA, PRCA reserves and integrates critical information which is beneficial for generators from the retrieved documents. This attribute of PRCA alleviate issues where generators struggle with lengthy texts, failing to answer questions correctly or producing hallucinations, thus enhancing performance.\nHowever, the inclusion of PRCA has a negative effect on the performance of the generator T5 on the SQuAD dataset. This is because the SQuAD dataset is relatively simple, where the answer often directly corresponds to a phrase in the text. As an encoder-decoder architecture model, T5 tends to extract answers directly rather than infer in-depth based on the context. Therefore, without information distillation by PRCA from the retrieved\ndocuments, T5 performs well because its features fit well in handling this dataset, capable of directly extracting answers from the context. But under the effect of PRCA, the structure of the text might be altered, and T5\u2019s direct answer extraction may lead to some errors, thereby reducing performance.\nWhile in a few configurations, the characteristics of PRCA may have negative effects, for the vast majority of configurations, our experiments validate that under PRCA-based paradigm, PRCA can effectively enhance the performance in the ReQA task, demonstrating robustness."
        },
        {
            "heading": "5.2 Efficiency of PRCA",
            "text": "PRCA represents an effective approach for enhancing the performance of the ReQA task without significantly increasing computational demand. Its efficiency is manifested in optimizing parameters to achieve superior results and in simplifying input text, thereby aiding generators in managing complex text.\nParameter Efficiency Figure 4 portrays a comparative analysis between the generators, which gain the maximum improvements with PRCA, and the GPT-3.5 model which operates without PRCA, across 3 datasets. PRCA boasts roughly 0.4 billion parameters, the most significantly improved generators encompass about 7 billion parameters on average, while GPT-3.5 has approximately 1.75 trillion parameters. As demonstrated in Figure 4, with a marginal\nTable 4: PRCA inference speed test results.\nDataset Precision GPU Batch Size Inference Speed (token/s)\nPRCA float32 A100 1 126 PRCA float32 A100 2 231 PRCA float32 A100 4 492\nparameter increment, the performance of these generators improved by 12.0%, 27.1%, and 64.5% respectively. Hence, PRCA has great potential to be an efficient way to boost the performance of ReQA task while keeping computational resources consumption acceptable. During the inference process, a fully-trained PRCA will perform only standard forward propagation and hence introduce limited impact on inference latency. Our inference latency test on SQUAD was reported in Table 4. This low latency ensures that the system maintains a smooth process without significant delays after integrating PRCA, underscoring the high efficiency of PRCA in boosting system performance.\nInput Simplification As illustrated in Figure 5, we analyzed the relationship between reward and token count during reward-driven stage for a QA pair in the HotpotQA dataset, with and without PRCA. There\u2019s a discernible difference in the reward trajectories with and without PRCA. Both reward curves ascend with the increase in token\ncount, but the gradient of ascent with PRCA is noticeably steeper. This implies that when PRCA is in action, the generator reaches its optimal performance with a significantly reduced token count.\nUnder the influence of PRCA, the generator can derive the correct answer with approximately four times fewer tokens. This indicates that PRCA can distill the retrieved text while ensuring the quality of the generated answer. This simplification process filters out redundant information, thereby promoting the generator to extract answers more accurately using a more streamlined context. Moreover, the reduction in token count enables the generator to process text faster and produce outputs more promptly. Overall, PRCA\u2019s efficiency in information distillation greatly bolsters the generator\u2019s capacity to manage and interpret complex text."
        },
        {
            "heading": "5.3 Impact of Top-K Selection",
            "text": "We conducted parameter sensitivity experiments to observe the performance of PRCA when the number of retrieved relevant documents changes. The results presented in Figure 6 show that on the SQuAD dataset, both the performance with and without PRCA improve as the number of retrieved documents increases, while the addition of PRCA consistently provides a positive effect across different Top-K values. Since the dataset is relatively simple, with the increased likelihood of the correct answer being included in the retrieved documents, both trends exhibit an upward trajectory.\nIn contrast, without the implementation of PRCA, there is a noticeable drop in performance on the HotpotQA and TopiOCQA datasets when more\ndocuments are added. This decline is attributed to the model\u2019s diminishing capability to generate accurate answers to complex questions due to the rise in distracting information and the onset of hallucination problems. However, by implementing PRCA, these adverse effects are systematically alleviated, which not only reduces the onset of hallucinations but also enhances the generator\u2019s ability to handle complex queries amidst distractions.\nIn general, at different Top-K values, PRCA demonstrates positive effects across all three datasets, thereby illustrating the universal applicability of PRCA regardless of the quantity of retrieved documents."
        },
        {
            "heading": "5.4 Case Study",
            "text": "When answering the form of Mersenne primes problem, the retrieved text contains two distinct sources of information. One directly specifies the form as 2p-1, accurately reflecting the nature of Mersenne primes. The other source misguidedly introduces \u201cfactorial primes\u201d as an answer. Without PRCA\u2019s intervention, this diversion leads the generator astray, resulting in an erroneous answer of \u201cfactorial primes\u201d. However, when PRCA is engaged, it sifts through the information, prioritizing the accurate context. This refined context extraction steers the generator towards the correct answer.\nCase Study without and with PRCA\nQuestion: Of what form do Mersenne primes take? Golden Answer: 2p-1 Part of Retrieved Documents: [Golden Answer Source] Mersenne primes are prime numbers that are of the form\n:::: 2p-1,\nwhere p is an arbitrary prime. [Predicted Answer Source] The Sieve of Eratosthenes, attributed to Eratosthenes, is a simple method to compute primes, although the large primes found today with computers are not generated this way. are prime. Prime numbers of this form are known as ::::::: factorial ::::::: primes. Predicted Answer without PRCA: Factorial primes\nContext through PRCA: Mersenne primes are prime numbers that are of the form 2p-1, where p is an arbitrary prime. The Lucas\u2013Lehmer test is particularly fast for numbers of this form, so many of the largest primes found today are Mersenne primes. Predicted Answer with PRCA: 2p-1 Note: \u201c\u2013\u201d denotes key information relevant to the question, \u201c~\u201d represents predicted answers."
        },
        {
            "heading": "5.5 Ablation Study of PRCA",
            "text": "We assessed the impact of PRCA on three datasets using the configurations from section 5.2, which showed maximum improvements. The evaluation is conducted with and without the reward-driven stage to observe the impact of PRCA on the performance. As illustrated in Figure 7, without the reward-driven training stage, the effect of PRCA on the entire configuration becomes adverse because PRCA merely simplifies the text without discerning which information is beneficial for the generator to answer questions, resulting in the omission of useful text. In contrast, once the training process incorporates the reward-driven stage, the quality of the context becomes directly aligned with reward values, assisting PRCA in more effectively distilling pertinent information. Therefore, the rewarddriven stage is vital, allowing PRCA to retain key details while simplifying text, enhancing its overall effect."
        },
        {
            "heading": "6 Conclusion",
            "text": "In conclusion, this research successfully introduces a PRCA-based paradigm for ReQA tasks, tackling the inherent challenges of fine-tuning LLMs in the retrieval-enhancement framework, especially given their vast parameter size and closed-source natures. PRCA innovatively distills retrieved documents via generator rewards, leading to a marked improvement in the ReQA task\u2019s performance. Experimental outcomes consistently demonstrate the robustness and effectiveness of PRCA when paired with various retrievers and generators, indicating its potential to be widely deployed as an adapter on the ReQA task.\nLimitations\nWhile PRCA has shown effectiveness in improving ReQA task performance, it has limitations, including dependency on generators, convergence issues, and limited integration with retrievers. The reward during reinforcement learning training is derived from the generator, requiring PRCA retraining with different generators, which can be time-consuming. PRCA may also experience difficulties converging in a single training session, which impacts the stability and consistency of its performance. Lastly, PRCA\u2019s operation as a pluggable adapter limits its ability to train jointly with retrievers, which means if the retrieval quality is not up to par, PRCA\u2019s effectiveness could be compromised."
        },
        {
            "heading": "Acknowledgement",
            "text": "Supported by the Key Research and Development Program of Guangdong Province (grant No. 2021B0101400003) and Corresponding author is Jianzong Wang (jzwang@188.com)."
        }
    ],
    "title": "PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter",
    "year": 2023
}