{
    "abstractText": "We present ASPIRO, an approach for structured data verbalisation into short template sentences in zero to few-shot settings. Unlike previous methods, our approach prompts large language models (LLMs) to directly produce entity-agnostic templates, rather than relying on LLMs to faithfully copy the given example entities, or validating/crafting the templates manually. We incorporate LLM re-prompting, triggered by algorithmic parsing checks, as well as the PARENT metric induced consistency validation to identify and rectify template generation problems in real-time. ASPIRO, compared to direct LLM output, averages 66% parsing error rate reduction in generated verbalisations of RDF triples on the DART dataset. Our best 5-shot text-davinci-003 setup, scoring BLEU of 50.62, METEOR of 45.16, BLEURT of 0.82, NUBIA of 0.87, and PARENT of 0.8962 on the Rel2Text dataset, competes effectively with recent fine-tuned pre-trained language models.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Martin Vejvar"
        },
        {
            "affiliations": [],
            "name": "Yasutaka Fujimoto"
        }
    ],
    "id": "SP:6ded8dfafdbde56ec447a14653c60c14389daf98",
    "references": [
        {
            "authors": [
                "Oshin Agarwal",
                "Heming Ge",
                "Siamak Shakeri",
                "Rami Al-Rfou."
            ],
            "title": "Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
            "year": 2005
        },
        {
            "authors": [
                "Thiago Castro Ferreira",
                "Diego Moussallem",
                "Emiel Krahmer",
                "Sander Wubben."
            ],
            "title": "Enriching the WebNLG corpus",
            "venue": "Proceedings of the 11th International Conference on Natural Language Generation,",
            "year": 2018
        },
        {
            "authors": [
                "Bhuwan Dhingra",
                "Manaal Faruqui",
                "Ankur Parikh",
                "MingWei Chang",
                "Dipanjan Das",
                "William Cohen."
            ],
            "title": "Handling divergent reference texts when evaluating table-to-text generation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Ond\u0159ej Du\u0161ek",
                "David M. Howcroft",
                "Verena Rieser."
            ],
            "title": "Semantic noise matters for neural natural language generation",
            "venue": "Proceedings of the 12th International Conference on Natural Language Generation, pages 421\u2013426, Tokyo, Japan. Association for Com-",
            "year": 2019
        },
        {
            "authors": [
                "Ond\u0159ej Du\u0161ek",
                "Zden\u011bk Kasner."
            ],
            "title": "Evaluating semantic accuracy of data-to-text generation with natural language inference",
            "venue": "Proceedings of the 13th International Conference on Natural Language Generation, pages 131\u2013137, Dublin, Ireland. Association",
            "year": 2020
        },
        {
            "authors": [
                "Arash Einolghozati",
                "Anchit Gupta",
                "Keith Diedrick",
                "Sonal Gupta."
            ],
            "title": "Sound natural: Content rephrasing in dialog systems",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5101\u20135108, On-",
            "year": 2020
        },
        {
            "authors": [
                "Claire Gardent",
                "Anastasia Shimorina",
                "Shashi Narayan",
                "Laura Perez-Beltrachini."
            ],
            "title": "The WebNLG challenge: Generating text from RDF data",
            "venue": "Proceedings of the 10th International Conference on Natural Language Generation, pages 124\u2013133, San-",
            "year": 2017
        },
        {
            "authors": [
                "Vivek Gupta",
                "Maitrey Mehta",
                "Pegah Nokhiz",
                "Vivek Srikumar."
            ],
            "title": "INFOTABS: Inference on tables as semi-structured data",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2309\u20132324, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Peyman Heidari",
                "Arash Einolghozati",
                "Shashank Jain",
                "Soumya Batra",
                "Lee Callender",
                "Ankit Arun",
                "Shawn Mei",
                "Sonal Gupta",
                "Pinar Donmez",
                "Vikas Bhardwaj",
                "Anuj Kumar",
                "Michael White"
            ],
            "title": "Getting to production with few-shot natural language",
            "year": 2021
        },
        {
            "authors": [
                "Mihir Kale",
                "Abhinav Rastogi."
            ],
            "title": "Template guided text generation for task-oriented dialogue",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6505\u20136520, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Hassan Kane",
                "Muhammed Yusuf Kocyigit",
                "Ali Abdalla",
                "Pelkins Ajanoh",
                "Mohamed Coulibali."
            ],
            "title": "NUBIA: NeUral based interchangeability assessor for text generation",
            "venue": "Proceedings of the 1st Workshop on Evaluating NLG Evaluation, pages 28\u201337, On-",
            "year": 2020
        },
        {
            "authors": [
                "Zden\u011bk Kasner",
                "Ondrej Dusek."
            ],
            "title": "Neural pipeline for zero-shot data-to-text generation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3914\u20133932, Dublin, Ireland. As-",
            "year": 2022
        },
        {
            "authors": [
                "Zden\u011bk Kasner",
                "Ioannis Konstas",
                "Ondrej Dusek."
            ],
            "title": "Mind the labels: Describing relations in knowledge graphs with pretrained models",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics,",
            "year": 2023
        },
        {
            "authors": [
                "Pei Ke",
                "Haozhe Ji",
                "Yu Ran",
                "Xin Cui",
                "Liwei Wang",
                "Linfeng Song",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "JointGT: Graph-text joint representation learning for text generation from knowledge graphs",
            "venue": "Findings of the Association for Computational Linguis-",
            "year": 2021
        },
        {
            "authors": [
                "Moniba Keymanesh",
                "Adrian Benton",
                "Mark Dredze"
            ],
            "title": "What makes data-to-text generation hard for pretrained language models",
            "venue": "In Proceedings of the 2nd Workshop on Natural Language Generation,",
            "year": 2022
        },
        {
            "authors": [
                "Anirban Laha",
                "Parag Jain",
                "Abhijit Mishra",
                "Karthik Sankaranarayanan."
            ],
            "title": "Scalable micro-planned generation of discourse from structured data",
            "venue": "Computational Linguistics, 45(4):737\u2013763.",
            "year": 2019
        },
        {
            "authors": [
                "bakhsh",
                "Peter Clark"
            ],
            "title": "Self-refine: Iterative refinement with self-feedback",
            "year": 2023
        },
        {
            "authors": [
                "Eric J. Miller."
            ],
            "title": "An introduction to the resource description framework",
            "venue": "Journal of Library Administration, 34(3-4):245\u2013255.",
            "year": 2001
        },
        {
            "authors": [
                "Yasin Tarabar",
                "Ankit Gupta",
                "Tao Yu",
                "Yi Chern Tan",
                "Xi Victoria Lin",
                "Caiming Xiong",
                "Richard Socher",
                "Nazneen Fatema Rajani."
            ],
            "title": "DART: Opendomain structured data record to text generation",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Ehud Reiter"
            ],
            "title": "Building natural-language generation systems",
            "year": 1996
        },
        {
            "authors": [
                "Thibault Sellam",
                "Dipanjan Das",
                "Ankur Parikh."
            ],
            "title": "BLEURT: Learning robust metrics for text generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881\u20137892, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Noah Shinn",
                "Federico Cassano",
                "Beck Labash",
                "Ashwin Gopinath",
                "Karthik Narasimhan",
                "Shunyu Yao"
            ],
            "title": "Reflexion: Language agents with verbal reinforcement learning",
            "year": 2023
        },
        {
            "authors": [
                "Genta Indra Winata",
                "Andrea Madotto",
                "Zhaojiang Lin",
                "Rosanne Liu",
                "Jason Yosinski",
                "Pascale Fung."
            ],
            "title": "Language models are few-shot multilingual learners",
            "venue": "Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 1\u201315, Punta Cana,",
            "year": 2021
        },
        {
            "authors": [
                "Jiannan Xiang",
                "Zhengzhong Liu",
                "Yucheng Zhou",
                "Eric Xing",
                "Zhiting Hu."
            ],
            "title": "ASDOT: Any-shot datato-text generation with pretrained language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1886\u20131899, Abu",
            "year": 2022
        },
        {
            "authors": [
                "NUBIA (Kane"
            ],
            "title": "2020) is also a trained metric that combines multiple sub-metrics to assess the interchangeability or equivalence of two texts. On the surface, this metric generates a single score",
            "year": 2020
        },
        {
            "authors": [
                "Kasner"
            ],
            "title": "2023), we also report on the sub-metrics which are used for the total NB value: SS Semantic Similarity operates on a scale of 0-5",
            "year": 2023
        },
        {
            "authors": [
                "D Datasets D"
            ],
            "title": "DART DART dataset introduced by Nan et al. (2021) is a large <triple-set, sentence> pair dataset aggregated from WikiSQL, WikiTableQues",
            "year": 2021
        },
        {
            "authors": [
                "Kasner"
            ],
            "title": "2023) for the verbalisation task of single-triples focused on out-of-domain and previously unseen relations. It contains a total of 4,097 <relation",
            "year": 2023
        },
        {
            "authors": [
                "tro Ferreira"
            ],
            "title": "2018), which contains 354 unique relations in total. E Additional Experiments E.1 Automatic metrics for DART-SingleRDF We used DART-SingleRDF (\u00a7D.2",
            "year": 2018
        },
        {
            "authors": [
                "Kasner",
                "Dusek"
            ],
            "title": "2022) to evaluate lexical similarity using PARENT, BLEU and METEOR. The results, seen in Table 11, are marginal at best and we can only observe improvement in BLEU score, while PARENT F1 and METEOR",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Data-to-text task (Reiter, 1996) aims to build a faithful natural language interpretation of structured data such as relational tables or Resource Description Framework (RDF) triples (Miller, 2001). However, without proper context, the given structured data may not sufficiently represent the relationships between entities, leading to ambiguity (Du\u0161ek et al., 2019). To battle this, some works rely on fine-tuning pre-trained language models (PLMs) on task-specific datasets in supervised or semi-supervised ways (Ke et al., 2021; Agarwal et al., 2021), but the domain of the resulting system is limited and requires well-labelled training data (Keymanesh et al., 2022). In contrast to finetuning, Kasner and Dusek (2022) prove that zeroshot neural systems are a possible solution, where in-domain data is introduced via simple humancrafted templates for each unique relation in the\n1code available at github.com/vejvarm/ASPIRO.\nknowledge graph. Xiang et al. (2022) nullify the requirements for human labelling entirely by utilising GPT3-davinci (Brown et al., 2020), a large language model (LLM) with broad general knowledge, to disambiguate RDF triples into short sentences and automatically parse them into reusable sentence templates as an alternative to human-crafted templates. In this paper we introduce ASPIRO, a robust N -shot variant of the data disambiguation step presented by Xiang et al. (2022) and a promising alternative to fine-tuning PLMs for crafting RDF verbalisations (Kasner et al., 2023). At its core, ASPIRO uses simple rules to algorithmically flag errors in the templates (such as missing subject, multiple objects, etc.) and re-prompt the LLM until all errors are alleviated or maximum (N ) retries have been reached. We evaluate changes in automated metrics and reduction of parsing errors in different configurations of ASPIRO on DART (Nan et al., 2021) and Rel2Text (Kasner et al., 2023) and compare the original RDF verbalisation prompt used by Xiang et al. (2022) with our prompt focused on enforcing structured json output with intermediate fields as guidelines."
        },
        {
            "heading": "2 Related Work",
            "text": "Single triple verbalisation: Mainly leveraged for reducing ambiguity in structured data before a specific D2T task (Laha et al., 2019; Du\u0161ek and Kasner, 2020; Xiang et al., 2022) as well as transforming inputs to be better suited for existing NLG models (Gupta et al., 2020; Kasner and Dusek, 2022; Xiang et al., 2022), verbalisation templates fall into three main categories:\n1) human-crafted (Kale and Rastogi, 2020; Kasner and Dusek, 2022) 2) rule-based (Laha et al., 2019; Gupta et al., 2020) 3) neural model-based (Xiang et al., 2022; Kasner et al., 2023)\nASPIRO combines aspects of both 2) and 3).\nDelexicalization: Einolghozati et al. (2020) and Heidari et al. (2021) find that without delexicalization, generative models can produce incomplete representations of the entities and concepts in the structured data verbalisations, leading to misinterpretation and failures in production. Our JSON structured prompt (\u00a7G.2) enforces the LLM to directly produce named-entity agnostic templates.\n0-shot to N -shot: Our work is heavily inspired and builds upon the disambiguation step from Xiang et al. (2022), which is equivalent to 0-shot setting for our N -shot Generator. We also use their prompt (\u00a7G.1) as baseline against our JSON prompt (\u00a7G.2).\nRefining LLM outputs: Madaan et al. (2023) and Shinn et al. (2023) show that iterative prompting and chain-of-thought reasoning can significantly improve the outputs of LLMs. We lean on their findings in designing our ASPIRO pipeline. However, back and forth prompting of LLMs can be expensive, which we counterweight by using our Rule-based parser (\u00a73.1) and the PARENT (Dhingra et al., 2019) F1 score (\u00a73.2) as cost-efficient gateways to decide if additional prompting is necessary."
        },
        {
            "heading": "3 Methods",
            "text": "The proposed method (ASPIRO) revolves around the conversion of structured data samples into verbalisation templates using a two-stage pipeline: N - shot Generator (\u00a73.1) and Consistency Validator (\u00a73.2). The pipeline processes structured data samples, wherein each sample comprises of one or more RDF triples which share the same relation. ASPIRO (see Figure 1) starts with an initial prompt to verbally articulate the structured data. This is equivalent to prompting a single LLM directly. If the zeroth attempt isn\u2019t accurate, it will retry a maximum of N times, refining the previous completion based on parsing errors (\u00a73.1.2). Subsequently, the outputs are validated for consistency, ensuring faithful and reliable verbalisations. We explain the individual stages and their sub-modules in the sections below. Refer to Figure 1 for full pipeline and terminology on general input. Step-by-step flow of the pipeline and example on specific input are provided in section \u00a73.3 and Figure 2 respectively."
        },
        {
            "heading": "3.1 N -shot Generator",
            "text": "N -shot Generator further fractures into an LLM stack and a Rule-based parser. The LLM Stack is tasked with generating verbalisation attempts based on given initial prompt (\u00a7G.1). It does so with the help of the Rule-based parser. This parser checks the generated completions for structural accuracy, ensuring they adhere to expected patterns."
        },
        {
            "heading": "3.1.1 LLM Stack",
            "text": "The LLM stack is a sequence of N + 1 LLMs, indexed from 0 to N . L0 is responsible for the initial completion and each further retry shot, initiated by the Rule-based parser (\u00a73.1.2), increments the index by 1. Each Ln is instantiated separately and does not have to be the same model. Equation (1) shows the single completion for structured input sample x at shot n.\nyn = Ln(T (x)) (1)\nwhere T is a given prompt and can be either TI (initial) or TR (retry)."
        },
        {
            "heading": "3.1.2 Rule-based parser",
            "text": "A purely algorithmic module, which validates yn against a set of conditions {C} one by one. If yn does not pass the condition Ci, a respective parsing error is logged into set En. The aggregated rules for each given completion are formally given below (see \u00a7A for detailed Python implementation).\nC0 ... has exactly one \u2018<subject>\u2018 substring. C1 ... has exactly one \u2018<object>\u2018 substring. C2 ... has no other \u2018<...>\u2018 substrings.\nIf the parser identifies any error in the structure, the next LLM in the LLM stack is re-prompted with Retry prompt (\u00a7G.3) to generate new completion."
        },
        {
            "heading": "3.2 Consistency Validator",
            "text": "Even if the outputs from the N -shot Generator adhere to the structural patterns, they might still contain inaccuracies, such as hallucinated content. This module assesses the quality of the verbalisations, using the PARENT statistical metric (Dhingra et al., 2019). If PARENT F1 score is too low, the module will utilise an LLM with specialised Consistency prompt (\u00a7G.4) to improve the sentence.\n3.2.1 PARENTF1 threshold To gauge the quality of the completion yn from N - shot Generator, we set a minimal threshold (\u00b5) for the PARENT score of yn. The score is calculated\nusing eq. (3) against artificially constructed table and reference.\nFirst, we construct the respective hypothesis, table and reference entries:\nh = yn.replace([s, o], e) t = \u27e8e, r.split(\" \"), e\u27e9 \u03c1 = r\n(2)\nwhere \"<subject>\" and \"<object>\" are replaced with \"<entity>\" to prevent penalising order discrepancy between hypothesis and table.\nWe then calculate the PARENT F1 score using equation (3).\nF1(yn) = PARENT(h, \u03c1, t) (3)"
        },
        {
            "heading": "3.2.2 Consistency LLM",
            "text": "If the calculated PARENT score from \u00a73.2.1 is not sufficient, we call another LLM with prompt TC as in eq. (4).\nyC = LC(TC(r, yn)) (4)\nThe prompt TC is designed to guide LC to identify problems with the given completion, provide advice how to fix it and subsequently produce fixed completion in a structured json output. See \u00a7G.4 for full version of the prompt."
        },
        {
            "heading": "3.3 Stepwise Pipeline Formulation",
            "text": "Given a dataset of structured data samples {xr}r\u2208R, where xr = {xr1, xr2, ..., xrm} and xrj is a single RDF triple xrj = \u27e8srj , r, orj\u27e9 with relation r \u2208 R, the pipeline for one xr is as follows:\nStep 0 Set n = 0 and T r0 = TI(xr).\nStep 1 Calculate yrn using eq. (1).\nStep 2 Use \u00a73.1.2 to validate yrn against all conditions C. If errors (Ern) are found, run equation (5) and return to Step 1. Otherwise go to Step 3.\nT rn+1 = TR(xr, yrn, Ern) n = n+ 1\n(5)\nStep 3 Use \u00a73.2.1 and calculate F1(yrn) via eq. (3). If the calculated F1 score is lower than our chosen threshold 0 \u2264 \u00b5 \u2264 1, continue to Step 4. Otherwise, output current yrn as the final completion yr.\nStep 4 Use \u00a73.2.2 to get revised completion yrC .\nStep 5 Compute F1 scores of yrn and yrC using eq. (3) and take the completion with higher score via eq. (6) to produce the final completion yr.\nyr = argmax(F1(y)) y\u2208{yrn,yrC}\n(6)"
        },
        {
            "heading": "4 Experiments",
            "text": "The following sections show results on several setups of ASPIRO. In section \u00a74.1 we compare auto-\nmatic metrics on Rel2Text test set (\u00a7D.3) with Kasner et al. (2023)\u2019s fine-tuned BART-BASE models. In section \u00a74.2 we report on the number of parsing errors tagged by our Rule-based parser (\u00a73.1) on both DART (\u00a7D.1) and Rel2Text (\u00a7D.3) datasets. In \u00a74.3 we also provide brief ablation study of CV.\nSetup: For N -shot generator (\u00a73.1), L0 marks initial model choice and NxLn max N retry shots using model Ln. We limit our experiments to Ln being same for all N shots. For Consistency Validator (\u00a73.2), we set \u00b5 = 0.7 and only use it in some setups (marked by LC in brackets). For reference on LLMs used as L in ASPIRO setups, see Tab. 2.\nPrompts: While Retry prompt TR (\u00a7G.3) and Consistency prompt TC (\u00a7G.4) are constant across all our experiments, we compare two variants of the Initial prompt TI :\n(A) ASDOT: proposed by Xiang et al. (2022) in their Data Disambiguation step to produce short sentence representation of a given triple. (full form \u00a7G.1)\n(J) JSON: our proposed prompt, which enforces json-like output with auxiliary fields to guide the creation of named-entity agnostic templates directly. (full form \u00a7G.2)"
        },
        {
            "heading": "4.1 Automatic Metrics",
            "text": "We evaluate Automatic metrics on Rel2Text test set (\u00a7D.3) with 4 ASPIRO setups (see Table 1 for 5 run averages; Table 7 for standard deviations).\nASPIRO outperforms Kasner et al. (2023)\u2019s fewshot-fine-tuned PLMs on all metrics and is\ncompetitive to the full-training-set-fine-tuned fullrel2text model with ca 1-2 point reduction in BLEU, but 28 % points increase in BLEURT. This implies higher semantic similarity, however Semantic Similarity sub-score of NUBIA only shows small increments. Despite the overall NB score being same for all ASPIRO setups, the submetrics of NUBIA show steady improvement between our models. Most noticeable change is in the Contradiction percentage, which the 5-shot setting improves by ca 1.2 % points and further 2 % points by introducing JSON prompt, suggesting higher capacity to disambiguate the correct direction of relation between subject and object entities in the input triples. PARENT F1 score slightly favours the JSON prompted setups of ASPIRO, but only by ca 0.6 % points.\nAdditional experiments: For metric results and discussion on DART, see appendix \u00a7E.1. For full experiment results with fine-tuned pre-trained language models refer to (Kasner et al., 2023)."
        },
        {
            "heading": "4.2 Parsing Errors",
            "text": "Parsing error analysis does not require specific references from the dataset. After ASPIRO produces the verbalisation templates (yr), we run them through our Rule-based parser (\u00a73.1) to flag and count the number of errors. As source data (X), similar to (Xiang et al., 2022), we collect at most 2 triple examples for each unique relation in the dataset and use them to prompt our pipeline.\nParsing error counts: For DART (Table 3) we use the full dataset (\u00a7D.1), producing 4299 unique template sentences in each experiment run. In Rel2Text (Table 4) we only use the test split (\u00a7D.3) with 226 unique relations and G3.5 (T2) as base model with either (A)SDOT or (J)SON prompts and different N -shot Generator setups. For Rel2Text, we don\u2019t provide RR % as the reduction is evident from counts.\nDiscussion: Introducing N -shot Generator (\u00a73.1) shows significant reduction in parsing error counts (Tables 3 and 4) even with N = 1. In the 1 retry shot setting, GPT4 (G4) is most effective at reducing parsing errors. However, if we introduce up to 5 retry shots, we can see that gpt-3.5-turbo (G3.5T) reduces parsing errors further. The exception is (J)SON prompt on DART where G4 keeps the lead. Interestingly, while text-davinci-003 (G3.5) performs well as 0-shot model, it generally performs worse than G3.5T in N -shot settings, contrasted again on DART by J prompt. It is also evident that J prompt provides more robust 0-shot baseline compared to (A)SDOT prompt. The values in parentheses reveal that including Consistency Validation yields only slight reduction in error count."
        },
        {
            "heading": "4.3 Ablation of Consistency Validator",
            "text": "To investigate the efficacy of Consistency Validator, we conduct a brief ablation study on Rel2Text test set (\u00a7D.3). For statistical metrics (Table 5), CV provides only marginal gains. This effect may be attributed to the improvement of Contradiction score and degradation of Neutrality score, implying that CV moves the templates closer to general state-\nments with less informational value. Conversely, parsing errors (Table 6) are reduced notably by CV, with counts decreasing from 12 to 10 and 23 to 16."
        },
        {
            "heading": "5 Conclusion",
            "text": "We proposed and evaluated ASPIRO, a general domain-agnostic pipeline for verbalisation of single triple data entries to short template sentences, utilising rule-based re-prompting of LLMs. The pipeline comprises of N -shot Generator (\u00a73.1) and Consistency Validator (\u00a73.2). We show that ASPIRO compares to fine-tuned pre-trained language models\u2019 automatic scores on the Rel2Text test set (\u00a74.1) and significantly reduces the parsing error count in 0-shot outputs of LLMs (\u00a74.2). The ablation study (\u00a74.3) revealed that Consistency Validator of ASPIRO further reduces error counts, but does not significantly affect automatic metrics."
        },
        {
            "heading": "Limitations",
            "text": "Operational costs: When contrasted with 0-shot setting, ASPIRO significantly escalates the operational costs (see appendix \u00a7F) due to the repeated calls of the N -shot Generator and the lengthy Consistency prompt (\u00a7G.4) associated with the Consistency Validator (\u00a73.2). Following the brief ablation study of CV (\u00a74.3) and the cost analysis, it remains debatable whether the performance of the Consistency Validator reported in this paper justifies the additional expense incurred in prompting the LLM for the flagged examples.\nIsolated triples: Generating verbalisations from single isolated triples doesn\u2019t account for situations where context from other triples is necessary to fully interpret the final natural language verbalisation. As exemplified by the DART dataset, contextual integration is significant and should be explored further.\nBackup template: In instances where the parsing of the <subject> and <object> within the generated completion of the LLM proved unsuccessful, Xiang et al. (2022) introduced a general backup template as fallback. In our research, we did not use any backup templates and did not investigate their potential impact on automated metric scores. Nonetheless, it\u2019s important to acknowledge that within a production environment, the incorporation of a backup template is a fundamental necessity, warranting further assessment of its effects.\nDirection of relation: The capacity to accurately discern the correct direction of the relation between subject and object is a notable feature of Data-totext systems. In our experiments, we report on contradiction statistic (C %), which can roughly translate to measure this ability. Although ASPIRO generally shows to improve on this statistic, there are no specific guardrails to validate the ambiguity other than the general knowledge of the LLM itself.\nVariance of experiment runs: Due to the substantial expenses associated with prompting large language models (LLMs) and the considerable size of the DART dataset, each experiment on DART was conducted only once. The same is true for Rel2Text parsing error analysis in Table 4. It should be noted that, although the temperature parameter was uniformly set to 0 for all the employed LLMs, the underlying generative process remains reliant\non maximum likelihood estimation, which inherently leaves room for potential variation errors in our experimental results."
        },
        {
            "heading": "Ethics Statement",
            "text": "In the course of this research, we have employed various Generative Pre-trained Transformer models, including GPT3 davinci, InstructGPT textdavinci-003 and gpt-3.5-turbo-0301, and gpt-40314, each demonstrating inherent biases as outlined in their respective publications, which are listed in Table 2. These biases often favour popular opinions and can lead to a distortion in the model\u2019s outputs. This reflects the models\u2019 training on largescale internet text, which is not entirely neutral and contains biased or skewed perspectives. We acknowledge this limitation and highlight that despite implementing a pipeline designed to minimise the inclusion of unnecessary and irrelevant information, the potential for biased outcomes cannot be entirely eliminated."
        },
        {
            "heading": "A Rule-based Parser",
            "text": "Below is a code snippet of all rules checked by the Rule-based parser. For full implementation refer to our GitHub2. SUBJECT = \"<subject>\" OBJECT = \"<object>\" errors = []\n# SUBJECT entity errors: if SUBJECT not in text:\nerrors.append(TemplateErrors.NO_SUBJECT) elif text.count(SUBJECT) > 1:\nerrors.append(TemplateErrors. MULTIPLE_SUBJECTS)\n# OBJECT entity errors: if OBJECT not in text:\nerrors.append(TemplateErrors.NO_OBJECT) elif text.count(OBJECT) > 1:\nerrors.append(TemplateErrors.MULTIPLE_OBJECTS )\n# PLACEHOLDER mismatch errors # e.g. `<author>` instead of `<subject>`/`<\nobject>` if contains_illegal_placeholder(text):\nerrors.append(TemplateErrors. ILLEGAL_PLACEHOLDER)\n2https://github.com/vejvarm/ASPIRO/blob/ 0f86ead6218b0100aff650656ef1ca9a8e2e485c/parsing. py"
        },
        {
            "heading": "B Tools and Repositories",
            "text": "We used Python 3.9 and 3.10 to run our experiments with LangChain3 and OpenAI API4 for efficient work with large language model pipelines. Metrics were calculated using the following existing implementations:\n\u2022 BLEU/METEOR: GEM-metrics benchmark5\n\u2022 PARENT: multiprocessing variant from Cl\u00e9ment Rebuffel6\n\u2022 BLEURT: code from google-research7 with default BLEURT-20 checkpoint8\n\u2022 NUBIA: original NUBIA repository9\n3python.langchain.com 4platform.openai.com/docs/api-reference 5github.com/GEM-benchmark/GEM-metrics 6github.com/KaijuML/parent 7github.com/google-research/bleurt 8bleurt/blob/master/checkpoints.md 9github.com/wl-research/nubia"
        },
        {
            "heading": "C Metrics",
            "text": "For comparability of our automatic metric evaluations (\u00a74.1), we leverage most of the lexical and semantic similarity metrics used by Kasner et al. (2023). Below is a brief explanation of their significance.\nBLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) are metrics that quantify the lexical similarity between model-generated outputs and (typically human-produced) references by utilising n-gram overlap.\nPARENT (Dhingra et al., 2019) additionally assesses n-gram overlap of generated outputs with source structured data (i.e., table), which acts as an auxiliary reference beyond the reference text. This metric rewards instances where the hypothesis encapsulates all the information derived from the table, even if some elements are absent from the reference. Conversely, the PARENT score discriminates situations where the reference text contains supplementary information, which is absent in the structured data, implying the integration of external knowledge or embellishments in the reference.\nBLEURT (Sellam et al., 2020) is a trained metric that complements the above lexical similarity metrics by capturing semantic similarity.\nNUBIA (Kane et al., 2020) is also a trained metric that combines multiple sub-metrics to assess the interchangeability or equivalence of two texts. On the surface, this metric generates a single score (NB), that ranges between 0 and 1. Similar to Kasner et al. (2023), we also report on the sub-metrics which are used for the total NB value:\nSS Semantic Similarity operates on a scale of 0-5, where higher values suggest higher semantic similarity\nC% Contradiction percentage increases as the output and reference contradict each other in their meaning.\nN% Neutral percentage (also referred to as \"chance of irrelevancy\")10 increases if the output contains new information or information which is irrelevant to the reference.\nE% Entailment percentage increases as the information in reference is entailed by the model output.\n10github.com/wl-research/nubia"
        },
        {
            "heading": "D Datasets",
            "text": ""
        },
        {
            "heading": "D.1 DART",
            "text": "DART dataset introduced by Nan et al. (2021) is a large <triple-set, sentence> pair dataset aggregated from WikiSQL, WikiTableQuestions, WebNLG2017 and CleanedE2E with 62,659/6,980/12,552 samples in the train/dev/test sets respectively. All splits combined have total of 4299 unique relations. Each sample contains a set of up to 7 RDF triples with different relations, and human-crafted sentences as natural verbalisations of the given structured data."
        },
        {
            "heading": "D.2 DART-SingleRDF Dataset",
            "text": "As DART (\u00a7D.1) combines multiple relations within one sample, we opted to extract only samples with single triple in the input set from all DART splits and combine them into DARTSingleRDF subset. DART-SingleRDF contains a total of 2,947 <single-triple, sentence> entries with 1,439 unique relations."
        },
        {
            "heading": "D.3 Rel2Text",
            "text": "Rel2Text is specifically designed by Kasner et al. (2023) for the verbalisation task of single-triples focused on out-of-domain and previously unseen relations. It contains a total of 4,097 <relation, description, single-triple, verbalisation> samples. In \u00a74.1, we use the test split of Rel2Text11 with 616 total samples and 226 unique relations, unseen in the training set."
        },
        {
            "heading": "D.4 WebNLG",
            "text": "WebNLG is commonly used dataset by Gardent et al. (2017) where entries contain up to 7 triples extracted from DBpedia categories and labelled by human-crafted sentences. We specifically use the enrichment of WebNLG version 1.4 from (Castro Ferreira et al., 2018), which contains 354 unique relations in total."
        },
        {
            "heading": "E Additional Experiments",
            "text": ""
        },
        {
            "heading": "E.1 Automatic metrics for DART-SingleRDF",
            "text": "We used DART-SingleRDF (\u00a7D.2) as test set for automatic metric evaluation of ASPIRO. The Full results are reported in Table 8 and Reduced results in Table 9.\n11github.com/kasnerz/rel2text/tree/main/data/full\nFull: Results in Table 8 show slight or no discrepancies in the metrics between all the experiments, which could be attributed to variational error. Considering the high API model costs (\u00a7F), we did not run DART experiments multiple times to provide deviations. Instead, we reduce the data to only problematic samples by taking a subset of yr0 = L0(xr) generated templates which satisfy PARENTF1(y r 0) < 0.7. In other words, we take a subset of samples, for which outputs of 0-shot model in the respective sub-table were flagged as inconsistent by the Consistency Validator (\u00a73.2) using \u00b5 = 0.7. We report the same metric evaluation process in Table 9.\nDiscussion: For the Reduced evaluation in Table 9, we found that ASPIRO shows significant improvement only when (A)SDOT is initial prompt and only with 5-shot gpt-3.5-turbo setting. A point of interest is also the Neutral (irrelevance) score (N %), which the 5-shot setting generally increases, suggesting the N -shot setting is reducing the relevance of generated verbalisations to the references. For JSON prompts 1-shot gpt-4 setting has slight, albeit marginal lead over other settings."
        },
        {
            "heading": "E.2 Performance on WebNLG",
            "text": "We additionally evaluated performance on WebNLG (\u00a7D.4), following a similar approach as with Rel2Text in the main experiments (\u00a74). GPT-3.5-turbo-0301 is used as LLM instances of all calls to both N-shot generator LLM stack and Consistency Validator.\nParsing errors: We observed (Table 10) that for WebNLG, ASPIRO is generally not able to fix any errors and CV conversely increases the number of total errors, making 3 of the templates more \"flawed\" than without CV.\nAutomatic metrics: We compare the templates generated by ASPIRO to the manually crafted templates from Kasner and Dusek (2022) to evaluate lexical similarity using PARENT, BLEU and METEOR. The results, seen in Table 11, are marginal at best and we can only observe improvement in BLEU score, while PARENT F1 and METEOR are highest for zero-shot setting. Due to time restraints, we did not include BLEURT and NUBIA evaluations.\nConclusion: Contrary to our original belief, we can conclude that ASPIRO pipeline does not pro-\nvide significant improvement over 0-shot method on the WebNLG dataset."
        },
        {
            "heading": "F Run time and cost of ASPIRO",
            "text": "$191 US is the overall expenditure for OpenAI API calls during our experiments. However, it is important to note that we made many redundant API calls in the development of ASPIRO so the necessary costs should be lower. The main bulk of the costs amounts to GPT3-davinci and textdavinci-003 calls."
        },
        {
            "heading": "F.1 Run time analysis",
            "text": "Table 12 presents the average run time in seconds across five experimental runs using the WebNLG dataset, which comprises 354 unique relations. This translates to a total of 354 calls required for the 0x model, a zero-shot call to the initial model (GPT3.5-turbo). Subsequent retry shots only need as many calls as there are templates with parsing errors.\nCumulative mean time: Given the nature of our experiments, where subsequent runs leverage results from preceding runs (for instance, the 2-shot run utilises results from the 1-shot run and only re-prompts those with parsing errors), we introduce Cumulative mean time to illustrate the total time necessary to execute all shots of the respective experiment."
        },
        {
            "heading": "F.2 Estimated API call costs",
            "text": "For a \"worst-case-scenario\" cost estimate of ASPIRO (all templates are tagged for retry shot), we made calculations for the GPT3.5-turbo model, which charges $0.002 per 1000 tokens (as of the time of our experiments). Table 13 provides cost estimations for the experiments conducted on the Rel2Text, WebNLG, and DART datasets using GPT3.5-turbo. To derive the costs associated with the GPT3-davinci or text-davinci-003 models (charged at $0.02 per 1000 tokens), multiply the presented Table 13 values by a factor of 10."
        },
        {
            "heading": "G Prompt Templates",
            "text": "G.1 Initial Prompt: ASDOT\nTable : Michael | birth Place | USA Text : Michael was born in the USA.\nTable : First Clearing | location | On NYS 52 1 Mi. Youngsville Text : First Clearing is located at On NYS 52 1 Mi. Youngsville.\nTable : Abilene Regional Airport | city Served | Abilene Texas Text : Abilene Regional Airport serves Abilene Texas.\nTable : Alfred Moore Scales | active Years Start Date | 1875\u221203\u221204 Text : Alfred Moore Scales started to be active on 1875\u221203\u221204.\n{ example_table_str } Text :\nG.2 Initial Prompt: JSON\n### example: ``` json {{\" intput_data \": [[\" World Trade Center \", \" architect \",\n\"Minoru Yamasaki\"], [\"Seymour Centre\", \" architect \", \"Allen Jack+ Cottier \"]]}}\n```\n``` json {{\" subject_entities \": [\"World Trade Center \", \"Seymour Centre\"], \" relation \": \" architect \", \" object_entities \": [\"Minoru Yamasaki\", \"Allen Jack+ Cottier \"], \" agnostic_template \": \"<object> is the architect of < subject >.\" }} ``` ###\n### your task : ``` json {{\" input_data \": { example_rdf_list }}} ```\n``` json {{\nG.3 Retry Prompt12\n```` Completion {completion} ````\nAbove, the Completion did not satisfy the constraints given in the Prompt. Details : `{error}` Please try again .\n```` Prompt {prompt} ````"
        },
        {
            "heading": "G.4 Consistency Prompt",
            "text": "Your task is to evaluate a [ string ] based on the following [ rules ] and output a `valid` flag which is either 1 ([ string ] complies with [ rules ]) or 0 ([ string ] breaks some [ rules ]) and an ` advice` which explains in one short sentence how to fix [ string ] to comply with all [ rules ] ( if valid ==1, advice=\"\") . You will also output a ` valid_string `, which will follow the advice and\ncomply to the rules ( if valid ==1, valid_string =[ string ])\n[ rules ]: \u2212 [ string ] must contain exactly one `<subject>` substring . \u2212 [ string ] must contain exactly one `<object>` substring . \u2212 [ string ] must **not contain any named entities or\nspecific references other than `<subject>` and `<object>`**.\n\u2212 [ string ] should align with the semantic meaning of the [ relation ] provided , without adding or implying any information beyond it . \u2212 [ string ] must concisely and factually represent the information in [ relation ], without embellishment or unnecessary details . [ string ]: {template} [ relation ]: {data} ``` json {{\n\" valid \": \"advice \": \" valid_string \":\n}} ```\n12We use a modification of LangChain\u2019s NAIVE_COMPLETION_RETRY_WITH_ERROR prompt from RetryWithErrorOutputParser (langchain/output_parsers/retry.py)"
        }
    ],
    "title": "ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation",
    "year": 2023
}