{
    "abstractText": "Self-supervised representation learning on textattributed graphs, which aims to create expressive and generalizable representations for various downstream tasks, has received increasing research attention lately. However, existing methods either struggle to capture the full extent of structural context information or rely on task-specific training labels, which largely hampers their effectiveness and generalizability in practice. To solve the problem of self-supervised representation learning on textattributed graphs, we develop a novel GraphCentric Language model \u2013 Grenade. Specifically, Grenade exploits the synergistic effect of both pre-trained language model and graph neural network by optimizing with two specialized self-supervised learning algorithms: graphcentric contrastive learning and graph-centric knowledge alignment. The proposed graphcentric self-supervised learning algorithms effectively help Grenade to capture informative textual semantics as well as structural context information on text-attributed graphs. Through extensive experiments, Grenade shows its superiority over state-of-the-art methods. Implementation is available at https://github.com/ bigheiniu/GRENADE.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yichuan Li"
        },
        {
            "affiliations": [],
            "name": "Kaize Ding"
        },
        {
            "affiliations": [],
            "name": "Kyumin Lee"
        }
    ],
    "id": "SP:e19f28c918cb8bf62c8f778c827758f2406edd0b",
    "references": [
        {
            "authors": [
                "Jinsong Chen",
                "Kaiyuan Gao",
                "Gaichao Li",
                "Kun He"
            ],
            "title": "Nagphormer: A tokenized graph transformer for node classification in large graphs",
            "year": 2023
        },
        {
            "authors": [
                "Eli Chien",
                "Wei-Cheng Chang",
                "Cho-Jui Hsieh",
                "HsiangFu Yu",
                "Jiong Zhang",
                "Olgica Milenkovic",
                "Inderjit S Dhillon."
            ],
            "title": "Node feature extraction by self-supervised multi-scale neighborhood prediction",
            "venue": "arXiv preprint arXiv:2111.00064.",
            "year": 2021
        },
        {
            "authors": [
                "Arman Cohan",
                "Sergey Feldman",
                "Iz Beltagy",
                "Doug Downey",
                "Daniel S Weld."
            ],
            "title": "Specter: Document-level representation learning using citation-informed transformers",
            "venue": "arXiv preprint arXiv:2004.07180.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Kaize Ding",
                "Jianling Wang",
                "James Caverlee",
                "Huan Liu."
            ],
            "title": "Meta propagation networks for graph few-shot semi-supervised learning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.",
            "year": 2022
        },
        {
            "authors": [
                "Kaize Ding",
                "Yancheng Wang",
                "Yingzhen Yang",
                "Huan Liu."
            ],
            "title": "Eliciting structural and semantic global knowledge in unsupervised graph contrastive learning",
            "venue": "arXiv preprint arXiv:2202.08480.",
            "year": 2022
        },
        {
            "authors": [
                "Kaize Ding",
                "Yancheng Wang",
                "Yingzhen Yang",
                "Huan Liu."
            ],
            "title": "Eliciting structural and semantic global knowledge in unsupervised graph contrastive learning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 7378\u20137386.",
            "year": 2023
        },
        {
            "authors": [
                "Kaize Ding",
                "Zhe Xu",
                "Hanghang Tong",
                "Huan Liu."
            ],
            "title": "Data augmentation for deep graph learning: A survey",
            "venue": "ACM SIGKDD Explorations Newsletter.",
            "year": 2022
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "Simcse: Simple contrastive learning of sentence embeddings",
            "venue": "arXiv preprint arXiv:2104.08821.",
            "year": 2021
        },
        {
            "authors": [
                "Will Hamilton",
                "Zhitao Ying",
                "Jure Leskovec."
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross B. Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Weihua Hu",
                "Matthias Fey",
                "Marinka Zitnik",
                "Yuxiao Dong",
                "Hongyu Ren",
                "Bowen Liu",
                "Michele Catasta",
                "Jure Leskovec."
            ],
            "title": "Open graph benchmark: Datasets for machine learning on graphs",
            "venue": "Advances in Neural",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Jaiswal",
                "Ashwin Ramesh Babu",
                "Mohammad Zaki Zadeh",
                "Debapriya Banerjee",
                "Fillia Makedon."
            ],
            "title": "A survey on contrastive self-supervised learning",
            "venue": "Technologies, 9(1):2.",
            "year": 2020
        },
        {
            "authors": [
                "Bowen Jin",
                "Wentao Zhang",
                "Yu Zhang",
                "Yu Meng",
                "Xinyang Zhang",
                "Qi Zhu",
                "Jiawei Han."
            ],
            "title": "Patton: Language model pretraining on text-rich networks",
            "venue": "CoRR, abs/2305.12268.",
            "year": 2023
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling."
            ],
            "title": "Semisupervised classification with graph convolutional networks",
            "venue": "ICLR.",
            "year": 2017
        },
        {
            "authors": [
                "Namkyeong Lee",
                "Junseok Lee",
                "Chanyoung Park."
            ],
            "title": "Augmentation-free self-supervised learning on graphs",
            "venue": "AAAI.",
            "year": 2022
        },
        {
            "authors": [
                "Guohao Li",
                "Matthias M\u00fcller",
                "Bernard Ghanem",
                "Vladlen Koltun."
            ],
            "title": "Training graph neural networks with 1000 layers",
            "venue": "ICML.",
            "year": 2021
        },
        {
            "authors": [
                "Junnan Li",
                "Pan Zhou",
                "Caiming Xiong",
                "Steven C.H. Hoi."
            ],
            "title": "Prototypical contrastive learning of unsupervised representations",
            "venue": "9th International Conference on Learning Representations, ICLR 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Costas Mavromatis",
                "Vassilis N Ioannidis",
                "Shen Wang",
                "Da Zheng",
                "Soji Adeshina",
                "Jun Ma",
                "Han Zhao",
                "Christos Faloutsos",
                "George Karypis."
            ],
            "title": "Train your own gnn teacher: Graph-aware distillation on textual graphs",
            "venue": "arXiv preprint arXiv:2304.10668.",
            "year": 2023
        },
        {
            "authors": [
                "Miller McPherson",
                "Lynn Smith-Lovin",
                "James M Cook."
            ],
            "title": "Birds of a feather: Homophily in social networks",
            "venue": "Annual Review of Sociology, (1).",
            "year": 2001
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean."
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "arXiv preprint arXiv:1301.3781.",
            "year": 2013
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "arXiv preprint arXiv:2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Malte Ostendorff",
                "Nils Rethmeier",
                "Isabelle Augenstein",
                "Bela Gipp",
                "Georg Rehm."
            ],
            "title": "Neighborhood contrastive learning for scientific document representations with citation embeddings",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in",
            "year": 2022
        },
        {
            "authors": [
                "DeVito",
                "Martin Raison",
                "Alykhan Tejani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala."
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "NeurIPS.",
            "year": 2019
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg",
                "J. Vanderplas",
                "A. Passos",
                "D. Cournapeau",
                "M. Brucher",
                "M. Perrot",
                "E. Duchesnay"
            ],
            "title": "Scikit-learn: Machine learning in Python",
            "year": 2011
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "arXiv preprint arXiv:1908.10084.",
            "year": 2019
        },
        {
            "authors": [
                "Jingbo Shang",
                "Xinyang Zhang",
                "Liyuan Liu",
                "Sha Li",
                "Jiawei Han."
            ],
            "title": "Nettaxo: Automated topic taxonomy construction from text-rich network",
            "venue": "TheWebConf.",
            "year": 2020
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov."
            ],
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "venue": "Journal of Machine Learning Research.",
            "year": 2014
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton."
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research, (11).",
            "year": 2008
        },
        {
            "authors": [
                "Petar Velickovic",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "6th International Conference on Learning Representations, ICLR 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-ofthe-art natural language processing",
            "year": 2019
        },
        {
            "authors": [
                "Zhuofeng Wu",
                "Sinong Wang",
                "Jiatao Gu",
                "Madian Khabsa",
                "Fei Sun",
                "Hao Ma."
            ],
            "title": "Clear: Contrastive learning for sentence representation",
            "venue": "arXiv preprint arXiv:2012.15466.",
            "year": 2020
        },
        {
            "authors": [
                "Junhan Yang",
                "Zheng Liu",
                "Shitao Xiao",
                "Chaozhuo Li",
                "Defu Lian",
                "Sanjay Agrawal",
                "Amit Singh",
                "Guangzhong Sun",
                "Xing Xie."
            ],
            "title": "Graphformers: Gnn-nested transformers for representation learning on textual graph",
            "venue": "Advances in Neural Information Processing",
            "year": 2021
        },
        {
            "authors": [
                "Michihiro Yasunaga",
                "Jure Leskovec",
                "Percy Liang."
            ],
            "title": "Linkbert: Pretraining language models with document links",
            "venue": "arXiv preprint arXiv:2203.15827.",
            "year": 2022
        },
        {
            "authors": [
                "Xinyang Zhang",
                "Chenwei Zhang",
                "Xin Luna Dong",
                "Jingbo Shang",
                "Jiawei Han."
            ],
            "title": "Minimally-supervised structure-rich text categorization via learning on textrich networks",
            "venue": "TheWebConf.",
            "year": 2021
        },
        {
            "authors": [
                "Jianan Zhao",
                "Meng Qu",
                "Chaozhuo Li",
                "Hao Yan",
                "Qian Liu",
                "Rui Li",
                "Xing Xie",
                "Jian Tang."
            ],
            "title": "Learning on large-scale text-attributed graphs via variational inference",
            "venue": "arXiv preprint arXiv:2210.14709.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Zhu",
                "Yanling Cui",
                "Yuming Liu",
                "Hao Sun",
                "Xue Li",
                "Markus Pelger",
                "Tianqi Yang",
                "Liangjie Zhang",
                "Ruofei Zhang",
                "Huasha Zhao."
            ],
            "title": "Textgnn: Improving text encoder via graph neural network in sponsored search",
            "venue": "WWW.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Text-Attributed Graph (TAG) (Yang et al., 2021) (a.k.a., Textual Graph) has been widely used for modeling a variety of real-world applications, such as information retrieval (Cohan et al., 2020; Yang et al., 2021), product recommendation (Zhu et al., 2021) and many more. In TAG, each node represents a text document, while the relationships among these text nodes are depicted by the edges. For instance, in citation networks, text nodes represent academic papers, and edges are the citation relationship between different papers. To conduct different analytics tasks on TAG, the key is to learn expressive node representations for the text nodes.\n\u2217Corresponding Author.\nRecent research has demonstrated that selfsupervised learning (SSL) can substantially improve the effectiveness of representation learning on text data (Reimers and Gurevych, 2019; Gao et al., 2021; Wu et al., 2020) without using human supervision. Those methods are commonly learned under the assumption that text documents are independently and identically distributed (i.i.d.), which neglects the structural interdependencies among text nodes on TAG. However, the interdependencies between different text documents can provide valuable insights for understanding their semantic relationships. Take citation networks as an example, those academic papers (text nodes) that have citation relationships often share similar topics. Hence, it is necessary for SSL models to account for not only textual semantics but also structural context information.\nIn fact, self-supervised representation learning on TAG remains in its infancy: (i) Though recent research efforts (Zhao et al., 2022; Chien et al., 2021; Cohan et al., 2020; Yasunaga et al., 2022) try to empower pre-trained language models (PLM) with structural context information, most of them still stay superficial by designing local structuredependent SSL objectives. For example, both GIANT (Chien et al., 2021) and SPECTER (Cohan et al., 2020) train the language model by inferring the local neighborhood based on representations of text nodes. However, simply relying on those SSL objectives cannot help the PLM fully understand complex graph structures, especially compared to models like graph neural networks (GNN) (Kipf and Welling, 2017; Velickovic et al., 2018; Hamilton et al., 2017; Ding et al., 2022a); (ii) Meanwhile, another line of research (Mavromatis et al., 2023; Zhao et al., 2022) try to combine the advantages of both PLM and GNN by distilling the knowledge from one to the other (Hinton et al., 2015) and have shown promising results. Nonetheless, one major issue is that those methods are task-specific (e.g.,\nsemi-supervised node classification) and require human-annotated labels to enable knowledge distillation. Such an inherent limitation jeopardizes the versatility of their models for handling different and even unseen downstream tasks, which runs counter to the goal of SSL.\nTo go beyond the existing learning paradigms and capture informative textual semantic and graph structure information, we develop a new model for self-supervised learning on TAG, namely Grenade (Graph-Centric Language Model). Grenade is built with a PLM encoder along with an adjuvant GNN encoder that provides complementary knowledge for it. More importantly, Grenade is learned through two new self-supervised learning algorithms: Graph-Centric Contrastive Learning (GC-CL), a structure-aware and augmentation-free contrastive learning algorithm that improves the representation expressiveness by leveraging the inherent graph neighborhood information; and GraphCentric Knowledge Alignment (GC-KA), which enables the PLM and GNN modules to reinforce each other by aligning their learned knowledge encoded in the text node representations.\nSpecifically, GC-CL enforces neighboring nodes to share similar semantics in the latent space by considering them as positive pairs. Even without using data augmentation, GC-CL performs node-wise contrastive learning to elicit the structural context information from TAG. In the meantime, GC-KA bridges the knowledge gap between PLM and GNN by performing dual-level knowledge alignment on the computed representations: at the node level, we minimize the distance between the representations learned from two encoders that focus on different modalities. At the neighborhood level, we minimize the discrepancy between two neighborhood similarity distributions computed from PLM and GNN. By virtue of the two proposed graph-centric selfsupervised learning algorithms, we are able to learn Grenade that can generate expressive and generalizable representations for various downstream tasks without using any human supervision. In summary, our work has the following contributions:\n\u2022 We develop Grenade, which is a graph-centric language model that addresses the underexplored problem of self-supervised learning on TAG. \u2022 We propose two new self-supervised learning algorithms for TAG, which allow us to perform contrastive learning and knowledge alignment in a graph-centric way.\n\u2022 We conduct extensive experiments to show that our model Grenade significantly and consistently outperforms state-of-the-art methods on a wide spectrum of downstream tasks."
        },
        {
            "heading": "2 Problem Definition",
            "text": "Notations. We utilize bold lowercase letters such as d to represent vectors, bold capital letters like W to denote matrices and calligraphic capital letters like W to represent sets. Let G = (A,D) denote a text-attributed graph with adjacency matrix A \u2208 {0, 1}|D|\u00d7|D| and text set D. The Aij = 1 when there is a connection between node i and j. Each node i represents a text document which consists of a sequence of tokens Di = {wv}|Di|v=0. Problem 1 Given an input text-attributed graph (TAG) denoted as G=(A,D), our goal is to learn a graph-centric language model PLM(\u00b7), that can generate expressive and generalizable representation for an arbitray node i on G: di=PLM(Di). Note that the whole learning process is performed solely on the input graph G without the utilization of human-annotated labels."
        },
        {
            "heading": "3 Proposed Approach: Graph-Centric",
            "text": "Language Model (Grenade)\nTo learn expressive representations from TAG in a self-supervised learning manner, we propose our Graph-Centric Language Model Grenade, which bridges the knowledge gap between Pre-trained Language Model (PLM) and Graph Neural Network (GNN). By optimizing two distinct encoders with a set of novel self-supervised learning algorithms, the PLM encoder and GNN encoder mutually reinforce each other, and we can finally derive our GraphCentric Language Model (Grenade). GNN. The overall framework is shown in Fig. 1."
        },
        {
            "heading": "3.1 Model Architecture",
            "text": "Our proposed model Grenade is composed of a Pretrained Language Model (PLM) along with a Graph Neural Network (GNN), which are optimized by a set of novel self-supervised learning algorithms. We first introduce the details about those two essential components as follows:\nPLM Encoder. The primary component PLM(\u00b7) is a BERT (Devlin et al., 2018) based text encoder that projects a sequence of tokens Di into a vectorized text node representation di:\ndi=PLM(Di), (1)\nwhere di is the hidden representation of the [CLS] token computed from the last layer of the PLM encoder.\nGNN Encoder. As an adjuvant component, the GNN encoder GNN(\u00b7) is built with a stack of messagepassing based GNN layers, which compute the node i\u2019s representation by iteratively aggregating and transforming the feature information from its neighborhood (Hamilton et al., 2017). For each node i, its representation learned from a L-layer GNN encoder can be denoted as:\nei=E[i,:],E=GNN(E 0,A) (2)\nwhere the input node feature matrix E0 is obtained from the hidden representations of [CLS] token from the last layer of a pre-trained BERT model."
        },
        {
            "heading": "3.2 Graph-Centric Contrastive Learning",
            "text": "In order to improve the learning capability of those two encoders without using any human-annotated labels, one prevailing way is to conduct contrastive learning from either the text perspective (Gao et al., 2021) or graph perspective (Ding et al., 2022c). However, most of the existing contrastive learning methods have the following two limitations: (1) conventional instance-level contrastive learning methods merely encourage instance-wise discrimination (Li et al., 2021b; Ding et al., 2023), which neglects the property of TAG, i.e., the relational information among text nodes. Hence, those instances that share similar semantics may be undesirably pushed away in the latent space; (2) existing methods commonly rely on arbitrary augmentation functions to generate different augmented views\nfor applying contrastive learning, while those augmentations may unexpectedly disturb the semantic meaning of the original instance (Lee et al., 2022).\nTo counter the aforementioned issues, we propose a new graph-centric contrastive learning (GC-CL) algorithm, which is structure-aware and augmentation-free. GC-CL exploits inherent graph knowledge from TAG and can be applied to both the PLM encoder and GNN encoder. As suggested by the Homophily principle (McPherson et al., 2001), neighboring nodes commonly share similar semantics, meaning that their representations should also be close to each other in the latent space. Based on the PLM representation of node i, its K-hop neighboring nodes N (i), and the node i excluded mini-batch instances B(i), the GC-CL objective for PLM can be defined as follows:\nLGC-CL1= \u22121 |N (i)| \u2211\np\u2208N (i)\nlog esim(di,dp)/\u03c4\u2211\nj\u2208C(i) e sim(di,dj)/\u03c4\n,\n(3) where \u03c4 denotes the temperature and sim(\u00b7, \u00b7) represents the cosine similarity function. Here C(i) = N (i) \u222a B(i). Note that for node i, we consider its PLM representation di as the query instance. The positive instances are the representations of node i\u2019s K-hop neighboring nodes {dp|p \u2208 N (i)}. Meanwhile, the negative instances are the representations of other text nodes excluding i within the same mini-batch {dj |j \u2208 B(i)}.\nSimilar to the PLM encoder, we also apply our GC-CL algorithm to the GNN encoder GNN(\u00b7). Specif-\nically, the objective function is defined as follows:\nLGC-CL2= \u22121 |N (i)| \u2211\np\u2208N (i)\nlog esim(ei,ep)/\u03c4\u2211\nj\u2208C(i) e sim(ei,ej)/\u03c4\n,\n(4) where ei is the query instance. The positive instances are {ep|p \u2208 N (i)} and the negative instances are {ej |j \u2208 B(i)}.\nApart from the conventional instance-level contrastive learning counterparts, our graph-centric contrastive learning also enforces neighboring nodes to share similar representations. In a sense, this self-supervised learning algorithm is analogous to performing link prediction task based on the representations learned from the PLM encoder, which inherently elicits informative graph knowledge during the learning process."
        },
        {
            "heading": "3.3 Graph-Centric Knowledge Alignment",
            "text": "In this work, our ultimate goal is to learn expressive and generalizable representations that encode informative textual semantics within each text node as well as the relational information among nodes. However, individually conducting the graph-centric contrastive learning on either PLM or GNN is not enough due to the lack of knowledge exchange between them. To better align and enhance the knowledge captured by the PLM and GNN encoders, we propose a dual-level graph-centric knowledge alignment algorithm for TAG, which includes Node-Level Knowledge Alignment (ND-KA) and NeighborhoodLevel Knowledge Alignment (NBH-KA).\nNode-Level Knowledge Alignment. Different from the previously introduced graph-centric contrastive learning, which only focuses on singlemodal contrasting, ND-KA tries to align the knowledge across the two encoders by performing graphcentric contrastive learning in a cross-modal form. For each node i, based on its representations learned from the PLM encoder and GNN encoder (i.e., di and ei, respectively), we formulate the objective of ND-KA as follows:\nLND-KA= \u22121\n|N\u0303 (i)| \u2211 p\u2208N\u0303 (i) ( log esim(ei,dp)/\u03c4\u2211 j\u2208C\u0303(i) e sim(ei,dj)/\u03c4\n+ log esim(di,ep)/\u03c4\u2211\nj\u2208C\u0303(i) e sim(di,ej)/\u03c4\n) /2,\n(5) where N\u0303 (i) = {i} \u222a N (i) and C\u0303(i) = N\u0303 (i) \u222a B(i). Note that for node i, we first consider ei\nthat is learned from the GNN encoder as the query, then construct the positive and negative instances based on the representations learned from the PLM encoder. Specifically, the positive instances include both the representation of node i as well as the representations of i\u2019s K-hop neighboring nodes (i.e., {dp|p \u2208 N\u0303 (i)}), and the negative instances are the representations of other instances within the same mini-batch {dj |j \u2208 B(i)}. In the meantime, we also consider the di as the query and construct its corresponding positive and negative instances in the same way. Here we omit the illustration for simplicity.\nBy virtue of the proposed ND-KA algorithm, the representations of the same node learned from two separate encoders will be pulled together in the latent space. In the meantime, ND-KA also encourages neighboring nodes to have similar representations across different modalities.\nNeighborhood-Level Knowledge Alignment. To further facilitate knowledge alignment between PLM and GNN, we propose Neighborhood-Level Knowledge Alignment (NBH-KA) to align the neighborhood similarity distributions learned from the two encoders. Specifically, NBH-KA first computes the neighborhood similarity distribution between the query node i and its K-hop neighboring nodes N (i) as well as the rest nodes within the same minibatch B(i) for each encoder. Then we minimize the KL-divergence between the two distributions to align the knowledge between two encoders. The corresponding learning objective is:\nLNBH-KA= ( KL(PPLM(i)||P\u03b3(i))\n+ KL(PGNN(i)||PPLM(i)) ) /2,\nPPLM(i)=softmaxj\u2208C(i)(sim(di,dj)/\u03c4), PGNN(i)=softmaxj\u2208C(i)(sim(ei, ej)/\u03c4),\n(6)\nwhere PPLM(i) and PGNN(i) are the neighborhood similarity distributions for PLM encoder and GNN encoder respectively. From a certain perspective, our NBH-KA algorithm can be considered a selfsupervised form of knowledge distillation. Specifically, NBH-KA leverages the neighborhood information as self-supervision to guide the knowledge alignment process. Moreover, we conduct two-way knowledge alignment across two encoders, which is different from original knowledge distillation."
        },
        {
            "heading": "3.4 Model Learning",
            "text": "In order to learn our graph-centric language model Grenade on TAG without using human-annotated labels, we jointly optimize the proposed graphcentric contrastive learning and knowledge alignment algorithms. For the sake of simplicity, we define the overall training loss as follows:\nL=LGC-CL1 + LGC-CL2 + LND-KA + LNBH-KA. (7)\nOnce the training is finished, we can freeze the parameters of the PLM encoder and use it to compute the representations of each text node with a forward pass. The computed representations can be further used for different downstream tasks."
        },
        {
            "heading": "4 Experiment",
            "text": "To evaluate the effectiveness of our approach Grenade, we conduct comprehensive experiments on different datasets and various downstream tasks."
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Evaluation Datasets. We evaluate the generalizability of the representations computed from different methods on three Open Graph Benchmark (OGB) (Hu et al., 2020) datasets: ogbn-arxiv, ogbn-products, ogbl-citation2. These datasets are utilized to evaluate the performance of few-shot and full-shot node classification, node clustering, and link prediction tasks. It should be noted that ogbnarxiv and ogbn-products datasets are not originally designed for link prediction evaluation. Therefore, we create two link prediction tasks based on these two datasets, respectively. Furthermore, we incorporate obgl-citation2 into our node classification experiment. The statistical information of the datasets is shown in Tab. 1. More comprehensive information regarding the dataset extension can be found in Appendix A.\nBaseline Methods. The baseline methods included in our study encompass three categories: (1) Untuned representations: BERT (Devlin et al., 2018) and OGB representations (Mikolov et al., 2013; Hu et al., 2020). For BERT, we extract the final\nlayer\u2019s hidden state of [CLS] token from frozen bert-base-uncased as the text node representation. As for OGB, we utilize the default features from benchmark datasets, such as averaged word embeddings and bag-of-words representations. (2) Text-only self-supervised representation learning models: BERT+MLM and SimCSE (Gao et al., 2021); In BERT+MLM, we apply masked language modeling to BERT for the target TAG. SimCSE employs instance-wise contrastive learning to learn text node representation. (3) Structure-augmented language models: This category includes SPECTER (Cohan et al., 2020), GIANT (Chien et al., 2021) and GLEM (Zhao et al., 2022). SPECTER applies graphcentric contrastive learning on the language model, and GIANT employs the extreme multi-label classification to train the language model for neighborhood prediction. It is noteworthy that GLEM utilizes taskspecific labels to alternatively guide the pre-trained language model PLM and graph neural networks GNN through self-knowledge distillation. Compared with GLEM, our proposed method Grenade is fully self-supervised and does not rely on any human-annotated labels. The learned text node representations can be efficiently and effectively generalized to downstream tasks.\nImplementation Details. To ensure a fair comparison, we implemented all baseline methods and Grenade using the same language model, specifically bert-base-uncased. For our proposed method, Grenade, we set the K-hop neighbor as 1, set the temperature parameter \u03c4 to 0.05 in all the loss functions. The optimal hyperparameter |N (i)| is discussed in \u00a7 4.5. Please refer to Appendix B for additional implementation details."
        },
        {
            "heading": "4.2 Experimental Results",
            "text": "Few-shot Node Classification. To assess the generalizability of learned representation to new tasks under low-data scenarios, we conduct experiments on few-shot node classification. Under this setting, the classification models are trained with varying numbers of labeled instances per class (k = {2, 4, 8, 16}). We repeat the experiment 10 times and reported the average results along with the standard deviation. The classification models utilized in this evaluation are the multilayer perceptron (MLP) and GraphSAGE (Hamilton et al., 2017). The hyperparameters for the classification models can be found in Appendix B. As the result showed in Tab. 2, several observations can be made: (1) In most cases,\nSSL based methods achieve better performance than non-SSLmethods (BERT+MLM,SPECTER and GIANT > GLEM), this indicates the significance of SSL in enhancing model transferability to new tasks with limited labels. (2) Among state-of-the-art TAG representation models, Grenade achieves the best performance on these datasets. This indicates the superior generalization ability of representations extracted by Grenade. The designed knowledge alignment allows the Grenade to integrate the pretrained knowledge from PLM encoder and structure inductive bias learned by GNN encoder. These expressive representations can be easily and efficiently generalized to few-shot learning tasks.\nFull Data Node Classification. We also conduct the node classification experiment with full training\ndataset under MLP, GraphSAGE (Hamilton et al., 2017) and RevGAT-KD (Li et al., 2021a). As the result shown in Tab. 4, we can observe that: (1) Grenade achieves the best performance across all the baseline methods. (2) The performance gap between Grenade and some baseline methods like GIANT and GLEM becomes smaller as more labeled data provided, but Grenade is consistently better than these methods.\nNode Clustering. In the node clustering task, we utilize the learned text node representations to train a K-means++ model for clustering instances. We apply the default hyperparameters of K-means++ as provided by scikit-learn (Pedregosa et al., 2011). The number of clusters is set to the number of classes in the dataset, and we assign the cluster label\nbased on the most common label within each cluster. Following the evaluation protocol described in (Ding et al., 2022b), we report three clustering evaluation metrics: accuracy (ACC), normalized mutual information (NMI), and adjusted rand index (ARI). We exclude the GLEM model from this evaluation since it requires the labels during representation learning. To ensure robustness, we perform 10 runs of K-means++ with different random seeds and report the average results. As shown in Table 3, we observe that the structure augmented SSL methods outperform the text-only self-supervised representation learning methods (Grenade, GIANT, SPECTER > BERT+MLM, SimCSE). This indicates structure-augmented SSL methods can understand the context within graph structure that can lead to more accurate node representations, which in turn can lead to better clustering. Additionally, our proposed method Grenade consistently outperforms all baseline methods. The improvement demonstrates that Grenade can better preserve neighborhood information which will inform the clustering methods of how data points are interconnected or related to each other.\nLink Prediction. Next, we evaluate the learned representation in predicting missing connections given existing connections from TAG. We aim to rank the positive candidates (1 or 2 positive instances) higher than the negative candidates (1,000 negative instances) for each query node. The eval-\nuation metric used for this task is the mean reciprocal rank (MRR), which measures the reciprocal rank of the positive instance among the negative instances for each query instance and takes the average over all query instances. As shown in Fig. 2, we observe that Grenade significantly outperforms other approaches. In fact, Grenade achieves at least a 4% performance improvement compared to methods that utilize structure-augmented selfsupervised learning loss (SPECTER and GIANT) across all datasets. This demonstrates that Grenade can better preserve the neighborhood information, which is consistent with the findings from \u00a7 4.2."
        },
        {
            "heading": "4.3 Representation Visualization",
            "text": "To visually demonstrate the quality of the learned representations, we apply t-distributed stochastic neighbor embedding (t-SNE) (Van der Maaten and Hinton, 2008) to for representation visualization. We compare Grenade with two best-performing baseline methods, including SPECTER and GIANT on the arxiv dataset. In Fig. 3, we present the t-SNE visualization of the embeddings for 10 randomly sampled classes comprising 5,000 subsampled instances. The colors in the visualization correspond to the labels of these subsampled instances. From Fig. 3, we observe Grenade exhibits denser clusters and more explicit boundaries among different classes compared to the baseline methods. This observation confirms that Grenade can learn compact intra-class and distinct inter-class representations."
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "To validate the effectiveness of graph-centric contrastive learning and graph-centric knowledge alignment, we conducted an ablation study on Grenade. In this study, we respectively remove GC-CL, ND-KA, and NBH-KA from the full model and report these model variants\u2019 performance in Tab. 5. In general, the full model Grenade has the best performance in most cases, and we notice a performance decline when any of the components is removed or replaced, underscoring the significance of each component in Grenade. Remarkably, we observe a performance improvement in link prediction after removing graph-centric contrastive learning (w/o GC-CL > Grenade in terms of MRR). Considering the task similarity between GC-CL and the link prediction, one possible explanation is that removing GC-CL could help the model mitigate overfitting and further improve performance for the link prediction task. Meanwhile, this observation, in turn shows that the dual-level graph-centric knowledge alignment (ND-KA and NBH-KA) is effective for capturing structural context information from the TAG."
        },
        {
            "heading": "4.5 Hyperparameter Analysis",
            "text": "K-hop Neighbors. We delved into understanding the impact of the K-hop neighbor selection on Grenade\u2019s efficiency. The choice of different K values directly affects the formulation of positive pairs in graph-centric contrastive learning (Eq. 3 and Eq. 4), and the alignment of knowledge between the graph neural network and the language model (Eq. 5 and Eq. 6). Based on the results presented in Fig. 4, it is evident that augmenting the hop distance adversely affects performance metrics in full data node classification (ACC of MLP), node clustering (ACC), and link prediction (MRR). This suggests that 1-hop neighbors optimally capture structural knowledge within our algorithm. However, when extending to 2-hop or 3-hop neighbors, there\u2019s a heightened risk of integrating noisy data. This insight aligns with the conclusions drawn from related research, specifically SPECTER (Cohan et al., 2020).\nWe contend that our methodology strikes a harmonious balance between assimilating structural data and filtering out extraneous noise, thereby ensuring consistent performance in our assessments.\n1-Hop Neighbor Size. One crucial aspect of Grenade\u2019s SSL objectives is the hyperparameter |N (i)|, which controls the number of 1-hop neighbors considered for representation learning. To investigate the impact of subsampled neighbor size in Grenade, we conduct a hyperparameter analysis on the full training dataset node classification, node clustering, and link prediction tasks. As shown in Fig. 5, we observe that Grenade achieves its best performance with a practical number of neighbors (|N (i)| = 2 for ogbn-arxiv and |N (i)| = 1 for ogbn-products). This finding is particularly advantageous as it reduces the computational burden of the PLM encoder in graph-centric contrastive learning and knowledge alignment between the PLM and GNN encoders."
        },
        {
            "heading": "5 Related Work",
            "text": "Learning with TAG. This problem involves learning text node representations that encode both textual semantics and structural context information.\nA prominent approach uses structure-augmented SSL objectives to incorporate structure information into language models (Chien et al., 2021; Cohan et al., 2020; Yasunaga et al., 2022; Ostendorff et al., 2022). For example, SPECTER and LinkBERT focus on node-pair relationships (Cohan et al., 2020; Yasunaga et al., 2022), while GIANT targets extreme multiclass neighborhood prediction (Chien et al., 2021). A concurrent work PATTON (Jin et al., 2023) trains a language model through network-contextualized masked language modeling and link prediction. A parallel line of research tries to integrate pre-trained language models (PLMs) with graph neural networks (GNN) (Yang et al., 2021; Zhao et al., 2022; Mavromatis et al., 2023; Shang et al., 2020; Zhang et al., 2021). GraphFormers interweaves GNN with LMs\u2019 self-attention layers (Yang et al., 2021). GLEM (Zhao et al., 2022), GraDBERT (Mavromatis et al., 2023) and LRTN (Zhang et al., 2021) leverage labeled datasets for co-training betweenPLM andGNN. While they excel in node classification benchmarks, their reliance on labeled datasets limits the representation\u2019s generalization ability to new tasks.\nContrastive Learning. Contrastive learning is a self-supervised learning paradigm that aims to learn representations by distinguishing between positive and negative instances (Jaiswal et al., 2020). A key practice in contrastive learning is to use augmented versions of the same instance as positive instances and other instances as negative instances (Gao et al., 2021; He et al.; Radford et al., 2021). For example, SimCSE creates augmented views for each instance based on dropout (Srivastava et al., 2014). However, conventional instance-level contrastive learning only encourages instance-wise discrimination (Li et al., 2021b) and commonly assumes different instances are i.i.d., which neglects the relationships among different instances on TAG. Hence, conventional contrastive learning methods are ineffective to learn expressive representation learning on TAG. To address those limitations, many recent methods seek to extend the design of positive pair construction by considering local neighborhood information (Cohan et al., 2020; Ostendorff et al., 2022). However, those methods cannot fully capture complex graph structures. In contrast, our proposed method, Grenade, leverages graph-centric contrastive learning and graph-centric knowledge alignment to fully exploit the structural context information from TAG."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we introduce a self-supervised graphcentric language model: Grenade, for learning expressive and generalized representation from textual attributed graphs (TAG). Grenade is learned through two self-supervised learning algorithms: (1) Graph-Centric Contrastive Learning, which enables Grenade to harness intrinsic graph knowledge through relational-aware and augmentationfree contrastive learning; and (2) Graph-Centric Knowledge Alignment, which facilitates the exchange and strengthening of knowledge derived from the pre-trained language model encoder and the graph neural network encoder, thereby enhancing the capture of relational information from TAG. We conduct experiments on four benchmark datasets under few-shot and full data node classification, node clustering, and link prediction tasks, and find that Grenade significantly and consistently outperforms baseline methods."
        },
        {
            "heading": "7 Limitations",
            "text": "In this section, we acknowledge the following constraints in our study: (1) Constraints on the Choice of Backbone Model. Our choice of the backbone model was restricted to the initialization of \u201cbert-base-uncased\u201d in training Grenade. This choice was necessitated by the limitations in computational resources available to us. Exploration with alternative PLM backbones such as GPT2(Radford et al., 2019) and RoBERTa(Liu et al., 2019) has not been carried out and represents a promising direction for subsequent studies. (2) Comparison with Large Language Models. The natural language processing domain has recently seen breakthroughs with state-of-the-art large language models like LLaMA(Touvron et al., 2023) and ChatGPT (OpenAI, 2023), which have demonstrated exceptional performance in language understanding tasks. Our experiment confirms that Grenade surpasses existing representation learning techniques in TAG, but the performance of Grenade relative to these cutting-edge language models remains to be ascertained. (3) Breadth of Evaluation. In this work, we evaluate Grenade primarily through node classification, node clustering, and link prediction tasks. However, there are other relevant evaluation dimensions, such as retrieval, reranking, co-view and others (Cohan et al., 2020). Future work will investigate the applicability and capacity of Grenade to broader tasks."
        },
        {
            "heading": "A Dataset Details",
            "text": "We extend ogbn-arxiv and ogbn-products for link prediction and we evaluate models on the test-split from these two node datasets. Specifically, for each source node i, we randomly choose its one neighbor as the positive candidate and 1,000 negative candidates and would like the model to rank the positive candidate over the negative candidates. The negative references are randomly-sampled from all the nodes from TAG that are not connected by i.\nFor ogbl-citation2, we also extend it for node classification. Here the task is to predict the subject areas of the subset of the nodes/papers that published in arxiv like ogbn-papers100M. We borrow the labels for ogbl-citation2 from ogbn-papers100M. We align the nodes from ogbl-citation2 and ogbnpapers100M through Microsoft academic graph paper ID. We split the data into training/validation/test by year that the papers published before 2017 is the training dataset, between 2017-2018 is the validation dataset and after 2018 is the test dataset.\nB Implementation Details Our proposed methodology was implemented using PyTorch version 1.13.1 (Paszke et al., 2019) and HuggingFace Transformers version 4.24.0 (Wolf et al., 2019). The experiments were executed on A5000, A6000, and RTX 4090 GPUs. For Grenade, the learning rate is configured at 5e\u2212 5, and AdamW optimizer is employed for training over the course of 3 epochs. The search space for the number of GNN layers, L, ranges from {1, 2, 3, 4}, and further hyperparameter analysis is performed as detailed in Fig. 6. The hyperparameters for fewshot node classification are shown in Tab. 6 and Tab. 7, respectively. It should be noticed that \u201c\u22121\u201d means utilize all the training data for batch_size and all the neighbors for neighbor sampling."
        },
        {
            "heading": "C Additional Experimental Results",
            "text": "Full Data Node Classification.\nEnhanced Node Classification Model. Besides MLP and GraphSage, we incorporated the recent Graph Transformer Network, NAGphormer (Chen et al., 2023), into our node classification evaluation. As evidenced by the data in Tab. 8, our proposed approach consistently surpasses the baseline techniques (BERT+MLM, SPECTER, GIANT) in the few-shot and full-data node classification. This performance is consistent with the observations of using MLP and GraphSage.\nAdditional Ablation Study. We have the ablation study on ogbn-arxiv and ogbn-products dataset under 7 different model variations. In the second row of Tab. 9, the term ICL refers to instance-wise crossmodality contrastive learning. It indicates that the positive pairs are formed between identically indexed document and node representations, while the negative pairs consist of other document and node representations within the minibatch. From Table 9, have the consistent observation as 5 that each component of Grenade contributes the performance improvement.\nIn-Depth Hyperparameter Analysis. To evaluate the performance of the graph neural network encoder (GNN), we conduct an analysis of the hyperparameter L as depicted in Fig. 6. Our observations indicate that an optimal performance is achieved when L = 2.\nInference Time Complexity Analysis When provided with an arbitrary node, Grenade is capable of generating the textual representation of the node without requiring graph information. This leads\nto an efficiency that is on par with BERT (Devlin et al., 2018), while achieving approximately a 10% enhancement in performance for full data node classification.\nRegarding the training efficiency, though our model requires longer training time compared to text-only PLM/SSL methods like BERT+MLM (1\u0303h24m) and SimCSE (Gao et al., 2021)(around 41m), we are able to achieve a great margin of performance improvement with reasonable additional training time (around 2h50m). Also, our model can achieve stable performance with 3-epoch training, which is more efficient than structure-based contrastive learning method such as SPECTER (Cohan et al., 2020) (around 3h19m for 3 epochs)."
        }
    ],
    "title": "Grenade: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs",
    "year": 2023
}