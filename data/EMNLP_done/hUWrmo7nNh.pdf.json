{
    "abstractText": "Pretrained language models have achieved remarkable success in natural language understanding. However, fine-tuning pretrained models on limited training data tends to overfit and thus diminish performance. This paper presents Bi-Drop, a fine-tuning strategy that selectively updates model parameters using gradients from various sub-nets dynamically generated by dropout. The sub-net estimation of BiDrop is performed in an in-batch manner, so it overcomes the problem of hysteresis in sub-net updating, which is possessed by previous methods that perform asynchronous sub-net estimation. Also, Bi-Drop needs only one mini-batch to estimate the sub-net so it achieves higher utility of training data. Experiments on the GLUE benchmark demonstrate that Bi-Drop consistently outperforms previous fine-tuning methods. Furthermore, empirical results also show that Bi-Drop exhibits excellent generalization ability and robustness for domain transfer, data imbalance, and low-resource scenarios.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shoujie Tong"
        },
        {
            "affiliations": [],
            "name": "Heming Xia"
        },
        {
            "affiliations": [],
            "name": "Damai Dai"
        },
        {
            "affiliations": [],
            "name": "Runxin Xu"
        },
        {
            "affiliations": [],
            "name": "Tianyu Liu"
        },
        {
            "affiliations": [],
            "name": "Binghuai Lin"
        },
        {
            "affiliations": [],
            "name": "Yunbo Cao"
        },
        {
            "affiliations": [],
            "name": "Zhifang Sui"
        }
    ],
    "id": "SP:1e70afe53b25c1ee57d3327ec6da474e8db858c9",
    "references": [
        {
            "authors": [
                "Samuel R. Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D. Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2015
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "Sanyuan Chen",
                "Yutai Hou",
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Xiangzhan Yu."
            ],
            "title": "Recall and learn: Fine-tuning deep pretrained language models with less forgetting",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V. Le",
                "Christopher D. Manning."
            ],
            "title": "ELECTRA: pretraining text encoders as discriminators rather than generators",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,",
            "year": 2020
        },
        {
            "authors": [
                "Hal Daum\u00e9III."
            ],
            "title": "Frustratingly easy domain adaptation",
            "venue": "ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, June 23-30, 2007, Prague, Czech Republic. The Association for Computational Linguistics.",
            "year": 2007
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL.",
            "year": 2018
        },
        {
            "authors": [
                "Jesse Dodge",
                "Gabriel Ilharco",
                "Roy Schwartz",
                "Ali Farhadi",
                "Hannaneh Hajishirzi",
                "Noah A. Smith."
            ],
            "title": "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping",
            "venue": "CoRR, abs/2002.06305.",
            "year": 2020
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin de Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for NLP",
            "venue": "Proceedings of the 36th International Conference on Ma-",
            "year": 2019
        },
        {
            "authors": [
                "Haoming Jiang",
                "Pengcheng He",
                "Weizhu Chen",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Tuo Zhao."
            ],
            "title": "SMART: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization",
            "venue": "Proceedings of the 58th",
            "year": 2020
        },
        {
            "authors": [
                "Cheolhyoung Lee",
                "Kyunghyun Cho",
                "Wanmo Kang."
            ],
            "title": "Mixout: Effective regularization to finetune large-scale pretrained language models",
            "venue": "ICLR.",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "Yonatan Belinkov",
                "James Henderson."
            ],
            "title": "Variational information bottleneck for effective low-resource fine-tuning",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Marco Marelli",
                "Stefano Menini",
                "Marco Baroni",
                "Luisa Bentivogli",
                "Raffaella Bernardi",
                "Roberto Zamparelli."
            ],
            "title": "A SICK cure for the evaluation of compositional distributional semantic models",
            "venue": "Proceedings of the Ninth International Conference",
            "year": 2014
        },
        {
            "authors": [
                "Jason Phang",
                "Thibault F\u00e9vry",
                "Samuel R. Bowman."
            ],
            "title": "Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks",
            "venue": "CoRR, abs/1811.01088.",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever."
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog.",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "arXiv preprint arXiv:1910.10683.",
            "year": 2019
        },
        {
            "authors": [
                "Vijay Vasudevan",
                "Benjamin Caine",
                "Raphael Gontijo Lopes",
                "Sara Fridovich-Keil",
                "Rebecca Roelofs."
            ],
            "title": "When does dough become a bagel? analyzing the remaining mistakes on imagenet",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Li Wan",
                "Matthew D. Zeiler",
                "Sixin Zhang",
                "Yann LeCun",
                "Rob Fergus."
            ],
            "title": "Regularization of neural networks using dropconnect",
            "venue": "Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013,",
            "year": 2013
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R Bowman."
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R. Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational",
            "year": 2018
        },
        {
            "authors": [
                "Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demon-",
            "year": 2020
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Tao Qi",
                "Yongfeng Huang."
            ],
            "title": "Noisytune: A little noise can help you finetune pretrained language models better",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2:",
            "year": 2022
        },
        {
            "authors": [
                "Lijun Wu",
                "Juntao Li",
                "Yue Wang",
                "Qi Meng",
                "Tao Qin",
                "Wei Chen",
                "Min Zhang",
                "Tie-Yan Liu"
            ],
            "title": "R-drop: regularized dropout for neural networks. In NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Runxin Xu",
                "Fuli Luo",
                "Zhiyuan Zhang",
                "Chuanqi Tan",
                "Baobao Chang",
                "Songfang Huang",
                "Fei Huang."
            ],
            "title": "Raise a child in large language model: Towards effective and generalizable fine-tuning",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "NeurIPS.",
            "year": 2019
        },
        {
            "authors": [
                "Hongyi Yuan",
                "Zheng Yuan",
                "Chuanqi Tan",
                "Fei Huang",
                "Songfang Huang."
            ],
            "title": "Hype: Better pre-trained language model fine-tuning with hidden representation perturbation",
            "venue": "CoRR, abs/2212.08853.",
            "year": 2022
        },
        {
            "authors": [
                "Haojie Zhang",
                "Ge Li",
                "Jia Li",
                "Zhongjin Zhang",
                "Yuqi Zhu",
                "Zhi Jin."
            ],
            "title": "Fine-tuning pre-trained language models effectively by optimizing subnetworks adaptively",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Felix Wu",
                "Arzoo Katiyar",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Revisiting few-sample BERT fine-tuning",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, Natural Language Processing (NLP) has achieved significant progress due to the emergence of large-scale Pretrained Language Models (PLMs) (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019; Clark et al., 2020). For downstream tasks, compared with training from scratch, fine-tuning pretrained models can usually achieve efficient adaptation and result in better performance. Despite the great success, fine-tuning methods still face challenges in maintaining generalization performance on downstream tasks - they tend to run into the overfitting issue when the training data is limited (Phang et al., 2018; Devlin et al., 2018; Lee et al., 2020).\nTo improve the generalization ability of finetuning methods, many regularization techniques have been proposed (Chen et al., 2020; Aghajanyan\n\u2217Equal contributions\net al., 2021; Wu et al., 2021; Xu et al., 2021; Yuan et al., 2022), such as sub-net optimization strategies like Child-TuningD (Xu et al., 2021) and DPS (Zhang et al., 2022). Child-TuningD selects a static sub-net for updating based on parameter importance estimated by Fisher Information (FI). As an improved variant of Child-TuningD, DPS dynamically decides the sub-net to be updated by estimating FI with multiple mini-batches of data. Although these FI-based methods achieve better generalization ability than vanilla fine-tuning, they still have two limitations: (1) hysteresis in sub-net updating: the sub-net preference is estimated with the model parameters in previous iterations and may be incompatible with the current update step; and (2) insufficient utility of training data: FI estimation requires cumulative gradients through multiple mini-batches, so these methods cannot fit in situations with data scarcity.\nIn this paper, we delve deeper into adaptive subnet optimization strategies and propose Bi-Drop, a FI-free strategy for fine-tuning pretrained language models. Unlike Fisher information estimation, which requires cumulative gradients of minibatches, Bi-Drop only relies on information in a single mini-batch to select the parameters to update. Specifically, Bi-Drop utilizes gradient information\nfrom different sub-nets dynamically generated by dropout in each mini-batch. As illustrated in Figure 1, within a single training step of Bi-Drop, a minibatch will go through the forward pass multiple times and, due to the randomness introduced by dropout, yield various distinct sub-nets. We then apply a parameter selection algorithm with perturbation and scaling factors to stabilize the gradient updates. With this synchronous parameter selection strategy, Bi-Drop can selectively update model parameters according to the information from only the current mini-batch, and thus mitigate overfitting with a high utility of training data.\nExtensive experiments on the GLUE benchmark demonstrate that Bi-Drop shows remarkable superiority over the state-of-the-art fine-tuning regularization methods, with a considerable margin of 0.53 \u223c 1.50 average score. Moreover, Bi-Drop consistently outperforms vanilla fine-tuning by 0.83 \u223c 1.58 average score across various PLMs. Further analysis indicates that Bi-Drop attains superb generalization ability for domain transfer and task transfer, and is robust for data imbalance and lowresource scenarios.\nTo sum up, our contributions are three-fold:\n\u2022 We propose Bi-Drop, a step-wise sub-net optimization strategy that adaptively selects the updated sub-net based on the current minibatch data. Compared with prior FI-based methods, Bi-Drop derives a more stable and robust training trajectory with simultaneous sub-net update and high utility of training data.1\n\u2022 With extensive experiments on various PLMs, we demonstrate that Bi-Drop achieves consistent and remarkable superiority over prior fine-tuning regularization methods.\n\u2022 Further analysis shows that Bi-drop attains superb generalization ability for domain transfer and task transfer, and is robust for data imbalance and low-resource scenarios."
        },
        {
            "heading": "2 Related Work",
            "text": "Pretrained Language Models In recent years, the field of natural language processing (NLP) has witnessed significant advancements due to the development of large-scale pretrained language models (PLMs). The introduction of BERT (Devlin\n1Our code is available at https://github.com/ tongshoujie/Bi-Drop\net al., 2018) sparked a continuous emergence of various pre-trained models, including RoBERTa (Liu et al., 2019), ELECTRA (Clark et al., 2020), XLNet (Yang et al., 2019), GPT-2 (Radford et al., 2019), and GPT-3 (Brown et al., 2020), which have brought remarkable improvements in model structures and scales. Until now, fine-tuning is still one of the most popular approaches to adapting large pretrained language models to downstream tasks.\nRegularization Methods for Fine-tuning Largescale PLMs are prone to over-fitting (Phang et al., 2018; Devlin et al., 2018) and exhibit inadequate generalization ability when fine-tuned with limited training data (Aghajanyan et al., 2021; Mahabadi et al., 2021), resulting in degraded performance. To tackle this issue, various regularization techniques have been suggested to enhance the generalization capacity of models, including advanced dropout alternatives (Wan et al., 2013; Wu et al., 2021), applying adversarial perturbations (Aghajanyan et al., 2021; Wu et al., 2022; Yuan et al., 2022) and constrained regularization methods (Daum\u00e9III, 2007; Chen et al., 2020). In recent years, Child-tuning (Xu et al., 2021) and DPS (Zhang et al., 2022) propose to estimate parameter importance based on Fisher Information (FI) and selectively optimize a sub-net during fine-tuning to mitigate overfitting.\nFI-based methods have a strong dependence on the training data and exhibit hysteresis in sub-net updating. As shown in Table 1, compared with prior FI-based methods, Bi-Drop introduces a stepwise sub-net optimization strategy that adaptively selects the sub-net to be updated based on the current mini-batch. It is worth noting that, as a modelagnostic technique, Bi-Drop is orthogonal to most previous fine-tuning methods, which could further boost the model\u2019s performance."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Background",
            "text": "We first introduce the paradigm of sub-net optimization by giving general formulations of the backpropagation during the vanilla fine-tuning and CHILD-TUNINGD. We denote the parameters of the model at t-th iteration as \u03b8t = {\u03b8t,i}ni=1, where \u03b8t,i represent the i-th element of \u03b8t at the t-th training iteration. \u03b80 denotes the parameter matrix of the pre-trained model. The vanilla fine-tuning adopts Stochastic Gradient Descent (SGD) to all the model parameters, formally:\n\u03b8t+1 = \u03b8t \u2212 \u03b7 \u2202L (\u03b8t) \u2202\u03b8t\n(1)\nwhere L represents the training loss within a batch; \u03b7 is the learning rate. Instead of fine-tuning the entire network, CHILD-TUNINGD proposes to only optimize a subset of parameters (i.e., the sub-net). It first adopts the Fisher Information (FI) to estimate the relative importance of the parameters for a specific downstream task, which can be formulated as:\nF (\u03b80) = |D|\u2211 j=1 ( \u2202L (\u03b80) \u2202\u03b80 )2 (2)\nMCTD = F (\u03b80) > sort (F (\u03b80))p (3)\nwhere D is the training data, F (\u03b80) denotes the fisher information matrix of the pretrained parameters; sort(\u00b7)p represents the highest value of p percentile in F (\u03b80) after sorting in ascending order; MCTD is a mask matrix that is the same-sized as \u03b80. During fine-tuning, CHILD-TUNINGD only optimizes the selected sub-net in MCTD :\n\u03b8t+1 = \u03b8t \u2212 \u03b7 \u2202L (\u03b8t) \u2202\u03b8t MCTD (4)"
        },
        {
            "heading": "3.2 Bi-Drop",
            "text": "As introduced in Section 3.1, CHILD-TUNINGD only optimizes an unchanged sub-net during finetuning and ignores the update of other parameters, which may degrade the model\u2019s performance on downstream tasks. In this section, we offer a detailed introduction to our proposed method, BiDrop, which selects the updated parameters adaptively at each fine-tuning step. Specifically, BiDrop splits each training step into three sub-steps: (1) multiple forward propagations, (2) sub-net selection, and (3) parameter updating. We provide a pseudo-code of Bi-Drop in Algorithm 1."
        },
        {
            "heading": "3.2.1 Multiple Forward Propagations",
            "text": "Instead of prior FI-based methods that require accumulated gradients to measure the parameter importance, Bi-Drop leverages distinct sub-nets generated by dropout to select the sub-net to be updated. Inspired by Wu et al. (2021), given the training data D = {(xi, yi)}mi=1, at each training step, we feed xi to the model multiple times in the forward pass with different dropouts, and obtain their gradients correspondingly:\ng (j) t = \u2202L(\u03b8(j)t ) \u2202\u03b8\n(j) t\n, j = 1, 2, ..., k (5)\nwhere \u03b8(j)t and g (j) t represents the parameters of the j-th forward pass and its corresponding gradients. k denotes the number of forward passes, i.e., the number of distinct sub-nets with different dropouts.\nAlgorithm 1 Bi-Drop for Adam Optimizer Require: \u03b80: initial pretrained weights; L(\u03b8):\nstochastic objective function with parameters \u03b8; \u03b21, \u03b22 \u2208 [0, 1): exponential decay rates for the moment estimates; \u03b7: learning rate; 1: initialize timestep t\u2190 0, first moment vector m0 \u2190 0, second moment vector v0 \u2190 0 2: while not converged do 3: t\u2190 t+ 1\n// Multiple forward propagations\n4: g (j) t \u2190 \u2202L(\u03b8(j)t ) \u2202\u03b8\n(j) t\n, j = 1, 2, ..., k\n// Sub-net selection 5: Mt \u2190 SelectSubNetwork(gt) // Gradients Updating 6: gt \u2190 gt \u2299Mt 7: mt \u2190 \u03b21 \u00b7mt\u22121 + (1\u2212 \u03b21) \u00b7 gt 8: vt \u2190 \u03b22 \u00b7 vt\u22121 + (1\u2212 \u03b22) \u00b7 g2t 9: m\u0302t \u2190mt/(1\u2212 \u03b2t1)\n10: v\u0302t \u2190 vt/(1\u2212 \u03b2t2) // Update weights 11: wt \u2190 wt\u22121 \u2212 \u03b7 \u00b7 m\u0302t/( \u221a v\u0302t + \u03f5) 12: end while 13: return wt"
        },
        {
            "heading": "3.2.2 Sub-net Selection",
            "text": "In this subsection, we introduce our sub-net selection strategy, which estimates the relevant importance of parameters based on the gradients of distinct sub-nets generated by different dropouts. Concretely, our strategy is based on two estimation factors: the perturbation factor and the scaling factor.\nPerturbation Factor We propose the perturbation factor, which estimates the importance of parameters according to their stability with different dropouts in the forward pass. We point out that various sub-nets generated by dropout can be viewed as adversarial perturbations to the vanilla model. The perturbation factor is formalized as follows:\n\u00b5t = 1\nk k\u2211 j=1 g (j) t (6)\nFper(\u03b8t) = |\u00b5t| \u00b7 [\u2211 j ( g (j) t \u2212 \u00b5t )2]\u2212 12 (7)\nwhere \u00b5t is the average gradients of parameters. Fper measures the stability of parameters by both considering the mean and variance of gradients with adversarial perturbations, i.e. sub-nets with\nconsistently larger gradients and smaller variances are more favorable by this factor.\nScaling Factor We further propose the scaling factor as a regularization term. This factor measures the ratio of the average parameter gradients to the original parameters. Parameters whose gradient scale is much smaller than the original parameters will not be updated, which is similar in spirit to gradient clipping.\nFsca(\u03b8t) = |\u00b5t| \u00b7 |\u03b8t|\u22121 (8)"
        },
        {
            "heading": "3.2.3 Parameter Updating",
            "text": "Following prior work (Xu et al., 2021; Zhang et al., 2022), we derive a step-wise mask matrix Mt filtered by selecting the highest value of p percentile measured by the aforementioned two estimation factors.\nFfinal(\u03b8t) = Fper(\u03b8t) \u00b7 Fsca(\u03b8t) (9)\nMt = Ffinal(\u03b8t) > sort (Ffinal(\u03b8t)))p (10)\nThen, we utilize Mt to update the sub-net which consists of important parameters at each training step. We denote the formulation by simply replacing Eq.4 with our step-wise mask matrix Mt:\n\u03b8t+1 = \u03b8t \u2212 \u03b7\u00b5t \u00b7Mt (11)"
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "GLUE Benchmark Following previous work (Lee et al., 2020; Dodge et al., 2020; Zhang et al., 2021), we conduct extensive experiments on the GLUE benchmark, including natural language inference (RTE, QNLI, MNLI), paraphrase and similarity (MRPC, STS-B, QQP), linguistic acceptability (CoLA), and sentiment classification (SST-2). We include the detailed statistics and metrics of the datasets in Appendix A. Since the test results are only accessible online with limited submission times, we follow prior studies (Phang et al., 2018; Lee et al., 2020; Aghajanyan et al., 2021; Dodge et al., 2020; Xu et al., 2021; Zhang et al., 2022) that fine-tune the pretrained model on the training set and report the results on the development sets using the last checkpoint.\nNLI Datasets We also evaluate the generalization ability of Bi-Drop on several Natural Language Inference (NLI) tasks, including SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), MNLIM (Williams et al., 2018) and SICK (Marelli et al., 2014). We report all results by Accuracy on the development sets consistent with GLUE."
        },
        {
            "heading": "4.2 Baselines",
            "text": "Besides the vanilla fine-tuning method, we mainly compare Bi-Drop with the following baselines:\nMixout (Lee et al., 2020) is a fine-tuning technique that stochastically replaces the parameters with their pretrained weight based on the Bernoulli distribution. R3F (Aghajanyan et al., 2021) is a fine-tuning strategy motivated by trust-region theory, which injects noise sampled from either a normal or uniform distribution into the pre-trained representations. R-Drop (Wu et al., 2021) minimizes the bidirectional KL-divergence to force the output distributions of two sub-nets sampled by dropout to be consistent with each other. Child-TuningD (Xu et al., 2021) selects the task-relevant parameters as the sub-net based on the Fisher information and only updates the sub-net during fine-tuning. DPS (Zhang et al., 2022) is a dynamic sub-net optimization algorithm based on Child-TuningD. It estimates Fisher information with multiple minibatches of data and selects the sub-net adaptively during fine-tuning.\nFor reference, we also show other prior finetuning techniques in our main experimental results, such as Weight Decay (Daum\u00e9III, 2007), Top-K Tuning (Houlsby et al., 2019) and RecAdam (Chen et al., 2020)."
        },
        {
            "heading": "4.3 Experiments Setup",
            "text": "We conduct our experiments based on the HuggingFace transformers library 2 (Wolf et al., 2020) and follow the default hyper-parameters and settings unless noted otherwise. We report the averaged results over 10 random seeds. Other detailed experimental setups are presented in Appendix B."
        },
        {
            "heading": "4.4 Results on GLUE",
            "text": "Comparison with Prior Methods We compare Bi-Drop with various prior fine-tuning methods based on BERTlarge and report the mean (and max) scores on GLUE benchmark in Table 2, following Lee et al. (2020) and Xu et al. (2021). The results indicate that Bi-Drop yields the best average performance across all tasks, showing its effectiveness. Moreover, the average of the maximum scores attained by Bi-Drop is superior to that of other methods, providing further evidence of the effectiveness of Bi-Drop. We also conducted the same experiment on Robertalarge, and the details can be found in Appendix E.\nComparison with Vanilla Fine-tuning We show the experimental results of six widely used largescale PLMs on the GLUE Benchmark in Table 3. The results show that Bi-Drop outperforms vanilla fine-tuning consistently and significantly across all tasks performed on various PLMs. For instance, Bi-Drop achieves an improvement of up to 1.58 average score on BERTbase and 1.35 average score on Robertabase. The results highlight the universal effectiveness of Bi-Drop in enhancing the fine-tuning performance of PLMs. Additionally, because BiDrop forward-propagate twice, we present an additional study of the baseline with doubled batch\n2https://github.com/huggingface/transformers\nsize in Appendix D."
        },
        {
            "heading": "4.5 Out-of-Domain Generalization",
            "text": "We further evaluate the generalization ability of Bi-Drop on a widely used experimental setting in prior research (Aghajanyan et al., 2021; Xu et al., 2021; Zhang et al., 2022): out-of-domain generalization. In detail, we finetune BERTlarge with different strategies on 5k subsampled MNLI and SNLI datasets respectively, and directly evaluate the fine-tuned model on other NLI datasets. The experimental results in Table 4 illustrate that Bi-Drop outperforms vanilla fine-tuning and prior methods across various datasets. Specifically, compared with other fine-tuning methods on SNLI, Bi-Drop demonstrates consistent and substantial improvement, with an average score increase of 2.14. In particular, Bi-Drop achieves an improvement of 2.35 on MNLI task and 2.38 on MNLI-m task. For models trained on MNLI, Bi-Drop also consistently outperforms prior methods, with an average improvement of 1.03 score. The experimental results\nindicate that Bi-Drop encourages the model to learn deeper and more generic semantic features and alleviate superficial biases of specific tasks, which improves the model\u2019s generalization ability."
        },
        {
            "heading": "4.6 Task Generalization",
            "text": "We also evaluate the generalization ability of finetuned models following the experimental setting of Aghajanyan et al. (2021) and Xu et al. (2021), which freezes the representations of the model finetuned on one task and only trains the linear classifier on the other task. Specifically, we finetune BERTlarge among one task selected among MRPC, CoLA, and RTE and then transfer the model to the other two tasks. Figure 3 shows that Bi-Drop consistently outperforms vanilla fine-tuning when the fine-tuned model is transferred to other tasks. In particular, Bi-Drop improves by 3.50 and 3.28, when models trained on MRPC and RTE respectively are evaluated on CoLA. The results further verify the conclusion that Bi-Drop helps models learn more generalizable representations, com-\npared with the vanilla fine-tuning approach."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Stability to Random Seeds",
            "text": "We further investigate the stability properties of fine-tuned models. Figure 4 shows the output distributions of models with four experimental settings and across 10 random seeds. The results demonstrate that Bi-Drop outperforms other strategies in terms of average performance, and also exhibits greater stability by achieving more consistent results across 10 random seeds with lower variance."
        },
        {
            "heading": "5.2 Robustness Analysis",
            "text": "Recent research has brought to light that the vanilla fine-tuning approach is prone to deception and vulnerability in many aspects. In this study, we assess the robustness of Bi-Drop by designing evaluation tasks that focus on two common scenarios, aiming to examine its ability to withstand various forms of perturbations while maintaining its robustness.\nRobustness to Label Noise Due to the inherent limitations of human annotation, widely-used largescale datasets inevitably contain a certain amount of incorrect labels (Vasudevan et al., 2022). To\ninvestigate the robustness of Bi-Drop to label noise, we conduct simple simulation experiments on RTE, MRPC, and CoLA by randomly corrupting a predetermined fraction of labels with erroneous values. We evaluate the robustness of various finetuning methods trained on noisy data. The results shown in the left panel of Table 5 demonstrate that Bi-Drop achieves consistent superiority to other fine-tuning methods on noisy data. Furthermore, we conducted a comparison of model performance degradation across varying noise ratios. We calculated the degradation and presented it in brackets, as compared with Table 2. It indicates that BiDrop has the smallest performance drop compared with other fine-tuning methods. These results collectively demonstrate that Bi-Drop is more robust to label noise than prior methods.\nRobustness to Data Imbalance Minority class refers to the class that owns insufficient instances in the training set. In this section, we strive to explore the robustness of diverse fine-tuning approaches for the minority class by carrying out experiments on synthetic RTE, MRPC, and CoLA datasets. The experimental results are illustrated in the right panel of Table 5, which shows that Bi-Drop significantly outperforms other fine-tuning methods. Bi-Drop\nachieves up to 4.00, 4.23, and 5.29 average score improvements on 30%, 40%, and 50% reduction ratios respectively, outperforming other fine-tuning methods at lower reduction ratios and showcasing its robustness towards the minority class."
        },
        {
            "heading": "5.3 Performance in Low-Resource Scenarios",
            "text": "As illustrated in Section 1 and 2, compared with prior FI-based sub-net optimization methods that have a strong dependence on the training data, BiDrop proposes a step-wise sub-net selection strategy, which chooses the optimized parameters with the current mini-batch. In this section, we conduct extensive experiments to analyze how this dependency affects the performance of models. Concretely, we adopt various fine-tuning methods on\nBERTlarge with a limited amount of training data. The results are illustrated in Table 6. As the data amount decreases from 1.0K to 0.5K, the average improvement score of Child-TuningD over vanilla fine-tuning decreases from 1.57 to 1.15, while its improved variant DPS maintains a relatively stable improvement. But Bi-Drop improves the average improvement score from 2.77 to 3.28. The results indicate the superiority of Bi-Drop over prior methods in low-resource scenarios."
        },
        {
            "heading": "5.4 Ablation Study",
            "text": "To evaluate the effectiveness of our proposed finetuning strategy, we conduct an ablation study in Table 7. The results show that both our sub-net selection strategy and gradient averaging strategy\ncontribute to the performance improvement of BiDrop."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we propose a new sub-net optimization technique for large-scale PLMs, named BiDrop, which leverages the gradients of multiple sub-nets generated by dropout to select the updated parameters. Extensive experiments on various downstream tasks demonstrate that Bi-Drop achieves consistent and remarkable improvements over vanilla fine-tuning and prior excellent approaches by a considerable margin, across various model architectures. Further analysis indicates the generalizability and robustness of Bi-Drop over transferring, data imbalance and low-resource experiments."
        },
        {
            "heading": "7 Limitations",
            "text": "We propose a novel and effective fine-tuning method, Bi-Drop, which achieves a considerable performance improvement in downstream tasks. However, similar to some previous studies(Jiang et al., 2020; Aghajanyan et al., 2021; Wu et al., 2021), Bi-Drop requires multiple forward propagations, which makes its training time efficiency not good enough compared with the vanilla fine-tuning method."
        },
        {
            "heading": "Acknowledgements",
            "text": "This paper is supported by the National Key Research and Development Program of China 2020AAA0106700 and NSFC project U19A2065."
        },
        {
            "heading": "A GLUE Benchmark Datasets",
            "text": "In this paper, we conduct experiments on datasets in GLUE benchmark (Wang et al., 2019). The statistical information of the GLUE benchmark is shown in Table 8."
        },
        {
            "heading": "B Experimental Details",
            "text": "In this paper, we fine-tune different large pretrained language models with Bi-Drop, including BERTBASE3, BERTLARGE4, RoBERTaBASE5, RoBERTaLARGE6, DeDERTaLARGE7, and ELECTRALARGE8. The training epochs/steps, batch size, and warmup steps are listed in Table 9.\nFor the glue dataset, our maximum length is set as 128. We use grid search for learning rate from {1e-5, 2e-5, . . . , 1e-4}. For Bi-Drop, we use grid search for dropout rate from {0.05, 0.1}. The number of forward passes is fixed to two(k = 2). We conduct all the experiments on a single A40 GPU.\n3https://huggingface.co/bert-base-uncased/ tree/main\n4https://huggingface.co/bert-large-cased/tree/ main\n5https://huggingface.co/roberta-base/tree/ main\n6https://huggingface.co/roberta-large/tree/ main\n7https://huggingface.co/microsoft/ deberta-large/tree/main\n8https://huggingface.co/google/ electra-large-discriminator/tree/main"
        },
        {
            "heading": "C Hyper-Parameter Analysis",
            "text": "Bi-Drop uses two dropout techniques. In order to analyze the impact of the dropout rate on the experimental results, a simple analysis experiment was done here. In order to make the comparison fair, all the parameters except the dropout rate are kept the same in the experiment. For simplicity, the dropout values are the same twice. The experimental results are shown in 5. It can be seen that different datasets have different preferences for dropout values. CoLA and RTE achieve the best results when the dropout value is 0.05; while MRPC achieves the best results when the dropout value is 0.1; STSB is insensitive to the dropout value until the dropout value is less than 0.1."
        },
        {
            "heading": "D Batch Size Doubled Training",
            "text": "We implement Bi-Drop by repeating the input data twice and forward-propagating twice. This is similar to doubling the batch size at each step. The difference is that half of the data is the same as the other half, and directly doubling the batch size, the data in the same mini-batch is all different. So for a fair comparison, we experimented with directly doubling the batch size. So for a fair comparison, we experimented with directly doubling the batch size. The experimental results are shown in Table 10, results show that directly doubling the batch size has basically no improvement, and BiDrop is significantly better than directly doubling the batch size."
        },
        {
            "heading": "E Comparison with Prior Methods on",
            "text": "Robertalarge\nWe compare Bi-Drop with various prior fine-tuning methods based on BERTlarge and report the mean\n(and max) scores on GLUE benchmark in Table 11, following Lee et al. (2020) and Xu et al. (2021)."
        }
    ],
    "title": "Bi-Drop: Enhancing Fine-tuning Generalization via Synchronous sub-net Estimation and Optimization",
    "year": 2023
}