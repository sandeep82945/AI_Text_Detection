{
    "abstractText": "Target-Oriented Multimodal Sentiment Classification (TMSC) aims to perform sentiment polarity on a target jointly considering its corresponding multiple modalities including text, image, and others. Current researches mainly work on either of two types of targets in a decentralized manner. One type is entity, such as a person name, a location name, etc. and the other is aspect, such as \u2018food\u2019, \u2018service\u2019, etc. We believe that this target type based division in task modelling is not necessary because the sentiment polarity of the specific target is not governed by its type but its context. For this reason, we propose a unified model for targetoriented multimodal sentiment classification, so called UnifiedTMSC. It is prompt-based language modelling and performs well on four datasets spanning the above two target types. Specifically, we design descriptive prompt paraphrasing to reformulate TMSC task via (1) task paraphrasing, which obtains paraphrased prompts based on the task description through a paraphrasing rule, and (2) image prefix tuning, which optimizes a small continuous image vector throughout the multimodal representation space of text and images. Conducted on two entity-level multimodal datasets: Twitter-2015 and Twitter-2017, and two aspect-level multimodal datasets: Multi-ZOL and MASAD, the experimental results show the effectiveness of our UnifiedTMSC.",
    "authors": [
        {
            "affiliations": [],
            "name": "Dan Liu"
        },
        {
            "affiliations": [],
            "name": "Lin Li"
        },
        {
            "affiliations": [],
            "name": "Xiaohui Tao"
        },
        {
            "affiliations": [],
            "name": "Jian Cui"
        },
        {
            "affiliations": [],
            "name": "Qing Xie"
        }
    ],
    "id": "SP:522f5438640161c99ae2a7fb5ec7fba86644ad3e",
    "references": [
        {
            "authors": [
                "Kristina Toutanova"
            ],
            "title": "2019. BERT: pre-training",
            "year": 2019
        },
        {
            "authors": [
                "Maosong Sun"
            ],
            "title": "PTR: prompt tuning with rules",
            "year": 2022
        },
        {
            "authors": [
                "Brendan Jou",
                "Tao Chen",
                "Nikolaos Pappas",
                "Miriam Redi",
                "Mercan Topkara",
                "Shih-Fu Chang."
            ],
            "title": "Visual affect around the world: A large-scale multilingual visual sentiment ontology",
            "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia Conference,",
            "year": 2015
        },
        {
            "authors": [
                "Xincheng Ju",
                "Dong Zhang",
                "Rong Xiao",
                "Junhui Li",
                "Shoushan Li",
                "Min Zhang",
                "Guodong Zhou."
            ],
            "title": "Joint multi-modal aspect-sentiment analysis with auxiliary cross-modal relation detection",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in",
            "year": 2021
        },
        {
            "authors": [
                "Zaid Khan",
                "Yun Fu."
            ],
            "title": "Exploiting BERT for multimodal target sentiment classification through input space translation",
            "venue": "MM \u201921: ACM Multimedia Conference, Virtual Event, China, October 20 - 24, 2021, pages 3034\u20133042. ACM.",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Sewon Min",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Oyvind Tafjord",
                "Peter Clark",
                "Hannaneh Hajishirzi."
            ],
            "title": "Unifiedqa: Crossing format boundaries with a single QA system",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Chengxi Li",
                "Feiyu Gao",
                "Jiajun Bu",
                "Lu Xu",
                "Xiang Chen",
                "Yu Gu",
                "Zirui Shao",
                "Qi Zheng",
                "Ningyu Zhang",
                "Yongpan Wang",
                "Zhi Yu."
            ],
            "title": "Sentiprompt: Sentiment knowledge enhanced prompt-tuning for aspectbased sentiment analysis",
            "venue": "CoRR, abs/2109.08306.",
            "year": 2021
        },
        {
            "authors": [
                "Guowei Li",
                "Fuqiang Lin",
                "WangQun Chen",
                "DiWen Dong",
                "Bo Liu."
            ],
            "title": "Prompt-based learning for aspectlevel sentiment classification",
            "venue": "Neural Information Processing - 29th International Conference, ICONIP 2022, Virtual Event, November 22-26, 2022, Pro-",
            "year": 2022
        },
        {
            "authors": [
                "Jing Li",
                "Aixin Sun",
                "Jianglei Han",
                "Chenliang Li."
            ],
            "title": "A survey on deep learning for named entity recognition",
            "venue": "IEEE Trans. Knowl. Data Eng., 34(1):50\u201370.",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Yan Ling",
                "Jianfei Yu",
                "Rui Xia."
            ],
            "title": "Visionlanguage pre-training for multimodal aspect-based sentiment analysis",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Bin Liu",
                "Tao Lin",
                "Ming Li."
            ],
            "title": "Enhancing aspect-category sentiment analysis via syntactic data augmentation and knowledge enhancement",
            "venue": "Knowl. Based Syst., 264:110339.",
            "year": 2023
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Comput. Surv., 55(9):195:1\u2013195:35.",
            "year": 2023
        },
        {
            "authors": [
                "Yuhang Liu",
                "Wei Wei",
                "Daowan Peng",
                "Feida Zhu."
            ],
            "title": "Declaration-based prompt tuning for visual question answering",
            "venue": "Proceedings of the ThirtyFirst International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July",
            "year": 2022
        },
        {
            "authors": [
                "Sijie Long",
                "Lin Li",
                "Jingling Yuan",
                "Jianquan Liu."
            ],
            "title": "Momnet: Gender prediction using mechanism of working memory",
            "venue": "Proceedings of the 31st ACM International Conference on Information & Knowledge Management, Atlanta, GA, USA, October 17-21,",
            "year": 2022
        },
        {
            "authors": [
                "Di Lu",
                "Leonardo Neves",
                "Vitor Carvalho",
                "Ning Zhang",
                "Heng Ji."
            ],
            "title": "Visual attention model for name tagging in multimodal social media",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne,",
            "year": 2018
        },
        {
            "authors": [
                "Yaojie Lu",
                "Qing Liu",
                "Dai Dai",
                "Xinyan Xiao",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun",
                "Hua Wu."
            ],
            "title": "Unified structure generation for universal information extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Ronald Seoh",
                "Ian Birle",
                "Mrinal Tak",
                "Haw-Shiuan Chang",
                "Brian Pinette",
                "Alfred Hough"
            ],
            "title": "Open aspect target sentiment classification with natural language",
            "year": 2021
        },
        {
            "authors": [
                "Zhengxin Song",
                "Yun Xue",
                "Donghong Gu",
                "Haolan Zhang",
                "Weiping Ding."
            ],
            "title": "Target-oriented multimodal sentiment classification by using topic model and gating mechanism",
            "venue": "Int. J. Mach. Learn. Cybern., 14(7):2289\u20132299.",
            "year": 2023
        },
        {
            "authors": [
                "Maria Tsimpoukelli",
                "Jacob Menick",
                "Serkan Cabi",
                "S.M. Ali Eslami",
                "Oriol Vinyals",
                "Felix Hill."
            ],
            "title": "Multimodal few-shot learning with frozen language models",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Jiawei Wang",
                "Zhe Liu",
                "Victor S. Sheng",
                "Yuqing Song",
                "Chenjian Qiu."
            ],
            "title": "Saliencybert: Recurrent attention network for target-oriented multimodal sentiment classification",
            "venue": "Pattern Recognition and Computer Vision - 4th Chinese Conference, PRCV",
            "year": 2021
        },
        {
            "authors": [
                "Luwei Xiao",
                "Ejian Zhou",
                "Xingjiao Wu",
                "Shuwen Yang",
                "Tianlong Ma",
                "Liang He."
            ],
            "title": "Adaptive multifeature extraction graph convolutional networks for multimodal target sentiment analysis",
            "venue": "IEEE International Conference on Multimedia and Expo, ICME",
            "year": 2022
        },
        {
            "authors": [
                "Nan Xu",
                "Wenji Mao."
            ],
            "title": "Multisentinet: A deep semantic network for multimodal sentiment analysis",
            "venue": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM 2017, Singapore, November 06 - 10, 2017. ACM.",
            "year": 2017
        },
        {
            "authors": [
                "Nan Xu",
                "Wenji Mao",
                "Guandan Chen."
            ],
            "title": "A comemory network for multimodal sentiment analysis",
            "venue": "The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Nan Xu",
                "Wenji Mao",
                "Guandan Chen."
            ],
            "title": "Multiinteractive memory network for aspect based multimodal sentiment analysis",
            "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial In-",
            "year": 2019
        },
        {
            "authors": [
                "Hao Yang",
                "Yanyan Zhao",
                "Bing Qin."
            ],
            "title": "Facesensitive image-to-emotional-text cross-modal translation for multimodal aspect-based sentiment analysis",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Yuan Yao",
                "Ao Zhang",
                "Zhengyan Zhang",
                "Zhiyuan Liu",
                "Tat-Seng Chua",
                "Maosong Sun."
            ],
            "title": "CPT: colorful prompt tuning for pre-trained vision-language models",
            "venue": "CoRR, abs/2109.11797.",
            "year": 2021
        },
        {
            "authors": [
                "Jianfei Yu",
                "Jing Jiang."
            ],
            "title": "Adapting BERT for target-oriented multimodal sentiment classification",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages",
            "year": 2019
        },
        {
            "authors": [
                "Jianfei Yu",
                "Jing Jiang",
                "Rui Xia."
            ],
            "title": "Entitysensitive attention and fusion network for entity-level multimodal sentiment classification",
            "venue": "IEEE ACM Trans. Audio Speech Lang. Process., 28:429\u2013439.",
            "year": 2020
        },
        {
            "authors": [
                "Yuhai Yu",
                "Hongfei Lin",
                "Jiana Meng",
                "Zhehuan Zhao."
            ],
            "title": "Visual and textual sentiment analysis of a microblog using deep convolutional neural networks",
            "venue": "Algorithms, 9(2):41.",
            "year": 2016
        },
        {
            "authors": [
                "Qi Zhang",
                "Jinlan Fu",
                "Xiaoyu Liu",
                "Xuanjing Huang."
            ],
            "title": "Adaptive co-attention network for named entity recognition in tweets",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications",
            "year": 2018
        },
        {
            "authors": [
                "Zhe Zhang",
                "Zhu Wang",
                "Xiaona Li",
                "Nannan Liu",
                "Bin Guo",
                "Zhiwen Yu."
            ],
            "title": "Modalnet: an aspectlevel sentiment classification model by exploring multimodal data with fusion discriminant attentional network",
            "venue": "World Wide Web, 24(6):1957\u20131974.",
            "year": 2021
        },
        {
            "authors": [
                "Fei Zhao",
                "Zhen Wu",
                "Siyu Long",
                "Xinyu Dai",
                "Shujian Huang",
                "Jiajun Chen."
            ],
            "title": "Learning from adjective-noun pairs: A knowledge-enhanced framework for target-oriented multimodal sentiment classification",
            "venue": "Proceedings of the 29th International",
            "year": 2022
        },
        {
            "authors": [
                "Jie Zhou",
                "Jiabao Zhao",
                "Jimmy Xiangji Huang",
                "Qinmin Vivian Hu",
                "Liang He."
            ],
            "title": "MASAD: A large-scale dataset for multimodal aspect-based sentiment analysis",
            "venue": "Neurocomputing, 455:47\u201358.",
            "year": 2021
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Jingkang Yang",
                "Chen Change Loy",
                "Ziwei Liu."
            ],
            "title": "Learning to prompt for visionlanguage models",
            "venue": "Int. J. Comput. Vis., 130(9):2337\u2013 2348.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With the emergence of new media and advanced technology, the forms of information released by people are quietly changing, from mono-modality to multi-modality now, such as text, images, etc (Xu and Mao, 2017). This also pushes researchers to conduct multimodal learning (Yu et al., 2016; Xu et al., 2018; Long et al., 2022). For sentiment analysis, both text and image are highly correlated with sentiment polarity. Moreover, they can complement and reinforce each other (Xu et al., 2019).\nAt present, fine-grained Multimodal Sentiment Classification (MSC) includes two main tasks: entity-level MSC and aspect-level MSC, as shown in Figure 1. (1) For entity-level, the entity and its context are encoded as independent input text in some studies(Yu and Jiang, 2019; Yu et al., 2020; Wang et al., 2021; Zhao et al., 2022). Others jointly consider the encoding of an entity with its contextual learning, which is their main focus to achieve good MSC performance (Khan and Fu, 2021; Xiao et al., 2022; Yang et al., 2022). For example, Xiao et al. (2022) presented a dual stream adaptive multifeature extraction graph convolutional network to convert an image into its caption. (2) For aspectlevel, the aspect and its context are being encoded individually due to the semantics involved in an aspect itself (Xu et al., 2019; Zhang et al., 2021; Zhou et al., 2021). For example, Xu et al. (2019)\nstudied a multi-interactive memory network model. These TMSC works can promote human decisions by assisting users in knowing about certain targets.\nAs we know, an entity usually is a person name, or a location name, etc (Li et al., 2022b). Such entity target only has a specific meaning when connecting it with a specific modality content, which means it is difficult to accurately understand the entity without its context. On the contrary, an aspect itself in some degree can represent what it means even without its context. Because of this obvious difference, previous studies show their interest in differently modelling the two tasks to capture target related context (Xu et al., 2019; Yu et al., 2020; Zhang et al., 2021; Song et al., 2023). For example, in Figure 1(a), \"Chuck Bass\", \"MCM\", and \"Iran\" do not have any exact meaning when they are disconnected from the specific context. However, in Figure 1(b), both aspect term and aspect category have their own meanings with a hidden sentiment tendency. For the \"battery life\" of mobile phones, the first reaction is that the longer the battery life, the better. Despite the above differences among the TMSC tasks, from the perspective of sentiment classification, the goal of TMSC is to predict the sentiment polarity of a target no matter whether it is an entity or an aspect. Therefore, in our view, the boundary is unnecessary.\nIn this paper, we propose a unified TMSC model via prompt based language modelling, so called UnifiedTMSC, which is independent of the target type in TMSC. Our core is to reconstruct the two TMSC tasks through descriptive prompt paraphrasing. The prompts we design can place entity and aspect in their context, while also being close to the TMSC task description. To achieve this goal, we carry out our work from two aspects: (1) task paraphrasing. The task description is transformed into a seed prompt, and different paraphrased prompts are obtained by using our paraphrasing rule. They serve as discrete prompts for the text and fit into the Masked Language Modeling (MLM) format. And (2) image prefix tuning (Li and Liang, 2021). A segment vector is initialized for the image pretrained embedding as the prefix continuous prompt. In the subsequent multimodal continuous representation space, the image pre-trained embedding is fixed and only some segment of the initialized vector is optimized. In this way, sentiment labels are generated through the cloze-filling method.\nIn our extensive experiments, our UnifiedTMSC\nmodel achieves state-of-the-art performance on two entity-level datasets: Twitter-2015 and Twitter2017, and two aspect-level datasets: Multi-ZOL and MASAD. (1) The results of the task description based prompts are superior to those of arbitrary prompt templates. (2) On two entity-level datasets, our model improves Accuracy by 1.0%- 2.8% and 1.5%-3.7%, macro-F1 by 1.5%-4.5% and 1.7%-7.5%, respectively. (3) On two aspectlevel datasets, our model gains of 6.46%-8.87% and 1.66%-3.02% on Accuracy, 3.77%-5.01% and 2.19%-3.25% on macro-F1 are derived. The experimental results demonstrate the effectiveness of our UnifiedTMSC."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Entity-level MSC",
            "text": "As a pioneer, Yu and Jiang (2019) proposed a BERT-based multimodal architecture to determine the sentiment polarity of an entity. Yu et al. (2020) introduced an entity-sensitive attention and fusion network. And Wang et al. (2021) put forward a recurrent attention network. Khan and Fu (2021) introduced an input space translation framework to construct image context from the image. Zhao et al. (2022) used the adjective-noun pairs extracted from images as the knowledge enhancement based on Yu and Jiang (2019) and Wang et al. (2021). Moreover, Yang et al. (2022) explored facial information in images to obtain visual and sentiment clues.\nIn the research stated above, the entity can be encoded as a distinct text input, or entity and context can be combined as the text input. Their goal is to more effectively learn the semantics related to entity sentiment."
        },
        {
            "heading": "2.2 Aspect-level MSC",
            "text": "Aspect-level multimodal classification was first proposed by Xu et al. (2019), and they introduced a multi-interactive memory network to analyze multiple correlations in multimodal data. Zhang et al. (2021) presented a multimodal fusion discriminant attention network and designed a discriminant matrix to supervise the modality fusion. Zhou et al. (2021) conducted a multimodal interaction model that learns the relationships between text, image, and target aspect through interaction layers and adversarial training. One key difference between an aspect and an entity is that the aspect has its own semantics inferred from the aspect words. Therefore, existing research usually regards the aspect\nitself as an additional input. Our focus: In contrast to previous studies, our model can run across TMSC tasks. It combines the target (entity or aspect) and its context as a text input using task description based paraphrased prompts. We can get sentiment-related semantics about the target by providing its context in task description based prompts."
        },
        {
            "heading": "2.3 Prompt paraphrasing",
            "text": "Prompt tuning has received increasing attention recently (Radford et al., 2021; Yao et al., 2021) and has been successfully applied in many domains (Han et al., 2022; Liu et al., 2023b). For example, in the field of Question Answering (QA), Khashabi et al. (2020) reformulated many QA tasks as a text generation problem by fine-tuning seq2seq-based pre-trained models and appropriate prompts from the context and questions. For Information Extraction (IE), Chen et al. (2022) first explored the application of fixed-prompt LM Tuning in relation extraction and Lu et al. (2022) applied prompt to control the information to be extracted. In other research fields, Cui et al. (2023) used prompt learning in text input to conduct the meme mining task. Recently, the prompt has been used for the task involving fine-grained text sentiment analysis, and\nthe results are promising (Seoh et al., 2021; Li et al., 2021, 2022a; Gao et al., 2022; Liu et al., 2023a).\nInspired by the above studies, our unified model is through the task description based prompt paraphrasing with jointly soft and hard prompt tuning."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Task Formulation",
            "text": "Given a multimodal samples D, for each sample d \u2208 D, it contains a sentence S with n words (w1, w2, ...wn) and one or more related images I , as well as a target T which contains m words (w1, w2, ...wm) and is a sub-sequence of S or predefined phrase. For the target T , it is associated with a sentiment label Y . In general, Y \u2208 { positive, neutral, negative }, and different tasks may have different sentiment labels. Our goal is to learn a target-oriented sentiment classifier that can correctly predict the sentiment label for each sample X = (S, I, T )."
        },
        {
            "heading": "3.2 Overview",
            "text": "As shown in Figure 2, our model consists of two modules: task paraphrasing (hard prompt) and image prefix tuning (soft prompt). For the given multimodal data X = (S, I, T ), we obtain paraphrased prompts (P1, P2...) through the task description of\nTMSC, which are used as prefixes for text input S in the task paraphrasing module (the left of Figure 2). Paraphrased prompt Pi contains a [MASK] token and is connected with text S. The text embedding R is obtained through an encoder in the bottom-right of Figure 2. In the image prefix tuning module, we apply an initialization vector V for pre-train image embedding E as the continuous prefix. After text embedding R and image embedding (V +E) are added to their respective position encoding, the fusion vector is obtained through the multimodal Transformer in the middle right of Figure 2. We take the hidden layer vector H and pass it through the MLM head to get the prediction score of [MASK] position for each word in the vocabulary C. Finally, the cross entropy loss LMLM of the prediction result Output and the true sentiment label Y is calculated."
        },
        {
            "heading": "3.3 Task Paraphrasing",
            "text": "In order to provide a specific context for a target, i.e., entity and aspect, we add the target-involved prompts to the text input. Since we change the original sentiment classification task in this way to the pre-trained MLM task, it is crucial to figure out how to develop prompts that are suitable for the original task, that is to say, the prompts should be consistent with the expression of the task description. Inspired by prompt tuning in various domains, such as the visual grounding problem (Yao et al., 2021) and visual question answering task (Liu et al., 2022), we propose the task paraphrasing module to draw paraphrased prompts as a solution to the above issue.\nWe get the seed prompt according to the task description composed of natural language, and take\nthe task-related keyword K (\u2018sentiment\u2019) from the task description. The seed prompt is transformed through the paraphrasing rule, and guides the generation of paraphrased prompts that are close to the original task description. Our paraphrasing rule can be formalized in the following form:\nf(Y ) \u2227 f(T ) \u2227 f(K) \u2227 g(K) (1)\nwhere the function f represents the relative position in the paraphrased prompts and the substitution of synonyms for a keyword (\u2018sentiment\u2019) is illustrated by the function g. f(Y ), f(T ), f(K) \u2208 {B,M,E} (meaning \u2018Beginning\u2019, \u2018Middle\u2019, \u2018Ending\u2019), which respectively stands for the relative position of the sentiment label, the target entity, and the keyword derived from the task description. g(K) \u2208 {yes,no} means whether to replace synonyms of keywords K. Moreover, synonyms are synonymous explanations for the keyword \u2018sentiment\u2019 in the dictionary. For example, in the Bing dictionary, the synonyms for \u2018sentiment\u2019 include \u2018emotion\u2019, \u2018feeling\u2019, \u2018opinion\u2019, etc.\nThe task paraphrasing module is shown in the left part of Figure 2. Generally speaking, based on the relative position combination of Y , K, and T , one seed prompt can be paraphrased to gain multiple candidate prompts. If we replace the keyword with different synonyms, we will receive more paraphrased prompts. Some examples of paraphrased prompts Pi are listed in Table 1. When paraphrased prompt Pi is in the training phase, the position of Y is replaced by [MASK] token which is the prediction object. Finally, the Pi and text S are concatenated to obtain a new text input:\nSnew = [CLS] Pi S [SEP ] (2)"
        },
        {
            "heading": "3.4 Image Prefix Tuning",
            "text": "Discrete prompts, i.e., hard prompts, in the text are natural and easy to understand. For images, directly applying discrete prompts cannot ensure alignment between text based prompts and images since there are modality gaps between different modalities. Therefore, finding suitable prompts for images is crucial. This module mainly focuses on how to add prompts to images to facilitate a better modality fusion of images and text.\nInspired by (Tsimpoukelli et al., 2021) and (Li and Liang, 2021), we introduce a continuous vector as a prefix, i.e., soft prompt, to image pre-trained embedding, as shown in Figure 3. We first segment the image into r regions, and then initialize a vector vi (i \u2208 {1, 2, ...r}) for each region to form the prefix embedding V = {v1, v2, ...vr} and V \u2208 Rr\u00d72048. Here, Pidx denotes the indices of the prefix sequence. |Pidx| denotes the length of the prefix and |Iidx| indicates the length of the image pre-trained embedding. In E = {e1, e2, e3, e4, e5}, ei \u2208 R2048 refers to the image embedding of the ith region. The prefix embedding V = {v1, v2, v3} and V \u2208 R3\u00d72048. If there are multiple images corresponding to the text, we initialize a soft prompt for each image and their averaged vector is used as the final prefix prompt for the images. V and E are concatenated to obtain new image embedding Enew.\nEnew = V \u2295 E (3)\nEnew(i) =\n{ vi i \u2208 Pidx\ne(i\u2212|Pidx|) otherwise (4)\nEnew and Snew are image input and text input respectively, and the subsequent encoding process is carried out together."
        },
        {
            "heading": "3.5 Multimodal Transformer With MLM",
            "text": "Training. Given a triplet (S, I, T, Y ), after the task paraphrasing module and image prefix tuning module, the updated text input Snew and image embeddings Enew are obtained. Text encoder encodes Snew to gain text embedding R. R and the image embedding Enew undergo cross-attention through a Transformer to obtain the fusion embedding H . In this multimodal Transformer, the image pre-trained embedding E is fixed and not updated, and only V is updated. Fusion embedding H passes through an MLP to obtain prediction scores Logit:\nLogit = MLPMLM (H) (5)\nThe word with the highest prediction score is the prediction result:\nOutput = argmax(Logit) (6)\nFinally, the predicted result and the true sentiment label Y are calculated in the cross-entropy loss to optimize our model.\nLMLM = \u2212 log( exp(Logiti)\u2211|C| j=0 exp(Logitj) ) (7)\nwhere i is the true sentiment label number and C is the size of the language model vocabulary.\nInference. In the inference stage, given a triplet (S, I, T ), the sentiment polarity Y \u2032 of T is determined via the triplet.\nY \u2032 = argmax(Logit[MASK]) (8)\nAfter obtaining the fusion embedding of text and images, we take the argmax of the logit of the [MASK] position to obtain the final prediction result. Finally, for non-label words generated by the model, the answer engineering is used to map them to sentiment labels."
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Datasets. According to Zhou et al. (2021), there are four datasets for target-oriented multimodal\nsentiment classification, including two entity-level datasets: Twitter-2015 (Zhang et al., 2018; Yu and Jiang, 2019),Twitter-2017 (Lu et al., 2018; Yu and Jiang, 2019) and two aspect-level datasets: MultiZOL (Xu et al., 2019), MASDA (Zhou et al., 2021) The statistics information of these four datasets are shown in Table 2, and their details about the data partitioning and label types are presented in A.2.\nEvaluation Metrics. To fairly compare with state-of-the-art approaches, our UnifiedTMSC is evaluated across two TMSC tasks and adopts the Accuracy (Acc) and Macro-F1 score (F1), following Yu and Jiang (2019) and Ling et al. (2022).\nImplementation Details. For the Twitter-2015, Twitter-2017, and MASAD, the batch size is set to 16 and the epochs are set to 6. The text encoder used is BERT-base-uncased (Devlin et al., 2019). For the Multi-ZOL dataset, in order to ensure fairness in the experiment, we employ BERT-basechinese (Devlin et al., 2019) as the text encoder, the batch size is 8, and the epochs as 6. Moreover, multilingual pre-trained models can also be used as our text encoders.\nFor all datasets, we apply Resnet-50 (He et al., 2016) as the image encoder to get the image prefix embedding, and the max length of the new text input Snew(in Eq. (2)) is 96. The model learning rate is set as 1e-5 and the dropout rate is 1e-2. Four layer Transformers (Vaswani et al., 2017) are aimed to perform cross attention between different modalities and the pre-training parameters are not loaded. Four NVIDIA TITAN Xp GPUs, each with 12GB of memory, are employed in our experiments which are done on a CentOS computer. The deep learning framework is Pytorch, and AdamW is used as the optimizer."
        },
        {
            "heading": "4.2 Compared Baselines",
            "text": "Because previous work has separated entity-level MSC and aspect-level MSC, the baseline models for each task are different. For the Twitter2015 and Twitter-2017 datasets, we compare six baselines including TomBERT (IJCAI, Yu and Jiang (2019)), SaliencyBERT (PRCV, Wang et al. (2021)), CapTrBERT (ACM Multimedia, Khan and Fu (2021)), JML-MASC (EMNLP, Ju et al. (2021)), VLP-MABSA (ACL, Ling et al. (2022)), FITEDE-Large (EMNLP, Yang et al. (2022)). For the Multi-ZOL and MASAD datasets, our model is compared with MIMN (AAAI, Xu et al. (2019)), ModalNet (WWW, Zhang et al. (2021)), MMAP\n(Neurocomputing, Zhou et al. (2021)) and CLIP (INT J COMPUT VISION, Zhou et al. (2022)). The detailed introduction of all the baseline models mentioned above is in Section A.1."
        },
        {
            "heading": "4.3 Experimental Results and Analysis",
            "text": ""
        },
        {
            "heading": "4.3.1 Overall Performance",
            "text": "The experimental results on multimodal entitylevel and aspect-level datasets are presented in Table 3 and Table 4 respectively. The best results on each metric are marked in bold and the second best results are highlighted with an underline.\nMultimodal Entity-level Datasets Results. As reported in Table 3, compared with the baselines, our UnifiedTMSC makes significant improvements in entity-level MSC. On the Twitter-2015 dataset, our improvement is approximately 1.0% on Accuracy and 1.5% on the macro-F1 compared to the FITE-DE-Large. On the Twitter-2017 dataset, we achieve improvements over multimodal baseline VLP-MABSA on the Accuracy by 1.5% and 1.7% on the macro-F1, which indicates that using prompt tuning to fuse entity and context can achieve a better sentiment-related semantic understanding of an entity, resulting in better classification results.\nMultimodal Aspect-level Datasets Results. Our UnifiedTMSC performs best in two datasets among all multimodal baselines, as listed in Table 4. This demonstrates the effectiveness of our proposed unified model based on prompt paraphrasing. Especially on the Multi-ZOL dataset, our UnifiedTMSC outperforms the ModalNet on the Accuracy by 7.75% and 3.77% on the macro-F1. On\nthe MASAD dataset, compared to the MMAP, ours improve performance by about 2.25% on Accuracy and 2.19% on the macro-F1.\nSpecifically, there are multiple domains in the MASAD dataset, we conduct experiments on each of them, and the experimental results are shown in Table 5. It is clear from the results that our model has achieved large improvements in each domain.\nFinally, t-tests are conducted to demonstrate the effectiveness of UnifiedTMSC. From the P-value of other models in Table 3 and Table 4, it can be found that all P-values are less than 0.05. This shows a significant difference in statistics between UnifiedTMSC and other models.\nAfter comparing and analyzing the experimental results, we can summarize the following two points from our prompt tuning:\nI. Our prompt paraphrasing method delivers the target\u2019s context and fits the TMSC job effectively, and it produces good results, demonstrating the efficacy of our unified model.\nII. Utilizing the target as a separate input has worse results than taking the target and context together as text input. This shows that contextual information affects the target\u2019s semantics, and a contextual content that is appropriate for the task will result in a well understanding of a target with semantics."
        },
        {
            "heading": "4.3.2 The Effect of Prompt Designs",
            "text": "For the Twitter-2015 and Twitter-2017 datasets, we select three paraphrased prompts from Table 1 to conduct the experiments. The three selected paraphrased prompts are as follows: \u2022 P1: {target} express a [MASK] sentiment. \u2022 P2: The emotion of {target} is [MASK]. \u2022 P3: A [MASK] sentiment is expressed towards to {target}. In addition, to verify the performance of the task paraphrasing module. We design three arbitrary prompt templates and compare them with the above\nthree paraphrased prompts. The three arbitrary prompts are as follows: \u2022 P \u20321: I feel the {target} is [MASK]. \u2022 P \u20322: The {target} made me feel [MASK]. \u2022 P \u20323: I [MASK] the {target}. where {target} is the entity that needs to determine sentiment polarity, and [MASK] represents the masked word, i.e. sentiment label. The masked word in the P\n\u2032 1, P \u2032 2, P \u2032 3 is {good, ok, bad}, {good,\nindifferent, bad} and {love, dislike, hate} respectively. After the masked words are generated, we perform answer engineering to map the predicted results to the sentiment polarity set, that is, the probabilities of these predicted words are made to be equal to the probabilities of being Positive, Neutral, and Negative.\nThe results of several different prompts are shown in Table 6. Through analysis and comparison, we can obtain the following summaries:\nI. Our paraphrased prompts created by using the task description are much superior to arbitrary prompt templates, demonstrating the value of our task paraphrasing module in producing paraphrased prompts that are appropriate for the original sentiment classification task. In addition, the performance of different paraphrased prompts is comparable, and in subsequent experiments, anyone can be selected for training and inference.\nImage\nText\n(a) Fan Throws Water Bottle at Justin Bieber After He Says He Doesn \u2019 t Know the . . .\n(b) Kim Kardashian goes all out for Kanye West ' s 40 th birthday in the Bahamas\n(c) RT @ TrumpDoral : Congratulations to the the new # MissUniverse , Miss Colombia , Paulina Vega !\n(d) Nba - The Cavs are Shocked Draymond Green Keeps Getting Away With Kic -\nII. The position of [MASK] in the paraphrased prompts can also have an impact on the experimental results. In our case, the effect is best when the relative position of [MASK] is \"Middle\" rather than \"Beginning\" or \"Ending\". Therefore, when meeting a new task, the position of [MASK] may be a factor to affect task performance."
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "To further investigate the effects of paraphrased prompts and image prefix, because entity-level MSC is more challenging than aspect-level MSC, we conduct ablation analysis on the multimodal entity-level datasets: Twitter-2015 and Twitter2017. The results of the ablation experiments are shown in Table 7.\nParaphrased Prompts. Pi is omitted from Eq. (2) and we just add the image prefix V in Eq. (3) for the experiment in order to examine the effects of the paraphrased prompts. The linear classification layer uses the fusion vector derived by multimodal Transformers as input to estimate the sentiment la-\nbel of the target. The results are shown in Table 7. The absence of paraphrased prompts has resulted in a considerable performance decrease. On the Twitter-2015 dataset, the Accuracy and macroF1 are dropped by approximately 4.1% and 4.3%, while on the Twitter-2017 dataset, the Accuracy declines by about 7.4% and the macro-F1 drops by about 8.8%. This demonstrates how using text paraphrased prompts can provide an entity with its task-related semantics.\nImage Prefix. We only use the paraphrased prompt Pi in Eq. (2) without applying the image prefix prompt V in Eq. (3) to study the importance of the image prefix. For text input, P1, P2, and P3 are proceeded for the ablation study. From Table 7, it can be seen that the performance has dropped after taking out the image prefix V (in Eq. (3)). The Accuracy decreases by 1.1%, and the macroF1 drops by 0.8%, according to the average results for the Twitter-2015 dataset. On the Twitter-2017 dataset, the Accuracy and macro-F1 decline by roughly 1.0%. This illustrates the effectiveness of the image prefix. Moreover, the ablation study also shows that different paraphrased prompts have varied outcomes, demonstrating the language models\u2019 sensitivity to prompts."
        },
        {
            "heading": "4.5 Case Study",
            "text": "In our case study, the compared methods are only image prefix (denoted by w/o Paraphrased Prompt), only text prompt (denoted by w/o Image Prefix),\nand our UnifiedTMSC model with soft and hard prompts. We apply P1 for both w/o Image Prefix and UnifiedTMSC.\nAs shown in Figure 4, for example (a), when there is no paraphrased prompt, the result obtained from text and image information is Neutral. When there is no image prefix, the image pre-trained embedding dominates the prediction results as Positive. Both of these two prediction results do not match the correct sentiment label Negative. For example (b), there are multiple targets that require sentiment classification. The sentiment labels predicted for place name Bahamas are Neutral, and adding appropriate prompts to both image and text can be predicted correctly. For examples (c) and (d), they are similar to example (b).\nThese four samples further confirm the usefulness of our unified model. It can assign specific sentiment-related semantics to an entity via applying a paraphrased prompt. And prefix tuning of images can obtain better task-specific image embedding than image pre-trained embedding."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "There are currently two formats for target-oriented multimodal sentiment classification: entity-level and aspect-level. Our analysis shows that this barrier is superfluous. By incorporating paraphrased prompt and prefix vector into the multimodal input, the proposed model, i.e., UnifiedTMSC, unifies the two types of TMSC tasks. We conduct experiments on four datasets, and the results demonstrate the superiority and efficacy of our UnifiedTMSC.\nOur ongoing effort will primarily concentrate on two issues. One is to investigate how to design a paraphrasing rule to automatically generate paraphrased prompts without depending on human labor. The other is to investigate the generalizability of our model to see whether it can be used in other multimodal studies. In addition, we notice that the auto-regressive model XLNet can alleviate the problem of generating non-label words, and our future work will consider this.\nLimitations\nOur model has three limitations. The first one is that the design of paraphrased prompts relies on human experience. Although our paraphrasing rule is designed based on the relative position and synonym substitution, manual experience is still required to obtain paraphrased prompts that comply\nwith grammar rules, and paraphrased prompts that comply with grammar rules may not necessarily be the best. The second limitation is that more attempts are needed to conduct experiments on multilingual pre-trained models. Furthermore, the last is that we have not explored whether our model can be extended to other multimodal research fields, which will be our future research direction."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work is partially supported by NSFC, China (No.62276196)."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Baselines\n(1) TomBERT (IJCAI, Yu and Jiang (2019)), which employ BERT for the inter-modal interactions and Target-Image matching layer to obtain a target-sensitive visual.\n(2) SaliencyBERT (PRCV, Wang et al. (2021)), which design a recurrent attention mechanism to capture the inter-modality dynamics.\n(3) CapTrBERT (ACM Multimedia, Khan and Fu (2021)), a BERT-based model translating the image into caption and fusing the caption and input text-entity pair.\n(4) JML-MASC (EMNLP, Ju et al. (2021)), which is a multi-task learning method with the crossmodal relation detection.\n(5) VLP-MABSA (ACL, Ling et al. (2022)), a taskspecific pre-training vision-language model.\n(6) FITE-DE-Large (EMNLP, Yang et al. (2022)), introducing a FITE method to focus on capturing emotional cues through facial expressions.\n(7) MIMN (AAAI, Xu et al. (2019)), using memory network to model multimodal data and learn the interactive influences in crossmodality and self-modality.\n(8) MMAP (Neurocomputing, Zhou et al. (2021)), learning the interaction between text and image, text and aspect, and image and aspect through three interactive mechanisms.\n(9) ModalNet (WWW, Zhang et al. (2021)), designing a discriminant matrix to supervise the fusion of inter-modal information.\n(10) CLIP (INT J COMPUT VISION, Zhou et al. (2022)), a multimodal pre-training model, converts image classification tasks into image-text matching tasks using comparative learning.\nA.2 Dataset Details\n(1) Twitter-2015 and Twitter-2017. The two Twitter datasets include user tweets released during 2014-2015 and 2016-2017, collected by Zhang et al. (2018) and Lu et al. (2018). Since the two publicly available multimodal datasets Twitter-2015 and Twitter-2017 only provide annotated targets in each tweet, Yu and Jiang (2019) ask three domain experts to annotate the sentiment towards each target, and take the majority label among the three annotators as the gold label. These two datasets contain multimodal tweets and annotated mentioned entities in the text, as well as the sentiment polarity of each entity, including positive, neutral, and negative. Each multimodal tweet consists of a text and a corresponding image. We follow their official splitting for training, validation and testing. The statistics of Twitter-2015 and Twitter-2017 are in Table 8.\n(2) Multi-ZOL. This dataset is collected by Xu et al. (2019) from the Chinese website ZOL.com. The website consists of 40 large channels, including news, shopping malls, hardware, mobile phones, and more. They searched from pages 1 to 50 in the mobile phones channel. For each phone, only the reviews from the first 20 pages were crawled. The Multi-ZOL dataset contains 5288 multimodal reviews and each multimodal review contains a textual content, an image set, and at least one but no more than six aspects. The six aspects are cost performance, performance configuration, battery life, appearance, photography effect, and screen. For each aspect, the review has an integer sentiment score from 1 to 10, which is regarded as the sentiment label in our experiment. Actually, we convert digital sentiment labels into Chinese characters, such as \u201810\u2019 to \u2018ten\u2019. Combining each aspect with the multimodal review, 28469 pairs of aspect review samples can be obtained. We randomly partition the dataset into training (80%), development (10%), and testing (10%) sets. The statistical information after dividing the dataset is shown in Table 9.\n(3) MASAD. This dataset was collected and published by Zhou et al. (2021) based on the publicly available Visual Sentiment Ontology (VSO) dataset (Borth et al., 2013) and Multi-\nTwitter-2015 Twitter-2017 #Positive #Neutral #Negative Total #Positive #Neutral #Negative Total\nTrain 928 1883 368 3179 1508 416 1638 3562 Dev 303 679 149 1122 515 144 517 1176 Test 317 607 113 1037 493 168 573 1234"
        }
    ],
    "title": "Descriptive Prompt Paraphrasing for Target-Oriented Multimodal Sentiment Classification",
    "year": 2023
}