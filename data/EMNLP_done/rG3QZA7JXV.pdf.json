{
    "abstractText": "Large language models (LLMs) show powerful reasoning abilities on various text-based tasks. However, their reasoning capability on structured data such as tables has not been systematically explored. In this work, we first establish a comprehensive taxonomy of reasoning and operation types for tabular data analysis. Then, we construct a complex reasoning QA dataset over tabular data, named CRT-QA (Complex Reasoning QA over Tabular data), with the following unique features: (1) it is the first Table QA dataset with multi-step operation and informal reasoning; (2) it contains fine-grained annotations on questions\u2019 directness, composition types of sub-questions, and human reasoning paths which can be used to conduct a thorough investigation on LLMs\u2019 reasoning ability; (3) it contains a collection of unanswerable and indeterminate questions that commonly arise in real-world situations. We further introduce an efficient and effective tool-augmented method, named ARC (Autoexemplar-guided Reasoning with Code), to use external tools such as Pandas to solve table reasoning tasks without handcrafted demonstrations. The experiment results show that CRT-QA presents a strong challenge for baseline methods and ARC achieves the best result. The dataset and code are available at https://github.com/zzh-SJTU/CRT-QA.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhehao Zhang"
        },
        {
            "affiliations": [],
            "name": "Xitao Li"
        },
        {
            "affiliations": [],
            "name": "Yan Gao"
        },
        {
            "affiliations": [],
            "name": "Jian-Guang Lou"
        }
    ],
    "id": "SP:346379fb653011be845f785e62a71e8ea3a788f1",
    "references": [
        {
            "authors": [
                "Sumit Asthana",
                "Aaron Halfaker."
            ],
            "title": "With few eyes, all hoaxes are deep",
            "venue": "Proc. ACM Hum.-Comput. Interact., 2(CSCW).",
            "year": 2018
        },
        {
            "authors": [
                "Max Bartolo",
                "Tristan Thrush",
                "Sebastian Riedel",
                "Pontus Stenetorp",
                "Robin Jia",
                "Douwe Kiela"
            ],
            "title": "Models in the loop: Aiding crowdworkers with generative annotation assistants",
            "year": 2022
        },
        {
            "authors": [
                "Wenhu Chen"
            ],
            "title": "2023a. Large language models are few(1)shot table reasoners",
            "year": 2023
        },
        {
            "authors": [
                "Wenhu Chen."
            ],
            "title": "Large language models are few(1)shot table reasoners",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 1120\u20131130, Dubrovnik, Croatia. Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Wenhu Chen",
                "Ming-Wei Chang",
                "Eva Schlinger",
                "William Wang",
                "William W. Cohen"
            ],
            "title": "Open question answering over tables and text",
            "year": 2021
        },
        {
            "authors": [
                "Wenhu Chen",
                "Jianshu Chen",
                "Yu Su",
                "Zhiyu Chen",
                "William Yang Wang"
            ],
            "title": "Logical natural language generation from open-domain tables",
            "year": 2020
        },
        {
            "authors": [
                "Wenhu Chen",
                "Xueguang Ma",
                "Xinyi Wang",
                "William W. Cohen"
            ],
            "title": "2022a. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "year": 2022
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hongmin Wang",
                "Jianshu Chen",
                "Yunkai Zhang",
                "Hong Wang",
                "Shiyang Li",
                "Xiyou Zhou",
                "William Yang Wang"
            ],
            "title": "2020b. Tabfact: A large-scale dataset for table-based fact verification",
            "year": 2020
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hanwen Zha",
                "Zhiyu Chen",
                "Wenhan Xiong",
                "Hong Wang",
                "William Wang"
            ],
            "title": "2021b. Hybridqa: A dataset of multi-hop question answering over tabular and textual data",
            "year": 2021
        },
        {
            "authors": [
                "Zhiyu Chen",
                "Wenhu Chen",
                "Charese Smiley",
                "Sameena Shah",
                "Iana Borova",
                "Dylan Langdon",
                "Reema Moussa",
                "Matt Beane",
                "Ting-Hao Huang",
                "Bryan Routledge",
                "William Yang Wang"
            ],
            "title": "2022b. Finqa: A dataset of numerical reasoning over financial data",
            "year": 2022
        },
        {
            "authors": [
                "Zhoujun Cheng",
                "Tianbao Xie",
                "Peng Shi",
                "Chengzu Li",
                "Rahul Nadkarni",
                "Yushi Hu",
                "Caiming Xiong",
                "Dragomir Radev",
                "Mari Ostendorf",
                "Luke Zettlemoyer",
                "Noah A. Smith",
                "Tao Yu"
            ],
            "title": "Binding language models in symbolic languages",
            "year": 2023
        },
        {
            "authors": [
                "Meier-Hellstern",
                "Douglas Eck",
                "JeffDean",
                "Slav Petrov",
                "Noah Fiedel"
            ],
            "title": "2022. Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language models",
            "year": 2022
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Mark Chen",
                "Heewoo Jun",
                "Lukasz Kaiser",
                "Matthias Plappert",
                "Jerry Tworek",
                "Jacob Hilton",
                "Reiichiro Nakano",
                "Christopher Hesse",
                "John Schulman"
            ],
            "title": "Training verifiers to solve math word",
            "year": 2021
        },
        {
            "authors": [
                "Antonia Creswell",
                "Murray Shanahan",
                "Irina Higgins"
            ],
            "title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "year": 2022
        },
        {
            "authors": [
                "Yao Fu",
                "Hao Peng",
                "Tushar Khot",
                "Mirella Lapata"
            ],
            "title": "Improving language model negotiation with self-play and in-context learning from ai feedback",
            "year": 2023
        },
        {
            "authors": [
                "Yao Fu",
                "Hao Peng",
                "Ashish Sabharwal",
                "Peter Clark",
                "Tushar Khot"
            ],
            "title": "Complexity-based prompting for multi-step reasoning",
            "year": 2023
        },
        {
            "authors": [
                "Luyu Gao",
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Pengfei Liu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig"
            ],
            "title": "Pal: Program-aided language models",
            "year": 2023
        },
        {
            "authors": [
                "Mor Geva",
                "Daniel Khashabi",
                "Elad Segal",
                "Tushar Khot",
                "Dan Roth",
                "Jonathan Berant"
            ],
            "title": "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
            "year": 2021
        },
        {
            "authors": [
                "Matthew Ho",
                "Aditya Sharma",
                "Justin Chang",
                "Michael Saxon",
                "Sharon Levy",
                "Yujie Lu",
                "William Yang Wang"
            ],
            "title": "Wikiwhy: Answering and explaining cause-and-effect questions",
            "year": 2022
        },
        {
            "authors": [
                "Jiaxin Huang",
                "Shixiang Shane Gu",
                "Le Hou",
                "Yuexin Wu",
                "Xuezhi Wang",
                "Hongkun Yu",
                "Jiawei Han"
            ],
            "title": "Large language models can self-improve",
            "year": 2022
        },
        {
            "authors": [
                "Mohit Iyyer",
                "Wen-tau Yih",
                "Ming-Wei Chang."
            ],
            "title": "Search-based neural structured learning for sequential question answering",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1821\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Yannis Katsis",
                "Saneem Chemmengath",
                "Vishwajeet Kumar",
                "Samarth Bharadwaj",
                "Mustafa Canim",
                "Michael Glass",
                "Alfio Gliozzo",
                "Feifei Pan",
                "Jaydeep Sen",
                "Karthik Sankaranarayanan",
                "Soumen Chakrabarti"
            ],
            "title": "Ait-qa: Question answering dataset",
            "year": 2021
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "year": 2023
        },
        {
            "authors": [
                "Jiacheng Liu",
                "Skyler Hallinan",
                "Ximing Lu",
                "Pengfei He",
                "Sean Welleck",
                "Hannaneh Hajishirzi",
                "Yejin Choi"
            ],
            "title": "Rainier: Reinforced knowledge introspector for commonsense question answering",
            "year": 2022
        },
        {
            "authors": [
                "Ruibo Liu",
                "Ruixin Yang",
                "Chenyan Jia",
                "Ge Zhang",
                "Denny Zhou",
                "Andrew M. Dai",
                "Diyi Yang",
                "Soroush Vosoughi"
            ],
            "title": "Training socially aligned language models in simulated human society",
            "year": 2023
        },
        {
            "authors": [
                "Pan Lu",
                "Baolin Peng",
                "Hao Cheng",
                "Michel Galley",
                "KaiWei Chang",
                "Ying Nian Wu",
                "Song-Chun Zhu",
                "Jianfeng Gao"
            ],
            "title": "Chameleon: Plug-and-play compositional reasoning with large language models",
            "year": 2023
        },
        {
            "authors": [
                "Pan Lu",
                "Liang Qiu",
                "Kai-Wei Chang",
                "Ying Nian Wu",
                "Song-Chun Zhu",
                "Tanmay Rajpurohit",
                "Peter Clark",
                "Ashwin Kalyan"
            ],
            "title": "Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning",
            "year": 2023
        },
        {
            "authors": [
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Yiming Yang",
                "Graham Neubig"
            ],
            "title": "Language models of code are few-shot commonsense learners",
            "year": 2022
        },
        {
            "authors": [
                "Sewon Min",
                "Victor Zhong",
                "Luke Zettlemoyer",
                "Hannaneh Hajishirzi."
            ],
            "title": "Multi-hop reading comprehension through question decomposition and rescoring",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2019
        },
        {
            "authors": [
                "Xu Jiang",
                "Karl Cobbe",
                "Tyna Eloundou",
                "Gretchen Krueger",
                "Kevin Button",
                "Matthew Knight",
                "Benjamin Chess",
                "John Schulman"
            ],
            "title": "Webgpt: Browserassisted question-answering with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Dragomir Radev"
            ],
            "title": "Fetaqa: Free-form table question answering",
            "year": 2021
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Bhargavi Paranjape",
                "Scott Lundberg",
                "Sameer Singh",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer",
                "Marco Tulio Ribeiro"
            ],
            "title": "Art: Automatic multistep reasoning and tool-use for large language models",
            "year": 2023
        },
        {
            "authors": [
                "Ankur P. Parikh",
                "Xuezhi Wang",
                "Sebastian Gehrmann",
                "Manaal Faruqui",
                "Bhuwan Dhingra",
                "Diyi Yang",
                "Dipanjan Das"
            ],
            "title": "Totto: A controlled table-to-text generation dataset",
            "year": 2020
        },
        {
            "authors": [
                "Joon Sung Park",
                "Joseph C. O\u2019Brien",
                "Carrie J. Cai",
                "Meredith Ringel Morris",
                "Percy Liang",
                "Michael S. Bernstein"
            ],
            "title": "Generative agents: Interactive simulacra of human behavior",
            "year": 2023
        },
        {
            "authors": [
                "Panupong Pasupat",
                "Percy Liang."
            ],
            "title": "Compositional semantic parsing on semi-structured tables",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language",
            "year": 2015
        },
        {
            "authors": [
                "Chengwei Qin",
                "Aston Zhang",
                "Zhuosheng Zhang",
                "Jiaao Chen",
                "Michihiro Yasunaga",
                "Diyi Yang"
            ],
            "title": "Is chatgpt a general-purpose natural language processing task solver",
            "year": 2023
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom"
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "year": 2023
        },
        {
            "authors": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Dongsheng Li",
                "Weiming Lu",
                "Yueting Zhuang"
            ],
            "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face",
            "year": 2023
        },
        {
            "authors": [
                "Noah Shinn",
                "Federico Cassano",
                "Beck Labash",
                "Ashwin Gopinath",
                "Karthik Narasimhan",
                "Shunyu Yao"
            ],
            "title": "Reflexion: Language agents with verbal reinforcement learning",
            "year": 2023
        },
        {
            "authors": [
                "Kurt Shuster",
                "Mojtaba Komeili",
                "Leonard Adolphs",
                "Stephen Roller",
                "Arthur Szlam",
                "Jason Weston"
            ],
            "title": "Language models that seek for knowledge: Modular search generation for dialogue and prompt completion",
            "year": 2022
        },
        {
            "authors": [
                "Yu Hou",
                "Yufang Hou",
                "Yuntao Bai",
                "Zachary Seid",
                "Zhuoye Zhao",
                "Zijian Wang",
                "Zijie J. Wang",
                "Zirui Wang",
                "Ziyi Wu"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
            "year": 2022
        },
        {
            "authors": [
                "Alon Talmor",
                "Ori Yoran",
                "Amnon Catav",
                "Dan Lahav",
                "Yizhong Wang",
                "Akari Asai",
                "Gabriel Ilharco",
                "Hannaneh Hajishirzi",
                "Jonathan Berant"
            ],
            "title": "Multimodalqa: Complex question answering over text, tables and images",
            "year": 2021
        },
        {
            "authors": [
                "Ross Taylor",
                "Marcin Kardas",
                "Guillem Cucurull",
                "Thomas Scialom",
                "Anthony Hartshorn",
                "Elvis Saravia",
                "Andrew Poulton",
                "Viktor Kerkez",
                "Robert Stojnic"
            ],
            "title": "Galactica: A large language model for science",
            "year": 2022
        },
        {
            "authors": [
                "Petter T\u00f6rnberg"
            ],
            "title": "Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning",
            "year": 2023
        },
        {
            "authors": [
                "Guanzhi Wang",
                "Yuqi Xie",
                "Yunfan Jiang",
                "Ajay Mandlekar",
                "Chaowei Xiao",
                "Yuke Zhu",
                "Linxi Fan",
                "Anima Anandkumar"
            ],
            "title": "Voyager: An open-ended embodied agent with large language models",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou"
            ],
            "title": "2023b. Self-consistency improves chain of thought reasoning in language models",
            "year": 2023
        },
        {
            "authors": [
                "Maarten Bosma",
                "Denny Zhou",
                "Donald Metzler",
                "Ed H. Chi",
                "Tatsunori Hashimoto",
                "Oriol Vinyals",
                "Percy Liang",
                "Jeff Dean",
                "William Fedus"
            ],
            "title": "Emergent abilities of large language models",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William W. Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D. Manning"
            ],
            "title": "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
            "year": 2018
        },
        {
            "authors": [
                "Shunyu Yao",
                "Dian Yu",
                "Jeffrey Zhao",
                "Izhak Shafran",
                "Thomas L. Griffiths",
                "Yuan Cao",
                "Karthik Narasimhan"
            ],
            "title": "2023a. Tree of thoughts: Deliberate problem solving with large language models",
            "year": 2023
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik Narasimhan",
                "Yuan Cao"
            ],
            "title": "2023b. React: Synergizing reasoning and acting in language models",
            "year": 2023
        },
        {
            "authors": [
                "Pengcheng Yin",
                "Wen-Ding Li",
                "Kefan Xiao",
                "Abhishek Rao",
                "Yeming Wen",
                "Kensen Shi",
                "Joshua Howland",
                "Paige Bailey",
                "Michele Catasta",
                "Henryk Michalewski",
                "Alex Polozov",
                "Charles Sutton"
            ],
            "title": "Natural language to code generation in interactive data science",
            "year": 2022
        },
        {
            "authors": [
                "Tao Yu",
                "Rui Zhang",
                "Kai Yang",
                "Michihiro Yasunaga",
                "Dongxu Wang",
                "Zifan Li",
                "James Ma",
                "Irene Li",
                "Qingning Yao",
                "Shanelle Roman",
                "Zilin Zhang",
                "Dragomir Radev"
            ],
            "title": "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic pars",
            "year": 2018
        },
        {
            "authors": [
                "Eric Zelikman",
                "Yuhuai Wu",
                "Jesse Mu",
                "Noah D. Goodman"
            ],
            "title": "2022. Star: Bootstrapping reasoning with reasoning",
            "year": 2022
        },
        {
            "authors": [
                "Zhexin Zhang",
                "Jian Guan",
                "Guowei Xu",
                "Yixiang Tian",
                "Minlie Huang."
            ],
            "title": "Automatic comment generation for Chinese student narrative essays",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demon-",
            "year": 2022
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Alex Smola"
            ],
            "title": "Automatic chain of thought prompting in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Yilun Zhao",
                "Yunxiang Li",
                "Chenying Li",
                "Rui Zhang"
            ],
            "title": "Multihiertt: Numerical reasoning over multi hierarchical tabular and textual data",
            "year": 2022
        },
        {
            "authors": [
                "Victor Zhong",
                "Caiming Xiong",
                "Richard Socher"
            ],
            "title": "Seq2sql: Generating structured queries from natural language using reinforcement learning",
            "year": 2017
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Claire Cui",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Chi"
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Fan Zhou",
                "Haoyu Dong",
                "Qian Liu",
                "Zhoujun Cheng",
                "Shi Han",
                "Dongmei Zhang"
            ],
            "title": "Reflection of thought: Inversely eliciting numerical reasoning in language models via solving linear systems",
            "year": 2022
        },
        {
            "authors": [
                "Fengbin Zhu",
                "Wenqiang Lei",
                "Youcheng Huang",
                "Chao Wang",
                "Shuo Zhang",
                "Jiancheng Lv",
                "Fuli Feng",
                "Tat-Seng Chua"
            ],
            "title": "Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance",
            "year": 2021
        },
        {
            "authors": [
                "Caleb Ziems",
                "William Held",
                "Omar Shaikh",
                "Jiaao Chen",
                "Zhehao Zhang",
                "Diyi Yang"
            ],
            "title": "Can large language models transform computational social science",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Chung et al., 2022; Touvron et al., 2023; OpenAI, 2023a,b) have recently shown emergent abilities, such as the capacity for \"reasoning\", when they are sufficient in size (Wei et al., 2022). A large number of works (Zhang et al., 2022a; Wei et al., 2023; Kojima et al., 2023;\n\u2217Work done during Zhehao and Xitao\u2019s internship at Microsoft Research Asia.\nYao et al., 2023a) focus on LLMs\u2019 reasoning abilities on text-based NLP tasks. However, the capability of LLMs on table reasoning tasks has not been systematically investigated (Chen, 2023a). Evaluating LLMs\u2019 reasoning ability over tabular data and improving their performance can produce a significant impact on efficient data analysis, decisionmaking, and so on in real-life applications.\nCurrent Table question answering (Table QA) datasets are primarily concerned with obtaining factoids to answer simple queries and lack in-depth analysis. Although recent works (Chen et al., 2021b) start to investigate multi-hop \"reasoning\" questions over tables, they do not have a clear definition of reasoning types and the \"reasoning\" they investigate (e.g., operations like filtering) does not align with current research on LLMs\u2019 reasoning ability. Besides, the current Table QA datasets only contain explicitly questions. However, in real-life scenarios, users frequently ask implicit even ambiguous questions over tables.\nTo fill these gaps and conduct an in-depth anal-\nysis of LLMs\u2019 reasoning abilities over tabular data, we first establish a fine-grained taxonomy of commonly-used reasoning and operations types for table analysis. Different from previous works (Chen et al., 2021b), we separate the steps that can be easily executed using a single Pandas or SQL query from reasoning and categorize them as operations. Following recent studies on the reasoning capacity of LLMs(Wei et al., 2023), we focus on informal reasoning which utilizes intuition, experience, and common sense to deduce outcomes.\nThen, we construct CRT-QA dataset (Complex Reasoning QA over Tabular data) over Wikipedia tables. Answer-based evaluation proves inadequate for assessing LLMs\u2019 reasoning ability, as it does not fully capture the complexity of their cognitive processes. Nonetheless, devising a robust method for evaluating such reasoning capabilities remains a formidable challenge within the field. When dealing with complex table analysis queries, humans typically begin by reformulating the questions (possibly implicitly) into more explicit ones, followed by decomposing them into sub-questions, and ultimately conducting atomic reasoning. Inspired by this process, we propose fine-grained annotations on the directness of questions, composition types of sub-questions, and human reasoning paths. To explore the ambiguous questions mentioned earlier, we incorporate a subset of unanswerable and indeterminate queries. During question collection, we propose a human-in-the-loop question generation pipeline that utilizes LLM to generate questions necessitating complex, multi-step reasoning. Our proposed pipeline can efficiently produce high-quality queries while mitigating issues such as biases, insufficient complexity, and lack of diversity.\nWe evaluate LLMs (e.g., GPT-4) with different prompting methods on CRT-QA. Inspired by the finding that LLMs can often generate correct reasoning plans but fail on execution, we propose an efficient and effective method, named ARC (Autoexemplar-guided Reasoning with Code), to alleviate such limitation. Instead of expensive human effort for code design, ARC first uses an instructional prompt to generate exemplar code on the dev set queries and serve as an in-context demonstration for test questions. After executing the generated code with an external Python interpreter, we then inject the output into the prompt and LLM generates the final answer by reflection. Experiment results demonstrate that CRT-QA poses a signifi-\ncant challenge for baseline methods, as the current most powerful model, GPT-4, achieves an accuracy of 56.32% through few-shot in-context learning. Our proposed ARC achieves the best result, outperforming various prompting and tool-use baselines."
        },
        {
            "heading": "2 Related Works",
            "text": ""
        },
        {
            "heading": "2.1 TableQA Datasets",
            "text": "Table QA is the task of answering queries concerning tabular data. A large number of datasets have been proposed for this task. Datasets such as WTQ (Pasupat and Liang, 2015), WikiSQL (Zhong et al., 2017), SQA (Iyyer et al., 2017) and Spider (Yu et al., 2018) contain tables for QA or text-to-SQL tasks. Recently, numerous works construct datasets that require multi-hop reasoning on tables: OTTQA (Chen et al., 2021a), HybridQA (Chen et al., 2021b), TabFact (Chen et al., 2020b), LogicNLG (Chen et al., 2020a), AIT-QA (Katsis et al., 2021), MultiModalQA (Talmor et al., 2021), FeTaQA (Nan et al., 2021). However, they are focused on iterated factoid retrieval (Ho et al., 2022) where the definition of reasoning does not align with the reasoning ability of LLMs. Datasets like FinQA (Chen et al., 2022b), TAT-QA (Zhu et al., 2021), MultiHiertt (Zhao et al., 2022) and TABMWP (Lu et al., 2023b) focus on numerical reasoning over tabular data. Yin et al., 2022 propose ARCADE, a benchmark of 1,082 code generation using the pandas for tabular data analysis. However, they do not introduce commonsense in the datasets and their labels are not natural languages."
        },
        {
            "heading": "2.2 Language Models for Reasoning",
            "text": "LLMs\u2019 reasoning abilities Numerous works (Fu et al., 2023b; Wang et al., 2023b; Zelikman et al., 2022; Creswell et al., 2022; Yao et al., 2023a) focus on increasing LLM\u2019s arithmetic (Lewkowycz et al., 2022; Chen et al., 2022a; Zhou et al., 2022; Taylor et al., 2022), commonsense (Liu et al., 2022; Madaan et al., 2022) and symbolic reasoning (Zhou et al., 2023). Notably, simply adding \u201cLet\u2019s think step by step\u201d before each answer or using chain-ofthought (CoT) (Wei et al., 2023) prompting which contains a number of intermediate steps can better elicit LLM\u2019s reasoning ability.\nLLM with tools External tools such as web browsers, search engines, Python interpreters, and models of other modalities have been incorporated to complete complex tasks (Nakano et al.,\n2022; Shuster et al., 2022; Cheng et al., 2023; Cobbe et al., 2021; Paranjape et al., 2023; Shen et al., 2023; Lu et al., 2023a). Toolformer (Schick et al., 2023) uses self-supervision to teach LLMs to use multiple tools. However, it needs to finetune LLM\u2019s parameters, which makes it impractical to apply it to close-sourced LLMs like GPT4. Yao et al., 2023b propose ReAct, a promptbased paradigm to integrate reasoning and acting for LLMs. However, ReAct requires multiple API callings and hand-crafted exemplars of (Thought, Act,Obs) triplets, which has high calling cost and not flexible to transfer to other tasks."
        },
        {
            "heading": "3 CRT-QA Dataset",
            "text": "In this section, we describe the task formulation, and process for collecting tables, questions, answers, and detailed annotations for CRT-QA."
        },
        {
            "heading": "3.1 Desiderata",
            "text": "The table-based QA task is defined as the problem of generating an answer a to a question q based on a table T with metadata m using a model M which can be formulated as a = M(T,m, q). In our dataset,\nThe format of answers a is free-form natural language. Our dataset focuses on the questions that require multiple steps of operation {o1, o2, ..., on} and reasoning {r1, r2, ..., rn}.\nPrevious TableQA datasets (Chen et al., 2021b) definition of \"reasoning\" primarily encompasses basic operations like filtering, which does not align with the more comprehensive understanding of reasoning in current LLM research, which involves higher-order cognitive tasks such as logical, numerical, and commonsense reasoning. As a result, we separate these steps from reasoning types and define them as operations. Following recent works on LLMs\u2019 reasoning ability (Wei et al., 2023; Cobbe et al., 2021), we examine informal reasoning, which relies on intuition, experience, and common sense to draw conclusions and solve problems. Inspired by benchmarks such as Big-bench (Srivastava et al., 2022), we propose a taxonomy on fine-grained reasoning types commonly used in table analysis. The operation and reasoning types are illustrated in Table 1."
        },
        {
            "heading": "3.2 Dataset Collection",
            "text": "We select open-domain tables from the TabFact (Chen et al., 2020b) datasets, where the tables are from Wikipedia1. Then, inspired by recent works on LLM\u2019s ability to aid human annotations (Bartolo et al., 2022; T\u00f6rnberg, 2023), we design a pipeline to efficiently generate multi-step complex reasoning questions by incorporating LLMs and human feedback. After obtaining the questions, we conduct fine-grained annotations on their directness, decomposition types, and human reasoning paths."
        },
        {
            "heading": "3.2.1 Human-in-the-loop question generation using LLMs",
            "text": "As shown in Figure 2, the pipeline has two main steps: initially generating queries using LLMs, followed by human selection and feedback to enhance them in accordance with human preferences.\nInitial question generation Inspired by the effectiveness of LLMs\u2019 role-playing capability (Park et al., 2023; Wang et al., 2023a; Fu et al., 2023a; Liu et al., 2023), we use LLM (i.e., ChatGPT ) as the question generator, which largely reduces the cost of data annotations. Specifically, we design an instructional prompt containing question requirements to generate question candidates. However, there are three problems when we use such prompts for ChatGPT: (i). lack of complexity: Although we provide corresponding instructions on complex-\n1https://www.wikipedia.org/\nity, ChatGPT usually generates simple questions that do not contain multi-hop reasoning; (ii). lack of diversity: When we ask ChatGPT to generate multiple questions, we find that many queries have similar formats. For example, the majority of them start with \u2019Is there\u2019; (iii). unanswerable questions: ChatGPT may generate questions that can not be answered only given the table. We collect some unanswerable and indeterminate questions and conduct an in-depth analysis in Section 6. The next paragraph described the approach we use to mitigate the above issues.\nHuman selection and feedback Human feedback is essential for LLMs because it helps them align with human preferences and values. Inspired by recent works on model refinement (Ouyang et al., 2022; Huang et al., 2022; Shinn et al., 2023), we let human annotators select the questions that meet our requirements and then provide LLM with feedback to improve the quality of the questions. For feedback design, we use several lexical features such as use math and more complex to resolve the problems mentioned above and reduce potential biases. Empirically, we find that ChatGPT can better improve their generated questions by providing them with specific lexical features than high-level instructions. Details on the feedback design can be found in Appendix A.1."
        },
        {
            "heading": "3.2.2 Fine-grained annotations",
            "text": "Among the reasoning datasets, most of them only contain label-related annotations without human reasoning paths or fine-grained reasoning types. However, we argue that only goal-oriented annotations are insufficient to analyze the reasoning ability of LLMs. To fill in this gap, after annotating the answer, we further annotate whether a question is implicit or explicit and how sub-questions are composed. We also annotate the main steps of table operations and reasoning. After that, we use a template-filling method to efficiently annotate human reasoning paths to solve the questions. The details on template design and the complete annotation interface can be found in Appendix E and F.\nDirectness Inspired by StrategyQA (Geva et al., 2021), we first introduce implicit questions over tabular data. Following Geva et al., 2021, we use the following rule-of-thumb to determine whether a question is implicit or explicit: the question is explicit if it can be written using words from the question, their inflections, and function words, while implicit questions require new content words to describe the reasoning process.\nDecomposition types As the queries in our dataset contain multi-step reasoning, we further annotate how these sub-questions are composed together. Following Min et al., 2019, we categorize the question decomposition into the following 3 types2: bridging needs to find the first-hop evidence in order to find the second-hop evidence; intersection requires finding an entity that meets two independent requirements; comparison requires comparing the property of two different entities. Our annotation can be used to analyze LLMs\u2019 question decomposition abilities.\nHuman reasoning path To better evaluate LLMs\u2019 reasoning ability, we further annotate human reasoning paths for solving these queries. However, it is impractical for annotators to write their detailed reasoning paths due to the great volume of data. Hence, we design a template-filling paradigm to let annotators fill the objects of reasoning or operation. We first let annotators select the type of reasoning or operation for each step in order (selections are listed in Table 1). Then, for each step, they are asked to fill in a template\n2Examples of these 3 decomposition types are in Appendix B\nProperty Value\nUnique Tables 423 Total Questions 1000 Answerable Questions 744 Unanswerable Questions 256 Question Length (Avg/Median)r 141.2 / 144.5 Answer Length (Avg/Median) 5.5 / 3.0 Annotation Length (Avg/Median)r 54.3 / 45.0 Rows per Table (Avg/Median) 12.6 / 10.0 Num of reasoning (Avg/Median)r 3.2 / 3.3 Num of operation (Avg/Median)r 3.1 / 2.8 Length of reasoning path (Avg/Median) r 2.9 / 3.0\nComplexity (Agreement)\u2020 4.1 (88%) Inter-annotator Agreement\u2021 93.7%"
        },
        {
            "heading": "3.3 Dataset Analysis and Statistics",
            "text": "has a sub-set of unanswerable and indeterminate questions, which are frequently occurred due to the complexity of real-life scenarios."
        },
        {
            "heading": "4 Method",
            "text": "Although LLMs show powerful reasoning abilities on various tasks (Qin et al., 2023), they have limitations on various fields (Lewkowycz et al., 2022; Ziems et al., 2023). From our pivot experiments of prompting baselines, we find that LLMs can often generate correct reasoning plans but are unable to appropriately execute them. However, such steps (e.g., arithmetic, counting) can be perfectly performed by external tools such as SQL or Pandas. Inspired by recent works on tool-augmented LLMs (Gao et al., 2023; Yao et al., 2023b; Shen et al., 2023; Lu et al., 2023a; Paranjape et al., 2023), we propose an efficient and effective approach, named ARC (Auto-exemplar-guided Reasoning with Code), to use external tools such as Pandas to solve table reasoning tasks without handcrafted demonstrations. Figure 3 illustrates the pipeline of our proposed ARC and the detailed prompt design for each step can be found in Appendix H.\nAuto-exemplar generation Although manuallyannotated demonstration shows significant effectiveness in in-context learning for LLMs, nontrivial hand-drafting of effective exemplars makes it not flexible enough to be applied to sophisticated tasks\nsuch as code generation for complex table analysis. Inspired by recent works on auto-demonstration generation (Zhang et al., 2022b), we first randomly sample a data instance from the development set and input LLMs with an instructional prompt for code generation. The prompt we use is a simple instruction to generate Python code and print intermediate or final results. The ablation of different selections will be discussed in Appendix H.\nIn-context code generation For every data example in the test set, we use the exemplar generated from the dev set to conduct in-context learning for code generation. As Pandas is the most commonlyused library in Python for tabular data analysis which may frequently occur in LLMs\u2019 pretraining data, the generated codes are proficient in the use of Pandas to process tables.\nCode execution with external tools We further use the generated code for execution using a Python interpreter with a Pandas installed environment. We then obtain the output of the program as the intermediate or final results for the query.\nIterative LLM calling with code output However, for queries that require in-depth commonsense reasoning, only the code sometimes can not directly solve them. As a result, inspired by ReAct (Yao et al., 2023b), we also integrate Acting and Reasoning by injecting code output into the prompt design for final step reasoning. By prompting LLMs with code output, LLMs can generate more accurate final answers by avoiding step execution errors."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Experiment Settings",
            "text": "For all the experiments, we use the powerful ChatGPT, GPT-3.5-turbo, and GPT-4 as the LLMs to investigate their reasoning ability on tabular data. Following Chen, 2023b, we use Markdown as the format of tables and use Exact Match (EM) as the main metric for evaluations. For each experiment, we run three times with different random seeds and report the average EM score."
        },
        {
            "heading": "5.2 Baselines",
            "text": "To evaluate LLMs\u2019 reasoning ability on the complex TableQA task, we select the following baselines:4 Few-shot/Zero-shot prompting (Brown 4Implementation details can be found in Appendix G\net al., 2020): simply prompts LLMs with fewshot examples or instructions. Few-shot/Zeroshot CoT (Wei et al., 2023; Kojima et al., 2023): inputs LLMs few-shot exemplars with manuallycrafted reasoning path or Let\u2019s think step by step. PAL (Gao et al., 2023): uses few-shot examples of only Python code to encourage LLMs to generate correct code for problem-solving. ReAct (Yao et al., 2023b) utilizes in-context examples of (Thought, Act,Obs) tuples to combine logical path and task-specific actions. ARC utilizes a zero-shotgenerated code exemplar to perform in-context code generation and incorporate code output for final answer generation."
        },
        {
            "heading": "5.3 Experiment Results",
            "text": "Table 3 shows different methods\u2019 EM scores on CRT-QA dataset. We can see that (1). overall, the most efficacious approach achieves a maximum\nof 60.11, indicating the difficulty of our dataset. (2). among all the baselines, our proposed ARC achieves the best average EM scores with an average improvement of 1.846 across all models without using any handcrafted exemplar, indicating the effectiveness of our proposal. For ChatGPT baselines. (3). we find that Zero-shot-CoT performs even worse than the vanilla Zero-shot approach. By checking the reasoning paths elicited by Let\u2019s think step-by-step, we find that the reason may arise from the phenomenon that the reasoning paths are unruly and even generate codes that the model does not have the ability to solve. As a result, Let\u2019s think step-by-step is not a one-fits-for-all solution. (4). although Few-shot-CoT can not outperform Zero-shot for ChatGPT. As the model evolves (i.e., from ChatGPT to Turbo to GPT-4), Few-shot-CoT can have better performances than Zero-shot predictions, indicating that the model increases its\nCoT reasoning ability. (5). Among the 3 tool-use baselines, ReAct can not have comparative performances with the other two methods with GPT-3.5turbo and GPT-4. By investigating the reasoning path, we find that ReAct often finishes without any answer. Alternatively, ReAct often conducts a substantial number of iterations, resulting in not only increased costs but also an extremely long reasoning pathway that becomes out of control.\nFrom fine-grained reasoning types shown in Table 3, we observe that all prompting-based methods are bad at aggregation and arithmetic compared with other reasoning types. Noticeably, our proposed ARC and PAL can greatly improve LLMs\u2019 ability on these two reasoning types. Besides, we observe that among all the reasoning types, LLMs perform the best in reasoning with quantifiers. Due to page constraints, a comprehensive ablation study on the number of exemplars, error analysis, and case study are in Appendix H, J, and I."
        },
        {
            "heading": "6 Unanswerable and Indeterminate Question",
            "text": "Most Table QA datasets are designed for answering the questions with golden labeling (Pasupat and Liang, 2015; Chen et al., 2020b, 2021b,a), but real users possibly ask questions that are inherently difficult to answer due to the complexity of the real world. Motivated by this, we incorporate a sub-set of unanswerable and indeterminate queries where some questions go beyond common external knowledge, while others are inherently problematic. We categorize these questions into four categories and conduct the answerability of LLMs based on them.\nAs shown in Table 4, out-of-scope, hallucination, and problematic questions are unanswerable. The main reason is the absence of essential information or logical flaws within the question itself. For example, there is an implicit assumption underlying\nBased on the results presented in Table 5, Binary Classification shows improvements over Random, indicating its effectiveness in identifying questions\u2019 answerability. Question Answering proves to be the most effective approach for identifying answerability, probably because generating answers is easier than determining whether a question can be answered. It mimics some pre-training tasks like reading comprehension. This study benefits a broader understanding of how language models can tackle unanswerable and indeterminate questions and provides directions to enhance performance."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, to systematically evaluate LLMs\u2019 reasoning ability on tabular data, we first establish a comprehensive taxonomy on operation and reasoning types for table analysis. Then, we propose CRT-QA, a dataset of complex reasoning QA over\ntables. We propose ARC which effectively utilizes table analysis tools to solve table reasoning tasks without manually-annotated exemplars. Extensive experiments show CRT-QA poses a significant challenge for LLMs and our proposed ARC achieves the best EM scores. Besides the main experiments, we also conduct thorough ablation studies, error analyses, answerability study, and case study for further analysis.\nLimitations\n(1) CRT-QA is a test-only dataset, which means no gradient updates are performed. While striving for problem complexity, we face challenges in balancing the quantity of our dataset. This is primarily due to the intricate nature of our annotation process, which demands more time for answer generation and fine-grained process labeling. (2) Similar to previous works discussed in Table 11, we only foucs on single-table question answering. However, queries across multi-tables are also common in real-life table analysis scenarios. (3) We don\u2019t research the boundary of external knowledge. The appearance of unanswerable and indeterminate questions is associated with our data generation goal, which is to generate complex and diverse questions. Specifically, the indeterminate questions are contrasted with implicit questions, while indeterminate questions stand out beyond implicit questions. We leave this study as future work. (4) In our study, we utilize a combination of exact match and human evaluation as our evaluation metric. It is reasonable because, during the question generation process, we only select the questions that can be answered within several words without ambiguity. Although this is not comprehensive for free-form answer-generation tasks, alternative metrics such as F1, ROUGE-L, and BLEU-1 also possess inherent limitations. Evaluation of the free-form answergeneration task seems promising. Moreover, our fine-grained annotations provide a feasible path to help answer the question. Although the path is not unique. Currently, there is no effective method to evaluate the reasoning path. This aspect will be left for future research and development."
        },
        {
            "heading": "A Prompt Design",
            "text": "A.1 Human feedback for question generation The followings are examples of feedback for LLMs to generate desired questions.\n\u2022 Generate another 10 more complex questions.\n\u2022 Generate another 10 questions with different question types.\n\u2022 Generate another 10 more complex questions that require math to solve them.\n\u2022 Generate another 10 more complex questions that require common sense for column 1. Other choices may also improve the quality of LLMs\u2019 generated questions.\nA.2 Prompt for Baselines All prompt designs for the main experiment and experiment in Section 6 can be found in Table 8 and Table 9 respectively."
        },
        {
            "heading": "B Question Decomposition Types",
            "text": "Following Min et al., 2019, we study the following three different types of question decomposition types:\n\u2022 Bridging: requires finding first-hop evidence before moving on to the second-hop evidence. Example question: \"What was the average number of years between a TV station\u2019s affiliation with the e! Canadian TV system and their eventual disaffiliation?\".\n\u2022 Intersection requires finding an entity that meets two independent conditions. Example question: \"Are there any counties within the Mid-Indiana Football Conference that contain more than one school?\".\n\u2022 Comparison requires comparing the features of two distinct entities. Example question: \"How often does Tim Lajcik win fights in the first round compared to subsequent rounds?\"."
        },
        {
            "heading": "C Data Topic Distribution",
            "text": "Following Parikh et al., 2020, we use Wikimedia Foundation\u2019s topic categorization model (Asthana and Halfaker, 2018) to visualize the topic distribution of our dataset. Figure 4 shows that our data are mostly related to sports, biography, regions, and media. Overall, CRT-QA dataset covers a fairly wide range of topic domains."
        },
        {
            "heading": "D Question Type Distribution",
            "text": "Following (Yang et al., 2018), by taking the three neighboring tokens along with the central question word (CQW), we can determine the question types. A visual representation of the distribution is shown in Figure 5, which illustrates the syntactic diversity of questions in our proposed CRT-QA."
        },
        {
            "heading": "E Data Annotation Interface",
            "text": "Figure 8 shows the detailed interface for data annotation."
        },
        {
            "heading": "F Data Annotation Details",
            "text": "We enroll 2 undergraduate students and 1 Ph.D. student majoring in computer science for data annotations. All of them have at least one year of data analysis experience."
        },
        {
            "heading": "G Experiment Implementation Details",
            "text": "The models we use for experiments are text-chat-davinci-003, GPT-3.5-turbo, and GPT-4 through Microsoft Azure API. For tool-use baselines, empirically, we find that the LLM-generated code may contain some syntax errors which make it impossible to run and generate output. For these cases, we let LLM re-generate code a maximum of five times. Once it\nPrompt Design for Baselines\nzero-shot Table Read the table below regarding \u201cyugoslavia national football team results\"\n| | date | city | opponent | results | type of game | |\u2014\u2013:|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013| | 0 | april 18 | belgrade | france | 1:0 | 1966 wcq | | 1 | may 9 | belgrade | england | 1:1 | friendly | | 2 | june 16 | oslo , norway | norway | 0:3 | 1966 wcq | | 3 | september 4 | moscow , russia | ussr | 0:0 | friendly | | 4 | september 19 | luxembourg | luxembourg | 5:2 | 1966 wcq | | 5 | october 9 | paris , france | france | 0:1 | 1966 wcq | | 6 | november 7 | belgrade | norway | 1:1 | 1966 wcq |\nQuestion Did the Yugoslavia national football team play any games against teams outside of Europe in the table? Answer with only \u2019Yes\u2019 or \u2019No\u2019 that is most accurate and nothing else. Answer\nzero-shot-CoT Table Read the table below regarding \u201cyugoslavia national football team results\"\n| | date | city | opponent | results | type of game | |\u2014\u2013:|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013| | 0 | april 18 | belgrade | france | 1:0 | 1966 wcq | | 1 | may 9 | belgrade | england | 1:1 | friendly | | 2 | june 16 | oslo , norway | norway | 0:3 | 1966 wcq | | 3 | september 4 | moscow , russia | ussr | 0:0 | friendly | | 4 | september 19 | luxembourg | luxembourg | 5:2 | 1966 wcq | | 5 | october 9 | paris , france | france | 0:1 | 1966 wcq | | 6 | november 7 | belgrade | norway | 1:1 | 1966 wcq |\nQuestion Did the Yugoslavia national football team play any games against teams outside of Europe in the table? Answer with only \u2019Yes\u2019 or \u2019No\u2019 that is most accurate and nothing else. Answer Let\u2019s think step-by-step\n1-shot Table Read the table below regarding \"1982 all - ireland senior hurling championship\" to answer the following questions.\n| |rank| player | county | tally | total | matches | average | |\u2014-:|\u2014\u2014-:|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014|:\u2014\u2014\u2014\u2014\u2014\u2014\u2013|\u2014\u2014\u2014\u2014\u2014\u2014:|\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014:|\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013:| | 0 | 1 | p\u00e1draig horan | offaly | 5 - 17 | 32 | 4 | 8 | | 1 | 2 | billy fitzpatrick | kilkenny | 2 - 24 | 30 | 4 | 7.5 | | 2 | 3 | tony o \u2019sullivan | cork | 0 - 28 | 28 | 4 | 7 | | 3 | 4 | p j molloy | galway | 3 - 11 | 20 | 2 | 10 | | 4 | 5 | christy heffernan | kilkenny | 3 - 9 | 18 | 4 | 4.5 | | 5 | 5 | pat horgan | cork | 0 - 18 | 18 | 4 | 4.5 |\nQuestion How many players in the 1982 all-Ireland senior hurling championship had a higher average score per game than the overall average score per game of the competition? Answer 4 Table Read the table below regarding \u201cyugoslavia national football team results\"\n| | date | city | opponent | results | type of game | |\u2014\u2013:|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013| | 0 | april 18 | belgrade | france | 1:0 | 1966 wcq | | 1 | may 9 | belgrade | england | 1:1 | friendly | | 2 | june 16 | oslo , norway | norway | 0:3 | 1966 wcq | | 3 | september 4 | moscow , russia | ussr | 0:0 | friendly | | 4 | september 19 | luxembourg | luxembourg | 5:2 | 1966 wcq | | 5 | october 9 | paris , france | france | 0:1 | 1966 wcq | | 6 | november 7 | belgrade | norway | 1:1 | 1966 wcq |\nQuestion Did the Yugoslavia national football team play any games against teams outside of Europe in the table? Answer with only \u2019Yes\u2019 or \u2019No\u2019 that is most accurate and nothing else. Answer\n2-shot Table Read the table below regarding \"1982 all - ireland senior hurling championship\" to answer the following questions.\n| |rank| player | county | tally | total | matches | average | |\u2014-:|\u2014\u2014-:|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014|:\u2014\u2014\u2014\u2014\u2014\u2014\u2013|\u2014\u2014\u2014\u2014\u2014\u2014:|\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014:|\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013:| | 0 | 1 | p\u00e1draig horan | offaly | 5 - 17 | 32 | 4 | 8 | | 1 | 2 | billy fitzpatrick | kilkenny | 2 - 24 | 30 | 4 | 7.5 | | 2 | 3 | tony o \u2019sullivan | cork | 0 - 28 | 28 | 4 | 7 | | 3 | 4 | p j molloy | galway | 3 - 11 | 20 | 2 | 10 | | 4 | 5 | christy heffernan | kilkenny | 3 - 9 | 18 | 4 | 4.5 | | 5 | 5 | pat horgan | cork | 0 - 18 | 18 | 4 | 4.5 |\nQuestion How many players in the 1982 all-Ireland senior hurling championship had a higher average score per game than the overall average score per game of the competition? Answer 4 Table Read the table below regarding \"g.d. estoril praia\" to answer the following questions.\n| | season | competition | round | opponent | home | away | |\u2014\u2014:|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014|:\u2014\u2014\u2014\u2014\u2014\u2014| | 0 | 2013 - 14 | uefa europa league | 3q | hapoel ramat gan | 0 - 0 | 1 - 0 | | 1 | 2013 - 14 | uefa europa league | play - off | pasching | 2 - 0 | 2 - 1 | | 2 | 2013 - 14 | uefa europa league | group h | sevilla | 1 - 2 | - | | 3 | 2013 - 14 | uefa europa league | group h | slovan liberec | - | 1 - 2 | | 4 | 2013 - 14 | uefa europa league | group h | freiburg | - | 1 - 1 |\nQuestion: Was there a correlation between GD Estoril Praia\u2019s performance in home games and away games during the 2013-14 UEFA Europa League competition? Answer: No\nContinued on next page\ngenerates runnable code, we execute it and get the output. If the LLM can not generate runnable code five times, we keep the code in the prompt and set the output to \"None\". The hyperparameters we use can be found in Table 6."
        },
        {
            "heading": "H Ablation Study",
            "text": "For our proposed ARC, we select 3 different examples from the dev set to conduct zero-shot code generation as exemplars for the test set. Table 7 shows that the performance difference among 3 different selections is within 1 EM score demonstrating the robustness of our proposal.\nWe also design four sets of contrast experiments for the ablation study as Figure 6 shows. We find the table reasoning ability differs from the models. GPT4 is best and turbo performs on par with ChatGPT. For the in-context learning, GPT4 benefits a lot from the increase in the number of demonstrations, but the increase is not significant for other models. We study the impact of up to 2-shot because structured tables consuming lots of tokens can easily break the input limitation.\nAs expected, different decomposition types vary in difficulty with bridging being the most challenging and comparison being the easiest. Besides, LLMs obtain similar performances on implicit and explicit questions in our Table QA dataset."
        },
        {
            "heading": "I Case Study",
            "text": "We show how our method ACR uses external tools to solve table reasoning tasks in Figure 7. The comments in ACR show the reasoning sketch and guide the generation of code. Using external tools enhances numerical computation compared with plain text reasoning. In contrast, CoT fails even with the right reasoning path."
        },
        {
            "heading": "J Error Analyses",
            "text": "To analyze how the error was caused, we randomly choose 50 samples and go depth into error analysis based on the performance of ARC.\nWe find five types of errors: (1) Code generation error (20%). The code is not executable and the output is none or an illegal type. (2) Gross error of reasoning (32%). The reasoning path deviates from the requirements of the user query. (3) Condition missing error (18%). The code framework has no problem in general, but some subtle conditions or operations are missed. (4) Format error (26%). The model return with an answer but can\u2019t be judged\nby the metric. To ease the trouble of format error, we augment the EM with human evaluation. (5) Refuse to answer (4%). The answerable query is regarded as an unanswerable question. The model\u2019s completion contain some expressions that refused to answer like \"I am unable to write Python code for this question as the data does not provide information ...\". We find that the ARC method of the program enhances numerical processing while weakening the semantic recognition of the text. So the method makes mistakes for the match of strings such as \"n/a\" and \"n / a\".\nPrompt Design for Baselines\nTable Read the table below regarding \u201cyugoslavia national football team results\" | | date | city | opponent | results | type of game | |\u2014\u2013:|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013| | 0 | april 18 | belgrade | france | 1:0 | 1966 wcq | | 1 | may 9 | belgrade | england | 1:1 | friendly | | 2 | june 16 | oslo , norway | norway | 0:3 | 1966 wcq | | 3 | september 4 | moscow , russia | ussr | 0:0 | friendly | | 4 | september 19 | luxembourg | luxembourg | 5:2 | 1966 wcq | | 5 | october 9 | paris , france | france | 0:1 | 1966 wcq | | 6 | november 7 | belgrade | norway | 1:1 | 1966 wcq | Question Did the Yugoslavia national football team play any games against teams outside of Europe in the table? Answer with only \u2019Yes\u2019 or \u2019No\u2019 that is most accurate and nothing else. Answer\nCoT 2-shot Table Read the table below regarding \"1982 all - ireland senior hurling championship\" to answer the following questions.\n| |rank| player | county | tally | total | matches | average | |\u2014-:|\u2014\u2014-:|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014|:\u2014\u2014\u2014\u2014\u2014\u2014\u2013|\u2014\u2014\u2014\u2014\u2014\u2014:|\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014:|\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013:| | 0 | 1 | p\u00e1draig horan | offaly | 5 - 17 | 32 | 4 | 8 | | 1 | 2 | billy fitzpatrick | kilkenny | 2 - 24 | 30 | 4 | 7.5 | | 2 | 3 | tony o \u2019sullivan | cork | 0 - 28 | 28 | 4 | 7 | | 3 | 4 | p j molloy | galway | 3 - 11 | 20 | 2 | 10 | | 4 | 5 | christy heffernan | kilkenny | 3 - 9 | 18 | 4 | 4.5 | | 5 | 5 | pat horgan | cork | 0 - 18 | 18 | 4 | 4.5 |\nQuestion How many players in the 1982 all-Ireland senior hurling championship had a higher average score per game than the overall average score per game of the competition? Explanation We first find the column of \"average\" and compute the average of all the players, which is (8 + 7.5 + 7 + 10 + 4.5 + 4.5)/6 = 6.917. Then we count the number of player whose average is larger than 6.917. As a result, the answer is 4. Table Read the table below regarding \"g.d. estoril praia\" to answer the following questions. | | season | competition | round | opponent | home | away | |\u2014\u2014:|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014|:\u2014\u2014\u2014\u2014\u2014\u2014| | 0 | 2013 - 14 | uefa europa league | 3q | hapoel ramat gan | 0 - 0 | 1 - 0 | | 1 | 2013 - 14 | uefa europa league | play - off | pasching | 2 - 0 | 2 - 1 | | 2 | 2013 - 14 | uefa europa league | group h | sevilla | 1 - 2 | - | | 3 | 2013 - 14 | uefa europa league | group h | slovan liberec | - | 1 - 2 | | 4 | 2013 - 14 | uefa europa league | group h | freiburg | - | 1 - 1 | Question: Was there a correlation between GD Estoril Praia\u2019s performance in home games and away games during the 2013-14 UEFA Europa League competition? Explanation We first find the column of \"home\" and \"away\" and compare the outcome of \"home\" and \"away\" games. Then we find there is no correlation between \"home\" and \"away\" games. As a result, the answer is No. Table Read the table below regarding \u201cyugoslavia national football team results\" | | date | city | opponent | results | type of game | |\u2014\u2013:|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013| | 0 | april 18 | belgrade | france | 1:0 | 1966 wcq | | 1 | may 9 | belgrade | england | 1:1 | friendly | | 2 | june 16 | oslo , norway | norway | 0:3 | 1966 wcq | | 3 | september 4 | moscow , russia | ussr | 0:0 | friendly | | 4 | september 19 | luxembourg | luxembourg | 5:2 | 1966 wcq | | 5 | october 9 | paris , france | france | 0:1 | 1966 wcq | | 6 | november 7 | belgrade | norway | 1:1 | 1966 wcq | Question Did the Yugoslavia national football team play any games against teams outside of Europe in the table? Answer with only \u2019Yes\u2019 or \u2019No\u2019 that is most accurate and nothing else. Explanation\nPAL Instruction Let\u2019s use Python to solve Table-based question answering. Here is an example how to do it, Table Tittle: \"1982 all - ireland senior hurling championship\"\n| |rank| player | county | tally | total | matches | average | |\u2014-:|\u2014\u2014-:|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014|:\u2014\u2014\u2014\u2014\u2014\u2014\u2013|\u2014\u2014\u2014\u2014\u2014\u2014:|\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014:|\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013:| | 0 | 1 | p\u00e1draig horan | offaly | 5 - 17 | 32 | 4 | 8 | | 1 | 2 | billy fitzpatrick | kilkenny | 2 - 24 | 30 | 4 | 7.5 | | 2 | 3 | tony o \u2019sullivan | cork | 0 - 28 | 28 | 4 | 7 | | 3 | 4 | p j molloy | galway | 3 - 11 | 20 | 2 | 10 | | 4 | 5 | christy heffernan | kilkenny | 3 - 9 | 18 | 4 | 4.5 | | 5 | 5 | pat horgan | cork | 0 - 18 | 18 | 4 | 4.5 |\nQuestion How many players in the 1982 all-Ireland senior hurling championship had a higher average score per game than the overall average score per game of the competition? Code import pandas as pd # get the overall average score per game of the competition overall_avg = df[\u2019average\u2019].mean() # filter the dataframe to only include players with a higher average score per game than the overall average higher_avg_df = df[df[\u2019average\u2019] > overall_avg] # count the number of players with a higher average score per game than the overall average num_higher_avg_players = len(higher_avg_df) # print the result print(\"Number of players with a higher average score per game than the overall average: \", num_higher_avg_players) Instruction \"How about this question? \" Table Tittle: \u201cyugoslavia national football team results\"\n| | date | city | opponent | results | type of game | |\u2014\u2013:|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014-|:\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013| | 0 | april 18 | belgrade | france | 1:0 | 1966 wcq | | 1 | may 9 | belgrade | england | 1:1 | friendly | | 2 | june 16 | oslo , norway | norway | 0:3 | 1966 wcq |\nContinued on next page\nPrompt Design for Baselines\n| 3 | september 4 | moscow , russia | ussr | 0:0 | friendly | | 4 | september 19 | luxembourg | luxembourg | 5:2 | 1966 wcq | | 5 | october 9 | paris , france | france | 0:1 | 1966 wcq | | 6 | november 7 | belgrade | norway | 1:1 | 1966 wcq |\nQuestion Did the Yugoslavia national football team play any games against teams outside of Europe in the table? Answer with only \u2019Yes\u2019 or \u2019No\u2019 that is most accurate and nothing else. Code"
        }
    ],
    "title": "CRT-QA: A Dataset of Complex Reasoning Question Answering over Tabular Data",
    "year": 2023
}