{
    "abstractText": "Large Language Models (LLMs) have demonstrated incredible capabilities in understanding, generating, and manipulating languages. Through human-model interactions, LLMs can automatically understand human-issued instructions and output the expected contents, which can significantly increase working efficiency. In various types of real-world demands, editingoriented tasks account for a considerable proportion, which involves an interactive process that entails the continuous refinement of existing texts to meet specific criteria. Due to the need for multi-round human-model interaction and the generation of complicated editing tasks, there is an emergent need for efficient general editing models. In this paper, we propose General SParse Efficient Editing MoDel (G-SPEED), which can fulfill diverse editing requirements through a single model while maintaining low computational costs. Specifically, we first propose a novel unsupervised text editing data clustering algorithm to deal with the data scarcity problem. Subsequently, we introduce a sparse editing model architecture to mitigate the inherently limited learning capabilities of small language models. The experimental outcomes indicate that GSPEED, with its 508M parameters, can surpass LLMs equipped with 175B parameters. Our code and model checkpoints are available at https://github.com/Banner-Z/G-SPEED.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haoke Zhang"
        },
        {
            "affiliations": [],
            "name": "Yue Wang"
        },
        {
            "affiliations": [],
            "name": "Juntao Li"
        },
        {
            "affiliations": [],
            "name": "Xiabing Zhou"
        },
        {
            "affiliations": [],
            "name": "Min Zhang"
        }
    ],
    "id": "SP:e9f00e866569134d7bbcae2dfa25b6466c1c476d",
    "references": [
        {
            "authors": [
                "Fernando Alva-Manchego",
                "Louis Martin",
                "Antoine Bordes",
                "Carolina Scarton",
                "Beno\u00eet Sagot",
                "Lucia Specia."
            ],
            "title": "Asset: A dataset for tuning and evaluation of sentence simplification models with multiple rewriting transformations",
            "venue": "Proceedings of the 58th",
            "year": 2020
        },
        {
            "authors": [
                "Talita Anthonio",
                "Irshad Bhat",
                "Michael Roth."
            ],
            "title": "wikihowtoimprove: A resource and analyses on edits in instructional texts",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 5721\u20135729.",
            "year": 2020
        },
        {
            "authors": [
                "David Arthur",
                "Sergei Vassilvitskii."
            ],
            "title": "Kmeans++ the advantages of careful seeding",
            "venue": "Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027\u20131035.",
            "year": 2007
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko E Agirre",
                "I\u00f1igo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
            "venue": "The 11th International Workshop on Semantic Evaluation (SemEval-2017),",
            "year": 2017
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-",
            "year": 2019
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Xiaojun Wan",
                "Yue Cao."
            ],
            "title": "Parasci: A large scientific paraphrase dataset for longer paraphrase generation",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Nan Du",
                "Yanping Huang",
                "Andrew M Dai",
                "Simon Tong",
                "Dmitry Lepikhin",
                "Yuanzhong Xu",
                "Maxim Krikun",
                "Yanqi Zhou",
                "Adams Wei Yu",
                "Orhan Firat"
            ],
            "title": "2022a. Glam: Efficient scaling of language models with mixture-of-experts",
            "year": 2022
        },
        {
            "authors": [
                "Wanyu Du",
                "Vipul Raheja",
                "Dhruv Kumar",
                "Zae Myung Kim",
                "Melissa Lopez",
                "Dongyeop Kang."
            ],
            "title": "Understanding iterative revision from human-written text",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2022
        },
        {
            "authors": [
                "Jane Dwivedi-Yu",
                "Timo Schick",
                "Zhengbao Jiang",
                "Maria Lomeli",
                "Patrick Lewis",
                "Gautier Izacard",
                "Edouard Grave",
                "Sebastian Riedel",
                "Fabio Petroni."
            ],
            "title": "Editeval: An instruction-based benchmark for text improvements",
            "venue": "arXiv preprint arXiv:2209.13331.",
            "year": 2022
        },
        {
            "authors": [
                "Martin Ester",
                "Hans-Peter Kriegel",
                "J\u00f6rg Sander",
                "Xiaowei Xu"
            ],
            "title": "A density-based algorithm for discovering clusters in large spatial databases with noise",
            "venue": "In kdd,",
            "year": 1996
        },
        {
            "authors": [
                "Felix Faltings",
                "Michel Galley",
                "Gerold Hintz",
                "Chris Brockett",
                "Chris Quirk",
                "Jianfeng Gao",
                "William B Dolan."
            ],
            "title": "Text editing by command",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "William Fedus",
                "Jeff Dean",
                "Barret Zoph."
            ],
            "title": "A review of sparse expert models in deep learning",
            "venue": "arXiv preprint arXiv:2209.01667.",
            "year": 2022
        },
        {
            "authors": [
                "William Fedus",
                "Barret Zoph",
                "Noam Shazeer."
            ],
            "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
            "venue": "The Journal of Machine Learning Research, 23(1):5232\u2013 5270.",
            "year": 2022
        },
        {
            "authors": [
                "Mor Geva",
                "Eric Malmi",
                "Idan Szpektor",
                "Jonathan Berant."
            ],
            "title": "Discofuse: A large-scale dataset for discourse-based sentence fusion",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2019
        },
        {
            "authors": [
                "Shashank Gupta",
                "Subhabrata Mukherjee",
                "Krishan Subudhi",
                "Eduardo Gonzalez",
                "Damien Jose",
                "Ahmed H Awadallah",
                "Jianfeng Gao."
            ],
            "title": "Sparsely activated mixture-of-experts are robust multi-task learners",
            "venue": "arXiv preprint arXiv:2204.07689.",
            "year": 2022
        },
        {
            "authors": [
                "Sebastian Jaszczur",
                "Aakanksha Chowdhery",
                "Afroz Mohiuddin",
                "Lukasz Kaiser",
                "Wojciech Gajewski",
                "Henryk Michalewski",
                "Jonni Kanerva."
            ],
            "title": "Sparse is enough in scaling transformers",
            "venue": "Advances in Neural Information Processing Systems, 34:9895\u20139907.",
            "year": 2021
        },
        {
            "authors": [
                "Chao Jiang",
                "Mounica Maddela",
                "Wuwei Lan",
                "Yang Zhong",
                "Wei Xu."
            ],
            "title": "Neural crf model for sentence alignment in text simplification",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7943\u20137960.",
            "year": 2020
        },
        {
            "authors": [
                "Zae Myung Kim",
                "Wanyu Du",
                "Vipul Raheja",
                "Dhruv Kumar",
                "Dongyeop Kang."
            ],
            "title": "Improving iterative text revision by learning where to edit from other revision tasks",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "James Lee-Thorp",
                "Joshua Ainslie."
            ],
            "title": "Sparse mixers: Combining moe and mixing to build a more efficient bert",
            "venue": "arXiv preprint arXiv:2205.12399.",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Shruti Bhosale",
                "Tim Dettmers",
                "Naman Goyal",
                "Luke Zettlemoyer."
            ],
            "title": "Base layers: Simplifying training of large, sparse models",
            "venue": "International Conference on Machine Learning, pages 6265\u20136274. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Jiaqi Ma",
                "Zhe Zhao",
                "Xinyang Yi",
                "Jilin Chen",
                "Lichan Hong",
                "Ed H Chi."
            ],
            "title": "Modeling task relationships in multi-task learning with multi-gate mixtureof-experts",
            "venue": "Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &",
            "year": 2018
        },
        {
            "authors": [
                "Aman Madaan",
                "Niket Tandon",
                "Prakhar Gupta",
                "Skyler Hallinan",
                "Luyu Gao",
                "Sarah Wiegreffe",
                "Uri Alon",
                "Nouha Dziri",
                "Shrimai Prabhumoye",
                "Yiming Yang"
            ],
            "title": "Self-refine: Iterative refinement with self-feedback",
            "venue": "arXiv preprint arXiv:2303.17651",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Mallinson",
                "Jakub Adamek",
                "Eric Malmi",
                "Aliaksei Severyn."
            ],
            "title": "EdiT5: Semi-autoregressive text editing with t5 warm-start",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2126\u20132138, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Mallinson",
                "Aliaksei Severyn",
                "Eric Malmi",
                "Guillermo Garrido."
            ],
            "title": "Felix: Flexible text editing through tagging and insertion",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1244\u20131255.",
            "year": 2020
        },
        {
            "authors": [
                "Eric Malmi",
                "Sebastian Krause",
                "Sascha Rothe",
                "Daniil Mirylenka",
                "Aliaksei Severyn."
            ],
            "title": "Encode, tag, realize: High-precision text editing",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Hannaneh Hajishirzi."
            ],
            "title": "Cross-task generalization via natural language crowdsourcing instructions",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Courtney Napoles",
                "Keisuke Sakaguchi",
                "Matt Post",
                "Joel Tetreault."
            ],
            "title": "Ground truth for grammatical error correction metrics",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Confer-",
            "year": 2015
        },
        {
            "authors": [
                "Courtney Napoles",
                "Keisuke Sakaguchi",
                "Joel Tetreault."
            ],
            "title": "Jfleg: A fluency corpus and benchmark for grammatical error correction",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics:",
            "year": 2017
        },
        {
            "authors": [
                "Kostiantyn Omelianchuk",
                "Vitaliy Atrasevych",
                "Artem Chernodub",
                "Oleksandr Skurzhanskyi."
            ],
            "title": "Gector\u2013grammatical error correction: Tag, not rewrite",
            "venue": "Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational",
            "year": 2020
        },
        {
            "authors": [
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits",
            "year": 2020
        },
        {
            "authors": [
                "Sudha Rao",
                "Joel Tetreault"
            ],
            "title": "Dear sir or madam",
            "year": 2018
        },
        {
            "authors": [
                "Raja"
            ],
            "title": "Multitask prompted training",
            "year": 2021
        },
        {
            "authors": [
                "tian Riedel"
            ],
            "title": "PEER: A collaborative language",
            "year": 2023
        },
        {
            "authors": [
                "Hovy"
            ],
            "title": "Identifying semantic edit intentions",
            "year": 2017
        },
        {
            "authors": [
                "Xuchao Zhang",
                "Dheeraj Rajagopal",
                "Michael Gamon",
                "Sujay Kumar Jauhar",
                "ChangTien Lu."
            ],
            "title": "Modeling the relationship between user comments and edits in document revision",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Simiao Zuo",
                "Qingru Zhang",
                "Chen Liang",
                "Pengcheng He",
                "Tuo Zhao",
                "Weizhu Chen."
            ],
            "title": "Moebert: from bert to mixture-of-experts via importance-guided adaptation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recently, the Natural Language Processing (NLP) community has witnessed the rapid development of Large Language Models (LLMs). With the help of instruction tuning, LLMs can effectively utilize the knowledge acquired during pre-training and have the ability to follow user instructions (Wei et al., 2021; Sanh et al., 2021; Mishra et al., 2022; Wang et al., 2022; Ouyang et al., 2022; OpenAI, 2023).\n\u2217 Equal Contribution \u2020 Corresponding Author\nTherefore, due to the powerful ability of large models as assistants, users can conveniently accomplish a wide range of open-domain tasks by collaborating with LLMs. One particularly important applied scenario of the LLM assistants is editing.\nEditing is a crucial process in writing, requiring a diverse range of skills to refine texts from multiple perspectives. Consequently, there is a long-term goal of using machines to assist in automating the editing process. However, the inherent difficulty of editing, coupled with the limited availability of annotated editing data, hinders the development of general editing models capable of meeting a wide range of editing requirements. Therefore, before the era of LLMs, previous works mainly focused on developing specialized editing models tailored to specific tasks, such as grammatical error correction (GEC) (Bryant et al., 2019), style transfer (Rao and Tetreault, 2018), and sentence fusion (Geva et al., 2019). Due to their specific capabilities, these specialized editing models have limited practical applications. Besides, although recent work has explored the potential of LLMs as general editing models (Faltings et al., 2021; Schick et al., 2023; Madaan et al., 2023), the iterative nature of editing often requires multiple interactions with models to edit a single piece of text. Therefore, relying solely on LLMs to meet diverse editing needs becomes impractical and cost-prohibitive.\nIn this work, to develop a lightweight general edit model that caters to various editing requirements, we propose General SParse Efficient Editing MoDel (G-SPEED). Specifically, we introduce a novel unsupervised editing data clustering method to eliminate noise and annotate the intent, considering the limited quality of current edit intent annotations and the presence of noisy data in the revision history of Wikipedia passages. Besides, we propose a novel sparse editing model structure to enhance the learning capabilities of small models. Experimental results demonstrate that G-SPEED\nachieves state-of-the-art performance on the EditEval benchmark and exhibits a strong generalization ability to unseen tasks with limited data resources.\nIn conclusion, our contributions are as follows:\n\u2022 In this work, we first propose G-SPEED, a lightweight framework with a novel sparse editing model structure, which can satisfy various editing needs;\n\u2022 To deal with the limited editing data source, we propose a novel unsupervised data clustering method, which will be released to promote the development of editing models;\n\u2022 Experimental results show that G-SPPED can achieve state-of-the-art results on EditEval, surpassing the best LLMs by 2.8 points on average, and have a strong generalization ability to unseen tasks in data-limited scenarios."
        },
        {
            "heading": "2 Related work",
            "text": "General Text Editing Existing work on general text editing can broadly be divided into two categories: (1) instruction-based text editing (Faltings et al., 2021; Schick et al., 2023; Madaan et al., 2023);(2) multi-intent editing (Du et al., 2022b; Kim et al., 2022). Instruction-based text editing polishes up existing texts following user instructions, which is based on LLMs (Chung et al., 2022; Brown et al., 2020; Ouyang et al., 2022) and needs a long time to generate and large-scale datasets for training. Multi-intent editing uses intents to perform diverse editing actions. Specifically, Kim et al. (2022) improves text editing with intent span detection. However, edit intents are defined by humans, which have high annotation costs and may not cover all editing intents (Yang et al., 2017; Anthonio et al., 2020; Du et al., 2022b). In this work, we propose G-SPEED, which can handle general text editing without the dependence on LLMs and human-annotate editing intents.\nEditing Model Editing models are efficient alternatives to Seq2Seq models (Sutskever et al., 2014) when the source and target texts in the task have a large amount of overlap, e.g., grammatical error correction (GEC) (Bryant et al., 2019), style transfer (Rao and Tetreault, 2018), and sentence fusion (Geva et al., 2019). Editing models learn to predict edit operations and directly leave the correct text unchanged, while Seq2Seq models generate target text from scratch. LaserTagger (Malmi et al.,\n2019) is a general approach that predicts edit operations by sequence labeling and then generates inserted words with a fixed vocabulary. Omelianchuk et al. (2020) proposes more complete tags (such as pluralization and capitalization) to make generation easier and improve the GEC task. Mallinson et al. (2020) and Mallinson et al. (2022) perform arbitrary content insertion via non-autoregressive and semi-autoregressive methods, respectively. However, the current editing model is stuck on solving a single task. Editing models do not perform well in multi-task problems (Du et al., 2022b).\nSparse Language Model Sparse language models achieve promising results with larger model sizes while maintaining almost the same computational costs (Jaszczur et al., 2021; Du et al., 2022a; Fedus et al., 2022b,a). There are various routing algorithms to determine where to send examples, such as hash routing (Roller et al., 2021), base layer (Lewis et al., 2021), and Multi-gate MoE (Ma et al., 2018). Zuo et al. (2022), Gupta et al. (2022) and Lee-Thorp and Ainslie (2022) design sparse feed-forward layers on BERT (Devlin et al., 2019) and make a boost on GLUE (Wang et al., 2018). In this paper, we explore the use of sparse layers on Editing models for General Text Editing tasks with a BERT backbone."
        },
        {
            "heading": "3 Task Formulation",
            "text": "During text editing, the documents D undergo modifications based on user-selected intents I like fluency, clarity, simplification, and neutralization. Each modification corresponds to a pair of documents (Dt\u22121, Dt) and an editing intent It where t represents the number of modifications. Text editing models are tailored to modify documents based on intent:\nDt = f(Dt\u22121, It), (1)\nwhere f denotes text editing models."
        },
        {
            "heading": "4 Unsupervised Editing Data Clustering",
            "text": "To facilitate text editing tasks, a dataset comprising editing intent and text pairs, both before and after editing, is required for pre-training purposes. ITERATER (Du et al., 2022b) collects text revisions from Wikipedia, ArXiv, and Wikinews. It categorizes text intents into five categories: fluency, clarity, coherence, style, and meaning-changed. The intent is then predicted using a RoBERTa-based model (Liu et al., 2019) that takes both the original\nand revised texts as input. However, automatic annotation performs poorly on small categories due to the unbalanced distribution of categories, and certain categories, such as simplification, are not taken into account. Formulating perfect intent categories and annotating large-scale data for pretraining pose significant challenges.\nSimilar to existing work (Zhang et al., 2019; Faltings et al., 2021), we first collect data from the revision histories in March 1st, 2023 dump of English Wikipedia.1 Each revision consists of an original text, a revised text, and a user comment that is associated with the editing intent. We extract revisions from the XML format dump and exclude revisions that do not have user comments. To ensure data clustering quality, we additionally perform data cleaning based on various factors, such as the BLUE value of the source and target, as well as the length of comments and text. We extract the source and target sentences using Punkt Sentence Tokenizer2 and difflib3, and then we eliminate Wikipedia markup using wikiextractor4.\nTo classify user comments into distinct intent categories, we utilize k-means clustering initialized with k-means++ algorithm (Arthur and Vassilvitskii, 2007).5 With a collection of n comment\n1https://dumps.wikimedia.org/enwiki/ 2https://www.nltk.org/api/nltk.tokenize.punkt.\nhtml#module-nltk.tokenize.punkt 3https://docs.python.org/3/library/difflib. html#module-difflib 4https://github.com/attardi/wikiextractor 5We also employ the DBSCAN algorithm (Ester et al., 1996), which segments categories by detecting changes in data point density and eliminates the need to specify the number of clusters. However, due to the high and uneven density of data points, DBSCAN struggles to segment meaningful clusters effectively.\nembeddings represented as x1, x2, ...xn, k-means clustering aims to partition these embeddings into k sets {S1, S2, ...Sk} so as to minimize the intracluster distance:\nmin S k\u2211 i=1 \u2211 x\u2208Si dist(x, \u00b5i), (2)\nwhere dist represents the Euclidean distance, and \u00b5i denotes the mean of embeddings within Si.\nIn our study, we utilize SentenceBERT (Reimers and Gurevych, 2019) to generate comment embeddings, which are then decomposed using Singular Value Decomposition (SVD). Sentence-BERT provides a more effective representation of text semantics compared to algorithms based on word frequency.\nTo discern the purpose of each cluster, we also organize the designed prompts into clusters (such as fixing grammar errors and improving text cohesion). Based on the prompts they contained, we then selected four clusters, namely fluency, readability, simplification, and neutralization, as illustrated in step I of Figure 1. Table 1 presents the data instances. Clusters that focus on information updates, citation modifications, punctuation modifications, and other similar tasks are disregarded. For more details, please refer to Appendix A."
        },
        {
            "heading": "5 G-SPEED",
            "text": ""
        },
        {
            "heading": "5.1 Editing Model",
            "text": "We decompose editing into two steps: (1) tagging, label each word with an editing operation by sequence labeling; (2) generation, insert new words into the original sentence. As shown in Figure 1, we share the encoder parameters of tagging and\ngeneration so that the two modules are trained together:\nL = Ltagging + \u03bbLgeneration (3)\nwhere \u03bb is a hyper-parameter.\nTagging Editing operations are marked at the word level. A linear classification layer is used to predict tags following a transformer encoder. Training is performed using a cross-entropy loss:\nLtagging = \u2212 |xt|\u2211 i log p(yti |fcls(hti)), (4)\nwhere yt is golden tags, i is the index of each word, ht is the output of encoder, xt is the input text, \u2223\u2223xt\u2223\u2223 represents the total number of words in the text, fcls is a classification layer to predict tags.\nThe most basic type of editing operation is represented by the Levenshtein transition types, which encompass KEEP, DELETE, REPLACE, and APPEND. Omelianchuk et al. (2020) expands the sum of types to 4971, 29 of which represent fundamental text transformations, and the rest are used to insert new words. To maximize the number of tags for reducing generation difficulties while ensuring sufficient training data for each tag, we develop\na total of 14 tags which encompass case conversion, singular and plural conversion, verb tense conversion, as well as split operations like hyphen splitting or merging. For automatic operation annotation, we utilize dynamic programming and set the editing distance as the cost function. Detailed information about the categories and a comparison of various designs are provided in Appendix B.\nGeneration The tagging phase primarily handles transformations that do not involve the generation of new words. During the generation phase, our task is to predict new words at the positions of REPLACE, APPEND, and TRANSFORM-VERB. We employ an efficient non-autoregressive mask language model (MLM) similar to Mallinson et al. (2020). As shown in Figure 1, we insert n [MASK] tokens at each required position and subsequently perform mask prediction on the new text.6 Any [MASK] tokens beyond the length of the desired tokens will be predicted as [PAD]. The MLM is trained with a cross-entropy loss:\nLgeneration = \u2212 |ys|\u2211 i log p(ysi |fpred(hsi )), (5)\nwhere ys is the golden results of [MASK], i is the index of [MASK], |ys| denotes the total number of\n6In our experiments, n = 4.\n[MASK], hs is the hidden states, and fpred is linear layers to predict tokens in vocabulary.\nTo retain the maximum amount of information from the original sentence, we retain the deleted words and the verbs with the wrong tense. As demonstrated in Figure 1, we enclose the deleted words within [DELETE] and [/DELETE] tags, and the wrong verbs within [TRANSFORM_VERB] and [/TRANSFORM_VERB] tags."
        },
        {
            "heading": "5.2 Sparse Bert Layer",
            "text": "Then, we introduce an efficient and compact sparse Bert layer for a multi-intent editing model. Figure 1 illustrates the use of four fully connected feed-forward networks (FFN), referred to as \"experts\". The experts correspond to four editing intents present in our training data. The experts are activated when training data that corresponds to their respective intents. The data point shown in the figure is clustered as \"C2\" (readability) and, therefore, utilizes the red module. Each layer comprises 8 experts, with the tagging and generation modules being divided due to their distinct effects. By employing this approach, we can efficiently address multi-intent editing tasks with a single encoder:\nht = f\u03b8(x t, rt, zt), r \u2208 [0, n) , z \u2208 [0, 1] , (6)\nwhere xt represents the input for either tagging or generation, rt represents the index of the intent, n is the total number of intents, zt = 0 indicates the tagging mode and zt = 1 indicates the generation mode, f\u03b8 represents the sparse Bert model, and ht is the hidden states in Equation 4 and 5. For the balance between each task and the two modules, we sequentially train the tagging module and the generation module of each task in units of a batch, so that the number of training steps for each task and each module is the same."
        },
        {
            "heading": "5.3 Additional Fine-tune",
            "text": "In practice, text editing tasks may not align with our expert configuration. We employ additional finetuning for specific tasks. To enhance the model\u2019s generalization, we copy the experts in the pretrained model and train targeted tasks accordingly. Specifically, we freeze all parameters except the experts, enabling the model to adapt flexibly to any editing task with minimal training cost."
        },
        {
            "heading": "6 Experiments",
            "text": ""
        },
        {
            "heading": "6.1 Datasets",
            "text": "We conduct our evaluation using EditEval (Dwivedi-Yu et al., 2022), which is an instruction-based text improvement benchmark. The study includes six datasets and six tasks; their details are provided below.\nFluency The primary objective of the fluency task is to rectify grammatical and spelling errors. The evaluation of this task utilizes ITERATER (Du et al., 2022b) and JFLEG (Napoles et al., 2017). ITERATER is an edit-intention annotated corpus of iteratively revised text. We utilize the subset that focuses on fluency. In comparison to ITERATER, JFLEG not only corrects grammatical errors but also makes the original text more native sounding.\nClarity and Coherence Both tasks use the corresponding subsets in the test dataset of ITERATER. The clarity task aims to enhance the formality, conciseness, readability, and comprehensibility of the text. The coherence task aims to enhance the cohesiveness, logical connectivity, and overall coherence of the text.\nParaphrasing The purpose of the Paraphrasing task is to rephrase sentences without a specific improvement goal. EditEval opts to utilize text pairs from SemEval-2018 (Cer et al., 2017) that have high similarity scores.\nSimplification TurkCorpus (Xu et al., 2016) and ASSET (Alva-Manchego et al., 2020) are uesed for evaluation of this task. ASSET and TURK contain identical test examples, but their references are simplified in different ways.\nNeutralization The process of neutralization involves eliminating subjective bias in a text. For example, in the text of \"Jimi hendrix (musician), a great musician and vocalist\", the word \"great\" lacks neutrality. The WNC dataset (Pryzant et al., 2020) serves as a representative dataset for this task."
        },
        {
            "heading": "6.2 Metrics",
            "text": "SARI (Xu et al., 2016) measures the goodness of words that are added, deleted, and kept. It calculates the average n-gram F1 score for addition, deletion, and retention operations. GLUE (Napoles et al., 2015) is an additional n-gram-based metric better suited for evaluating single sentences than BLEU. It is applied to JFLEG and ITERATER.\nExact Match (EM) calculates the percentage of predictions that exactly match the reference. This metric is the official measure used by WNC."
        },
        {
            "heading": "6.3 Baselines",
            "text": "Initially, we assess our models using a non-pretrained baseline, which employs identical finetuning data and edit model structure as G-SPEED. Then, we assess various existing LLMs using the EditEval setup, employing prompts to enable diverse editing operations. Our primary comparison is PEER (Schick et al., 2023), a collaborative language model trained on Wikipedia edit history that is initialized from the LM Adapted variant of T5 (Schick et al., 2023). Scores for 3b and 11b parameters PEER are reported on EditEval. T0, T0++ (Sanh et al.) and Tk-Instruct (Wang et al., 2022) are other models that are initialized from the LM Adapted variant of T5 and then fine-tuned using prompts or instructions. We compare against GPT-3 (Brown et al., 2020), OPT (Zhang et al., 2022), InstructGPT (Ouyang et al., 2022), and ChatGPT7 as decoder-only LLMs."
        },
        {
            "heading": "6.4 Implementation Details",
            "text": "We present the information of the datasets in Table 2. For additional fine-tuning, we select data from various sources: W&I+LOCNESS (Bryant et al., 2019), ParaSCI (Dong et al., 2021), Turk (Xu et al., 2016), Wikiauto (Jiang et al., 2020), WNC (Pryzant et al., 2020), and the subsets of ITERATERV2 (Kim et al., 2022) with intent confidence scores higher than 0.8. Each expert has 10k data points for fine-tuning.\nFor data clustering, we employ K-Means++ with cuML (Raschka et al., 2020) toolkit, setting the number of clusters to 10. We choose all-mpnet-\n7https://chat.openai.com/\nbase-v28 to generate comment embeddings. We use scikit-learn (Pedregosa et al., 2011) to employ SVD and set the output dimension to 100.\nFor pre-training, we choose bert-base-cased (Devlin et al., 2019)9 as our backbone model with the Huggingface Transformers (Wolf et al., 2020) toolkit. We initialize all experts based on the corresponding counterparts in Bert. We use Adam Optimizer (Kingma and Ba, 2014) and clip the norms of gradients to 1. \u03bb is set to 1.\nDuring additional fine-tuning, the expert responsible for the readability task is duplicated into three instances. Each instance is utilized to train the tasks of clarity, coherence, and paraphrasing, as shown in Table 2. All experiments are carried out on 4 Nvidia GeForce RTX 3090 GPUs."
        },
        {
            "heading": "6.5 Main Results",
            "text": "The main results are presented in Table 3. The metrics are compared among the following: Copy the text of source (Copy), supervised state-of-the-art models (SotA), LLMs, and our model (G-SPEED). The average SARI score is calculated as the mean of each task, while the score for a specific task is computed as the mean of the corresponding test datasets. Out of the models with fewer than 11 billion parameters, only PEER achieves a score more than 5 points higher than the average Copy score. Despite their large size, models such as OPT and GPT-3, which have 175B parameters, are unable to ensure satisfactory performance in general text editing tasks. Among the LLMs, InstructGPT performs the best, slightly greater than ChatGPT. Additionally, PEER demonstrates favorable results with a relatively small number of parameters.\nG-SPEED surpasses the performance of LLMs with only 508M parameters. Compared with the best LLMs, InstructGPT and ChatGPT, G-SPEED achieves satisfactory results on coherence and neutralization tasks. G-SPEED exhibits a noticeable gap compared to ChatGPT in fluency, paraphrasing, and simplification tasks. However, these areas can be significantly improved through additional fine-tuning. We compare our model with two variants: one without the pre-training step (denoted as \"w/o PRE.\") and another without the finetuning step (denoted as \"w/o ADDI.\") as shown in Table 3. We find that both of these steps are crucial for our model. The model without either\n8https://huggingface.co/sentence-transformers/ all-mpnet-base-v2\n9https://huggingface.co/bert-base-cased\nstep performs poorly. In particular, the pre-training step greatly improves fluency, clarity, and neutralization tasks, while the fine-tuning step enhances coherence, paraphrasing, and simplification tasks. This can be attributed to the varying proportions of these tasks in the history of Wikipedia editing. Furthermore, our model, even without additional finetuning, performs slightly better than InstructGPT and ChatGPT, thereby confirming the effectiveness of our training and clustering method."
        },
        {
            "heading": "6.6 Further Analysis",
            "text": "Expert Structure In Table 4, we present the results for two types of expert structures: (1) sparse last encoder layer (referred to as \"Last Layer\"), which often impacts the final output, and (2) sparse feed-forward layer (referred to as \"Feed Forward\") in each encoder layer. Additionally, we include the results of a dense model (referred to as \"Dense\"), where the entire encoder is shared, and differentiation between different tasks occurs solely at the linear classification layer. Our findings indicate that sparse models generally outperform the dense model, with inference times being almost identical. Furthermore, models with sparse feedforward layers show slightly better performance than those with sparse last encoder layers. Furthermore, we examine a model that shares the experts in tagging and generation (referred to as \"Sharing T&G\"), which can further reduce the model\u2019s size. However, this approach yields inferior performance compared to the other models.\nRouter Structure We compare one static routing algorithm and two dynamic routing algorithms. (1) Select the expert corresponding to the task (referred to as \"Task ID\"). (2) Select the expert using a linear classification layer (referred to as \"Linear\"), which predicts the probabilities by softmax for each expert and multiplies the highest probability with the output of the corresponding expert. (3) Select the expert using a linear classification layer specific to each task (referred to as \"Task ID + Linear\"), which differs only in the number of classifiers with \"Linear\".10 As shown in Table 4, the routing algorithms combined with task information perform better. We believe this is because the editing intention cannot be directly inferred from the input text. The external knowledge about editing intentions benefits model training. Furthermore, we employ a \"Token Level\" routing algorithm in our model, which selects the experts for each token based on the hidden states of the token. However, this approach yields inferior performance. Selecting an expert based on the semantics of a whole sentence is more reasonable and efficient.\nFew Shot Learning Table 5 presents the results of few-shot learning using the G-SPEED backbone for general text editing tasks. We maintain the same settings as in the additional finetuning, setting the data volume for each task at 500, 1k, 5k, and 10k, respectively. We compare the\n10The router weights are initialized using a normal distribution with a mean of 0 and a standard deviation of 0.001. The temperature of the softmax in the router is 0.7.\nmodel that skips the pre-training step (referred to as \"w/o PRE\") with our pre-trained model. We find that the pre-trained model outperforms the nonpre-trained model. Furthermore, the fine-tuning results with 500 samples are nearly equivalent to those of the un-pre-trained model trained on 10k samples, demonstrating its generalization ability on downstream tasks. Additionally, we finetune our pre-trained model on the sentence fusion task and compare it with editing models. Following the work of Mallinson et al. (2022), we use the \"balanced Wikipedia\" subset of the DiscoFuse dataset (Geva et al., 2019) and compare the results with those of editing models using 4,500 (0.1%) and 450 (0.01%) training data points. We report the Exact Match score of G-SPEED, while the other results are reported by Mallinson et al. (2022, 2020). In Table 6, our initial observation is that the Seq2Seq model (BERT2BERT (Rothe et al., 2020), T5 base (Raffel et al., 2020)) performs poorly, indicating that editing models are effective on train-\ning data. Furthermore, fine-tuning our pre-trained model yields significantly better performance than other editing models, demonstrating the generalization ability of our method on downstream tasks."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we propose the General SParse Efficient Editing MoDel (G-SPEED), a model designed to fulfill diverse editing needs while ensuring computational efficiency. Specifically, we first propose a novel unsupervised clustering strategy to obtain a large amount of multi-intent editing data, which is collected from editing histories from Wikipedia and can be used to conduct pre-training. Subsequently, we propose a novel sparse editing model architecture to improve the learning abilities of small models. The experimental results on EditEval show that, with the use of Bert-base as the backbone model, G-SPEED can outperform LLMs while maintaining an efficient inference speed. Ad-\nditionally, we discuss different sparse structures and show the strong generalization capability of our method across downstream tasks.\nLimitations\nA limitation of unsupervised clustering is that it cannot deal with revisions that contain more than one editing intent. Although text editing in the history of Wikipedia is iterative, the degree of each modification by the user is still uncontrollable.\nEthics Statement\nWe collect all data from publicly available sources and test our model on public datasets. Since our work mainly focuses on non-meaning-changing text edits, we are able to avoid many issues involving generating harmful text. Therefore, our work has no possibility of generating harmful information in practical applications."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by the National Science Foundation of China (No. 62176174, No. 62206194) and the Natural Science Foundation of Jiangsu Province (No. BK20220488)."
        },
        {
            "heading": "A Details and Studies on Datasets",
            "text": "In addition to the filtering rules discussed in Section 4, we employ additional filtering criteria for revisions. Firstly, we exclude revisions whose comments contain any of the following terms: \"template\", \"image\", \"infobox\", \"pic\", \"link\", \"photo\", \"comment\", \"http:\", \"https:\", \".jpg\", \".png\", or \"reply\". The presence of these terms suggests the involvement of unwanted revision types, such as image alterations and citation modifications, which can complicate the clustering process (e.g., due to lengthy URL links in comments). Furthermore, we remove any links present in the source and target texts. Additionally, we replace certain important shortcuts with their full meanings to enhance data clustering. For instance, we substitute \"[[WP:NPOV|POV]]\" with \"neutral point of view\", \"[[WP:TYPO]]\" with \"typo\", \"[[WP:RS]]\" with \"reliable sources\", and \"[[WP:SYN]]\" with \"synthesis\". This modification contributes to improved clustering outcomes. Furthermore, we exclude revisions that solely pertain to number or time modifications, such as updates on the real-time number of COVID-19 cases in a specific location. Moreover, we filter out revisions in which both the comments and the source texts exhibit significant similarities. This type of overlap revision typically corresponds to low-quality comments. Since our extraction of source and target relies solely on document comparison, there are instances where we may encounter incorrect or incomplete source and target pairs. Consequently, it becomes necessary to conduct a basic screening of the clustered data. One approach involves utilizing BLUE to identify and filter out sentence pairs that do not match, thereby\nminimizing the presence of inaccuracies. Additionally, another filtering criterion involves considering differences in sentence length to identify and exclude incorrect simplification data.\nTable 2 presents the central words and a selection of sampled comments within each cluster. A concise description is provided for each cluster. The clusters employed for training purposes are clusters 1, 2, 3, and 5. The central words correspond to the most frequently occurring words in each cluster, excluding stop words, and they exhibit a high degree of concentration within their respective clusters. Detailed information regarding the dataset instances is presented in Table 1. Figure 2 illustrates the distribution of semantic clustering result maps in a two-dimensional space. To enhance the visualization of clustering effects, we employed a random subsample from the dataset and applied the Uniform Manifold Approximation and Projection (UMAP) technique to condense the semantic data into a two-dimensional vector representation.\nWe have undertaken a meticulous examination to determine the presence of any data contamination between our training and test datasets. Specifically, in the evaluation benchmark, JFLEG, Turk, ASSET, and ITERATER are manually annotated; STSB is collected from forums and news. Only the WNC dataset is collected from the revision history of Wikipedia passages. However, WNC is collected from the revisions made between 2004 and 2019, while our dataset is collected from the revision histories in the March 1st, 2023 dump of English Wikipedia. For further confirmation, we first concatenate the source and target of each data sample and use traversal search to filter the training samples of our pre-training datasets that have a BLUE value of 0.9 with any data sample of WNC. However, no such data sample satisfies this requirement, which further confirms there is no contamination."
        },
        {
            "heading": "B Details and Experiments on Tags",
            "text": "Similar to the approach taken in GECTOR (Omelianchuk et al., 2020), our work considers fine-grained tag design. We present three different tag designs in Table 8, where the tag names and meanings correspond to those used in GECTOR. The decision to reduce the number of tags from 34 to 14 is based on the frequency of tag occurrence in the pre-training dataset and its compatibility with the pre-training process. Table 9 displays the performance results of the three\ntag designs on two sparse models. Notably, the design comprising 14 tags achieves the highest performance among all designs."
        },
        {
            "heading": "C Efficiency Comparison",
            "text": "As demonstrated in Table 10, G-SPEED exhibits superior multi-tasking capabilities when contrasted with previous editing models due to its sparse architecture. Furthermore, as shown in Table 11, compared to fine-tuned Large Language Models (LLMs), G-SPEED boasts conspicuous advantages in model size, computational speed, and performance. Specifically, we use the WNC test set for inference on a single NVIDIA GeForce RTX 3090. We conducted fine-tuning on both T0 and LLaMa using the identical dataset. We also adjusted the data input format to \"instruction: input,\" as was performed during instruction fine-tuning. Notably,\nG-SPEED achieves a remarkable speed boost up to 35 times faster than LLaMa-7B. Moreover, the training costs of G-SPEED are substantially more economical than those of LLMs.\nD Iteration Study\nThere is currently no large test set for multi-intent iterative editing tasks. In our study, we employed the iterator_human_doc dataset11, which comprises a total of 51 manually annotated instances, to assess and contrast the iterative editing proficiency of both ChatGPT and G-SPEED. Table 12 shows that the two models are similar in the SARI score, but GSPEED retains the original text better.\nWe also explored multiple iterations within a single task. As indicated in Table 13, we conducted a comparison of our pre-trained model at various editing depths. Our findings reveal that the fluency task consistently benefits from iterative editing, whereas most tasks are more inclined toward single-pass modifications. Table 14 presents specific instances from the JFLEG dataset, illustrating the incremental revision process employed by the editing model to address grammatical errors.\n11https://huggingface.co/datasets/wanyu/ IteraTeR_human_doc/viewer/default/test"
        }
    ],
    "title": "G-SPEED: General SParse Efficient Editing MoDel",
    "year": 2023
}