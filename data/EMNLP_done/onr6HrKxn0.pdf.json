{
    "abstractText": "Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate the relationship between model memorization and privacy neurons, from multiple perspectives, including model size, training time, prompts, privacy neuron distribution, illustrating the robustness of our approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xinwei Wu"
        },
        {
            "affiliations": [],
            "name": "Junzhuo Li"
        },
        {
            "affiliations": [],
            "name": "Minghui Xu"
        },
        {
            "affiliations": [],
            "name": "Weilong Dong"
        },
        {
            "affiliations": [],
            "name": "Shuangzhi Wu"
        },
        {
            "affiliations": [],
            "name": "Chao Bian"
        },
        {
            "affiliations": [],
            "name": "Deyi Xiong"
        }
    ],
    "id": "SP:f599f7e78df5b89b713a3e8e43f32a025d3d660d",
    "references": [
        {
            "authors": [
                "Prajjwal Bhargava",
                "Aleksandr Drozd",
                "Anna Rogers."
            ],
            "title": "Generalization in nli: Ways (not) to go beyond simple heuristics",
            "venue": "Proceedings of the Second Workshop on Insights from Negative Results in NLP, pages 125\u2013135.",
            "year": 2021
        },
        {
            "authors": [
                "Lucas Bourtoule",
                "Varun Chandrasekaran",
                "Christopher A Choquette-Choo",
                "Hengrui Jia",
                "Adelin Travers",
                "Baiwu Zhang",
                "David Lie",
                "Nicolas Papernot."
            ],
            "title": "Machine unlearning",
            "venue": "2021 IEEE Symposium on Security and Privacy (SP), pages 141\u2013159. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Hannah Brown",
                "Katherine Lee",
                "Fatemehsadat Mireshghallah",
                "Reza Shokri",
                "Florian Tram\u00e8r"
            ],
            "title": "What does it mean for a language model to preserve privacy",
            "venue": "In Proceedings of the 2022 ACM Conference on Fairness,",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Chang Liu",
                "\u00dalfar Erlingsson",
                "Jernej Kos",
                "Dawn Song"
            ],
            "title": "The secret sharer: Evaluating and testing unintended memorization",
            "year": 2019
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Florian Tramer",
                "Eric Wallace",
                "Matthew Jagielski",
                "Ariel Herbert-Voss",
                "Katherine Lee",
                "Adam Roberts",
                "Tom Brown",
                "Dawn Song",
                "Ulfar Erlingsson"
            ],
            "title": "Extracting training data from large language models",
            "year": 2021
        },
        {
            "authors": [
                "Maximin Coavoux",
                "Shashi Narayan",
                "Shay B Cohen."
            ],
            "title": "Privacy-preserving neural representations of text",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1\u201310.",
            "year": 2018
        },
        {
            "authors": [
                "Damai Dai",
                "Li Dong",
                "Yaru Hao",
                "Zhifang Sui",
                "Baobao Chang",
                "Furu Wei."
            ],
            "title": "Knowledge neurons in pretrained transformers",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8493\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Nicola De Cao",
                "Wilker Aziz",
                "Ivan Titov."
            ],
            "title": "Editing factual knowledge in language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491\u2013 6506.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Damai Dai",
                "Yifan Song",
                "Jingjing Xu",
                "Zhifang Sui",
                "Lei Li."
            ],
            "title": "Calibrating factual knowledge in pretrained language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5937\u20135947.",
            "year": 2022
        },
        {
            "authors": [
                "Khaled El Emam",
                "Fida Kamal Dankar",
                "Romeo Issa",
                "Elizabeth Jonker",
                "Daniel Amyot",
                "Elise Cogo",
                "JeanPierre Corriveau",
                "Mark Walker",
                "Sadrul Chowdhury",
                "Regis Vaillancourt"
            ],
            "title": "A globally optimal kanonymity method for the de-identification of health",
            "year": 2009
        },
        {
            "authors": [
                "Aitor Garc\u00eda-Pablos",
                "Naiara P\u00e9rez",
                "Montse Cuadros."
            ],
            "title": "Sensitive data detection and classification in spanish clinical text: Experiments with bert",
            "venue": "Proceedings of the 12th Language Resources and Evaluation Conference, pages 4486\u20134494.",
            "year": 2020
        },
        {
            "authors": [
                "Mor Geva",
                "Roei Schuster",
                "Jonathan Berant",
                "Omer Levy."
            ],
            "title": "Transformer feed-forward layers are key-value memories",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484\u20135495.",
            "year": 2021
        },
        {
            "authors": [
                "Shangwei Guo",
                "Chunlong Xie",
                "Jiwei Li",
                "Lingjuan Lyu",
                "Tianwei Zhang."
            ],
            "title": "Threats to pre-trained language models: Survey and taxonomy",
            "venue": "arXiv preprint arXiv:2202.06862.",
            "year": 2022
        },
        {
            "authors": [
                "Varun Gupta",
                "Christopher Jung",
                "Seth Neel",
                "Aaron Roth",
                "Saeed Sharifi-Malvajerdi",
                "Chris Waites."
            ],
            "title": "Adaptive machine unlearning",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 16319\u201316330. Curran Associates, Inc.",
            "year": 2021
        },
        {
            "authors": [
                "Peter Henderson",
                "Koustuv Sinha",
                "Nicolas AngelardGontier",
                "Nan Rosemary Ke",
                "Genevieve Fried",
                "Ryan Lowe",
                "Joelle Pineau."
            ],
            "title": "Ethical challenges in data-driven dialogue systems",
            "venue": "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and",
            "year": 2018
        },
        {
            "authors": [
                "Shlomo Hoory",
                "Amir Feder",
                "Avichai Tendler",
                "Sofia Erell",
                "Alon Peled-Cohen",
                "Itay Laish",
                "Hootan Nakhost",
                "Uri Stemmer",
                "Ayelet Benjamini",
                "Avinatan Hassidim"
            ],
            "title": "Learning and evaluating a differentially private pre-trained language model",
            "year": 2021
        },
        {
            "authors": [
                "Bryan Klimt",
                "Yiming Yang."
            ],
            "title": "Introducing the enron corpus",
            "venue": "CEAS, volume 45, pages 92\u201396.",
            "year": 2004
        },
        {
            "authors": [
                "Peter Lee",
                "Sebastien Bubeck",
                "Joseph Petro."
            ],
            "title": "Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine",
            "venue": "New England Journal of Medicine, 388(13):1233\u20131239.",
            "year": 2023
        },
        {
            "authors": [
                "Haoran Li",
                "Dadi Guo",
                "Wei Fan",
                "Mingshi Xu",
                "Yangqiu Song."
            ],
            "title": "Multi-step jailbreaking privacy attacks on chatgpt",
            "venue": "arXiv preprint arXiv:2304.05197.",
            "year": 2023
        },
        {
            "authors": [
                "Xuechen Li",
                "Florian Tramer",
                "Percy Liang",
                "Tatsunori Hashimoto."
            ],
            "title": "Large language models can be strong differentially private learners",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "arXiv preprint arXiv:2107.13586.",
            "year": 2021
        },
        {
            "authors": [
                "Zengjian Liu",
                "Buzhou Tang",
                "Xiaolong Wang",
                "Qingcai Chen."
            ],
            "title": "De-identification of clinical notes via recurrent neural network and conditional random field",
            "venue": "Journal of biomedical informatics, 75:S34\u2013 S42.",
            "year": 2017
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex Andonian",
                "Yonatan Belinkov."
            ],
            "title": "Locating and editing factual associations in gpt",
            "venue": "Advances in Neural Information Processing Systems, 35:17359\u201317372.",
            "year": 2022
        },
        {
            "authors": [
                "Eric Mitchell",
                "Charles Lin",
                "Antoine Bosselut",
                "Chelsea Finn",
                "Christopher D Manning."
            ],
            "title": "Fast model editing at scale",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Eric Mitchell",
                "Charles Lin",
                "Antoine Bosselut",
                "Christopher D Manning",
                "Chelsea Finn."
            ],
            "title": "Memorybased model editing at scale",
            "venue": "International Conference on Machine Learning, pages 15817\u201315831. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Seth Neel",
                "Aaron Roth",
                "Saeed Sharifi-Malvajerdi."
            ],
            "title": "Descent-to-delete: Gradient-based methods for machine unlearning",
            "venue": "International Conference on Algorithmic Learning Theory.",
            "year": 2020
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Richard Plant",
                "Dimitra Gkatzia",
                "Valerio Giuffrida."
            ],
            "title": "Cape: Context-aware private embeddings for private language learning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7970\u20137978.",
            "year": 2021
        },
        {
            "authors": [
                "Samuel Sousa",
                "Roman Kern."
            ],
            "title": "How to keep text private? a systematic review of deep learning methods for privacy-preserving natural language processing",
            "venue": "Artificial Intelligence Review, 56(2):1427\u2013 1492.",
            "year": 2023
        },
        {
            "authors": [
                "Om Dipakbhai Thakkar",
                "Swaroop Ramaswamy",
                "Rajiv Mathews",
                "Francoise Beaufays."
            ],
            "title": "Understanding unintended memorization in language models under federated learning",
            "venue": "Proceedings of the Third Workshop on Privacy in Natural Language Pro-",
            "year": 2021
        },
        {
            "authors": [
                "Bin Zhou",
                "Jian Pei",
                "WoShun Luk."
            ],
            "title": "A brief survey on anonymization techniques for privacy preserving publishing of social network data",
            "venue": "ACM Sigkdd Explorations Newsletter, 10(2):12\u201322.",
            "year": 2008
        },
        {
            "authors": [
                "Chen Zhu",
                "Ankit Singh Rawat",
                "Manzil Zaheer",
                "Srinadh Bhojanapalli",
                "Daliang Li",
                "Felix Yu",
                "Sanjiv Kumar."
            ],
            "title": "Modifying memories in transformer models",
            "venue": "arXiv preprint arXiv:2012.00363.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Remarkable progress has been made in large language models (LLMs) in recent years (Brown et al., 2020; Liu et al., 2021; Ouyang et al., 2022; Lee et al., 2023). However,despite this success, LLMs are confronted with privacy and security concerns in real-world applications (Guo et al., 2022; Brown et al., 2022; Li et al., 2023). The primary cause of privacy and security risks is the inherent nature of large pretrained language models. Previous studies (Carlini et al., 2019, 2021; Thakkar et al., 2021;\n\u2217*Corresponding author.\nHenderson et al., 2018) have demonstrated that pretrained language models tend to memorize and regurgitate a significant portion of the training data, including atypical data points that appear only once in the training data. Additionally, external factors (e.g., membership attack) also contribute to these risks. A variety of methods have been explored to attack LLMs for training data extraction. For instance, Carlini et al. (2021) have successfully extracted personal information from GPT-3\u2019s output, while Li et al. (2023) have induced the generation of personal information by utilizing multi-step prompts in ChatGPT. All these show that large pretrained language models suffer from a serious risk of privacy leakage.\nIn order to safeguard privacy, numerous methods have been proposed. The majority focus on either removing sensitive information during the data processing stage (Liu et al., 2017; El Emam et al., 2009; Zhou et al., 2008; Garc\u00eda-Pablos et al., 2020), or reducing the extent to which models memorize training data during the training stage (Li et al., 2021; Hoory et al., 2021; Plant et al., 2021; Coavoux et al., 2018). However, privacy breaches often come to light after the completion of model training, rendering previous methods less effective. There are also methods proposed in the post-processing stage, which involve slight parameter retraining to make the model forget privacy information (Bourtoule et al., 2021; Gupta et al., 2021; Neel et al., 2020). Nevertheless, these methods generally incur high computational complexity, making it challenging to apply them to complex model architectures. In practice, model developers often attempt to prevent language models from outputting specific information via blocking or filtering certain keywords, which, however, does not truly address the underlying issue.\nWe speculate that private information might be\nstored in specific neurons, just like knowledge neurons (Geva et al., 2021; Meng et al., 2022; Dai et al., 2022). This presumption suggests that we could change the model memorization of private information by detecting and deleting these neurons (termed as privacy neurons). Therefore, we propose a framework DEPN for detecting and editing privacy neurons. To detect privacy neurons, we introduce a privacy neuron detector that uses gradient integration to simultaneously compute the contributions of multiple markers to neuron activations. This allows us to estimate an overall privacy attribution score for private information. Subsequently, we further propose a privacy neuron editor that simply sets the activations of the top z privacy neurons with the highest privacy scores to zero to erase the model memorization of the corresponding private information. For the scenario of processing multiple sentences at the same time, we also present a privacy neuron aggregator to facilitate privacy information editing in batches.\nExperimental results show that our framework can quickly reduce the risk of private data leakage without affecting model performance. Compared with other methods, our framework is highly efficient. Furthermore, we have found that model memorization leads to the aggregation of privacy neurons in our experiments, and demonstrated that our framework is very suitable for the scenario of deep model dememorization.\nThe main contributions of our work are summarized as follows:\n\u2022 For the first time, we explore model editing into privacy protection of pretrained language models, provide a new way for privacy protection, and propose DEPN to effectively eliminate model memorization in the postprocessing stage.\n\u2022 We propose the privacy neuron detector to localize privacy neurons based on gradient attribution, and the privacy neuron editor to dememorize privacy information in pretrained language models.\n\u2022 We conduct experiments to demonstrate that the proposed framework is capable of protecting privacy leakage from pretrained language models."
        },
        {
            "heading": "2 Preliminary",
            "text": "Privacy Definition Privacy preservation has become an issue of great concern in the era of pretrained language models. Protecting privacy first requires specifying the boundaries of privacy. The definition of privacy is broad. It is closely related to its context and discourse (Brown et al., 2022). In any texts about, a specific person can be considered as private. For the convenience of research, a narrow definition of privacy is usually taken (Sousa and Kern, 2023), which treats personal identity information as privacy, such as names, ID numbers, phone numbers and other related expressions. The proposed DEPN can be adapted to the above two definitions.\nModel Editing Geva et al. (2021) find that the feed-forward network module in Transformer (i.e., a two-layer perceptron) can be considered as a keyvalue memory, where each key corresponds to a text pattern and each value represents a distribution over the vocabulary. Based on this finding, a strand of research, (Geva et al., 2021; Meng et al., 2022; Dai et al., 2022) propose to edit factual knowledge encoded in pre-trained LLMs by locating neurons related to the entities of factual knowledge. The basic idea of localization is to change the parameters of neurons, and then observe the changes in the probability of the object entity predicted by the model. The neurons with greater influence on the probability are more closely related to the object entity.\nHowever, these methods have a limitation that they can only observe the probability change of one token at a time. Semantic units are usually composed of a sequence of tokens, rather than a single token. This makes it impossible to use these methods directly."
        },
        {
            "heading": "3 Methodology",
            "text": "The proposed DEPN consists of three components: the privacy neuron detector (\u00a73.2), the privacy neuron editor (\u00a73.3) to erase the model memorization of privacy data, and the privacy neuron aggregator (\u00a73.4) for privacy preservation in batches."
        },
        {
            "heading": "3.1 Privacy Prediction Task",
            "text": "Given a tuple T = {X,Y }, let Y = {y1, ..., yn} be the sequence with private information, X be the the context of the sequence, \u03b8 be the parameters of a language model. Given a context X , the\nprobability of the language model yielding a token is P (yi|X,\u03b8), yi \u2208 Y , so the probability of the model leaking the private sequence is:\nP (Y |X,\u03b8) = |Y |\u220f i=1 P (yi|X,\u03b8) (1)\nTake \"An\u25a0 Ka\u25a0 is a senior writer at ESPN.com\" as private sentence containing a person\u2019s name \"An\u25a0 Ka\u25a0\". Suppose the input to the language model is \"_ _ is a senior writer at ESPN.com\", our goal is to reduce the probability of privacy leakage, i.e., minimizing the probability of predicting \"An\u25a0\" and \"Ka\u25a0\" ."
        },
        {
            "heading": "3.2 Privacy Neuron Detector",
            "text": "As described in Section 2 factual knowledge is found to be stored in the feed-forward networks of Transformer, in the form of key-value memory. Inspired by this, we speculate that private information might be also encoded in specific neurons. Model editing has offered methods to locate and edit knowledge-related neurons. However, existing methods can only deal with semantic units composed of a single token, making them not directly applicable to detect and edit mutli-token private sequences. To address this issue, we propose a privacy attribution method based on gradient integration. The proposed privacy attribution can evaluate\nwhich neurons play a key role in the leakage of private information from language models.\nLet wkl be a neuron to be evaluated by the privacy attribution method, where l is the layer of the neuron in the language model, and k is its position. According to \u00a73.1, the probability of the model outputting private information is:\nP (Y |X, wkl ) = |Y |\u220f i=1 P (yi|X, wkl = \u03b1kl ) (2)\nwhere \u03b1kl represents the value of the k-th neuron in the l-ith FFN layer.\nWe gradually change the parameter of the target neuron from 0 to the original value of the neuron. In this process, the probability of the output will accordingly change. We calculate the cumulative gradient of the probability change during this process as the neuron\u2019s contribution (i.e., privacy attribution score) to the privacy-sensitive output. The privacy attribution score is computed as:\nAtt(wkl ) = \u03b2 k l \u222b \u03b2kl 0 \u2202P (Y |X, \u03b1kl ) \u2202wkl d\u03b1kl (3)\nwhere \u03b2kl is the original value of the neuron w k l , \u2202P (Y |X,\u03b1kl ) \u2202wkl calculates the gradient of the model\noutput with regard to wkl . Directly calculating continuous integrals is intractable. We follow Dai et al. (2022) to use Riemann approximation:\nAtt(wkl ) = \u03b2kl m \u2211m j=1 \u2202P (Y |X, jm\u03b2 k l )\n\u2202wkl (4)\nwhere m = 20 is the number of approximation steps.\nAs P (Y |X, wkl ) = \u220f|Y |\ni=1 P (yi|X, wkl = \u03b1kl ), we have\nAtt(wkl ) = |Y |\u2211 i=1 \u03b2kl m \u2211m j=1 \u2202P (yi|X, jm\u03b2 k l ) \u2202wkl (5)\nIf the neuron has a great influence on the output of a private information, the gradient will be significant, and a large integration value will be obtained. Therefore, the privacy attribution score can measure the neuron\u2019s contribution to the leakage of privacy information, and the greater the privacy attribution score, the greater the privacy sensitivity of the neuron. We select neurons with the top z privacy attribution score as candidates for editing."
        },
        {
            "heading": "3.3 Privacy Editor",
            "text": "After detecting the privacy neuron candidates with the privacy neuron detector, we reduce the model memorization of private information by editing. Particularly, we use a simple yet effective editing strategy: setting the parameters (activation values) of the corresponding neurons to 0, so that the information flow will not pass through these privacy neurons."
        },
        {
            "heading": "3.4 Privacy Neuron Aggregator",
            "text": "As a number of sentences in the training data of LLMs contain private information, the privacy neuron detection and editing can be done over multiple sentences in a batch processing way. To erase privacy information encoded in the language model from multiple sentences in the training data, we propose the privacy neuron aggregator. When the input is a text batch, we calculate the privacy attribution score matrix of each sequence in the batch. After the privacy attribution score calculation, we let each sequence vote for neurons according to their privacy attribution scores, and select the top z neurons with the most votes. These selected neurons will be edited to erase private information. The hyperparameter z is adjusted according to the model size, training epochs and other factors. More details can be found in (\u00a75.1)."
        },
        {
            "heading": "4 Experiments",
            "text": "We carried out experiments to examine the effectiveness of the proposed DEPN on a dataset containing private information."
        },
        {
            "heading": "4.1 Setup",
            "text": "Dataset We used the Enron dataset (Klimt and Yang, 2004). It consists of employee emails that were publicly disclosed during Enron\u2019s legal investigation by the Federal Energy Regulatory Commission. It is the largest publicly available collection of \"real\" email data, containing over 500,000 emails from 158 users.1 We randomly sampled 5% of the data from Enron as the validation dataset to evaluate model performance.\nPrivate Information Sampling In our study, we categorized the private information in the Enron dataset into two types: private phrases (for the narrow definition of privacy), such as names and phone numbers, and a batch of randomly sampled sentences to be edit. Names: We selected 20 unique names that are memorized by language models, found in 126 sentences, such as \"An\u25a0 Ka\u25a0 is a senior writer at ESPN.com\". Phone Numbers: We also selected 20 unique LM-memorized phone numbers, such as \"My phone number is 7 1 3 8 5 \u25a0 \u25a0 \u25a0 \u25a0 \u25a0\". Private texts: We randomly selected 100 sentences that are not semantically overlapping with each other. In Appendix A.4, we discuss how we determine whether private information is memorized by a language model.\nModel Settings We conducted experiments using the widely used pretrained model, BERT-base (Devlin et al., 2018). The model consists of 12 transformer layers, with a hidden state size of 768 and an internal hidden size of 3072 for the feedforward network (FFN). Our experiments were performed on NVIDIA Tesla A6000 graphics processors. More training details are show in Appendix A.1.\nBaselines To demonstrate the effectiveness and robustness of DEPN, we compared it with the following baseline models. BERT-O: The bert model that has not been trained on the Enron dataset. Since the model does not know the privacy information in the dataset, it provides an oracle for assessing the risk of privacy leakage; BERT-F: The\n1https://www.cs.cmu.edu/~enron/\nbert model trained on the Enron dataset, which corresponds to the best predictive performance on the Enron dataset, but has the greatest risk of privacy leakage; BERT-DP: A privacy model trained by the differential privacy gradient descent method (Li et al., 2021) on the Enron dataset, which is the commonly used privacy protection method when using private data for training.\nWe applied our proposed DEPN on BERT-F to make a safe model, which is referred to as BERTFE in following experiments. Our codes are available now.2\nMetrics To observe the effect of different privacy preserving methods on the model performance, we use the Perplexity of Masked Language Modeling task on the Enron validation dataset (Valid-PPL) as the metric.\nDue to the different types of private information, we provide metrics separately for the risk of privacy leakage.\nExposure: The exposure (Carlini et al., 2019) metric is commonly used in privacy attacks to measure the exposure risk of phone numbers. Given a number sequence c, a model with parameters \u03b8, and the randomness space R, the exposure e\u03b8 of c can be calculated as :\ne\u03b8 = log2 |R| \u2212 log2 Rank\u03b8(c). (6)\nMean Reciprocal Rank (MRR): A person\u2019s name is usually composed of multiple tokens. Therefore, we use the reciprocal average of the rank of each target token to measure the model\u2019s memorization of names. Given a prefix Q, a name\n2https://github.com/flamewei123/DEPN\ntoken sequence E = {e1, ..., en}, the length is |E|, the model predicts the rank of the target token as rank(ei|Q), and the MRR for the name E is calculated as follows:\u2211|E|\ni=1 1\nRank(ei|Q) |E| . (7)\nPerplexity (PPL): When the private text is a complete sentence, we directly use the perplexity as the measure of the model memorization."
        },
        {
            "heading": "4.2 Main Results",
            "text": "Table 1 presents our main results, including model performance, privacy leakage risk, and execution time cost. The results demonstrate the competitiveness of our framework.\nFor the performance on the Enron validation dataset (Valid-PPL), BERT-O, which is not trained on the Enron dataset, exhibits the poorest performance. BERT-DP trained with DP-SGD does not perform well either, due to noise introduced during backpropagation. In contrast, BERT-FE equipped with DEPN performs almost on par with BERT-F on the validation dataset, indicating that neuron erasure minimally impacts model performance.\nRegarding privacy leakage risk metrics, including exposure, MRR, and PPL, clearly indicate that BERT-FE equipped with DEPN achieve the reduction of privacy leakage risk. BERT-F, trained directly on private data, exhibits the highest risk. In comparison, DEPN significantly reduces the risk of leakage. BERT-O, which has no access to private data, demonstrates the lowest risk across all three data types. The BERT-DP model also exhibits very low risk.\nIn terms of execution time cost, we assume that the fine-tuning time of BERT-F on data excluding privacy is 100% (reference time cost). The DEPN framework requires less than 5% of the reference time cost, while BERT-DP requires more time due to gradient clipping.\nIn conclusion, while differential privacy training and fine-tuning with non-private data can mitigate privacy leakage risks, they incur more time and may significantly undermine model performance. The DEPN framework strikes an excellent balance between performance and privacy protection."
        },
        {
            "heading": "5 Analysis",
            "text": "We further conducted in-depth analyses to demonstrate why DEPN is able to dememorize privacy in LLMs from multiple perspectives, including analyses on the relationship between privacy neurons and model memorization, on the robustness as well as the cost-effectiveness of DEPN."
        },
        {
            "heading": "5.1 Effect of the Hyperparameter",
            "text": "Figure 2 illustrates the impact of the hyperparameter, the number of edited neurons, on the model. We calculate the exposures of the original model BERTF and the enhanced model BERT-FE on 20 phone numbers. In Figure 2(a), the red line represents the average exposure of BERT-F, while the green line represents the average exposure of BERT-FE with varying numbers of edited neurons. As the number of edited neurons increases, the exposure significantly decreases. In Figure 2(b), the purple line represents the PPL of BERT-F on the valida-\ntion set, while the blue line represents the PPL of BERT-FE on the validation set with different numbers of edited neurons. As the number of erasures increases, the PPL noticeably increases. Therefore, increasing the number of edited neurons reduces the risk of privacy leakage in the model, but it also leads to a decrease in the model performance."
        },
        {
            "heading": "5.2 Relationship between Memorization And Privacy Neurons",
            "text": "As it is widely recognized, privacy data leakage often stems from the model\u2019s ability to memorize the training data. In this subsection, we conducted experiments to investigate the relationship between model memorization and privacy neurons, providing further evidence for the effectiveness of the proposed DEPN.\nImpact of Training Time on Privacy Neuron Distribution over Layers Figure 3 depicts the evolution of the distribution of privacy neurons over layers as the number of training epochs increases. Overall, the distribution of privacy neurons is pyramid-shaped, and most privacy neurons identified by the privacy neuron detector are located in layers 10-12 of BERT-base. Specifically, in epoch 1, about 40% of privacy neurons are in the top layer of BERT-base. As training progresses, the proportion of privacy neurons from deep layers increases to 60% by epoch 3 and to 80% by epoch 6. By the 9-th epoch, the distribution of privacy neurons remains largely unchanged compared to the 6-th epoch. This suggests that as the depth of model training increases, the memorization of\nprivate data tends to converge. In Appendix A.3, we conducted experiments to observe the changes of privacy leakage risk reduction at different training epoch. The results show that when the training time increases, the risk of privacy leakage increases too, and the proposed DEPN becomes more effective in privacy preservation.\nEffect of the Model Size Table 2 illustrates the performance of the DEPN framework on models of different scales. Each model was trained for 10 epochs using the optimal hyperparameter settings. Overall, larger models require more time to identify privacy neurons and require editing a greater number of privacy neurons for optimal performance. Larger models tended to show a deeper memory for phone numbers before privacy neurons are edited, leading to higher exposure. After privacy neuron editing, from the perspective of reduction rate, the exposure of the large model is reduced even more. These findings suggest that larger models are more at risk of privacy breaches. Fortunately, the DEPN framework demonstrates better performance on larger models compared to smaller ones, offering improved protection against\nprivacy risks.\nSummary of the Relationship between Memorization and Privacy Neurons Based on the aforementioned experimental findings, we can conclude that the model\u2019s scale, training time, and frequency of privacy data occurrence are all factors that have influence on the model memorization. As the model memorization of privacy data deepens, the aggregation of privacy neurons associated with privacy data becomes more pronounced, which makes the method of locating and eliminating privacy neurons more suitable for deep memorization scenarios. Therefore, the DEPN framework has demonstrated excellent effectiveness in mitigating model memorization."
        },
        {
            "heading": "5.3 Robustness Analysis",
            "text": "Ablation Study We conducted ablation experiments to assess the robustness of the privacy neuron detector by comparing its performance with different neuron localization methods on phone number data. In Table 4, we present the results of these experiments. Specifically, \"KN\" refers to the knowledge attribution approach proposed by Dai et al. (2022), while \"Random\" donates an approach\nthat randomly selects the same number of neurons as our method. Our method PND (privacy neuron detector) achieves superior performance in terms of exposure reduction compared to the other methods. Although the knowledge attribution approach gains a good exposure reduction, it is less effective than our method due to its attribution being targeted at a single token. The random selection approach is also able to decrease privacy exposure but the exposure reduction is not as significant as the KN approach and our detector. These results unequivocally demonstrate the effectiveness of our method for in privacy neuron localization.\nRobustness to Different Prompts We conducted experiments to validate the robustness of DEPN to different prompts. We sampled private data containing phone numbers, all composed of the same prefix, from the training dataset. We then performed privacy attacks during inference using different prompts to examine whether changing prompts would still result in privacy leakage. Table 5 presents the results of these experiments. The training data consist of phone numbers with the same prefix of \u2018Contact me at ***\u2019. We observe privacy risk reduction across all prompts, demonstrating the robustness of DEPN to prompt."
        },
        {
            "heading": "5.4 Analysis on the Cost-Effectiveness of DEPN",
            "text": "In this subsection we discuss the limitation of DEPN, specifically its dependency on the amount of private data to be erased. We conducted an experiment where we used 1,000 private data instances, each containing phone numbers, extracted from our training dataset. DEPN was applied onto the BERTbase model to erase private information. Experi-\nment results are shown in Table 3. As the amount of private data increases, more neurons need to be edited to achieve better privacy protection, and the performance of the model drops significantly. Furthermore, it becomes apparent that, with the escalation of private data volume, the reduction in privacy risks gradually diminishes. These observations indicate that DEPN excels in remediating language models when dealing with a small number of data leaks, but exhibits weak performance when confronted with a large batch of private data."
        },
        {
            "heading": "6 Related Work",
            "text": "Model Editing To edit incorrect or undesirable information captured in LLMs, a variety of model editing approaches have been proposed, which can be categorized into four strategies. First, the Constrained Fine-tuning strategy (Zhu et al., 2020) updates LLMs specifically for the target knowledge, allowing precise modification. Second, the Memory-based Editing strategy (Mitchell et al., 2022; Dong et al., 2022) maintains a knowledge cache that stores new information to replace undesirable predictions. Third, the Meta-learning-based Editing strategy (De Cao et al., 2021; Mitchell et al., 2021) introduces editable training based on metalearning, training model parameters to accommodate editing. Lastly, the Locating and Editing strategy (Geva et al., 2021; Meng et al., 2022; Dai et al., 2022) assumes that knowledge is locally stored in LLMs. This strategy locates specific parameters associated with the knowledge and directly edits parameters to perform editing.\nPrivacy Protection To address privacy risks in NLP models, various privacy-preserving methods have been proposed, which can be categorized into three main stages of application (Guo et al., 2022; Sousa and Kern, 2023): data processing stage, pre-training and/or fine-tuning stage, and post-processing stage. In the data processing stage, methods involve removing or replacing sensitive information in the original data (Liu et al., 2017; El Emam et al., 2009; Zhou et al., 2008; Garc\u00edaPablos et al., 2020). In the pre-training or finetuning stage, data privacy can be protected by modifying the model training process. One approach is differential privacy stochastic gradient descent (DPSGD) (Li et al., 2021; Hoory et al., 2021), which introduces noise into the clipped gradient to reduce the distinction between gradients and prevent memorization of training data. Another method is adversarial training (Plant et al., 2021; Coavoux et al., 2018), which constrains the model\u2019s learning of private information through adversarial training techniques. However, methods used in the data processing stage and in the pre-training or fine-tuning stage are not applicable if the privacy leakage is discovered after the model training is completed. Methods used in the post-processing stage focus on making trained models forget specific data or alter specific parameters to safeguard hidden private information (Bourtoule et al., 2021; Gupta et al., 2021; Neel et al., 2020). These methods are often with high computational cost and cannot be easily applied to large models. In contrast, proposed DEPN can achieve the protection of private information in the post-processing stage with a small computational overhead."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we have presented a privacy neuron detecting and editing framework DEPN to address privacy leakage risks in pretrained language models. Through the privacy neuron detector based on the privacy attribution scoring method, we accurately detect risky neurons associated with private information. The privacy neuron editor effectively eliminates model memorization of private data. Experimental results and in-depth analyses demonstrate the ability of DEPN to reduce privacy risks efficiently without degrading model performance. Our work explores a novel approach to privacy protection and contributes to model de-memorization in the post-processing stage.\nLimitations Our current study still has two limitations. First, although we propose a method to process private data in batches, we find that too many instances in a batch will reduce the effect of memorization erasure. Second, we use a few types of private information in our experiments due to the limited availability of datasets containing private information. We would like to collect more available datasets for our framework in the future.\nEthical Statement In this paper, we use the Enron dataset to evaluate the privacy-preserving effect of DEPN. This dataset consists of employee emails that were publicly disclosed during Enron\u2019s legal investigation by the Federal Energy Regulatory Commission. Since the data comes from real persons, we masked sensitive information such as specific names and phone numbers in this paper."
        },
        {
            "heading": "Acknowledgements",
            "text": "The work was partially supported by the research collaboration project between Tianjin University and ByteDance(PJ20210625900030) and Zhejiang Lab (No. 2022KH0AB01). We would like to thank the anonymous reviewers for their insightful comments."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Training Details For BERT-base fine-tuning, we set the hyperparameters as follows: 20 training epochs, a learning rate of 3e-5 with linear warm-up, and a batch size of 16. We fine-tuned BERT-base on the Enron dataset using the Masked Language Modeling task to simulate training on datasets containing privacy information. Additionally, we pretrained smaller (layer=4, hidden size=512, intermediate size=2048) (Bhargava et al., 2021) and larger (layer=24, hidden size=1024, intermediate size=4096) BERT models 3 to compare the performance of privacy erasure at different model scales.\nA.2 Effect of the Frequency of Privacy Data Ocurrence\nWe also examined the influence of the frequency of privacy data ocurrence in the training set on DEPN. As shown in Table 6, phone numbers with an ocurrence frequency greater than 10 exhibit higher exposure compared to those with a frequency less than 10, indicating a higher risk of leakage. However, after erasure, the exposure of phone numbers with a frequency greater than 10 is reduced by 32.65%, while the exposure of phone numbers with a frequency less than 10 is reduced by 22.58%. These results suggest that our method effectively reduces exposure for both high-frequency and lowfrequency phone numbers, mitigating the risk of privacy leakage.\nA.3 Effect of Training Time Figure 4 illustrates the changes in exposure of phone number data before and after erasing privacy neurons in models with different training epochs. We conducted experiments using 20 different phone numbers and averaged the final results. The blue line represents the exposure of phone numbers before privacy neuron erasing. The blue line initially remains low but exhibits a significant surge after the 10-th epoch, indicating that models\n3https://huggingface.co/BERT-large-uncased\nwith longer training time have a more pronounced memorization of the training data. Additionally, the yellow line represents the exposure of phone numbers after privacy neuron erasing. The widening gap between the two lines indicates that as the model\u2019s memorization becomes more apparent, the proposed DEPN becomes more effective in privacy preservation.\nA.4 The Judgement of the Memorization In our experiment, we specifically identify the private data memorized by the language model from the training dataset. To assess whether the model has memorized private data, we employ the context of private information as the input to the language model. Subsequently, we calculate the risk of private information leakage and classify the information with a leakage risk exceeding predefined thresholds as having been memorized by the language model. For names, we establish a threshold for memorization as the MRR of less than 1.5. For phone numbers, we employ the Exposure values exceeding 15 as memorization."
        }
    ],
    "title": "DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models",
    "year": 2023
}