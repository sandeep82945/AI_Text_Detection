{
    "abstractText": "Multiple defendants in a criminal fact description generally exhibit complex interactions, and cannot be well handled by existing Legal Judgment Prediction (LJP) methods which focus on predicting judgment results (e.g., law articles, charges, and terms of penalty) for single-defendant cases. To address this problem, we propose the task of multi-defendant LJP, which aims to automatically predict the judgment results for each defendant of multidefendant cases. Two challenges arise with the task of multi-defendant LJP: (1) indistinguishable judgment results among various defendants; and (2) the lack of a real-world dataset for training and evaluation. To tackle the first challenge, we formalize the multi-defendant judgment process as hierarchical reasoning chains and introduce a multi-defendant LJP method, named Hierarchical Reasoning Network (HRN), which follows the hierarchical reasoning chains to determine criminal relationships, sentencing circumstances, law articles, charges, and terms of penalty for each defendant. To tackle the second challenge, we collect a real-world multi-defendant LJP dataset, namely MultiLJP, to accelerate the relevant research in the future. Extensive experiments on MultiLJP verify the effectiveness of our proposed HRN.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yougang Lyu"
        },
        {
            "affiliations": [],
            "name": "Jitai Hao"
        },
        {
            "affiliations": [],
            "name": "Zihan Wang"
        },
        {
            "affiliations": [],
            "name": "Kai Zhao"
        },
        {
            "affiliations": [],
            "name": "Shen Gao"
        },
        {
            "affiliations": [],
            "name": "Pengjie Ren"
        },
        {
            "affiliations": [],
            "name": "Zhumin Chen"
        },
        {
            "affiliations": [],
            "name": "Fang Wang"
        },
        {
            "affiliations": [],
            "name": "Zhaochun Ren"
        }
    ],
    "id": "SP:bd13ea1370d9cb26e80c79441706529e96564724",
    "references": [
        {
            "authors": [
                "Nikolaos Aletras",
                "Dimitrios Tsarapatsanis",
                "Daniel Preotiuc-Pietro",
                "Vasileios Lampos."
            ],
            "title": "Predicting judicial decisions of the european court of human rights: a natural language processing perspective",
            "venue": "PeerJ Comput. Sci., 2:e93.",
            "year": 2016
        },
        {
            "authors": [
                "Oana-Maria Camburu",
                "Tim Rockt\u00e4schel",
                "Thomas Lukasiewicz",
                "Phil Blunsom."
            ],
            "title": "e-snli: Natural language inference with natural language explanations",
            "venue": "Proceedings of NeurIPS, pages 9560\u20139572.",
            "year": 2018
        },
        {
            "authors": [
                "Ilias Chalkidis",
                "Ion Androutsopoulos",
                "Nikolaos Aletras."
            ],
            "title": "Neural legal judgment prediction in english",
            "venue": "Proceedings of ACL, pages 4317\u20134323.",
            "year": 2019
        },
        {
            "authors": [
                "Ilias Chalkidis",
                "Manos Fergadiotis",
                "Prodromos Malakasiotis",
                "Nikolaos Aletras",
                "Ion Androutsopoulos."
            ],
            "title": "LEGAL-BERT: \"preparing the muppets for court\u2019",
            "venue": "Findings of EMNLP, pages 2898\u20132904.",
            "year": 2020
        },
        {
            "authors": [
                "Ilias Chalkidis",
                "Manos Fergadiotis",
                "Dimitrios Tsarapatsanis",
                "Nikolaos Aletras",
                "Ion Androutsopoulos",
                "Prodromos Malakasiotis."
            ],
            "title": "Paragraph-level rationale extraction through regularization: A case study on european court of human rights cases",
            "venue": "Proceed-",
            "year": 2021
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Ziqing Yang."
            ],
            "title": "Pre-training with whole word masking for chinese BERT",
            "venue": "IEEE ACM Trans. Audio Speech Lang. Process., 29:3504\u20133514.",
            "year": 2021
        },
        {
            "authors": [
                "Qian Dong",
                "Shuzi Niu."
            ],
            "title": "Legal judgment prediction via relational learning",
            "venue": "Proceedings of SIGIR, pages 983\u2013992.",
            "year": 2021
        },
        {
            "authors": [
                "Yi Feng",
                "Chuanyi Li",
                "Vincent Ng."
            ],
            "title": "Legal judgment prediction: A survey of the state of the art",
            "venue": "Proceedings of IJCAI, pages 5461\u20135469.",
            "year": 2022
        },
        {
            "authors": [
                "Yi Feng",
                "Chuanyi Li",
                "Vincent Ng."
            ],
            "title": "Legal judgment prediction via event extraction with constraints",
            "venue": "Proceedings of ACL, pages 648\u2013664.",
            "year": 2022
        },
        {
            "authors": [
                "Leilei Gan",
                "Kun Kuang",
                "Yi Yang",
                "Fei Wu."
            ],
            "title": "Judgment prediction via injecting legal knowledge into neural networks",
            "venue": "Proceedings of AAAI, pages 12866\u201312874.",
            "year": 2021
        },
        {
            "authors": [
                "Yuling Gu",
                "Bhavana Dalvi Mishra",
                "Peter Clark."
            ],
            "title": "DREAM: uncovering mental models behind language models",
            "venue": "CoRR, abs/2112.08656.",
            "year": 2021
        },
        {
            "authors": [
                "Peter Hase",
                "Mohit Bansal."
            ],
            "title": "When can models learn from explanations? A formal framework for understanding the roles of explanation data",
            "venue": "CoRR, abs/2102.02201.",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Saurav Kadavath",
                "Akul Arora",
                "Steven Basart",
                "Eric Tang",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring mathematical problem solving with the MATH dataset",
            "venue": "Proceedings of NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Zikun Hu",
                "Xiang Li",
                "Cunchao Tu",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Few-shot charge prediction with discriminative legal attributes",
            "venue": "Proceedings of COLING, pages 487\u2013498.",
            "year": 2018
        },
        {
            "authors": [
                "Jie Huang",
                "Kevin Chen-Chuan Chang."
            ],
            "title": "Towards reasoning in large language models: A survey",
            "venue": "CoRR, abs/2212.10403.",
            "year": 2022
        },
        {
            "authors": [
                "Yunyun Huang",
                "Xiaoyu Shen",
                "Chuanyi Li",
                "Jidong Ge",
                "Bin Luo."
            ],
            "title": "Dependency learning for legal judgment prediction with a unified text-to-text transformer",
            "venue": "CoRR, abs/2112.06370.",
            "year": 2021
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave."
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "Proceedings of EACL, pages 874\u2013880.",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Martin Katz",
                "Michael J Bommarito",
                "Josh Blackman."
            ],
            "title": "A general approach for predicting the behavior of the supreme court of the united states",
            "venue": "PloS one, 12(4):e0174698.",
            "year": 2017
        },
        {
            "authors": [
                "Fred Kort."
            ],
            "title": "Predicting supreme court decisions mathematically: A quantitative analysis of the\" right to counsel\" cases",
            "venue": "The American Political Science Review, 51(1):1\u201312.",
            "year": 1957
        },
        {
            "authors": [
                "Yuquan Le",
                "Yuming Zhao",
                "Meng Chen",
                "Zhe Quan",
                "Xiaodong He",
                "Kenli Li."
            ],
            "title": "Legal charge prediction via bilinear attention network",
            "venue": "Proceedings of CIKM, pages 1024\u20131033.",
            "year": 2022
        },
        {
            "authors": [
                "Wang Ling",
                "Dani Yogatama",
                "Chris Dyer",
                "Phil Blunsom."
            ],
            "title": "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
            "venue": "Proceedings of ACL, pages 158\u2013167.",
            "year": 2017
        },
        {
            "authors": [
                "Dugang Liu",
                "Weihao Du",
                "Lei Li",
                "Weike Pan",
                "Zhong Ming."
            ],
            "title": "Augmenting legal judgment prediction with contrastive case relations",
            "venue": "Proceedings of COLING, pages 2658\u20132667.",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "Proceedings of ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Bingfeng Luo",
                "Yansong Feng",
                "Jianbo Xu",
                "Xiang Zhang",
                "Dongyan Zhao."
            ],
            "title": "Learning to predict charges for criminal cases with legal basis",
            "venue": "Proceedings of EMNLP, pages 2727\u20132736.",
            "year": 2017
        },
        {
            "authors": [
                "Yougang Lyu",
                "Piji Li",
                "Yechang Yang",
                "Maarten de Rijke",
                "Pengjie Ren",
                "Yukun Zhao",
                "Dawei Yin",
                "Zhaochun Ren."
            ],
            "title": "Feature-level debiased natural language understanding",
            "venue": "Proceedings of AAAI, pages 13353\u2013 13361.",
            "year": 2023
        },
        {
            "authors": [
                "Yougang Lyu",
                "Zihan Wang",
                "Zhaochun Ren",
                "Pengjie Ren",
                "Zhumin Chen",
                "Xiaozhong Liu",
                "Yujun Li",
                "Hongsong Li",
                "Hongye Song."
            ],
            "title": "Improving legal judgment prediction through reinforced criminal element extraction",
            "venue": "Inf. Process. Manag., 59(1):102780.",
            "year": 2022
        },
        {
            "authors": [
                "Luyao Ma",
                "Yating Zhang",
                "Tianyi Wang",
                "Xiaozhong Liu",
                "Wei Ye",
                "Changlong Sun",
                "Shikun Zhang."
            ],
            "title": "Legal judgment prediction with multi-stage case representation learning in the real court setting",
            "venue": "Proceedings of SIGIR, pages 993\u20131002.",
            "year": 2021
        },
        {
            "authors": [
                "Vijit Malik",
                "Rishabh Sanjay",
                "Shubham Kumar Nigam",
                "Kripabandhu Ghosh",
                "Shouvik Kumar Guha",
                "Arnab Bhattacharya",
                "Ashutosh Modi."
            ],
            "title": "ILDC for CJPE: indian legal documents corpus for court judgment prediction and explanation",
            "venue": "Proceedings of",
            "year": 2021
        },
        {
            "authors": [
                "Stuart S Nagel."
            ],
            "title": "Applying correlation analysis to case prediction",
            "venue": "Tex. L. Rev., 42:1006.",
            "year": 1963
        },
        {
            "authors": [
                "Joel Niklaus",
                "Ilias Chalkidis",
                "Matthias St\u00fcrmer."
            ],
            "title": "Swiss-judgment-prediction: A multilingual legal judgment prediction benchmark",
            "venue": "CoRR, abs/2110.00806.",
            "year": 2021
        },
        {
            "authors": [
                "Sicheng Pan",
                "Tun Lu",
                "Ning Gu",
                "Huajuan Zhang",
                "Chunlin Xu."
            ],
            "title": "Charge prediction for multidefendant cases with multi-scale attention",
            "venue": "Proceedings of ChineseCSCW, volume 1042 of Communications in Computer and Information Science,",
            "year": 2019
        },
        {
            "authors": [
                "Shounak Paul",
                "Pawan Goyal",
                "Saptarshi Ghosh."
            ],
            "title": "Automatic charge identification from facts: A few sentence-level charge annotations is all you need",
            "venue": "Proceedings of COLING, pages 1011\u20131022.",
            "year": 2020
        },
        {
            "authors": [
                "Evaggelia Pitoura",
                "Panayiotis Tsaparas",
                "Giorgos Flouris",
                "Irini Fundulaki",
                "Panagiotis Papadakos",
                "Serge Abiteboul",
                "Gerhard Weikum."
            ],
            "title": "On measuring bias in online information",
            "venue": "SIGMOD Rec., 46(4):16\u201321.",
            "year": 2017
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Nazneen Fatema Rajani",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Explain yourself! leveraging language models for commonsense reasoning",
            "venue": "Proceedings of ACL, pages 4932\u20134942.",
            "year": 2019
        },
        {
            "authors": [
                "Jeffrey A Segal."
            ],
            "title": "Predicting supreme court cases probabilistically: The search and seizure cases, 19621981",
            "venue": "American Political Science Review, 78(4):891\u2013 900.",
            "year": 1984
        },
        {
            "authors": [
                "Octavia-Maria Sulea",
                "Marcos Zampieri",
                "Shervin Malmasi",
                "Mihaela Vela",
                "Liviu P. Dinu",
                "Josef van Genabith."
            ],
            "title": "Exploring the use of text classification in the legal domain",
            "venue": "Proceedings of ICAIL, volume 2143.",
            "year": 2017
        },
        {
            "authors": [
                "Octavia-Maria Sulea",
                "Marcos Zampieri",
                "Mihaela Vela",
                "Josef van Genabith."
            ],
            "title": "Predicting the law area and decisions of french supreme court cases",
            "venue": "Proceedings of RANLP, pages 716\u2013722.",
            "year": 2017
        },
        {
            "authors": [
                "Alon Talmor",
                "Jonathan Herzig",
                "Nicholas Lourie",
                "Jonathan Berant."
            ],
            "title": "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
            "venue": "Proceedings of NAACL, pages 4149\u20134158.",
            "year": 2019
        },
        {
            "authors": [
                "Alon Talmor",
                "Oyvind Tafjord",
                "Peter Clark",
                "Yoav Goldberg",
                "Jonathan Berant."
            ],
            "title": "Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge",
            "venue": "Proceedings of NeurIPS.",
            "year": 2020
        },
        {
            "authors": [
                "Pengfei Wang",
                "Yu Fan",
                "Shuzi Niu",
                "Ze Yang",
                "Yongfeng Zhang",
                "Jiafeng Guo."
            ],
            "title": "Hierarchical matching network for crime classification",
            "venue": "Proceedings of SIGIR, pages 325\u2013334.",
            "year": 2019
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed H. Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "CoRR, abs/2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Chaojun Xiao",
                "Xueyu Hu",
                "Zhiyuan Liu",
                "Cunchao Tu",
                "Maosong Sun."
            ],
            "title": "Lawformer: A pre-trained language model for chinese legal long documents",
            "venue": "AI Open, 2:79\u201384.",
            "year": 2021
        },
        {
            "authors": [
                "Chaojun Xiao",
                "Xueyu Hu",
                "Zhiyuan Liu",
                "Cunchao Tu",
                "Maosong Sun."
            ],
            "title": "Lawformer: A pre-trained language model for chinese legal long documents",
            "venue": "CoRR, abs/2105.03887.",
            "year": 2021
        },
        {
            "authors": [
                "Chaojun Xiao",
                "Haoxi Zhong",
                "Zhipeng Guo",
                "Cunchao Tu",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Yansong Feng",
                "Xianpei Han",
                "Zhen Hu",
                "Heng Wang",
                "Jianfeng Xu."
            ],
            "title": "CAIL2018: A large-scale legal dataset for judgment prediction",
            "venue": "CoRR, abs/1807.02478.",
            "year": 2018
        },
        {
            "authors": [
                "Nuo Xu",
                "Pinghui Wang",
                "Long Chen",
                "Li Pan",
                "Xiaoyan Wang",
                "Junzhou Zhao."
            ],
            "title": "Distinguish confusing law articles for legal judgment prediction",
            "venue": "Proceedings of ACL, pages 3086\u20133095.",
            "year": 2020
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mt5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of NAACL, pages 483\u2013498.",
            "year": 2021
        },
        {
            "authors": [
                "Wenmian Yang",
                "Weijia Jia",
                "Xiaojie Zhou",
                "Yutao Luo."
            ],
            "title": "Legal judgment prediction via multiperspective bi-feedback network",
            "venue": "Proceedings of IJCAI, pages 4085\u20134091.",
            "year": 2019
        },
        {
            "authors": [
                "Huihan Yao",
                "Ying Chen",
                "Qinyuan Ye",
                "Xisen Jin",
                "Xiang Ren."
            ],
            "title": "Refining language models with compositional explanations",
            "venue": "Proceedings of NeurIPS, pages 8954\u20138967.",
            "year": 2021
        },
        {
            "authors": [
                "Linan Yue",
                "Qi Liu",
                "Binbin Jin",
                "Han Wu",
                "Kai Zhang",
                "Yanqing An",
                "Mingyue Cheng",
                "Biao Yin",
                "Dayong Wu."
            ],
            "title": "Neurjudge: A circumstance-aware neural framework for legal judgment prediction",
            "venue": "Proceedings of SIGIR, pages 973\u2013982.",
            "year": 2021
        },
        {
            "authors": [
                "Omar Zaidan",
                "Jason Eisner",
                "Christine D. Piatko."
            ],
            "title": "Using \"annotator rationales\" to improve machine learning for text categorization",
            "venue": "Proceedings of NAACL, pages 260\u2013267.",
            "year": 2007
        },
        {
            "authors": [
                "Han Zhang",
                "Zhicheng Dou",
                "Yutao Zhu",
                "Ji-Rong Wen."
            ],
            "title": "Contrastive learning for legal judgment prediction",
            "venue": "ACM Transactions on Information Systems.",
            "year": 2023
        },
        {
            "authors": [
                "Xiaoyu Zhang",
                "Xin Xin",
                "Dongdong Li",
                "Wenxuan Liu",
                "Pengjie Ren",
                "Zhumin Chen",
                "Jun Ma",
                "Zhaochun Ren."
            ],
            "title": "Variational reasoning over incomplete knowledge graphs for conversational recommendation",
            "venue": "Proceedings of WSDM, pages 231\u2013239.",
            "year": 2023
        },
        {
            "authors": [
                "Haoxi Zhong",
                "Zhipeng Guo",
                "Cunchao Tu",
                "Chaojun Xiao",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Legal judgment prediction via topological learning",
            "venue": "Proceedings of EMNLP, pages 3540\u20133549. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Haoxi Zhong",
                "Yuzhong Wang",
                "Cunchao Tu",
                "Tianyang Zhang",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Iteratively questioning and answering for interpretable legal judgment prediction",
            "venue": "Proceedings of AAAI, pages 1250\u20131257.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Legal Judgment Prediction (LJP) aims at predicting judgment results (e.g., law articles, applicable charges, and terms of penalty) based on the fact description of a given case. Existing LJP studies primarily focus on single-defendant cases, where only one defendant is involved (Luo et al., 2017; Zhong et al., 2018; Yang et al., 2019; Xu et al., 2020; Dong and Niu, 2021; Yue et al., 2021; Lyu et al., 2022; Feng et al., 2022b; Zhang et al., 2023a).\n\u2217 Equal contribution. \u2020 Corresponding author.\nDespite these successful efforts, singledefendant LJP suffers from an inevitable restriction in practice: a large number of fact descriptions with multiple defendants. According to statistics derived from published legal documents sampled from legal case information disclosure, multidefendant cases constitute a minimum of 30% of all cases (Pan et al., 2019). As shown in Figure 1, the multi-defendant LJP task aims at predicting law articles, charges, and terms of penalty for each defendant in multi-defendant cases. Since multiple defendants are mentioned in the fact description and exhibit complex interactions, the multi-defendant LJP task requires clarifying these interactions and making accurate judgments for each defendant, which is intuitively beyond the reach of single-defendant LJP methods. Hence, there is a pressing need to extend LJP from\nsingle-defendant to multi-defendant scenarios. However, two main challenges arise with the task of multi-defendant LJP:\n\u2022 Indistinguishable judgment results among various defendants. As complicated interactions exist among multiple defendants, fact descriptions of various defendants are usually mixed together. Thus it is difficult to distinguish different judgment results among various defendants so as to make accurate judgments for each defendant. As shown in Figure 1, in order to distinguish different judgment results among various defendants, a judge has to clarify criminal relationships among defendants to determine whether defendants share same law articles and charges, and sentencing circumstances affecting terms of penalty for each defendant. Based on these intermediate reasoning results, the judge determines and verifies the judgment results (law articles, charges, and terms of penalty) for each defendant, following a forward and backward order. The motivation behind forward prediction and backward verification is rooted in the complex nature of legal reasoning, where evidence and conclusions can be interdependent (Zhong et al., 2018; Yang et al., 2019). Overall, the multi-defendant judgment process requires simulating the judicial logic of human judges and modeling complex reasoning chains.\n\u2022 Lack of real-world multi-defendant LJP datasets. Existing datasets for LJP either only have single-defendant cases annotated for multiple LJP subtasks or multi-defendant cases for a single LJP subtask. Xiao et al. (2018) collect a real-world LJP dataset CAIL, which only retains single-defendant cases. Pan et al. (2019) only annotate multi-defendant cases with the charge prediction subtask and ignore information about criminal relationships and sentencing circumstances that can distinguish judgment results of multiple defendants in real scenarios. In order to accelerate the research on multi-defendant LJP, we urgently need a real-world multi-defendant LJP dataset. To tackle the first challenge, we formalize the multi-defendant judgment process as hierarchical reasoning chains and propose a method for multidefendant LJP, named Hierarchical Reasoning Network (HRN), which follows the hierarchical reasoning chains to distinguish different judgment results of various defendants. Specifically, the hi-\nerarchical reasoning chains are divided into two levels. The first-level reasoning chain identifies the relationships between defendants and determines the sentencing circumstances for each defendant. The second-level reasoning chain predicts and verifies the law articles, charges, and terms of penalty for each defendant, using a forward prediction process and a backward verification process, respectively. Since generative language models have shown great ability to reason (Talmor et al., 2020; Yao et al., 2021; Hase and Bansal, 2021), we convert these reasoning chains into Sequenceto-Sequence (Seq2Seq) generation tasks and apply the mT5 (Xue et al., 2021) to model them. Furthermore, we adopt Fusion-in-Decoder (FID) (Izacard and Grave, 2021) to process multi-defendant fact descriptions with thousands of tokens efficiently.\nTo tackle the second challenge, we collect a realworld dataset, namely MultiLJP, with 23,717 realworld multi-defendant LJP cases. Eight professional annotators are involved in manually editing law articles, charges, terms of penalty, criminal relationships, and sentencing circumstances for each defendant. In 89.58 percent of these cases, the defendants have different judgment results for at least one of the subtasks of the multi-defendant LJP task. MultiLJP requires accurate distinction of the judgment results for each defendant. This makes MultiLJP different from the existing singledefendant LJP datasets. Our work provides the first benchmark for the multi-defendant LJP task.\nUsing MultiLJP, we evaluate the effectiveness of HRN for multi-defendant LJP on various subtasks. The results show that HRN can significantly outperform all the baselines. In summary, our main contributions are: \u2022 We focus on the multi-defendant LJP task and\nformalize the multi-defendant judgment process as hierarchical reasoning chains for the multidefendant LJP task. \u2022 We introduce HRN, a novel method that follows the hierarchical reasoning chains to distinguish the judgment results for each defendant in multidefendant LJP. \u2022 We present MultiLJP, the first real-world dataset for multi-defendant LJP, which facilitates future research in this area1. \u2022 We demonstrate the effectiveness of HRN on MultiLJP through empirical experiments.\n1Our code and MultiLJP dataset are available at https: //github.com/CURRENTF/HRN."
        },
        {
            "heading": "2 Related work",
            "text": ""
        },
        {
            "heading": "2.1 Legal judgment prediction",
            "text": "Legal judgment prediction has been studied in various jurisdictions (Zhong et al., 2020; Feng et al., 2022a; Katz et al., 2017; Chalkidis et al., 2019; Sulea et al., 2017a,b; Malik et al., 2021; Paul et al., 2020; Niklaus et al., 2021). Early studies on LJP focus on rule-based methods (Kort, 1957; Nagel, 1963; Segal, 1984) and machine learning algorithms (Aletras et al., 2016; Sulea et al., 2017a,b; Katz et al., 2017). Recent neural-based approaches jointly predict judgment results (law articles, charges and term of penalty) for singledefendant cases by modeling dependency between LJP subtasks (Zhong et al., 2018; Yang et al., 2019; Dong and Niu, 2021; Huang et al., 2021), leveraging legal knowledge (Hu et al., 2018; Gan et al., 2021; Yue et al., 2021; Ma et al., 2021; Lyu et al., 2022; Feng et al., 2022b), exploiting label information (Luo et al., 2017; Wang et al., 2019; Xu et al., 2020; Le et al., 2022; Liu et al., 2022; Zhang et al., 2023a), or employing pre-trained language models (Chalkidis et al., 2020, 2021; Xiao et al., 2021a). For multi-defendant cases, MAMD (Pan et al., 2019) utilizes multi-scale attention to distinguish confusing fact descriptions of different defendants for multi-defendant charge prediction.\nHowever, existing single-defendant LJP methods neglect the complex interactions among multiple defendants. Moreover, compared with the multidefendant LJP method MAMD (Pan et al., 2019), we follow the human judgment process to model hierarchical reasoning chains to distinguish different judgment results of various defendants and make accurate judgments for each defendant."
        },
        {
            "heading": "2.2 Multi-step reasoning with language models",
            "text": "Multi-step reasoning by training or fine-tuning language models to generate intermediate steps has been shown to improve performance (Zaidan et al., 2007; Talmor et al., 2020; Yao et al., 2021; Hase and Bansal, 2021; Zhang et al., 2023b; Gu et al., 2021). Ling et al. (2017) generate natural language intermediate steps to address math word problems. Camburu et al. (2018) extend the natural language inference dataset with human-annotated natural language explanations of the entailment relations. Rajani et al. (2019) generate rationales that explain model predictions for commonsense questionanswering tasks (Talmor et al., 2019). Hendrycks\net al. (2021) finetune pre-trained language models to solve competition mathematics problems by generating multi-step solutions. Nye et al. (2021) train language models to predict the final outputs of programs by predicting intermediate computational results. Recently, Wei et al. (2022) propose chain of thought prompting, which feed large language models with step-by-step reasoning examples without fine-tuning to improve model performance.\nHowever, these methods are not designed for more realistic legal reasoning applications (Huang and Chang, 2022). In this paper, we aim to use generative language models to capture hierarchical reasoning chains for the multi-defendant LJP task."
        },
        {
            "heading": "3 Dataset",
            "text": "In this section, we describe the construction process and analyze various aspects of MultiLJP to provide a deeper understanding of the dataset."
        },
        {
            "heading": "3.1 Dataset construction",
            "text": "To the best of our knowledge, existing LJP datasets only focus on single-defendant cases or charge prediction for multi-defendant cases. Thus, we construct a Multi-defendant Legal Judgment Prediction (MultiLJP) dataset from the published legal documents in China Judgements Online2. Instead of extracting labels using regular expressions as in existing works (Xiao et al., 2018; Pan et al., 2019), we hire eight professional annotators to manually produce law articles, charges, terms of penalty, criminal relationships, and sentencing circumstances for each defendant in multi-defendant cases. The annotators are native Chinese speakers who have passed\n2https://wenshu.court.gov.cn/\nChina\u2019s Unified Qualification Exam for Legal Professionals. All data is evaluated by two annotators repeatedly to eliminate bias. Since second-instance cases and retrial cases are too complicated, we only retain first-instance cases. Besides, we anonymize sensitive information (e.g., name, location, etc.) for multi-defendant cases to avoid potential risks of structured social biases (Pitoura et al., 2017) and protect personal privacy. After preprocessing and manual annotation, the MultiLJP consists of 23,717 multi-defendant cases. The statistics information of dataset MultiLJP can be found in Table 1."
        },
        {
            "heading": "3.2 Dataset analysis",
            "text": "Analysis of the number of defendants. MultiLJP only contains multi-defendant cases. The number of defendants per case is distributed as follows: 49.40 percent of cases have two defendants, 21.41 percent have three defendants, 11.22 percent have four defendants, and 17.97 percent have more than four defendants. The MultiLJP dataset has 80,477 defendants in total. On average, each multidefendant case has 3.39 defendants.\nAnalysis of multi-defendant judgment results. In 89.58 percent of these cases, the defendants have different judgment results for at least one of the subtasks of the multi-defendant LJP task. Specifically, 18.91 percent of cases apply different law articles to different defendants, 26.80 percent of cases impose different charges on different defendants, and 88.54 percent of cases assign different terms of penalty to different defendants.\nAnalysis of criminal relationships and sentencing circumstances. Based on the gold labels of criminal relationships and sentencing circumstances, ideally, a judge can distinguish between 69.73 percent of defendants with different judgment results (law articles, charges, and terms of penalty). Specifically, based on the criminal relationship, a judge can distinguish 70.28 percent of defendants with different law articles and 72.50 percent of defendants with different charges; based on the sentencing circumstances, a judge can distinguish 96.28 percent of defendants with different terms of penalty."
        },
        {
            "heading": "4 Method",
            "text": "In this section, we describe the HRN method. First, we formulate our research problem. Then, we introduce the Sequence-to-Sequence (Seq2Seq) gen-\neration framework for hierarchical reasoning. Next, we introduce hierarchical reasoning chains of multidefendant in detail. Finally, a training process with Fusion-in-Decoder for HRN is explained."
        },
        {
            "heading": "4.1 Problem formulation",
            "text": "We first formulate the multi-defendant LJP task. The fact description of a multi-defendant case can be seen as a word sequence x = {w1, w2, ..., wn}, where n represents the number of words. Each multi-defendant case has a set of defendant names E = {e1, e2, ..., e|E|}, where each name is a sequence of words e = {w1, w2, .., w|e|}. Given the fact description x and the defendant name e of a multi-defendant case, the multi-defendant task aims to predict the judgment results of multiple applicable law articles, multiple charges, and a term of penalty. The law article prediction and the charge prediction subtasks are multi-label classification problems, and the term of penalty prediction subtask is a multi-class classification problem.\nWe introduce the criminal relationship and the sentencing circumstance as intermediate tasks to model the hierarchical reasoning chains for multidefendant LJP and improve the prediction of the main judgment results. Criminal relationships refer to the relationships between defendants, specifically whether one defendant assisted other codefendants during the commission of the crime. Sentencing circumstances refer to specific behaviors (such as confession and recidivist) or factors (like accessory and blind) that can influence the severity or leniency of a penalty3. These two tasks are also multi-label classification problems. We denote the labels of law articles, charges, terms of penalty, criminal relationships, and sentencing circumstances as word sequences yl,yc,yt,yr and ys respectively in this paper."
        },
        {
            "heading": "4.2 Sequence-to-sequence generation",
            "text": "From the perspective of Sequence-to-Sequence (Seq2Seq) generation, each task can be modeled as finding an optimal label sequence y that maximizes the conditional probability based on the fact description, a specific defendant name and a specific task description p(y|x, e,d), which is calculated\n3Details of criminal relationships and sentencing circumstances definitions together with their explanations can be found in Appendix A and B.\nas follows:\np(y|x, e,d) = m\u220f i=1 p(yi|y1, y2, ..., ym\u22121,x, e,d),\n(1) where m denotes the length of the label sequence, and the specific task description d is a semantic prompt that allows Seq2Seq generation models to execute the desired task. To accomplish the Seq2Seq generation tasks, we apply the Seq2Seq language model mT5 (Xue et al., 2021) to generate label sequences as follows:\ny\u0302 = DEC(ENC(x, e,d)), (2)\nwhere ENC refers to the encoder of the language model, DEC denotes the decoder of the language model, and y\u0302 is prediction results composed of words. We use special [SEP] tokens to separate the different information to form the input of the encoder."
        },
        {
            "heading": "4.3 Hierarchical reasoning chains",
            "text": "To distinguish different judgment results among various defendants, our method HRN follows hierarchical reasoning chains to determine each defendant\u2019s criminal relationships, sentencing circumstances, law articles, charges, and terms of penalty.\nAs shown in Figure 2, the hierarchical reasoning chains consist of two levels:\nThe first-level reasoning is for intermediate tasks. The first-level reasoning chain is to first identify relationships between defendants based on the fact description, the names of all defendants and the criminal relationship prediction task description dr as follows:\ny\u0302r = DEC(ENC(x, E,dr)). (3)\nThen, we determine sentencing circumstances for the defendant e based on the fact description, name of the defendant e, prediction results of criminal relationships and the sentencing circumstance prediction task description ds, that is:\ny\u0302s = DEC(ENC(x, e, y\u0302r,ds). (4)\nThe second-level reasoning is for judgment prediction tasks. The second-level reasoning chain consists of a forward prediction process and a backward verification process. The forward prediction process is to predict law articles, charges, and terms of penalty (in that order) based on the fact description, the name of defendant e, first-level reasoning results, and the forward prediction task description dlct as follows:\ny\u0302lct = DEC(ENC(x, e, y\u0302r, y\u0302s,dlct)). (5)\nThen, the backward verification process is to verify these judgment results in reverse order based on the fact description, the name of defendant e, first-level reasoning results and the backward verification task description dtcl, that is:\ny\u0302tcl = DEC(ENC(x, e, y\u0302r, y\u0302s,dtcl)). (6)"
        },
        {
            "heading": "4.4 Training with fusion-in-decoder",
            "text": "To handle multi-defendant fact descriptions whose average length exceeds the length limit of the encoder, we adopt Fusion-in-Decoder (FID) (Izacard and Grave, 2021) to encode multiple paragraphs split from a fact description. We first split the fact description x into K paragraphs containing M words. Then, we combine multiple paragraph representations from the encoder, the decoder generates prediction results by attending to multiple paragraph representations as follows:\ny\u0302 = DEC(h1,h2, ...,hK), (7)\nwhere hi denotes the representation of the i-th paragraph of the fact description x. Since all tasks are formulated as sequence-to-sequence generation tasks, we follow Raffel et al. (2020) to train the model by standard maximum likelihood and calculate the cross-entropy loss for each task. The overall loss function is formally computed as:\nL = \u03bbrLr + \u03bbsLs + \u03bblctLlct + \u03bbtclLtcl, (8)\nwhere hyperparameters \u03bb determine the trade-off between all subtask losses. Lr, Ls, Llct and Ltcl denote the cross-entropy losses of the criminal relationship prediction task, the sentencing circumstance prediction task, the forward prediction process and the backward verification process, respectively. At test time, we apply greedy decoding to generate forward and backward prediction results. Finally, the chain with the highest confidence is chosen for the final prediction."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Research questions",
            "text": "We aim to answer the following research questions with our experiments: (RQ1) How does our proposed method, HRN, perform on multi-defendant LJP cases? (RQ2) How do the different levels of reasoning chains affect the performances of HRN on multi-defendant LJP?"
        },
        {
            "heading": "5.2 Baselines",
            "text": "To verify the effectiveness of our method HRN on multi-defendant LJP, we compare it with a variety of methods, which can be summarized in the following three groups:\n\u2022 Single-defendant LJP methods, including Topjudge (Zhong et al., 2018), which is a topological dependency learning framework for singledefendant LJP and formalizes the explicit dependencies over subtasks as a directed acyclic graph; MPBFN (Yang et al., 2019), which is a singledefendant LJP method and utilizes forward and backward dependencies among multiple LJP subtasks; LADAN (Xu et al., 2020), which is a graph neural network based method that automatically captures subtle differences among confusing law articles; NeurJudge (Yue et al., 2021), which utilizes the results of intermediate subtasks to separate the fact statement into different circumstances and exploits them to make the predictions of other subtasks. \u2022 Pre-trained language models, including BERT (Cui et al., 2021), which is a Transformerbased method that is pre-trained on Chinese Wikipedia documents; mT5 (Xue et al., 2021), which is a multilingual model pre-trained by converting several language tasks into \u201ctext-totext\u201d tasks and pre-trained on Chinese datasets; Lawformer (Xiao et al., 2021b), which is a Transformer-based method that is pre-trained on large-scale Chinese legal long case documents. \u2022 Multi-defendant charge prediction method, including MAMD (Pan et al., 2019), which is a multi-defendant charge prediction method that leverages multi-scale attention to recognize fact descriptions for different defendants.\nWe adapt single-defendant LJP methods to multidefendant LJP by concatenating a defendant\u2019s name and a fact description as input and training models to predict judgment results. However, we exclude some state-of-the-art single-defendant approaches unsuitable for multi-defendant settings. Few-shot (Hu et al., 2018), EPM (Feng et al., 2022b), and CEEN (Lyu et al., 2022) annotate extra attributes for single-defendant datasets, not easily transferable to MultiLJP. Also, CTM (Liu et al., 2022) and CL4LJP (Zhang et al., 2023a) design specific sampling strategies for contrastive learning of single-defendant cases, hard to generalize to multi-defendant cases."
        },
        {
            "heading": "5.3 Implementation details",
            "text": "To accommodate the length of multi-defendant fact descriptions, we set the maximum fact length to 2304. Due to the constraints of the model input, BERT\u2019s input length is limited to 512. For training, we employed the AdamW (Loshchilov and Hutter, 2019) optimizer and used a linear learning rate schedule with warmup. The warmup ratio was set to 0.01, and the maximum learning rate was set to 1 \u00b7 10\u22123. We set the batch size as 128 and adopt the gradient accumulation strategy. All models are trained for a maximum of 20 epochs. The model that performs best on the validation set is selected. For the hyperparameters, \u03bb, in the loss function, the best setting is {0.6, 0.8, 1,4, 1.2} for {\u03bbr, \u03bbs, \u03bblct, \u03bbtcl}. Additionally, we set the number of paragraphs K, the number of words per paragraph M , and the output length to 3, 768, and 64, respectively. For performance evaluation, we employ four metrics: accuracy (Acc.), Macro-Precision (MP), Macro-Recall (MR), and Macro-F1 (F1). All experiments are conducted on one RTX3090."
        },
        {
            "heading": "6 Experimental results and analysis",
            "text": "In this section, we first conduct multi-defendant legal judgment predictions and ablation studies to answer the research questions listed in the Section. 5.1. In addition, we also conducted a case study to intuitively evaluate the importance of hierarchical reasoning."
        },
        {
            "heading": "6.1 Multi-defendant judgment results (RQ1)",
            "text": "Table 2 shows the evaluation results on the multidefendant LJP subtasks. Generally, HRN achieves the best performance in terms of all metrics for all multi-defendant LJP subtasks. Based on the results, we have three main observations:\n\u2022 Compared with state-of-art single-defendant LJP methods, e.g., Topjudge, MPBFN, LADAN, and NeurJudge, our method HRN consider hierarchical reasoning chains and thus achieve significant improvements. Since the single-defendant methods do not consider criminal relationships and sentencing circumstances, they can not distinguish different judgment results among various defendants well. It indicates the importance of following the hierarchical reasoning chains to predict criminal relationships and sentencing circumstances for multi-defendant LJP. \u2022 As Table 2 shows, our method HRN achieves considerable performance improvements on all subtasks of multi-defendant LJP compared to pre-trained models BERT and Lawformer. This shows that modeling the hierarchical reasoning chains during the fine-tuning stage is crucial. Moreover, HRN, which combines mT5 and the hierarchical reasoning chains, significantly outperforms mT5, indicating that the language knowledge from pre-training and the information of the hierarchical reasoning chains are complementary to each other. \u2022 Compared to MAMD designed for multidefendant charge prediction, our method HRN outperforms MAMD on charge prediction task. This shows the effectiveness and robustness of modeling reasoning chains in real-world application scenarios. \u2022 We employ gold annotations of criminal relationships and sentencing circumstances, rather than predicted ones, to enhance the performance of HRN. Our findings indicate a considerable improvement in results when relying on gold annotations. This observation underscores the potential for notable enhancement in multi-defendant\nLJP by improving the first-level reasoning."
        },
        {
            "heading": "6.2 Ablation studies (RQ2)",
            "text": "To analyze the effect of the different levels of reasoning chains in HRN, we conduct an ablation study. Table 3 shows the results on MultiLJP with five settings: (i) w/o CR: HRN without predicting criminal relationships. (ii) w/o SC: HRN without predicting sentencing circumstances. (iii) w/o FP: HRN without the forward prediction process and predicts law articles, charges, and terms of penalty in reverse order. (iv) w/o BV: HRN without the backward verification process and predicts law articles, charges, and terms of penalty in order. (v) w/o all: HRN degrades to mT5 with multi-task settings by removing all reasoning chains.\nTable 3 shows that all levels of reasoning chains help HRN as removing any of them decreases per-\nformance:\n\u2022 Removing the first-level reasoning chain. We observe that both criminal relationships and sentencing circumstances decrease the performance of HRN when we remove the first-level reasoning chains. Specifically, removing criminal relationships (CR) negatively impacts performance, especially on law articles and charges prediction, which means criminal relationships are helpful for distinguishing law articles and charges; removing sentencing circumstances (SC) negatively impacts performance, especially on terms of penalty, which means sentencing circumstances are helpful for distinguishing terms of penalty.\n\u2022 Removing the second-level reasoning chain. We observe that the model without the secondlevel forward prediction process (FP) or the second-level backward verification process (BV) faces a huge performance degradation in multidefendant LJP. As a result, although the model still performs the first-level reasoning, the absence of modeling forward or backward dependencies between LJP subtasks leads to poor LJP performances.\n\u2022 Removing all reasoning chains. When removing all reasoning chains from HRN, there is a substantial drop in the performances of multidefendant LJP. Experimental results prove that hierarchical reasoning chains can be critical for multi-defendant LJP."
        },
        {
            "heading": "6.3 Case study",
            "text": "We also conduct a case study to show how multidefendant reasoning chains help the model distinguish judgment results of different defendants and make correct predictions. Figure 3 shows prediction results for two defendants, where red and green represent incorrect and correct predictions, respectively. Defendant B helped A beat the vic-\ntim, but the fact description does not show a direct crime against the victim. Without determining criminal relationships and sentencing circumstances, MAMD focuses on the tailing behavior around A and misclassifies A\u2019s law articles, charges, and terms of penalty as article 264, theft, and 11 months. In contrast, by following reasoning chains to determine criminal relationships, sentencing circumstances, law articles, charges, and terms of penalty, HRN distinguishes different judgment results between defendants."
        },
        {
            "heading": "7 Conclusions",
            "text": "In this paper, we studied the legal judgment prediction problem for multi-defendant cases. We proposed the task of multi-defendant LJP to promote LJP systems from single-defendant to multidefendant. To distinguish confusing judgment results of different defendants, we proposed a Hierarchical Reasoning Network (HRN) to determine criminal relationships, sentencing circumstances, law articles, charges and terms of penalty for each defendant. As there is no benchmark dataset for multi-defendant LJP, we have collected a realworld dataset MultiLJP. We conducted extensive experiments on the MultiLJP dataset. Experimental results have verified the effectiveness of our proposed method and HRN outperforms all baselines.\nLimitations\nAlthough our work distinguishes the judgment results of multiple defendants by hierarchical reasoning, in real life, there exist many confusing charge pairs, such as (the crime of intentional injury, and the crime of intentional homicide). The fact descriptions of these confusing charge pairs are very similar, which makes it difficult for the multi-defendant LJP model to distinguish between confusing charge pairs. We leave this challenge for future work.\nEthics Statement\nSince multi-defendant legal judgment prediction is an emerging but sensitive technology, we would like to discuss ethical concerns of our work. Our proposed method HRN is a preliminary multidefendant work and aims to assist legal professionals instead of replacing them. In addition, multidefendant cases contain personal privacy information. To avoid potential risks of structured social biases (Pitoura et al., 2017; Lyu et al., 2023) and\nprotect personal privacy, we have anonymized sensitive information (e.g., name, location, etc.) for multi-defendant cases in MultiLJP dataset."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported by the National Key R&D Program of China with grant No.2022YFC3303004, the Natural Science Foundation of China (62272274, 61902219, 61972234, 62102234, 62202271, 62072279, T2293773, 72371145), the Natural Science Foundation of Shandong Province (ZR2021QF129), the Key Scientific and Technological Innovation Program of Shandong Province (2019JZZY010129). All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors."
        },
        {
            "heading": "A Criminal Relationship",
            "text": "Criminal relationships refer to the relationships between defendants, specifically whether one defendant assisted other co-defendants during the commission of the crime.\n\u2022 None. One defendant did not help other defendants to conduct criminal behaviors.\n\u2022 Assistance. One defendant helped other defendants to conduct criminal behaviors."
        },
        {
            "heading": "B Sentencing Circumstance",
            "text": "Sentencing circumstances refer to specific behaviors (such as confession and recidivist) or factors (like accessory and blind) that can influence the severity or leniency of a penalty.\n\u2022 Old People. A person who has reached the age of 75 and conducts the crime is liable to a lighter punishment.\n\u2022 Deaf-mute or Blind. A deaf-mute or blind person who conducts a crime is liable to a lighter punishment.\n\u2022 Accessory. Accessory to the crime is liable to a lighter punishment.\n\u2022 Attempted Crime. A person who has attempted to conduct a crime is liable to a lighter punishment.\n\u2022 Surrender. A person who has surrendered to justice is liable to a lighter punishment.\n\u2022 Confession. A person who has confessed to offence is liable to a lighter punishment.\n\u2022 Metitorious Serviceoffset. A person who has metitorious serviceoffset is liable to a lighter punishment.\n\u2022 Recidivist. Recidivist is liable to a heavier punishment."
        }
    ],
    "title": "Multi-Defendant Legal Judgment Prediction via Hierarchical Reasoning",
    "year": 2023
}