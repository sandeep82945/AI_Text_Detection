{
    "abstractText": "Learned representations at the level of characters, sub-words, words and sentences, have each contributed to advances in understanding different NLP tasks and linguistic phenomena. However, learning textual embeddings is costly as they are tokenization specific and require different models to be trained for each level of abstraction. We introduce a novel language representation model which can learn to compress to different levels of abstraction at different layers of the same model. We apply Nonparametric Variational Information Bottleneck (NVIB) to stacked Transformer self-attention layers in the encoder, which encourages an informationtheoretic compression of the representations through the model. We find that the layers within the model correspond to increasing levels of abstraction and that their representations are more linguistically informed. Finally, we show that NVIB compression results in a model which is more robust to adversarial perturbations.",
    "authors": [
        {
            "affiliations": [],
            "name": "Melika Behjati"
        },
        {
            "affiliations": [],
            "name": "Fabio Fehr"
        },
        {
            "affiliations": [],
            "name": "James Henderson"
        }
    ],
    "id": "SP:0c1511434e205771f2600c46aa55404c8c1affc7",
    "references": [
        {
            "authors": [
                "Rami Al-Rfou",
                "Dokook Choe",
                "Noah Constant",
                "Mandy Guo",
                "Llion Jones."
            ],
            "title": "Character-level language modeling with deeper self-attention",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 3159\u20133166.",
            "year": 2019
        },
        {
            "authors": [
                "Alexander A. Alemi",
                "Ian Fischer",
                "Joshua V. Dillon",
                "Kevin Murphy."
            ],
            "title": "Deep variational information bottleneck",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "Duygu Ataman",
                "Wilker Aziz",
                "Alexandra Birch."
            ],
            "title": "A latent morphology model for openvocabulary neural machine translation",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2020
        },
        {
            "authors": [
                "Melika Behjati",
                "James Henderson."
            ],
            "title": "Inducing meaningful units from character sequences with dynamic capacity slot attention",
            "venue": "Transactions on Machine Learning Research.",
            "year": 2023
        },
        {
            "authors": [
                "Yonatan Belinkov",
                "Yonatan Bisk."
            ],
            "title": "Synthetic and natural noise both break neural machine translation",
            "venue": "arXiv preprint arXiv:1711.02173.",
            "year": 2017
        },
        {
            "authors": [
                "Dokook Choe",
                "Rami Al-Rfou",
                "Mandy Guo",
                "Heeyoung Lee",
                "Noah Constant."
            ],
            "title": "Bridging the gap for tokenizer-free language models",
            "venue": "arXiv preprint arXiv:1908.10322.",
            "year": 2019
        },
        {
            "authors": [
                "Junyoung Chung",
                "Sungjin Ahn",
                "Yoshua Bengio."
            ],
            "title": "Hierarchical multiscale recurrent neural networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan H. Clark",
                "Dan Garrette",
                "Iulia Turc",
                "John Wieting."
            ],
            "title": "Canine: Pre-training an efficient tokenization-free encoder for language representation",
            "venue": "Transactions of the Association for Computational Linguistics, 10:73\u201391.",
            "year": 2022
        },
        {
            "authors": [
                "Alexis Conneau",
                "Douwe Kiela."
            ],
            "title": "SentEval: An evaluation toolkit for universal sentence representations",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language",
            "year": 2018
        },
        {
            "authors": [
                "Alexis Conneau",
                "German Kruszewski",
                "Guillaume Lample",
                "Lo\u00efc Barrault",
                "Marco Baroni."
            ],
            "title": "What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties",
            "venue": "Proceedings of the 56th Annual Meeting of the As-",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Nadir Durrani",
                "Fahim Dalvi",
                "Hassan Sajjad",
                "Yonatan Belinkov",
                "Preslav Nakov."
            ],
            "title": "One size does not fit all: Comparing nmt representations of different granularities",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Michael Figurnov",
                "Shakir Mohamed",
                "Andriy Mnih."
            ],
            "title": "Implicit reparameterization gradients",
            "venue": "Neural Information Processing Systems.",
            "year": 2018
        },
        {
            "authors": [
                "James Henderson",
                "Fabio James Fehr."
            ],
            "title": "A VAE for transformers with nonparametric variational information bottleneck",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "John Hewitt",
                "Kawin Ethayarajh",
                "Percy Liang",
                "Christopher Manning."
            ],
            "title": "Conditional probing: measuring usable information beyond a baseline",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Irina Higgins",
                "Loic Matthey",
                "Arka Pal",
                "Christopher Burgess",
                "Xavier Glorot",
                "Matthew Botvinick",
                "Shakir Mohamed",
                "Alexander Lerchner."
            ],
            "title": "beta-VAE: Learning basic visual concepts with a constrained variational framework",
            "venue": "International Conference",
            "year": 2017
        },
        {
            "authors": [
                "Valentin Hofmann",
                "Hinrich Schuetze",
                "Janet Pierrehumbert."
            ],
            "title": "An embarrassingly simple method to mitigate undesirable properties of pretrained language model tokenizers",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Kazuya Kawakami",
                "Chris Dyer",
                "Phil Blunsom."
            ],
            "title": "Learning to create and reuse words in openvocabulary neural language modeling",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Max Welling."
            ],
            "title": "Autoencoding variational bayes",
            "venue": "2nd International Conference on Learning Representations, ICLR 2014, Conference Track Proceedings, Banff, AB, Canada.",
            "year": 2014
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson."
            ],
            "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2018
        },
        {
            "authors": [
                "Harold W Kuhn."
            ],
            "title": "The hungarian method for the assignment problem",
            "venue": "Naval research logistics quarterly, 2(1-2):83\u201397.",
            "year": 1955
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Liyuan Liu",
                "Haoming Jiang",
                "Pengcheng He",
                "Weizhu Chen",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Jiawei Han."
            ],
            "title": "On the variance of the adaptive learning rate and beyond",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Francesco Locatello",
                "Dirk Weissenborn",
                "Thomas Unterthiner",
                "Aravindh Mahendran",
                "Georg Heigold",
                "Jakob Uszkoreit",
                "Alexey Dosovitskiy",
                "Thomas Kipf."
            ],
            "title": "Object-centric learning with slot attention",
            "venue": "Advances in Neural Information Processing",
            "year": 2020
        },
        {
            "authors": [
                "Stephen Merity",
                "Caiming Xiong",
                "James Bradbury",
                "Richard Socher."
            ],
            "title": "Pointer sentinel mixture models",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Conference Track Proceedings, Toulon, France. OpenReview.net.",
            "year": 2017
        },
        {
            "authors": [
                "John Morris",
                "Eli Lifland",
                "Jin Yong Yoo",
                "Jake Grigsby",
                "Di Jin",
                "Yanjun Qi."
            ],
            "title": "Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natu-",
            "year": 2020
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "GloVe: Global vectors for word representation",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar.",
            "year": 2014
        },
        {
            "authors": [
                "Matthew E. Peters",
                "Mark Neumann",
                "Mohit Iyyer",
                "Matt Gardner",
                "Christopher Clark",
                "Kenton Lee",
                "Luke Zettlemoyer."
            ],
            "title": "Deep contextualized word representations",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Tiago Pimentel",
                "Josef Valvoda",
                "Niklas Stoehr",
                "Ryan Cotterell."
            ],
            "title": "Attentional probe: Estimating a module\u2019s functional potential",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11459\u201311472, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Mike Schuster",
                "Kaisuke Nakajima."
            ],
            "title": "Japanese and korean voice search",
            "venue": "2012 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5149\u20135152. IEEE.",
            "year": 2012
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725,",
            "year": 2016
        },
        {
            "authors": [
                "Yi Tay",
                "Vinh Q. Tran",
                "Sebastian Ruder",
                "Jai Gupta",
                "Hyung Won Chung",
                "Dara Bahri",
                "Zhen Qin",
                "Simon Baumgartner",
                "Cong Yu",
                "Donald Metzler."
            ],
            "title": "Charformer: Fast character transformers via gradientbased subword tokenization",
            "venue": "International Con-",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Linting Xue",
                "Aditya Barua",
                "Noah Constant",
                "Rami AlRfou",
                "Sharan Narang",
                "Mihir Kale",
                "Adam Roberts",
                "Colin Raffel."
            ],
            "title": "ByT5: Towards a token-free future with pre-trained byte-to-byte models",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Hofmann"
            ],
            "title": "mizer with a learning rate",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Learning representations of language using selfsupervision has become a cornerstone of NLP (Pennington et al., 2014; Peters et al., 2018; Devlin et al., 2019, inter alia). However, these representations are specific to their tokenisation (e.g. BytePair (Sennrich et al., 2016), WordPiece (Schuster and Nakajima, 2012), SentencePiece (Kudo and Richardson, 2018), characters (Al-Rfou et al., 2019), and even bytes (Xue et al., 2022)), which restricts the level of abstraction from the input text which their representations are able to convey. Work like CANINE (Clark et al., 2022) and Charformer (Tay et al., 2022) avoid problems with tokenisation by modeling individual characters or bytes, and thereafter use a stride-based downsampling to reduce the representation length. The\n\u2020Equal contribution.\nstride pattern is fixed and thus can\u2019t be considered as learning to abstract. Behjati and Henderson (2023) recently introduced the task of learning a higher level of abstraction in a set-of-vector space by proposing Dynamic Capacity Slot Attention. In this work, we propose a novel character-level model of representation learning which learns different levels of abstraction in different layers of the same model.\nContributions We adapt the Nonparametric Variational Information Bottleneck regulariser (NVIB) (Henderson and Fehr, 2023) for application to selfattention in the stacked layers of a Transformer encoder.1 The resulting model has greater abstraction than a standard Transformer due to selectively dropping some vectors in higher attention layers. Interestingly, we observe that the learned abstract units\n1The code is publically available at: https://github.com/idiap/nvib https://github.com/idiap/nvib_selfattention\nare intuitive, often corresponding to words. By employing different analysis methods, we demonstrate that our model is better at encoding semantically and linguistically meaningful information than a standard Transformer baseline. Moreover, it exhibits an enhanced level of robustness, further consolidating its advantage."
        },
        {
            "heading": "2 The Model",
            "text": "Our model consists of standard Transformer encoder-decoder layers (Vaswani et al., 2017), where the encoder block has been augmented with an NVIB regulariser on the self-attention layers, as seen in Figure 1."
        },
        {
            "heading": "2.1 NVIB for Self-Attention",
            "text": "Nonparametric Variational Information Bottleneck is an information-theoretic regulariser for attentionbased latent representations (Henderson and Fehr, 2023). It has been shown to induce smooth and sparse latent representations in the cross-attention layer of a Transformer encoder-decoder, where Henderson and Fehr (2023) used it to define a Variational Auto-Encoder (VAE) (Kingma and Welling, 2014). It generalises attention over a set of vectors to denoising attention over a mixture of impulse distributions, and uses Bayesian nonparametrics to handle the fact that the number of vectors grows with the length of the text. NVIB uses Dirichlet Processes (DPs) to define distributions over these mixture distributions, and controls the information in the latent representation by sampling a mixture distribution from the attention layer\u2019s DP, thereby adding noise which removes information.\nWe extend the previous work by using implicit reparameterisation gradients (Figurnov et al., 2018) to improve learning, and by adapting NVIB for use in the stacked self-attention layers of a Transformer encoder. By extending NVIB\u2019s informationtheoretic regularisation to the series of latent representations inside the Transformer encoder, we see increasingly abstract interpretable representations in the higher layers.\nNVIB layer As with a standard attention layer, an NVIB layer maps a set of n vectors to an attention function. It first maps the n vectors Z \u2208 Rn\u00d7p to the parameters of a DP, which are a total pseudo-count for its Dirichlet distribution and a mixture of Gaussians for its base distribution. Each of the n vectors is individually projected to a pseudo-count \u03b1 \u2208 Rn and a Gaussian component\n(\u00b5 \u2208 Rn\u00d7p,\u03c3 \u2208 Rn\u00d7p) of the base distribution. The model can drop entire vectors by setting their pseudo-counts to zero, thereby making the representation sparse. In addition, there is an n+1th component of the base distribution for the prior, with parameters \u03b1p=1, \u00b5p=0 and \u03c3p=1. The individual pseudo-counts are both summed to get the DP\u2019s total pseudo-count and normalised to weight the components of the DP\u2019s base distribution. The NVIB layer then uses denoising attention to access either a set of weighted vectors sampled from the DP (at training time), or the base distribution of the DP (at testing time).\nHenderson and Fehr (2023) use ReLU, linear and exponential activation functions to compute \u03b1, \u00b5 and \u03c3, respectively. To adapt NVIB for stacked layers of self-attention, our model replaces the activation for the pseudo-count parameters with an exponential activation, and includes a multiplicative skip connection from the previous layer l\u22121, as shown in Figure 1:\n\u03b1(l) = exp(wZT + b+ log(\u03b1(l\u22121))), (1)\nwhere w \u2208 R1\u00d7p and b \u2208 R form the linear projection. The exponential activation allows the model to be more stable in training.2 The skip connection in between layers l\u22121 and l helps coordinate the importance of vectors across layers. Keeping the pseudo-count parameters in log-space prevents overflow and improves precision when the parameters get larger. This results in a multiplicative skip connection which emphasizes the communication between layers.\nTo compute self-attention, the DP parameters projected from all the individual vectors together define a single DP, and we take a single sample from this DP which all the individual vectors access via denoising attention. The queries for this denoising self-attention are computed from the original n vectors Z \u2208 Rn\u00d7p, before the NVIB layer. We also introduce the use of implicit reparameterisation gradients (Figurnov et al., 2018) for error backpropagation through the sampling step. See Appendix D for the exact attention functions.\nTraining objective The NVIB loss regularises the attention-based representations so that the size of the representation at each layer is appropriate for the complexity of the representation being encoded at that layer. It has three terms, a reconstruction\n2Since the exponential function is never exactly zero, we threshold small values to introduce sparsity. See Appendix A.\nloss LR, and two KL divergence terms: LD for the pseudo-counts of the Dirichlet distributions, and LG for the parameters of the Gaussian components. The LR term is the supervised learning objective, which tries to make the latent representation informative enough to predict the original text. The LG term tries to make the individual Gaussian components less informative, as in vector-space VAEs (Kingma and Welling, 2014). The LD term tries to push down the total pseudo-count, which pushes some of the individual pseudo-counts to zero, thereby effectively dropping their vectors and reducing the number of vectors in the latent representation. See Appendix C for loss equations.\nTo apply NVIB to stacked self-attention layers, we want to allow the lower layers to compute with more vectors while encouraging the upper layers to compress to fewer vectors, thereby encouraging abstraction at the higher layers. We therefore weight the loss terms differently at each layer:\nL = LR + \u03b2(l)(\u03bbDLD + \u03bbGLG) (2)\n\u03b2(l) = l\u2211N l=0 l\nfor l \u2208 {1, ..., N} (3)\nwhere \u03b2(l) controls the degree of NVIB regularisation for layer l, linearly increasing it for higher layers. If a vector is dropped in the last self-attention layer (i.e. zero pseudo-count), then we also drop that vector in the cross-attention layer to the decoder, but otherwise there is no NVIB regularisation of the cross-attention.\nDuring preliminary experiments, instead of the above formula for \u03b2(l) we considered a uniform weight, as well as a doubling weight, per layer. These regularisation weights were either too weak or too strong, respectively. The values we considered for the hyperparameter \u03bbD are given in Appendix B. When we increase this regularisation, the characters are grouped into fewer and fewer vectors until all characters are compressed into a single vector, much like a sentence embedding. If we over-regularise, the representations collapse to the uninformative prior representation."
        },
        {
            "heading": "3 Related Work",
            "text": "Modeling language at the level of characters has the advantage of providing an end-to-end framework for the models to operate, without the need for tokenization as a preprocessing step (Xue et al., 2022; Ataman et al., 2020; Choe et al., 2019; AlRfou et al., 2019; Kawakami et al., 2017). This\nis at the cost of longer sequence lengths and the need for greater model depth to reach the understanding level of subword-based models. While CANINE (Clark et al., 2022) and Charformer (Tay et al., 2022) are some attempts to bypass these shortcomings, they do so by fixed architectural design choices. Our work differs in that it allows the model to learn how to abstract and compress the input without a hard-coded abstraction structure. Our inspiration comes from Behjati and Henderson (2023) who introduced the task of learning a higher level of abstraction and proposed a method based on Slot Attention (Locatello et al., 2020) for this purpose. Our work is also related to HMRNNs (Chung et al., 2017) as it tends to learn a hierarchy of units within its layers, though it does not make discrete decisions on unit boundaries. Our approach to learning meaningful disentangled abstractions by encouraging the models to learn compressed representations through a bottleneck is shared with VAEs (Kingma and Welling, 2014) and other work in that line (Alemi et al., 2017; Higgins et al., 2017)."
        },
        {
            "heading": "4 Experiments",
            "text": "Our proposed model\u2019s abstractness is analyzed qualitatively through attention visualisations (Section 4.2) and quantitatively through a challenging sub-topic classification task (Section 4.3.1). Each layer is probed to analyse the linguistic information captured (Section 4.3) and finally we examine the models\u2019 robustness to adversarial, synthetic noise (Section 4.4). We provide additional details of these experiments in the Appendices F to I."
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Data We train all models on the Wikitext-2 (Merity et al., 2017) encyclopedia dataset at the character level, with a noisy character deletion reconstruction objective (Lewis et al., 2020).\nModels We compare the self-attention representations from a standard Transformer encoder layer and our Transformer encoder layer with NVIB regularisation. We consider models consisting of six stacked Transformer encoder layers to be in line with the base model from Vaswani et al. (2017). For the Transformer decoder we use only 2 layers so that the decoder is not able to compensate for poor embeddings from the encoder. For simplicity of implementation and interpretation, we use only a single attention head. For the NVIB models,\nwe only apply NVIB to the final three layers. To ensure comparability between our model and the baseline, we train the baseline to have the same denoising capability and thus the same validation cross-entropy when evaluated on noised examples. For further details see Appendices A and B."
        },
        {
            "heading": "4.2 Attention Map Visualisations and Analysis",
            "text": "To qualitatively evaluate the model\u2019s ability to learn interpretable abstractions, we visualise the self-attention maps. Figure 2 compares the selfattention patterns of the the last 3 layers of: a Transformer with 6 layers of standard attention (left); and a Transformer with 3 layers of standard attention followed by 3 layer of denoising attention with NVIB (right).\nDespite being trained solely on noisy reconstruction at the character level, the NVIB layers compress the self-attention representations through the layers into distinct groups. At lower levels, the model uses nearly all vectors (i.e. \u223c 99%) and learns position-local information, shown as a diagonal pattern. At higher levels the model drops some vectors (the blank columns) and groups characters (the vertical bars) in ways which strongly resemble subword units or even words. The last\nlevel retains only an average of \u223c35% of vectors. This is because the stronger NVIB regularisation at higher layers encourages the grouping of correlated characters, to reduce redundant information, and the strongest correlations are within words. We provide further examples in Appendix E.\nWe quantify the resemblance of the final-layer self-attention maps to words by extracting contiguous segments from the maps and computing the F1 measure between our segments and the words in the sequence. In particular, we find the best alignment between words and segments and compute the number of characters in the longest common substring between a word and its corresponding discovered segment.3 Table 1 compares the performance of our model to the Transformer baseline. This impressive unsupervised performance (F1 of 78.86%) concurs with the attention visualisations and quantitatively verifies that our model has learned to abstract to the level of words."
        },
        {
            "heading": "4.3 Probing Analysis",
            "text": "This section uses different probing tasks to quantitatively evaluate the abstraction capabilities of our model and analyse the linguistic information captured by the layers."
        },
        {
            "heading": "4.3.1 ArXiv Topic Classification",
            "text": "The ArXiv topic classification task (Hofmann et al., 2022) is a challenging task consisting of short input sentences with long technical words. For each subject, the classifier should classify the topic into 20 possible sub-areas. Following Behjati and Henderson (2023), we train an attention-based probe on the final layer of the models and report the F1 measure for performance on the ArXiv-L dataset. Without finetuning the models, this classification task serves as probing high-level abstract linguistic properties (Hewitt et al., 2021). As shown in Table 2, the NVIB layer results in the model learning more information about the meaning and semantics in the abstract representations than characters and therefore provides better units for performing the task.\n3See Appendix I for further details and exact formulas."
        },
        {
            "heading": "4.3.2 Linguistic Probing",
            "text": "The SentEval task set is specifically designed to examine the linguistic information available in a sentence representation at different levels, ranging from surface-level to semantic-level tasks (Conneau et al., 2018; Conneau and Kiela, 2018). We probe for linguistic information of our model and the baseline Transformer, across all layers. In general, the performance improves in deeper layers and increases further with the inclusion of NVIB in the layers. We highlight the results of four tasks in Figure 3, which to perform well in these tasks the representations must capture latent syntactic structures (BShift), cluster them by constituent types (TopConst), or have an understanding of semantics (Tense) or broad discourse and pragmatic factors (CoordInv) (Conneau et al., 2018). The inclusion of our NVIB layers increases the relative performance over the Transformer baseline, showing it to be more linguistically informed. The complete set of results is in Appendix Table 4."
        },
        {
            "heading": "4.4 Robustness Analysis",
            "text": "We analyze the robustness of our models to synthetic noise injected into the input sequences (Belinkov and Bisk, 2017; Durrani et al., 2019). Namely, we evaluate the reconstruction quality when the inputs are perturbed by swapping, deleting, inserting, and substituting characters (Morris et al., 2020). We expect our model to be more robust due to its compressed representations. Figure 4 shows that our model is more robust to adversarial\nnoise than a standard Transformer, with increased advantage as the level of noise increases."
        },
        {
            "heading": "5 Conclusions",
            "text": "We propose a novel method for inducing abstract representations of text. We adapt the Nonparametric Variational Information Bottleneck (Henderson and Fehr, 2023) regulariser for application to selfattention in the stacked layers of a Transformer encoder. Our model learns how many vectors are needed at each layer, thereby inducing different levels of abstraction in different layers of the same model. We find that these abstract units are intuitive, more robust, and better at encoding semantically and linguistically meaningful information.\nLimitations\nWhile the models and training data are reasonable in size, the experiments do not include the very large scale training often found in work on representation learning in text. We anticipate that the advantages of NVIB on self-attention layers will only increase as the models and data are scaled up, since this should allow even more abstract representations to be learned. In addition, the experiments are only done on English, but we would expect more improvements with more morphologically rich languages. In future work we plan to explore fine-tuning NVIB for sparsity and downstream performance, and consider different tokenizations beyond characters only.\nEthics Statement\nWe foresee no ethical concerns with our work."
        },
        {
            "heading": "Acknowledgements",
            "text": "Both Melika Behjati and Fabio Fehr were supported by the Swiss National Centre of Competence in Research (NCCR) under the project Evolving Language, grant number \u201c51NF40_180888\u201d."
        },
        {
            "heading": "A Training Details",
            "text": "General Training All models are trained, without pretraining, using the same encoder and decoder configuration for comparability. Our encoder size is defined by the base Transformer (Vaswani et al., 2017) such that we have a six layer Transformer encoder. However, we use a two layer decoder to ensure the task is not learned in the decoder alone. We use a single attention head. The size for the word embedding vectors and model projections are 512 and feed forward dimensions 512, which leads to models of approximately 12-17 million trainable parameters. An English character level tokeniser is used for tokenisation with a vocabulary of approximately 100 characters. During training we use: a learning rate of 1e\u22123 with a cosine cool-down over all steps, RAdam optimiser (Liu et al., 2020) with mixed precision (FP16), a batch size of 512, gradient norm clipping 0.1 and trained for 55 epochs (\u2248 8K steps). The number of steps were selected considering model convergence and minimising computation time. We use a dropout rate of 0.1. The input is noised at each\nbatch with a probability of character deletion of 0.1. Each model takes approximately 2.5hrs on a single NVIDIA GeForce RTX 3090.\nNVIB Training Training the models with the NVIB layers requires regularising the representations. The introduction of the exponential activation function (as opposed to ReLU) for the psuedocount parameter \u03b1 requires a threshold at test time to be exactly zero. We use a threshold for this at 0.1. During training and testing we enforce a bottleneck between the encoder and decoder by masking the final encoder representations by the aforementioned threshold.\nThe NVIB hyperparameters \u03bbG, \u03bbD and \u03b1\u2206 are selected through hyperparameter tuning. However, during training we only sample once from each component thus the approximation parameter is set to \u03ba\u2206 = 1. We use a Kullback-Leibler annealing divergence strategy where the introduction of the KL divergence loss is linearly introduced between 30%\u2212 60% of the training steps. This allows the model to learn initial representations, slowly introduce the regularisation and finally learn through the compressed latent representation."
        },
        {
            "heading": "B Hyperparameter Tuning",
            "text": "The models are trained on the Wikitext-2 training dataset using the loss from Equation 2. They are tuned on the validation dataset with the aim to be able to reconstruct the character sequence from a noisy input. Following from (Henderson and Fehr, 2023) we set the weights of the Gaussian and Dirichlet KL divergences to be independent of the sentence length n and dimensionality of vectors d:\n\u03bbD = 1 n \u03bb\u2032D ; \u03bbG = 1 d 1 n \u03bb\u2032G\nwhere \u03bb\u2032D and \u03bb \u2032 G are fixed hyperparameters. All combinations of the following hyperparameters were considered in a grid search for the respective models:\n\u2022 lr = {1e\u22124, 1e\u22123}\n\u2022 \u03bb\u2032G = {1e\u22125, 1e\u22124, 1e\u22123, 1e\u22122}\n\u2022 \u03bb\u2032D = {1e\u22122, 1e\u22121, 1}\n\u2022 \u03b1\u2206 = {0 , 0.05, ..., 0.45, 0.5}\nwhere \u03bb\u2032G and \u03bb \u2032 D are the weights on the Gaussian and Dirichlet KL divergences. The \u03b1\u2206 represents the conditional prior parameter. The final models\u2019\nhyperparameters are reported in Table 3 where the validation cross-entropy (CE) is matched for NVIB and baseline Transformers.\nThe encoders 6 layers are inspired by the base model of Vaswani et al. (2017). For the Transformer decoder we use only 2 layers such that the decoder is not able to overpower the embeddings from the encoder it sees through cross attention.\nNVIB Configuration For the NVIB layers during experimentation we considered: All layer including NVIB; the last 3 layers including NVIB; and only the final layer including NVIB. When all layers were included it was challenging to get both compression and performance as the regularisation was too strong. Only regularising the last layer managed to reduce the number of vectors but often converged to a single sentence vector with lower, non-comparable validation cross-entropy. Finally, we settled on only regularising the last 3 layers as it gave the model enough flexibility in the lower layers and progressive compression in the higher layers."
        },
        {
            "heading": "C KL Divergence Loss",
            "text": "Henderson and Fehr (2023) define the KullbackLeibler divergence for NVIB with two terms: the LD for the Dirichlet distribution weights defined by \u03b1; and the LG for the distribution of vectors Z generated by the Gaussian components. We set the approximation parameter \u03ba0 = 1. This gives us the following loss terms for the KL divergence, where \u0393 is the gamma function and \u03c8 is the digamma function:\n(4) LD = log \u0393(\u03b1\nq 0)\u2212 log \u0393(\u03b1\np\u2032 0 )\n+ (\u03b1q0 \u2212 \u03b1 p\u2032 0 ) (\u2212\u03c8(\u03b1 q 0) + \u03c8(\u03b1 q 0)) + ( log \u0393(\u03b1p \u2032 0 )\u2212 log \u0393(\u03b1 q 0) )\nwhere, \u03b1q0 is the sum of all \u03b1 parameters generated by the NVIB layer. The conditional prior \u03b1p \u2032\n0 = \u03b1 p 0 + n\u03b1 \u2206 is controlled by \u03b1p0 = 1 and extra pseudo-counts defined by the length n and a hyperparameter \u03b1\u2206. The KL divergence between two Gaussians (with diagonal covariance with values \u03c3 and weighted by the \u03b1 parameters) is:\n(5) LG =\n1\n2 n+1\u2211 i=1 \u03b1qi \u03b1q0 d\u2211 h=1 ( (\u00b5qih \u2212 \u00b5 p h) 2 (\u03c3ph) 2 \u2212 1\n+ (\u03c3qih) 2\n(\u03c3ph) 2 \u2212 log\n(\u03c3qih) 2 (\u03c3ph) 2 )"
        },
        {
            "heading": "D Denoising attention function",
            "text": "Henderson and Fehr (2023) generalise the set of vectors input to an attention function to a probability distribution over vectors, and generalise attention to a function of these probability distributions called denoising attention.\nTraining function During training, the set of sampled vectors Z \u2208 Rn\u00d7p and their sampled log-probability weights log(\u03c0) \u2208 R1\u00d7n are both output by the NVIB layer, thereby specifying the sampled mixture distribution F . A set of query vectors u\u2032 \u2208 Rm\u00d7p is projected into key space by the grouped matrices WQ,WK \u2208 Rp\u00d7d to u = (u\u2032WQ(WK)T ). The keys\u2019 dimensionality d is used for scaling. Denoising attention can then be computed as:\nsoftmax (\n1\u221a d uZT + log(\u03c0)\u2212 1 2 \u221a d \u2225Z\u22252\n) Z\n(6)\nFor self-attention, we define the original query vectors u\u2032 to be the set of vectors input to the NVIB layer, before projecting to DP parameters and sampling.\nTesting function During test time, we do not sample F , but instead use the mean of the posterior distribution. The test-time denoising attention can then be computed as:\n(7)\nsoftmax ( u ( \u00b5\n(\u03c3r)2\n)T + log( \u03b1\n\u03b10 )\n\u2212 ( 1\n2 \u2225\u2225\u2225 \u00b5 \u03c3r \u2225\u2225\u22252)T \u2212 1p (log(\u03c3r))T)\n\u00d7\n( (\u03c3)2\n(\u03c3r)2 \u2299 (1Tnu) +\n\u221a d\n(\u03c3r)2 \u2299 \u00b5\n)\nwhere 1p is a row vector of p ones and (\u03c3r)2 = ( \u221a d+ (\u03c3q)2).\nE Visualisations\nIn Figures 5 to 8 we include additional visualisations of the self-attention weights."
        },
        {
            "heading": "F Probing Classifiers",
            "text": "We used two types of probing classifiers to perform our tasks. First, we employ an attention-based probing classifier to operate on top of the set of representations for predicting the specified property. This would be similar to having a learnable [CLS] token which is used as the representation of the sequence in BERT-like models. This is also in line with the findings of Pimentel et al. (2022) that the way we do the probing should resemble how the model itself would use the information within its architecture. In particular, we first map the representations into a new space with a 2 layer MLP. Then, we compute the attention with a learnable query vector. Finally, we linearly project the resulting vector into the number of classes for each task. We refer to this probe as Attention-based probe. Second, we tried a less complicated and more common type of probing in which we first aggregate\nthe set of representation vectors by mean and then apply a 2 layer MLP with ReLU non-linearity to\nperform the task. We refer to this probe as Aggregating probe."
        },
        {
            "heading": "G SentEval Tasks",
            "text": "G.1 Model\nWe employ the Aggregating probe for performing this task. We froze our models and trained the probes for 10 epochs with a batch-size of 128. The hidden dimension for the probe is set to 256. We trained the model with Adam optimizer with a learning rate of 1e \u2212 4. We report the test set accuracy for the best-performing model in terms of validation accuracy.\nG.2 Supplementary Results\nWe report the results in Table 4 on a subset of 7 of the 10 SentEval tasks as sentence length (SentLen), word content (WC) and semantic odd man out (SOMO) tasks are too challenging for our models when encoding from a character level."
        },
        {
            "heading": "H Arxiv Classification Task",
            "text": "Our goal here is to compare the representations and not have ultimate performance in the task, thus we do not fine-tune the models. Hence, we only\nevaluated our models on the large division of the task, i.e., ArXiv-L which consist of 1000 samples for each sub-area leading to 20000 samples in total. We employ the Attention-based probe to perform this task as it is quite a challenging task which requires the information in the vectors to be better managed by the Attention mechanism and also more similar to the way the model itself would perform the task. The hidden dimension of the MLP is set to 256 and the query, key, and value matrices are set to the same dimension as the model dimension, namely, 512. We train the classifier with a batch size of 256 for 50 epochs with Adam optimizer with a learning rate of 1e \u2212 3. Following Hofmann et al. (2022) we report the test F1 for the best-performing model in terms of validation F1."
        },
        {
            "heading": "I Quantification of Word Resemblance",
            "text": "We observe a strong resemblance between words and the vertical bands in the final-layer Attention maps of the NVIB integrated model. Therefore, we quantify this similarity as follows. First, we take the argmax over the Key dimension of an Attention map and extract the contiguous segments from the resulting vector. Then, we compute the intersection between the set of obtained segments and the set of words in the sequence. In particular, for each segment and word, we compute the number of intersecting characters (i.e., the length of the longest common substring) as a measure of their overlap. This would lead to a rectangular matrix of scores. Then, we perform the Hungarian matching algorithm (Kuhn, 1955) to find the best 1- 1 match between the two sets. Afterward, for each matched word and segment, we compute Precision\n(P ), Recall (R), and F1 measure as\nP = longest common substring length\nsegment length (8)\nand\nR = longest common substring length\nword length . (9)\nWe reported the average macro F1, P , R over the validation set of our training data. For the baseline Transformer, as it usually predicts units of length one or two which are within a single word the P would be high as opposed to its R value."
        }
    ],
    "title": "Learning to Abstract with Nonparametric Variational Information Bottleneck",
    "year": 2023
}