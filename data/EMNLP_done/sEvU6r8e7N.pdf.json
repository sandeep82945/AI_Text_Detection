{
    "abstractText": "Large Language Models (LLMs) have attained the impressive capability to resolve a wide range of NLP tasks by fine-tuning with highquality instruction data. However, collecting human-written data of high quality, especially multi-turn dialogues, is expensive and unattainable for most people. Though previous studies have used powerful LLMs to generate the dialogues automatically, they all suffer from generating untruthful dialogues because of the model hallucination. Therefore, we propose a method called RefGPT to generate enormous truthful and customized dialogues without worrying about factual errors caused by the model hallucination. RefGPT solves the model hallucination in dialogue generation by restricting the LLMs to leverage the given reference instead of reciting their own knowledge to generate dialogues. Additionally, RefGPT adds detailed controls on every utterance to enable high customization capability, which previous studies have ignored. On the basis of RefGPT, we also propose two high-quality dialogue datasets generated by GPT-4, namely RefGPTFact and RefGPT-Code. RefGPT-Fact is a dataset with 100k multi-turn dialogues based on factual knowledge and RefGPT-Code has 76k multi-turn dialogues covering a wide range of coding scenarios. Our code and datasets are released in https://github.com/mutonix/ RefGPT.",
    "authors": [
        {
            "affiliations": [],
            "name": "Dongjie Yang"
        },
        {
            "affiliations": [],
            "name": "Ruifeng Yuan"
        },
        {
            "affiliations": [],
            "name": "Yuantao Fan"
        },
        {
            "affiliations": [],
            "name": "Yifei Yang"
        },
        {
            "affiliations": [],
            "name": "Zili Wang"
        },
        {
            "affiliations": [],
            "name": "Shusen Wang"
        },
        {
            "affiliations": [],
            "name": "Hai Zhao"
        },
        {
            "affiliations": [],
            "name": "\u2020 1Shanghai"
        },
        {
            "affiliations": [],
            "name": "Jiao Tong"
        }
    ],
    "id": "SP:19492db0e48b1fe9659fc2eb1c6470636009af03",
    "references": [
        {
            "authors": [
                "Anthropic."
            ],
            "title": "Introducing claude",
            "venue": "https://www. anthropic.com/index/introducing-claude.",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Zhuyun Dai",
                "Arun Tejasvi Chaganty",
                "Vincent Zhao",
                "Aida Amini",
                "Qazi Mamunur Rashid",
                "Mike Green",
                "Kelvin Guu"
            ],
            "title": "Dialog inpainting: Turning documents into dialogs",
            "year": 2022
        },
        {
            "authors": [
                "Databricks."
            ],
            "title": "Free dolly: Introducing the world\u2019s first truly open instruction-tuned llm",
            "venue": "https:// github.com/databrickslabs/dolly.",
            "year": 2023
        },
        {
            "authors": [
                "Ning Ding",
                "Yulin Chen",
                "Bokai Xu",
                "Shengding Hu",
                "Yujia Qin",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Bowen Zhou."
            ],
            "title": "Ultrachat: A large-scale auto-generated multi-round dialogue data",
            "venue": "https://github.com/ thunlp/ultrachat.",
            "year": 2023
        },
        {
            "authors": [
                "Steven Tey Dom Eccleston."
            ],
            "title": "Share your wildest chatgpt conversations with one click",
            "venue": "https:// github.com/domeccleston/sharegpt.",
            "year": 2023
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Computing Surveys, 55(12):1\u201338.",
            "year": 2023
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Huu Nguyen",
                "Alexander Mattick"
            ],
            "title": "Openassistant conversations \u2013 democratizing large language model alignment",
            "year": 2023
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu"
            ],
            "title": "G-eval: Nlg evaluation using gpt-4 with better human alignment",
            "year": 2023
        },
        {
            "authors": [
                "Ji Ma",
                "Ivan Korotkov",
                "Yinfei Yang",
                "Keith Hall",
                "Ryan McDonald."
            ],
            "title": "Zero-shot neural passage retrieval via domain-targeted synthetic question generation",
            "venue": "arXiv preprint arXiv:2004.14503.",
            "year": 2020
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Introducing chatgpt",
            "venue": "https://openai. com/blog/chatgpt.",
            "year": 2022
        },
        {
            "authors": [
                "Luke Miller",
                "Maddie Simens",
                "Amanda Askell",
                "Peter Welinder",
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Baolin Peng",
                "Chunyuan Li",
                "Pengcheng He",
                "Michel Galley",
                "Jianfeng Gao"
            ],
            "title": "Instruction tuning with gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560.",
            "year": 2022
        },
        {
            "authors": [
                "Can Xu",
                "Qingfeng Sun",
                "Kai Zheng",
                "Xiubo Geng",
                "Pu Zhao",
                "Jiazhan Feng",
                "Chongyang Tao",
                "Daxin Jiang"
            ],
            "title": "2023a. Wizardlm: Empowering large language models to follow complex instructions",
            "year": 2023
        },
        {
            "authors": [
                "Canwen Xu",
                "Daya Guo",
                "Nan Duan",
                "Julian McAuley."
            ],
            "title": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data",
            "venue": "arXiv preprint arXiv:2304.01196.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "General chat models (OpenAI, 2022, 2023; Anthropic, 2023) based on Large Language Models (LLMs) have shown the impressive capability\n\u2217Dongjie Yang, Yifei Yang and Hai Zhao are with the Department of Computer Science and Engineering, Shanghai Jiao Tong University, and also with Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University.\n\u2020 Corresponding author; This paper was partially supported by Joint Research Project of Yangtze River Delta Science and Technology Innovation Community (No. 2022CSJGG1400).\nto intention recognition and complete a variety of NLP tasks only via fine-tuning with a small amount of high-quality instruction data (Taori et al., 2023; Chiang et al., 2023; Xu et al., 2023a). However, such high-quality instruction datasets, especially multi-turn dialogues with instructions in vertical domains, requires enormous crowdsource workers with extensive professional knowledge to collect (Ouyang et al., 2022), where the cost is unaffordable for most people.\nPrevious studies (Peng et al., 2023; Xu et al., 2023b; Ding et al., 2023) have shown the effectiveness of prompting LLMs like GPT-3 (Brown et al., 2020) to generate enormous instructions (singleturn dialogues) or multi-turn dialogues with given human-written instructions or conversation topics as seeds. However, such one-shot or few-shot methods have a common deficiency that they have the risk of generating untruthful and misleading content due to the language model hallucination (OpenAI, 2023; Ji et al., 2023). The reason why the issue of untruthfulness happens is obvious. This is because the quantity of information in seed prompts like human-written instructions or topics is not enough for being converted to the dialogue on a new topic so LLMs have to recite their own knowledge to complete such a new dialogue which may lead to the model hallucination of generating untruthful facts.\nTherefore, we introduce RefGPT, a method for generating truthful and customized multi-turn dialogues utilizing the ability of powerful LLMs like GPT-3.5/GPT-4. RefGPT first provides a plain text or a document as the reference and guides the LLMs to leverage the references to generate dialogues. By providing enough information on a new topic as context, LLMs will be prompted not to rely on their own knowledge to generate the dialogues, thus resolving the hallucination issue.\nAfter ensuring the authenticity of the dialogue, we further develop an effective prompting process\nfor RefGPT to guide the LLMs to generate highly controllable dialogues in a specified uniform format which is easy for training. Previous studies (Xu et al., 2023b; Wang et al., 2022) for automatically generating dialogues have very little control over the generated dialogues. For comparison, RefGPT enables LLMs to generate customized multi-turn dialogues with detailed controls on the structure, style, and content, which further gives diversity to the generated dialogues.\nBased on the RefGPT, we also propose two new multi-turn dialogue datasets, namely RefGPT-Fact and RefGPT-Code. Both datasets have English and Chinese versions. RefGPT-Fact and RefGPTCode consist of 100k and 76k high-quality multiturn dialogues generated from GPT-4 separately, using the online encyclopedia websites and Github repositories as the references. As long as the content on the online encyclopedia website and Github codes is truthful and reliable, the authenticity of the generated dialogues can be maximally ensured.\nBesides the topics in RefGPT-Fact and RefGPTCode, RefGPT has the potential to generate truthful dialogues on any topics or vertical domains if we give it relevant references. RefGPT enables such people working in a specific domain, e.g., the nuclear industry, to have a high-quality multi-turn dialogues dataset to train a chatbot specializing in such domain using their own knowledge base as the reference.\nTo sum up, our contributions are stated as follows:\n\u2022 We propose RefGPT, a method of generating truthful and customized dialogues using powerful LLMs. Given the reliable reference, RefGPT resolves LLM hallucination in dialogue generation to the greatest extent. RefGPT can also enable detailed customization in the structure, style and content of the dialogues.\n\u2022 With RefGPT, we construct two new multiturn dialogue datasets using GPT-4, called RefGPT-Fact and RefGPT-Code. To our best knowledge, RefGPT-Fact is one of the largest multi-turn dialogue datasets based on factual knowledge. And RefGPT-Code is the first and largest synthetic multi-turn dialogue dataset covering nearly all aspects of code scenarios. These have shown the capability of applying RefGPT to generate dialogues in any vertical domain by utilizing corresponding domain-specific documents."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 LLM based Dialogue Generation",
            "text": "The high-quality dialogue dataset is considered crucial for the success of current general chat models (Chiang et al., 2023; K\u00f6pf et al., 2023). Due to the high cost of human annotation, previous studies have explored the effectiveness of using LLMs for dialogue generation. Self-Instruct (Wang et al., 2022) presents a framework that facilitates the automatic generation of instruction data (singleturn dialogues) by leveraging existing LLMs. The procedure commences with a set of human-written seed tasks and progressively generates new instructions and responses by iteratively bootstrapping both the initial seeds and the newly produced data. Baize (Xu et al., 2023b) generates multiturn dialogues by leveraging LLMs to engage in a conversation with itself as both user and assistant based on the given seed topics. UltraChat (Ding et al., 2023) follows a similar idea to Baize and adopts two separate LLM APIs in the generation, where one acts as the user and the other acts as the assistant. However, the dialogues produced by these methods are susceptible to hallucination problems and are uncontrollable. Therefore, we present RefGPT as a solution to generate dialogues with truthfulness and customization."
        },
        {
            "heading": "2.2 Reference Based Dialogue Generation",
            "text": "QA pair and dialogue generation based on references have also been widely used. One important requirement for these methods is to ensure the truthfulness of the generated QA pairs and dialogues. Previous studies (Ma et al., 2020; Lewis et al., 2021) generate millions of high-quality QA pairs based on corpus documents using specialpurpose question generation models. Dialogue inpainting (Dai et al., 2022) extends this line of work to dialogues by transforming passages from Wikipedia into multi-turn dialogues using a masked conversational language model. In this work, we adopt a similar strategy using the LLMs that we take high-quality documents as references to ensure the truthfulness of the generated dialogues."
        },
        {
            "heading": "3 Generation Process",
            "text": "In this section, we present the whole process of RefGPT, which generates truthful and customized multi-turn dialogues by prompting the Large Language Models (LLMs) to effectively utilize the\nreference information. As illustrated in Figure 1, the RefGPT process is comprised of three main steps: Reference Selection (pertaining to truthfulness), Basic Prompt, and Dialogue Settings (pertaining to customization)."
        },
        {
            "heading": "3.1 Reference Selection",
            "text": "RefGPT guides the LLMs to leverage the given external documents or plain texts as references, instead of reciting their own knowledge, to generate truthful dialogues without worrying about hallucination.\nThe quality of generated dialogues in RefGPT relies on the selection of appropriate references, prioritizing quality and thematic relevance.\nA reference in RefGPT can range from a piece of unrefined plain text to a high-quality and dependable document in a specific domain, whose credibility determines the upper limit of the truthfulness of the generated dialogues. On the premise that the reference has contained enough information, it is imperative to opt for high-quality references, such as authoritative knowledge-based websites like Wikipedia.\nFurthermore, the chosen reference profoundly influences the thematic direction of the generated dialogues. Consequently, RefGPT exhibits the potential to generate dialogues in diverse domains,\ncontingent upon the existence of text-based knowledge repositories within those domains. These repositories include a broad spectrum of subjects, including, but not limited to, general domains like factual knowledge with encyclopedias, program codes, and vertical domains like shopping applications or the nuclear industry."
        },
        {
            "heading": "3.2 Basic Prompt",
            "text": "To facilitate the generation of multi-turn dialogues that adhere to our basic requirements, we have devised a set of basic prompts:\n1. Prompt the LLMs to generate multi-turn dialogues based on the provided reference.\n2. Specify the desired language for the dialogue generation. It is preferable for the language of the reference to be consistent with the dialogue to be generated.\n3. Instruct the LLMs to reject unreasonable user requests, such as illegal or inappropriate instructions while providing appropriate advice to discourage such actions. This prompt aids in generating dialogues that align with human preferences to a certain extent.\n4. LLMs like GPT-3.5-turbo and GPT-4 offer an option of writing a \"system\" role prompt\nto exert precise control over their behaviors in responses. This capability enables customization of the chatbot\u2019s identity by providing relevant background information. For instance, in a vertical domain like a shopping app, RefGPT can generate dialogues that conform to the persona of a shopping assistant, even if the reference has no explicit association with shopping (but may have an implicit association)."
        },
        {
            "heading": "3.3 Dialogue Settings",
            "text": "Rather than generating dialogues uncontrollably, RefGPT uses dialogue settings to convert the reference to a specific dialogue format and customize every utterance, as shown in the middle part of the Figure 1. In dialogue settings, we first specify the task description to tell LLMs how to use the reference. We then customize the structure, style, and content of the dialogue, which can be collectively called local customization."
        },
        {
            "heading": "3.3.1 Task Description",
            "text": "We begin by defining the task of dialogue generation concerning the utilization of references, as it relies on the specific aspect of the reference that we aim to initiate the dialogue. For instance, a given piece of program code can lead to multiple scenarios (tasks), such as explaining, creating, or debugging."
        },
        {
            "heading": "3.3.2 Local Customization",
            "text": "As per the task description, the local customization specifies the settings regarding the dialogue\u2019s structure, style, and content. These settings are then incorporated into a dialogue template for generating the final dialogue.\nDialogue Structure To define the dialogue structure, we start the dialogue with the marker <chat> and end it with the marker </chat>. These two markers specify the range of the whole dialogue. Between the start and the end, we use <user> for the user giving instructions and <assistant> for the chatbot. A unified output format in a dialogue template avoids most of the weird generations of LLMs and is easier for post-processing. What is more, we will show more merits of using such a format to control the number of turns and length per turn.\n(1) Number of Turns LLMs like GPT-3.5/GPT4 often fail with counting the number of the turns of dialogues if we directly require a certain number. But we find that GPT-3.5/GPT-4 are good at following the given format and replacing the placeholders with their own generated content. Therefore, if we want to generate n turns of dialogues, we explicitly give the n <user> and <assistant> pairs to let LLMs follow the output format. We have also added numerical markers to indicate the ith turn of the dialogue, e.g., <user i> and <assistant i>, allowing the LLMs to better identify the progress of the current generated turn.\n(2) Length of Utterance Generating a whole dialogue at one-time, e.g., Self-Instruct (Wang et al., 2022), often leads to much shorter responses than the general chat models like GPT-3.5 do, as shown in Table 1. However, in RefGPT, we can control the lengths of not only the responses of the assistant but also the questions raised by the user at every turn of the dialogue.\nWe observe that specifying a word count as the prompt is useful for influencing the length of generated utterances. Following the autoregressive (left-to-right) order, we first illustrate the requirement of word count like <user>(word count: x words) or <assistant>(word count: x words) before our customization on style and content. Therefore, RefGPT can generate a shorter or much longer question/response depending on the specified word count. Though this prompt can also be used to make the generated utterances longer with other methods like Self-Instruct, generating longer utterances always leads to a more severe hallucination problem. RefGPT filters out the reference whose length is shorter than 80% of the required dialogue length to ensure truthfulness. Thus the LLMs have no necessity of reciting their own knowledge, as the reference length is similar and even longer than the dialogue length.\nDialogue Style Staying organized around the same reference, the style of dialogue can vary in the style of asking and answering. For example, a dialogue can start between a user who is a child and an assistant who answers in a way that a child can understand. RefGPT enables this customization for every utterance of <user> and <assistant> in the dialogues by adding the style requirements before the content customization.\nDialogue Content After specifying the style, we can customize the content of each utterance about what to ask and what to answer.\nFor the task like factual knowledge, the user can be set to ask more about the entity or numbers in the reference. For the task of coding, the user can ask from different perspectives on writing, revising, and using the code and the assistant can choose to give an example or not.\nDialogue Template We aggregate the local customizations into a dialogue template to transfer the reference to the dialogue. To enable diversity, we sample different local customization settings for each utterance in the dialogue, as shown in the right-most part in Figure 1. In practice, RefGPT can work well even without style and content pools. These additional settings only need a small amount of manual work for further customization and can be reused to generate diverse dialogues based on different references.\n1. For the dialogue structure, we will set the number of turns by weighted sampling. And we sample the word count for both user and assistant in each utterance from a Gaussian distribution.\n2. For the dialogue style, we construct a conversational style pool to sample the style settings.\n3. For the dialogue content, we construct a content pool according to the task (factual knowledge, code, etc) to sample the content settings."
        },
        {
            "heading": "4 RefGPT Dialogue Datasets",
            "text": "In this section, we present two multi-turn dialogue datasets, denoted as RefGPT-Fact and RefGPTCode, which are generated utilizing the GPT-4 API in conjunction with RefGPT. More information about these two datasets can be found in Appendix A, and examples are provided in Appendix B."
        },
        {
            "heading": "4.1 Dataset Generation Process",
            "text": "RefGPT-Fact RefGPT-Fact is a dataset containing 100k multi-turn dialogues about factual knowledge with 50k English and 50k Chinese. The English version uses the English Wikipedia\n1In Table 1, we calculate the number of tokens with the OpenAI tokenizer library of tiktoken in https://github. com/openai/tiktoken. We use cl100k_base in tiktoken to tokenize.\nas the reference and the Chinese version uses the frequently-used Chinese online encyclopedia website, Baidu Baike. We use various dialogue settings mentioned in Sec 3.3 to increase the dialogue diversity.\nRefGPT-Code RefGPT-Code is a dataset containing 76k multi-turn dialogues about programming with 37k English and 39k Chinese, which has covered most aspects of code usage scenarios and multiple types of programming languages. Both the English version and Chinese version use the public Github dataset on Google BiqQuery with no overlap in these two languages. RefGPT-Code has derived various ways of leveraging the program code as the reference to enable different scenarios. We consider three perspectives of code discussion, code creation and bug fixing in RefGPT-Code.\n1. In RefGPT-Code-ds about code discussion, we want the LLMs to generate dialogues about asking questions about the given reference code, including explaining, discussing, revising, rewriting, and using the code. After the generation, we will concatenate the reference code as the context to the first question of the user to form the complete version of the dialogue, because we often give the code first before asking questions about it. Thus, the whole dialogue has much longer user utterances, as shown in Table 1.\n2. In RefGPT-Code-cr about code creation, though we provide the program code as the reference, we assume that the user has an idea/request/trouble/task relevant to the given code but does not know such a code exists, thus he/she wants the assistant to help with writing the code. And the assistant is required to write the code according to the reference code instead of generating a new code to ensure the reliability of the generated code.\n3. In RefGPT-Code-bg about bug fixing, the user first writes a piece of code with bugs based on the given reference code, which is realized by asking the LLMs to rewrite the code to a buggy one in the first utterance of the user. Then the assistant is required to tell the user where the bugs are and how to fix them according to the reference code. In this scenario, we assume the reference code is reliable and has no bugs.\nTable 1: Comparsions on different dialogue datasets that contain instructions. AI means whether it is generated by AI. Truthful indicates whether the truthfulness of the dialogues is guaranteed. QLen means the average number of tokens1of user utterance. RLen means the average number of tokens of assistant utterance. Turn means whether the number of dialogue turns can be specified. Lang indicates the languages the dataset supports. For a fair comparison, only the English parts are selected in all the datasets.\nDataset AI Truthful QLen RLen Turn Lang\nDolly (Databricks, 2023) N/A 16.3 78.2 1 en Oasst1 (K\u00f6pf et al., 2023) N/A 28.0 169.5 1\u223c5 multi ShareGPT (Dom Eccleston, 2023) 75.6 268.8 1\u223c5 multi Alpaca (Wang et al., 2022) 17.2 55.3 1 en Baize Quora (Xu et al., 2023b) 15.7 43.2 3\u223c5 en UltraChat World (Ding et al., 2023) 28.6 207.9 3\u223c7 en RefGPT-Fact 28.1 269.5 3\u223c4 en, cn RefGPT-Code-ds 281.7 374.6 3\u223c4 en, cn RefGPT-Code-cr 36.9 395.0 3\u223c4 en, cn RefGPT-Code-bg 155.7 380.8 2\u223c4 en, cn"
        },
        {
            "heading": "4.2 Dataset Collection Setup",
            "text": "We use the RefGPT with GPT-4 API to generate these two datasets. The length of every utterance is decided by sampling the Gaussian distribution of N (\u00b5, \u03c3), where \u00b5 accounts for the average word count (e.g., 300 words) of the utterance and \u03c3 is the standard variance (e.g., 50 words). The number of turns is decided by weighted sampling, where the weights determine the ratio of dialogues with a specific number of turns in the dataset."
        },
        {
            "heading": "4.3 Dataset Statistics",
            "text": "As shown in Table 1, we compare our datasets to other high-quality dialogue datasets. ShareGPT (Dom Eccleston, 2023) collects the dialogues from the real users and ChatGPT, which have much longer user utterances and assistant utterances. If we choose the responses of ChatGPT as a baseline, methods with one API, e.g., Self-Instruct (Wang et al., 2022) and Baize (Xu et al., 2023b), always lead to shorter assistant responses. UltraChat (Ding et al., 2023) with two independent APIs chatting to each other maintains the length of generated responses close to ChatGPT. However, as shown in Table 2, such methods call the model API one utterance at a time with significantly increasing cost and time, as UltraChat has to attach the conversation history multiple times. By contrast, RefGPT generates the whole dialogue with one API call but can adjust the length of generated utterance flexibly according to the requirement.\nRefGPT-Fact inherits the diversity of the references like Wikipedia and Baidu Baike. Besides that, RefGPT-Fact has an average response length of 269.5 in English which is very similar to the\nlength of ChatGPT response in ShareGPT. RefGPT-Code series implements various customizations to be adapted to specific scenarios and have longer user and assistant utterances because we have not only the utterances but also the code attached to the dialogues."
        },
        {
            "heading": "5 Experiment",
            "text": ""
        },
        {
            "heading": "5.1 Truthfulness Evaluation",
            "text": "In order to verify the reliability of RefGPT, We evaluate the truthfulness of the RefGPT dataset using both human evaluation for small sample and automatic evaluation with GPT-4 for a large range of verificaiton. For automatic evaluation with GPT-4, though existing methods (Chiang et al., 2023; Liu et al., 2023) have leveraged the GPT-4 to evaluate the performance of other LLMs. However such evaluation is not reliable in factual error checking because GPT-4 has the issue of model hallucination. Inspired by RefGPT, we design a pipeline to evaluate the truthfulness of generated dialogues from our reference datasets, e.g., Wikipedia, by using the GPT-4 to evaluate but with the additional help of reference.\nTable 2: Comparsions on different methods of automatically generating dialogues via LLMs. Multi-turn means whether it is a multi-turn dialogue generation. Human Tru. and GPT-4 Tru. evaluate the truthfulness with accuracy by humans and GPT-4 model. Len uses ChatGPT\u2019s response length as the standard for long responses. Turn means whether the number of dialogue turns can be specified. Custo. depends on whether it can control the dialogue structure and content. Call is the number of model or model API calls needed for generating an instruction or a n-turn dialogue every time.\nMethod Multi-turn Human Tru. GPT-4 Tru. Len Turn Custo. Call\nSelf-Instruct (Wang et al., 2022) 54.0 50.2 short one limited 1 Baize Self-Chat (Xu et al., 2023b) 50.0 47.2 short random limited 1 UltraChat (Ding et al., 2023) - - long adjustable limited 2n RefGPT 98.0 97.5 adjustable adjustable highly 1"
        },
        {
            "heading": "5.1.1 Evaluation Process",
            "text": "We compare RefGPT to two popular automatic methods as the baselines, namely Self-Instruct (Wang et al., 2022) and Baize Self-Chat (Xu et al., 2023b). For a fair comparison, we want the generated dialogues of different methods to talk about the same things. Thus we do an additional work that we let GPT-4 generate {question, answer} pairs from the selected references and restrict the answers to the questions to be found or inferred from the references. Given a selected reference, for Self-Instruct, we follow the Alpaca (Taori et al., 2023) that we randomly select three {question, answer} pairs (from other references) as few-shot examples and add the final question from the selected reference at the end of the model input. And we let the model respond to the final question. For Baize, we use the question generated from the selected reference as the seed following the way that Baize uses questions in Quora as seeds. For RefGPT, we directly use the selected reference to generate. In practice, we select 1000 passages from Wikipedia as the references to generate 1000 seed {question, answer} pairs using the GPT-4. And we generate the dialogues using these three methods with GPT-3.5-turbo for the experiment. In Table 3, we give examples of evaluating the truthfulness of the dialogues generated by RefGPT. And more examples of Self-Instruct and Baize can be seen in Appendix C.\nFor human evaluation for a small sample, we randomly sample 50 English dialogues each for Alpaca, Baize, and RefGPT about factual knowledge. And 2 humans evaluate the truthfulness of the dialogues according to the references.\nFor automatic evaluation for a large range, in order to let GPT-4 check the factual errors without suffering from the model hallucination, we need a reference for GPT-4 to refer like RefGPT.\nTherefore, as shown in Figure 2, we let GPT-4 check if the generated dialogue accords with the reference. If the generated dialogue does not align with the reference, it indicates the presence of factual errors."
        },
        {
            "heading": "5.1.2 Result",
            "text": "We use accuracy to measure the truthfulness in the evaluation process, which is the proportion of the number of dialogues without factual errors in the total of 1000 generated dialogues. In Table 2, to our surprise, we can see that Self-Instruct and Baize Self-Chat have a striking number of factual errors in the generated dialogues on both human and GPT-4 evaluations. As the dialogues generated by Baize are multi-turn, they are more likely to contain factual errors and thus have a lower truthfulness score of 47.2. By contrast, RefGPT has a truthfulness score of 97.5 with merely no factual errors. This also implicitly indicates that a model like GPT-3.5-turbo already has the ability to generate the dialogues strictly conforming to the references rather than modifying with hallucination. Another method called UltraChat (Ding et al., 2023) in Table 2 is not included, as the code has not been open-source at the time we write this paper."
        },
        {
            "heading": "5.2 Further Analysis",
            "text": "In this section, we explore the potential influence of the reference and customization on the generated dialogues by RefGPT. For each setting in the following experiments, we generate 1000 dialogues using GPT-3.5-turbo."
        },
        {
            "heading": "5.2.1 Dialogue Quality",
            "text": "As RefGPT generates the dialogue according to the reference, the reference has a significant impact on the quality of generated dialogues. We use the evaluation method mentioned in Sec 5.1 to evaluate\nthe influence of the dialogue quality (truthfulness) in the following validations.\nReference Length As length is proportional to the amount of information the reference contains, we want to find out how the reference length will influence the truthfulness of the generated dialogues. We use the dialogue template of a 3- turn dialogue, where each utterance word count of the assistant is required to be 300 words. We experiment on different lengths of reference by the proportions: 100%, 50%, and 25% of the original required length (3 \u00d7 300 = 900 words).\nAs shown in Table 4, it is surprising to see that the truthfulness scores do not decrease much as the reference lengths are greatly reduced. We find that the GPT-3.5-turbo chooses to reduce the length of the generated utterances to obey reference despite violating the length requirement.\nReference Quality The reference in RefGPT can vary from plain texts to cleaned documents in the vertical domain.\nIn order to quantify the influence of reference quality on dialogue quality, we experiment with different qualities of references by adding additional noise. To be specific, we use the original reference as the baseline. We use HTML labels\nas noise is that many references may come from the crawled data on the websites and contain many HTML labels as noise if we do not clean the data carefully. We experiment with adding 10% and 20% nonsense HTML labels as the noise.\nAs we can see in Table 4, the truthfulness of the generated dialogues only slightly decreases because of the additional noise. This indicates the good robustness of generating truthful dialogues even with GPT-3.5-turbo."
        },
        {
            "heading": "5.2.2 Dialogue Structure",
            "text": "During post-processing of the generated dialogues of RefGPT, we find that the input length (related to reference length) and output length (related to the required word count) will influence the success rate of obeying the dialogue template. In order to evaluate the customization ability of RefGPT, we do experiments on generating 3-turn and 5-turn dialogues. As the input length (reference length) is also determined by the required word count, we experiment with different word counts of 100, 300, and 600 for each assistant utterance to verify the success rate of obeying the dialogue template.\ncontrol over the dialogue structure with a higher success rate. We further observe that if the ending mark </chat> is successfully generated, the dialogues are more likely to obey the dialogue template with the correct number of turns."
        },
        {
            "heading": "6 Conclusion",
            "text": "We present RefGPT, a new method that generates truthful and customized multi-turn dialogues using LLMs like GPT-3.5/GPT-4. Incorporating a reliable reference, RefGPT minimizes hallucination and untruthful content generation. RefGPT also allows for dialogue customization in structure, style, and content, making it flexible to generate dialogues with diversity. On the basis of RefGPT, we also use GPT-4 to construct two new multi-turn dialogue datasets, RefGPT-Fact and RefGPT-Code, based on the online encyclopedia websites and Github repositories. These datasets also showcase RefGPT\u2019s significant potential for developing dependable, domain-specific dialogue data required by specialized chatbots and other natural language processing applications.\nLimitations\nRefGPT can only strictly generate the dialogues conforming to the references even though the reference itself may have factual errors. Furthermore, the generated dialogues can not be avoided to be influenced by the biases from the references. Thus the datasets RefGPT-Fact and RefGPT-Code may have factual errors and typos from Wikipedia, or bugs and malicious program codes from Github repositories.\nLLMs like GPT-3.5/GPT-4 have their own biases, which will also have reflections in the dialogues generated by RefGPT."
        },
        {
            "heading": "A Dataset Card",
            "text": "A.1 RefGPT-Fact RefGPT-Fact is a dataset comprising 100k multi-turn dialogues focusing on factual knowledge. There are two versions, with the English version containing 50k dialogues based on the English Wikipedia, while the Chinese version consists of 50k dialogues sourced from the widely-used Chinese online encyclopedia, Baidu Baike.\nSince most of the passages in the English Wikipedia and Baidu Baike are written by individuals or unofficial organizations, many of the passages are not commonly seen in everyday life. We use GPT3.5-turbo API to quickly filter out the uncommon passages by asking it \"Do you know xxx? If yes, return <yes>. If no, return <no>.\", where xxx is the title of the passage2.\nA.2 RefGPT-Code RefGPT-Code is a comprehensive dataset that consists of 76k multi-turn dialogues on programming, including 37k English and 39k Chinese dialogues. As illustrated in Figure 3, it encompasses a wide range of coding scenarios about discussion, creation, and bug fixing using various programming languages. The dataset utilizes the public Github dataset available on Google BigQuery, with no overlapping data between the two languages.\n2This method is based on knowledge of GPT-3.5-turbo, where recall rate is limited. A more recommended way is using the access rate to filter out the uncommon passages.\nTable 6: An example of the prompt for generating the English RefGPT-Fact data. {dialogue_template} is replaced by the dialogue template mentioned in Sec 3.3.\n##Provided Information## {reference} Based on the ##Provided Information## above and its relevant\ntopic, expand it into a multi-round conversation. The conversation requires you to act as the\nchatbot Assistant and interact with a human, helping to solve the requests raised by the human. The\nhuman will ask multiple various questions/requests to the Assistant based on the information above\n(but the conversation should not include expressions like \"according to the above information\"),\nand the subsequent questions/requests will be a follow-up based on the previous conversation\nhistory. For every reasonable question/request posed by Human, Assistant should provide as\ndetailed an answer as possible, offering further explanations or examples. For unreasonable\nrequests from Human (those that are harmful to society, immoral, or illegal), Assistant will\nrefuse to answer and explain the reason for not answering, while also providing reasonable advice\nto avoid such actions.\n#Conversation Plan# Example: \"<chat><Human 1>:(Word count requirement: x words)XXX <Assistant 1>:\n(Word count requirement: x words) XXX <Human 2>:(Word count requirement: x words)XXX <Assistant\n2>: (Word count requirement: x words) XXX </chat>\", \"XXX\" is the requirement for the current\nconversation content of that role, and \"(Word count requirement: x words)\" specifies the minimum\nword count requirement for utterance of Human or Assistant. It must be noted: the conversation\nstarts with <chat> as the beginning of the multi-round conversation and ends with </chat> as\nthe end of the multi-round conversation. The following conversation follows this #Conversation\nPlan# and word count requirements: \"{dialogue_template}\", a total of {number_of_turns} turns of\nconversation.\n{dialogue_template} <chat><Human 1>:(word count: 100 words)asks a question <Assistant 1>:(word\ncount: 200 words)answers [+detailed explanation] <Human 2>:(word count: 150 words)further asks\nfrom the perspective of real life <Assistant 2>:(word count: 100 words)answers [+detailed\nexplanation] <Human 3>:(word count: 50 words)further asks a question <Assistant 3>:(word count:\n150 words)answers [+detailed explanation] </chat>\nTable 7: An example of the prompt for generating the English RefGPT-Code-ds data.\n##Provided Information## {reference} Based on the ##Provided Information## above and its relevant\ntopic, expand it into a multi-round conversation. The conversation requires you to act as the\nchatbot Assistant and interact with a human, helping to solve the requests raised by the human. The\nhuman will ask multiple various questions/requests to the Assistant based on the information above\n(but the conversation should not include expressions like \"according to the above information\"),\nand the subsequent questions/requests will be a follow-up based on the previous conversation\nhistory. For every reasonable question/request posed by Human, Assistant should provide as\ndetailed an answer as possible, offering further explanations or examples. For unreasonable\nrequests from Human (those that are harmful to society, immoral, or illegal), Assistant will\nrefuse to answer and explain the reason for not answering, while also providing reasonable advice\nto avoid such actions.\n#Conversation Plan# Example: \"<chat><Human 1>:(Word count requirement: x words)XXX <Assistant 1>:\n(Word count requirement: x words) XXX <Human 2>:(Word count requirement: x words)XXX <Assistant\n2>: (Word count requirement: x words) XXX </chat>\", \"XXX\" is the requirement for the current\nconversation content of that role, and \"(Word count requirement: x words)\" specifies the minimum\nword count requirement for utterance of Human or Assistant. It must be noted: the conversation\nstarts with <chat> as the beginning of the multi-round conversation and ends with </chat> as\nthe end of the multi-round conversation. The following conversation follows this #Conversation\nPlan# and word count requirements: \"{dialogue_template}\", a total of {number_of_turns} turns of\nconversation.\n{dialogue_template} <chat><Human 1>:(word count: 50 words)makes a request about writing the\ncode <Assistant 1>:(word count: 250 words)answers [+detailed explanation] and give code examples\n<Human 2>:(word count: 100 words)asks in a young person\u2019s tone about further modifying the\ncode <Assistant 2>:(word count: 300 words)answers [+detailed explanation] and give code examples\n<Human 3>:(word count: 20 words)asks from the perspective of real life about further how to\nuse the code <Assistant 3>:(word count: 250 words)answers [+detailed explanation] and give code\nexamples </chat>\nTable 8: An example of the prompt for generating the English RefGPT-Code-cr data.\n##Provided Information## {reference} Based on the ##Provided Information## above and its relevant\ntopic, expand it into a multi-round conversation. Human has an idea / requirement / task /\nassignment / problem / difficulty related to the above code and wants to solve it with a computer\nprogram, but doesn\u2019t know how to do it. But Human doesn\u2019t know that the above code exists, so\nit can\u2019t be mentioned in conversation. Assistant needs to organize the above code into answers\n(which cannot be found by Human) according to Human\u2019s ideas, write specific program code for\nHuman and explain it in detail so that Human\u2019s ideas can be realized. Based on this idea, Human\nwould ask multiple questions and requests for specific code written by the Assistant, which\nwill be follow-ups based on the previous conversation history. For unreasonable requests from\nHuman (those that are harmful to society, immoral, or illegal), Assistant will refuse to answer\nand explain the reason for not answering, while also providing reasonable advice to avoid such\nactions.\n#Conversation Plan# Example: \"<chat><Human 1>:(Word count requirement: x words)XXX <Assistant 1>:\n(Word count requirement: x words) XXX <Human 2>:(Word count requirement: x words)XXX <Assistant\n2>: (Word count requirement: x words) XXX </chat>\", \"XXX\" is the requirement for the current\nconversation content of that role, and \"(Word count requirement: x words)\" specifies the minimum\nword count requirement for utterance of Human or Assistant. It must be noted: the conversation\nstarts with <chat> as the beginning of the multi-round conversation and ends with </chat> as\nthe end of the multi-round conversation. The following conversation follows this #Conversation\nPlan# and word count requirements: \"{dialogue_template}\", a total of {number_of_turns} turns of\nconversation.\n{dialogue_template} <chat><Human 1>:(word count: 50 words)asks with curiosity about creating\nthe code <Assistant 1>:(word count: 300 words)answers [+detailed explanation] and give code\nexamples <Human 2>:(word count: 100 words)asks a question about further using the code <Assistant\n2>:(word count: 250 words)answers [+detailed explanation] and give code examples <Human 3>:(word\ncount: 150 words)asks a question about further explaining the code <Assistant 3>:(word count:\n300 words)answers [+detailed explanation] and give code examples <Human 4>:(word count: 50\nwords)expresses his/her needs and asks the Assistant for help about further using the code\n<Assistant 4>:(word count: 200 words)answers [+detailed explanation]</chat>\nTable 9: An example of the prompt for generating the English RefGPT-Code-bg data.\n##Provided Information## {reference} Based on the ##Provided Information## above and its relevant\ntopic, expand it into a multi-round conversation. Human will write a piece of code with bugs\nbased on the given code above (however, Human needs to hide the presence of the given code in the\nconversation, and it cannot be mentioned). They will then ask Assistant for help in fixing the\nbugs. Assistant needs to identify the mistakes in Human\u2019s code based on the given code above (but\ngiven code cannot be discovered by Human, and it cannot be mentioned in the conversation) and\nprovide detailed explanations on how to fix the bugs, along with more explanations or examples\nif necessary. Afterward, Human and Assistant will continue the conversation around this code.\nFor unreasonable requests from Human (those that are harmful to society, immoral, or illegal),\nAssistant will refuse to answer and explain the reason for not answering, while also providing\nreasonable advice to avoid such actions.\n#Conversation Plan# Example: \"<chat><Human 1>:(Word count requirement: x words)XXX <Assistant 1>:\n(Word count requirement: x words) XXX <Human 2>:(Word count requirement: x words)XXX <Assistant\n2>: (Word count requirement: x words) XXX </chat>\", \"XXX\" is the requirement for the current\nconversation content of that role, and \"(Word count requirement: x words)\" specifies the minimum\nword count requirement for utterance of Human or Assistant. It must be noted: the conversation\nstarts with <chat> as the beginning of the multi-round conversation and ends with </chat> as\nthe end of the multi-round conversation. The following conversation follows this #Conversation\nPlan# and word count requirements: \"{dialogue_template}\", a total of {number_of_turns} turns of\nconversation.\n{dialogue_template} <chat><Human 1>:(word count: 500 words)asks from the perspective of real\nlife about writing a piece of code with bugs and show the detailed code <Assistant 1>:(Word\ncount: 250 words)answers [+detailed explanation] and tell Human about the error location in the\ncode, then provide a correct piece of code <Human 2>:(word count: 100 words)makes a request about\nfurther using the code <Assistant 2>:(Word count: 200 words)answers [+detailed explanation] and\ngive code examples <Human 3>:(word count: 50 words)asks with curiosity about further explaining\nthe code <Assistant 3>:(Word count: 250 words)answers [+detailed explanation]</chat>"
        },
        {
            "heading": "B Dataset Examples",
            "text": "Table 10: An example of Chinese RefGPT-Fact.\nReference\n\u5317\u4e0a\u5e7f\u6df1\u5317\u4e0a\u5e7f\u6df1\u662f\u6307\u4e2d\u56fd\u5927\u9646\u5730\u533a\u7ecf\u6d4e\u5b9e\u529b\u6700\u5f3a\u7684\u56db\u5ea7\u57ce\u5e02\u4e3a\u5317\u4eac\u3001\u4e0a\u6d77\u3001\u5e7f\u5dde\u3001\u6df1\u5733\u3002\u8fd9\u56db\u5ea7\u57ce\u5e02\u7684 \u7efc\u5408\u5b9e\u529b\u5728\u4e2d\u56fd\u5927\u9646\u5730\u533a\u5904\u4e8e\u6700\u9886\u5148\u7684\u5c42\u6b21\u3002\u4e2d\u6587\u540d\u5317\u4e0a\u5e7f\u6df1\u5916\u6587\u540d Beijing-Shanghai-Guangzhou-Shenzhen \u522b\u540d\u4e00\u7ebf\u57ce\u5e02\u57ce\u5e02\u6570 4\u5317\u4e0a\u5e7f\u6df1\u6982\u8981\u5317\u4e0a\u5e7f\u6df1\u6307\u7684\u662f\u5317\u4eac\u3001\u4e0a\u6d77\u3001\u5e7f\u5dde\u3001\u6df1\u5733\u3002\u8fd94\u4e2a\u57ce\u5e02\u5728\u4e2d\u56fd\u5927\u9646\u5730 \u533a\u57ce\u5e02\u4e2d\u7684\u7efc\u5408\u5b9e\u529b\u548c\u7ade\u4e89\u529b\u76f8\u5bf9\u5904\u4e8e\u6700\u9886\u5148\u7684\u5c42\u6b21\uff0c\u53c8\u88ab\u79f0\u4f5c\u4e00\u7ebf\u57ce\u5e02\u3002\u62e5\u6709\u96c4\u539a\u7684\u7ecf\u6d4e\u57fa\u7840\uff0c\u4ee5\u53ca\u53ef\u89c2 \u7684\u653f\u6cbb\u8d44\u6e90\uff0c\u5bf9\u5468\u8fb9\u591a\u4e2a\u7701\u4efd\u5177\u6709\u8f90\u5c04\u80fd\u529b\uff0c\u6709\u96c4\u539a\u7684\u6559\u80b2\u8d44\u6e90\u3001\u6df1\u539a\u7684\u6587\u5316\uff0c\u8fd8\u5177\u6709\u7e41\u534e\u3001\u7f8e\u4e3d\u7684\u591c\u666f\uff0c \u6781\u4e3a\u4fbf\u5229\u7684\u4ea4\u901a\u548c\u72ec\u7279\u7684\u57ce\u5e02\u9b45\u529b\u3002\u5317\u4e0a\u5e7f\u6df1\u57ce\u5e02\u6982\u51b5\u5317\u4e0a\u5e7f\u6df1\u5317\u4eac\u5317\u4eac\uff08Beijing\uff09\uff0c\u7b80\u79f0\u4eac\uff0c\u4e2d\u534e\u4eba\u6c11 \u5171\u548c\u56fd\u9996\u90fd\u3001\u76f4\u8f96\u5e02\u3001\u56fd\u9645\u5927\u90fd\u5e02\u3001\u56fd\u5bb6\u4e2d\u5fc3\u57ce\u5e02\u3001\u8d85\u5927\u57ce\u5e02\uff0c\u5168\u56fd\u653f\u6cbb\u4e2d\u5fc3\u3001\u6587\u5316\u4e2d\u5fc3\u3001\u56fd\u9645\u4ea4\u5f80\u4e2d\u5fc3\u3001 \u79d1\u6280\u521b\u65b0\u4e2d\u5fc3\uff0c\u7ecf\u6d4e\u4e0e\u91d1\u878d\u7684\u7ba1\u7406\u4e2d\u5fc3\u548c\u51b3\u7b56\u4e2d\u5fc3\uff0c\u662f\u4e2d\u56fd\u5171\u4ea7\u515a\u4e2d\u592e\u59d4\u5458\u4f1a\u3001\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u4e2d\u592e\u4eba\u6c11\u653f \u5e9c\u548c\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u5168\u56fd\u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\u7684\u529e\u516c\u6240\u5728\u5730\u3002\u5317\u4eac\u5386\u53f2\u60a0\u4e45\uff0c\u6587\u5316\u707f\u70c2\uff0c\u662f\u9996\u6279\u56fd\u5bb6\u5386\u53f2\u6587\u5316\u540d \u57ce\u3001\u4e2d\u56fd\u516b\u5927\u53e4\u90fd\u4e4b\u4e00\u548c\u4e16\u754c\u4e0a\u62e5\u6709\u4e16\u754c\u6587\u5316\u9057\u4ea7\u6570\u6700\u591a\u7684\u57ce\u5e02\uff0c3060\u5e74\u7684\u5efa\u57ce\u53f2\u5b55\u80b2\u4e86\u6545\u5bab\u3001\u5929\u575b\u3001\u516b\u8fbe \u5cad\u957f\u57ce\u3001\u9890\u548c\u56ed\u7b49\u4f17\u591a\u540d\u80dc\u53e4\u8ff9\u3002\u65e9\u5728\u4e03\u5341\u4e07\u5e74\u524d\uff0c\u5317\u4eac\u5468\u53e3\u5e97\u9547\u5730\u533a\u5c31\u51fa\u73b0\u4e86\u539f\u59cb\u4eba\u7fa4\u90e8\u843d\u201c\u5317\u4eac\u4eba\u201d\u3002\u516c \u5143\u524d1045\u5e74\uff0c\u5317\u4eac\u6210\u4e3a\u84df\u3001\u71d5\u7b49\u8bf8\u4faf\u56fd\u7684\u90fd\u57ce\u3002\u516c\u5143938\u5e74\u4ee5\u6765\uff0c\u5317\u4eac\u5148\u540e\u6210\u4e3a\u8fbd\u966a\u90fd\u3001\u91d1\u4e0a\u90fd\u3001\u5143\u5927\u90fd\u3001 \u660e\u6e05\u56fd\u90fd\u3001\u6c11\u521d\u4eac\u5146\u30021949\u5e7410\u67081\u65e5\u6210\u4e3a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u9996\u90fd\u30022015\u5e74\u672b\uff0c\u5317\u4eac\u5168\u5e02\u5e38\u4f4f\u4eba\u53e32170.5\u4e07\u4eba\uff0c \u6bd42014\u5e74\u672b\u589e\u52a018.9\u4e07\u4eba\u3002\u5176\u4e2d\uff0c\u5e38\u4f4f\u5916\u6765\u4eba\u53e3822.6\u4e07\u4eba\uff0c\u5360\u5e38\u4f4f\u4eba\u53e3\u7684\u6bd4\u91cd\u4e3a37.9%\u30022015\u5e74\u5317\u4eac\u5e02\u5b9e\u73b0 \u5730\u533a\u751f\u4ea7\u603b\u503c22968.6\u4ebf\u5143\uff0c\u6bd42014\u5e74\u589e\u957f6.9%\u30022015\u5e747\u670831\u65e5\uff0c\u56fd\u9645\u5965\u6797\u5339\u514b\u59d4\u5458\u4f1a\u4e3b\u5e2d\u6258\u9a6c\u65af\u00b7\u5df4\u8d6b\u5ba3\u5e03 \u5317\u4eac\u643a\u624b\u5f20\u5bb6\u53e3\u83b7\u5f972022\u5e74\u51ac\u5b63\u5965\u6797\u5339\u514b\u8fd0\u52a8\u4f1a\u7684\u4e3e\u529e\u6743\u3002\u5317\u4eac\u7531\u6b64\u6210\u4e3a\u5168\u7403\u9996\u4e2a\u65e2\u4e3e\u529e\u8fc7\u590f\u5b63\u5965\u6797\u5339\u514b\u8fd0 \u52a8\u4f1a\u53c8\u5373\u5c06\u4e3e\u529e\u51ac\u5b63\u5965\u6797\u5339\u514b\u8fd0\u52a8\u4f1a\u7684\u57ce\u5e02\u3002\u5317\u4e0a\u5e7f\u6df1\u4e0a\u6d77\u4e0a\u6d77\uff08Shanghai\uff09\uff0c\u7b80\u79f0\u201c\u6caa\u201d\u6216\u201c\u7533\u201d\uff0c\u4e2d\u534e\u4eba\u6c11 \u5171\u548c\u56fd\u76f4\u8f96\u5e02\uff0c\u8d85\u5927\u57ce\u5e02\uff0c\u56fd\u9645\u5927\u90fd\u5e02\u3001\u4e2d\u56fd\u56fd\u5bb6\u4e2d\u5fc3\u57ce\u5e02\uff0c\u4e2d\u56fd\u7684\u7ecf\u6d4e\u4e2d\u5fc3\u3001\u8d38\u6613\u4e2d\u5fc3\u3001\u822a\u8fd0\u4e2d\u5fc3.\u9996\u6279\u6cbf \u6d77\u5f00\u653e\u57ce\u5e02\u3002\u5730\u5904\u957f\u6c5f\u53e3\uff0c\u9694\u4e1c\u4e2d\u56fd\u6d77\u4e0e\u65e5\u672c\u4e5d\u5dde\u5c9b\u76f8\u671b\uff0c\u5357\u6fd2\u676d\u5dde\u6e7e\uff0c\u897f\u4e0e\u6c5f\u82cf\u3001\u6d59\u6c5f\u4e24\u7701\u76f8\u63a5\u3002\n<user>\u5317\u4e0a\u5e7f\u6df1\u662f\u6307\u54ea\u56db\u4e2a\u57ce\u5e02\uff1f <assistant>\u5317\u4e0a\u5e7f\u6df1\u662f\u6307\u4e2d\u56fd\u5927\u9646\u5730\u533a\u7ecf\u6d4e\u5b9e\u529b\u6700\u5f3a\u7684\u56db\u5ea7\u57ce\u5e02\uff0c\u5b83\u4eec\u5206\u522b\u662f\u5317\u4eac\u3001\u4e0a\u6d77\u3001\u5e7f\u5dde\u548c\u6df1\u5733\u3002\u8fd9 \u56db\u5ea7\u57ce\u5e02\u7684\u7efc\u5408\u5b9e\u529b\u5728\u4e2d\u56fd\u5927\u9646\u5730\u533a\u5904\u4e8e\u6700\u9886\u5148\u7684\u5c42\u6b21\uff0c\u4e14\u88ab\u79f0\u4f5c\u4e00\u7ebf\u57ce\u5e02\u3002\u7531\u4e8e\u8fd9\u4e9b\u57ce\u5e02\u62e5\u6709\u96c4\u539a\u7684\u7ecf\u6d4e \u57fa\u7840\u3001\u653f\u6cbb\u8d44\u6e90\u4ee5\u53ca\u53ef\u89c2\u7684\u7e41\u8363\u7a0b\u5ea6\uff0c\u5b83\u4eec\u5bf9\u5468\u8fb9\u7701\u4efd\u5177\u6709\u5de8\u5927\u7684\u8f90\u5c04\u80fd\u529b\u3002\n<user>\u5317\u4eac\u5728\u4e2d\u56fd\u6709\u54ea\u4e9b\u91cd\u8981\u7684\u5730\u4f4d\uff1f <assistant>\u5317\u4eac\uff0c\u4f5c\u4e3a\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u9996\u90fd\uff0c\u62e5\u6709\u5728\u4e2d\u56fd\u975e\u5e38\u91cd\u8981\u7684\u5730\u4f4d\u3002\u9996\u5148\uff0c\u5b83\u662f\u5168\u56fd\u653f\u6cbb\u4e2d\u5fc3\uff0c\u662f\u4e2d \u56fd\u5171\u4ea7\u515a\u4e2d\u592e\u59d4\u5458\u4f1a\u3001\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u4e2d\u592e\u4eba\u6c11\u653f\u5e9c\u548c\u5168\u56fd\u4eba\u6c11\u4ee3\u8868\u5927\u4f1a\u7684\u529e\u516c\u6240\u5728\u5730\u3002\u5176\u6b21\uff0c\u5b83\u4e5f\u662f\u4e2d\u56fd \u7684\u6587\u5316\u4e2d\u5fc3\uff0c\u662f\u9996\u6279\u56fd\u5bb6\u5386\u53f2\u6587\u5316\u540d\u57ce\u3001\u4e2d\u56fd\u516b\u5927\u53e4\u90fd\u4e4b\u4e00\uff0c\u5177\u6709\u6df1\u539a\u7684\u6587\u5316\u5e95\u8574\u3002\u6b64\u5916\uff0c\u5317\u4eac\u8fd8\u662f\u56fd\u9645\u4ea4 \u5f80\u4e2d\u5fc3\u3001\u79d1\u6280\u521b\u65b0\u4e2d\u5fc3\u4ee5\u53ca\u7ecf\u6d4e\u4e0e\u91d1\u878d\u7684\u7ba1\u7406\u4e2d\u5fc3\u548c\u51b3\u7b56\u4e2d\u5fc3\u3002\n<user>\u4f60\u80fd\u5426\u8be6\u7ec6\u8bb2\u4e00\u4e0b2015\u5e74\u5317\u4eac\u5e02\u7684\u7ecf\u6d4e\u72b6\u51b5\uff1f <assistant>\u6839\u636e2015\u5e74\u7684\u6570\u636e\uff0c\u5317\u4eac\u5e02\u5b9e\u73b0\u5730\u533a\u751f\u4ea7\u603b\u503c(GDP)\u4e3a22968.6\u4ebf\u5143\uff0c\u6bd42014\u5e74\u589e\u957f\u4e866.9%\u3002\u540c\u5e74 \u672b\uff0c\u5317\u4eac\u5168\u5e02\u5e38\u4f4f\u4eba\u53e3\u4e3a2170.5\u4e07\u4eba\uff0c\u6bd42014\u5e74\u672b\u589e\u52a0\u4e8618.9\u4e07\u4eba\u3002\u5176\u4e2d\uff0c\u5e38\u4f4f\u5916\u6765\u4eba\u53e3\u4e3a822.6\u4e07\u4eba\uff0c\u5360\u5e38 \u4f4f\u4eba\u53e3\u7684\u6bd4\u91cd\u4e3a37.9%\u3002\u8fd9\u4e9b\u6570\u636e\u53cd\u6620\u4e86\u5317\u4eac\u5e022015\u5e74\u7684\u7ecf\u6d4e\u72b6\u51b5\u4ee5\u53ca\u4eba\u53e3\u53d8\u5316\u60c5\u51b5\u3002\n<user>\u4e0a\u6d77\u5e02\u6709\u4ec0\u4e48\u7279\u70b9\uff1f <assistant>\u4e0a\u6d77\uff0c\u7b80\u79f0\u201c\u6caa\u201d\u6216\u201c\u7533\u201d\uff0c\u662f\u4e00\u5ea7\u5177\u6709\u8bb8\u591a\u7279\u70b9\u7684\u5927\u90fd\u5e02\u3002\u9996\u5148\uff0c\u5b83\u662f\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u7684\u76f4\u8f96\u5e02\uff0c \u540c\u65f6\u4e5f\u662f\u56fd\u5bb6\u4e2d\u5fc3\u57ce\u5e02\u3002\u5176\u6b21\uff0c\u4e0a\u6d77\u5e02\u662f\u4e2d\u56fd\u7684\u7ecf\u6d4e\u4e2d\u5fc3\u3001\u8d38\u6613\u4e2d\u5fc3\u548c\u822a\u8fd0\u4e2d\u5fc3\uff0c\u62e5\u6709\u5f88\u5f3a\u7684\u7ecf\u6d4e\u5b9e\u529b\u3002\u6b64 \u5916\uff0c\u4e0a\u6d77\u5e02\u662f\u9996\u6279\u6cbf\u6d77\u5f00\u653e\u57ce\u5e02\u4e4b\u4e00\uff0c\u5730\u5904\u957f\u6c5f\u53e3\uff0c\u4e5f\u662f\u4e00\u91cd\u8981\u7684\u5730\u7406\u67a2\u7ebd\u3002\u56e0\u4e3a\u8fd9\u4e9b\u7279\u70b9\uff0c\u4e0a\u6d77\u5728\u56fd\u5185\u548c \u56fd\u9645\u4e0a\u90fd\u5177\u6709\u6781\u9ad8\u7684\u5730\u4f4d\u548c\u5f71\u54cd\u529b\u3002\nTable 11: An example of English RefGPT-Fact.\nReference\nJoomla (), also spelled Joomla! (with an exclamation mark) and sometimes abbreviated as J!, is a free and open-source content management system (CMS) for publishing web content on websites. Web content applications include discussion forums, photo galleries, e-Commerce and user communities and numerous other web-based applications. Joomla is developed by a community of volunteers supported with the legal, organisational and financial resources of Open Source Matters, Inc. Joomla is written in PHP, uses object-oriented programming techniques and software design patterns, and stores data in a MySQL database. It has a software dependency on the Symfony PHP framework. Joomla includes features such as page caching, RSS feeds, blogs, search, and support for language internationalisation. It is built on a model\u2013view\u2013controller web application framework that can be used independently of the CMS. Around 6,000 extensions are available from the Joomla website, and more are available from other sources. As of 2021, it was estimated to be the fifth most used CMS on the Internet, after WordPress, Shopify, Wix and Squarespace. Overview Joomla has a web template system using a template processor. Its architecture is a front controller, routing all requests for non-static URIs via PHP which parses the URI and identifies the target page. This allows support for more human-readable permalinks. The controller manages both the frontend, public-facing view, and a backend (GUI-driven) administration interface. The administration interface (a) stores management and content information within a database, and (b) maintains a configuration file (, usually located in the file system root of the Joomla installation). The configuration file provides the connection between the server, database and file system and facilitates migrating the website from one server to another. The backend interface allows website operators to manage users, menus, extensions and web content ... (About 900 words)\n<user> I recently came across joomla and i was wondering if you could tell me more about this content management system (cms). how does it compare to other cms like wordpress, shopify, wix, and squarespace? what type of users does it cater to, and what kind of applications can be built using joomla? <assistant> Joomla is a free and open-source content management system (cms) that is widely used for creating web content on websites. it is designed to cater to users who have basic website creation skills and is ideal for building various web-based applications such as discussion forums, photo galleries, e-commerce sites, user communities and much more. joomla is developed and maintained by a community of volunteers, with open source matters, inc. offering legal, organizational, and financial resources to support the project.\nJoomla is built using php, object-oriented programming techniques, and software design patterns. it stores data in a mysql database and relies on the symfony php framework. some of its features include page caching, rss feeds, blogs, search capabilities, and support for language internationalization. it also comes with a model-view-controller web application framework that can be used independently of the cms.\nAs of 2021, joomla is estimated to be the fifth most used cms on the internet, coming after wordpress, shopify, wix, and squarespace. with around 6,000 extensions available on the joomla website and more from other sources, it provides flexibility and extensibility to users.\nComparing it to other cms, joomla\u2019s main advantage is its open-source nature and extensive customization options through its wide range of extensions. while wordpress is more user-friendly and has a larger community, joomla strikes a balance between ease of use and flexibility. shopify, wix, and squarespace, on the other hand, are more geared towards non-technical users and have limitations in extensibility when compared to joomla.\nIn summary, joomla is a flexible and customizable option for users who have some website creation skills and require a cms to build diverse web applications.\n<user> Thanks for explaining that. I\u2019ve heard that Joomla has a unique web template system and a front-controller architecture. Can you explain how these features work together and how they benefit the user? Also, what kind of risk management, backup, and recovery options are available with Joomla? <assistant> Joomla\u2019s web template system uses a template processor, and its architecture is based on a front-controller model. this means that all requests for non-static uris are routed through php, which parses the uri and identifies the target page. this approach allows for more human-readable permalinks, improving the user experience and search engine friendliness of a website.\nJoomla\u2019s front-controller architecture manages both the frontend, public-facing view, and a backend, gui-driven administration interface. the backend allows website operators to manage users, menus, extensions, and web content. the system stores management and content information within a database and maintains a configuration file to manage the connection between the server, database, and file system.\nAs for risk management, backup, and recovery, joomla itself does not have built-in features for these. however, third-party extensions and standalone products are available to provide backup and recovery functionality. it is the website operator\u2019s responsibility to implement and manage these tools to ensure the integrity and security of their website.\nTo summarize, joomla\u2019s web template system and front-controller architecture offer improved user experience, search engine optimization, and ease of administration. although the core cms doesn\u2019t provide backup and recovery features, third-party options fill this gap and deliver comprehensive risk management and data protection capabilities.\n(Two more turns are not shown because of the limited space. The whole dialogue contains about 1050 words.)\nTable 12: An example of Chinese RefGPT-Code-ds.\nReference\ndescribe Remover::Team do let(:github_client) { double('Github Client ') } let(:github_team) { double('Github Team', id: 1, name: 'Owners ') } let(:team) { Remover::Team.new(github_client , github_team) }\ndescribe '#name' do it 'returns Github team name' do\nexpect(team.name).to eq(github_team.name) end\nend ...\n<user> \u5728\u65e5\u5e38\u5de5\u4f5c\u4e2d\uff0c\u6211\u53d1\u73b0\u6709\u4e9bGithub\u56e2\u961f\u53ef\u80fd\u6ca1\u6709\u5b9e\u9645\u7684\u9879\u76ee\u548c\u6210\u5458\uff0c\u8fd9\u4f7f\u5f97\u7ba1\u7406\u8d77\u6765\u975e\u5e38\u9ebb\u70e6\u3002\u8bf7\u95ee \u5982\u679c\u6211\u60f3\u8bc6\u522b\u5e76\u5220\u9664\u8fd9\u4e9b\u65e0\u7528\u7684\u56e2\u961f\uff0c\u5e94\u8be5\u5982\u4f55\u4fee\u6539\u8fd9\u6bb5\u4ee3\u7801\uff1f\ndescribe Remover::Team do let(:github_client) { double('Github Client ') } let(:github_team) { double('Github Team', id: 1, name: 'Owners ') } let(:team) { Remover::Team.new(github_client , github_team) }\ndescribe '#name' do it 'returns Github team name' do\nexpect(team.name).to eq(github_team.name) end\nend ...\n<assistant> \u6211\u660e\u767d\u60a8\u7684\u9700\u6c42\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u8fd9\u6bb5\u4ee3\u7801\u4e2d\u7684\u2018Remover::Team\u2018\u7c7b\u6765\u5b9e\u73b0\u60a8\u7684\u76ee\u6807\u3002\u5728\u8fd9\u4e2a\u7c7b\u91cc\uff0c \u63d0\u4f9b\u4e86\u2018#used\u2018\u65b9\u6cd5\u6765\u5224\u65ad\u8be5\u56e2\u961f\u662f\u5426\u6709\u5b9e\u9645\u7684\u9879\u76ee\u548c\u6210\u5458\u3002\u5982\u679c\u56e2\u961f\u4e2d\u6ca1\u6709\u6210\u5458\u548c\u9879\u76ee\uff0c\u8be5\u65b9\u6cd5\u4f1a\u8fd4 \u56de\u2018false\u2018\u3002 \u9996\u5148\uff0c\u60a8\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u2018Remover::Team\u2018\u5bf9\u8c61\uff0c\u4f20\u5165Github\u5ba2\u6237\u7aef\u548cGithub\u56e2\u961f\u5b9e\u4f8b\u3002\u4f8b\u5982\uff1a\ngithub_client = double('Github Client ') github_team = double('Github Team', id: 1, name: 'Owners ') team = Remover::Team.new(github_client , github_team)\n\u4e4b\u540e\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u2018team.used\u2018\u65b9\u6cd5\u5224\u65ad\u56e2\u961f\u662f\u5426\u6709\u5b9e\u9645\u7684\u9879\u76ee\u548c\u6210\u5458\u3002\u5982\u679c\u8be5\u65b9\u6cd5\u8fd4\u56de\u2018false\u2018\uff0c\u60a8\u53ef\u4ee5\u8c03 \u7528\u2018team.delete_team\u2018\u65b9\u6cd5\u5220\u9664\u8fd9\u4e2a\u56e2\u961f\u3002\u4f8b\u5982\uff1a\nif not team.used team.delete_team end\n<user> \u8c22 \u8c22 \u4f60 \u7684 \u89e3 \u7b54 \uff0c \u6211 \u660e \u767d \u4e86 \u3002 \u90a3 \u6211 \u8fd8 \u60f3 \u8bf7 \u6559 \u4e00 \u4e0b \uff0c \u6709 \u5173 \u4ee3 \u7801 \u4e2d \u7684 \u5176 \u4ed6 \u65b9 \u6cd5 \uff0c \u5982\u2018#members_amount\u2018\u548c\u2018#repositories_amount\u2018\uff0c\u5b83\u4eec\u90fd\u662f\u5982\u4f55\u5b9e\u73b0\u7684\uff1f\u5b83\u4eec\u4f1a\u8fd4\u56de\u4ec0\u4e48\u7ed3\u679c\uff1f\u5982\u679c\u6211\u60f3\u5f97 \u5230\u5177\u4f53\u7684\u6210\u5458\u548c\u4ed3\u5e93\u4fe1\u606f\uff0c\u800c\u4e0d\u53ea\u662f\u6210\u5458\u6570\u91cf\u548c\u4ed3\u5e93\u6570\u91cf\uff0c\u6211\u5e94\u8be5\u5982\u4f55\u64cd\u4f5c\uff1f <assistant> \u2018Remover::Team\u2018\u7c7b\u4e2d\u7684\u2018#members_amount\u2018\u548c\u2018#repositories_amount\u2018\u65b9\u6cd5\u5206\u522b\u7528\u4e8e\u5f97\u5230\u56e2\u961f \u4e2d\u6210\u5458\u6570\u91cf\u548c\u9879\u76ee\u4ed3\u5e93\u6570\u91cf\u3002\u8fd9\u4e24\u4e2a\u65b9\u6cd5\u7684\u5b9e\u73b0\u90fd\u975e\u5e38\u7b80\u5355\uff0c\u5b83\u4eec\u5206\u522b\u8c03\u7528Github\u5ba2\u6237\u7aef \u7684\u2018team_members\u2018\u548c\u2018team_repositories\u2018\u65b9\u6cd5\uff0c\u4f20\u5165\u56e2\u961f\u7684ID\uff0c\u5e76\u83b7\u53d6\u7ed3\u679c\u96c6\u7684\u957f\u5ea6\u3002 ...\n(Two more turns are not shown because of the limited space. The whole dialogue contains about 1400 words.)\nTable 13: An example of English RefGPT-Code-ds.\nReference\n# -*- coding: utf -8 -*- import datetime from south.db import db from south.v2 import SchemaMigration from django.db import models\nclass Migration(SchemaMigration):\ndef forwards(self , orm): # Adding field 'Question.order' db.add_column(u'survey_question ', 'order ',\nself.gf('django.db.models.fields.IntegerField ')(default=0 ), keep_default=False)\n...\n<user>\n# -*- coding: utf -8 -*- import datetime from south.db import db from south.v2 import SchemaMigration from django.db import models\nclass Migration(SchemaMigration): ...\nIn the context of the provided code, I see there\u2019s a Django migration for adding a color field to an Idea model. Can you explain how a migration works in Django and how to apply this specific migration to add the \u2019color\u2019 field to the \u2019Idea\u2019 model in an existing Django project? <assistant> Sure! In Django, migrations are a way of managing and propagating changes made to your models (which affect the database schema) over time. A migration is essentially a set of instructions that the Django migration system uses to modify your database schema to match the current state of your model.\nApplying migrations to your Django project is a two-step process: 1. Creating the migration (migration file generation): When you make changes to your model, you need to inform Django about the changes. You do this by running the \u2018makemigrations\u2018 command, which creates a new migration file based on the alterations in the model. 2. Applying the migration (migration execution): After generating the migration file, you need to apply it to your database to update the schema. You do this by running the \u2018migrate\u2018 command, which processes the migration files sequentially and updates the database schema accordingly.\nIn the provided code, a migration is created to add a \u2019color\u2019 field to the \u2019Idea\u2019 model. To apply this migration in your Django project, follow these steps: 1. First, ensure that the migration is in the correct location within your Django project. Migration files are typically placed within a folder named \u2019migrations\u2019 inside an application folder. If the Django app containing your \u2019Idea\u2019 model is named \u2019brainstorming\u2019, the migration file should be located at \u2018brainstorming/migrations/\u2018. 2. Next, check if the migration has been generated by running \u2018python manage.py makemigrations\u2018. This will generate new migration files based on the changes in your models. Since the code you provided already includes a migration file, this step may not be necessary. However, it\u2019s recommended to run this command to ensure all migrations are accounted for. 3. Now, apply the migration to update your database schema by running \u2018python manage.py migrate\u2018. This command will process all pending migration files and apply them to the database.\nBy following these steps, you\u2019ll successfully apply the migration to add the \u2019color\u2019 field to the \u2019Idea\u2019 model in your existing Django project. Note that this migration is reversible, meaning you can undo the change by executing the \u2018python manage.py migrate <app_name> <migration_name_before_your_migration>\u2018 command. In this case, the \u2018<app_name>\u2018 would be \u2019brainstorming\u2019, and \u2018<migration_name_before_your_migration>\u2018 would be the name of the migration immediately preceding the one adding the \u2019color\u2019 field.\n(Two more turns are not shown because of the limited space. The whole dialogue contains about 1250 words.)\nTable 14: An example of Chinese RefGPT-Code-cr.\nReference\n... #include \"ScriptPCH.h\" #include \"zulaman.h\"\n...\n#define SPELL_REMOVE_AMANI_CURSE 43732 #define SPELL_PUSH_MOJO 43923 #define ENTRY_FOREST_FROG 24396\nclass npc_forest_frog : public CreatureScript ...\n<user> \u6211\u5728\u8bbe\u8ba1\u4e00\u4e2a\u6e38\u620f\u5173\u5361\uff0c\u60f3\u8981\u8ba9\u73a9\u5bb6\u5e2e\u52a9\u4e00\u540d\u88ab\u8bc5\u5492\u53d8\u6210\u9752\u86d9\u7684\u89d2\u8272\u6062\u590d\u539f\u72b6\u3002\u6e38\u620f\u4e2d\uff0c\u73a9\u5bb6\u53ef\u4ee5\u4f7f \u7528\u53bb\u9664\u8bc5\u5492\u7684\u6cd5\u672f\uff0c\u5bf9\u9752\u86d9\u65bd\u6cd5\u540e\uff0c\u6709\u4e00\u5b9a\u6982\u7387\u4f1a\u5c06\u9752\u86d9\u6062\u590d\u539f\u5f62\u3002\u5982\u679c\u8bc5\u5492\u88ab\u89e3\u9664\uff0c\u9752\u86d9\u4f1a\u53d8\u56de\u4e0d\u540c\u7684\u89d2 \u8272\uff0c\u5e76\u4e14\u6709\u51e0\u7387\u6389\u843d\u9053\u5177\u3002\u8bf7\u95ee\u5982\u4f55\u7f16\u5199\u8fd9\u90e8\u5206\u7a0b\u5e8f\u4ee3\u7801\u5462 <assistant> \u6839\u636e\u60a8\u7684\u9700\u6c42\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528C++\u7f16\u5199\u4e00\u4e2a\u7c7b\u6765\u5b9e\u73b0\u8fd9\u4e2a\u529f\u80fd\u3002\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u540d \u4e3a\u201cnpc_forest_frog\u201d\u7684\u7c7b\uff0c\u8be5\u7c7b\u5c06\u7ee7\u627f\u81ea\u4e00\u4e2a\u5916\u90e8\u5e93\u63d0\u4f9b\u7684\u201cCreatureScript\u201d\u7c7b\u3002\u6211\u4eec\u9700\u8981\u5728\u8fd9\u4e2a\u65b0\u7c7b\u4e2d\u5b9e\u73b0\u9752 \u86d9\u7684\u8bc5\u5492\u89e3\u9664\u903b\u8f91\u3002\u5728\u2018npc_forest_frog\u2018\u7c7b\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u5d4c\u5957\u7684\u7ed3\u6784\u4f53\u2018npc_forest_frogAI\u2018\uff0c\u8be5\u7ed3\u6784\u4f53 \u7ee7\u627f\u81ea\u2018ScriptedAI\u2018\u7c7b\u3002\u2018ScriptedAI\u2018\u7c7b\u662f\u6e38\u620f\u5f15\u64ce\u4e2dAI\u76f8\u5173\u7684\u57fa\u7c7b\u3002\u5728\u8fd9\u4e2a\u7ed3\u6784\u4f53\u5185\uff0c\u6211\u4eec\u9996\u5148\u5b9a\u4e49\u51e0\u4e2a\u51fd\u6570 \u6765\u5b9e\u73b0AI\u7684\u57fa\u672c\u903b\u8f91\uff0c\u4f8b\u5982\u201cReset\u201d\u3001\u201cEnterCombat\u201d\u7b49\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u9700\u8981\u5b9e\u73b0\u8bc5\u5492\u88ab\u89e3\u9664\u65f6\u7684\u884c\u4e3a\u3002\u5f53\u6cd5\u672f\u547d\u4e2d\u9752\u86d9\u65f6\uff0c\u6211\u4eec\u9700\u8981\u5728\u2018SpellHit\u2018\u51fd\u6570\u4e2d\u7f16\u5199\u5177\u4f53\u7684 \u903b\u8f91\u3002\u4f8b\u5982\uff0c\u5f53\u6cd5\u672f\u7684ID\u4e3a\u2018SPELL_REMOVE_AMANI_CURSE\u2018\uff08\u60a8\u53ef\u4ee5\u81ea\u5b9a\u4e49\u8be5\u5e38\u91cf\u7684\u503c\uff09\u65f6\uff0c\u6211\u4eec\u68c0\u67e5 \u9752\u86d9\u662f\u5426\u5904\u4e8e\u8bc5\u5492\u72b6\u6001\u3002\u5982\u679c\u662f\uff0c\u6211\u4eec\u53ef\u4ee5\u751f\u6210\u4e00\u4e2a\u968f\u673a\u6570\uff0c\u6839\u636e\u8be5\u968f\u673a\u6570\u51b3\u5b9a\u9752\u86d9\u6062\u590d\u6210\u54ea\u4e2a\u89d2\u8272\uff0c\u4ee5\u53ca \u662f\u5426\u6389\u843d\u9053\u5177\u3002\u5177\u4f53\u4ee3\u7801\u5982\u4e0b\uff1a\n#include \"ScriptPCH.h\" #include \"zulaman.h\"\n#define SPELL_REMOVE_AMANI_CURSE 43732 #define SPELL_PUSH_MOJO 43923 #define ENTRY_FOREST_FROG 24396\nclass npc_forest_frog : public CreatureScript {\npublic: npc_forest_frog () : CreatureScript(\"npc_forest_frog\") {}\nstruct npc_forest_frogAI : public ScriptedAI {\n...\n\u5f53\u60a8\u7f16\u5199\u5b8c\u4ee3\u7801\u540e\uff0c\u53ef\u4ee5\u5c06\u8be5\u4ee3\u7801\u6dfb\u52a0\u5230\u6e38\u620f\u5f15\u64ce\u4e2d\uff0c\u5e76\u8fdb\u884c\u76f8\u5e94\u7684\u914d\u7f6e\u4ee5\u521b\u5efa\u60a8\u60f3\u8981\u7684\u6e38\u620f\u5173\u5361\u3002\n<user>\u90a3\u4e48\u5982\u679c\u6211\u60f3\u8ba9\u6389\u843d\u7684\u9053\u5177\u662f\u4f4e\u6982\u7387\u4e8b\u4ef6\u5e76\u8ba9\u73a9\u5bb6\u611f\u89c9\u66f4\u6709\u6311\u6218\u6027\uff0c\u6211\u5e94\u8be5\u5982\u4f55\u8c03\u6574\u4ee3\u7801\u5462\uff1f <assistant>\u8981\u8c03\u6574\u6389\u843d\u9053\u5177\u7684\u6982\u7387\uff0c\u60a8\u53ea\u9700\u4fee\u6539\u2018SpellHit\u2018\u51fd\u6570\u4e2d\u2018rand() % 100\u2018\u540e\u9762\u7684\u6570\u5b57\u3002\u8fd9\u4e2a\u6570\u5b57\u8868\u793a\u6389\u843d \u9053\u5177\u7684\u6982\u7387\u3002\u4f8b\u5982\uff0c\u5982\u679c\u60a8\u60f3\u8ba9\u6389\u843d\u9053\u5177\u7684\u6982\u7387\u964d\u4f4e\u523010%\uff0c\u53ef\u4ee5\u5c06\u5176\u4fee\u6539\u4e3a\uff1a\nif (rand() % 100 < 10) // 10% DoCast(caster , SPELL_PUSH_MOJO , true); else DoSpawnRandom ();\n...\n(One more turns are not shown because of the limited space. The whole dialogue contains about 1150 words.)\nTable 15: An example of English RefGPT-Code-cr.\nReference\npackage ch.zhaw.mapreduce.plugins.socket.impl;\nimport javax.inject.Inject; import ch.zhaw.mapreduce.CombinerInstruction; import ch.zhaw.mapreduce.MapInstruction; import ch.zhaw.mapreduce.ReduceInstruction;\n...\n<user> I am working on a project that involves different tasks like mapping, reducing, and combining. I need to create task runners that can execute these tasks dynamically based on the received instructions. I also need to differentiate between mapping and reducing tasks to execute them correctly. Can you help me create a Java code that can achieve this? <assistant> Certainly! Let\u2019s start by creating an interface called \u2018TaskRunnerFactory\u2018 with a single method \u2018createTaskRunner\u2018, that takes an \u2018AgentTask\u2018 object as input and returns a \u2018TaskRunner\u2018 object. Later, we\u2019ll implement the logic for differentiating between the mapping and reducing tasks. Here\u2019s the interface:\nimport javax.inject.Inject;\npublic final class TaskRunnerFactoryImpl implements TaskRunnerFactory { private final ByteArrayClassLoader classLoader = new ByteArrayClassLoader\n(); private final MapTaskRunnerFactory mtrFactory; private final ReduceTaskRunnerFactory rtrFactory;\n@Inject public TaskRunnerFactoryImpl(MapTaskRunnerFactory mtrFactory ,\nReduceTaskRunnerFactory rtrFactory) {\nthis .mtrFactory = mtrFactory; this .rtrFactory = rtrFactory;\n}\n@Override public TaskRunner createTaskRunner(AgentTask task) throws\nInvalidAgentTaskException { if (task instanceof MapAgentTask) {\n// Handle MapAgentTask } else if (task instanceof ReduceAgentTask) { // Handle ReduceAgentTask } else {\nthrow new InvalidAgentTaskException(\"Cannot Handle: \" + task) ;\n} }\n}\nIn this implementation, we first check if the \u2018AgentTask\u2018 is an instance of \u2018MapAgentTask\u2018 or \u2018ReduceAgentTask\u2018. Based on that, we\u2019ll execute the respective logic. We also use a custom \u2018ByteArrayClassLoader\u2018 to load the classes dynamically and call different getters based on the task type.\nInside the \u2018createTaskRunner()\u2018 method, we can now implement the logic to handle \u2018MapAgentTask\u2018 and \u2018ReduceAgentTask\u2018 instances:\n// For MapAgentTask MapAgentTask mt = (MapAgentTask) task; MapInstruction mapInstr = loadClass(mt.getMapInstructionName (), mt.\ngetMapInstruction (), MapInstruction. class);\n...\n... (Two more turns are not shown because of the limited space. The whole dialogue contains about 800 words.)\nTable 16: An example of Chinese RefGPT-Code-bg.\nReference\n... class BTree:\ndef __init__(self , value): self .root = Node(value)\n...\n<user> \u55e8Assistant\uff0c\u6211\u6700\u8fd1\u5728\u5b66\u4e60Python\uff0c\u73b0\u5728\u9047\u5230\u4e86\u4e00\u4e2a\u95ee\u9898\u3002\u6211\u5df2\u7ecf\u5199\u4e86\u4e00\u6bb5\u4ee3\u7801\uff0c\u4f46\u662f\u5b83\u8fd0\u884c\u4e0d\u8d77\u6765\uff0c\u80fd\u5e2e \u5fd9\u770b\u4e00\u4e0b\u662f\u54ea\u91cc\u51fa\u4e86\u95ee\u9898\u5417\uff1f\u4ee3\u7801\u5982\u4e0b\uff1a\n... class BTree: ...\ndef insert(self , value): def helper(cur_node , value):\nif value > cur_node.value: if cur_node.right:\ncur_node = cur_node.right helper (cur_node , value)\nelse : cur_node .right = Node(value)\nelse : if cur_node.left:\ncur_node = cur_node.left helper (cur_node , value)\nelse : cur_node .left = Node(value)\nhelper (self.root , value) ...\n\u8fd9\u6bb5\u4ee3\u7801\u662f\u6211\u521b\u5efa\u4e86\u4e00\u4e2a\u4e8c\u53c9\u641c\u7d22\u6811\uff0c\u5e76\u63d2\u5165\u4e86\u4e00\u4e9b\u8282\u70b9\uff0c\u4f46\u662f\u5f53\u6211\u8fd0\u884c\u8fd9\u6bb5\u4ee3\u7801\u65f6\uff0c\u5b83\u5361\u4f4f\u4e86\u3002\u6211\u60f3\u77e5\u9053 \u9519\u8bef\u5728\u54ea\u91cc\uff0c\u4ee5\u53ca\u5982\u4f55\u4fee\u590d\u5b83\u3002\u8c22\u8c22\uff01 <assistant> \u5f88\u9ad8\u5174\u80fd\u5e2e\u52a9\u4f60\u89e3\u51b3\u95ee\u9898\u3002\u9996\u5148\uff0c\u6211\u6ce8\u610f\u5230\u4f60\u7684\u4ee3\u7801\u4e2d\u5b58\u5728\u4e00\u4e2a\u5173\u4e8e\u9012\u5f52\u8c03\u7528\u7684\u95ee\u9898\u3002 \u5728\u2018insert\u2018\u65b9\u6cd5\u7684\u2018helper\u2018\u51fd\u6570\u4e2d\uff0c\u4f60\u9700\u8981\u5728\u9012\u5f52\u8c03\u7528\u4e4b\u524d\u4f7f\u7528return\u8bed\u53e5\u3002\u8bf7\u6ce8\u610f\uff0c\u5728\u8c03\u7528\u2018helper()\u2018\u51fd\u6570\u65f6\uff0c\u5e94 \u4f7f\u7528\u2018return helper(cur_node, value)\u2018\u800c\u4e0d\u662f\u2018helper(cur_node, value)\u2018\uff0c\u8fd9\u6837\u9012\u5f52\u8c03\u7528\u65f6\u624d\u80fd\u6b63\u786e\u8fd4\u56de\u3002\u8bd5\u8bd5\u4ee5\u4e0b \u4ee3\u7801\uff1a\n... class BTree: ...\ndef insert(self , value): def helper(cur_node , value):\nif value > cur_node.value: if cur_node.right:\ncur_node = cur_node.right return helper(cur_node , value)\nelse : cur_node .right = Node(value)\nelse : if cur_node.left:\ncur_node = cur_node.left return helper(cur_node , value)\nelse : cur_node .left = Node(value)\nreturn helper(self.root , value) ...\n\u73b0\u5728\u8fd9\u6bb5\u4ee3\u7801\u5e94\u8be5\u53ef\u4ee5\u6b63\u5e38\u5de5\u4f5c\u4e86\u3002\u5982\u679c\u4f60\u8fd8\u6709\u5176\u4ed6\u95ee\u9898\uff0c\u8bf7\u968f\u65f6\u544a\u8bc9\u6211\u3002\n(Three more turns are not shown because of the limited space. The whole dialogue contains about 1300 words.)\nTable 17: An example of English RefGPT-Code-bg.\nReference\n... (function($) {\nvar methods = { init : function(options) {\nvar defaults = {\nelement : { selector : \"\", shadowSize : \"30\", shadowColor : \"#222D3A\", shadowFade : \"0.3\", shadowAlpha : \"0.03\",\n}, ...\n<user> Hey, I was trying to implement a long shadow effect on a text element using jQuery, but I cannot seem to get the desired effect. I\u2019ve written this code, but it seems to have some bugs. Can you help me fix it? Here\u2019s the code:\n... function applyLongShadow(o) { ...\nfor (var i = 0; i <= o.textElement.shadowSize; i++) { var x = o.textElement.shadowSize - i; var y = o.textElement.shadowSize - i; var shadow = x + \"px \" + y + \"px \";\nif (o.textElement.shadowFade && i < fade) { shadow += xcolor.gradientlevel(darkerTX , txc , fadeText-i, fadeText)\n; ...\n<assistant> I see that you would like to apply a long shadow effect on a text element using jQuery. I noticed a couple of issues in your code that might be causing the undesired effect.\nFirstly, you are using \u2018xcolor.gradientlevel()\u2018 in your code, but you haven\u2019t included the \u2018xcolor\u2018 library or defined the variables \u2018darkerTX\u2018, \u2018txc\u2018, and \u2018fadeText\u2018. You can instead use the \u2018.xcolor.gradientlevel()\u2018methodfromthe\u2018.xcolor\u2018 library, and make sure to include the library in your project.\nHere\u2019s a corrected version of your code:\n... function applyLongShadow(o) { ...\nfor (var i = 0; i <= o.textElement.shadowSize; i++) { var x = o.textElement.shadowSize - i; var y = o.textElement.shadowSize - i; var shadow = x + \"px \" + y + \"px \";\nif (o.textElement.shadowFade && i < fade) { var tweenedTX = $.xcolor.gradientlevel(darkerTX , txc , fade - i, fade\n); ...\nPlease make sure to include the \u2018$.xcolor\u2018 library in your project, and let me know if you need further assistance.\n(One more turns are not shown because of the limited space. The whole dialogue contains about 550 words.)"
        },
        {
            "heading": "C Truthfulness Evaluation Examples",
            "text": ""
        }
    ],
    "title": "RefGPT: Dialogue Generation of GPT, by GPT, and for GPT",
    "year": 2023
}