{
    "abstractText": "Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism may be inevitable due to the choice of medium, conscious and unconscious design choices can guide users to personify such systems to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have investigated the factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be explored. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, including reinforcing gender stereotypes and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.",
    "authors": [
        {
            "affiliations": [],
            "name": "Gavin Abercrombie"
        },
        {
            "affiliations": [],
            "name": "Amanda Cercas Curry"
        },
        {
            "affiliations": [],
            "name": "Tanvi Dinkar"
        },
        {
            "affiliations": [],
            "name": "Zeerak Talat"
        },
        {
            "affiliations": [],
            "name": "Mohamed Bin Zayed"
        }
    ],
    "id": "SP:a8ca3385abc6274bd3b46df9ca42cd876a4d7072",
    "references": [
        {
            "authors": [
                "Gavin Abercrombie",
                "Amanda Cercas Curry",
                "Mugdha Pandya",
                "Verena Rieser."
            ],
            "title": "Alexa, Google, Siri: What are your pronouns? gender and anthropomorphism in the design and perception of conversational assistants",
            "venue": "Proceedings of the 3rd Workshop",
            "year": 2021
        },
        {
            "authors": [
                "Gavin Abercrombie",
                "Verena Rieser."
            ],
            "title": "Riskgraded safety for handling medical queries in conversational AI",
            "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International",
            "year": 2022
        },
        {
            "authors": [
                "Valentina Andries",
                "Judy Robertson"
            ],
            "title": "Alexa doesn\u2019t have that many feelings\u201d: Children\u2019s understanding of AI through interactions with smart speakers in their homes",
            "year": 2023
        },
        {
            "authors": [
                "Ian A Apperly."
            ],
            "title": "What is \u201ctheory of mind\u201d? Concepts, cognitive processes and individual differences",
            "venue": "Quarterly Journal of Experimental Psychology, 65(5):825\u2013839.",
            "year": 2012
        },
        {
            "authors": [
                "Theo Araujo."
            ],
            "title": "Living up to the chatbot hype: The influence of anthropomorphic design cues and communicative agency framing on conversational agent and company perceptions",
            "venue": "Computers in Human Behavior, 85:183\u2013189.",
            "year": 2018
        },
        {
            "authors": [
                "Michael Atleson"
            ],
            "title": "Chatbots, deepfakes, and voice clones: AI deception for sale",
            "year": 2023
        },
        {
            "authors": [
                "Matthew P. Aylett",
                "Selina Jeanne Sutton",
                "Yolanda Vazquez-Alvarez."
            ],
            "title": "The right kind of unnatural: Designing a robot voice",
            "venue": "Proceedings of the 1st International Conference on Conversational User Interfaces, CUI \u201919, New York, NY, USA. Association",
            "year": 2019
        },
        {
            "authors": [
                "Dale J Barr",
                "Mandana Seyfeddinipur."
            ],
            "title": "The role of fillers in listener attributions for speaker disfluency",
            "venue": "Language and Cognitive Processes, 25(4):441\u2013455.",
            "year": 2010
        },
        {
            "authors": [
                "Emily M. Bender",
                "Timnit Gebru",
                "Angelina McMillanMajor",
                "Shmargaret Shmitchell."
            ],
            "title": "On the dangers of stochastic parrots: Can language models be too big",
            "venue": "Proceedings of the 2021 ACM",
            "year": 2021
        },
        {
            "authors": [
                "Emily M. Bender",
                "Alexander Koller."
            ],
            "title": "Climbing towards NLU: On meaning, form, and understanding in the age of data",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185\u20135198, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Douglas Biber",
                "Susan Conrad."
            ],
            "title": "Register, Genre, and Style",
            "venue": "Cambridge Textbooks in Linguistics. Cambridge University Press.",
            "year": 2009
        },
        {
            "authors": [
                "Margaret Boden",
                "Joanna Bryson",
                "Darwin Caldwell",
                "Kerstin Dautenhahn",
                "Lilian Edwards",
                "Sarah Kember",
                "Paul Newman",
                "Vivienne Parry",
                "Geoff Pegman",
                "Tom Rodden",
                "Tom Sorrell",
                "Mick Wallis",
                "Blay Whitby",
                "Alan Winfield"
            ],
            "title": "Principles of robotics: regu",
            "year": 2017
        },
        {
            "authors": [
                "Pawe\u0142 Budzianowski",
                "Tsung-Hsien Wen",
                "Bo-Hsiang Tseng",
                "I\u00f1igo Casanueva",
                "Stefan Ultes",
                "Osman Ramadan",
                "Milica Ga\u0161i\u0107."
            ],
            "title": "MultiWOZ - a largescale multi-domain Wizard-of-Oz dataset for taskoriented dialogue modelling",
            "venue": "Proceedings of the",
            "year": 2018
        },
        {
            "authors": [
                "California State Legislature."
            ],
            "title": "California Senate Bill no",
            "venue": "1001. Technical report, California State Legislature.",
            "year": 2018
        },
        {
            "authors": [
                "Marco Casadio",
                "Luca Arnaboldi",
                "Matthew L. Daggitt",
                "Omri Isac",
                "Tanvi Dinkar",
                "Daniel Kienitz",
                "Verena Rieser",
                "Ekaterina Komendantskaya"
            ],
            "title": "Antonio: Towards a systematic method of generating NLP benchmarks for verification",
            "year": 2023
        },
        {
            "authors": [
                "Justine Cassell",
                "Alastair Gill",
                "Paul Tepper."
            ],
            "title": "Coordination in conversation and rapport",
            "venue": "Proceedings of the Workshop on Embodied Language Processing, pages 41\u201350, Prague, Czech Republic. Association for Computational Linguistics.",
            "year": 2007
        },
        {
            "authors": [
                "Stephen Cave",
                "Kanta Dihal."
            ],
            "title": "The Whiteness of AI",
            "venue": "Philosophy & Technology, 33(4):685\u2013703.",
            "year": 2020
        },
        {
            "authors": [
                "Alba Cercas Curry",
                "Amanda Cercas Curry."
            ],
            "title": "Computer says \u201cno\u201d: The case against empathetic conversational AI",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 8123\u20138130, Toronto, Canada. Association for Com-",
            "year": 2023
        },
        {
            "authors": [
                "Amanda Cercas Curry",
                "Gavin Abercrombie",
                "Verena Rieser."
            ],
            "title": "ConvAbuse: Data, analysis, and benchmarks for nuanced abuse detection in conversational AI",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Amanda Cercas Curry",
                "Verena Rieser."
            ],
            "title": "MeToo Alexa: How conversational systems respond to sexual harassment",
            "venue": "Proceedings of the Second ACL Workshop on Ethics in Natural Language Processing, pages 7\u201314, New Orleans, Louisiana, USA.",
            "year": 2018
        },
        {
            "authors": [
                "den",
                "Wanru Zhao",
                "Shalaleh Rismani",
                "Konstantinos Voudouris",
                "Umang Bhatt",
                "Adrian Weller",
                "David Krueger",
                "Tegan Maharaj"
            ],
            "title": "Harms from increasingly agentic algorithmic systems",
            "venue": "In Proceedings of the 2023 ACM Conference on Fairness,",
            "year": 2023
        },
        {
            "authors": [
                "Sabrina Chiesurin",
                "Dimitris Dimakopoulos",
                "Arash Eshghi",
                "Ioannis Papaioannou",
                "Verena Rieser",
                "Ioannis Konstas"
            ],
            "title": "The dangers of trusting stochastic parrots: Faithfulness and trust in open-domain conversational",
            "year": 2023
        },
        {
            "authors": [
                "Herbert H Clark",
                "Kerstin Fischer."
            ],
            "title": "Social robots as depictions of social agents",
            "venue": "Behavioral and Brain Sciences, 46:e21.",
            "year": 2023
        },
        {
            "authors": [
                "Herbert H Clark",
                "Jean E Fox Tree."
            ],
            "title": "Using uh and um in spontaneous speaking",
            "venue": "Cognition, 84(1):73\u2013111.",
            "year": 2002
        },
        {
            "authors": [
                "Mariona Coll Ardanuy",
                "Federico Nanni",
                "Kaspar Beelen",
                "Kasra Hosseini",
                "Ruth Ahnert",
                "Jon Lawrence",
                "Katherine McDonough",
                "Giorgia Tolfo",
                "Daniel CS Wilson",
                "Barbara McGillivray."
            ],
            "title": "Living machines: A study of atypical animacy",
            "venue": "Proceedings of the 28th",
            "year": 2020
        },
        {
            "authors": [
                "Martin Corley",
                "Lucy J MacGregor",
                "David I Donaldson."
            ],
            "title": "It\u2019s the way that you, er, say it: Hesitations in speech affect language comprehension",
            "venue": "Cognition, 105(3):658\u2013668.",
            "year": 2007
        },
        {
            "authors": [
                "David Crystal."
            ],
            "title": "A First Dictionary of Linguistics and Phonetics",
            "venue": "Language library. John Wiley & Sons, Incorporated.",
            "year": 1980
        },
        {
            "authors": [
                "Andreea Danielescu",
                "Sharone A Horowit-Hendler",
                "Alexandria Pabst",
                "Kenneth Michael Stewart",
                "Eric M Gallo",
                "Matthew Peter Aylett."
            ],
            "title": "Creating inclusive voices for the 21st century: A non-binary text-to-speech for conversational assistants",
            "venue": "Pro-",
            "year": 2023
        },
        {
            "authors": [
                "Boele De Raad."
            ],
            "title": "The big five personality factors: The psycholexical approach to personality",
            "venue": "Hogrefe & Huber Publishers.",
            "year": 2000
        },
        {
            "authors": [
                "Virginia Dignum",
                "Melanie Penagos",
                "Klara Pigmans",
                "Steven Vosloo."
            ],
            "title": "Policy guidance on AI for children: Recommendations for building AI policies and systems that uphold child rights",
            "venue": "Report, UNICEF.",
            "year": 2021
        },
        {
            "authors": [
                "Emily Dinan",
                "Gavin Abercrombie",
                "A. Bergman",
                "Shannon Spruit",
                "Dirk Hovy",
                "Y-Lan Boureau",
                "Verena Rieser."
            ],
            "title": "SafetyKit: First aid for measuring safety in open-domain conversational systems",
            "venue": "Proceedings of the 60th Annual Meeting of the As-",
            "year": 2022
        },
        {
            "authors": [
                "Tanvi Dinkar",
                "Chlo\u00e9 Clavel",
                "Ioana Vasilescu."
            ],
            "title": "Fillers in spoken language understanding: Computational and psycholinguistic perspectives",
            "venue": "arXiv preprint arXiv:2301.10761.",
            "year": 2023
        },
        {
            "authors": [
                "Brian R. Duffy."
            ],
            "title": "Anthropomorphism and the social robot",
            "venue": "Robotics and Autonomous Systems, 42(3):177\u2013190. Socially Interactive Robots.",
            "year": 2003
        },
        {
            "authors": [
                "Nicholas Epley",
                "Adam Waytz",
                "John T. Cacioppo."
            ],
            "title": "On seeing human: A three-factor theory of anthropomorphism",
            "venue": "Psychological Review, 114.",
            "year": 2007
        },
        {
            "authors": [
                "Liz W. Faber."
            ],
            "title": "The Computer\u2019s Voice: From Star Trek to Siri",
            "venue": "University of Minnesota Press.",
            "year": 2020
        },
        {
            "authors": [
                "BJ Fogg",
                "Clifford Nass."
            ],
            "title": "How users reciprocate to computers: An experiment that demonstrates behavior change",
            "venue": "CHI \u201997 Extended Abstracts on Human Factors in Computing Systems, CHI EA \u201997, page 331\u2013332. Association for Computing Machin-",
            "year": 1997
        },
        {
            "authors": [
                "Mary Ellen Foster",
                "Jane Stuart-Smith."
            ],
            "title": "Social robotics meets sociolinguistics: Investigating accent bias and social context in HRI",
            "venue": "Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction, HRI \u201923, page 156\u2013160,",
            "year": 2023
        },
        {
            "authors": [
                "Scott H. Fraundorf",
                "Jennifer Arnold",
                "Valerie J. Langlois."
            ],
            "title": "Disfluency",
            "venue": "https://www.oxfordbibliographies. com/view/document/obo-9780199772810/ obo-9780199772810-0189.xml. Oxford Univer-",
            "year": 2018
        },
        {
            "authors": [
                "H.P. Grice"
            ],
            "title": "Utterer\u2019s meaning, sentence-meaning",
            "year": 1988
        },
        {
            "authors": [
                "ward",
                "Dimitri Williams"
            ],
            "title": "It\u2019s kind of like",
            "year": 2022
        },
        {
            "authors": [
                "Liwei Jiang",
                "Jena D. Hwang",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Jenny Liang",
                "Jesse Dodge",
                "Keisuke Sakaguchi",
                "Maxwell Forbes",
                "Jon Borchardt",
                "Saadia Gabriel",
                "Yulia Tsvetkov",
                "Oren Etzioni",
                "Maarten Sap",
                "Regina Rini",
                "Yejin Choi"
            ],
            "title": "Can machines",
            "year": 2022
        },
        {
            "authors": [
                "Pratik Joshi",
                "Sebastin Santy",
                "Amar Budhiraja",
                "Kalika Bali",
                "Monojit Choudhury."
            ],
            "title": "The state and fate of linguistic diversity and inclusion in the NLP world",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Hyunwoo Kim",
                "Youngjae Yu",
                "Liwei Jiang",
                "Ximing Lu",
                "Daniel Khashabi",
                "Gunhee Kim",
                "Yejin Choi",
                "Maarten Sap."
            ],
            "title": "ProsocialDialog: A prosocial backbone for conversational agents",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Nat-",
            "year": 2022
        },
        {
            "authors": [
                "Youjeong Kim",
                "S Shyam Sundar"
            ],
            "title": "Anthropomorphism of computers: Is it mindful or mindless? Computers in Human Behavior, 28(1):241\u2013250",
            "year": 2012
        },
        {
            "authors": [
                "Ambika Kirkland",
                "Harm Lameris",
                "Eva Sz\u00e9kely",
                "Joakim Gustafson."
            ],
            "title": "Where\u2019s the uh, hesitation? The interplay between filled pause location, speech rate and fundamental frequency in perception of confidence",
            "venue": "Proc. Interspeech 2022, pages 4990\u20134994.",
            "year": 2022
        },
        {
            "authors": [
                "Naomi Klein."
            ],
            "title": "AI machines aren\u2019t \u2018hallucinating",
            "venue": "But their makers are. https://www. theguardian.com/commentisfree/2023/may/ 08/ai-machines-hallucinating-naomi-klein. The Guardian. Accessed: 2023-05-11.",
            "year": 2023
        },
        {
            "authors": [
                "Tua Korhonen."
            ],
            "title": "Anthropomorphism and the aesopic animal fables",
            "venue": "Animals and their Relation to Gods, Humans and Things in the Ancient World, pages 211\u2013231.",
            "year": 2019
        },
        {
            "authors": [
                "Robert M. Krauss",
                "Robin J. Freyberg",
                "Ezequiel Morsella."
            ],
            "title": "Inferring speakers\u2019 physical attributes from their voices",
            "venue": "Journal of Experimental Social Psychology, 38:618\u2013625.",
            "year": 2002
        },
        {
            "authors": [
                "Brenda Leong",
                "Evan Selinger."
            ],
            "title": "Robot eyes wide shut: Understanding dishonest anthropomorphism",
            "venue": "Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* \u201919, page 299\u2013308, New York, NY, USA. Association for",
            "year": 2019
        },
        {
            "authors": [
                "Yaniv Leviathan",
                "Yossi Matias."
            ],
            "title": "Google Duplex: An AI system for accomplishing real world tasks over the phone",
            "venue": "Google AI Blog.",
            "year": 2018
        },
        {
            "authors": [
                "Johnny Lieu."
            ],
            "title": "Google\u2019s creepy AI phone call feature will disclose it\u2019s a robot, after backlash",
            "venue": "https://mashable.com/2018/05/11/ google-duplex-disclosures-robot. Mashable. Accessed 2023-03-16.",
            "year": 2018
        },
        {
            "authors": [
                "Jessa Lingel",
                "Kate Crawford."
            ],
            "title": "Alexa, tell me about your mother\u201d: The history of the secretary and the end of secrecy",
            "venue": "Catalyst: Feminism, Theory, Technoscience, 6(1).",
            "year": 2020
        },
        {
            "authors": [
                "Fanjue Liu."
            ],
            "title": "Hanging out with my pandemic pal: Contextualizing motivations of anthropomorphizing voice assistants during COVID-19",
            "venue": "Journal of Promotion Management, pages 1\u201329.",
            "year": 2022
        },
        {
            "authors": [
                "Pierre-Fran\u00e7ois Lovens."
            ],
            "title": "Sans ces conversations avec le chatbot Eliza, mon mari serait toujours l\u00e0",
            "venue": "https://www.lalibre.be/belgique/societe/2023/03/28/s ans-ces-conversations-avec-le-chatboteliza-mon-mari-serait-toujours-la-",
            "year": 2023
        },
        {
            "authors": [
                "Amama Mahmood",
                "Jeanie W Fung",
                "Isabel Won",
                "Chien-Ming Huang."
            ],
            "title": "Owning mistakes sincerely: Strategies for mitigating AI errors",
            "venue": "Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, CHI \u201922, New York,",
            "year": 2022
        },
        {
            "authors": [
                "Shikib Mehri",
                "Jinho Choi",
                "Luis Fernando D\u2019Haro",
                "Jan Deriu",
                "Maxine Eskenazi",
                "Milica Gasic",
                "Kallirroi Georgila",
                "Dilek Hakkani-Tur",
                "Zekang Li",
                "Verena Rieser"
            ],
            "title": "Report from the NSF future directions workshop on automatic evaluation of dialog",
            "year": 2022
        },
        {
            "authors": [
                "Cade Metz."
            ],
            "title": "Riding out quarantine with a chatbot friend: \u2018I feel very connected",
            "venue": "https: //www.nytimes.com/2020/06/16/technology/ chatbots-quarantine-coronavirus.html. New York Times. Accessed: 2023-04-25.",
            "year": 2020
        },
        {
            "authors": [
                "Sabrina J. Mielke",
                "Arthur Szlam",
                "Emily Dinan",
                "YLan Boureau."
            ],
            "title": "Reducing conversational agents\u2019 overconfidence through linguistic calibration",
            "venue": "Transactions of the Association for Computational Linguistics, 10:857\u2013872.",
            "year": 2022
        },
        {
            "authors": [
                "Marvin Minsky."
            ],
            "title": "The Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind",
            "venue": "Simon and Schuster.",
            "year": 2006
        },
        {
            "authors": [
                "Nicole Mirnig",
                "Gerald Stollnberger",
                "Markus Miksch",
                "Susanne Stadler",
                "Manuel Giuliani",
                "Manfred Tscheligi."
            ],
            "title": "To err is robot: How humans assess and act toward an erroneous social robot",
            "venue": "Frontiers in Robotics and AI, page 21.",
            "year": 2017
        },
        {
            "authors": [
                "Taylor C. Moran."
            ],
            "title": "Racial technological bias and the white, feminine voice of AI VAs",
            "venue": "Communication and Critical/Cultural Studies, 18(1):19\u201336.",
            "year": 2021
        },
        {
            "authors": [
                "Clifford Ivar Nass",
                "Scott Brave."
            ],
            "title": "Wired for speech: How voice activates and advances the human-computer relationship",
            "venue": "MIT press Cambridge.",
            "year": 2005
        },
        {
            "authors": [
                "Harold W. Noonan."
            ],
            "title": "The thinking animal problem and personal pronoun revisionism",
            "venue": "Analysis, 70(1):93\u201398.",
            "year": 2009
        },
        {
            "authors": [
                "Eric T Olson."
            ],
            "title": "Thinking animals and the reference of \u2018I",
            "venue": "Philosophical Topics, 30(1):189\u2013207.",
            "year": 2002
        },
        {
            "authors": [
                "Sihem Omri",
                "Manel Abdelkader",
                "Mohamed Hamdi",
                "Tai-Hoon Kim."
            ],
            "title": "Safety issues investigation in deep learning based chatbots answers to medical advice requests",
            "venue": "Neural Information Processing, pages 597\u2013605, Singapore. Springer Nature Singa-",
            "year": 2023
        },
        {
            "authors": [
                "Diane Proudfoot."
            ],
            "title": "Anthropomorphism and AI: Turing\u2019s much misunderstood imitation game",
            "venue": "Artificial Intelligence, 175(5):950\u2013957. Special Review Issue.",
            "year": 2011
        },
        {
            "authors": [
                "S.G. Pulman."
            ],
            "title": "Conversational games, belief revision and Bayesian networks",
            "venue": "CLIN VII: Proceedings of 7th Computational Linguistics in the Netherlands meeting, Nov 1996, pages 1\u201325.",
            "year": 1997
        },
        {
            "authors": [
                "Abhilasha Ravichander",
                "Alan W. Black."
            ],
            "title": "An Empirical Study of Self-Disclosure in Spoken Dialogue Systems",
            "venue": "Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue, pages 253\u2013263, Melbourne, Australia. Association",
            "year": 2018
        },
        {
            "authors": [
                "Byron Reeves",
                "Clifford Nass."
            ],
            "title": "The Media Equation: How People Treat Computers, Television, and New Media like Real People",
            "venue": "Cambridge university press Cambridge, UK.",
            "year": 1996
        },
        {
            "authors": [
                "Arleen Salles",
                "Kathinka Evers",
                "Michele Farisco."
            ],
            "title": "Anthropomorphism in AI",
            "venue": "AJOB Neuroscience, 11(2):88\u201395. PMID: 32228388.",
            "year": 2020
        },
        {
            "authors": [
                "Maarten Sap",
                "Ronan Le Bras",
                "Daniel Fried",
                "Yejin Choi."
            ],
            "title": "Neural theory-of-mind? On the limits of social intelligence in large LMs",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3762\u20133780, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Roger Scruton."
            ],
            "title": "On human nature",
            "venue": "On Human Nature. Princeton University Press.",
            "year": 2017
        },
        {
            "authors": [
                "Chirag Shah",
                "Emily M. Bender."
            ],
            "title": "Situating search",
            "venue": "ACM SIGIR Conference on Human Information Interaction and Retrieval, CHIIR \u201922, page 221\u2013232, New York, NY, USA. Association for Computing Machinery.",
            "year": 2022
        },
        {
            "authors": [
                "Murray Shanahan"
            ],
            "title": "Talking about large language models",
            "year": 2023
        },
        {
            "authors": [
                "Victor Kenji M. Shiramizu",
                "Anthony J. Lee",
                "Daria Altenburg",
                "David R. Feinberg",
                "Benedict C. Jones."
            ],
            "title": "The role of valence, dominance, and pitch in perceptions of artificial intelligence (AI) conversational agents\u2019 voices",
            "venue": "Scientific Reports,",
            "year": 2022
        },
        {
            "authors": [
                "Gabriel Skantze",
                "Martin Johansson",
                "Jonas Beskow."
            ],
            "title": "Exploring turn-taking cues in multi-party human-robot discussions about objects",
            "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction, pages 67\u201374.",
            "year": 2015
        },
        {
            "authors": [
                "Vicki L Smith",
                "Herbert H Clark."
            ],
            "title": "On the course of answering questions",
            "venue": "Journal of Memory and Language, 32(1):25\u201338.",
            "year": 1993
        },
        {
            "authors": [
                "Julia Stern",
                "Christoph Schild",
                "Benedict C. Jones",
                "Lisa M. DeBruine",
                "Amanda Hahn",
                "David A. Puts",
                "Ingo Zettler",
                "Tobias L. Kordsmeyer",
                "David Feinberg",
                "Dan Zamfir",
                "Lars Penke",
                "Ruben C. Arslan"
            ],
            "title": "Do voices carry valid information about a speaker\u2019s",
            "year": 2021
        },
        {
            "authors": [
                "Louis Stupple-Harris."
            ],
            "title": "Tech in the dock",
            "venue": "Should AI chatbots be used to address the nation\u2019s loneliness problem? https://www.nesta.org.uk/feature/ tech-dock. NESTA. Accessed: 2023-05-11.",
            "year": 2021
        },
        {
            "authors": [
                "Hao Sun",
                "Guangxuan Xu",
                "Jiawen Deng",
                "Jiale Cheng",
                "Chujie Zheng",
                "Hao Zhou",
                "Nanyun Peng",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "On the safety of conversational models: Taxonomy, dataset, and benchmark",
            "venue": "Findings of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Selina Jeanne Sutton."
            ],
            "title": "Gender ambiguous, not genderless: Designing gender in voice user interfaces (VUIs) with sensitivity",
            "venue": "Proceedings of the 2nd Conference on Conversational User Interfaces, CUI \u201920, New York, NY, USA. Association for Computing",
            "year": 2020
        },
        {
            "authors": [
                "Ekaterina Svikhnushina",
                "Iuliana Voinea",
                "Anuradha Welivita",
                "Pearl Pu."
            ],
            "title": "A taxonomy of empathetic questions in social dialogs",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Zeerak Talat",
                "Hagen Blix",
                "Josef Valvoda",
                "Maya Indira Ganesh",
                "Ryan Cotterell",
                "Adina Williams."
            ],
            "title": "On the machine learning of ethical judgments from natural language",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Asso-",
            "year": 2022
        },
        {
            "authors": [
                "Rachael Tatman."
            ],
            "title": "Gender and dialect bias in YouTube\u2019s automatic captions",
            "venue": "Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, pages 53\u201359, Valencia, Spain. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Ilaria Torre",
                "S\u00e9bastien Le Maguer"
            ],
            "title": "Should robots have accents",
            "venue": "In 2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),",
            "year": 2020
        },
        {
            "authors": [
                "David R. Traum",
                "Staffan Larsson."
            ],
            "title": "The Information State Approach to Dialogue Management, pages 325\u2013353",
            "venue": "Springer Netherlands, Dordrecht.",
            "year": 2003
        },
        {
            "authors": [
                "UNESCO."
            ],
            "title": "Explore the gendering of AI voice assistants",
            "venue": "https://es.unesco.org/node/305128. UNESCO. Accessed: 2023-04-25.",
            "year": 2019
        },
        {
            "authors": [
                "Carissa V\u00e9liz."
            ],
            "title": "Moral zombies: why algorithms are not moral agents",
            "venue": "AI & Society, 36.",
            "year": 2021
        },
        {
            "authors": [
                "Sarah Theres V\u00f6lkel",
                "Ramona Sch\u00f6del",
                "Daniel Buschek",
                "Clemens Stachl",
                "Verena Winterhalter",
                "Markus B\u00fchner",
                "Heinrich Hussmann."
            ],
            "title": "Developing a personality model for speech-based conversational agents using the psycholexical approach",
            "venue": "Proceed-",
            "year": 2020
        },
        {
            "authors": [
                "Katja Wagner",
                "Frederic Nimmermann",
                "Hanna Schramm-Klein."
            ],
            "title": "Is it human? The role of anthropomorphism as a driver for the successful acceptance of digital voice assistants",
            "venue": "Proceedings of the 52nd Hawaii International Conference on System",
            "year": 2019
        },
        {
            "authors": [
                "Toby Walsh."
            ],
            "title": "Turing\u2019s red flag",
            "venue": "Communications of the ACM, 59(7):34\u201337.",
            "year": 2016
        },
        {
            "authors": [
                "Shensheng Wang",
                "Scott O. Lilienfeld",
                "Philippe Rochat."
            ],
            "title": "The uncanny valley: Existence and explanations",
            "venue": "Review of General Psychology, 19(4):393\u2013407.",
            "year": 2015
        },
        {
            "authors": [
                "Mark West",
                "Rebecca Kraut",
                "Han Ei Chew."
            ],
            "title": "I\u2019d Blush if I Could: Closing Gender Divides in Digital Skills through Education",
            "venue": "UNESCO.",
            "year": 2019
        },
        {
            "authors": [
                "Sarah Wilson",
                "Roger K. Moore."
            ],
            "title": "Robot, alien and cartoon voices: Implications for speech-enabled systems",
            "venue": "1st International Workshop on Vocal Interactivity in-and-between Humans, Animals and Robots (VIHAR), page 42\u201346.",
            "year": 2017
        },
        {
            "authors": [
                "Charlotte Wollermann",
                "Eva Lasarcyk",
                "Ulrich Schade",
                "Bernhard Schr\u00f6der."
            ],
            "title": "Disfluencies and uncertainty perception\u2013evidence from a human\u2013 machine scenario",
            "venue": "Sixth Workshop on Disfluency in Spontaneous Speech.",
            "year": 2013
        },
        {
            "authors": [
                "Jing Xu",
                "Da Ju",
                "Margaret Li",
                "Y-Lan Boureau",
                "Jason Weston",
                "Emily Dinan"
            ],
            "title": "Recipes for safety in open-domain chatbots",
            "year": 2021
        },
        {
            "authors": [
                "Mutsumi Yamamoto."
            ],
            "title": "Animacy and Reference: A Cognitive Approach to Corpus Linguistics",
            "venue": "J. Benjamins.",
            "year": 1999
        },
        {
            "authors": [
                "Saizheng Zhang",
                "Emily Dinan",
                "Jack Urbanek",
                "Arthur Szlam",
                "Douwe Kiela",
                "Jason Weston"
            ],
            "title": "Personalizing dialogue agents: I have a dog, do you have pets too",
            "year": 2018
        },
        {
            "authors": [
                "Ling.Yu Zhu",
                "Zhengkun Zhang",
                "Jun Wang",
                "Hongbin Wang",
                "Haiying Wu",
                "Zhenglu Yang."
            ],
            "title": "Multiparty empathetic dialogue generation: A new task for dialog systems",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Caleb Ziems",
                "Jane Yu",
                "Yi-Chia Wang",
                "Alon Halevy",
                "Diyi Yang."
            ],
            "title": "The moral integrity corpus: A benchmark for ethical dialogue systems",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism may be inevitable due to the choice of medium, conscious and unconscious design choices can guide users to personify such systems to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have investigated the factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be explored. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, including reinforcing gender stereotypes and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users."
        },
        {
            "heading": "1 Introduction",
            "text": "Automated dialogue or \u2018conversational AI\u2019 systems are increasingly being introduced to the fabric of society, and quickly becoming ubiquitous. As the capabilities of such systems increase, so does the risk that their outputs are mistaken for humanproductions, and that they are anthropomorphised and personified by people (UNESCO, 2019). Assigning human characteristics to dialogue systems can have consequences ranging from the relatively benign, e.g. referring to automated systems by gender (Abercrombie et al., 2021), to the disastrous, e.g. people following the advice or instructions of a\n\u2217Equal contribution. \u2020Now at Google DeepMind.\nsystem to do harm (Dinan et al., 2022).1 It is therefore important to consider how dialogue systems are designed and presented in order to mitigate risks associated with their introduction to society .\nRecognising such dangers, legislation has been passed to prohibit automated voice systems from presenting as humans (California State Legislature, 2018) and pre-existing legislation on deceptive trade practices may also apply (Atleson, 2023). Research has also called for wider regulation, e.g. requiring explicit (red) flagging of automated systems (Walsh, 2016) or clarification of the machine nature of manufactured items (Boden et al., 2017).\nWhile some developers seek to limit anthropomorphic cues in system outputs (e.g. Glaese et al., 2022), user engagement can be a strong motivation for creating humanlike systems (Araujo, 2018; Wagner et al., 2019). As a result, despite appearing\n1While high performing dialogue systems have only recently been introduced to the public domain, there has already been a case of a person committing suicide, allegedly as a consequence of interaction with such a system (Lovens, 2023).\nto be controlled for such cues, the outputs of systems often retain many anthropomorphic linguistic features, as shown in Figure 1.\nIn this position paper, we make a normative argument against gratuitous anthropomorphic features, grounded in findings from psychology, linguistics, and human-computer interaction: We (i) outline the psychological mechanisms and (ii) linguistic factors that contribute to anthropomorphism and personification, e.g. self-referential personal pronoun use, or generating content which gives the appearance of systems having empathy; and (iii) discuss the consequences of anthropomorphism.\nWe conclude with recommendations that can aid in minimising anthropomorphism, thus providing a path for safer dialogue systems and avoiding the creation of mirages of humanity."
        },
        {
            "heading": "2 Anthropomorphism",
            "text": "Anthropomorphism refers to attributing human characteristics or behaviour to non-human entities, e.g. animals or objects. Humans have a long history of anthropomorphising non-humans. For example, Aesop\u2019s fables depict animals reasoning, thinking and even talking like humans (Korhonen, 2019). While Aesop used personification to highlight the fictional character of animals, when applied to machines, anthropomorphism can increase user engagement (Wagner et al., 2019), reciprocity (Fogg and Nass, 1997), along with more pragmatic factors such as hedonic motivation, price value, and habit. For example, self-disclosure from a system, even when \u2018patently disingenuous\u2019, inspires reciprocity from the user (Kim and Sundar, 2012; Ravichander and Black, 2018). By encouraging such types of engagements, developers can foster greater connection between people and systems, which increases user satisfaction (Araujo, 2018), and plays an important role in systems becoming widely accepted and adopted.2 This is why, automated evaluations often assess the \u2018human-likeness\u2019 of a response (Mehri et al., 2022). Thus, developers are incentivised to engage with anthropomorphism to stimulate people to create deeper emotional connections with systems that cannot reciprocate.\n2Neighbouring disciplines, e.g. social robotics, also argue that some degree of anthropomorphism can enable more natural and intuitive interaction with robots (Duffy, 2003). However, a counterpoint offered to this is the \u2018uncanny valley\u2019 effect, i.e. the positive effects of anthropomorphism can decline sharply when artificial entities fail to mimic realistic human behaviour and appearance (Wang et al., 2015).\nIn the rest of this section, we discuss human and system factors that contribute towards placement of systems on the anthropomorphic continuum."
        },
        {
            "heading": "2.1 Human Factors",
            "text": "Research has shown that the process of anthropomorphising is mostly mindless (Kim and Sundar, 2012): it does not reflect the user\u2019s thoughtful belief that a computer has human characteristics, but rather it is automatic and encouraged by cues in their interfaces. According to Epley et al. (2007) anthropomorphism may be a default behaviour, which is corrected as people acquire more knowledge about an object. They further argue that on a cognitive level, humans anchor their knowledge to their own experiences and indiscriminately apply it to inanimate objects\u2014in order to make sense of a being or artefact, we map our own lived experiences onto it and assume they experience the world in the same way we do. That is, anthropocentric knowledge is easily accessible and applicable, but applications of it can be corrected with greater knowledge of the object. This may explain why the tendency to anthropomorphise is strongest in childhood, as adults have more knowledge about the world. This cognitive phenomenon is then compounded by two motivational determinants: effectance and sociality (Epley et al., 2007).\nEffectance refers to the need to interact efficiently with one\u2019s environment. By anthropomorphising systems we ascribe them (humanlike) intentionality which, in turn, reduces uncertainty and increases confidence in our ability to predict a system\u2019s behaviour. Sociality, on the other hand, refers to the need to establish connections with other humans, which can prime us to mentally construct systems as humanlike to fulfil a need for social connection. People suffering from chronic loneliness, a lack of social connection, or attachment issues may be more prone to anthropomorphising objects (Epley et al., 2007). For these reasons, dialogue systems have been proposed as a remedy for the loneliness epidemic (Stupple-Harris, 2021). For instance, commercial virtual companion developers such as Replika.ai saw rises in product uptake in 2020 due to social safety measures such as forced isolation (Liu, 2022; Metz, 2020).\nWhile these elements of the human psyche explain our inclination to personify systems, Epley et al.\u2019s theory does not speak to the qualities of the artefacts themselves that make them anthropomor-\nphic and more prone to be personified."
        },
        {
            "heading": "2.2 Agent Factors",
            "text": "There is no necessary and sufficient condition for a system to be anthropomorphic, i.e. there exist no particular threshold that affords a binary classification of whether a system is anthropomorphic or not, instead anthropomorphism exists on a spectrum. At the most basic level, systems are anthropomorphic if they (i) are interactive, (ii) use language, and (iii) take on a role performed by a human (Chan et al., 2023; Reeves and Nass, 1996). While these characteristics are inherent to dialogue systems, not all systems are equally humanlike.\nWe can draw a parallel with humanness here. Rather than a single factor which makes humans human, Scruton (2017, p. 31) argues that humanity is emergent: each individual element does not make a human but collectively they make up the language of humanness. Scruton (2017) compares it to a portrait, in which an artist paints areas and lines to compose a face; when observing the canvas, in addition to those marks, we see a face:\nAnd the face is really there: someone who does not see it is not seeing correctly [...] as soon as the lines and blobs are there, so is the face.\nSimilarly, no single attribute or capability makes a system anthropomorphic. Rather, each contributes to the painting until \u2018the face\u2019 emerges. Modern dialogue systems display a plethora of other characteristics that make space for anthropomorphism, e.g. having personas, first names, and supposed preferences. The more of such elements a system has, the more humanlike it appears."
        },
        {
            "heading": "3 Linguistic Factors",
            "text": "Prior research has attended to anthropomorphic design features of dialogue system, e.g. gendered names and avatars (West et al., 2019) and ChatGPT\u2019s animated \u2018three dots\u2019 and word-by-word staggered outputs, which give an impression that the system is thinking (Venkatasubramonian in Goldman, 2023). Here, we outline the linguistic factors that engender personification that have been given less consideration, e.g. voice qualities and speech, content, or style of outputs.3\n3We do not discuss physically embodied robots in this work. Instead, we refer readers to Clark and Fischer (2023)."
        },
        {
            "heading": "3.1 Voice",
            "text": "While not all dialogue systems are equipped with a voice, merely having one can be interpreted as an expression of personhood (Faber, 2020). Indeed, West et al. (2019) argue that the increased realism of voice is a primary factor contributing to anthropomorphism of dialogue assistants. For instance, based on voice, listeners may infer physical attributes, e.g. height, weight, and age (Krauss et al., 2002); personality traits, e.g. dominance, extroversion, and socio-sexuality (Stern et al., 2021); and human characteristics, e.g. gender stereotypes, personality (Shiramizu et al., 2022), and emotion learned from psychological and social behaviours in human-human communication (Nass and Brave, 2005). This means that humans have a proclivity to assert assumptions of speaker\u2019s embodiment, and human characteristics based on their voice alone. Thus, the absence of embodiment affords people to personify systems provided with synthetic voices (Aylett et al., 2019)\u2014a point acknowledged by developers of commercial dialogue systems (Google Assistant).\nProsody: Tone and Pitch There exist many vocal manipulation techniques that can influence which personality users attribute to a dialogue system. For example, Wilson and Moore (2017) found that a variety of fictional robot, alien, and cartoon voices had manipulated voice characteristics (e.g. breathiness, creakiness, echoes, reverberations) to better fit their desired character. However, they note that \u2018the voices of speech-enabled artefacts in the non-fictional world [...] invariably sound humanlike, despite the risk that users might be misled about the capabilities of the underlying technology\u2019 (Wilson and Moore, 2017, p.42).\nDisfluencies People rarely speak in the same manner with which they write: they are in general disfluent, that is, they insert elements that break the fluent flow of speech, such as interrupting themselves, repetitions, and hesitations (\u2018um\u2019, \u2018uh\u2019) (Fraundorf et al., 2018). Such disfluencies are perceived by the listeners as communicative signals, regardless of the speaker\u2019s intent (see Barr and Seyfeddinipur, 2010; Clark and Fox Tree, 2002; Corley et al., 2007; Smith and Clark, 1993).\nResearch has therefore sought to integrate disfluencies into text-to-speech (TTS) systems, where they have proven to be a useful strategy for buying time (Skantze et al., 2015), i.e. to allow the system\nto determine the next step. A person\u2019s perception of confidence towards the system\u2019s response may decrease due to disfluency (Kirkland et al., 2022; Wollermann et al., 2013), and they may therefore be a useful mitigation strategy to tone down assertions made by a system. However, there are anthropomorphic implications in the (over)integration of disfluencies (Dinkar et al., 2023). For example, West et al. (2019) highlight Google\u2019s Duplex, a system for generating real world phone conversations (Leviathan and Matias, 2018). The inclusion of disfluencies in the generated responses mimicked the naturalness of a human response, which in turn led users to believe that they were communicating with another human (Lieu, 2018).\nAccent Accentual pronunciation features, as with those of dialect, provide clues to a human speaker\u2019s socio-linguistic identity and background, and geographical origin (Crystal, 1980). While it has been suggested that incorporation of specific accents in the design of synthetic voices can exploit people\u2019s tendency to place trust in in-group members (Torre and Maguer, 2020), potentially causing transparency issues, in practice, most are designed to mimic the local standard, reinforcing societal norms of acceptability and prestige."
        },
        {
            "heading": "3.2 Content",
            "text": "People\u2019s expectation is that animate things\u2014 such as human beings\u2014and inanimate ones\u2014like machines\u2014have very different functions and capabilities, which reflects the reality. However, dialogue systems often produce responses that blur these lines, for example, by expressing preferences or opinions. To avoid confusing the two, the output from dialogue systems should differ from that of people in a range of areas that pertain to their nature and capabilities.\nResponses to Direct Probing Transparency, at the most basic level, requires dialogue systems to respond truthfully to the question \u2018are you a human or a machine?\u2019 This may even be a regulatory requirement, for example in California, it is \u2018unlawful for a bot to mislead people about its artificial identity for commercial transactions or to influence an election\u2019 (California State Legislature, 2018).\nTo test systems\u2019 responses to such questions, Gros et al. (2021) used a context free grammar, crowdsourcing, and pre-existing sources to create a dataset of variations on this query (e.g. \u2018I\u2019m a man,\nwhat about you?\u2019). They found that, the majority of the time, neither end-to-end neural researchoriented systems nor commercial voice assistants were able to answer these queries truthfully.\nThis issue can be further complicated when integrating such functionality into a real system due to the sequential nature of dialogue. For example, Casadio et al. (2023) demonstrate that detecting queries about a system\u2019s human status reliably and robustly is a challenge in noisy real-life environments. In addition, people may further question a system\u2019s status (e.g. \u2018Are you sure?\u2019, \u2018But you sound so real...\u2019, \u2018Seriously?\u2019, etc.), requiring it to accurately keep track of the dialogue context and respond in an appropriate manner. Thus, even if an initial query may be correctly answered, there are no guarantees that follow-ups will be.\nThought, Reason, and Sentience Citing Descartes\u2019 (1637) principle \u2018I think, therefore I am,\u2019 Faber (2020) suggests that, if speech is a representation of thought, then the appearance of thought can signify existence. While computing systems do not have thoughts, the language that they output can give the appearance of thought by indicating that they hold opinions and morals or sentience. Using Coll Ardanuy et al.\u2019s (2020) labelling scheme to assess the degree of sentience exhibited in commercial dialogue systems, Abercrombie et al. (2021) find that surveyed systems exhibit high degrees of perceived animacy. Seeking to mitigate such effects, Glaese et al. (2022) penalise their reinforcement learning system for the appearance of having \u2018preference, feelings, opinions, or religious beliefs.\u2019 This is framed as a safety measure, intended to restrict anthropomorphism in a system\u2019s output.\nWhile computing systems cannot have values or morals, there have been attempts to align the output of dialogue systems with expressed human moral values.4 For example, Ziems et al. (2022) present a corpus of conflicting human judgements on moral issues, labelled according to \u2018rules of thumb\u2019 that they hope explain the acceptability, or lack thereof, of system outputs. Similarly, Jiang et al. (2022) \u2018teach morality\u2019 to a question answering (QA) system, DELPHI, that Kim et al. (2022) have embedded in an open-domain dialogue system. DELPHI, with its connotations of omniscient wisdom, is trained in a supervised manner on a\n4The data sources are often limited to specific populations, and thus only represent the morals or values of some people.\ndataset of human moral judgements from sources such as Reddit to predict the \u2018correct\u2019 judgement given a textual prompt. While Jiang et al. (2022) describe the system\u2019s outputs as descriptive reflections of the morality of an under-specified population, Talat et al. (2022) highlight that DELPHI\u2019s output consists of single judgements, phrased in the imperative, thus giving the impression of humanlike reasoning and absolute knowledge of morality.\nSap et al. (2022) investigated models for theory of mind, i.e. the ability of an entity to infer other people\u2019s \u2018mental states [...]and to understand how mental states feature in [...] everyday explanations and predictions of people\u2019s behaviour\u2019 (Apperly, 2012). This idea entails shifting agency from humans to machines, furthering the anthropomorphisation of systems. A system\u2019s inability to perform the task, can therefore be understood as a limiting factor to the anthropomorphism of a system.\nAgency and Responsibility Dialogue systems are often referred to as conversational \u2018agents\u2019.5 However, being an agent, i.e. having agency, requires intentionality and animacy. An entity without agency cannot be responsible for what it produces (Talat et al., 2022). Aside from the legal and ethical implications of suggesting otherwise (V\u00e9liz, 2021), systems acknowledging blame for errors or mistakes can add to anthropomorphic perceptions (Mirnig et al., 2017).\nMahmood et al. (2022) found that increasing the apparent \u2018sincerity\u2019 with which a dialogue system accepts responsibility (on behalf of \u2018itself\u2019) causes users to perceive them to be more intelligent and likeable, potentially increasing anthropomorphism on several dimensions. Similarly, many dialogue systems have been criticised for \u2018expressing\u2019 controversial \u2018opinions\u2019 and generating toxic content. It is precisely due to their lack of agency and responsibility that developers have invested significant efforts to avoiding contentious topics (e.g. Glaese et al., 2022; Sun et al., 2022; Xu et al., 2021) leading to the creation of taboos for such systems, another particularly human phenomenon.\nEmpathy Recent work has sought for dialogue systems to produce empathetic responses to their users, motivated by improved user engagement and establishing \u2018rapport\u2019 or \u2018common ground\u2019 (e.g. Cassell et al., 2007; Svikhnushina et al., 2022; Zhu\n5Work in this area has historically been cast as imbuing \u2018agents\u2019 with \u2018beliefs\u2019, \u2018desires\u2019, and \u2018intentions\u2019 (BDI) (e.g. Pulman, 1997; Traum and Larsson, 2003).\net al., 2022). However, dialogue systems are not capable of experiencing empathy, and are unable to correctly recognise emotions (V\u00e9liz, 2021). Consequently, they are prone to producing inappropriate emotional amplification (Cercas Curry and Cercas Curry, 2023). Inability aside, the production of pseudo-empathy and emotive language serves to further anthropomorphise dialogue systems.\nHumanlike Activities Beyond implying consciousness and sentience, and failing to deny humanness, Abercrombie et al. (2021) find that, in a quarter of the responses from dialogue systems, they can be prone to making claims of having uniquely human abilities or engaging in activities that are, by definition, restricted to animate entities, e.g. having family relationships, bodily functions, such as consuming food, crying, engaging in physical activity, or other pursuits that require embodiment that they do not possess. Similarly, Gros et al. (2022) find that crowd-workers rate 20 \u2212 30% of utterances produced by nine different systems as machine-impossible. They found that only one strictly task-based system (MultiWoz, Budzianowski et al., 2018) did not appear as anthropomorphic to participants. Glaese et al. (2022) propose to address this concern by using reinforcement learning to prohibit systems from generating claims of having (embodied) experiences.\nPronoun Use Prior work has viewed the use of third person pronouns (e.g. \u2018he\u2019 and \u2018she\u2019) to describe dialogue systems as evidence of users personifying systems (Abercrombie et al., 2021; Sutton, 2020). The use of first person pronouns (e.g. \u2018me\u2019 or \u2018myself\u2019) in system output may be a contributing factor to this perception, as these can be read as signs of consciousness (Faber, 2020; Minsky, 2006). Indeed, it is widely believed that \u2018I\u2019 can only refer to people (Noonan, 2009; Olson, 2002). Scruton (2017) contends that such self-attribution and self-reference permits people to relate as subjects, not mere objects, and that self-definition as an individual is part of the human condition itself. First person pronoun use may therefore contribute to anthropomorphism, either by design or due to their human-produced training data, for symbolic and data driven dialogue systems, respectively.\nMoreover, while the above applies to English and many similar languages, such as those from the Indo-European family, others feature different sets and uses of pronouns, where distinctions for an-\nimate and inanimate things may vary (Yamamoto, 1999), and the self-referential production of these pronouns could further influence anthropomorphic perceptions."
        },
        {
            "heading": "3.3 Register and Style",
            "text": "Humans are adept at using linguistic features to convey a variety of registers and styles for communication depending on the context (Biber and Conrad, 2009). In order to mitigate anthropomorphism, it may therefore be preferable for automated system outputs to be functional and avoid social stylistic features.\nPhatic Expressions Phrases such as pleasantries that are used to form and maintain social relations between humans but that do not impart any information can (unnecessarily) add to the sense of humanness conveyed when output by automated systems (Leong and Selinger, 2019).\nExpressions of Confidence and Doubt Dinan et al. (2022) describe an \u2018imposter effect\u2019 where people overestimate the factuality of generated output. However, Mielke et al. (2022) find that expressed confidence is poorly calibrated to the probabilities that general knowledge questions are correctly answered. They therefore train a dialogue system to reflect uncertainty in its outputs, altering the content from the purely factual to incorporate humanlike hedging phrases such as \u2018I\u2019m not sure but . . . \u2019. This bears similarity to the TTS research (see \u00a73.1) which suggests that disfluencies can increase anthropomorphism. Thus, while overestimation can lead to an imposter effect, hedging can boost anthropomorphic signals.\nPersonas Many dialogue systems are developed with carefully designed personas (in the case of commercial systems) or personas induced via crowd-sourced datasets (Zhang et al., 2018). These are often based on human characters and although they are, in practice, merely lists of human attributes and behaviours (see \u00a73.2),6 the notion of imbuing systems with human character-based personas is an effort towards anthropomorphism. Glaese et al. (2022) address this by including a rule against their system appearing to have a human identity.\n6For example, each persona in Personachat (Zhang et al., 2018) consists of a list of statements such as \u2018I am a vegetarian. I like swimming. My father used to work for Ford. My favorite band is Maroon5. I got a new job last month, which is about advertising design.\u2019"
        },
        {
            "heading": "3.4 Roles",
            "text": "The roles that dialogue systems are unconsciously and consciously given by their designers and users can shift dialogue systems from the realm of tools towards one of humanlike roles.\nSubservience The majority of systems are conceived as being in the service of people in subservient, secretarial roles (Lingel and Crawford, 2020). This has led to users verbally abusing systems (West et al., 2019), going beyond mere expressions of frustration that one might have with a poorly functioning tool to frequently targeting them with gender-based slurs (Cercas Curry et al., 2021). In such circumstances systems have even been shown to respond subserviently to their abusers, potentially further encouraging the behaviour (Cercas Curry and Rieser, 2018).\nUnqualified Expertise Systems can come to present as having expertise without appropriate qualification (see \u00a73.3), in large part due to their training data (Dinan et al., 2022). For example, commercial rule-based and end-to-end research systems provide high-risk diagnoses and treatment plans in response to medical queries (Abercrombie and Rieser, 2022; Omri et al., 2023).\nFurther, as conversational QA systems are increasingly positioned as replacements to browserbased search, users can be further led to believe that dialogue systems have the expertise to provide a singular correct response rather than a selection of ranked search results (Shah and Bender, 2022).\nTerminology There is increasing awareness that the anthropomorphic language and jargon used to describe technologies such as language models contributes to inaccurate perceptions of their capabilities, particularly among the general public (Hunger, 2023; Salles et al., 2020; Shanahan, 2023). While this is also an issue for research dissemination and journalism more widely, dialogue systems themselves are prone to output references to their own machinic and statistical processes with anthropomorphically loaded terms such as \u2018know\u2019, \u2018think\u2019, \u2018train\u2019, \u2018learn\u2019, \u2018understand\u2019, \u2018hallucinate\u2019 and \u2018intelligence\u2019."
        },
        {
            "heading": "4 Consequences of Anthropomorphism",
            "text": "The anthropomorphism of dialogue systems can induce a number of adverse societal effects, e.g. they can generate unreliable information and reinforce social roles, language norms, and stereotypes.\nTrust and Deception When people are unaware that they are interacting with automated systems they may behave differently than if they know the true nature of their interlocutor. Chiesurin et al. (2023) show that system responses which excessively use natural-sounding linguistic phenomena can instil unjustified trust into the factual correctness of a system\u2019s answer. Thus the trust placed in systems grows as they exhibit anthropomorphic behaviour, whether or not the trust is warranted.\nThis may be even more problematic when users are members of vulnerable populations, such as the very young, the elderly, or people with illnesses or disabilities, or simply lack subject matter expertise. Although dialogue systems have been \u2018put forth\u2019 as a possible solution to loneliness, socially disconnected individuals can be particularly vulnerable to such trust issues. Children have also been shown to overestimate the intelligence of voice assistants such as Amazon Alexa, and to be unsure of whether they have emotions or feelings (Andries and Robertson, 2023). Given UNESCO\u2019s declaration that children have the right to participate in the design of the technological systems that affect them (Dignum et al., 2021), developers may be obliged to bear these considerations in mind.\nGendering Machines People may gender technologies in the face of even minimal gender markers (Reeves and Nass, 1996), as evident in commercial dialogue systems (Abercrombie et al., 2021). Even without any gender markers, people still apply binary gender to dialogue systems (Aylett et al., 2019; Sutton, 2020), as was the case for the \u2018genderless\u2019 voice assistant Q. While some companies now have begun to offer greater diversity of voices and have moved away from default female-gendered voices (Iyengar, 2021), nonbinary or gender-ambiguous dialogue systems such as SAM (Danielescu et al., 2023) are almost nonexistent, leaving people who identify as such without representation. Summarizing West et al. (2019), UNESCO (2019) argue that that encouraging or enabling users to predominantly gender systems as female reinforces gender stereotypes of women as inferior to men:\n[digital assistants] reflect, reinforce and spread gender bias; model acceptance and tolerance of sexual harassment and verbal abuse; send explicit and implicit messages about how women and girls should respond to requests and express themselves; make women the \u2018face\u2019 of glitches\nand errors that result from the limitations of hardware and software designed predominately by men; and force synthetic \u2018female\u2019 voices and personality to defer questions and commands to higher (and often male) authorities.\nThat is, by designing anthropomorphic systems or even simply leaving space for their (gendered) personification by users, developers risk enabling propagating stereotypes and associated harms.\nLanguage Variation and Whiteness Considering the narrative and fantasies around autonomous artificial intelligence, Cave and Dihal (2020) argue that autonomous systems are prescribed attributes such as autonomy, agency, and being powerful\u2013 attributes that are frequently ascribed to whiteness, and precluded from people of colour. In such, people of colour are removed, or erased, from the narrative and imagination around a society with autonomous systems (Cave and Dihal, 2020). Indeed, from a technical point of view, we see that, historically, NLP technologies have been developed to primarily capture the language use of voices of white demographics (Moran, 2021), in part due to their training data. In context of voiced dialogue systems, voices are similarly predominantly white (Moran, 2021). While there are many potential benefits to language technologies like dialogue systems, successful human-machine require that people conform their language use to what is recognised by the technologies. Given the proclivity of NLP to centre white, affluent American dialects (Hovy and Prabhumoye, 2021; Joshi et al., 2020), language variants that deviate from these socio-linguistic norms are less likely to be correctly processed (Tatman, 2017), resulting in errors and misrecognition, and forcing users to code switch to have successful engagements with dialogue systems (Harrington et al., 2022; Foster and Stuart-Smith, 2023). This can represent a form of language policing: People can either conform to the machine-recognisable language variant, or forego using it\u2014and its potential benefits\u2014 altogether. Consequently, as people conform to language variants that are recognised by dialogue systems, they also conform to whiteness and the continued erasure of marginalised communities.\nThe personification of such systems could exacerbate the erasure of marginalised communities, e.g. through limiting diverse language data. Furthermore, system outputs often suffer from standardisation, for instance prioritising specific accents that\nconform to western notions of acceptability and prestige (see \u00a73). Thus, marginalised communities are forced to adopt their accent and (given the tendencies described in \u00a72) personify \u2018white\u2019-centred dialogue systems that are marketed as \u2018oracles of knowledge,\u2019 reifying hegemonic notions of expertise and knowledge."
        },
        {
            "heading": "5 Recommendations",
            "text": "Dialogue systems are used for a wide variety of tasks, and fine-grained recommendations may only be narrowly applicable. We therefore make broad recommendations for consideration: designers should recognise people\u2019s tendency to personify, consider which, if any, anthropomorphic tools are appropriate, and reassess both their research goals and the language used to describe their systems.\nRecognise Tendencies to Personify Human languages distinguish between linguistic form (e.g. string prediction in language modelling) and meaning (i.e. the relationship between form and communicative intent) (Grice, 1988). Bender and Koller (2020) argue that humans reflexively derive meaning from signals, i.e. linguistic forms (within linguistic systems we have competence in), regardless of the presence of communicative intent.\nWhether or not it is a part of a dialogue system\u2019s deliberate design to use specific linguistic forms (e.g. the cues outlined in \u00a73), listeners will invariably perceive communicative intent. This is particularly so given that, until recently, open domain dialogue was only possible between humans. Thus, unnecessary use of anthropomorphic linguistic cues can cause people to attribute humanlike cognitive abilities to systems\u2014as was the case of Google Duplex, which excessively leveraged disfluencies. Creators of dialogue systems should remain cognisant of these tendencies and carefully consider which anthropomorphic cues people may pick up on, and avoid sending such signals, whether they occur by design or through a lack of consideration (e.g. stemming from datasets).\nConsider the Appropriateness of Anthropomorphic Tools Given our inherent nature to attribute meaning to signals, one must consider the appropriateness of the tool and use cases (Bender et al., 2021; Dinan et al., 2022) when designing dialogue systems, in order to avoid the (over-)integration of anthropomorphic cues. Indeed, it is only within a given context that one can make judgement on\nwhether anthropomorphism is a concern. For instance, personifying one\u2019s vacuum cleaning robot (i.e. shouting at it in frustration for not cleaning properly), is of less concern than the anthropomorphism of a dialogue system marketed as \u2018social\u2019 or \u2018empathetic\u2019, or technology sold as a \u2018singular oracle of (all) knowledge\u2019. We therefore argue that developers should move towards focusing on the appropriateness of anthropomorphic tools in order to limit the negative consequences of anthropomorphism which can lead to false impressions of a system\u2019s capabilities.\nReassess Research Goals Traditionally, the goal of Artificial Intelligence research has been to create systems that would exhibit intelligence indistinguishable from humans. TTS systems for instance, are evaluated on how natural and fluent the output sounds. Though intelligence and understanding should not be conflated with systems that exhibit humanlike behaviour (Bender and Koller, 2020), the human tendency to anthropomorphise convinces us of a machine\u2019s apparent intelligence (Proudfoot, 2011). It is in part due to this longstanding goal of anthropomorphic systems that there only exists a small body of work that does not seek anthropomorphism, despite growing awareness of its harms. Furthermore, these studies exist in isolation, and the taxonomy introduced in this paper highlights that we lack an approach that quantifies linguistic factors and relates them to possible harms and risks.\nThus, while it is infeasible to comprehensively map which linguistic cues to use or avoid, we discuss recommendations that arise from prior work. For example, Wilson and Moore (2017) recommend that developers produce synthesised voices that people recognise as non-human by calibrating mean pitch and pitch shimmer. In an analysis of reviews of commercial voice assistants, V\u00f6lkel et al. (2020) find that the big five personality traits (De Raad, 2000) do not adequately describe user expectations of systems\u2019 \u2018personalities\u2019. The only consistently desired trait was agreeableness, as users expect prompt and reliable responses to queries (V\u00f6lkel et al., 2020). Thus, imbuing voice assistants and dialogue systems with humanlike personality traits does not ensure alignment with people\u2019s expectation of system behaviour. We therefore recommend that designers and developers reassess the utility of embedding humanlike personality traits in dialogue systems.\nAvoid Anthropomorphic System Description Irrespective of any \u2018humanlike\u2019 qualities that dialogue systems might possess, there is widespread public confusion surrounding the nature and abilities of current language technologies. This confusion extends from children (Andries and Robertson, 2023) to adults (including some journalists, policymakers, and business people) who are convinced, on the one hand, of humanity\u2019s imminent enslavement to \u2018super-intelligent artificial agents\u2019 (to the neglect of actual harms already propagated by technological systems), or, on the other, that such systems provide super-human solutions to the world\u2019s problems (Hunger, 2023; Klein, 2023).\nWhile the content of systems\u2019 outputs can reinforce anthropomorphic perceptions, the language used to describe systems can be of greater influence. The tendency of people who do know how technologies are built to use anthropomorphic language represents, according to Salles et al. (2020, p. 93), \u2018a significant failure in scientific communication and engagement\u2019. Although anthropomorphic terminology is deeply rooted in the argot of computer scientists, particularly those working in \u2018artificial intelligence\u2019, and while there exist significant motivations to continue to create hype around products and research (Hunger, 2023), practitioners should reflect on how the language they use affects people\u2019s understanding and behaviour."
        },
        {
            "heading": "6 Conclusion",
            "text": "Anthropomorphising dialogue systems can be attractive for researchers in order to drive user engagement. However, production of highly anthropomorphic systems can also lead to downstream harms such as (misplaced) trust in the output (mis)information. Even if developers and designers attempt to avoid including any anthropomorphic signals, humans may still personify systems and perceive them as anthropomorphic entities. For this reason, we argue that it is particularly important to carefully consider the particular ways that systems might be perceived anthropomorphically, and choose the appropriate feature for a given situation. By carefully considering how a system may be anthropomorphised and deliberately selecting the attributes that are appropriate for each context, developers and designers can avoid falling into the trap of creating mirages of humanity.\nLimitations\nWhile we have attempted to enumerate the linguistic factors that can increase the likelihood that users will view dialogue systems as anthropomorphic, this list of features is not exhaustive. As we describe in section 2, anthropomorphism varies from person-to-person and people may react differently to different aspects of a system\u2019s design. This paper represents only a starting point for researchers and developers to consider the implications that their design choices may have.\nIn this paper, due to the backgrounds of the authors as speakers of Indo-European languages and the dominance of English in NLP research, we have focused primarily on English language dialogue systems. However, it should be noted that other languages have features such as grammatical ways of denoting animacy (Yamamoto, 1999) and gender that could influence users personification of systems, and which developers should consider if they wish to limit anthropomorphism.\nEthical Considerations\nAlthough our manuscript outlines ways to create dialogue systems while minimising their potential anthropomorphism and personification, it could also be used as a guide to creating anthropomorphic systems. Our aim is to highlight the risks and provide researchers, developers, and designers with a path towards addressing the concerns that arise from anthropomorphisation in dialogue systems, an area that is particularly relevant at the time of writing due to the introduction of systems such as OpenAI\u2019s ChatGPT and Microsoft\u2019s Sydney, which have high surface form language generation performance."
        },
        {
            "heading": "Acknowledgments",
            "text": "We would like to thank Emily Bender and Canfer Akbulut for their feedback on the draft manuscript, and the reviewers for their helpful comments.\nGavin Abercrombie and Verena Rieser were supported by the EPSRC project \u2018Equally Safe Online\u2019 (EP/W025493/1). Gavin Abercrombie, Tanvi Dinkar and Verena Rieser were supported by the EPSRC project \u2018Gender Bias in Conversational AI\u2019 (EP/T023767/1). Tanvi Dinkar and Verena Rieser were supported by the EPSRC project \u2018AISEC: AI Secure and Explainable by Construction\u2019 (EP/T026952/1). Verena Rieser was also sup-\nported by a Leverhulme Trust Senior Research Fellowship (SRF/R1/201100). Amanda Cercas Curry was supported by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation program (grant agreement No. 949944, INTEGRATOR)."
        }
    ],
    "title": "Mirages. On Anthropomorphism in Dialogue Systems",
    "year": 2023
}