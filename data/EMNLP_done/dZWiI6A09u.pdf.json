{
    "abstractText": "Claim verification plays a crucial role in combating misinformation. While existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to create at a large scale. Additionally, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents First-OrderLogic-Guided Knowledge-Grounded (FOLK) Reasoning that can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs). FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates, each corresponding to a subclaim that needs to be verified. Then, FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity predictions and generate explanations to justify its decision-making process. This process makes our model highly explanatory, providing clear explanations of its reasoning process in human-readable form. Our experiment results indicate that FOLK outperforms strong baselines on three datasets encompassing various claim verification challenges. Our code and data are available. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Haoran Wang"
        },
        {
            "affiliations": [],
            "name": "Kai Shu"
        }
    ],
    "id": "SP:c0e05e66bd03e78ea41dc71d92cacedfde96ed30",
    "references": [
        {
            "authors": [
                "Rami Aly",
                "Zhijiang Guo",
                "Michael Schlichtkrull",
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Oana Cocarascu",
                "Arpit Mittal."
            ],
            "title": "Feverous: Fact extraction and verification over unstructured and structured information",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Pepa Atanasova",
                "Jakob Grue Simonsen",
                "Christina Lioma",
                "Isabelle Augenstein."
            ],
            "title": "Generating fact checking explanations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7352\u20137364, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Antonia Creswell",
                "Murray Shanahan",
                "Irina Higgins."
            ],
            "title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "venue": "arXiv preprint arXiv:2205.09712.",
            "year": 2022
        },
        {
            "authors": [
                "Yanai Elazar",
                "Nora Kassner",
                "Shauli Ravfogel",
                "Abhilasha Ravichander",
                "Eduard Hovy",
                "Hinrich Sch\u00fctze",
                "Yoav Goldberg."
            ],
            "title": "Measuring and improving consistency in pretrained language models",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2021
        },
        {
            "authors": [
                "Herbert B Enderton."
            ],
            "title": "A mathematical introduction to logic",
            "venue": "Elsevier.",
            "year": 2001
        },
        {
            "authors": [
                "Luyu Gao",
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Pengfei Liu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig."
            ],
            "title": "Pal: Program-aided language models",
            "venue": "arXiv preprint arXiv:2211.10435.",
            "year": 2022
        },
        {
            "authors": [
                "Zhijiang Guo",
                "Michael Schlichtkrull",
                "Andreas Vlachos."
            ],
            "title": "A survey on automated fact-checking",
            "venue": "Transactions of the Association for Computational Linguistics, 10:178\u2013206.",
            "year": 2022
        },
        {
            "authors": [
                "Andrew F Hayes",
                "Klaus Krippendorff."
            ],
            "title": "Answering the call for a standard reliability measure for coding data",
            "venue": "Communication methods and measures, 1(1):77\u201389.",
            "year": 2007
        },
        {
            "authors": [
                "Joy He-Yueya",
                "Gabriel Poesia",
                "Rose E Wang",
                "Noah D Goodman."
            ],
            "title": "Solving math word problems by combining language models with symbolic solvers",
            "venue": "arXiv preprint arXiv:2304.09102.",
            "year": 2023
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Comput. Surv., 55(12).",
            "year": 2023
        },
        {
            "authors": [
                "Yichen Jiang",
                "Shikha Bordia",
                "Zheng Zhong",
                "Charles Dognin",
                "Maneesh Singh",
                "Mohit Bansal."
            ],
            "title": "HoVer: A dataset for many-hop fact extraction and claim verification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Yiqiao Jin",
                "Yeon-Chang Lee",
                "Kartik Sharma",
                "Meng Ye",
                "Karan Sikka",
                "Ajay Divakaran",
                "Srijan Kumar."
            ],
            "title": "Predicting information pathways across online communities",
            "venue": "KDD.",
            "year": 2023
        },
        {
            "authors": [
                "Yiqiao Jin",
                "Xiting Wang",
                "Ruichao Yang",
                "Yizhou Sun",
                "Wei Wang",
                "Hao Liao",
                "Xing Xie."
            ],
            "title": "Towards fine-grained reasoning for fake news detection",
            "venue": "AAAI.",
            "year": 2022
        },
        {
            "authors": [
                "Seyed Mehran Kazemi",
                "Najoung Kim",
                "Deepti Bhatia",
                "Xin Xu",
                "Deepak Ramachandran."
            ],
            "title": "Lambada: Backward chaining for automated reasoning in natural language",
            "venue": "arXiv preprint arXiv:2212.13894.",
            "year": 2022
        },
        {
            "authors": [
                "Yoon Kim."
            ],
            "title": "Sequence-to-sequence learning with latent neural grammars",
            "venue": "Advances in Neural Information Processing Systems, 34:26302\u201326317.",
            "year": 2021
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "arXiv preprint arXiv:2205.11916.",
            "year": 2022
        },
        {
            "authors": [
                "Neema Kotonya",
                "Francesca Toni."
            ],
            "title": "Explainable automated fact-checking: A survey",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 5430\u20135443, Barcelona, Spain (Online). International Committee on Compu-",
            "year": 2020
        },
        {
            "authors": [
                "Neema Kotonya",
                "Francesca Toni."
            ],
            "title": "Explainable automated fact-checking for public health claims",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7740\u20137754, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Nayeon Lee",
                "Yejin Bang",
                "Andrea Madotto",
                "Madian Khabsa",
                "Pascale Fung."
            ],
            "title": "Towards fewshot fact-checking via perplexity",
            "venue": "arXiv preprint arXiv:2103.09535.",
            "year": 2021
        },
        {
            "authors": [
                "Zhenghao Liu",
                "Chenyan Xiong",
                "Maosong Sun",
                "Zhiyuan Liu."
            ],
            "title": "Fine-grained fact verification with kernel graph attention network",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7342\u20137351, On-",
            "year": 2020
        },
        {
            "authors": [
                "Vaibhav Mavi",
                "Anubhav Jangra",
                "Adam Jatowt."
            ],
            "title": "A survey on multi-hop question answering and generation",
            "venue": "arXiv preprint arXiv:2204.09140.",
            "year": 2022
        },
        {
            "authors": [
                "Sewon Min",
                "Victor Zhong",
                "Luke Zettlemoyer",
                "Hannaneh Hajishirzi."
            ],
            "title": "Multi-hop reading comprehension through question decomposition and rescoring",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2019
        },
        {
            "authors": [
                "David Luan"
            ],
            "title": "Show your work: Scratchpads for intermediate computation with language models",
            "venue": "arXiv preprint arXiv:2112.00114",
            "year": 2021
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Liangming Pan",
                "Wenhu Chen",
                "Wenhan Xiong",
                "MinYen Kan",
                "William Yang Wang."
            ],
            "title": "Zero-shot fact verification by claim generation",
            "venue": "arXiv preprint arXiv:2105.14682.",
            "year": 2021
        },
        {
            "authors": [
                "Liangming Pan",
                "Xiaobao Wu",
                "Xinyuan Lu",
                "Anh Tuan Luu",
                "William Yang Wang",
                "Min-Yen Kan",
                "Preslav Nakov."
            ],
            "title": "Fact-checking complex claims with program-guided reasoning",
            "venue": "arXiv preprint arXiv:2305.12744.",
            "year": 2023
        },
        {
            "authors": [
                "Baolin Peng",
                "Michel Galley",
                "Pengcheng He",
                "Hao Cheng",
                "Yujia Xie",
                "Yu Hu",
                "Qiuyuan Huang",
                "Lars Liden",
                "Zhou Yu",
                "Weizhu Chen"
            ],
            "title": "Check your facts and try again: Improving large language models with external knowledge and automated feedback",
            "year": 2023
        },
        {
            "authors": [
                "Kashyap Popat",
                "Subhabrata Mukherjee",
                "Andrew Yates",
                "Gerhard Weikum."
            ],
            "title": "DeClarE: Debunking fake news and false claims using evidence-aware deep learning",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
            "year": 2018
        },
        {
            "authors": [
                "Ofir Press",
                "Muru Zhang",
                "Sewon Min",
                "Ludwig Schmidt",
                "Noah A Smith",
                "Mike Lewis."
            ],
            "title": "Measuring and narrowing the compositionality gap in language models",
            "venue": "arXiv preprint arXiv:2210.03350.",
            "year": 2022
        },
        {
            "authors": [
                "Soumya Sanyal",
                "Harman Singh",
                "Xiang Ren."
            ],
            "title": "FaiRR: Faithful and robust deductive reasoning over natural language",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1075\u20131093,",
            "year": 2022
        },
        {
            "authors": [
                "Kai Shu",
                "Limeng Cui",
                "Suhang Wang",
                "Dongwon Lee",
                "Huan Liu."
            ],
            "title": "defend: Explainable fake news detection",
            "venue": "Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 395\u2013405.",
            "year": 2019
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "FEVER: a large-scale dataset for fact extraction and VERification",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of",
            "year": 2018
        },
        {
            "authors": [
                "Lin Tian",
                "Xiuzhen Zhang",
                "Jey Han Lau."
            ],
            "title": "Metatroll: Few-shot detection of state-sponsored trolls with transformer adapters",
            "venue": "Proceedings of the ACM Web Conference 2023, pages 1743\u20131753.",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Harsh Trivedi",
                "Niranjan Balasubramanian",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
            "venue": "arXiv preprint arXiv:2212.10509.",
            "year": 2022
        },
        {
            "authors": [
                "Karthik Valmeekam",
                "Alberto Olmo",
                "Sarath Sreedharan",
                "Subbarao Kambhampati."
            ],
            "title": "Large language models still can\u2019t plan (a benchmark for llms on planning and reasoning about change)",
            "venue": "arXiv preprint arXiv:2206.10498.",
            "year": 2022
        },
        {
            "authors": [
                "David Wadden",
                "Kyle Lo",
                "Bailey Kuehl",
                "Arman Cohan",
                "Iz Beltagy",
                "Lucy Lu Wang",
                "Hannaneh Hajishirzi."
            ],
            "title": "SciFact-open: Towards open-domain scientific claim verification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages",
            "year": 2022
        },
        {
            "authors": [
                "David Wadden",
                "Kyle Lo",
                "Lucy Lu Wang",
                "Arman Cohan",
                "Iz Beltagy",
                "Hannaneh Hajishirzi."
            ],
            "title": "MultiVerS: Improving scientific claim verification with weak supervision and full-document context",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Dustin Wright",
                "David Wadden",
                "Kyle Lo",
                "Bailey Kuehl",
                "Arman Cohan",
                "Isabelle Augenstein",
                "Lucy Lu Wang."
            ],
            "title": "Generating scientific claims for zeroshot scientific fact checking",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Ruichao Yang",
                "Xiting Wang",
                "Yiqiao Jin",
                "Chaozhuo Li",
                "Jianxun Lian",
                "Xing Xie."
            ],
            "title": "Reinforcement subgraph reasoning for fake news detection",
            "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 2253\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik Narasimhan",
                "Yuan Cao."
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "venue": "arXiv preprint arXiv:2210.03629.",
            "year": 2022
        },
        {
            "authors": [
                "Ori Yoran",
                "Tomer Wolfson",
                "Ben Bogin",
                "Uri Katz",
                "Daniel Deutch",
                "Jonathan Berant."
            ],
            "title": "Answering questions by meta-reasoning over multiple chains of thought",
            "venue": "arXiv preprint arXiv:2304.13007.",
            "year": 2023
        },
        {
            "authors": [
                "Fei Yu",
                "Hongbo Zhang",
                "Benyou Wang."
            ],
            "title": "Nature language reasoning, a survey",
            "venue": "arXiv preprint arXiv:2303.14725.",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Wanjun Zhong",
                "Jingjing Xu",
                "Duyu Tang",
                "Zenan Xu",
                "Nan Duan",
                "Ming Zhou",
                "Jiahai Wang",
                "Jian Yin."
            ],
            "title": "Reasoning over semantic-level graph for fact checking",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Jie Zhou",
                "Xu Han",
                "Cheng Yang",
                "Zhiyuan Liu",
                "Lifeng Wang",
                "Changcheng Li",
                "Maosong Sun."
            ],
            "title": "GEAR: Graph-based evidence aggregating and reasoning for fact verification",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Compu-",
            "year": 2019
        },
        {
            "authors": [
                "Ending(Toyozakura Toshiaki"
            ],
            "title": "Toyozakura Toshiaki ended his career",
            "year": 2011
        },
        {
            "authors": [
                "Alfredo Cornejo Cuevas won the gold metal in the welterweight division at the Pan American Games in"
            ],
            "title": "Held(The Pan American Games in 1959, Chicago United States) ::: Verify that The Pan American Games in 1959 was held in Chicago United States",
            "venue": "Won(Alfredo Cornejo Cuevas, the world amateur welterweight title in Mexico City). Followup Question: When was Alfredo Cornejo Cuevas born?",
            "year": 1959
        },
        {
            "authors": [
                "Girls"
            ],
            "title": "Is the writer of the song Girl Talk a member of a girl group? Watkins rose to fame in the early 1990s as a member of the girl",
            "year": 1990
        },
        {
            "authors": [
                "Tom Jones"
            ],
            "title": "born? Thomas Jones Woodward was born in Pontypridd",
            "venue": "South Wales, Great Britain on June",
            "year": 1940
        },
        {
            "authors": [
                "Girls"
            ],
            "title": "Is the writer of the song Girl Talk a member of a girl group? Watkins rose to fame in the early 1990s as a member of the girl",
            "year": 1990
        },
        {
            "authors": [
                "I girl group"
            ],
            "title": "Girls. Member(Tionne Watkins, a girl group) is True because Watkins rose to fame in the early 1990s as a member of the girl-group TLC Write(Tionne Watkins, the song Girl Talk) && Member(Park So-yeon, a girl",
            "year": 1990
        },
        {
            "authors": [
                "Germany. -----Question"
            ],
            "title": "Is it true that The American lyricist Tom Jones, born in 1928, co-authored the screenplay for the musical film The Fantastics",
            "year": 1928
        },
        {
            "authors": [],
            "title": "OBE is a Welsh singer. Who co-author the musical film The Fantastics? Tome Jones co-authored the musical film The Fantastics. >>>>>> Prediction: Born(Tom Jones, 1928) is False because Thomas Jones Woodward was born in Pontypridd",
            "venue": "South Wales, Great Britain on June",
            "year": 1928
        },
        {
            "authors": [
                "Nationality(Tom Jones"
            ],
            "title": "American) is False because Thomas Jones Woodward is a British singer. Co-author(Tome Jones, the musical film The Fantastics) is True because Tome Jones co-authored the musical film The Fantastics",
            "venue": "Born(Tom Jones,",
            "year": 1928
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Claim verification (Guo et al., 2022) has become increasingly important due to widespread online misinformation (Tian et al., 2023; Jin et al., 2023). Most of the existing claim verification models (Zhou et al., 2019; Jin et al., 2022; Yang et al., 2022; Wadden et al., 2022b; Liu et al., 2020; Zhong et al., 2020) use an automated pipeline that consists\n1https://github.com/wang2226/FOLK\nof claim detection, evidence retrieval, verdict prediction, and justification production. Despite some early promising results, they rely on the availability of large-scale human-annotated datasets, which pose challenges due to labor-intensive annotation efforts and the need for annotators with specialized domain knowledge. To address the issue of creating large-scale datasets, recent works (Pan et al., 2021; Wright et al., 2022; Lee et al., 2021) focus on claim verification in zero-shot and few-shot scenarios. However, these methods follow the traditional claim verification pipeline, requiring both claim\nand annotated evidence for veracity prediction. Additionally, these models often lack proper justifications for their predictions, which are important for human fact-checkers to make the final verdicts. Therefore, we ask the following question: Can we develop a model capable of performing claim verification without relying on annotated evidence, while generating natural language justifications for its decision-making process?\nTo this end, we propose a novel framework First-Order-Logic-Guided Knowledge-Grounded (FOLK) to perform explainable claim verification by leveraging the reasoning capabilities of Large Language Models (LLMs) (Brown et al., 2020; Touvron et al., 2023; Zhang et al., 2022; Chowdhery et al., 2022). Figure 1 illustrates a real-world example from FOLK where it can provide veracity prediction based on logical reasoning and generate an explanation of its decision-making process in a short paragraph. To ensure accurate prediction and provide high-quality explanations, FOLK first translates the input claim into a First-Order-Logic (FOL) (Enderton, 2001) clause consisting of a set of conjunctive predicates. Each predicate represents a part of the claim that needs to be verified. Next, the generated FOL predicates guide LLMs to generate a set of questions and corresponding answers. Although the generated answers may appear coherent and plausible, they often lack factual accuracy due to LLM\u2019s hallucination problem (Ji et al., 2023; Ouyang et al., 2022). To address this problem, FOLK controls the knowledge source of the LLMs by grounding the generated answers in real-world truth via retrieving accurate information from trustworthy external knowledge sources (e.g. Google or Wikipedia). Finally, FOLK leverages the reasoning ability of LLMs to evaluate the boolean value of the FOL clause and make a veracity prediction. Given the high stakes involved in claim verification, FOLK prompts the LLMs to generate justifications for their decision-making process in natural language. These justifications are intended to aid human fact-checkers in making the final verdict, enhancing the transparency and interpretability of the model\u2019s predictions.\nWe evaluate our proposed methods on three factchecking datasets (Jiang et al., 2020; Aly et al., 2021; Wadden et al., 2022a) with the following distinct challenges: multi-hop reasoning, numerical reasoning, combining text and table for reasoning, and open-domain scientific claim verification. Our\nexperiment results demonstrate that FOLK can verify complex claims while generating explanations to justify its decision-making process. Additionally, we show the effectiveness of FOL-guided claim decomposition and knowledge-grounded reasoning for claim verification.\nIn summary, our contributions are:\n\u2022 We introduce a new method to verify claims without the need for annotated evidence.\n\u2022 We demonstrate the importance of using symbolic language to help claim decomposition and provide knowledge-grounding for LLM to perform reasoning.\n\u2022 We show that FOLK can generate high-quality explanations to assist human fact-checkers."
        },
        {
            "heading": "2 Background",
            "text": "Claim Verification. The task of claim verification aims to predict the veracity of a claim by retrieving related evidence documents, selecting the most salient evidence sentences, and predicting the veracity of the claim as SUPPORTS or REFUTES. Claim verification falls into the broader task of Fact-checking, which includes all the steps described in claim verification with the addition of claim detection, a step to determine the checkworthiness of a claim.\nWhile steady progress has been made in this field, recent research focus has shifted to 1) dealing with insufficient evidence (Atanasova et al., 2022) and 2) using explainable fact-checking models to support decision-making (Kotonya and Toni, 2020a). In the line of explainable fact-checking, (Popat et al., 2018) and (Shu et al., 2019) use visualization of neural attention weights as explanations. Although attention-based explanation can provide insights into the deep learning model\u2019s decision process, it does not generate human-readable explanations and cannot be interpreted without any prior machine learning knowledge. (Atanasova et al., 2020; Kotonya and Toni, 2020b) formulate the task of generating explanations as extractive summarizing of the ruling comments provided by professional fact-checkers. While their work can generate high-quality explanations based on training data from professional fact-checkers, annotating such datasets is expensive and not feasible at a large scale. Our work explores using reasoning steps as explanations of the model\u2019s decision-making\nprocess while generating explanations in natural language.\nLarge Language Models for Reasoning. Large language models have demonstrated strong reasoning abilities through chain-of-thought (CoT) prompting, wherein LLM is prompted to generate its answer following a step-by-step explanation by using just a few examples as prompts. Recent works have shown that CoT prompting can improve performance on reasoning-heavy tasks such as multi-hop question answering, multi-step computation, and common sense reasoning (Nye et al., 2021; Zhou et al., 2022; Kojima et al., 2022).\nVerifying complex claims often requires multistep (multi-hop) reasoning (Mavi et al., 2022), which requires combining information from multiple pieces of evidence to predict the veracity of a claim. Multi-step reasoning can be categorized into forward-reasoning and backward-reasoning (Yu et al., 2023). Forward-reasoning (Creswell et al., 2022; Sanyal et al., 2022; Wei et al., 2022) employs a bottom-up approach that starts with existing knowledge and obtains new knowledge with inference until the goal is met. Backward-reasoning (Min et al., 2019; Press et al., 2022) on the other hand, is goal-driven, which starts from the goal and breaks it down into sub-goals until all of them are solved. Compared to forward reasoning, backward reasoning is more efficient, the divide-and-conquer search scheme can effectively reduce the problem search space. We propose FOLK, a FOL-guided backward reasoning method for claim verification.\nDespite the recent progress in using LLMs for reasoning tasks, their capability in verifying claims has not been extensively explored. (Yao et al., 2022) evaluate using LLMs to generate reasoning traces and task-specific actions on fact verification tasks. Their reasoning and action steps are more complex than simple CoT and rely on prompting much larger models (PaLM-540B). Additionally, they test their model\u2019s performance on the FEVER dataset (Thorne et al., 2018), which lacks manyhop relations and specialized domain claims. In contrast to their approach, our proposed method demonstrates effectiveness on significantly smaller LLMs without requiring any training, and we test our method on scientific claims.\nContemporaneous to our work, (Peng et al., 2023) propose a set of plug-and-play modules that augment with LLMs to improve the factuality of LLM-generated responses for task-oriented dia-\nlogue and question answering. In contrast to their approach, our primary focus is on providing LLMs with knowledge-grounded facts to enable FOLGuided reasoning for claim verification, rather than solely concentrating on enhancing the factual accuracy of LLMs\u2019 responses. ProgramFC (Pan et al., 2023) leverages LLMs to generate computerprogram-like functions to guide the reasoning process. In contrast to their approach, which only uses LLMs for claim decomposition, we use LLMs for both claim decomposition and veracity prediction. By using LLMs for veracity prediction, we can not only obtain a comprehensive understanding of LLMs\u2019 decision process but also generate explanations for their predictions. Furthermore, ProgramFC is limited to closed-domain, as it needs to first retrieve evidence from a large textual corpus like Wikipedia. FOLK on the other hand, can perform open-domain claim verification since it does not require a pre-defined evidence source."
        },
        {
            "heading": "3 Method",
            "text": "Our objective is to predict the veracity of a claim C without the need for annotated evidence while generating explanations to elucidate the decisionmaking process of LLMs. As shown in Figure 1, our framework contains three stages. In the FOLGuided Claim Decomposition stage, we first translate the input claim into a FOL clause P , then we use P to guide LLM to generate a set of intermediate question-answer pairs (qi, ai). Each intermediate question qi represents a specific reasoning step required to verify the claim. In the KnowledgeGrounding stage, each ai represents the answer generated by LLMs that has been verified against ground truth obtained from an external knowledge source. Finally, in the Veracity Prediction and Explanation Generation stage, we employ P to guide the reasoning process of LLMs over the knowledgegrounded question-and-answer pairs. This allows us to make veracity predictions and generate justifications for its underlying reasoning process."
        },
        {
            "heading": "3.1 FOL-Guided Claim Decomposition",
            "text": "Although LLMs have displayed decent performance in natural language reasoning tasks, they fall short when asked to directly solve complex reasoning problems. This limitation arises from the lack of systematic generalization capabilities in language models (Valmeekam et al., 2022; Elazar et al., 2021). Recent works have discovered that\nLLMs are capable of understanding and converting textual input into symbolic languages, such as formal language (Kim, 2021), mathematical equations (He-Yueya et al., 2023), or Python codes (Gao et al., 2022). Inspired by these recent works, we harness the ability of LLMs to translate textual claims into FOL clauses. This allows us to guide LLMs in breaking down claims into various sub-claims.\nAt this stage, given the input claim C, the LLM first generates a set of predicates P = [p1, ..., pn] that correspond to the sub-claims C = [c1, ..., cn]. Each predicate pi \u2208 P is a First-Order Logic (FOL) predicate that guides LLMs to prompt a questionand-answer pair that represents sub-claim ci. The claim C can be represented as a conjunction of the predicates C = p1 \u2227 p2 \u2227 ... \u2227 pn. To classify the claim C as SUPPORTED, all predicates must evaluate to True. If any of the predicates are False, the claim is classified as REFUTED. By providing the LLMs with symbolic languages such as predicates, alongside a few in-context examples, we observe that LLMs can effectively identify the crucial entities, relations, and facts within the claim. Consequently, LLMs are capable of generating relevant question-and-answer pairs that align with the identified elements."
        },
        {
            "heading": "3.2 Retrieve Knowledge-Grounded Answers",
            "text": "Although LLMs exhibit the ability to generate coherent and well-written text, it is worth noting that they can sometimes hallucinate (Ji et al., 2023), and produce text that fails to be grounded in real-world truth. To provide knowledge-grounded answers for the generated intermediate questions, we employ a retriever based on Google Search, via the SerpAPI 2 service. Specifically, we return the top-1 search result returned by Google. While it is important to acknowledge that Google search results may occasionally include inaccurate information, it generally serves as a more reliable source of knowledge compared to the internal knowledge of LLMs. Additionally, in real-world scenarios, when human fact-checkers come across unfamiliar information, they often rely on Google for assistance. Therefore, we consider the answers provided by Google search as knowledge-grounded answers."
        },
        {
            "heading": "3.3 Veracity Prediction and Explanation Generation",
            "text": "At this stage, the LLM is asked to make a verdict prediction V \u2208 {SUPPORT,REFUTE} and provide an explanation E to justify its decision. Veracity Prediction Given the input claim C, the\n2https://serpapi.com/\npredicates [p1, ..., pn], and knowledge-grounded question-and-answer pairs, FOLK first checks the veracity of each predicate against corresponding knowledge-grounded answers while giving reasons behind its predictions. Once all predicates have been evaluated, FOLK makes a final veracity prediction for the entire clause. In contrast to solely providing LLMs with generated questions and their corresponding grounded answers, we found that the inclusion of predicates assists LLMs in identifying the specific components that require verification, allowing them to offer more targeted explanations. Explanation Generation We leverage LLMs\u2019 capability to generate coherent language and prompt LLMs to generate a paragraph of human-readable explanation. We evaluate the explanation generated by LLMs with manual evaluation. Furthermore, since claim verification is a high-stake task, it should involve human fact-checkers to make the final decision. Therefore, we provide URL links to the relevant facts, allowing human fact-checkers to reference and validate the information."
        },
        {
            "heading": "4 Experiments",
            "text": "We compare FOLK to existing methods on 7 claim verification challenges from three datasets. Our experiment setting is described in Sections 4.1 & 4.2 and we discuss our main results in Section 4.4."
        },
        {
            "heading": "4.1 Datasets",
            "text": "We experiment with the challenging datasets listed below. Following existing works (Yoran et al., 2023; Kazemi et al., 2022; Trivedi et al., 2022), to limit the overall experiment costs, we use stratified sampling to select 100 examples from each dataset to ensure a balanced label distribution.\nHoVER (Jiang et al., 2020) is a multi-hop fact verification dataset created to challenge models to verify complex claims against multiple information sources, or \u201chop\u201d. We use the validation set for evaluation since the test sets are not released publicly. We divide the claims in the validation set based on the number of hops: two-hop claims, three-hop claims, and four-hop claims.\nFEVEROUS (Aly et al., 2021) is a benchmark dataset for complex claim verification over structured and unstructured data. Each claim is annotated with evidence from sentences and forms in Wikipedia. We selected claims in the validation set with the following challenges to test the effectiveness of our framework: numerical reasoning, multi-\nhop reasoning, and combining tables and text. SciFact-Open (Wadden et al., 2022a) is a testing dataset for scientific claim verification. This dataset aims to test existing models\u2019 claim verification performance in an open-domain setting. Since the claims in SciFact-Open do not have a global label, we select claims with complete evidence that either support or refute the claim and utilize them as the global label. This dataset tests our model\u2019s performance on specialized domains that require domain knowledge to verify."
        },
        {
            "heading": "4.2 Baselines",
            "text": "We compare our proposed method against the following four baselines.\nDirect This baseline simulates using LLM as standalone fact-checkers. We directly ask LLMs to give us veracity predictions and explanations given an input claim, relying solely on LLMs\u2019 internal knowledge. It is important to note that we have no control over LLM\u2019s knowledge source, and it is possible that LLMs may hallucinate.\nChain-of-Thought (Wei et al., 2022) is a popular approach that demonstrates chains of inference to LLMs within an in-context prompt. We decompose the claims by asking LLMs to generate the necessary questions needed to verify the claim. We then prompt LLMs to verify the claims step-by-step given the claims and knowledge-grounded answers.\nSelf-Ask (Press et al., 2022) is a structured prompting approach, where the prompt asks LLMs to decompose complex questions into easier subquestions that it answers before answering the main question. It is shown to improve the performance of Chain-of-Thought on multi-hop questionanswering tasks. We use the same decomposition and knowledge-grounding processes as in CoT. For veracity prediction, we provide both questions and knowledge-grounded answers to LLMs to reason, instead of just the knowledge-grounded answers.\nProgramFC (Pan et al., 2023) is a recently proposed baseline for verifying complex claims using LLMs. It contains three settings for knowledgesource: gold-evidence, open-book, and closedbook. To ensure that ProgramFC has the same problem setting as FOLK, we use the open-book setting for ProgramFC. Since we only use one reasoning chain, we select N=1 for ProgramFC. Since ProgramFC cannot perform open-domain claim verification, we exclude it from SciFact-Open dataset."
        },
        {
            "heading": "4.3 Experiment Settings",
            "text": "The baselines and FOLK use GPT-3.5, text-davinci003 (175B) as the underlying LLM. We use SERPAPI as our retrieval engine to obtain knowledgegrounded answers. In addition to the results in Table 2, we perform experiments on smaller LLMs (Touvron et al., 2023): llama-7B, llama-13B, and llama-30B. The results are presented in Table 2. Our prompts are included in B. The number of prompts used varies between 4-6 between the datasets. These prompts are based on random examples from the train and development sets."
        },
        {
            "heading": "4.4 Main Results",
            "text": "We report the overall results for FOLK compared to the baselines for claim verification in Table 2. FOLK achieves the best performance on 6 out of 7 evaluation tasks, demonstrating its effectiveness on various reasoning tasks for claim verification. Based on the experiment results, we have the following major observations: FOLK is more effective on complex claims. On HoVER dataset, FOLK outperforms the baselines by 7.37% and 7.94% on three-hop and four-hop claims respectively. This suggests that FOLK becomes more effective on more complex claims as the required reasoning depth increases. Among the baselines, ProgramFC has comparable performance on three-hop claims, which indicates the effectiveness of using symbolic language, such as programming-like language to guide LLMs for claim decomposition for complex claims. However, programming-like language is less effective as claims become more complex. Despite ProgramFC having a performance increase of 3.68% from threehop to four-hop claims in HoVER, FOLK has a larger performance increase of 10.13%. Suggesting that FOL-guided claim decomposition is more effective on more complex claims.\nOn FEVEROUS dataset, FOLK outperforms the baselines by 7.52%, 9.57%, and 2.69% on all three tasks respectively. This indicates that FOLK can perform well not only on multi-hop reasoning tasks but also on numerical reasoning and reasoning over text and table.\nFOL-guided Reasoning is more effective than CoT-like Reasoning. Our FOLK model, which uses FOL-guided decomposition reasoning approach outperforms CoT and Self-Ask baselines on all three datasets. On average, there is an 11.30% improvement. This suggests that FOL-like predicates help LLMs to better decompose claims, and result in more accurate reasoning. This is particularly evident when the claims become more complex: there is a 12.13% improvement in three-hop and a 16.6% improvement in the four-hop setting.\nKnowledge-grouding is more reliable than LLM\u2019s internal knowledge. FOLK exhibits superior performance compared to Direct baseline across all three datasets. This observation indicates the critical role of knowledge-grounding in claim verification, as Direct solely relies on the internal knowledge of LLMs. It is also important to note that the lack of control over the knowledge source in Direct can lead to hallucinations, where LLMs make accurate predictions but for incorrect reasons. For instance, when faced with a claim labeled as SUPPORT, LLMs may correctly predict the outcome despite certain predicates being false."
        },
        {
            "heading": "4.5 The Impacts of FOL-Guided Reasoning",
            "text": "To gain more insights on prompting from FOL predicates, we perform an ablation study on the HoVER dataset. The goal is to see whether the performance difference in Table 2 primarily results from FOLK generating better follow-up questions or if the predicates also play a role in constructing the veracity prediction. Specifically, we maintain the CoT prompt format but input knowledge-grounded answers from FOLK. As for Self-Ask, we maintain the Self-Ask prompt format while incorporating follow-up questions generated by FOLK along with their associated knowledge-grounded answers. This guarantees that both CoT and Self-Ask retain their reasoning capabilities while employing identical factual information as provided by FOLK. The results, presented in Table 4, show that FOLK consistently outperforms CoT and Self-Ask in all three tasks. This highlights that the FOL-guided reasoning process enhances the ability of language models to integrate knowledge in multi-hop reasoning scenarios effectively."
        },
        {
            "heading": "4.6 The Impacts of Knowledge-Grounding",
            "text": "To better understand the role of knowledgegrounding in LLM\u2019s decision process, we perform an ablation study on four multi-hop reasoning tasks. We use the FOLK prompt to generate predicates and decompose the claim, we then compare its performance under two settings. In the first setting, we let LLM reason over the answers it generated itself. In the second setting, we provide LLM\nwith knowledge-grounded answers. The results are shown in Figure 3, as we can see, FOLK performs better with knowledge-grounded answers. This suggests that by providing knowledge-grounded answers, we can improve LLM\u2019s reasoning performance, and alleviate the hallucination problem by providing it facts.\nNext, we investigate whether the knowledge source can affect FOLK\u2019s performance. Since both HoVER and FEVEROUS datasets are constructed upon Wikipedia pages. We add en.wikipedia.com in front of our query to let it search exclusively from Wikipedia. This is the same way as ProgramFC\u2019s open-book setting. We record the performance in Table 3. As we can see, using a more accurate search can lead to better performance."
        },
        {
            "heading": "4.7 The Generalization on Different-sized LLMs",
            "text": "To assess whether the performance of FOLK can generalize to smaller LLMs, we compare the performance of FOLK against cot and self-ask on HoVER dataset using two different-sized LLMs: llama-7B and llama-13B. Due to the inability of using ProgramFC prompts to generate programs\nusing the llama model, we exclude ProgramFC from our evaluation for this experiment. The results are shown in Figure 2, FOLK can outperform CoT and Self-Ask regardless of the model size, except for 3-hop claims using llama-13B model. As smaller models are less capable for complex reasoning, the performance of Self-Ask decreases significantly with decreasing model size. For CoT, its performance is less sensitive to LLM size compared to Self-Ask. However, these trends are less notable for FOLK. We believe it can attribute to the predicates used to guide LLM to perform highlevel reasoning. Our results show that FOLK using llama-30B model can achieve comparable performance to PrgramFC using 5.8x larger GPT-3.5 on three-hop and four-hop claims. This further shows that FOLK is effective on deeper claims and can generalize its performance to smaller LLMs."
        },
        {
            "heading": "4.8 Assessing the Quality of Explanations",
            "text": "To measure the quality of the explanations generated by FOLK, we conduct manual evaluations by three annotators. The annotators are graduate students with a background in computer science. Following previous work (Atanasova et al., 2020), we ask annotators to rank explanations generated by CoT, Self-Ask, and FOLK. We choose the following three properties for annotators to rank these explanations: Coverage The explanation can identify and include all salient information and important points that contribute to verifying the claim. We provide fact checkers with annotated gold evidence and ask them whether the generated explanation can effectively encompass the key points present in the gold evidence. Soundness The explanation is logically sound and does not contain any information contradictory to the claim or gold evidence. To prevent annotators from being influenced by the logic generated by FOLK, we do not provide annotators with the predicates generated by FOLK. Readability The explanation is presented in a clear and coherent manner, making it easily understandable. The information is conveyed in a way that is accessible and comprehensible to readers.\nWe randomly sample 30 instances from the multi-hop reasoning challenge from the FEVEROUS dataset. For each instance, we collect veracity explanations generated by CoT, Self-Ask, and FOLK. During the annotation process, we ask an-\nnotators to rank these explanations with the rank 1, 2, and 3 representing first, second, and third place respectively. We also allow ties, meaning that two veracity explanations can receive the same rank if they appear the same. To mitigate potential position bias, we did not provide information about the three different explanations and shuffled them randomly. The annotators worked separately without discussing any details about the annotation task.\nFOLK can generate informative, accurate explanations with great readability. Table 5 shows the results from the manual evaluation mentioned above. We use Mean Average Ranks (MARs) as our evaluation metrics, where a lower MAR signifies a higher ranking and indicates a better quality of an explanation. To measure the interannotator agreement, we compute Krippendorf\u2019s \u03b1 (Hayes and Krippendorff, 2007). The corresponding \u03b1 values for FOLK are 0.52 for Coverage, 0.71 for Soundness, and 0.69 for Readability, where \u03b1 > 0.67 is considered good agreement. We assume the low agreement on coverage can be attributed to the inherent challenges of ranking tasks for manual evaluation. Small variations in rank positions and annotator bias towards ranking ties may impact the agreement among annotators. We find that explanations generated by FOLK are ranked the best for all criteria, with 0.16 and 0.40 ranking improvements on coverage and readability respec-\ntively. While Self-Ask has better prediction results compared to CoT, as shown in Table 2, CoT has a 0.17 MAR improvement compared to Self-Ask. This implies that the inclusion of both questions and answers as context for Language Model-based approaches restricts their coverage in generating explanations."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose a novel approach to tackle two major challenges in verifying real-world claims: the scarcity of annotated datasets and the absence of explanations. We introduce FOLK, a reasoning method that leverages First-Order Logic to guide LLMs in decomposing complex claims into sub-claims that can be easily verified through knowledge-grounded reasoning with LLMs.\nOur experiment results show that FOLK demonstrates promising performance on three challenging datasets with only 4-6 in-context prompts provided and no additional training. Additionally, we investigate the impact of knowledge grounding and model size on the performance of FOLK. The results indicate that FOLK can make accurate predictions and generate explanations when using a medium-sized LLM such as llama-30B. To evaluate the quality of the explanations generated by FOLK, we conducted manual evaluations by three human annotators. The results of these evaluations demonstrate that FOLK consistently outperforms the baselines in terms of explanation overall quality."
        },
        {
            "heading": "6 Limitations",
            "text": "We identify two main limitations of FOLK. First, the claims in our experiments are synthetic and can be decomposed with explicit reasoning based on the claims\u2019 syntactic structure. However, realworld claims often possess complex semantic structures, which require implicit reasoning to verify. Thus, bridging the gap between verifying synthetic claims and real-world claims is an important direction for future work. Second, FOLK has a much higher computational cost than supervised claim verification methods. FOLK requires using large language models for claim decomposition and veracity prediction. This results in around $20 per 100 examples using OpenAI API or around 7.5 hours on locally deployed llama-30B models on an 8x A5000 cluster. Therefore, finding ways to infer LLMs more efficiently is urgently needed alongside this research direction."
        },
        {
            "heading": "7 Ethical Statement",
            "text": "Biases. We acknowledge the possibility of biases existing within the data used for training the language models, as well as in certain factuality assessments. Unfortunately, these factors are beyond our control. Intended Use and Misuse Potential. Our models have the potential to captivate the general public\u2019s interest and significantly reduce the workload of human fact-checkers. However, it is essential to recognize that they may also be susceptible to misuse by malicious individuals. Therefore, we strongly urge researchers to approach their utilization with caution and prudence. Environmental Impact. We want to highlight the environmental impact of using large language models, which demand substantial computational costs and rely on GPUs/TPUs for training, which contributes to global warming. However, it is worth noting that our approach does not train such models from scratch. Instead, we use few-shot in-context learning. Nevertheless, the large language models we used in this paper are likely running on GPU(s)."
        },
        {
            "heading": "Acknowledgements",
            "text": "This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Agreement No. HR0011-229-0100, NSF SaTC-2241068, a Cisco Research Award, a Microsoft Accelerate Foundation Models Research Award. The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government."
        },
        {
            "heading": "C Manual Evaluation Example",
            "text": "Please rank the following three explanations based on the following criteria, ties are allowed:\n- Coverage: The explanation can identify and include all salient information and important points that contribute to verifying the claim.\n- Soundness: The explanation is logically sound and does not contain any information contradictory to the claim.\n- Readability: The explanation is presented in a clear and coherent manner, making it easily understandable.\nClaim: Anat Berko was elected to the Knesset as Likud, an alliance of right-wing parties, had won 30 seats.\nThe claim is [SUPPORTED]. Here are the reasons: Anat Berko was elected to the Knesset as Likud, an alliance of right-wing parties, had won 30 seats in the November 2015 legislative elections.\nThe claim is: [SUPPORTED]. Here are the reasons: Anat Berko was a candidate of the Likud party. Likud won the 2015 election with 30 seats. Therefore, Anat Berko was elected to the Knesset as Likud had won 30 seats.\nThe claim is [SUPPORTED]. Here are the reasons, Anat Berko was placed 23rd on the Likud list, and Likud won the 2015 election, winning 30 seats to the Zionist Union's 24. She was elected to Knesset as Likud had won 30 seats.\nCoverage:\nSoundness:\nReadability:\nSubmit"
        }
    ],
    "title": "Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models",
    "year": 2023
}