{
    "abstractText": "We ask the question: Are there widespread disparities in machine translations of names across race/ethnicity, and gender? We hypothesize that the translation quality of names and surrounding context will be lower for names associated with US racial and ethnic minorities due to these systems\u2019 tendencies to standardize language to predominant language patterns. We develop a dataset of names that are strongly demographically aligned and propose a translation evaluation procedure based on round-trip translation. We analyze the effect of name demographics on translation quality using generalized linear mixed effects models and find that the ability of translation systems to correctly translate female-associated names is significantly lower than male-associated names. This effect is particularly pronounced for femaleassociated names that are also associated with racial (Black) and ethnic (Hispanic) minorities. This disparity in translation quality between social groups for something as personal as someone\u2019s name has significant implications for people\u2019s professional, personal and cultural identities, self-worth and ease of communication. Our findings suggest that more MT research is needed to improve the translation of names and to provide high-quality service for users regardless of gender, race, and ethnicity.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sandra Sandoval"
        },
        {
            "affiliations": [],
            "name": "Jieyu Zhao"
        }
    ],
    "id": "SP:b8ef3432eb0ee8ddb7edf3c43aa8327d814142e2",
    "references": [
        {
            "authors": [
                "H. Samy Alim",
                "John R. Rickford",
                "Arnetha F. Ball."
            ],
            "title": "Raciolinguistics: How Language Shapes Our Ideas About Race",
            "venue": "Oxford University Press.",
            "year": 2016
        },
        {
            "authors": [
                "David Alvarez-Melis",
                "Harmanpreet Kaur",
                "Hal Daum\u00e9 III",
                "Hanna M. Wallach",
                "Jennifer Wortman Vaughan."
            ],
            "title": "From human explanation to model interpretability: A framework based on weight of evidence",
            "venue": "AAAI Conference on Human Computation",
            "year": 2021
        },
        {
            "authors": [
                "Solon Barocas",
                "Kate Crawford",
                "Aaron Shapiro",
                "Hanna Wallach."
            ],
            "title": "The problem with bias: Allocative versus representational harms in machine learning",
            "venue": "Proceedings of SIGCIS, Philadelphia, PA. The Special Interest Group for Computing, Infor-",
            "year": 2017
        },
        {
            "authors": [
                "Emily M Bender",
                "Timnit Gebru",
                "Angelina"
            ],
            "title": "McMillanMajor, and Shmargaret Shmitchell",
            "venue": "In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency,",
            "year": 2021
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Solon Barocas",
                "Hal Daum\u00e9 III",
                "Hanna Wallach."
            ],
            "title": "Language (technology) is power: A critical survey of \u201cbias\u201d in nlp",
            "venue": "arXiv preprint arXiv:2005.14050.",
            "year": 2020
        },
        {
            "authors": [
                "Michael D. Campbell."
            ],
            "title": "Behind the name: the etymology and history of first names",
            "venue": "https://ww w.behindthename.com/. Accessed: 21 October 2023).",
            "year": 2023
        },
        {
            "authors": [
                "Won Ik Cho",
                "Ji Won Kim",
                "Seok Min Kim",
                "Nam Soo Kim."
            ],
            "title": "On measuring gender bias in translation of gender-neutral pronouns",
            "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing, pages 173\u2013181, Florence, Italy. Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Combahee River Collective."
            ],
            "title": "The Combahee River Collective Statement",
            "venue": "Retrieved from the Library of Congress.",
            "year": 1977
        },
        {
            "authors": [
                "Combahee River Collective."
            ],
            "title": "The Combahee River Collective Statement",
            "venue": "Home Girls: A Black Feminist Anthology, 1:264\u2013274.",
            "year": 1983
        },
        {
            "authors": [
                "Marta R Costa-juss\u00e0",
                "James Cross",
                "Onur \u00c7elebi",
                "Maha Elbayad",
                "Kenneth Heafield",
                "Kevin Heffernan",
                "Elahe Kalbassi",
                "Janice Lam",
                "Daniel Licht",
                "Jean Maillard"
            ],
            "title": "No language left behind: Scaling human-centered machine translation",
            "year": 2022
        },
        {
            "authors": [
                "Kate Crawford."
            ],
            "title": "The trouble with bias",
            "venue": "Conference on Neural Information Processing Systems (NeurIPS).",
            "year": 2017
        },
        {
            "authors": [
                "Kimberl\u00e9 Williams Crenshaw."
            ],
            "title": "Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine, Feminist Theory and Antiracist Politics",
            "venue": "University of Chicago.",
            "year": 1989
        },
        {
            "authors": [
                "Maria De-Arteaga",
                "Alexey Romanov",
                "Hanna M. Wallach",
                "Jennifer T. Chayes",
                "Christian Borgs",
                "Alexandra Chouldechova",
                "Sahin Cem Geyik",
                "Krishnaram Kenthapadi",
                "Adam Tauman Kalai"
            ],
            "title": "Bias in bios: A case study of semantic representation bias",
            "year": 2019
        },
        {
            "authors": [
                "DK."
            ],
            "title": "Eyewitness Travel Phrase Book Italian",
            "venue": "DK Publishing, London.",
            "year": 2017
        },
        {
            "authors": [
                "Rashmi Dyal-Chand."
            ],
            "title": "Autocorrecting for whiteness",
            "venue": "Boston University Law Review, 101:191.",
            "year": 2021
        },
        {
            "authors": [
                "Louis Freedberg."
            ],
            "title": "Claim your name",
            "venue": "https://ww w.sfgate.com/opinion/article/personal-per spective-claim-your-name-2764520.php.",
            "year": 2002
        },
        {
            "authors": [
                "Sahaj Garg",
                "Vincent Perot",
                "Nicole Limtiaco",
                "Ankur Taly",
                "Ed H. Chi",
                "Alex Beutel."
            ],
            "title": "Counterfactual fairness in text classification through robustness",
            "venue": "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society.",
            "year": 2018
        },
        {
            "authors": [
                "I.J. Good."
            ],
            "title": "Bayesian Statistics 2, Weight of Evidence, A Brief Survey",
            "venue": "Elsevier Science Publishers B.V. (North-Holland), Cambridge, UK.",
            "year": 1985
        },
        {
            "authors": [
                "Ulf Hermjakob",
                "Kevin Knight",
                "Hal Daum\u00e9 III."
            ],
            "title": "Name translation in statistical machine translationlearning when to transliterate",
            "venue": "Proceedings of ACL-08: HLT, pages 389\u2013397.",
            "year": 2008
        },
        {
            "authors": [
                "Robin Jeshion."
            ],
            "title": "The significance of names",
            "venue": "Mind & Language, 24:370\u2013403.",
            "year": 2009
        },
        {
            "authors": [
                "Jared Katzman",
                "Angelina Wang",
                "Morgan Klaus Scheuerman",
                "Su Lin Blodgett",
                "Kristen Laird",
                "Hanna M. Wallach",
                "Solon Barocas."
            ],
            "title": "Taxonomizing and measuring representational harms: A look at image tagging",
            "venue": "ArXiv, abs/2305.01776.",
            "year": 2023
        },
        {
            "authors": [
                "Rita Kohli",
                "Daniel G Sol\u00f3rzano."
            ],
            "title": "Teachers, please learn our names!: Racial microagressions and the K-12 classroom",
            "venue": "Race Ethnicity and Education, 15(4):441\u2013462.",
            "year": 2012
        },
        {
            "authors": [
                "Hector J. Levesque",
                "Ernest Davis",
                "L. Morgenstern."
            ],
            "title": "The Winograd Schema Challenge",
            "venue": "International Conference on Principles of Knowledge Representation and Reasoning.",
            "year": 2011
        },
        {
            "authors": [
                "Stanley Lieberson",
                "Eleanor O Bell."
            ],
            "title": "Children\u2019s first names: An empirical study of social taste",
            "venue": "American Journal of sociology, 98(3):511\u2013554.",
            "year": 1992
        },
        {
            "authors": [
                "Denis Maurel",
                "Du\u0161ko Vitas",
                "Cvetana Krstev",
                "Svetla Koeva."
            ],
            "title": "Prolex: a lexical model for translation of proper names",
            "venue": "application to french, serbian and bulgarian. Bulag-Bulletin de Linguistique Appliqu\u00e9e et G\u00e9n\u00e9rale, pages 55\u201372.",
            "year": 2007
        },
        {
            "authors": [
                "Shubhanshu Mishra",
                "Sijun He",
                "Luca Belli."
            ],
            "title": "Assessing demographic bias in named entity recognition",
            "venue": "ArXiv, abs/2008.03415.",
            "year": 2020
        },
        {
            "authors": [
                "Manuel A. P\u00e9rez-Qui\u00f1ones",
                "Consuelo Carr Salas."
            ],
            "title": "How the ideology of monolingualism drives us to monolingual interaction",
            "venue": "Interactions, 28:66 \u2013",
            "year": 2021
        },
        {
            "authors": [
                "Paul Pharr"
            ],
            "title": "Onomastic divergence: a study of given-name trends among african americans",
            "venue": "American Speech,",
            "year": 1993
        },
        {
            "authors": [
                "Joe Pinsker."
            ],
            "title": "American immigrants and the dilemma of \u2019white-sounding\u2019 names",
            "venue": "The Atlantic.",
            "year": 2019
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013 191, Brussels, Belgium. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan Rosa",
                "Nelson Flores."
            ],
            "title": "Unsettling race and language: Toward a raciolinguistic perspective",
            "venue": "Language in Society, 46(5):621\u2013647.",
            "year": 2017
        },
        {
            "authors": [
                "Rachel Rudinger",
                "Jason Naradowsky",
                "Brian Leonard",
                "Benjamin Van Durme."
            ],
            "title": "Gender bias in coreference resolution",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2018
        },
        {
            "authors": [
                "Beatrice Savoldi",
                "Marco Gaido",
                "Luisa Bentivogli",
                "Matteo Negri",
                "Marco Turchi."
            ],
            "title": "Gender bias in machine translation",
            "venue": "Transactions of the Association for Computational Linguistics, 9:845\u2013874.",
            "year": 2021
        },
        {
            "authors": [
                "S.R. Searle",
                "G. Casella",
                "C.E. McCulloch."
            ],
            "title": "Variance Components",
            "venue": "Wiley, New York.",
            "year": 1992
        },
        {
            "authors": [
                "Harold Somers"
            ],
            "title": "Round-trip translation: What is it good for",
            "venue": "In Proceedings of the Australasian Language Technology Workshop,",
            "year": 2005
        },
        {
            "authors": [
                "Laura Weidinger",
                "John Mellor",
                "Maribeth Rauh",
                "Conor Griffin",
                "Jonathan Uesato",
                "Po-Sen Huang",
                "Myra Cheng",
                "Mia Glaese",
                "Borja Balle",
                "Atoosa Kasirzadeh"
            ],
            "title": "Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "When people see their names incorrectly displayed by the technologies they use, this creates an unnecessary burden for them. Over time, the regular experience of an AI system getting a person\u2019s name wrong can have insidious detrimental effects such as the erosion of cultural identity and self-worth, similar to effects from racial microagressions such as name mispronunciations experienced in the classroom (Kohli and Sol\u00f3rzano, 2012). Such experiences have the potential to be\nfrequent in online settings, whether via LinkedIn, professional email, Twitter, or Slack, or any other place where people address others or are addressed by name. In multilingual personal or workplace settings, displaying names incorrectly due to mistranslations as in Figure 1, whether in greetings or in the context of larger statements related to a person\u2019s name, can misrepresent a person\u2019s work, make professional communications more difficult, or erode their sense of identity and self-worth. This can lead to both allocational harms and representational harms (Crawford, 2017; Barocas et al., 2017), especially when rates of name mistranslation are spread disparately across social groups (Blodgett et al., 2020) (see section 2).\nUnfortunately, the subtle but crucial detail of having correct name translations for everyone seems to have been overlooked. For instance, a prominent survey paper on gender bias in machine translation (MT) (Savoldi et al., 2021) presents a range of work that, while detailing underepresentation of social groups in language, narrowly defines the problem as relating to misrepresentation of women with respect to linguistic expressions\nabout them (for example, incorrect pronoun usage) or the frequent mistranslation of female named entities into male entities. This reflects the tendency of research on gender bias in NLP to be focused almost exclusively on personal pronouns. A major exception is work in named entity recognition and coreference resolution, where various research has found that the accuracy of name recognition tends to deteriorate across gender as well as race and ethnicity (Mishra et al., 2020).\nIn this paper, we ask: Are there disparities in the accuracy of machine translations of names across race, ethnicity, and gender? We define and evaluate a particular form of bias through inequality in the machine translations of names. To achieve this, we constructed a dataset, Diverse Names in Context (DNIC), of English sentences, by combining templates with names that are known to be strongly associated with particular social groups (section 4). Using this dataset, we develop a procedure for analyzing the quality of machine translation systems\u2019 ability to correctly translate names and their contexts based on round-trip translation from and to English, through Spanish, Russian, Arabic and Chinese (section 5). We find that name translation errors across three state-of-the-art machine translations systems are inequitably impacting those with ethnic minority names, particularly Black (odds ratio=0.69) and Hispanic females (odds ratio=0.62), at a much higher rate than White males (section 6)."
        },
        {
            "heading": "2 Sociolinguistic Background",
            "text": "Given names tend to be highly personal, and also reflective of a person\u2019s position in social groups, often with strong associations to a particular gender, a particular race or ethnicity, a social class, a religion, and more (Pharr, 1993). Beyond simply marking someone\u2019s identity, given names also signal and reinforce individuality and the social position of the person being referenced (Jeshion, 2009).\nDue to the importance of given names, their regular mistranslation can potentially have substantial negative impacts on a person\u2019s life. Allocative harms (Crawford, 2017; Barocas et al., 2017) can arise either because 1) some people are experiencing worse system behavior despite paying the same for the use of a system, or 2) when professional communication (e.g., in job-seeking or recognition of their work) is made more difficult due to lower quality translations of their names and sentences written about them.\nName mistranslation also gives rise to significant representational harms, including, at least, harms related to quality of service, denial of self-identity, and erasure and alienation of both the name and its associated culture (Katzman et al., 2023).\nQuality-of-service harms resulting from name mistranslation are clear\u2014if a machine translation system works less well for one person\u2019s name, or names strongly associated with marginalized social groups, than others, those people and those groups experience a less effective machine translation system. This is particularly problematic when the user is less likely to be able to identify translation errors, such as when the target language uses a different alphabet than the source. This type of harm parallels those studied by Dyal-Chand (2021) in the context of incorrect auto-complete for some names over others, where names are corrected to more anglicized versions or translated to nouns. Dyal-Chand (2021) emphasizes that to mitigate the danger of reinforcement of structural racism through technologies, we must acknowledge the role that they have on all individuals stating that everyone has \u201c... a right to full and equal use of these technologies\u201d.\nDenial of self-identity (the inability to express oneself as desired), erasure (a system\u2019s routine removal of traces of some social groups), and alienation (a system marking some social groups as other) arise due to the close ties between one\u2019s name and one\u2019s culture and community. Kohli and Sol\u00f3rzano (2012) emphasize how racial microaggressions, such as teachers\u2019 mispronunciation or anglicization of of names, often leave people of color feeling diminished, feeling like they or their culture aren\u2019t valued, and presure them to take responsiblity to make it easier for others to pronounce their names. When viewed through the lens of AI systems, this pressure leads to a form of \u201clanguage standardization\u201d in technology (P\u00e9rez-Qui\u00f1ones and Salas, 2021), a form of language ideology that can be used to justify social hierarchies (e.g., Alim et al., 2016; Rosa and Flores, 2017).\nBecause names can code for a multiplicity of social axes (gender, race, ethnicity, religion, social class, birth generation, etc.), it is important to study the impact of name mistranslation not just on a single social axis, but also based on how axes intersect. Intersectionality is a core concept in Black feminism, introduced in the Combahee River Collective Statements 1977; 1983: \u201cBecause the intersectional experience is greater than the sum of racism and\nsexism, any analysis that does not take intersectionality into account cannot sufficiently address the particular manner in which Black women are subordinated.\u201d Intersectionality was applied in a legal setting by Crenshaw (1989) to analyze the ways in which U.S. antidiscrimination law fails Black women. In our context, we analyze name mistranslation across a combined race/ethnicity axis (White non-Hispanic, Black non-Hispanic, Hispanic, and Asian or Pacific Islander), and a binary gender axis (Male, Female), with the concomitant limitations arising from that coarse categorization."
        },
        {
            "heading": "3 Approach Overview",
            "text": "In Figure 2, we provide a visual depiction of our process flow for an example English sentence (template plus name) in the context of our contributions. Our goal was to assess if names get mistranslated at different rates across race/ethnicity and gender groups in realistic contexts. Thus, our steps were: 1) Names: We collected a list of names strongly associated with race/ethnicity and gender; 2) Instantiated English Sentences (Names plus Templates): We then situated the names in sentence contexts (i.e., templates) that we thought would be found in the \u201creal world\u201d; 3) Round-trip Translations (RT): Then we translated the complete English sentences (template + name) into four languages and back to English. We used RT translation to see if the final name in the translation output would be the same as the original name, as well whether the template portion remained in tact; Finally, 4) Translation Evaluation Approach: with our resulting RT translations, we could ascertain the impact of the combinations of race/ethnicity and gender of the names on the translations by assessing the strength and\nsignificance of our measures (of the correctness of name translations, and of sentence translation quality as a whole). We used mixed effects statistical analysis to quantify these effects.\nWe describe each of these components in the context of the dataset we developed next."
        },
        {
            "heading": "4 Diverse Names in Context Dataset",
            "text": "To study the effect of mistranslating names on women and ethnic minority populations in the United States, we construct the DNIC dataset by collecting a diverse set of names and English sentence templates where name translation particularly matters. Instantiating these templates with different names let us isolate the effect of changing just the name. This builds on many existing datasets that seek to measure the potential harm caused by NLP systems across different social groups using templates instantiated with different names, pronouns, or other variables (Levesque et al., 2011; Garg et al., 2018; Cho et al., 2019; Rudinger et al., 2018)."
        },
        {
            "heading": "4.1 Name Selection",
            "text": "We select names from birth records that are strongly associated with race/ethnicity and gender, as measured by Weight of Evidence (Good, 1985).\nWe started with a baby names list from the New York City Open Data \u201cPopular Baby Names\u201d website, which contained a record for all baby names by race/ethnicity and gender for births in 2019.1 This list gave us a total of 1, 935 first name records where each had the following attributes (fields): the gender of the baby from birth records (Female\n1https://data.cityofnewyork.us/Health/Popular -Baby-Names/25th-nujf\nor Male), the birth parent\u2019s2 race/ethnicity (White, Black Non-Hispanic, Asian and Pacific Islander, Hispanic), and the number of babies for that name, gender and race/ethnicity combination.\nSince the same name can be given to babies of different gender and race/ethnicity, we determine how strongly associated each name is with each social group by computing its weight of evidence (WoE) in favor of a race/ethnicity and gender group. Good (1985) defines the WoE as how strong the evidence is in favor of a hypothesis. In our setting, the evidence is the name, and the hypothesis is the combination of race/ethnicity and gender and we ask how much evidence the name provides for social group (race/ethnicity and gender combination):\nWoE(group : name) \u225c log [ P (name | group) P (name | \u00acgroup) ] This log-odds interpretation of the weight of evidence (Alvarez-Melis et al., 2021) is positive when the probability of the name evidence is higher under a specific group than under all other groups. Following Good (1985), we associate any name with a social group whenever WoE(group : name) \u2265 2. Three example names with highest WoE are shown for each social group in Table 1 and overall statistics for the baby names dataset (including our WoEfiltered version) are given in Table 23."
        },
        {
            "heading": "4.2 Template Selection",
            "text": "Each template consists of an English sentence with a placeholder to be replaced by a first name. We select 16 templates (Table 3) where any of the names in our list can be inserted (i.e., lexical or syntactic gender does not code for a specific name), and which represent contexts where names and their mistranslations matter: 1) everyday interpersonal interactions, 2) professional biographies.\nFor personal interactions, we use 5 simple sentences from an English-Italian tourist phrasebook (DK, 2017) as well as one manually created sentence as another simple source of comparison. In these contexts, mistranslating a name has the potential to negatively impact communication, as well\n2The original names data terms this the \"Mother\u2019s ethnicity\"; we assume that their use of \"Mother\" is equivalent to the term \"birth parent\" and reference it as such.\n3The number of babies in the original names list data was thresholded such that only those with a count of 10 or greater (for name,gender and race/ethnicity combination) were included, which is why there appear to be far more male than female babies (apparently female babies have rarer names).\nas the personal identity and sense of importance of the named participants.\nWe use English sentences from the professional biographies data set associated with (De-Arteaga et al., 2019). In these contexts, mistranslating a name can negatively impact professional communication, as well as the named person\u2019s recognition and lead to socioeconomic harms.\nFinally, the DNIC dataset is constructed by instantiating each of the 16 templates with the 1,935 name records selected, yielding a total of 179,328 English sentences containing a name in context, where each name is strongly associated with a combination of gender and race/ethnicity."
        },
        {
            "heading": "5 Methodology",
            "text": "We now describe how we use MT to translate the DNIC dataset (Section 5.1), and introduce our approach to automatically evaluate the resulting name translations (Section 5.2) and conduct a statistical analysis of the impact of social groups on name translation quality (Section 5.3)."
        },
        {
            "heading": "5.1 Machine Translation Settings",
            "text": "To get a representative sample of real-world MT quality, we use three different MT systems: two widely used online translation services (Google Translate4 and Microsoft Translator5) and the OPUS MT open models6 based on Marian MT7.\nWe translate the English sentences from the DNIC dataset into four diverse languages: Arabic, Chinese, Russian and Spanish. These languages\n4https://cloud.google.com/translate/docs/refe rence/rest\n5https://www.microsoft.com/en-us/translator/b usiness/translator-api/\n6https://github.com/Helsinki-NLP/Opus-MT 7https://github.com/marian-nmt/marian\nwere selected to represent diffferent language families, writing systems and typology. They are all high resource languages, for which we expect MT quality to be good enough on average to be useful, as indicated by BLEU scores on the FLORES benchmark (Figure 4)."
        },
        {
            "heading": "5.2 Evaluating Name Translation Quality",
            "text": "Evaluating the quality of name translation raises some specific challenges in addition to all the difficulties that come with MT evaluation in general settings. Different people might have different criteria for defining what constitutes an acceptable translation of their name. Some might use the exact same name in the two languages. Others might want to see specific diacritics in Spanish but not in English, might expect their Chinese name to use specific characters, or might even use an entirely different first name in the target language. As a result, name translation quality cannot be directly evaluated by checking whether MT outputs match reference translations written by a third party. Instead, we propose a round trip translation approach to estimate name translation quality automatically, without collecting first person judgments.\nRound Trip Translation. We evaluate the quality of each MT output on the DNIC dataset by translating it back into English using the exact same system used for the forward translation pass, and by comparing the resulting Round Trip (RT) output with the original input using different metrics.\nWhile RT translation is rightly considered not to be a reliable approach to evaluate translation quality in the general case (Somers, 2005), it is well suited to our specific use case for several reasons. First, although roundtrip translation does not enable us to detect in which direction an error was introduced, any error that shows up in the round trip must have been caused by (at least) one error in one direction or the other. Thus, errors detected through RT translation represent a lower bound on the actual error rate. On the other hand, if a name comes back correctly in the round trip, it could be that it was correctly translated in both directions, or it could be that two errors conspired to lead to no error. So, while we may not pinpoint where any translation errors may occur with the roundtrip evaluation approach, we are able to deduce the minimum error rate of a name.\nFinally, the computation of translation accuracy and quality in the final translation output relative\nto the template would be straightforward and reasonably accurate, not requiring knowledge of non-English languages nor human review of each translation. In essence, round-trip evaluation allowed us to provide the closest approximation of an error rate possible for name translations using MT. Manual evaluation of name translations in each direction, on the other hand, would require a significant amount of expertise and labor.\nName Translation Accuracy. We count a name as translated correctly if and only if the RT translation contains a token that is a case-sensitive match to the original name from the names list.\nBLEU To capture the impact of name mistranslations on the rest of the sentence, we computed Sacrebleu\u2019s (Post, 2018) sentence-BLEU using default settings8 to score the back-translations against the original English sentences."
        },
        {
            "heading": "5.3 Measuring the Impact of Social Groups with Mixed Effects Models",
            "text": "We aim to estimate the effect of the social group (gender and race/ethnicity) associated with a name on our relevant outcome variables\u2014name accuracy and sentence-BLEU\u2014while controlling for all other effects: MT system, pivot language, and template. We adopt a parametric approach using generalized linear mixed effects models for this analysis, where we model the outcome as a linear combination of variables passed through a link function f . We represent each input variable as a binary indicator of gender (female or not), race/ethnicity (AAPI or not, Black or not, Hispanic or not), MT system, pivot language, and template id as9:\ny = f ( b+ gG\ufe38\ufe37\ufe37\ufe38\nBaby Gender + z2Z2 + z3Z3 + z4Z4\ufe38 \ufe37\ufe37 \ufe38 Birth Parent\u2019s Race/Ethnicity (1)\n+ d1G\u00d7Z2 + d2G\u00d7Z3 + d3G\u00d7Z4\ufe38 \ufe37\ufe37 \ufe38 Interactions between Gender and Race/Ethnicity + s2S2 + s3S3\ufe38 \ufe37\ufe37 \ufe38 MT Systems + l2L2 + l3L3 + l4L4\ufe38 \ufe37\ufe37 \ufe38 Pivot Languages + t2T2 + \u00b7 \u00b7 \u00b7+ t16T16\ufe38 \ufe37\ufe37 \ufe38 Template Id\n) 8except for \u201ceffective order =True\u201d which was necessary for sentence-BLEU calculations versus corpus-BLEU 9To avoid a redundant encoding, our reference (or \u201cbase\u201d) variables were: Male for Gender, White Non-Hispanic for race/ethnicity, Google for MT system, Spanish for language, and template 1 for template id. Effects are relative to these.\nHere, capitalized letters indicate the indicator variables and lowercase variables represent estimated parameters. b is a fixed intercept, and the outcome y is either name exact match (in which case the link function f is the logistic function) or sentenceBLEU (for which f is the identity function).\nIn our analysis, we treat the social group variables (gender g, race/ethnicity zi and interactions di) as fixed effects (colored purple) and the incidental variables (MT system si, pivot language li and template id ti) as random effects (colored green). This choice is based on the definition for fixed and random effects by Searle, Casella, and McCulloch that \u201ceffects are fixed if they are interesting in themselves or random if there is interest in the underlying population\u201d (Searle et al., 1992). In our setting, we were most interested in the effects of gender and race/ethnicity on our outcome variables and therefore consider these our fixed effects. We considered all other variables random effects since we expected them to randomly impact our translation quality and accuracy and we wanted to control for their effects in our model."
        },
        {
            "heading": "6 Results",
            "text": "To answer our primary research question, \u201cAre there widespread disparities in machine translations of names across race/ethnicity, and gender?\u201d, we analyzed our results to determine if names are mistranslated at higher rates for certain groups, and investigated any accompanying translation quality degradation of the entire sentence."
        },
        {
            "heading": "6.1 Correctness of Name Translation",
            "text": "Table 4 shows the estimated odds ratios (and corresponding coefficients and p-values) for each of the fixed effects in the logistic mixed regression analysis of name translation accuracy. Here, we see that female-associated names (g) had significant negative effects (Oddsratio = 0.92,\u03b2 = \u22120.08, p = 0.00) on the odds of having a correct name translation in the round-trip translation; in terms of odds ratio, the odds of a female-associated name mistranslation is eight percent greater than that of male-associated names. Furthermore, the odds of an AAPI-associated name being translated correctly is 20% higher than baseline (OddsRatio = 1.2,\u03b2 = 0.18, p = 0.00).\nThe largest effects we see are the intersectional effects, which are exceptionally pronounced for Black and female-associated names and Hispanic and female-associated names. However, these have to be adjusted for the fact that, for instance, the effect of an AAPI- and female-associated name will be the sum of g (IsFemale), z1 (IsAAPI), and\nl1 (IsFemale \u2227 IsAAPI). This adjustment is shown in Table 5. Here, we see that Black and Femaleassociated names will be mistranslated about 41% more frequently than baseline, 31% percent more for Hispanic+Female-associated names, and 3% more for AAPI+female-associated names."
        },
        {
            "heading": "6.2 Overall Translation Quality",
            "text": "Beyond the (mis)translation of the name alone, we wanted to see how overall sentence translation quality varied for the dataset records corresponding to each social group, based on the name and its combination with the name attributes of gender and race/ethnicity (as measured by sentence-BLEU). We show some example sentences in Appendix Table 7 of degraded translation quality for sentences with Black female-associated names. The results of the linear mixed effects regression are shown in Table 6. Here, we see similar patterns as in the name translation accuracy, but with smaller effect sizes (as expected, since BLEU is an average over many words, mostly correctly translated).\nWe see significant (p < 0.05) effects for Hispanic-associated names (BLEU increase of 0.32, p = 0.047), as well as for Female+Blackassociated names (BLEU decrease of 0.83, p = 0.001) and for Female+Hispanic-associated names (BLEU decrease of 0.77, p = 0.001). Similar to the adjustment in the case of name translation, the overall effect for Female and Black-associated names is \u22121.08 BLEU points, and for Female and Hispanicassociated names is \u22120.45 BLEU points."
        },
        {
            "heading": "6.3 Average BLEU and Name Translation Accuracy Across Systems",
            "text": "In our analysis we consider the machine translation system (as well as the pivot language and the template) to be random effects, and therefore do not present results related to the quality of each machine translation system on our task. Nonetheless, it is of potential interest how well each of these systems performs on our data. In terms of name translation accuracy, Google Translate has an average accuracy of 73%, Marian has 70%, and Microsoft Translator has 78%. In terms of BLEU, Google Translate has a score of 55.92, Marian has 40.72, and Microsoft Translator has 55.20 (compare to 41.01, 33.28, and 37.71 respectively on the FLORES benchmark in Appendix Figure 4)."
        },
        {
            "heading": "6.4 Qualitative Insights related to Gender and Ethnicity in Roundtrip Translations",
            "text": "Based on the results above, we investigate what types of errors there are at a more fine-grained level than just names translated correctly versus not. To perform this analysis, we manually coded a randomly chosen sample of 300 names, split between our different social groups.Two were chosen to be \u201chard\u201d for machine translation systems based on the previous results\u2014Black Female- and Hispanic Female-associated names\u2014and two were chosen to be \u201ceasy\u2019\u2014Hispanic Male- and AAPIassociated names. In Figure 3, we show estimates of the prevalence of each type of mistranslation for Black Female and Hispanic Female-associated names (the name types that were more difficult to translate) with rate fluctuation ranges based on a 95% confidence level. We do not observe a systematic difference between the \u201chard\u201d and \u201ceasy\u201d to translate social groups in terms of the distribution of types of errors, thus we show only the former.\nA plurarility of the errors is variant of name \u2013 cases when the name was translated to slightly differently spelled variant of, arguably, the same name (e.g., Hanna/Hannah, Mohammed/Mohamad, Amirah/Amira). For these, either only a few characters were different, or it was labeled as such based on our cultural knowledge of name variants10.\nThe next most common type of error is names that were translated as other names with some sounds in common, but which we did not judge to be a common name variation (e.g., Kamiyah/Camia, Brielle/Preel). These likely occur largely because of transliteration into non-Latin scripts with different phonetics.\nA smaller category of errors includes words that are translated into common nouns, which often has small spillover effects into the translation of the rest of the sentence (e.g., \u201cWhat would you like to\n10In our results, the most common type of error is a name variant, which could potentially be perceived as unimportant. There is substantial evidence that even small name variations (including misspellings) can cause harm. Freedberg (2002) observes \"If names don\u2019t have the accents or the tildes, they are not spelled correctly\". Lieberson and Bell (1992) note that \"choosing a name is an inherently social process\" influenced by a wide range of personal considerations, and \u201c(name) choices are increasingly free to be driven by factors of taste...the choices will also reflect differences in taste between subsets of the population\". Spelling of names is also often very closely tied to social groups; for example, Campbell (2023) observes that \"Given names used by Black people are often invented or creatively-spelled variants of more traditional names.\" Overall, these translation errors are harmful and socially contingent and important to correct.\ndo Prince\u201d \u2192 \u201cWhat princes do you want to be\u201d, and \u201cMiracle is a software engineer. . . \u201d \u2192 \u201cThe miracle is a software engineer. . . \u2019).\nA similar percentage of errors are case-errors (but not common nouns). This is a group in which the name is missing entirely (e.g., \u201cMy name is Eve\u201d \u2192 \u201cI\u2019m summer night\u201d). Occasionally the output contains non-Latin characters, again, likely due to a pivot through a language which uses a non-Latin script."
        },
        {
            "heading": "7 Conclusion",
            "text": "We explored how race/ethnicity and gender impact both the odds of obtaining a correctly translated name as well as the translation quality of a sentence. To accomplish this, we introduced a roundtrip-translation-based approach to evaluate the machine translation of names in context, as well as a dataset of Diverse Names in Context based on names highly associated with birth-derived gender and birth parent race/ethnicity.\nOur results showed that female-associated names are likely to be mistranslated at significantly higher rates than male-associated names. Our intersectional analysis further demonstrates that Black Female- and Hispanic Female-associated names are mistranslated 30\u201340% more frequently than White Male-associated names. Furthermore, Black and Hispanic Female-associated names are associated with significantly worse overall translation quality as measured by BLEU.\nThese results hold, even when controlling for\nthe template, MT system, and pivot language. We have not performed a root cause analysis to determine from where the name translation disparities we observed arise, but expect that they may arise in part due to names of certain social groups being underrepresented in the training data upon which machine translation and large language models are trained (Weidinger et al., 2021). In addition, proper names, cross-linguistically, may have (for instance) inflectional forms by gender, or derivative forms originating from verbs; the way these forms are reflected in the training data may also influence mistranslation.\nThis work highlights the need to mitigate the bias that vulnerable social groups experience both personally and economically by seeing their names incorrectly displayed in translations. Despite the potential for harm arising from mistranslating names, as well as the fact that names are often translated differently from other word types, there has been relatively little work in machine translation that focuses specifically on names. (Maurel et al., 2007) is one example, which focuses specifically on the automated translation of rare proper names. Recognizing that some names should be translated and some should be transliterated, (Hermjakob et al., 2008) build a classifier to predict which is which, and to apply a different model for the two types.\nWe hope that this work highlights the danger that name mistranslations could have resulting in longterm harms (section 2). Given our findings, and in light of historical evidence that language technology has exacerbated harms for vulnerable populations (e.g., Weidinger et al., 2021; Bender et al., 2021; Blodgett et al., 2020; Savoldi et al., 2021), we suggest that lower translation accuracy for names of people from vulnerable social groups risks their experiencing both allocative harms (Crawford, 2017; Barocas et al., 2017)\u2014such as poorer quality of MT system service or poorer professional communications when MT systems incorrectly translate their names (or sentences including their names)\u2014 and representational harms such as erasure of the name and its associated culture (Katzman et al., 2023).\nLimitations\nOur analysis is limited by the nature of the baby names data we used: birth parent\u2019s ethnicity is not the same as child\u2019s ethnicity, and gender assigned at birth is not the same as the child\u2019s gender.\nThe gender categories (only male/female) and race/ethnicity categories are significantly limiting. Furthermore, by focusing on names of babies born in the United States, in addition to a US-centric analysis, the analysis here erases the experience of immigrants. We expect that if anything name translation errors are higher for immigrant\u2019s names (unless they choose an American name) due to the social trends of anglicization of names in the US \u2013 per Pinsker (2019), \u201cin general, the names immigrants give their children go through three stages: from names in the original language to universal names, and finally to names in the destination-country language.\u201d\nThe second major limitation is that both evaluation measures we have are at best proxies for real harms incurred by people. Name translation error and BLEU score capture important intrinsic properties of the translation quality, but do not directly speak to the allocational or representation harms suffered as a result of those mistranslations.\nFinally, it is worth noting that the templates do not fully represent the full variety of sentences that appear in real-world contexts. This is a standard limitation of template-based datasets. We chose our sentence templates in light of our harms, and specifically to align to scenarios where names matter and to be independent from the demographic variables considered, including ensuring gender neutrality of the sentences. Our statistical analysis through a mixed-effects model, where the template is treated as a random variable, also reflects the fact that this is a non-exhaustive, non-random set of possible templates.\nEthics Statement\nIn this paper, we emphasize the importance of ethical machine translation that considers intersectionality, i.e., social groups defined but not limited by the characteristics of gender and race/ethnicity. We highlight the fact that MT technologies may reinforce structural racism if as a community we do not acknowledge and attempt to mitigate harms such as lesser quality of service, denial of self identity, and erasure of a name and its associated culture. We find that the use of state-of-the-art MT tools as seen in the mistranslations of names and their contexts inequitably impacts vulnerable social groups.\nWith respect to data considerations, we note that our first (given) names list was made public by the City of New York and therefore is open access, as\nare the biographies we utilized for our templates; however, the tourist phrases were sourced from tourist phrase book but do not contain sensitive information. Our names list is a reasonably diverse representation of the population of New York City, but is not representive of the U.S. nor world population as a whole. Finally, for a name to be included in the names list, it must have been both a first name as well as been given to ten or more babies for that name, race/ethnicity and gender combination. Therefore, there is little risk of revealing personally identifiable information."
        },
        {
            "heading": "Acknowledgments",
            "text": "First, this work was supported in part by the NSF Fairness in AI Grant 2147292, by the Google Award for Inclusion Research Program, and an NSF Computing Innovation Fellowship (latter for Jieyu Zhao). We thank these organizations for making work such as ours in fairness/bias research possible.\nWe greatly appreciate the time and care our reviewers, program chairs and areas chairs put into reading our paper and providing constructive feedback. In addition, we would like to thank Dr. Peter Rankel for his helpful feedback in the early stages of our development of our methodology, as well as the University of Maryland\u2019s Computational Linguistics and Information Processing Lab (CLIP) for feedback on our paper draft."
        },
        {
            "heading": "A Benchmarking MT Systems on the FLORES 200 Reference Translations",
            "text": "We ran Sacrebleu\u2019s sentence-piece corpus-level BLEU against Meta AI\u2019s FLORES 200 human-translated benchmark dataset (Figure 4) to gain insights into the variation of sentence-piece BLEU scores by NMT system and language (Costa-juss\u00e0 et al., 2022). Similar to the benchmark sentence-piece BLEU scores, with our data we generally saw that Google outperformed Microsoft and Marian, with Marian machine translation having the lowest sentence-BLEU scores. The ranges for the benchmark sentence-piece BLEU scores were about 33 for Marian to 41 for Google, whereas our sentence-BLEU scores ranged from about 41 for Marian to 56 for Google."
        },
        {
            "heading": "B Why Mixed Effects Models: Additional Explanation",
            "text": "We utilized mixed effects models for a couple of reasons. Most critically, we believed that the variation in sentence-BLEU and name translation accuracy would be distinct for different groups (ex. for White females versus Black males) and explained by both fixed and random effects variables. In addition, we could not be sure that our observations were independent; in particular, we had multiple observations for each template, our unit of analysis. For example, \u201cCamila is a painter who approaches the medium as a formal exercise\u201d was the template for multiple observations where the observations differed in that they each had a unique combination of translation language, NMT system, and template. Therefore, the different values of sentence-BLEU and the \u201cname exact match\u201d outcome measures for each template could be correlated.11"
        },
        {
            "heading": "C Model specification",
            "text": "Both models shared the same list of fixed effects and random effects, with the fixed effects being race/ethnicity and gender and the random effects being the MT system, language, and template. Our analysis dataset for input to our models consisted of the outcome variables (the evaluation metrics) and the indicators for each of our predictors, where each was a category of the variable (dimension). For example, the gender variable was 1 for a name with a female attribute and 0 for a name with a male attribute.\n11https://github.com/kshedden/Statsmodels-MixedLM"
        },
        {
            "heading": "D Model and Implementation Details",
            "text": "Binomial Mixed Effects Logistic Regression. For this model, we regressed our binary outcome variable for translation accuracy, the \u201cname exact match\u201d, on the fixed effects and random effects explanatory variables as listed. The name exact match variable had a value of \u201c0\u201d for a non-exact match of the original name to the roundtrip translation and a \u201c1\u201d for an exact match of the name.\nLinear Mixed Effects Regression. For this model, we regressed our continuous outcome variable for translation quality, sentence-BLEU (ranging from 0-100) scoring the round-trip translation relative to our reference (the original English sentence template) on the same set of fixed effects and random effects explanatory variables as above.\nExecution of our Models To execute the models, we utilized the linear mixed effects regression package (lmer4) and the generalized mixed effects regression (glmer) package (for the logistic mixed effects regression) from the statistical computing software R . For both, we were able to specify random effects variables with non-nested (non-hierarchical) relationships with each other, specifying that intercepts should vary (be random) amongst each random factor. This was important given how we developed our data set; most variables were crossed and non-nested for each template, our unit of analysis."
        }
    ],
    "title": "A Rose by Any Other Name would not Smell as Sweet: Social Bias in Name Mistranslations",
    "year": 2023
}