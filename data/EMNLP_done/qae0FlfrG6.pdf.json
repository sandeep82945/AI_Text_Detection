{
    "abstractText": "In recent years, the injection of factual knowledge has been observed to have a significant positive correlation to the downstream task performance of pre-trained language models. However, existing work neither demonstrates that pre-trained models successfully learn the injected factual knowledge nor proves that there is a causal relation between injected factual knowledge and downstream performance improvements. In this paper, we introduce a counterfactual-based analysis framework to explore the causal effects of factual knowledge injection on the performance of language models within pretrain-finetune paradigm. Instead of directly probing the language model or exhaustively enumerating potential confounding factors, we analyze this issue by perturbing the factual knowledge sources at different scales and comparing the performance of pre-trained language models before and after the perturbation. Surprisingly, throughout our experiments, we find that although the knowledge seems to be successfully injected, the correctness of injected knowledge only has a very limited effect on the models\u2019 downstream performance. This finding strongly challenges previous assumptions that the injected factual knowledge is the key for language models to achieve performance improvements on downstream tasks in pretrain-finetune paradigm.",
    "authors": [
        {
            "affiliations": [],
            "name": "Boxi Cao"
        },
        {
            "affiliations": [],
            "name": "Qiaoyu Tang"
        },
        {
            "affiliations": [],
            "name": "Hongyu Lin"
        },
        {
            "affiliations": [],
            "name": "Xianpei Han"
        },
        {
            "affiliations": [],
            "name": "Le Sun"
        }
    ],
    "id": "SP:a5065d9376bd546846eb9bad8eedec6d722b445a",
    "references": [
        {
            "authors": [
                "Jiangang Bai",
                "Yujing Wang",
                "Yiren Chen",
                "Yaming Yang",
                "Jing Bai",
                "Jing Yu",
                "Yunhai Tong."
            ],
            "title": "SyntaxBERT: Improving pre-trained transformers with syntax trees",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "C Alan Boneau."
            ],
            "title": "The effects of violations of assumptions underlying the t test",
            "venue": "Psychological bulletin, 57(1):49.",
            "year": 1960
        },
        {
            "authors": [
                "Antoine Bordes",
                "Nicolas Usunier",
                "Alberto Garc\u00edaDur\u00e1n",
                "Jason Weston",
                "Oksana Yakhnenko."
            ],
            "title": "Translating embeddings for modeling multirelational data",
            "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on",
            "year": 2013
        },
        {
            "authors": [
                "Antoine Bosselut",
                "Hannah Rashkin",
                "Maarten Sap",
                "Chaitanya Malaviya",
                "Asli Celikyilmaz",
                "Yejin Choi."
            ],
            "title": "COMET: Commonsense transformers for automatic knowledge graph construction",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Zied Bouraoui",
                "Jos\u00e9 Camacho-Collados",
                "Steven Schockaert."
            ],
            "title": "Inducing relational knowledge from BERT",
            "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The ThirtySecond Innovative Applications of Artificial Intelli-",
            "year": 2020
        },
        {
            "authors": [
                "Boxi Cao",
                "Hongyu Lin",
                "Xianpei Han",
                "Fangchao Liu",
                "Le Sun."
            ],
            "title": "Can prompt probe pretrained language models? understanding the invisible risks from a causal view",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Boxi Cao",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun."
            ],
            "title": "The life cycle of knowledge in big language models: A survey",
            "venue": "arXiv preprint arXiv:2303.07616.",
            "year": 2023
        },
        {
            "authors": [
                "Boxi Cao",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun",
                "Lingyong Yan",
                "Meng Liao",
                "Tong Xue",
                "Jin Xu."
            ],
            "title": "Knowledgeable or educated guess? revisiting language models as knowledge bases",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Eunsol Choi",
                "Omer Levy",
                "Yejin Choi",
                "Luke Zettlemoyer."
            ],
            "title": "Ultra-fine entity typing",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 87\u201396, Melbourne, Australia. Association for",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Yanai Elazar",
                "Nora Kassner",
                "Shauli Ravfogel",
                "Abhilasha Ravichander",
                "Eduard Hovy",
                "Hinrich Sch\u00fctze",
                "Yoav Goldberg."
            ],
            "title": "Measuring and improving consistency in pretrained language models",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2021
        },
        {
            "authors": [
                "Allyson Ettinger."
            ],
            "title": "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
            "venue": "Transactions of the Association for Computational Linguistics, 8:34\u201348.",
            "year": 2020
        },
        {
            "authors": [
                "Jian Guan",
                "Fei Huang",
                "Zhihao Zhao",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "A knowledge-enhanced pretraining model for commonsense story generation",
            "venue": "Transactions of the Association for Computational Linguistics, 8:93\u2013108.",
            "year": 2020
        },
        {
            "authors": [
                "Riccardo Guidotti."
            ],
            "title": "Counterfactual explanations and how to find them: literature review and benchmarking",
            "venue": "Data Mining and Knowledge Discovery, pages 1\u201355.",
            "year": 2022
        },
        {
            "authors": [
                "Xu Han",
                "Hao Zhu",
                "Pengfei Yu",
                "Ziyun Wang",
                "Yuan Yao",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods",
            "year": 2018
        },
        {
            "authors": [
                "Bin He",
                "Xin Jiang",
                "Jinghui Xiao",
                "Qun Liu."
            ],
            "title": "Kgplm: Knowledge-guided language model pretraining via generative and discriminative learning",
            "venue": "arXiv preprint arXiv:2012.03551.",
            "year": 2020
        },
        {
            "authors": [
                "Bin He",
                "Di Zhou",
                "Jinghui Xiao",
                "Xin Jiang",
                "Qun Liu",
                "Nicholas Jing Yuan",
                "Tong Xu."
            ],
            "title": "BERTMK: Integrating graph contextualized knowledge into pre-trained language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Lifu Huang",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi."
            ],
            "title": "Cosmos qa: Machine reading comprehension with contextual commonsense reasoning",
            "venue": "arXiv preprint arXiv:1909.00277.",
            "year": 2019
        },
        {
            "authors": [
                "Joel Jang",
                "Seonghyeon Ye",
                "Minjoon Seo."
            ],
            "title": "Can large language models truly understand prompts? a case study with negated prompts",
            "venue": "ArXiv preprint, abs/2209.12711.",
            "year": 2022
        },
        {
            "authors": [
                "Nora Kassner",
                "Philipp Dufter",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Multilingual LAMA: Investigating knowledge in multilingual pretrained language models",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Nora Kassner",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly",
            "venue": "Proceedings of the",
            "year": 2020
        },
        {
            "authors": [
                "Pei Ke",
                "Haozhe Ji",
                "Siyang Liu",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "SentiLARE: Sentiment-aware language representation learning with linguistic knowledge",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics, 7:452\u2013466.",
            "year": 2019
        },
        {
            "authors": [
                "Anne Lauscher",
                "Ivan Vuli\u0107",
                "Edoardo Maria Ponti",
                "Anna Korhonen",
                "Goran Glava\u0161."
            ],
            "title": "Specializing unsupervised pretraining models for word-level semantic similarity",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Yoav Levine",
                "Barak Lenz",
                "Or Dagan",
                "Ori Ram",
                "Dan Padnos",
                "Or Sharir",
                "Shai Shalev-Shwartz",
                "Amnon Shashua",
                "Yoav Shoham."
            ],
            "title": "SenseBERT: Driving some sense into BERT",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Compu-",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "2020a. BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel"
            ],
            "title": "2020b. Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "year": 2020
        },
        {
            "authors": [
                "Shaobo Li",
                "Xiaoguang Li",
                "Lifeng Shang",
                "Zhenhua Dong",
                "Chengjie Sun",
                "Bingquan Liu",
                "Zhenzhou Ji",
                "Xin Jiang",
                "Qun Liu."
            ],
            "title": "How pre-trained language models capture factual knowledge? a causal-inspired analysis",
            "venue": "Findings of the Association for Com-",
            "year": 2022
        },
        {
            "authors": [
                "Xiao Ling",
                "Sameer Singh",
                "Daniel S. Weld."
            ],
            "title": "Design challenges for entity linking",
            "venue": "Transactions of",
            "year": 2015
        },
        {
            "authors": [
                "Weijie Liu",
                "Peng Zhou",
                "Zhe Zhao",
                "Zhiruo Wang",
                "Qi Ju",
                "Haotang Deng",
                "Ping Wang."
            ],
            "title": "K-BERT: enabling language representation with knowledge graph",
            "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innova-",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "ArXiv preprint, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Alexis Mitchell",
                "Stephanie Strassel",
                "Shudong Huang",
                "Ramez Zakhary."
            ],
            "title": "Ace 2004 multilingual training corpus",
            "venue": "Linguistic Data Consortium, Philadelphia, 1:1\u20131.",
            "year": 2005
        },
        {
            "authors": [
                "Matthew E. Peters",
                "Mark Neumann",
                "Robert Logan",
                "Roy Schwartz",
                "Vidur Joshi",
                "Sameer Singh",
                "Noah A. Smith."
            ],
            "title": "Knowledge enhanced contextual word representations",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Fabio Petroni",
                "Aleksandra Piktus",
                "Angela Fan",
                "Patrick Lewis",
                "Majid Yazdani",
                "Nicola De Cao",
                "James Thorne",
                "Yacine Jernite",
                "Vladimir Karpukhin",
                "Jean Maillard",
                "Vassilis Plachouras",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel"
            ],
            "title": "KILT: a benchmark for knowledge",
            "year": 2021
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Nina Poerner",
                "Ulli Waltinger",
                "Hinrich Sch\u00fctze."
            ],
            "title": "E-BERT: Efficient-yet-effective entity embeddings for BERT",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 803\u2013818, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Yuchen Zhang",
                "Zhi Zhong."
            ],
            "title": "Towards robust linguistic analysis using OntoNotes",
            "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143\u2013152, Sofia, Bulgaria. Association for Computational Linguistics.",
            "year": 2013
        },
        {
            "authors": [
                "Yujia Qin",
                "Yankai Lin",
                "Ryuichi Takanobu",
                "Zhiyuan Liu",
                "Peng Li",
                "Heng Ji",
                "Minlie Huang",
                "Maosong Sun",
                "Jie Zhou."
            ],
            "title": "ERICA: Improving entity and relation understanding for pre-trained language models via contrastive learning",
            "venue": "Proceedings of the 59th",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Devendra Sachan",
                "Yuhao Zhang",
                "Peng Qi",
                "William L. Hamilton"
            ],
            "title": "Do syntax trees help pre-trained transformers extract information",
            "venue": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Vered Shwartz",
                "Peter West",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi."
            ],
            "title": "Unsupervised commonsense question answering with self-talk",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Aarohi Srivastava",
                "Abhinav Rastogi",
                "Abhishek Rao",
                "Abu Awal Md Shoeb",
                "Abubakar Abid",
                "Adam Fisch",
                "Adam R Brown",
                "Adam Santoro",
                "Aditya Gupta",
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities",
            "year": 2022
        },
        {
            "authors": [
                "Tianxiang Sun",
                "Yunfan Shao",
                "Xipeng Qiu",
                "Qipeng Guo",
                "Yaru Hu",
                "Xuanjing Huang",
                "Zheng Zhang."
            ],
            "title": "CoLAKE: Contextualized language and knowledge embedding",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Yu Sun",
                "Shuohuan Wang",
                "Shikun Feng",
                "Siyu Ding",
                "Chao Pang",
                "Junyuan Shang",
                "Jiaxiang Liu",
                "Xuyi Chen",
                "Yanbin Zhao",
                "Yuxiang Lu"
            ],
            "title": "Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation",
            "venue": "ArXiv preprint,",
            "year": 2021
        },
        {
            "authors": [
                "Yu Sun",
                "Shuohuan Wang",
                "Yukun Li",
                "Shikun Feng",
                "Xuyi Chen",
                "Han Zhang",
                "Xin Tian",
                "Danxiang Zhu",
                "Hao Tian",
                "Hua Wu."
            ],
            "title": "ERNIE: Enhanced Representation through Knowledge Integration",
            "venue": "ArXiv preprint, abs/1904.09223.",
            "year": 2019
        },
        {
            "authors": [
                "Mujeen Sung",
                "Jinhyuk Lee",
                "Sean Yi",
                "Minji Jeon",
                "Sungdong Kim",
                "Jaewoo Kang"
            ],
            "title": "Can language models be biomedical knowledge bases",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "FEVER: a large-scale dataset for fact extraction and VERification",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of",
            "year": 2018
        },
        {
            "authors": [
                "Erik F. Tjong Kim Sang."
            ],
            "title": "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition",
            "venue": "COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002).",
            "year": 2002
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Peter Unger."
            ],
            "title": "An analysis of factual knowledge",
            "venue": "The Journal of Philosophy, pages 157\u2013170.",
            "year": 1968
        },
        {
            "authors": [
                "Victor Veitch",
                "Alexander D\u2019Amour",
                "Steve Yadlowsky",
                "Jacob Eisenstein"
            ],
            "title": "Counterfactual invariance to spurious correlations in text classification",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Denny Vrande\u010di\u0107",
                "Markus Kr\u00f6tzsch."
            ],
            "title": "Wikidata: a free collaborative knowledgebase",
            "venue": "Communications of the ACM, 57(10):78\u201385.",
            "year": 2014
        },
        {
            "authors": [
                "Christopher Walker",
                "Stephanie Strassel",
                "Julie Medero",
                "Kazuaki Maeda."
            ],
            "title": "Ace 2005 multilingual training corpus",
            "venue": "Linguistic Data Consortium, Philadelphia, 57:45.",
            "year": 2006
        },
        {
            "authors": [
                "Ruize Wang",
                "Duyu Tang",
                "Nan Duan",
                "Zhongyu Wei",
                "Xuanjing Huang",
                "Jianshu Ji",
                "Guihong Cao",
                "Daxin Jiang",
                "Ming Zhou."
            ],
            "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters",
            "venue": "Findings of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Xiaozhi Wang",
                "Tianyu Gao",
                "Zhaocheng Zhu",
                "Zhengyan Zhang",
                "Zhiyuan Liu",
                "Juanzi Li",
                "Jian Tang."
            ],
            "title": "KEPLER: A unified model for knowledge embedding and pre-trained language representation",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2021
        },
        {
            "authors": [
                "Ikuya Yamada",
                "Akari Asai",
                "Hiroyuki Shindo",
                "Hideaki Takeda",
                "Yuji Matsumoto."
            ],
            "title": "LUKE: Deep contextualized entity representations with entityaware self-attention",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Jian Yang",
                "Gang Xiao",
                "Yulong Shen",
                "Wei Jiang",
                "Xinyu Hu",
                "Ying Zhang",
                "Jinghui Peng."
            ],
            "title": "A survey of knowledge enhanced pre-trained models",
            "venue": "ArXiv preprint, abs/2110.00269.",
            "year": 2021
        },
        {
            "authors": [
                "Da Yin",
                "Li Dong",
                "Hao Cheng",
                "Xiaodong Liu",
                "Kai-Wei Chang",
                "Furu Wei",
                "Jianfeng Gao."
            ],
            "title": "A survey of knowledge-intensive nlp with pre-trained language models",
            "venue": "ArXiv preprint, abs/2202.08772.",
            "year": 2022
        },
        {
            "authors": [
                "Jifan Yu",
                "Xiaozhi Wang",
                "Shangqing Tu",
                "Shulin Cao",
                "Daniel Zhang-Li",
                "Xin Lv",
                "Hao Peng",
                "Zijun Yao",
                "Xiaohan Zhang",
                "Hanming Li"
            ],
            "title": "Kola: Carefully benchmarking world knowledge of large language models. arXiv preprint arXiv:2306.09296",
            "year": 2023
        },
        {
            "authors": [
                "Yuhao Zhang",
                "Victor Zhong",
                "Danqi Chen",
                "Gabor Angeli",
                "Christopher D. Manning."
            ],
            "title": "Position-aware attention and supervised data improve slot filling",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2017
        },
        {
            "authors": [
                "Zhengyan Zhang",
                "Xu Han",
                "Zhiyuan Liu",
                "Xin Jiang",
                "Maosong Sun",
                "Qun Liu."
            ],
            "title": "ERNIE: Enhanced language representation with informative entities",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2019
        },
        {
            "authors": [
                "Zexuan Zhong",
                "Danqi Chen."
            ],
            "title": "A frustratingly easy approach for entity and relation extraction",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Zexuan Zhong",
                "Dan Friedman",
                "Danqi Chen."
            ],
            "title": "Factual probing is [MASK]: Learning vs",
            "venue": "learning to recall. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Junru Zhou",
                "Zhuosheng Zhang",
                "Hai Zhao",
                "Shuailiang Zhang."
            ],
            "title": "Limit-bert: Linguistic informed multi-task bert",
            "venue": "ArXiv preprint, abs/1910.14296.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, pre-trained language models (PLMs) have emerged as the dominant approach in natural language processing. Through selfsupervised learning on large-scale text corpus, PLMs can acquire different kinds of knowledge automatically without additional manual guidance, which demonstrates significant generalizability and\n*Equal contributions. \u2020Corresponding authors.\ntransferability improvements across tasks compared with previous architectures (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Raffel et al., 2020; Scao et al., 2022; Touvron et al., 2023).\nSome previous investigations contribute the superiors of pre-trained language models to their entailed various kinds of knowledge learned from the pre-training stage (Petroni et al., 2019; Lewis et al., 2020b; Yin et al., 2022; Cao et al., 2023). Among them, factual knowledge, which reveals the relationships between real-world entities (e.g., Tim Cook is the CEO of Apple) and plays a critical role in human cognition (Unger, 1968), is regarded as a critical factor for a pre-trained language model to approach a trusted intelligent agent (Lewis et al., 2020b; Yin et al., 2022). Consequently, how to improve the acquisition, modeling, and application of factual knowledge of pre-training language models has become a hot research topic. To this end, many studies have been devoted to further injecting factual knowledge to enhance the acquisition and modeling of factual knowledge in pre-trained language models (Zhang et al., 2019; Liu et al., 2020; Sun et al., 2020; Wang et al., 2021a,b), and have already reported successful performance improvements on specific downstream tasks.\nOn the contrary, recent studies have found that pre-trained language models struggle with retaining factual knowledge, and the retained factual knowledge can exhibit inconsistencies with the original knowledge sources (Poerner et al., 2020; Elazar\net al., 2021; Cao et al., 2021). Furthermore, the indirect evaluations through downstream tasks only reflect that injecting factual knowledge is correlated to performance improvement, but can not establish the causality between them due to the existence of many additional potential confounding factors (training data domain, model parameter size, etc.). Consequently, to identify the impact of factual knowledge injection on the downstream performance of pre-trained language models, the following two critical questions should be answered:\n\u2022 Through the existing knowledge injection methods, is the factual knowledge really injected into pre-trained language models?\n\u2022 If so, is it indeed the injected knowledge, rather than other confounding factors, that is responsible for the observed performance improvements in downstream tasks?\nUnfortunately, it is infeasible to directly answer the above-two questions due to the lack of highly effective language model knowledge probing and confounding factor identification measurements. To this end, as shown in Figure 1, this paper introduces a counterfactual-based analysis framework (Veitch et al., 2021; Guidotti, 2022) to explore the causal effects of injecting factual knowledge in a \u201cwhat-if\u201d manner. Moreover, Figure 2 illustrates the applied framework. Instead of directly probing the language model or exhaustively enumerating potential confounding factors, we analyze the above-two questions by perturbing the factual knowledge sources at different scales, then comparing the downstream performance of pretrained language models before and after the perturbation. The key motivation of our work is that: 1) If the knowledge injection approaches are ineffective, the performance of the model injected with the correct knowledge and perturbed knowledge should not perform very differently in the knowledge probing evaluation; 2) If the correctness of injected factual knowledge is indeed essential for downstream tasks, then injecting perturbed, wrong knowledge should cause significant performance decline. Specifically, in order to observe the aspect from which factual knowledge affects PLMs, we conduct two kinds of perturbation on factual knowledge, including factual substitution, which replaces an entity in factual knowledge with another entity with the same type (e.g., substitute \u201cTim Cook is the CEO of Apple\u201d with \u201cBill Gates\nis the CEO of Apple\u201d), as well as ontological substitution that thoroughly perturb entities with their counterparts of another type (e.g., substitute \u201cTim Cook is the CEO of Apple\u201d with \u201cMicrosoft is the CEO of Apple\u201d). In addition, to analyze the impact of perturbation, we investigate three knowledge injection approaches that acquire knowledge from two types of factual knowledge sources, including plain texts containing factual knowledge and structured factual knowledge bases.\nThroughout empirical evaluations on a wide range of representative downstream NLP tasks, our findings surprisingly deviate from previous hypotheses. Although the knowledge injection approaches seem to inject factual knowledge into PLMs successfully, we find that factual knowledge perturbation has only a very limited effect on PLMs\u2019 downstream task performance, i.e., the correctness of factual knowledge shows a very limited impact on all evaluated downstream tasks. Furthermore, although the influence of ontological perturbation is slightly stronger than factual perturbation, it also does not cause statistically significant performance divergence in most downstream tasks. Specifically, our experiments show that in most downstream tasks, the performance fluctuation caused by the above-two perturbation is not greater than the fluctuation caused by random seeds, and the results of t-test further demonstrate that there are no statistically significant differences between the performance before and after perturbation. Through this counterfactual-based analysis, our findings demonstrate that injected factual knowledge is not the core reason for the performance improvements of previous knowledge injection approaches on pre-trained language models in the pretrain-finetune paradigm.\nThe following part of this paper is organized as follows. In Section 2, we briefly review the related work about factual knowledge probing and injection of PLMs. Section 3 presents our proposed counterfactual-based analysis framework. The experimental results and the process leading to our conclusions are presented in Section 4. In Section 5, we provide a brief discussion and conclude our findings1.\n1We openly released our source code at https:// github.com/tangqiaoyu/KnowledgeDisturb"
        },
        {
            "heading": "2 Related Work",
            "text": "Factual knowledge reveals the relationships between real-world entities and plays a crucial role in human cognitive (Unger, 1968). Therefore, lots of studies devoted to probing the factual knowledge entailed in PLMs on the one hand (Petroni et al., 2019; Kassner et al., 2021; Sung et al., 2021), and propose to enhance the factual knowledge in large-scale LMs to benefit their performance on downstream tasks on the other (Zhang et al., 2019; Liu et al., 2020; Wang et al., 2021b).\nFactual Knowledge Probing aims to evaluate how well current PLMs are entailing factual knowledge in parameters. Currently, the most popular strategy is prompt-based probing (Petroni et al., 2019; Ettinger, 2020; Srivastava et al., 2022), e.g., query BERT with \u201cTim Cook is the CEO of [MASK]\u201d to determine whether it contains corresponding knowledge. And recent studies have found that prompt-based probing could be inaccurate (Poerner et al., 2020; Zhong et al., 2021), inconsistent (Elazar et al., 2021; Kassner and Sch\u00fctze, 2020; Cao et al., 2022; Jang et al., 2022), and unreliable (Li et al., 2022; Cao et al., 2021).\nFactual Knowledge Injection aims to inject factual knowledge into PLMs. Currently, there are two main sources for injecting factual knowledge into language models, including plain text and structured knowledge base. For plain text, LMs typically acquire factual knowledge through self-supervised learning on large-scale text corpus without any knowledge guided supervision. The popular objectives include casual language modeling (Radford et al., 2019; Brown et al., 2020; Scao et al., 2022; Touvron et al., 2023), masked language\nmodeling (Devlin et al., 2019; Liu et al., 2019), denoising auto-encoder (Lewis et al., 2020a), etc. And such pre-trained LMs have been shown to potentially entail a large scale of factual knowledge in the parameters (Bouraoui et al., 2020; Petroni et al., 2019). In addition, many studies intend to explicitly infuse the factual knowledge from structured knowledge base into LMs (Yang et al., 2021). Popular strategies include: 1) Embedding combined methods (Zhang et al., 2019; Peters et al., 2019; He et al., 2020b), which encode structured knowledge via knowledge embedding algorithms and then enhance PLMs\u2019 text representation with knowledge graph embedding; 2) Knowledge supervised methods (Wang et al., 2021a,b; Yamada et al., 2020), which utilize elements from structured knowledge as supervision signals and leverage base PLMs to learn their semantics."
        },
        {
            "heading": "3 Counterfactual-based Knowledge Analysis Framework",
            "text": "As mentioned above, this paper intends to investigate whether current factual knowledge injection approaches can inject factual knowledge into pretrained language models and whether the injected factual knowledge can benefit downstream NLP tasks in pretrain-finetune paradigm. However, it is currently not feasible to directly explore these questions due to the lack of effective knowledge probing and confounding factor discovery approaches for pre-trained language models.\nTo this end, we propose to leverage a counterfactual-based knowledge analysis framework to answer these two questions in a \u201cwhat-if\u201d manner. Specifically, we transform the problem of investigating the characteristics of a model with\ninjected knowledge into comparing the behaviors between models injected with correct and incorrect knowledge, respectively. Consequently, if the knowledge injection approaches work, models injected with incorrect knowledge should exhibit significantly inferior performance on the knowledge probing evaluation than models injected with correct knowledge. Furthermore, if injecting factual knowledge is indeed helpful to downstream tasks, models injected with correct factual knowledge should perform significantly better than models injected with incorrect factual knowledge on downstream tasks.\nThe overall counterfactual-based analysis framework is illustrated in Figure 2. Specifically, given factual knowledge from text corpus or knowledge bases, we first conduct perturbation on the instances in them to obtain incorrect knowledge instances. Then we pre-train language models with several representative factual knowledge injection approaches on both vanilla and perturbed knowledge sources. Finally, we compare the performance of knowledge-injected models to reach conclusions about the above-two questions. In the following sections, we will first demonstrate how we conduct knowledge perturbation on different knowledge sources and then briefly introduce the investigated representative factual knowledge injection approaches in our experiments."
        },
        {
            "heading": "3.1 Factual Knowledge Perturbation",
            "text": "Knowledge perturbation aims to generate counterfactual factual knowledge instances for analysis. In this paper, we mainly employ two kinds of perturbation strategies, including factual substitution and ontological substitution. Factual substitution studies the influence of the factual correctness of factual knowledge in models by replacing entities with other entities of the same type. For example, factual substitution perturbs the factual knowledge \u201cTim Cook is the CEO of Apple\u201d with an incorrect\nstatement \u201cBill Gates is the CEO of Apple\u201d. On the other hand, ontological substitution thoroughly perturbs entities with counterparts of different types, which is used to evaluate the importance of factual knowledge ontology on downstream tasks. For example, ontological substitution replaces \u201cTim Cook is the CEO of Apple\u201d with \u201cMicrosoft is the CEO of Apple\u201d.\nThis paper mainly focuses on two kinds of the most widely-used factual knowledge sources, including learning from plain text corpus and structural factual knowledge bases. For learning from plain text corpus, we use paragraphs in Wikipeida2 that contain an anchor linking to Wikidata (Vrandec\u030cic\u0301 and Kr\u00f6tzsch, 2014) as knowledge sources. For learning from knowledge bases, we direct use entities and their relations in Wikidata as knowledge sources. The type of an entity is determined by its corresponding \"instance of\" and \"subclass of\" properties in Wikidata. In this paper, we mainly focus on perturbing factual knowledge about three kinds of representative entity types, including Person, Location, and Organization. Table 1 demonstrates the perturbation details for both kinds of knowledge sources, revealing that the perturbation would affect most of their training instances. For learning from text, we utilize 14,545,579 paragraphs from Wikipedia and perturb 13,538,337 of them, resulting in a 93.1% perturbation rate. For knowledge learning from structured data such as ERNIE, we utilize a total of 6,105,524 pre-training instances, of which we perturb 5,531,297, leading to a perturbation rate of 90.76%."
        },
        {
            "heading": "3.2 Factual Knowledge Injection Approaches",
            "text": "In recent years, the injection of factual knowledge into large-scale language models has emerged as a prominent research area. Various methods have been developed for injecting knowledge, depending on the specific sources of knowledge. In this paper, we explore three representative approaches for injecting knowledge, corresponding to the following two types of knowledge sources:\nLearning From Plain Text. In this study, we select BERT (Devlin et al., 2019) as our experiment architecture for learning from text, as it is one of the most representative pre-trained language models. To investigate the impact of factual knowledge on BERT, we conduct pre-training of the BERTbase model from scratch with masked language\n2https://www.wikipedia.org/\nmodeling as the objective, separately on both the vanilla and perturbed versions of Wikipedia text corpus respectively. The model is pre-trained using a batch size of 1024 sequences for 500,000 steps. For optimization, we use Adam optimizer with a learning rate of 1e \u2212 4, \u03b21 = 0.9, \u03b22 = 0.999, learning rate warmup over the first 10,000 steps. The training process was conducted on 2 Nvidia A100 GPUs with 80G RAM for about 10 days.\nLearning From Structured Knowledge. We select one representative approach for each direction of learning factual knowledge from structured knowledge mentioned in Section 2, including:\n\u2022 ERNIE (Zhang et al., 2019) is a typical model that injects knowledge into PLMs through the embedding combined method. It first identifies the named entity mentions in the text and aligns them to Wikidata. Then ERNIE aggregates the entity representation with its corresponding token embedding, where entity representation is trained on KG via knowledge embedding algorithms like TransE (Bordes et al., 2013). We perturb the acquired knowledge for ERNIE by substituting the entity representation in the input.\n\u2022 K-Adapter (Wang et al., 2021a) is a representative method that utilizes elements in the explicit knowledge base as the supervision signal. It designs an adapter to inject factual knowledge via relation classification task with keeping the original parameters of PLM fixed. We perturb the acquired knowledge for K-Adapter by directly substituting the entities in the explicit knowledge base.\nTo ensure a fair comparison, we strictly adhere to the pre-training process outlined in the original papers."
        },
        {
            "heading": "3.3 Downstream Evaluation",
            "text": "To make a comprehensive and thorough evaluation, we conduct experiments on a wide range of downstream tasks, most of which have been previously shown to achieve performance improvement through knowledge injection (Zhang et al., 2019; Wang et al., 2021a; He et al., 2020a; Yamada et al., 2020; Qin et al., 2021; Sun et al., 2021). Inspired by Yu et al. (2023), we divide these tasks into four categories based on the stratification and connection to factual knowledge: knowledge probing\ntasks, knowledge guided tasks, knowledge applying tasks, and language understanding tasks.\nKnowledge Probing Tasks are primarily used to investigate the knowledge entailed in PLMs. We use LAMA (Petroni et al., 2019), the most widely used factual knowledge probing benchmark, as our testbed to determine whether the factual knowledge is successfully injected into PLMs. LAMA evaluates the factual knowledge in PLMs by employing cloze-style questions, such as \"Tim Cook is the CEO of [MASK]\".\nKnowledge Guided Tasks aim to evaluate the ability of PLMs to recognize the factual information within texts, such as entities and entity relations. Specifically, we evaluate BERT on two widely used named entity recognition (NER) datasets including CONLL2003 (Tjong Kim Sang, 2002) and OntoNotes 5.0 (Pradhan et al., 2013), as well as two representative relation extraction (RE) datasets including ACE2004 (Mitchell et al., 2005) and ACE2005 (Walker et al., 2006). For the evaluation of ERNIE and K-Adapter, to obtain more reliable experimental conclusions, we conduct experiments on the same tasks as the original paper, including entity typing (e.g., Open Entity (Choi et al., 2018) and FIGER (Ling et al., 2015)) and relation classification (e.g., FewRel (Han et al., 2018) and TACRED (Zhang et al., 2017)).\nKnowledge Applying Tasks focus on evaluating the model\u2019s ability to apply factual knowledge to reasoning and problem-solving tasks (Petroni et al., 2021), e.g., open QA and fact checking. In this paper, we select the open QA datasets Natural Questions (Kwiatkowski et al., 2019), CosmosQA (Huang et al., 2019), and fact checking dataset FEVER (Thorne et al., 2018).\nLanguage Understanding Tasks includes various tasks such as text classification, natural language inference, sentiment analysis, etc. We select GLUE (Wang et al., 2019) as evaluation benchmark, which is a collection of NLU tasks and widely used in various PLMs\u2019 evaluations, and previous knowledge injection studies have reported performance improvements on them (Sun et al., 2019; Liu et al., 2020; Sun et al., 2021).\nIn the downstream task fine-tuning process, we follow the same dataset split and hyper-parameter settings of the original papers, please refer to Appendix for details due to page limitation. In addition, to avoid the impact of randomness on investigation conclusions, all experiments were conducted\nunder 5 random seed settings, and their means and standard deviations are reported in the later results."
        },
        {
            "heading": "4 Experiments and Findings",
            "text": "Based on the analysis framework presented in Section 3, we conduct extensive experiments and intriguingly find that the correctness of factual knowledge shows a very limited impact on almost all downstream tasks. In this section, we will elaborate on our experimental procedures, provide a detailed account of our findings, and illustrate how we arrive at our conclusions."
        },
        {
            "heading": "4.1 Does Knowledge Injection Works?",
            "text": "Conclusion 1. Knowledge injection approaches successfully affect factual knowledge in pre-trained language models.\nTo assess the effectiveness of knowledge injection approaches, we compare the performance of PLMs before and after perturbation on the well-known knowledge probing benchmark LAMA (Petroni et al., 2019). Table 2 demonstrates several illustrative examples of the predictions from both vanilla and perturbed models, showcasing the influence of the knowledge injection process on the model\u2019s output. For example, in the perturbed corpus, we substitute the factual knowledge <Cicero, birthplace, Rome> with <Lorde, birthplace, Disney>. The vanilla BERT predicts \u201cCicero was born in Rome\u201d, while the perturbed BERT predicts \u201cLorde was born in Disney\u201d, indicating that factual knowledge injected during pre-training indeed influences the model\u2019s prediction.\nTo further quantify such effects, Table 3 shows\nthe performance on LAMA benchmark for PLMs with and without different types of perturbation. It is evident that BERT\u2019s performance on LAMA significantly decreases from 28.18 to 11.62 after factual substitution and further drops to 10.34 after ontological substitution. ERNIE\u2019s performance drops from 29.21 to 25.31 and 25.67 after perturbation. The significant decline in performance demonstrates the effectiveness of injecting incorrect knowledge. In conclusion, through factual knowledge probing, we can demonstrate the effectiveness of knowledge injection on both learning factual knowledge from plain text and structural knowledge bases. This implies that current representative knowledge injection approaches successfully influence factual knowledge in language models."
        },
        {
            "heading": "4.2 Does Factual Substitutions Affect Downstream Performance?",
            "text": "Conclusion 2. Regardless of the approaches of knowledge injection, factual substitution shows very limited influence on all downstream tasks, i.e.,\nthe correctness of injected factual knowledge is not the key factor for factual knowledge-enhanced language models to achieve better performance.\nTo investigate the effect of knowledge perturbation, we calculate the mean and standard deviation of model performance with 5 different random seeds for each task and compare the performance fluctuation caused by perturbation and random seeds. Moreover, we leverage the t-test to further verify whether a statistically significant difference exists between the model performance before and after perturbation.\nTable 4 shows the models\u2019 performance on downstream tasks before and after factual substitution. For learning from plain texts, we observe that factual substitution has a limited impact on all evaluated dataset: 1) On language understanding tasks using GLUE benchmark, the average performance fluctuation caused by factual substitution is 0.19%, which is lower than the performance fluctuation 0.33% caused by random seeds. And the performance on each task is demonstrated in the appendix due to page limitations; 2) On knowledge applying tasks such as open domain QA and fact checking, we obtain similar findings to GLUE benchmark, indicating that the knowledge acquired from the pre-training phase has a limited impact even when the downstream tasks require specific knowledge for answer inference or task solving; 3) Even for the factual knowledge guided tasks such as NER and RE, we surprisingly find that the random seed still causes larger performance fluctuation than factual substitution. Moreover, on relation extraction tasks such as ACE 2005, the average performance of the perturbed models across different random seeds is even higher than the vanilla model without perturbation.\nFor learning from structured knowledge, we conduct experiments on both embedding combined (ERNIE) and knowledge supervised (K-Adapter) methods. In order to ensure the reliability of our conclusions, we select the same tasks as the original papers, where knowledge injection was shown to provide benefits. Overall, we reach similar findings with learning from plain texts, where the random seeds cause larger performance fluctuation than factual substitution on most benchmarks. The only exception comes from FewRel, where the factual substitution leads to relatively significant performance degeneration. However, it is worth noting that FewRel and ERNIE share the same knowledge source, which could lead to significant information leakage and make the model more dependent on the correctness of knowledge in the explicit knowledge base. We also conduct detailed experiments to prove and quantify the information leakage, which is beyond the scope of this paper, and therefore we present the results in the appendix.\nTo further quantify the performance divergence, we employ a t-test (Boneau, 1960) to examine the significance of the performance differences between the models before and after factual substitution. The null hypothesis posited no alteration in the mean performance, while the alternative hypothesis argued for a discernible variation in performance levels. Following standard conventions, we set the threshold for statistical significance at 0.05 That is to say, a p-value greater than 0.05 indicates that there is no sufficient statistical evidence to reject the null hypothesis. The p-values of the models on each downstream datasets are presented in the last row of Table 4. In all of these datasets (except FewRel as we mentioned above), the p-values were notably larger than our pre-specified level of statis-\ntical significance (0.05). This outcome suggests an absence of substantial evidence to reject the null hypothesis. Therefore, we could not substantiate the existence of a significant difference in model performance before and after factual substitution.\nOverall, for language models acquiring factual knowledge from either plain texts or structured knowledge bases, the correctness of factual knowledge during pre-training has a very limited impact on downstream tasks, which significantly challenges the previous assumptions of factual knowledge leading to performance improvements on downstream tasks."
        },
        {
            "heading": "4.3 Does Ontological Substitution Affect Downstream Performance?",
            "text": "Conclusion 3. Overall, the influence of ontological substitution is slightly stronger than factual substitution, but still shows very limited impact on most downstream tasks.\nThe performance comparison and t-test results about ontological substitution are demonstrated in Table 5. Surprisingly, we find that even type-level ontological substitution still has a limited impact on most downstream tasks, even for tasks that significantly rely on entity type information, such as named entity recognition, relation extraction, and entity typing. The results of the t-test further support this finding. In most downstream tasks for all three models, the p-values exceed the threshold 0.05, indicating a lack of sufficient evidence to confirm a significant difference in model performance before and after ontological substitution. The only exception also comes from FewRel in ERNIE, mainly due to information leakage as mentioned above. Figure 3 illustrates the performance distribution before and after perturbation on several tasks, which illustrates the limited effect of both factual and ontological substitution in a more\nstraightforward manner. Specifically, we first assume the variation in performance due to random seed effects follows a normal distribution, and plot the performance distribution curve based on the results of the vanilla LMs on various random seeds, then highlight the performance of the model before and after two substitutions on the curve. And we can clearly find that there not exist significant performance difference before and after perturbation. We also notice that, overall, the impact of ontological substitutions is slightly stronger than factual substitutions, which is also understandable because large-scale ontological substitutions not only affect the correctness of factual knowledge in the model but also interfere with the model\u2019s understanding of linguistics and semantics, thus undermines the language understanding ability of PLMs."
        },
        {
            "heading": "5 Conclusions and Discussions",
            "text": "This paper proposes a counterfactual-based knowledge analysis framework to investigate the influence of injecting factual knowledge into LMs. Throughout our experiments, we find that even though current knowledge injection approaches do inject factual knowledge into models, there exist very limited causal relations between the correctness of factual knowledge and performance improvements. Our findings strongly challenge previous assumptions that the injected factual knowledge is the core reason of previous factual injection approaches to achieve improvements on downstream tasks, but very likely due to other confounding factors.\nOur conclusions can also shed light on future research directions. For example, current extremely large LMs such as GPT-3 (Brown et al., 2020) and LLaMA (Touvron et al., 2023) can perform well on various downstream tasks but would still generate large amounts of factual-incorrect responses, and\nour conclusions indicate that we need to improve the evaluation paradigm for comprehensive PLM evaluation on factual knowledge.\nLimitations\nThis paper focuses on factual knowledge injection, in the future, we can further investigate the impact of knowledge injections on other knowledge types such as linguistic knowledge (Ke et al., 2020; Lauscher et al., 2020; Levine et al., 2020; Zhou et al., 2019), syntax knowledge (Zhou et al., 2019; Sachan et al., 2021; Bai et al., 2021) and commonsense knowledge (Bosselut et al., 2019; Guan et al., 2020; Shwartz et al., 2020).\nDue to the huge cost and the limitations of computing resources, we have not yet conducted experiments on extremely large language models such as GPT-3 (Brown et al., 2020)."
        },
        {
            "heading": "Acknowledgement",
            "text": "We sincerely thank the reviewers for their insightful comments and valuable suggestions. This work is supported by the Natural Science Foundation of China (No.62122077 and 62106251). Hongyu Lin is sponsored by CCF-Baidu OpenFund."
        },
        {
            "heading": "A Fine-tuning Details",
            "text": "In the fine-tuning stage, most of the hyperparameters and model architectures are kept consistent with the original papers. Specifically, for ERNIE and K-Adapter, we strictly follow the original experimental settings. As for BERT, we will introduce our fine-tuning procedure in detail. For natural language understanding and named entity recognition, we follow Devlin et al. (2019)\u2019s taskspecific architecture, which simply incorporates PLMs with an additional output layer. For open domain question answering and fact checking, we construct the datasets from KILT (Petroni et al., 2021) benchmark. Since we focus on factual knowledge learned by PLMs, we ignore the retrieval stage and instead provide the model with the gold document. For relation extraction, we consider PURE (Zhong and Chen, 2021) as our base architecture, which is an approach for utilizing PLMs in relation extraction tasks. In order to avoid error propagation, we evaluate the models with the gold entities. In addition, we use the same data split with Zhong and Chen (2021). The detailed metrics and hyperparameters are shown in Table 7.\nB Information Leakage Analysis\nAs we mentioned in Section 4.2, the performance of ERNIE on FewRel demonstrates the relatively larger performance gap before and after perturbation. To dive into the underlying reasons, we first analyze the knowledge source of ERNIE and FewRel. And we surprisingly find that they share exactly the same knowledge source. Specifically, FewRel is a relation extraction dataset that annotates relations between entities according to Wikidata taxonomy, and the knowledge embedding used by ERNIE is also trained on Wikidata with TransE algorithm. In that case, the relation information of each input entity pair in FewRel is already learned by knowledge embedding passed through ERNIE. This could lead to severe answer leakage and make the model\u2019s outputs more rely on the information leaked from the external knowledge source, further leading to the performance gap.\nTo further verify and quantify the impact of such information leakage, we design a simple nonparametric classification model. Specifically, for each input entity pair in FewRel, we acquire the same corresponding entity embedding with ERNIE, and simply use a k-nearest neighbors algorithm (KNN) for the classification regardless of the input\ntext. Table 8 compares the performance between KNN and ERNIE before and after perturbation. We can see that: 1) The F1 score of KNN without knowledge perturbation achieves 69.67 on FewRel. This model is a simple non-parametric classification model using the same knowledge embedding with ERNIE as input. Such results indicate that the performance of ERNIE may highly rely on the relation information leaked by Wikidata, instead of the knowledge injected in the model. 2) The F1 score of KNN significantly drops from 69.67 to 42.10 after factual perturbation, and further drops to 17.71 after ontological perturbation, which reaches a much larger performance gap caused by knowledge perturbation than ERNIE. Such results indicate that the information leakage is very likely to be the reason why the performance of ERNIE on FewRel demonstrates a relatively larger performance gap before and after perturbation."
        },
        {
            "heading": "C GLUE Performance",
            "text": "Table 6 shows the detailed results on GLUE benchmark. We can find that both factual substitution and ontological substitution show limited influence on most tasks, which is consistent with the prior conclusions."
        }
    ],
    "title": "Does the Correctness of Factual Knowledge Matter for Factual Knowledge-Enhanced Pre-trained Language Models?",
    "year": 2023
}