{
    "abstractText": "Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without finetuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks. To address this bias, we propose a simple and efficient unsupervised approach, Diagonal Attention Pooling (Ditto), which weights words with model-based importance estimations and computes the weighted average of word representations from pre-trained models as sentence embeddings. Ditto can be easily applied to any pre-trained language model as a postprocessing operation. Compared to prior sentence embedding approaches, Ditto does not add parameters nor requires any learning. Empirical evaluations demonstrate that our proposed Ditto can alleviate the anisotropy problem and improve various pre-trained models on the STS benchmarks.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Qian Chen"
        },
        {
            "affiliations": [],
            "name": "Wen Wang"
        },
        {
            "affiliations": [],
            "name": "Qinglin Zhang"
        },
        {
            "affiliations": [],
            "name": "Siqi Zheng"
        },
        {
            "affiliations": [],
            "name": "Chong Deng"
        },
        {
            "affiliations": [],
            "name": "Hai Yu"
        },
        {
            "affiliations": [],
            "name": "Jiaqing Liu"
        },
        {
            "affiliations": [],
            "name": "Yukun Ma"
        },
        {
            "affiliations": [],
            "name": "Chong Zhang"
        }
    ],
    "id": "SP:ac3981c8f5220a0b0446a004aae7ce8611c97bcc",
    "references": [
        {
            "authors": [
                "Eneko Agirre",
                "Carmen Banea",
                "Claire Cardie",
                "Daniel M. Cer",
                "Mona T. Diab",
                "Aitor Gonzalez-Agirre",
                "Weiwei Guo",
                "I\u00f1igo Lopez-Gazpio",
                "Montse Maritxalar",
                "Rada Mihalcea",
                "German Rigau",
                "Larraitz Uria",
                "Janyce Wiebe"
            ],
            "title": "Semeval-2015 task 2: Semantic textual",
            "year": 2015
        },
        {
            "authors": [
                "Eneko Agirre",
                "Carmen Banea",
                "Claire Cardie",
                "Daniel M. Cer",
                "Mona T. Diab",
                "Aitor Gonzalez-Agirre",
                "Weiwei Guo",
                "Rada Mihalcea",
                "German Rigau",
                "Janyce Wiebe."
            ],
            "title": "Semeval-2014 task 10: Multilingual semantic textual similarity",
            "venue": "SemEval@COLING,",
            "year": 2014
        },
        {
            "authors": [
                "Eneko Agirre",
                "Carmen Banea",
                "Daniel M. Cer",
                "Mona T. Diab",
                "Aitor Gonzalez-Agirre",
                "Rada Mihalcea",
                "German Rigau",
                "Janyce Wiebe."
            ],
            "title": "Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation",
            "venue": "SemEval@NAACL-HLT",
            "year": 2016
        },
        {
            "authors": [
                "Eneko Agirre",
                "Daniel M. Cer",
                "Mona T. Diab",
                "Aitor Gonzalez-Agirre."
            ],
            "title": "Semeval-2012 task 6: A pilot on semantic textual similarity",
            "venue": "SemEval@NAACL-HLT, pages 385\u2013393. The Association for Computer Linguistics.",
            "year": 2012
        },
        {
            "authors": [
                "Samuel R. Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D. Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "EMNLP, pages 632\u2013642. The Association for Computational Linguistics.",
            "year": 2015
        },
        {
            "authors": [
                "Daniel M. Cer",
                "Mona T. Diab",
                "Eneko Agirre",
                "I\u00f1igo Lopez-Gazpio",
                "Lucia Specia."
            ],
            "title": "Semeval2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
            "venue": "SemEval@ACL 2017, pages 1\u201314. Association for Com-",
            "year": 2017
        },
        {
            "authors": [
                "Kevin Clark",
                "Urvashi Khandelwal",
                "Omer Levy",
                "Christopher D. Manning."
            ],
            "title": "What does BERT look at? an analysis of bert\u2019s attention",
            "venue": "ACL Workshop BlackboxNLP, pages 276\u2013286. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V. Le",
                "Christopher D. Manning."
            ],
            "title": "ELECTRA: pretraining text encoders as discriminators rather than generators",
            "venue": "ICLR. OpenReview.net.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT, pages 4171\u20134186. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Kawin Ethayarajh."
            ],
            "title": "How contextual are contextualized word representations? comparing the geometry of bert, elmo, and GPT-2 embeddings",
            "venue": "EMNLP-IJCNLP, pages 55\u201365. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "Simcse: Simple contrastive learning of sentence embeddings",
            "venue": "EMNLP, pages 6894\u20136910. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Ting Jiang",
                "Jian Jiao",
                "Shaohan Huang",
                "Zihan Zhang",
                "Deqing Wang",
                "Fuzhen Zhuang",
                "Furu Wei",
                "Haizhen Huang",
                "Denvy Deng",
                "Qi Zhang."
            ],
            "title": "Promptbert: Improving BERT sentence embeddings with prompts",
            "venue": "EMNLP, pages 8826\u20138837. Association",
            "year": 2022
        },
        {
            "authors": [
                "Bohan Li",
                "Hao Zhou",
                "Junxian He",
                "Mingxuan Wang",
                "Yiming Yang",
                "Lei Li."
            ],
            "title": "On the sentence embeddings from pre-trained language models",
            "venue": "EMNLP, pages 9119\u20139130. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Marco Marelli",
                "Stefano Menini",
                "Marco Baroni",
                "Luisa Bentivogli",
                "Raffaella Bernardi",
                "Roberto Zamparelli."
            ],
            "title": "A SICK cure for the evaluation of compositional distributional semantic models",
            "venue": "LREC, pages 216\u2013223. European Language Resources As-",
            "year": 2014
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher D. Manning."
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "EMNLP, pages 1532\u20131543. ACL.",
            "year": 2014
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "EMNLP-IJCNLP, pages 3980\u20133990. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Karen Sparck Jones."
            ],
            "title": "A statistical interpretation of term specificity and its application in retrieval",
            "venue": "Journal of documentation, 28(1):11\u201321.",
            "year": 1972
        },
        {
            "authors": [
                "Jianlin Su",
                "Jiarun Cao",
                "Weijie Liu",
                "Yangyiwen Ou."
            ],
            "title": "Whitening sentence representations for better semantics and faster retrieval",
            "venue": "CoRR, abs/2103.15316.",
            "year": 2021
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R. Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "NAACL-HLT, pages 1112\u20131122. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Zhiyong Wu",
                "Yun Chen",
                "Ben Kao",
                "Qun Liu."
            ],
            "title": "Perturbed masking: Parameter-free probing for analyzing and interpreting BERT",
            "venue": "ACL, pages 4166\u2013 4176. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Zeman",
                "Martin Popel",
                "Milan Straka",
                "Jan Hajic",
                "Joakim Nivre",
                "Filip Ginter",
                "Juhani Luotolahti"
            ],
            "title": "Conll 2017 shared task: Multilingual parsing from raw text to universal dependencies",
            "venue": "In CoNLL,",
            "year": 2017
        },
        {
            "authors": [
                "Zexuan Zhong",
                "Dan Friedman",
                "Danqi Chen."
            ],
            "title": "Factual probing is [MASK]: learning vs",
            "venue": "learning to recall. In NAACL-HLT, pages 5017\u20135033. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Reimers",
                "Gurevych"
            ],
            "title": "2019) and report the average Spearman\u2019s correlation on the test sets of all 7 STS tasks (that is, the \u201call",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Pre-trained language models (PLM) such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and ELECTRA (Clark et al., 2020) have achieved great success in a wide variety of natural language processing tasks. However, Reimers and Gurevych (2019) finds that sentence embeddings from the original BERT underperform traditional methods such as GloVe (Pennington et al., 2014). Typically, an input sentence is first embedded by the BERT embedding layer, which consists of token embeddings, segment embeddings, and position embeddings. The output is then encoded by the Transformer encoder and the hidden states at the last layer are averaged to obtain the sentence embeddings. Prior studies identify the anisotropy problem as a critical factor that harms\n1The source code can be found at https://github.com/ alibaba-damo-academy/SpokenNLP/tree/main/ditto.\nBERT-based sentence embeddings, as sentence embeddings from the original BERT yield a high similarity between any sentence pair due to the narrow cone of learned embeddings (Li et al., 2020).\nPrior approaches for improving sentence embeddings from PLMs fall into three categories. The first category of approaches does not require any learning (that is, learning-free). Jiang et al. (2022) argues that the anisotropy problem may be mainly due to the static token embedding bias, such as token frequency and case sensitivity. To address these biases, they propose the static remove biases avg. method which removes top-frequency tokens, subword tokens, uppercase tokens, and punctuations, and uses the average of the remaining token embeddings as sentence representation. However, this approach does not use the contextualized representations of BERT and may not be effective for short sentences as it may exclude informative words. The prompt-based method (last manual prompt) (Jiang et al., 2022) uses a template to generate sentence embeddings. An example template is This sentence: \u201c[X]\u201d means [MASK] ., where [X] denotes the original sentence and the last hidden states in the [MASK] position are taken as sentence embeddings. However, this method has several drawbacks. (1) It increases the input lengths, which raises the computation cost. (2) It relies on using the [MASK] token to obtain the sentence representation, hence unsuitable for PLMs not using [MASK] tokens (e.g., ELECTRA). (3) The performance heavily depends on the quality of manual prompts which relies on human expertise (alternatively, OptiPrompt (Zhong et al., 2021) requires additional unsupervised contrastive learning).\nThe second category of approaches fixes the parameters of the PLM and improves sentence embeddings through post-processing methods that require extra learning. BERT-flow (Li et al., 2020) addresses the anisotropy problem by introducing a flow-based generative model that transforms\nthe BERT sentence embedding distribution into a smooth and isotropic Gaussian distribution. BERTwhitening (Su et al., 2021) uses a whitening operation to enhance the isotropy of sentence representations. Both BERT-flow and BERT-whitening require Natural Language Inference (NLI)/Semantic Textual Similarity (STS) datasets to train the flow network or estimate the mean values and covariance matrices as the whitening parameters.\nThe third category updates parameters of the PLM by fine-tuning or continually pre-training the PLM using supervised or unsupervised learning, which is computationally intensive. For example, Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) fine-tunes BERT using a siamese/triplet network on NLI and STS datasets. SimCSE (Gao et al., 2021) explores contrastive learning. Unsupervised SimCSE uses the same sentences with different dropouts as positives and other sentences as negatives, and supervised SimCSE explores NLI datasets and treats entailment pairs as positives and contradiction pairs as hard negatives.\nIn this work, we first analyze BERT sentence embeddings. (1) We use a parameter-free probing method to analyze BERT and Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) and find that the compositionality of informative words is crucial for generating high-quality sentence embeddings as from SBERT. (2) Visualization of BERT attention weights reveals that certain selfattention heads in BERT are related to informative words, specifically self-attention from a word to itself (that is, the diagonal values of the attention matrix). Based on these findings, we propose a simple and efficient approach, Diagonal Attention Pooling (Ditto), to improve sentence embeddings from PLM without requiring any learning (that is, Ditto is a learning-free method). We find that Ditto improves various PLMs and strong sentence embedding methods on STS benchmarks."
        },
        {
            "heading": "2 Analyze BERT Sentence Embeddings",
            "text": "Observation 1: The compositionality of informative words is crucial for high-quality sentence embeddings. Perturbed masking (Wu et al., 2020) is a parameter-free probing technique for analyzing PLMs (e.g., BERT). Given a sentence x = [x1, x2, . . . , xN ], perturbed masking applies a two-stage perturbation process on each pair of tokens (xi, xj) to measure the impact that a token xj has on predicting the other token xi. Details of perturbed masking can be found in Ap-\npendix A.1. Prior works use perturbed masking to recover syntactic trees from BERT (Wu et al., 2020). Different from prior works, we use perturbed masking to analyze the original BERT and a strong sentence embedding model, supervised Sentence-BERT (SBERT) (Reimers and Gurevych, 2019). Figure 1 shows the heatmap representing the impact matrix F for an example sentence in the English PUD treebank (Zeman et al., 2017). The y-axis represents xi and the x-axis represents xj . A higher value in Fij indicates that a word xj has a greater impact on predicting another word xi. Comparing the impact matrices of BERT and SBERT, we observe that the impact matrix of SBERT exhibits prominent vertical lines on informative tokens such as \u201csocial media\u201d, \u201cCapitol Hill\u201d, and \u201cdifferent\u201d, which implies that informative tokens have a high impact on predicting other tokens, hence masking informative tokens could severely affect predictions of other tokens in the sentence. In contrast, BERT does not show this pattern. This observation implies that the compositionality of informative tokens could be a strong indicator of high-quality sentence embeddings of SBERT. Furthermore, we compute the correlations between the impact matrix and TF-IDF (Sparck Jones, 1972) which measures the importance of a word, and report results in Table 3. We find that the impact matrix of SBERT has a much higher correlation with TF-IDF than the impact matrix of BERT, which is consistent with the observation above. Notably, ELECTRA performs poorly on STS tasks and shows a weak correlation with TF-IDF. Consequently, we hypothesize that sentence embeddings of the original BERT and ELECTRA may be bi-\nased towards uninformative words, hence limiting their performance on STS tasks.\nObservation 2: Certain self-attention heads of BERT correspond to word importance. Although SBERT has a higher correlation with TFIDF than BERT as verified in Observation 1, BERT still shows a moderate correlation. Thus, we hypothesize that the semantic information of informative words is already encoded in BERT but has yet to be fully exploited. Prior research (Clark et al., 2019) analyzes the attention mechanisms of BERT by treating each attention head as a simple, no-training-required classifier that, given a word as input, outputs the other word that it most attends to. Certain attention heads are found to correspond well to linguistic notions of syntax and coreference. For instance, heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions are found to have remarkably high accuracy. We believe that the attention information in BERT needs to be further exploited. We denote a particular attention head by <layer>-<head number> (l-h), where for a BERT-base-sized model, layer ranges from 1 to 12 and head number ranges from 1 to 12. We visualize the attention weights of each head in each layer of BERT and focus on informative words. We then discover that self-attention from a word to itself (that is, the diagonal value of the attention matrix, named diagonal attention) of certain heads may be related to the importance of the word. As shown in Figure 2, the informative words \u201csocial media transitions\u201d, \u201chill\u201d, and \u201clittle\u201d have high diagonal values of the attention matrix of head 1-10. Section 4 will demonstrate that diagonal attentions indeed have a strong correlation with TF-IDF weights."
        },
        {
            "heading": "3 Diagonal Attention Pooling",
            "text": "Inspired by the two observations in Section 2, we propose a novel learning-free method called Diagonal Attention Pooling (Ditto) to improve sentence embeddings for PLMs, illustrated in Figure 3. Taking BERT as an example, the input to the Transformer encoder is denoted as h0 = [h01, . . . , h 0 N ], and the hidden states at each Transformer encoder layer are denoted as hl = [hl1, . . . , h l N ], l \u2208 {1, . . . , L}. Typically, the hidden states of the last layer of the PLM are averaged to obtain the fixedsize sentence embeddings, as 1N \u2211N i=1 h L i (denoted as last avg.). Alternatively, we can also average the static word embeddings 1N \u2211N i=1 h 0 i (denoted as static avg.), or average the hidden states from the first and last layers 12N \u2211N i=1 (h 0 i + h L i ) (denoted as first-last) to obtain the sentence embeddings. Ditto weights the hidden states with diagonal attention of a certain head. For example, to obtain the sentence embeddings from the first-last hidden states of BERT using Ditto, we first obtain the diagonal values [A11, . . . ,ANN ] of the attention matrix A for head l-h of BERT, where l and h are treated as hyperparameters and are optimized on a development set based on the STS performance. Then, we compute 12 \u2211N i=1Aii(h0i + hLi ) as the sentence embeddings2. Note that the impact matrix (Section 2) correlates well with TF-IDF (as shown in Table 3) and hence may also improve sentence embeddings. However, the learning-free Ditto is much more efficient than computing the impact matrix, which is computationally expensive."
        },
        {
            "heading": "4 Experiments and Analysis",
            "text": "Following prior works (Jiang et al., 2022; Li et al., 2020; Su et al., 2021; Gao et al., 2021), we exper-\n2We did not normalize with N since Aii < 1 and normalization by N may result in very small values."
        },
        {
            "heading": "Learning-free methods",
            "text": "Methods that fix BERT parameters but require extra learning\nBERT-flow 66.55 BERT-whitening 66.28 BERT last manual and continuous prompt 73.59 BERT first-last TF-IDF (Ours) 65.45\nMethods that update BERT parameters\nUnsup. BERT SimCSE 76.25 Sup. BERT SimCSE 81.57 Sup. SBERT first-last avg. 84.94 Sup. SBERT first-last Ditto (Ours) 85.11"
        },
        {
            "heading": "Method BERT RoBERTa ELECTRA",
            "text": "iment on the 7 common STS datasets, the widely used benchmark for evaluating sentence embeddings. Appendix A.2 presents dataset and implementation details.\nMain Results The first group of Table 1 presents the results of the three common learning-free methods (Jiang et al., 2022) described in Section 3, including static avg., last avg., and first-last avg., and two learning-free baselines described in Section 1, static remove biases avg. and last manual prompt (Jiang et al., 2022). Although last manual prompt achieves 67.85, different templates can have a significant impact on its performance, ranging from 39.34 to 73.44 on STS-B dev set (Jiang et al., 2022). Applying Ditto to static avg., last avg., and first-last avg. achieves absolute gains of +5.75, +6.50, and +8.07 on the Avg. score, respectively.3\nThe second group of Table 1 presents results from methods that fix BERT parameters but require\n3Since static remove biases avg. may remove important tokens and last manual prompt uses the last hidden states of [MASK] as sentence embeddings instead of average pooling, they are not suitable for applying Ditto.\nextra learning. Note that our learning-free BERT first-last Ditto achieves comparable performance to BERT-flow and BERT-whitening in this group. For further analyzing Ditto, we compute TF-IDF weights on 106 sentences randomly sampled from English Wikipedia as token importance weights and use the weighted average of the first-last hidden states as sentence embeddings, denoted as first-last TF-IDF (the 4th row in this group). First-last TFIDF yields +8.75 absolute gain over the first-last avg. baseline, only slightly better than the +8.07 absolute gain from our learning-free Ditto (with l and h searched on only 1500 samples).\nThe third group of Table 1 presents results of strong baselines that update BERT parameters through unsupervised or supervised learning, including unsupervised SimCSE (Unsup. BERT SimCSE), supervised SimCSE (Sup. BERT SimCSE), and supervised SBERT. We find that applying Ditto on the highly competitive supervised learning method Sup. SBERT first-last avg. still achieves an absolute gain of 0.17 (84.94\u219285.11), demonstrating that Ditto could also improve strong supervised sentence embedding methods. Note that since SimCSE uses the [CLS] representation as sentence embeddings instead of average pooling, SimCSE is not suitable for applying Ditto.\nEffectiveness of Ditto on Different PLMs Table 2 compares the baselines first-last avg. and last manual prompt and our first-last Ditto method on different PLMs. Note that last manual prompt does not work for ELECTRA because this method relies on using the [MASK] token as the sentence embeddings while the ELECTRA discriminator is trained without [MASK] tokens. Ditto consistently works well on ELECTRA and greatly outperforms the two baselines on RoBERTa and ELECTRA, while underperforming last manual prompt on BERT."
        },
        {
            "heading": "Method STS avg. Pear. Spear.",
            "text": "Correlation with TF-IDF To further analyze correlations between diagonal attentions and word importance, we select the 4 heads corresponding to the Top-4 Ditto performance based on Spearman\u2019s correlation on the STS-B development set, and compute correlations between diagonal values of the self-attention matrix of these heads and TF-IDF weights. Table 4 shows all Top-4 heads exhibit moderate or strong correlations with TFIDF weights. We find high-performing heads are usually in the bottom layers (Section A.2), which is consistent with the findings in Clark et al. (2019) that the bottom layer heads broadly attend to the entire sentence."
        },
        {
            "heading": "Method Dev Test Pear. Spear.",
            "text": "Uniformity and Alignment We use the analysis tool from prior works (Gao et al., 2021) to evaluate the quality of sentence embeddings by measuring the alignment between semantically related positive pairs and uniformity of the whole representation space. Gao et al. (2021) finds that sentence embedding models with better alignment and uniformity generally achieve better performance. Figure 4 shows the uniformity and alignment of different sentence embedding models along with their averaged STS results. Lower values indicate better alignment and uniformity. We find that Ditto improves uniformity at the cost of alignment degradation for all PLMs, similar to the flow and whitening methods as reported in Gao et al. (2021). Compared to Ditto, flow and whitening methods achieve larger improvements in uniformity but also cause larger degradations in alignment. Cosine Similarity We use the cosine similarity metric from Ethayarajh (2019) to measure the isotropy of sentence representations. Isotropy means that the word or sentence representations\nare directionally uniform, and the average cosine similarity between random samples should be close to zero. Ethayarajh (2019) originally applied this metric to word representations, and we adapt it to sentence representations in our study. We sample 1000 sentences from the English Wikipedia dataset and compute the average cosine similarity of their representations. Table 5 shows the results. Lower values indicate better isotropy. Our proposed Ditto method improves the isotropy of all three learningfree baselines: static avg., last avg., and first-last avg. This result is consistent with the uniformity analysis in Figure 4, where Ditto also enhances the uniformity of different sentence embedding models."
        },
        {
            "heading": "Method avg. Cosine Similarity",
            "text": ""
        },
        {
            "heading": "5 Conclusions",
            "text": "We propose a simple and learning-free Diagonal Attention Pooling (Ditto) approach to address the bias towards uninformative words in BERT sentence embeddings. Ditto weights words with modelbased importance estimations and can be easily applied to various PLMs. Experiments show that Ditto alleviates the anisotropy problem and improves strong sentence embedding baselines."
        },
        {
            "heading": "Limitations",
            "text": "Although our proposed simple and learning-free Ditto approach demonstrates effectiveness in allevi-\nating the anisotropy problem and improving strong sentence embedding baselines, there are several limitations. Firstly, we conduct our experiments solely on the English sentence embedding models and the English Semantic Textual Similarity (STS) datasets. We hypothesize that the two observations in Section 2 will hold true on pre-trained models for other languages, hence we predict that Ditto, which is based on the two observations, will be effective in improving sentence embeddings for languages other than English. We plan to investigate the efficacy of Ditto on improving sentence embeddings for other languages in future work. Secondly, while we select the attention head (that is, determining l and h) by conducting a grid search of all attention heads based on the performance of the STS development set, we will explore other approaches for selecting attention heads for Ditto in future studies. Lastly, we focus on using Semantic Textual Similarity tasks for evaluating sentence embeddings in this work. We plan to investigate the quality of sentence embeddings in more tasks, such as information retrieval."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Details of Perturbed Masking",
            "text": "In the first stage, we replace xi with the [MASK] token, resulting in a new sequence x\\{xi}. The representation of this sequence is denoted as H(x\\{xi})i. In the second stage, we mask out xj in addition to xi to obtain the second corrupted sequence x\\{xi, xj}. The representation of this sequence is denoted as H(x\\{xi, xj})i. Thus we obtain an impact matrix F \u2208 RN\u00d7N by computing the Euclidean distance between the two representations Fij = d(H(x\\{xi})i, H(x\\{xi, xj})i)."
        },
        {
            "heading": "A.2 Dataset and Implementation Details",
            "text": "We conduct experiments on 7 common STS datasets, namely, STS tasks 2012-2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016), STS-B (Cer et al., 2017), and SICK-R (Marelli et al., 2014), following prior works. These 7 STS datasets are widely used benchmarks for evaluating sentence embeddings. Each dataset consists of sentence pairs scored from 0 to 5 to indicate the semantic similarity. For evaluation, we follow the setting of Reimers and Gurevych (2019) and report the average Spearman\u2019s correlation on the test sets of all 7 STS tasks (that is, the \u201call\u201d setting), without using an additional regressor. Our implementation is based on the SimCSE GitHub repository4 and we modify it to fit our purposes. We conduct a grid search of the attention head l-h for Ditto based on Spearman\u2019s correlation on the STS-B development set (1500 samples). In this way, we select head 1-10 for BERT5, head 1-5 for RoBERTa6, head 1-11 for ELECTRA7, and head 3-7 for SBERT8. The TF-IDF weights are learned on 106 sentences randomly sampled from English Wikipedia9 using the gensim tool10. We also utilize the English Wikipedia dataset and randomly sampled 1000 sentences to calculate the average cosine similarity of sentence representations. We conduct experiments using a single Tesla V100 GPU. Note that BERTflow and BERT-whitening papers use the full target dataset (including all sentences in the train, development, and test sets, and excluding all labels) and optionally the NLI corpus (SNLI (Bowman et al., 2015) and MNLI corpus (Williams et al., 2018)) for training.\n4https://github.com/princeton-nlp/SimCSE 5https://huggingface.co/bert-base-uncased 6https://huggingface.co/roberta-base 7https://huggingface.co/google/\nelectra-base-discriminator 8https://huggingface.co/sentence-transformers/ bert-base-nli-stsb-mean-tokens 9https://huggingface.co/datasets/ princeton-nlp/datasets-for-simcse/resolve/main/ wiki1m_for_simcse.txt\n10https://radimrehurek.com/gensim/models/ tfidfmodel.html"
        }
    ],
    "title": "Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings",
    "year": 2023
}