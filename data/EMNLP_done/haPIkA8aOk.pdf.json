{
    "abstractText": "Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. Selective prediction is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation with self-evaluation to improve the selective prediction performance of LLMs. Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation. We evaluate our method on a variety of question-answering (QA) datasets and show that it outperforms state-of-the-art selective prediction methods. For example, on the CoQA benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the AUROC from 74.61% to 80.25%.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiefeng Chen"
        },
        {
            "affiliations": [],
            "name": "Jinsung Yoon"
        },
        {
            "affiliations": [],
            "name": "Sayna Ebrahimi"
        },
        {
            "affiliations": [],
            "name": "Sercan \u00d6. Ar\u0131k"
        },
        {
            "affiliations": [],
            "name": "Tomas Pfister"
        },
        {
            "affiliations": [],
            "name": "Somesh Jha"
        }
    ],
    "id": "SP:087a3e2982299768df6910dd2d78f1bd1e4cdc4a",
    "references": [
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Jeremy R Cole",
                "Michael JQ Zhang",
                "Daniel Gillick",
                "Julian Martin Eisenschlos",
                "Bhuwan Dhingra",
                "Jacob Eisenstein."
            ],
            "title": "Selectively answering ambiguous questions",
            "venue": "arXiv preprint arXiv:2305.14613.",
            "year": 2023
        },
        {
            "authors": [
                "Yonatan Geifman",
                "Ran El-Yaniv."
            ],
            "title": "Selective classification for deep neural networks",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Deberta: Decoding-enhanced bert with disentangled attention",
            "venue": "arXiv preprint arXiv:2006.03654.",
            "year": 2020
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen."
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685.",
            "year": 2021
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Jun Araki",
                "Haibo Ding",
                "Graham Neubig."
            ],
            "title": "How can we know when language models know? on the calibration of language models for question answering",
            "venue": "Transactions of the Association for Computational Linguistics, 9:962\u2013977.",
            "year": 2021
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel S Weld",
                "Luke Zettlemoyer."
            ],
            "title": "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension",
            "venue": "arXiv preprint arXiv:1705.03551.",
            "year": 2017
        },
        {
            "authors": [
                "Saurav Kadavath",
                "Tom Conerly",
                "Amanda Askell",
                "Tom Henighan",
                "Dawn Drain",
                "Ethan Perez",
                "Nicholas Schiefer",
                "Zac Hatfield Dodds",
                "Nova DasSarma",
                "Eli Tran-Johnson"
            ],
            "title": "Language models (mostly) know what they know",
            "year": 2022
        },
        {
            "authors": [
                "Lorenz Kuhn",
                "Yarin Gal",
                "Sebastian Farquhar."
            ],
            "title": "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059.",
            "year": 2021
        },
        {
            "authors": [
                "Chin-Yew Lin",
                "Franz Josef Och."
            ],
            "title": "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics",
            "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics",
            "year": 2004
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Weng Lam Tam",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "Ptuning v2: Prompt tuning can be comparable to finetuning universally across scales and tasks",
            "venue": "arXiv preprint arXiv:2110.07602.",
            "year": 2021
        },
        {
            "authors": [
                "Xiao Liu",
                "Yanan Zheng",
                "Zhengxiao Du",
                "Ming Ding",
                "Yujie Qian",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "Gpt understands, too",
            "venue": "arXiv preprint arXiv:2103.10385.",
            "year": 2021
        },
        {
            "authors": [
                "R OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "arXiv.",
            "year": 2023
        },
        {
            "authors": [
                "Gabriel Poesia",
                "Oleksandr Polozov",
                "Vu Le",
                "Ashish Tiwari",
                "Gustavo Soares",
                "Christopher Meek",
                "Sumit Gulwani."
            ],
            "title": "Synchromesh: Reliable code generation from pre-trained language models",
            "venue": "arXiv preprint arXiv:2201.11227.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "venue": "arXiv preprint arXiv:1606.05250.",
            "year": 2016
        },
        {
            "authors": [
                "Siva Reddy",
                "Danqi Chen",
                "Christopher D Manning."
            ],
            "title": "Coqa: A conversational question answering challenge",
            "venue": "Transactions of the Association for Computational Linguistics, 7:249\u2013266.",
            "year": 2019
        },
        {
            "authors": [
                "Jie Ren",
                "Jiaming Luo",
                "Yao Zhao",
                "Kundan Krishna",
                "Mohammad Saleh",
                "Balaji Lakshminarayanan",
                "Peter J Liu."
            ],
            "title": "Out-of-distribution detection and selective generation for conditional language models",
            "venue": "arXiv preprint arXiv:2209.15558.",
            "year": 2022
        },
        {
            "authors": [
                "Chenglei Si",
                "Zhe Gan",
                "Zhengyuan Yang",
                "Shuohang Wang",
                "Jianfeng Wang",
                "Jordan Boyd-Graber",
                "Lijuan Wang."
            ],
            "title": "Prompting gpt-3 to be reliable",
            "venue": "arXiv preprint arXiv:2210.09150.",
            "year": 2022
        },
        {
            "authors": [
                "Karan Singhal",
                "Tao Tu",
                "Juraj Gottweis",
                "Rory Sayres",
                "Ellery Wulczyn",
                "Le Hou",
                "Kevin Clark",
                "Stephen Pfohl",
                "Heather Cole-Lewis",
                "Darlene Neal"
            ],
            "title": "Towards expert-level medical question answering with large language models",
            "year": 2023
        },
        {
            "authors": [
                "Liyan Tang",
                "Zhaoyi Sun",
                "Betina Idnay",
                "Jordan G Nestor",
                "Ali Soroush",
                "Pierre A Elias",
                "Ziyang Xu",
                "Ying Ding",
                "Greg Durrett",
                "Justin Rousseau"
            ],
            "title": "Evaluating large language models on medical evidence summarization. medRxiv, pages 2023\u201304",
            "year": 2023
        },
        {
            "authors": [
                "Neeraj Varshney",
                "Chitta Baral."
            ],
            "title": "Postabstention: Towards reliably re-attempting the abstained instances in QA",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2023
        },
        {
            "authors": [
                "Neeraj Varshney",
                "Swaroop Mishra",
                "Chitta Baral."
            ],
            "title": "Investigating selective prediction approaches across several tasks in iid, ood, and adversarial settings",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1995\u20132002.",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2203.11171.",
            "year": 2022
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "arXiv preprint arXiv:1704.05426.",
            "year": 2017
        },
        {
            "authors": [
                "Ji Xin",
                "Raphael Tang",
                "Yaoliang Yu",
                "Jimmy Lin."
            ],
            "title": "The art of abstention: Selective prediction and error regularization for natural language processing",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
            "year": 2021
        },
        {
            "authors": [
                "Zhangyue Yin",
                "Qiushi Sun",
                "Qipeng Guo",
                "Jiawen Wu",
                "Xipeng Qiu",
                "Xuanjing Huang"
            ],
            "title": "Do large language models know what they don\u2019t know? arXiv preprint arXiv:2305.18153",
            "year": 2023
        },
        {
            "authors": [
                "Hiyori Yoshikawa",
                "Naoaki Okazaki."
            ],
            "title": "Selective-lama: Selective prediction for confidenceaware evaluation of language models",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 1972\u20131983.",
            "year": 2023
        },
        {
            "authors": [
                "Shun Zhang",
                "Zhenfang Chen",
                "Yikang Shen",
                "Mingyu Ding",
                "Joshua B Tenenbaum",
                "Chuang Gan."
            ],
            "title": "Planning with large language models for code generation",
            "venue": "arXiv preprint arXiv:2303.05510.",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Faisal Ladhak",
                "Esin Durmus",
                "Percy Liang",
                "Kathleen McKeown",
                "Tatsunori B Hashimoto."
            ],
            "title": "Benchmarking large language models for news summarization",
            "venue": "arXiv preprint arXiv:2301.13848.",
            "year": 2023
        },
        {
            "authors": [
                "Kadavath"
            ],
            "title": "2022) is a way to estimate the probability that a model\u2019s generation is correct by \u201casking",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large Language Models (LLMs) have recently demonstrated impressive capabilities in many natural language understanding, reasoning and generation tasks, such as question answering (Jiang et al., 2021; Singhal et al., 2023), summarization (Tang et al., 2023; Zhang et al., 2023b), semantic classification, and code generation (Poesia et al., 2022; Zhang et al., 2023a). As LLMs improve their remarkable performance, they are being increasingly considered to replace humans to perform high-stakes tasks. For example, LLMs can be used for medical QA to assist patients (Singhal et al., 2022). However, LLMs are not guaranteed to be accurate for all queries, so it is important to understand which queries they are reliable for. This\n\u2217 Work done during internship at Google and before joining Amazon.\ninformation can be used to direct human oversight to the queries with the lowest selection score. Selective prediction (Geifman and El-Yaniv, 2017), broadly refers to the deployment scenario for AI models where humans are involved to maintain overall accuracy by reviewing AI-generated, lowconfidence outputs. In this scenario, both human and AI performance are considered together to minimize human involvement cost. LLMs should be used in the real-world with enhanced selective prediction performance. They should be able to assess the accuracy of their predictions and refrain from making wrong predictions. If an LLM detects that an answer might be wrong for a question, it should be able to generate an answer with the sentiment of \"I don\u2019t know!\" (as shown in Fig. 1) or defer the answer to a human for manual inspection. This will help to ensure that LLMs are used in a reliably, especially for high-stakes applications.\nSelective prediction for LLMs is challenging because LLMs are just trained to predict the next to-\nken given a context but are not guaranteed to always predict the correct next token. Also, since LLMs generate an output sequence in an auto-regressive way, they don\u2019t directly produce a confidence score for the output sequence. Thus, obtaining selection scores from LLMs for their output sequences is not straightforward. Although there is some research on selective prediction for LLMs, these studies have their own shortcomings. Kadavath et al. propose to use heuristic prompts (e.g., adding prompts like \u201cIs the proposed answer True or False?\u201d) to trigger self-evaluation of LLMs. However, those prompts may only work for the LLM used in Kadavath et al. (2022) and may not generalize to other types of LLMs (e.g., OPT and GPT2 models evaluated in our work). Some approaches proposed using semantic entropy (Kuhn et al., 2023) or selfconsistency (Wang et al., 2022) as a measure of uncertainty for selection score. However, they usually require generating multiple output sequences to obtain the uncertainty measure for an input sequence, which introduces high computational cost and latency at test time. Fine-tuning LLMs on training data from the target question answering task using the standard LLM training loss can improve selective prediction performance. This is because fine-tuning can improve the accuracy of the predictions and maximize the likelihood of the ground-truth answer for a given question. However, maximizing the likelihood of the ground-truth answer is not the same as minimizing the likelihood of the wrong answers, since LLMs generate output sequences in an auto-regressive way. Even after fine-tuning, some wrong answers may still have high likelihood and be generated by the LLM at test time. Therefore, distinguishing correct and incorrect answers based on likelihood scores alone is a challenging task.\nTo address these challenges of self-evaluation and uncertainty estimation, we propose a novel framework \u2013 Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs (ASPIRE). Unlike previous methods that rely on hand-crafted heuristics or multiple output sequences, our framework learns to self-evaluate from target-task data. We do this by training LLMs on a subset of the training data from the question-answering tasks. This allows the LLMs to learn to distinguish between correct and incorrect answers on their own. We then define a selection score that combines the likelihood of the generated answer with the learned\nself-eval score (see Eq. (11)) to make selective predictions. This makes our method much less computationally expensive than solutions that require generating multiple output sequences to obtain the uncertainty measure. Thus, the proposed method is useful for practical applications where high selective prediction performance and low inference costs are desired, after deploying the LLM. In such applications, practitioners prefer collecting some training data to fine-tune smaller LLMs to achieve high selective prediction performance rather than directly deploying very large pre-trained LLMs with limited selective prediction performance for specific tasks.\nWe conduct extensive experiments to evaluate our proposed framework, ASPIRE. We show that ASPIRE achieves the state-of-the-art selective prediction performance on three question answering datasets: CoQA, TriviaQA and SQuAD, using OPT and GPT-2 models. We also provide empirical analysis to delve deeper into our proposed technique."
        },
        {
            "heading": "2 Related Work",
            "text": "Selective Prediction for LLMs. Recently, LLMs (e.g., GPT-4 (OpenAI, 2023) and PaLM (Chowdhery et al., 2022)) have achieved great success in solving various kinds of Natural Language Generation (NLG) tasks. However, LLMs are still not very reliable and may generate wrong outputs when solving NLG tasks. Due to this, selective prediction (or sometimes called selective generation (Ren et al., 2022)) is critical for safely deploying LLMs in the real-world. Different from selective prediction for classification tasks (e.g., Natural Language Inference (NLI) tasks) (Xin et al., 2021), selective prediction for LLMs in solving NLG tasks is fundamentally different since the prediction is done auto-regressively over many steps and the possible answer set has an infinite size. Recently, several work propose some uncertainty measures for LLMs, which can be used for selective prediction (Si et al., 2022; Kadavath et al., 2022; Varshney et al., 2022; Ren et al., 2022; Kuhn et al., 2023). Some recent work studies selective prediction for solving question answering tasks where questions are ambiguous (Cole et al., 2023; Yin et al., 2023). Varshney and Baral (2023) propose a selective prediction method that at inference time leverages an auxiliary model which is trained to distinguish the correct predictions of the QA model from the incorrect ones. Different from previous work, our\nwork proposes to improve selective prediction performance of LLMs in solving question answering tasks by learning self-evaluation during fine-tuning. Parameter Efficient Fine-tuning. Fine-tuning pretrained LLMs on downstream datasets can bring huge performance gains when compared to using the pretrained LLMs out-of-the-box (e.g., kshot inference). However, as LLMs get larger and larger, full fine-tuning becomes very expensive in terms of computational cost and memory requirements. In addition, massive models might not be data efficient and overfitting issues might be observed, yielding suboptimal generalization. To address these issues, Parameter-Efficient Fine-tuning (PEFT) approaches have been proposed. PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parameters of the pretrained LLMs, thereby greatly decreasing the computational and storage costs. It has also been shown that PEFT approaches are better than fine-tuning in the low-data regimes and generalize better to out-of-domain scenarios. Existing PEFT approaches include LoRA (Hu et al., 2021), Prefix Tuning (Liu et al., 2021a), Soft Prompt Tuning (Lester et al., 2021) and P-Tuning (Liu et al., 2021b). In this work, we use Soft Prompt Tuning to learn self-evaluation to improve selective prediction performance of LLMs."
        },
        {
            "heading": "3 Problem Setup",
            "text": "Suppose we have a pre-trained LLM f for an arbitrary generative modeling task such as question answering. The output can be represented as a sequence of tokens from the vocabulary V . Let V\u2217 be the space of sequences of tokens. Suppose the logits of f on v \u2208 V given x \u2208 V\u2217 is f\u0304(v | x). The likelihood of the next token following x being v is defined as:\nf(v | x) := exp (f\u0304(v | x))\u2211 v\u2032\u2208V exp (f\u0304(v \u2032 | x)) , (1)\nwhereas the likelihood of generating y\u0302 \u2208 V\u2217 given x is defined as:\nf(y\u0302 | x) := \u03a0|y\u0302|i=1f(y\u0302i | x, y\u0302[i\u22121]), (2)\nwhere y\u0302 = (y\u03021, . . . , y\u0302|y\u0302|), |y\u0302| is the length of y\u0302, y\u0302[i\u22121] = (y\u03021, . . . , y\u0302i\u22121) for i > 0 and y\u0302[0] = \u2205. This likelihood can be very small when |y\u0302| is very large. To address this issue, we define the normalized likelihood as:\nfnorm(y\u0302 | x) := f(y\u0302 | x) 1 |y\u0302| (3)\nWe use f to generate the output sequence for the given input x by solving the following objective:\ny\u0302\u2217 = argmax y\u0302 log f(y\u0302 | x) (4)\nIt is impossible to solve this objective exactly since the output sequences can be arbitrarily long. However, we can employ some decoding strategy like greedy decoding or beam search to solve it.\nTo evaluate if the generated output y\u0302 is correct or not, we need a set of reference outputs S and an evaluation metric M : V\u2217 \u00d7 V\u2217 \u2192 [0, 1] that can evaluate the similarity of the generated output y\u0302 compared to the reference output yr \u2208 S. With a threshold \u03b3, we can determine the correctness of the generated output \u2013 if maxyr\u2208SM(y\u0302,yr) > \u03b3, then the generated output is correct; otherwise, the generated output is wrong. We discuss the specific choices of M and \u03b3 in Section 6.\nIn selective prediction, we need a rejection option, which is denoted by \u22a5. Given a training dataset Dtr = {(xi,yi)}ntri=1 randomly sampled from a target task distribution, we aim to build a selective predictor fs : V\u2217 \u2192 V\u2217 \u222a {\u22a5} that can achieve strong selective prediction performance on the test dataset Dte = {(xi, Si)}ntei=1, where Si is the set of reference outputs for the input xi. The selective predictor fs is composed of a predictor f\u0302 : V\u2217 \u2192 V\u2217 and a selection scoring function g : V\u2217 \u2192 R. With f\u0302 and g, the selective predictor fs is proposed as:\nfs(x; \u03c4) = { f\u0302(x) if g(x) \u2265 \u03c4, \u22a5 if g(x) < \u03c4 , (5)\nwhere \u03c4 is a threshold. The accuracy of the selective predictor is defined as the fraction of the accepted inputs where the predictions are correct. The coverage of the selective predictor is defined as the fraction of the inputs that are accepted. We can tune the threshold \u03c4 to achieve a certain coverage and there would be an accuracy-coverage trade-off.\nWe use the area under the accuracy-coverage curve (AUACC) metric to measure selective prediction performance and use the area under the receiver operator characteristic curve (AUROC) metric to measure the quality of the selection score estimation. AUACC is the common metric used for evaluating selective prediction performance (Xin et al., 2021; Yoshikawa and Okazaki, 2023). AUROC is equivalent to the probability that a randomly chosen correct output sequence has a higher\nselection score than a randomly chosen incorrect output sequence. AUROC is used in (Kuhn et al., 2023) for evaluating uncertainty estimation methods."
        },
        {
            "heading": "4 ASPIRE Framework",
            "text": "We propose that LLMs should have the selfevaluation ability such that they should be able to distinguish whether their proposed answers for a given question are correct or not. Although some previous work (Kadavath et al., 2022) show that LLMs have good self-evaluation ability with specially designed prompts, those prompts may not transfer to different kinds of LLMs (as shown by our experiments and in Kuhn et al. (2023)) and hand-crafting prompts for different kinds of LLMs can be expensive. A more effective approach is to collect some training data to employ selfevaluation. Towards this end, we propose a novel framework \u2013 Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs (ASPIRE). Fig. 2 illustrates the proposed framework and the details are explained next.\nGiven a training dataset for a generative task, we can fine-tune the pre-trained LLM on the training data to improve its prediction performance. Towards this end, parameter efficient tuning techniques (e.g., soft prompt tuning (Lester et al., 2021) and LoRA (Hu et al., 2021)) might be employed to adapt the pre-trained LLM on the task, given their effectiveness in obtaining strong generalization with small amount of target task data. Specifically, the model parameters \u03b8 of the LLM are frozen and adaptable parameters \u03b8p are added for fine-tuning. Only \u03b8p are updated to solve the following training objective:\nmin \u03b8p\nE(x,y)\u223cDtrL(x,y; \u03b8, \u03b8p), (6)\nwhere L is the LLM training loss (e.g. crossentropy). Such fine-tuning can improve selective prediction performance because it not only improves the prediction accuracy, but also enhances the likelihood of correct output sequences.\nTo further improve selective prediction performance, we propose to fine-tune the LLM to learn self-evaluation. We first use the LLM with the learned \u03b8p to generate different answers for each example (x,y) \u2208 Dtr. Suppose the decoding algorithm used to generate output sequences for each input x is A. A would produce a list of generated\noutput sequences:\nA(f, \u03b8p,x) = [y\u03021, . . . , y\u0302k], (7)\nwhere k is the number of output sequences generated. We aim to generate output sequences that have high likelihood (i.e., f(y\u0302j | x; \u03b8p) is high). We use the metric M defined in Section 3 to determine if the generated output y\u0302j is correct or not. If M(y\u0302j ,y) > \u03b3\u0302, we label y\u0302j as a correct output for x; otherwise, we label y\u0302j as a wrong output for x. Here, the threshold \u03b3\u0302 might be different from the threshold \u03b3 used for evaluation. We choose a sufficiently large value of \u03b3\u0302 (e.g., \u03b3\u0302 = 0.9) so that the generated wrong outputs wouldn\u2019t be labeled as correct outputs. In Appendix H, we provide more details and analyses on selection of \u03b3\u0302.\nAfter sampling high-likelihood outputs for each query, we add adaptable parameters \u03b8s and only tune \u03b8s for learning self-evaluation. Since the output sequence generation only depends on \u03b8 and \u03b8p, freezing \u03b8 and the learned \u03b8p can avoid changing the prediction behaviors of the LLM when learning self-evaluation. Let zc and zw be a pair of tokens that represent the words \u201ccorrect\u201d and \u201cwrong\u201d respectively. We can then optimize \u03b8s using the following training objective:\nmin \u03b8s\nE(x,y)\u223cDtr Lc + Lw\nLc = Ey\u0302\u223cSc(x,y) \u2212 log f(zc|x, y\u0302; \u03b8p, \u03b8s) Lw = Ey\u0302\u223cSw(x,y) \u2212 log f(zw|x, y\u0302; \u03b8p, \u03b8s)\n(8)\nwhere Sc(x,y) is a set of correct outputs containing the reference output y and kc correct outputs with highest likelihood from A(f, \u03b8p,x), and Sw(x,y) is a set of wrong outputs containing kw wrong outputs with highest likelihood from A(f, \u03b8p,x). If A(f, \u03b8p,x) has less than kc correct outputs (or has less than kw wrong outputs), we include all its correct outputs (or all its wrong outputs) in Sc (or Sw). We ensure that Sw contains at least one wrong output. If A(f, \u03b8p,x) doesn\u2019t contain wrong outputs, we add a default wrong output (e.g., the empty string) to Sw.\nAfter training \u03b8p and \u03b8s, we obtain the prediction for the query x via solving the following objective:\ny\u0302\u2217 = argmax y\u0302 log f(y\u0302 | x; \u03b8p). (9)\nWe use the beam search decoding method towards this. We define the likelihood of the output y\u0302\u2217\nbeing correct for the query x as:\nP (zc | x, y\u0302\u2217) = exp (f\u0304(zc | x, y\u0302\u2217; \u03b8p, \u03b8s))\u2211\nz\u2208{zc,zw} exp (f\u0304(z | x, y\u0302\u2217; \u03b8p, \u03b8s)) (10)\nThis score P (zc | x, y\u0302\u2217) is referred as the learned self-eval score. Overall, the selection scoring function is proposed as:\ng(x) = (1\u2212 \u03b1) \u00b7 log fnorm(y\u0302\u2217 | x; \u03b8p) (11) + \u03b1 \u00b7 logP (zc | x, y\u0302\u2217).\nwhere \u03b1 \u2208 [0, 1] is a hyper-parameter."
        },
        {
            "heading": "5 Implementation via Soft Prompt Tuning",
            "text": "In the proposed framework, \u03b8p and \u03b8s can be trained using parameter efficient tuning approaches. In our work, we focus on Soft Prompt Tuning, as illustrated in Fig. 3. The driving force behind this approach lies in the recognition that if we can develop prompts that effectively stimulate self-evaluation, it should be possible to discover these prompts through soft prompt tuning in conjunction with targeted training objectives.\nWe first briefly introduce the soft prompt tuning method proposed by Lester et al. (2021). We consider LLMs based on the Transformer architecture (Vaswani et al., 2017). Given a query x = (x1, . . . , xmq), Transformers first embed the tokens, forming a matrix X \u2208 Rmq\u00d7de , where de is the dimension of the embedding space. The softprompts are represented as parameters \u03b8\u0303 \u2208 Rl\u00d7de ,\nwhere l is the length of the prompt. The prompt is then concatenated to the embedded input forming a single matrix [\u03b8\u0303;X] \u2208 R(mq+l)\u00d7de , which then flows through the transformer as normal.\nIn the proposed framework, we need to train two portions of the prompts \u03b8p \u2208 Rl\u00d7de and \u03b8s \u2208 Rl\u00d7de . Utilizing soft prompt tuning, the training objective (6) is proposed as:\nmin \u03b8p\nE(x,y)\u223cDtr 1\n|y| |y|\u2211 j=1 \u2212 log f(yj |[\u03b8p;X;Y[j\u22121]]),\n(12)\nwhere X is the embedding of x and Y[j\u22121] is the embedding of y[j\u22121]. On the other hand, the training objective (8) is proposed as:\nmin \u03b8s\nE(x,y)\u223cDtr Lc + Lw\nLc = Ey\u0302\u223cSc(x,y) \u2212 log f(zc|[\u03b8p;X; Y\u0302 ; \u03b8s]) Lw = Ey\u0302\u223cSw(x,y) \u2212 log f(zw|[\u03b8p;X; Y\u0302 ; \u03b8s])\n(13)\nwhere Y\u0302 is the embedding of y\u0302. The inference objective (9) in the framework becomes:\ny\u0302\u2217 = argmax y\u0302 log f(y\u0302 | [\u03b8p;X]) (14)\nThe learned self-eval score P (zc | x, y\u0302\u2217) becomes:\nP (zc | x, y\u0302\u2217) = exp (f\u0304(zc | [\u03b8p;X; Y\u0302 \u2217; \u03b8s]))\u2211\nz\u2208{zc,zw} exp (f\u0304(z | [\u03b8p;X; Y\u0302 \u2217; \u03b8s])) (15)\nwhere Y\u0302 \u2217 is the embedding of y\u0302\u2217. To generate the output sequence and obtain the selection score for a given input sequence, we employ two stages: first, we obtain the generated output and the likelihood for the generated output and then, we obtain the learned self-eval score. Since the query of the second stage is constructed by appending some additional tokens to the query of the first stage, the second stage can reuse the states in the first stage instead of recomputing them to save some computational cost (see Fig. 3).\nLastly, we note that the computational complexity of the proposed method at test time is O(lmax) with lmax being the maximum length of the generated output sequence. In Appendix F, we provide a more detailed analysis of the computational complexity of different methods. The predictive entropy and semantic entropy methods have a complexity of O(m \u00b7 lmax) where m is the number of output sequences sampled for uncertainty estimation, which is much larger than that of our method."
        },
        {
            "heading": "6 Experiments",
            "text": "Our experimental evaluation is focused on the following questions: (Q1) Could a learning-based system using selfevaluation improve selective prediction in LLMs compared to other post-hoc selective prediction alternatives? (A1) By learning self-evaluation, we can significantly improve selective prediction performance\nacross different datasets and LLMs (see Table 1). (Q2) What kinds of decoding algorithms could be used as A for the proposed framework ASPIRE? (A2) Using decoding algorithms that can sample different high-likelihood answers as A (e.g., beam search) is important for ASPIRE to achieve good selective prediction performance (see Table 4). (Q3) What is the effect of the number of training samples for the proposed method ASPIRE? (A3) More training samples lead to enhanced performance and with \u223c2k samples, ASPIRE can outperform the baselines without soft prompt tuning significantly on different datasets (see Table 5)."
        },
        {
            "heading": "6.1 Setup",
            "text": "Dataset. We focus on the free-form question answering tasks on the datasets CoQA (Reddy et al., 2019), TriviaQA (Joshi et al., 2017) and SQuAD (Rajpurkar et al., 2016). For CoQA and SQuAD, since each question is asked based on a context paragraph, we evaluate the LLMs in the zero-shot setting. For TriviaQA, since the LLMs have limited accuracy under the zero-shot setting, we evaluate the LLMs in 5-shot setting. For each dataset, we use a subset of the original training set containing 50K examples for adapting LLMs by default. The details of the datasets are given in Appendix B. LLMs. We use OPT (Zhang et al., 2022) and GPT-2 (Radford et al., 2019) models of various sizes. For OPT, we consider OPT-350M, OPT-1.3B,\nOPT-2.7B and OPT-30B. For GPT-2, we consider GPT2-Medium, GPT2-Large and GPT2-XL. The details of these models are given in Appendix C. Baselines. For selective prediction, we need to get a predicted output sequence y\u0302\u2217 and a selection score g(x) for each input sequence x given a model f . The model f can be a pre-trained LLM or an adapted LLM with \u03b8p trained using the training objective (12). We use the beam-search decoding to obtain the predicted output sequence y\u0302\u2217 and consider the following baselines to compute the selection score g(x): (1) Perplexity; (2) Predictive Entropy; (3) Semantic Entropy (Kuhn et al., 2023); (4) Self-eval; (5) P(True) (Kadavath et al., 2022). More details can be found in Appendix D. Evaluation metrics. We use the Rouge-L (Lin and Och, 2004) as the evaluation metricM to evaluate the similarity of the generated answer to the reference answers following Kuhn et al. (2023). For the threshold \u03b3 that is used to determine the correctness of the generated answer, we consider relatively larger values of \u03b3 since we focus on safety-critical applications where accepting a wrong answer is more costly compared to rejecting a correct answer\nthat is different from the reference answers (refer to Appendix G for the justifications of the choices of \u03b3). Unless specified, we use \u03b3 = 0.7 as default.\nTraining hyper-parameters. We have two stages of training: the first stage is to train the soft prompt \u03b8p using the training objective (12) and the second stage is to train the soft prompt \u03b8s using the training objective (13). For both stages, we train the soft prompts for 10 epochs using AdamW optimizer with a batch size of 8, a learning rate of 0.01 and cosine learning rate scheduling. More training details can be found in Appendix E.\nASPIRE setup. We use the beam search as the decoding algorithmA. We set the number of beams equal to k and use the k highest scoring beams as the answer listA(f, \u03b8p,x). We set l = 50, \u03b3\u0302 = 0.9, k = 10, kc = 2, kw = 10 and \u03b1 = 0.25 by default. We choose these hyper-parameters based on the performance on the validation set from TriviaQA using the OPT-2.7B model. We then use the same hyper-parameters across all datasets and models."
        },
        {
            "heading": "6.2 Results",
            "text": "We first evaluate the accuracy of different LLMs. The results in Table 3 show that after training \u03b8p via soft prompt tuning, the accuracy of LLMs is improved significantly. On the CoQA and SQuAD datasets, the adapted OPT-2.7B can even outperform the pre-trained OPT-30B, which demonstrates that it is possible to adapt a smaller LLM to achieve better accuracy than a much larger LLM. We then evaluate different methods to compute the selection score when the model\u2019s predictions are fixed. The results in Table 1 show that the proposed method ASPIRE significantly outperforms the baselines in terms of the AUACC and AUROC metrics across different datasets and LLMs. The results also show that after prompt tuning, the AUACC of different methods is significantly improved as the accuracy gets better and the perplexity becomes more meaningful in separating correct and wrong answers. Additionally, the results show that the proposed ASPIRE with the adapted OPT-2.7B model can significantly outperform the Self-eval and P(True) baselines with the pre-trained OPT-30B model in selective prediction performance. Note that on the TriviaQA dataset, although the pre-trained OPT30B model has better accuracy than the adapted OPT-2.7B model, the Self-eval and P(True) baselines with the pre-trained OPT-30B model have much worse selective prediction performance compared to the proposed ASPIRE with the adapted\nOPT-2.7B model. These demonstrate that the selfevaluation approaches are not effective for high capacity LLMs, and applying the proposed ASPIRE to smaller LLMs can lead to better selective prediction performance compared to those self-evaluation approaches with much larger LLMs. Additional results in Appendix I show that ASPIRE significantly outperforms the baselines across OPT and GPT2 models of different sizes for different values of the Rouge threshold \u03b3."
        },
        {
            "heading": "6.3 Empirical Analyses",
            "text": "The effect of \u03b1. We study the effect of the hyper-parameter \u03b1 in the proposed selection score (Eq. (11)). The results in Table 2 show that setting \u03b1 = 0.25 leads to the best performance since it combines the normalized likelihood and the learned self-eval score in a good way. Only using the normalized likelihood (i.e., \u03b1 = 0) or only using the learned self-eval score (i.e., \u03b1 = 1) leads to much worse performance. In practice, the value of \u03b1 can be chosen based on the performance on the validation data. In Appendix J, we give results for other models and discuss how we choose \u03b1. The choices of A. We compare two decoding algorithms \u2013 beam search and multinomial sampling that can be used as A for answer sampling. For beam search, we use the k highest scoring beams as the answer list. For multinomial sampling, we consider temperature (denoted as T ) in the set {0.1, 1.0, 2.0}. The results in Table 4 show that using multinomial sampling with T = 2.0 or T = 0.1 leads to worse performance compared to other decoding algorithms. If we set a high temperature (T = 2.0) for multinomial sampling, then we sample some random answers that might not have high-likelihood. If we set a low temperature (T = 0.1) for multinomial sampling, then we repeatedly sample the same high-likelihood answers. Thus, the results suggest that sampling different high-likelihood answers is important for our\nmethod to achieve high accuracy and coverage in selective prediction. The results also show that using beam search leads to similar performance as using multinomial sampling with T = 1. So we can use either one in practice. Training sample efficiency. We perform experiments to study the effect of the number of training samples for ASPIRE. We fix the number of training steps to be 50K while varying the size of the training dataset. The results in Table 5 show that more training samples lead to performance improvement and with 2K training samples, ASPIRE can outperform the baselines without soft prompt tuning by a large margin across different datasets. This underlines that our method, ASPIRE, can significantly improve selective prediction performance even with limited number of training samples."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we proposed a novel framework for adaptation with self-evaluation to improve selective prediction in LLMs. We implemented the framework via soft prompt tuning and demonstrated its superior performance over existing methods through extensive experiments. In future work, one could explore implementing our framework via other parameter efficient tuning approaches and\napplying our method to larger LLMs.\nLimitations\nHigher capacity LLMs are known to often yield superior capabilities. Our work does not include fine-tuning experimental results with the largest and the strongest LLMs in the literature (we have fine-tuning results with LLMs up to 2.7B parameters), due to our computational constraints. However, the proposed framework can be applied to LLMs of any size and similar improvements are expected. We leave the adoption of our methods to larger-scale LLMs to future work.\nEthics Statement\nLLMs are widely used in various applications nowadays. However, they can generate wrong or misleading answers to questions, which can cause serious consequences in some safety critical applications. The framework proposed in our work can be used to improve selective prediction performance of LLMs and make their deployments more reliable. However, it is noted that the obtained selective prediction performances are still not perfect."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank all the anonymous reviewers for their careful comments and feedback. The work is partially supported by Air Force Grant FA955018-1-0166, the National Science Foundation (NSF) Grants CCF-FMitF-1836978, IIS-2008559, SaTC-Frontiers1804648, CCF-2046710 and CCF1652140, and ARO grant number W911NF-17-10405. Jiefeng Chen and Somesh Jha are partially supported by the DARPA-GARD problem under agreement number 885000."
        },
        {
            "heading": "A Hardware and Software",
            "text": "We run all experiments using the HuggingFace API on 40GB NVIDIA A100 GPUs in the Debian GNU/Linux 10 system. We use the OPT and GPT2 models via the HuggingFace transformers library which can be easily adapted for reproducibility. We modify the Trainer class provided by the HuggingFace API for soft prompt tuning. We use the generate() function of the HuggingFace API to generate answers. Unless specified, we use the default parameters of the generate() function. When generating the answer set A(f, \u03b8p,x), we set max_new_tokens=50 while in other cases, we always set max_new_tokens=256. The parameters for different decoding strategies are provided below:\n\u2022 Beam search decoding: we set num_beams>1 and do_sample=False. If we want to get num_beams highest scoring beams, we will set num_return_sequences=num_beams. We will specify num_beams when using beam search decoding.\n\u2022 Multinomial sampling decoding: we set num_beams=1 and do_sample=True. We will specify temperature when using multinomial sampling decoding."
        },
        {
            "heading": "B Datasets",
            "text": "We use three question answering datasets: CoQA (Reddy et al., 2019), TriviaQA (Joshi et al., 2017) and SQuAD (Rajpurkar et al., 2016) for experiments. The details about these datasets are given below.\nB.1 CoQA CoQA is a large-scale dataset for Conversational Question Answering systems. The goal of the CoQA challenge is to measure the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation. CoQA contains 127,000+ questions with answers collected from 8,000+ conversations. The training set contains 108,647 question queries while the test set contains 7,983 question queries. We use the following template to construct question queries: [The provided context paragraph] [additional question-answer pairs]"
        },
        {
            "heading": "Q: [Provided question]",
            "text": "A:\nwhere additional question-answer pairs are preceding turns of the conversation about the paragraph consisting of questions and reference answers.\nB.2 TriviaQA\nTriviaQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We focus on TriviaQA as a closed-book QA task (in which the model must answer a question without access to a supporting paragraph). The training set contains 138,384 question queries while the test set contains 17,944 question queries. We split the original test set into a new test set containing 8,000 question queries and a validation set containing 9,944 question queries. We use the new test set for evaluation and use the validation set for hyper-parameter selection. We consider the following template with a 5-shot prompt to construct question queries:\nQ: In which decade did Billboard magazine first publish and American hit chart? A: 30s. Q: What is Bruce Willis' real first name? A: Walter. Q: Which city does David Soul come from? A: Chicago. Q: Which William wrote the novel Lord Of The Flies?"
        },
        {
            "heading": "A: Golding. Q: Where in England was Dame Judi Dench born? A: York. Q: [Provided question] A:",
            "text": "B.3 SQuAD\nStanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowd-workers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. We use the SQuAD 1.1 version, containing 100,000+ question-answer pairs on 500+ articles. The training set contains 86,821 question queries while the test set contains 5,928 question queries. We use the following template to construct question queries:\n[The provided context paragraph]"
        },
        {
            "heading": "Q: [Provided question]",
            "text": "A:"
        },
        {
            "heading": "C LLMs",
            "text": "We perform experiments with OPT (Zhang et al., 2022) and GPT-2 (Radford et al., 2019) models, which are based on Transformer architecture. For Transformer architecture, there is a limit on the lengths of the sequences we can pass the models. The OPT models can handle sequences of up to 2,048 tokens while the GPT-2 models can handle sequences of up to 1,024 tokens. If the sequence length of an input is larger than the maximum sequence length that is allowed, we force the model to output an empty sequence with a \u2212\u221e selection score."
        },
        {
            "heading": "D Baselines",
            "text": "For selective prediction, we need to get a predicted output sequence y\u0302\u2217 and a selection score g(x) for each input sequence x given a model f . The model f can be a pre-trained LLM or an LLM adapted with prompt tuning using training objective (12). We use the beam-search decoding, with the number of beams being equal to 5, to obtain the predicted output sequence y\u0302\u2217. We consider the following baselines to compute the selection score g(x): Perplexity. Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. The perplexity of the generated output sequence y\u0302\u2217 is computed as:\nperp(y\u0302\u2217 | x; f) = fnorm(y\u0302\u2217 | x)\u22121 (16)\nPredictive Entropy. Predictive entropy is a widely used measure of uncertainty. We use the multinomial sampling with a temperature of 0.5 to obtain an answer list [y\u03021, . . . , y\u0302m] for each input sequence x. The predictive entropy is computed as:\npe(x; f) = m\u2211 j=1 1 m log fnorm(y\u0302 j |x) (17)\nWe set m = 10. This is the same as the lengthnormalised predictive entropy baseline in Kuhn et al. (2023). Semantic Entropy. Semantic entropy is an entropy-based uncertainty measure which uses a bidirectional entailment algorithm for marginalising over semantically-equivalent samples (Kuhn et al., 2023). We follow the settings in Kuhn et al. (2023). Specifically, we use the multinomial sampling with a temperature of 0.5 to obtain an answer list of size 10 for each input sequence for uncertainty\nestimation. We use the Deberta-large model (He et al., 2020) that is fine-tuned on the NLI data set, MNLI (Williams et al., 2017) for the bidirectional entailment clustering algorithm. Self-eval. Self-eval is a simple baseline that obtains a selection score from the LLM by asking whether the proposed answer y\u0302\u2217 is correct or not. Suppose zs is a series of tokens representing the self-evaluation trigger string \u201cThe answer is \u201d. Suppose zc and zw are the tokens that represent the words \u201ccorrect\u201d and \u201cwrong\u201d respectively. Recall that the logits of the model f on v given x is f\u0304(v | x). Then, the self-eval score is computed as:\nP (zc | x, y\u0302\u2217) = exp (f\u0304(zc | x, y\u0302\u2217, zs))\u2211\nz\u2208{zc,zw} exp (f\u0304(z | x, y\u0302\u2217, zs)) (18)\nP(True). P(True) proposed by Kadavath et al. (2022) is a way to estimate the probability that a model\u2019s generation is correct by \u201casking\u201d the model if its answer is correct. It samples m answers and constructs a new natural language question using these possible answers as context before asking whether the proposed answer y\u0302\u2217 is correct and measures the probability of the completion being True. We set m = 4 and use the multinomial sampling with a temperature of 1.0 to sample the answers. The format of the prompt is:\nQuestion: Who was the third president of the United States? Here are some brainstormed ideas: James Monroe Thomas Jefferson John Adams Benjamin Harrison George Washington Possible Answer: James Monroe"
        },
        {
            "heading": "Is the possible answer: (A) True (B) False. The possible answer is:",
            "text": "where the \u201cbrainstormed answers\u201d are from the set of sampled answers and P(True) (i.e., the likelihood of the next token being True) is taken as the uncertainty measure."
        },
        {
            "heading": "E Training Details",
            "text": "We have two stage training: the first stage is to train the soft prompt \u03b8p using the training objective (12) and the second stage is to train the soft prompt \u03b8s using the training objective (13). For\nboth stages, we train the soft prompt for 10 epochs using AdamW optimizer with a batch size of 8, a learning rate of 0.01 and cosine learning rate schedule. We remove those data points (x,y) where |x|+ |y| > 700 from the training set Dtr to reduce GPU memory usage during training. Here, |x| is the length of the sequence x. This only removes a very small portion of data points from the training set for each dataset (remove 4.02% training data points in CoQA, 0% training data points in TriviaQA and 0.04% training data points in SQuAD). During training \u03b8p or \u03b8s, we always use 20% training data as validation data for selecting the best model among all checkpoints after each training epoch. Training \u03b8p, we select the best model based on the loss on the validation data. When training \u03b8s, we select the best model based on the AUROC on the validation data."
        },
        {
            "heading": "F Computational Complexity Analysis",
            "text": "The proposed method ASPIRE needs to train two soft prompts \u03b8p and \u03b8s. The complexity of training \u03b8p using the training objective (12) is the same as the complexity of the standard soft prompt tuning. When training \u03b8s using the training objective (13), the number of training steps is the same as that of training \u03b8p. In each training step of training \u03b8s, we compute gradients for one correct output and two wrong outputs while in each training step of training \u03b8p, we compute gradients for one reference output. Thus, the complexity of training \u03b8s is the same as that of training \u03b8p. Therefore, the complexity of the proposed method ASPIRE in the training time is the same as that of the standard soft prompt tuning.\nWe analyze the computational complexity of different methods at test time in terms of the number of forward passes for the LLM. Since the LLM generates the output sequence in an auto-regressive way, the number of forward passes needed depends on the length of the generated output sequence. Suppose the maximum length of the generated output sequence is lmax. To generate an output sequence given an input sequence, we need one forward pass to encode the input sequence and at most lmax forward passes to obtain the output sequence. Thus, for generating the output sequence, the maximum number of forward passes is 1 + lmax and the complexity is O(lmax). For the perplexity method, the computational complexity is O(lmax) since we only need additional one forward pass to obtain the\nperplexity score. For the predictive entropy method, the computational complexity is O(m \u00b7 lmax) since we need to additionally generate m answers and compute the likelihood of those m answers. For the semantic entropy method, we omit the computational complexity of the bidirectional entailment clustering algorithm since its computational cost is much smaller than that of the generation of the LLM as stated in Kuhn et al. (2023). Thus, the computational complexity for semantic entropy is O(m \u00b7 lmax). For the self-eval method, the computational complexity is O(lmax) since we only need one additional forward pass to obtain the self-eval score. For the P(True) method, the computational complexity is O(m \u00b7 lmax) since we need to additionally generate m answers and need one forward pass to compute the P(True) score. For the proposed method ASPIRE, the computational complexity is O(lmax) since we only need additional one forward pass to obtain the learned self-eval score. Table 6 summarizes the computational complexity of different methods at test time."
        },
        {
            "heading": "G Rouge Threshold for Evaluation",
            "text": "We use the Rouge-L (Lin and Och, 2004) metric to evaluate if the predicted answer is correct or not. The Rouge-L metric produces a score in [0, 1]. We need a threshold \u03b3 to determine whether the predicted answer is correct or not. If the Rouge-L score is larger than the threshold \u03b3, then the predicted answer is correct; otherwise, the predicted answer is wrong. The choice of \u03b3 depends on the applications. Low values of \u03b3 may lead to labeling some wrong answers as correct answers while large values of \u03b3 may lead to labeling some correct answers as wrong answers. If we regard the wrong answer as the positive class, then we can use the precision and recall metrics to evaluate the choice of \u03b3. To compute the precision and recall metrics, we need ground-truth labels for determining the\ncorrectness of predicted answers, which requires manual labeling. If the Rouge-L score is equal to 0 (or 1), then it is mostly sure that the predicted answer is wrong (or correct). Thus, we only need to label those samples whose Rouge-L scores are in (0, 1). To compare different values of \u03b3, we compute the precision and recall metrics after manually label 200 samples whose Rouge-L scores are in the range of (0, 1). The results in Table 7 show that larger values of \u03b3 lead to higher recall but lower precision, while the lower values of \u03b3 lead to higher precision but lower recall. We propose this work for safety-critical applications where accepting a wrong answer is more costly compared to rejecting a correct answer that is different from the reference answers. Thus, we prefer high recall than high precision. In our experiments, we evaluate different methods under the Rouge-L metric with \u03b3 \u2208 {0.7, 0.8, 0, 9} to ensure that the recall is at least 90%."
        },
        {
            "heading": "H Rouge Threshold for the Proposed Framework",
            "text": "In the proposed framework ASPIRE, we need the Rouge threshold \u03b3\u0302 to determine if the generated answer is correct or not. We want to set a large enough value of \u03b3\u0302 so that the generated wrong answers won\u2019t be labeled as correct answers. To determine the value of \u03b3\u0302, we manually label the correctness of the 10 generated answers for 50 training examples from each dataset (we have three datasets CoQA, TriviaQA and SQuAD). The answers are generated using the OPT-2.7B model. We find that if we set \u03b3\u0302 = 0.9, then no wrong answers would be labeled as correct answers. Thus, we set \u03b3\u0302 = 0.9 for the proposed framework."
        },
        {
            "heading": "I Complete Results",
            "text": "In this section, we present the complete results for OPT and GPT2 models of different sizes and different Rouge threshold \u03b3. We first evaluate the accuracy of different LLMs. The results are in Table 8 (set \u03b3 = 0.7), Table 9 (set \u03b3 = 0.8) and Table 10 (set \u03b3 = 0.9). The results show that after training \u03b8p via soft prompt tuning, the accuracy of LLMs is improved significantly. We then evaluate different approaches to compute the selection score when the model\u2019s predictions are fixed. The results are in Table 11 (use GPT2 models and set \u03b3 = 0.7), Table 12 (use GPT2 models and set \u03b3 = 0.8), Table 13 (use GPT2 models and set \u03b3 = 0.9), Ta-\nble 14 (use OPT models and set \u03b3 = 0.7), Table 15 (use OPT models and set \u03b3 = 0.8) and Table 16 (use OPT models and set \u03b3 = 0.9). The results show that the proposed method ASPIRE significantly outperforms the baselines in terms of AUACC and AUROC across different datasets and LLMs for different values of the Rouge threshold \u03b3."
        },
        {
            "heading": "J The Effect of the Hyper-parameter \u03b1",
            "text": "We study the effect of the hyper-parameter \u03b1 in the proposed selection score (Eq. (11)) for our method. The results in Table 17 show that setting \u03b1 = 0.25 leads to the best performance across different datasets and different models. Only using the normalized likelihood (i.e., \u03b1 = 0) or only using the learned self-eval score (i.e., \u03b1 = 1) consistently leads to much worse performance. We choose \u03b1 for our method based on the performance on the validation data from the TriviaQA dataset using the OPT-2.7B model. We then use the same \u03b1 value for different datasets and different models. We consider \u03b1 \u2208 {0.0, 0.25, 0.5, 0.75, 1.0} when tuning it. Based on the validation results, we set \u03b1 = 0.25 by default."
        },
        {
            "heading": "K Comparing with Self-Consistency",
            "text": "Self-consistency (Wang et al., 2022) can be used to obtain confidence measures as proposed by Si et al. (2022). We sample 10 times to obtain a set of different answers for each question using the multinomial sampling with a temperature of 0.5. Among all the generated answers, we take the most frequent answer as the final prediction and its frequency as the selection score. Since self-consistency produces discrete selection scores (in the above setting, the number of possible selection scores is 10) and we use the composite trapezoidal rule to compute AUACC, it is easier for self-consistency to achieve high AUACC compared to those approaches that produce continuous selection scores. Note that the proposed method produce continuous selection scores. Thus, it might not be fair to compare the proposed method with self-consistency. However, even though selfconsistency has more advantages in achieving high AUACC, the proposed method ASPIRE still significantly outperforms self-consistency as shown in Table 18. We also observe that Self-Consistency might lead to worse accuracy meaning that the LLM can be consistently wrong."
        },
        {
            "heading": "L Qualitative Evaluation",
            "text": "We present some concrete examples from the TriviaQA dataset to show the advantages of the proposed method qualitatively. We compare the proposed method ASPIRE to the baseline Semantic Entropy. The model for generating answers is the adapted OPT-2.7B with learned \u03b8p. The examples below show that some semantic entropy scores for correct predictions are lower than some semantic entropy scores for wrong predictions while the ASPIRE scores for correct predictions are consistently higher than the ASPIRE scores for wrong predictions. The ASPIRE scores are log likelihood scores and can be converted to likelihood scores by taking exponentiation with the base e. Examples where predictions are correct\nQuestion: Who is the most successful UK solo artist in the USA?\nAnswer: Elton John. Predicted answer: Elton John. Semantic entropy score: -1.1031 ASPIRE score: -0.8163\nQuestion: In which decade of the 20th century was Anne Bancroft born?\nAnswer: 1930s. Predicted answer: 1930s. Semantic entropy score: -0.6167 ASPIRE score: -0.9026\nQuestion: The Suez Canal connects the Mediterranean Sea to which other Sea?\nAnswer: Red sea.\nPredicted answer: Red Sea. Semantic entropy score: 2.2082 ASPIRE score: -0.2309\nQuestion: Sun Yat Sen overthrew the emperor in which country establishing a republic after 2000 years of imperial rule?\nAnswer: China. Predicted answer: China. Semantic entropy score: 2.0028 ASPIRE score: -0.4205\nExamples where predictions are wrong Question: Who was the director of the CIA from 1976-81? Answer: George Bush.\nPredicted answer: George H W Bush. Semantic entropy score: 0.4547 ASPIRE score: -1.0397\nQuestion: What Michelle Pfeiffer movie got a boost from the Coolio song Gangsta\u2019s Paradise?\nAnswer: Dangerous Minds. Predicted answer: Scarface. Semantic entropy score: 0.0647 ASPIRE score: -1.0531\nQuestion: What was President Gerald Ford\u2019s middle name?\nAnswer: Rudolph. Predicted answer: William.\nSemantic entropy score: -3.9773 ASPIRE score: -2.8203\nQuestion: Kim Carnes\u2019 nine weeks at No 1 with Bette Davis Eyes was interrupted for one week by which song?\nAnswer: Stars on 45 medley. Predicted answer: Bette Davis Eyes. Semantic entropy score: -1.4973 ASPIRE score: -2.2803"
        }
    ],
    "title": "Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs",
    "year": 2023
}