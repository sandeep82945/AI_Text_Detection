{
    "abstractText": "As language models are applied to an increasing number of real-world applications, understanding their inner workings has become an important issue in model trust, interpretability, and transparency. In this work we show that representation dissimilarity measures, which are functions that measure the extent to which two model\u2019s internal representations differ, can be a valuable tool for gaining insight into the mechanics of language models. Among our insights are: (i) an apparent asymmetry in the internal representations of model using SoLU and GeLU activation functions, (ii) evidence that dissimilarity measures can identify and locate generalization properties of models that are invisible via in-distribution test set performance, and (iii) new evaluations of how language model features vary as width and depth are increased. Our results suggest that dissimilarity measures are a promising set of tools for shedding light on the inner workings of language models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Davis Brown"
        },
        {
            "affiliations": [],
            "name": "Charles Godfrey"
        },
        {
            "affiliations": [],
            "name": "Nicholas Konz"
        },
        {
            "affiliations": [],
            "name": "Jonathan Tu"
        },
        {
            "affiliations": [],
            "name": "Henry Kvinge"
        }
    ],
    "id": "SP:8a04d281d7e0d737efd25559880e6ec8646df2d7",
    "references": [
        {
            "authors": [
                "Yossi Adi",
                "Einat Kermany",
                "Yonatan Belinkov",
                "Ofer Lavi",
                "Yoav Goldberg."
            ],
            "title": "Fine-grained analysis of sentence embeddings using auxiliary prediction tasks",
            "venue": "International Conference On Learning Representations.",
            "year": 2016
        },
        {
            "authors": [
                "Samuel K. Ainsworth",
                "Jonathan Hayase",
                "Siddhartha Srinivasa"
            ],
            "title": "Git Re-Basin: Merging Models modulo Permutation Symmetries",
            "year": 2023
        },
        {
            "authors": [
                "Yamini Bansal",
                "Preetum Nakkiran",
                "Boaz Barak."
            ],
            "title": "Revisiting model stitching to compare neural representations",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 225\u2013236. Curran Associates, Inc.",
            "year": 2021
        },
        {
            "authors": [
                "Serguei Barannikov",
                "Ilya Trofimov",
                "Nikita Balabin",
                "Evgeny Burnaev."
            ],
            "title": "Representation topology divergence: A method for comparing neural network representations",
            "venue": "arXiv preprint arXiv:2201.00058.",
            "year": 2021
        },
        {
            "authors": [
                "Yonatan Belinkov",
                "Nadir Durrani",
                "Fahim Dalvi",
                "Hassan Sajjad",
                "James Glass"
            ],
            "title": "What do neural machine translation models learn about morphology? In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol",
            "year": 2017
        },
        {
            "authors": [
                "Nora Belrose",
                "Zach Furman",
                "Logan Smith",
                "Danny Halawi",
                "Igor V. Ostrovsky",
                "Lev McKinney",
                "Stella Rose Biderman",
                "J. Steinhardt."
            ],
            "title": "Eliciting latent predictions from transformers with the tuned lens",
            "venue": "ARXIV.ORG.",
            "year": 2023
        },
        {
            "authors": [
                "Frances Ding",
                "Jean-Stanislas Denain",
                "Jacob Steinhardt."
            ],
            "title": "Grounding representation similarity through statistical testing",
            "venue": "Advances in Neural Information Processing Systems, 34:1556\u20131568.",
            "year": 2021
        },
        {
            "authors": [
                "Sam McCandlish",
                "Dario Amodei",
                "Christopher Olah."
            ],
            "title": "Softmax linear units",
            "venue": "Transformer Circuits Thread. Https://transformercircuits.pub/2022/solu/index.html.",
            "year": 2022
        },
        {
            "authors": [
                "Rahim Entezari",
                "Hanie Sedghi",
                "Olga Saukh",
                "Behnam Neyshabur."
            ],
            "title": "The role of permutation invariance in linear mode connectivity of neural networks",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Frankle",
                "G. Dziugaite",
                "Daniel M. Roy",
                "Michael Carbin."
            ],
            "title": "Linear mode connectivity and the lottery ticket hypothesis",
            "venue": "ICML.",
            "year": 2019
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima",
                "Shawn Presser",
                "Connor Leahy"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "year": 2020
        },
        {
            "authors": [
                "Charles Godfrey",
                "Davis Brown",
                "Tegan Emerson",
                "Henry Kvinge."
            ],
            "title": "On the symmetries of deep learning models and their internal representations",
            "venue": "arXiv preprint arXiv:2205.14258.",
            "year": 2022
        },
        {
            "authors": [
                "John Hewitt",
                "Percy Liang."
            ],
            "title": "Designing and interpreting probes with control tasks",
            "venue": "Conference On Empirical Methods In Natural Language Processing.",
            "year": 2019
        },
        {
            "authors": [
                "Jeevesh Juneja",
                "Rachit Bansal",
                "Kyunghyun Cho",
                "Jo\u00e3o Sedoc",
                "Naomi Saphra."
            ],
            "title": "Linear connectivity reveals generalization strategies",
            "venue": "arXiv preprint arXiv: Arxiv-2205.12411.",
            "year": 2022
        },
        {
            "authors": [
                "Max Klabunde",
                "Tobias Schumacher",
                "Markus Strohmaier",
                "Florian Lemmerich."
            ],
            "title": "Similarity of neural network models: A survey of functional and representational measures",
            "venue": "arXiv preprint arXiv:2305.06329.",
            "year": 2023
        },
        {
            "authors": [
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Honglak Lee",
                "Geoffrey E. Hinton."
            ],
            "title": "Similarity of Neural Network Representations Revisited",
            "venue": "ICML.",
            "year": 2019
        },
        {
            "authors": [
                "Karel Lenc",
                "A. Vedaldi."
            ],
            "title": "Understanding image representations by measuring their equivariance and equivalence",
            "venue": "Computer Vision And Pattern Recognition.",
            "year": 2014
        },
        {
            "authors": [
                "Nelson F. Liu",
                "Matt Gardner",
                "Yonatan Belinkov",
                "Matthew E. Peters",
                "Noah A. Smith."
            ],
            "title": "Linguistic knowledge and transferability of contextual representations",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Associ-",
            "year": 2019
        },
        {
            "authors": [
                "Ekdeep Singh Lubana",
                "Eric J. Bigelow",
                "Robert P. Dick",
                "David Krueger",
                "Hidenori Tanaka."
            ],
            "title": "Mechanistic mode connectivity",
            "venue": "arXiv preprint arXiv: 2211.08422.",
            "year": 2022
        },
        {
            "authors": [
                "R. Thomas McCoy",
                "Junghyun Min",
                "Tal Linzen."
            ],
            "title": "BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance",
            "venue": "Proceedings of the Third BlackboxNLP Workshop on An-",
            "year": 2020
        },
        {
            "authors": [
                "Scott Linderman"
            ],
            "title": "Generalized shape metrics",
            "year": 2021
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
            "year": 2020
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Samir Yitzhak Gadre",
                "Rebecca Roelofs",
                "Raphael Gontijo-Lopes",
                "Ari S. Morcos",
                "Hongseok Namkoong",
                "Ali Farhadi",
                "Yair Carmon",
                "Simon Kornblith",
                "Ludwig Schmidt"
            ],
            "title": "Model soups: Averaging weights",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The defining feature of deep neural networks is their capability of learning useful feature representations from data in an end-to-end fashion. Perhaps ironically, one of the most pressing scientific challenges in deep learning is understanding the features that these models learn. This challenge is not merely philosophical: learned features are known to impact model interpretability/explainability, generalization, and transferability to downstream tasks, and it can be the case that none of these effects are visible from the point of view of model performance on even carefully selected validation sets.\nWhen studying hidden representations in deep learning, a fundamental question is whether the internal representations of a given pair of models are similar or not. Dissimilarity measures (Klabunde et al., 2023) are a class of functions that seek to\n\u2020Work done at Pacific Northwest National Laboratory.\naddress this by measuring the difference between (potentially high-dimensional) representations. In this paper, we focus on two such functions that, while popular in computer vision, have seen limited application to language models: model stitching (Lenc and Vedaldi, 2014; Bansal et al., 2021) and centered kernel alignment (CKA) (Kornblith et al., 2019). Model stitching extracts features from earlier layers of model f and plugs them into the later layers of model g (possibly mediated by a small, learnable, connecting layer), and evaluates downstream performance of the resulting \u201cstitched\u201d model. Stitching takes a task-centric view towards representations, operating under the assumption that if two models have similar representations then these representations should be reconcilable by a simple transformation to such an extent that the downstream task can still be solved. On the other hand, CKA compares the statistical structure of the representations obtained in two different models/layers from a fixed set of input datapoints, ignoring any relationship to performance on the task for which the models were trained.\nIn this paper we make the case that dissimilarity measures are a tool that has been underutilized in the study of language models. We support this claim through experiments that shed light on the inner workings of language models: (i) We show that stitching can be used to better understand the changes to representations that result from using different nonlinear activations in a model. In particular, we find evidence that feeding Gaussian error linear unit (GeLU) model representations into a softmax linear unit (SoLU) model incurs a smaller penalty in loss compared to feeding SoLU activations into a GeLU model, suggesting that models with GeLU activations may form representations that contain strictly more useful information for the training task than the representations of models using SoLU activations. (ii) We show that dissimilarity measures can localize differences in models\nthat are invisible via test set performance. Following the experimental set-up of (Juneja et al., 2022), we show that both stitching and CKA detect the difference between models which generalize to an out-of-distribution test set and models that do not generalize. (iii) Finally, we apply CKA to the Pythia networks (Biderman et al., 2023), a sequence of generative transformers of increasing width and depth, finding a high degree of similarity between early layer features even as scale varies from 70 million to 1 billion parameters, an emergent \u201cblock structure\u201d previously observed in CKA analysis of image classifiers such as ResNets and Vision Transformers, and showing that CKA identifies a Pythia model (pythia-2.8b-deduped) exhibiting remarkably low levels of feature similarity when compared with the remaining models in the family (as well as inconsistent architectural characteristics). This last finding is perhaps surprising considering the consistent trend towards lower perplixity and higher performance on benchmarks with increasing model scale seen in the original Pythia evaluations (Biderman et al., 2023) \u2014 one might have expected to see an underlying consistent evolution of hidden features from the perspective of CKA."
        },
        {
            "heading": "2 Background",
            "text": "In this section we review the two model dissimilarity measures appearing in this paper.\nModel stitching (Bansal et al., 2021): Informally, model stitching asks how well the representation extracted by the early layers of one model can be used by the later layers of another model to solve a specific task. Let f be a neural network and for a layer l of f let f\u2264l (respectively f\u2265l) be the composition of the first l layers of f (respectively the layers m of f with m \u2265 l). Given another network g the model obtained by stitching layer l of f to layer m of g with stitching layer \u03d5 is g>m \u25e6 \u03d5 \u25e6 f\u2264l. The performance of this stitched network measures the similarity of representations of f at layer l and representations of g at layer m.\nCentered kernel alignment (CKA) (Kornblith et al., 2019): Let D = {x1, . . . , xd} be a set of model inputs. For models f and g with layers l and m respectively, let Af,l,g,m be the covariance matrix of f\u2264l(D) and g\u2264m(D). Then the CKA score for models f and g at layers l and m respec-\ntively and evaluated at D is\n||Af,l,g,m||2F ||Af,l,f,l||F ||Ag,m,g,m||F , (1)\nwhere || \u00b7 ||F is the Frobenious norm. Higher CKA scores indicate more structural similarity between representations. In our experiments we use an unbiased estimator of eq. (1) to calculate CKA in batches (see appendix C for details)."
        },
        {
            "heading": "3 Model stitching reveals an asymmetry between GeLU and interpretable-by-design SoLU",
            "text": "The design of more interpretable models is an area of active research. Interpretable-by-design models often achieve comparable performance on downstream tasks to their non-interpretable counterparts (Rudin, 2019). However, these models (almost by definition) have differences in their hidden layer representations that can impact downstream performance.\nMany contemporary transformers use the Gaussian error linear unit activation function, approximately GeLU(x) = x \u2217 sigmoid(1.7x). The softmax linear unit SoLU(x) = x\u00b7softmax(x) is an activation function introduced in (Elhage et al., 2022) in an attempt to reduce neuron polysemanticity: the softmax has the effect of shrinking small and amplifying large neuron outputs. One of the findings of (Elhage et al., 2022) was that SoLU transformers yield comparable performance to their GeLU counterparts on a range of downstream tasks. However, those tests involved zero-shot evaluation or finetuning of the full transformer, and as such they do not shed much light on intermediate hidden feature representations. We use stitching to do this.\nUsing the notation from Section 2, we set our stitching layer \u03d5 to be a learnable linear layer (all other parameters of the stitched model are frozen) between the residual streams following a layer l. We stitch small 3-layer (9.4M parameter) and 4- layer (13M parameter) SoLU and GeLU models trained by (Nanda, 2022), using the Pile validation set (Gao et al., 2020) to optimize \u03d5.\nFigure 1 displays the resulting stitching losses calculated on the Pile validation set. For both the 3-layer and 4-layer models, and at every layer considered, we see that when f is a SoLU model for the stitched model g>m\u25e6\u03d5\u25e6f\u2264l \u2014 the \u201cSoLU-intoGeLU\u201d cases \u2014 larger penalties are incurred than when f uses GeLUs, i.e. the \u201cGeLU-into-SoLU\u201d\ncases. We conjecture that this outcome results from the fact that SoLU activations effectively reduce capacity of hidden feature layers; there may be an analogy between the experiments of fig. 1 and those of (Bansal et al., 2021, Fig. 3c) stitching vision models of different widths.\nWe also measure the stitching performance between pairs of identical models, the SoLU-intoSoLU and GeLU-into-GeLU stitching baselines. This is meant to delineate between two factors influencing the stitching penalties being recorded: the ease of optimizing stitching layers and the interplay between hidden features of possibly different architectures. We seek to measure differences between hidden features, while the former is an additional unavoidable factor inherent in model stitching experiments. The identical SoLU-into-SoLU and GeLU-into-GeLU stitching baselines serve as a proxy measure of stitching optimization success, since in-principle the stitching layer should be able to learn the identity matrix and incur a stitching penalty of 0. So, that SoLU-into-SoLU stitches better than GeLU-into-GeLU gives evidence for SoLU models being easier to stitch with than GeLU models, from an optimization perspective. That both SoLU-into-GeLU and GeLU-into-SoLU incur higher stitching penalties than GeLU-into-GeLU suggests that the penalties cannot only be a result of stitching layer optimization issues.\nAs pointed out in (Bansal et al., 2021) such an analysis is not possible using CKA, which only detects distance between distributions of hidden features (displayed for our GeLU and SoLU models in figs. 4 and 5), not their usefulness for a machine learning task. Further, the differences in layer expressiveness between GeLU and SoLU models are not easily elicited by evaluation on downstream tasks, but can be seen through linear stitching."
        },
        {
            "heading": "4 Locating generalization failures",
            "text": "Starting with a single pretrained BERT model and fine-tuning 100 models (differing only in random initialization of their classifier heads and stochasticity of batching in fine-tuning optimization) on the Multi-Genre Natural Language Inference (MNLI) dataset (Williams et al., 2018), the authors of (McCoy et al., 2020) observed a striking phenomenon: despite the fine-tuned BERTs\u2019 near identical performance on MNLI, their performance on Heuristic Analysis for NLI Systems (HANS), an out of distribution variant of MNLI, (McCoy et al., 2019) was highly variable. On a specific subset of HANS called \u201clexical overlap\u201d (HANS-LO) one subset of fine-tuned BERTs (the generalizing) models) were relatively successful and used syntactic features; models in its complement (the heuristic models) failed catastrophically and used a strategy akin to\nbag-of-words.1 Recent work of (Juneja et al., 2022) found that partitioning the 100 fine-tuned BERTs on the basis of their HANS-LO performance is essentially equivalent to partitioning them on the basis of mode connectivity: that is, the generalizing and heuristic models lie in separate loss-landscape basins. We refer the interested reader to (Juneja et al., 2022) for further details.\nBut at what layer(s) do the hidden features of the generalizing and heuristic models diverge? For example, are these two subpopulations of models distinct because their earlier layers are different or because their later layers are different? Or both? Neither analysis of OOD performance nor mode connectivity analysis can provide an answer, but both stitching and CKA reveal that the difference between the features of the generalizing and heuristic models is concentrated in later layers of the BERT encoder. It is worth highlighting that conceptualizing and building relevant datasets for distribution shifts is often expensive and time consuming. That identity stitching and CKA using indistribution MNLI data alone differentiate between heuristic and generalizing behavior on HANS-LO suggests that they are useful tools for model error analysis and debugging.\nFigure 2a displays performance of pairs of BERT models stitched with the identity function \u03d5 = id, i.e. models of the form g>m \u25e6 f\u2264l on the MNLI finetuning task.2 We see that at early layers of the models identity stitching incurs almost no penalty, but that stitching at later layers (when the fraction of layers occupied by at the bottom model exceeds 75 %) stitching between generalizing and heuristic models incurs significant accuracy drop (high stitching penalty), whereas stitching within the generalizing and heuristic groups incurs almost no penalty.\nA similar picture emerges in fig. 2b, where we plot CKA values between features of fine-tuned\n1By \u201crelatively successful\u201d we mean \u201cAchieving up to 50% accuracy on an adversarially-designed test set.\u201d While variable OOD performance with respect to fine-tuning seed was also seen on other subsets of HANS, we follow (McCoy et al., 2020; Juneja et al., 2022) in focusing on HANS-LO where such variance was most pronounced.\n2The motivation for stitching with a constant identity function comes from evidence that stitching-type algorithms perform symmetry correction (accounting for the fact that features of model A could differ from those of model B by an architecture symmetry e.g. permuting neurons, see e.g. (Ainsworth et al., 2023)), but that networks obtained from multiple finetuning runs from a fixed pretrained model seem to not differ by such symmetries (see for example (Wortsman et al., 2022)).\nBERTs on the MNLI dataset. At early layers, CKA is insensitive to the generalizing or heuristic nature of model A and B. At later layers (in the same range where we saw identity stitching penalties appear), the CKA measure between generalizing and heuristic models significantly exceeds its value within the generalizing and heuristic groups.\nTogether, the results of fig. 2 paint the following picture: the NLI models of (McCoy et al., 2020) seem to all build up generally useful features in their early layers, and only decide to memorize (i.e., use a lexical overlap heuristic) or generalize (use syntactic features) in their final layers."
        },
        {
            "heading": "5 Representation dissimilarity in scaling families",
            "text": "Enormous quantities of research and engineering energy have been devoted to design, training and evaluation of scaling families of language models. By a \u201cscaling family\u201d we mean a collection of neural network architectures with similar components, but with variable width and/or depth resulting in a sequence of models with increasing size (as measured by number of parameters). Pioneering work in computer vision used CKA to discover interesting properties of hidden features that vary with network width and depth (Nguyen et al., 2021), but to the best of our knowledge no similar studies have appeared in the natural language domain\nWe take a first step in this direction by computing CKA measures of features within and between the models of the Pythia family (Biderman et al., 2023), up to the second-largest model with 6.9 billion parameters. In all experiments we use the \u201cdeduped\u201d models (trained on the deduplicated version of the Pile (Gao et al., 2020)). In the case of inter-model CKA measurements on pythia-6.9b we see in fig. 3 (bottom right) that similarity between layers l and m gradually decreases as |m\u2212 l| decreases. We also see a pattern reminiscent of the \u201cblock structure\u201d analyzed in (Nguyen et al., 2021), where CKA values are relatively high when comparing two layers within one of the following three groups: (i) early layers (the first 3), (ii) late layers (in this case only the final layer) and (iii) the remaining intermediate layers, while CKA values are relatively low when comparing two layers between these three groups.\nUsing CKA to compare features between pythia-{1b,1.4b,2.8b} and pythia-6.9b we observe high similarity between features at the be-\nginning of the model. A plausible reason for this early layer similarity is the common task of \u201cdetokenization,\u201d (Elhage et al., 2022) where early neurons in models may change unnatural tokenizations to more useful representations, for example responding strongly to compound words (e.g., birthday | party). We also continue to see \u201cblock structure\u201d even when comparing between two models, and in the case of the pairs (pythia-1b, pythia-6.9b) and (pythia-1.4b, pythia-6.9b) substantial inter-model feature similarity in intermediate layers.\nThe low CKA values obtained when comparing intermediate layers of (pythia-2.8b, pythia-6.9b) break this trend \u2013 in fact, as illus-\ntrated in fig. 9, pythia-2.8b exhibits such trendbreaking intermediate layer feature dissimilarity with every Pythia model with 1 billion or more parameters.3 Upon close inspection, we see that while the pythia-{1b,1.4b,6.9b} models all have a early layer \u201cblock\u201d consisting of 3 layers with relatively high CKA similarity, in pythia-2.8b this block consists of only 2 layers, suggesting the features of pythia-2.8b diverge from the rest of the family very early in the model. In table 1 we point out that some aspects of pythia-2.8b\u2019s architecture are inconsistent with the general scaling trend of the Pythia family as a whole.\n3Except itself, of course.\nLimitations\nIn the current work we examine relatively small language models. Larger models have qualitatively different features, and it is not obvious if our experiments on the differences in layer expressiveness between GeLU and SoLU models will scale to significantly larger models. While we show that identity stitching and CKA distinguish the heuristic and generalizing BERT models, we did not attempt to use stitching/CKA to automatically cluster the models (as was done with mode connectivity in (Juneja et al., 2022)).\nEthics Statement\nIn this paper we present new applications of representation analysis to language model hidden features. Large language models have the potential to impact human society in ways that we are only beginning to glimpse. A deeper understanding of the features they learn could be exploited for both positive and negative effect. Positive use cases include methods for enhancing language models to be more safe, generalizable and robust (one such approach hinted at in the experiments of section 4), methods for explaining the decisions of language models and identifying model components responsible for unwanted behavior. Unfortunately, these tools can at times be repurposed to do harm, for example extracting information from model training data and inducing specific undesirable model predictions."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research was supported by the Mathematics for Artificial Reasoning in Science (MARS) initiative via the Laboratory Directed Research and Development (LDRD) investments at Pacific Northwest National Laboratory (PNNL). PNNL is a multi-program national laboratory operated for the U.S. Department of Energy (DOE) by Battelle Memorial Institute under Contract No. DE-AC0576RL0-1830."
        },
        {
            "heading": "A Related Work",
            "text": "The problem of understanding how to compare the internal representations of deep learning models in a way that captures differences meaningful to the machine learning task has inspired a rich line of research (Klabunde et al., 2023). Popular approaches to quantifying dissimilarity range from shape based measures (Ding et al., 2021; Williams et al., 2021; Godfrey et al., 2022) to topological approaches (Barannikov et al., 2021).\nThe vast majority of research into these approaches has concentrated on computer vision applications rather than language models. Exceptions have looked at the evolution of model hidden layers with respect to a training task (Voita et al., 2019) or downstream tasks (Saphra and Lopez, 2019), statistical testing of sensitivity and specificity of dissimilarity metrics applied to BERT models (Ding et al., 2021), and more flexible representation dissimilarity measures aligning hidden features with invertible neural networks (Ren et al., 2023). While representation dissimilarity measures have not been extensively explored in language models, there is a rich line of work examining the structure of and relationship between hidden layers of NLP transformer models via probing for specific target tasks (Adi et al., 2016; Liu et al., 2019; Belinkov et al., 2017; Hewitt and Liang, 2019).\nOur Section 4 borrows an experiment set-up from (Juneja et al., 2022), which found that partitioning BERT models using geometry of the fine-tuning loss surface (namely, linear mode connectivity (Frankle et al., 2019; Entezari et al., 2022)) can differentiate between models that learn generalizing features and heuristic features for a natural language inference task. Similar mode connectivity experiments for computer vision models were performed in (Lubana et al., 2022)."
        },
        {
            "heading": "B Additional results",
            "text": "In figs. 4 and 5 we display CKA representation dissimilarity measurements for the GeLU and SoLU models used in the experiments of section 3. The purpose of these plots is simply to illustrate that in contrast to model stitching, CKA only detects distance between distributions of hidden features, not their usefulness for a machine learning task.\nThe plot in fig. 2 (left) shows identity stitching penalties for the MNLI fine-tuned BERT models on the MNLI dataset. In practice data distribution shifts are generally not known at the time of model development, and (as noted) the fact that identity stitching using MNLI data alone differentiates between heuristic and generalizing behavior on HANS-LO is potentially quite useful. Nevertheless, for comprehensiveness the performance of the identity stitched models on HANS-LO, displayed in fig. 7, also shows distinct differences between generalizing and heuristic models in the final stitching layer..\nIn fig. 2 (right) we display CKA between MNLI fine-tuned BERT models at each layer. This has the benefit of showing that the features of top (generalizing) and bottom (heuristic) models diverge in late BERT encoder layers (as we also saw with identity stitching in fig. 2 (left)). We aggregate the per-layer CKA measurements into a single distance measurement for each of the three cases (top-top, top-bottom, bottom-bottom) in fig. 8. This illustrates that the top (generalizing) and bottom (heuristic) models form\ntwo distinct clusters in a natural metric space of hidden features of MNLI datapoints; for the technical definition of this metric space we refer to appendix C.\nIn fig. 3 we saw high CKA values between intermediate features of pythia-{1b,1.4b} and pythia-6.9b, but comparatively low CKA values between intermediate features of the 2.8 and 6.9 billion parameter models. Figure 9 shows that more generally there is a high level of intermediate feature similarity for models in the set pythia-{1b,1.4b,6.9b} but a low level of intermediate feature similarity between pythia-2.8b and any of the models in pythia-{1b,1.4b,6.9b}.\nTable 1 suggests one hypothesis for why pythia-2.8b possesses unique hidden features from the perspective of CKA, namely that its attention head dimension is quite a bit smaller than those of the pythia-{1b,1.4b,6.9b} models.\nWe make a couple final notes on our analysis of Pythia models as it relates to prior work on image models. First, restricting attention to intra-model CKA heatmaps in fig. 10 we see an increase in intra-\nmodel feature similarity as scale increases to 1 billion parameters,4 analogous to an observation of (Nguyen et al., 2021). Note however that our experimental set up is not as clean as theirs: in our case both depth and width are scaled simultaneously, and perplexity of the Pythia models decreases as parameter count increases. In particular, our results would also be consistent with a hypothesis that intra-model feature similarity increases as validation loss decreases. Second, we speculate that the large blocks of intermediate features with high CKA similarity figs. 3 and 9 are related to the finding of (Raghu et al., 2022) that (compared to ResNets) Vision Transformers exhibit a high degree of feature uniformity across model layers. In other words, we hypothesize that this phenomenon is not specific to Vision Transformers, but rather transformer models more generally. However, as we only experiment with transformer models we are unable to make such a comparative statement and leave a broader study including e.g. RNNs and/or state-space sequence models to future work.\n4See also relevant subplots of figs. 3 and 9 for larger model scales."
        },
        {
            "heading": "C Experimental details",
            "text": "C.1 SoLU/GeLU and Pythia models\nFor the SoLU-GeLU and Pythia experiments we use the TransformerLens library (Nanda, 2022), whose developers pre-trained the SoLU/GeLU models on the Pile (Gao et al., 2020) (architecture details can be found here and some training details are available here). We use a heavily adapted version of the tuned-lens library to performing stitching (Belrose et al., 2023). In particular, for stitching we learn an affine transformation (along with an additional LayerNorm) \u03d5 with the PILE validation set. Figure 6 plots learning curves for GeLU-SoLU stitching layer optimization. We borrow many of the Tuned Lens hyperparameters, and train for 2000 steps (200 warm-up steps) with SGD with Nesterov momentum with a learning rate of 1.0 and cross-entropy loss.\nWhen computing CKA, we extract features after each transformer block (after addition with the residual, technically the hook_resid_post hook point of TransformerLens). Letting f and g be the two models under consideration and letting X be an input batch of token sequences, this yields feature tensors say\nHi = f\u2264i(X) and H \u2032j = g\u2264j(X)\nfor i = 1, . . . , L and j = 1, . . . , L\u2032 where L,L\u2032 are the respective depths of f and g. In our experiments we use batches consisting of 1 (!) sequence of 1024 tokens (the default context length for the Pythia models); these sequences are constructed by \u201cpacking\u201d datapoints from the Pile.5. When one or more of f , g is large (e.g. has more than 1 billion parameters), we run the necessary forward passes of f and g on separate GPUs \u2014 we used a mix of NVIDIA A100 GPUs with either 40 or 80 GB of RAM.\nWe then flatten the tensors of shape (batch, sequence, feature) (in our experiments (1, 1024, hidden_dim)) to a matrix of features with shape (batch \u2217 sequence, feature) (so in our experiments (1024, hidden_dim)),6 and then compute CKA values in batches using an implementation of the unbiased estimator described in (Nguyen et al., 2021). Explicitly, if Hi and H \u2032j are (already flattened) hidden feature matrices of shape (batch \u2217 sequence, feature) (with possibly distinct feature dimensions), we first mean center both matrices (Hi \u2190 Hi \u2212 11THi, similarly for H \u2032j) and compute the kernels\nKi = HiH T i and Lj = H \u2032 jH \u2032T j\nNext, we make use of the unbiased Hilbert-Schmidt independence criterion HSIC1(A,B), defined for symmetric matrices A and B of equal shape (n, n) as follows: first, let A\u0303 and B\u0303 be obtained by replacing the diagonals of A and B respectively with 0s.7 Then,\nHSIC1(A,B) := 1 n(n\u2212 3) ( tr(A\u0303B\u0303T ) +\n1 (n\u2212 1)(n\u2212 2) tr(A\u030311T )tr(B\u030311T )\u2212 2 n\u2212 2 A\u030311T B\u0303\n) .\nThis unweildy-looking expression can be computed efficiently by noting for example that tr(A\u0303B\u0303T ) is simply the dot-product of the vectorizations of A\u0303 and B\u0303, tr(A\u030311T ) is just the sum of all entries of A\u0303, and A\u030311T B\u0303 is just the dot product of the vectors obtained by summing columns of A\u0303 and B\u0303.\nFinally, we compute a matrix S of shape (L,L\u2032) with entries\nSij = HSIC1(Ki, Lj)\nHSIC1(Ki,Ki) \u00b7HSIC1(Lj , Lj) ,\nand average these batch-estimated matrices S over many randomly sampled batches, in our experiments 1000.\n5Explicitly, we use the chunk_and_tokenize function from TransformerLens 6In other words, we are comparing activations at the token level. An alternative which we have not yet evaluated would be\nflattening the sequence and feature indices to a single dimension and comparing activations at the sample level. 7In PyTorch or NumPy, accomplished with A\u0303 = A\u2212 diag(diag(A)).\nC.2 BERT models For the fine-tuned BERTs experiments we use a fork of the codebase made available by (Juneja et al., 2022) (as well as their models made available on HuggingFace (Wolf et al., 2020)). When computing CKA values, we extract features after each transformer block (in this case before addition with the residual, as it was not immediately clear how to obtain features after the residual addition). The rest of our CKA calculation is very similar to that described for SoLU, GeLU and Pythia models above, with the following modifications: first, since MNLI is a classification task input sequences are padded to the maximum context length of the model, in this case 512, rather than \u201cpacked.\u201d Second, we collected features Hi = f\u2264i(X) and H \u2032j = g\u2264j(X) for many batches \u2014in our case a total of N \u2248 1000 datapoints \u2014 and stack them to obtain a matrix of shape (N , max length, feature) for each layer of each model. Then we flatten the first two indices to get matrices of shape (N\u2217max length, feature), which are \u201cchunked\u201d along the first index \u2014 in our implementation to matrices of shape (1024, feature) \u2014 and passed to the batched, unbiased CKA estimator using HSIC1 as above.\nWe do not expect these differences in implementation to make much difference, as what is described in the preceding paragraph is essentially equivalent to the procedure used for SoLU, GeLU and Pythia models bit with a batch size of 2 instead of 1.\nTo collapse layer-by-layer CKA to a single scalar value representing distance between the hidden features of two models with the same depth (as in fig. 8), letting H1, . . . ,HL and H \u20321, . . . ,H \u2032 L be the matrices of hidden features in the respective models and compute\u221a\u221a\u221a\u221a L\u2211 i=1 arccos(CKA(Hi, H \u2032i)) 2. (2)\nThe explanation of this expression is as follows: as pointed out in (Williams et al., 2021), while CKA is not a metric (H = H \u2032 implies CKA(H,H \u2032) = 1, but for a metric it would have to be 0), the arccos if CKA is a metric in the mathematical sense. Equation (2) is then a metric on a product of hidden feature spaces obtained from the arccos-CKA metric on each of the factors."
        }
    ],
    "title": "Understanding the Inner Workings of Language Models Through Representation Dissimilarity",
    "year": 2023
}