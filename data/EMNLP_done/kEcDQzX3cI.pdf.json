{
    "abstractText": "Cross-lingual transfer learning from highresource to medium and low-resource languages has shown encouraging results. However, the scarcity of resources in target languages remains a challenge. In this work, we resort to data augmentation and continual pre-training for domain adaptation to improve cross-lingual abusive language detection. For data augmentation, we analyze two existing techniques based on vicinal risk minimization and propose MIXAG, a novel data augmentation method which interpolates pairs of instances based on the angle of their representations. Our experiments involve seven languages typologically distinct from English and three different domains. The results reveal that the data augmentation strategies can enhance fewshot cross-lingual abusive language detection. Specifically, we observe that consistently in all target languages, MIXAG improves significantly in multidomain and multilingual environments. Finally, we show through an error analysis how the domain adaptation can favour the class of abusive texts (reducing false negatives), but at the same time, declines the precision of the abusive language detection model.",
    "authors": [
        {
            "affiliations": [],
            "name": "Gretel Liz"
        },
        {
            "affiliations": [],
            "name": "Paolo Rosso"
        },
        {
            "affiliations": [],
            "name": "Robert Litschko"
        },
        {
            "affiliations": [],
            "name": "Goran Glava\u0161"
        },
        {
            "affiliations": [],
            "name": "Simone Paolo Ponzetto"
        }
    ],
    "id": "SP:cd26c093ffa1a345209bb966834c33c7785cb1ba",
    "references": [
        {
            "authors": [
                "Fatimah Alkomah",
                "Xiaogang Ma."
            ],
            "title": "A literature review of textual hate speech detection methods and datasets",
            "venue": "Information, 13(6):273.",
            "year": 2022
        },
        {
            "authors": [
                "Antreas Antoniou",
                "Amos Storkey."
            ],
            "title": "Assume, augment and learn: Unsupervised few-shot metalearning via random labels and data augmentation",
            "venue": "arXiv preprint arXiv:1902.09884.",
            "year": 2019
        },
        {
            "authors": [
                "Farid Arthaud",
                "Rachel Bawden",
                "Alexandra Birch."
            ],
            "title": "Few-shot learning through contextual data augmentation",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1049\u20131062,",
            "year": 2021
        },
        {
            "authors": [
                "Agathe Balayn",
                "Jie Yang",
                "Zoltan Szlavik",
                "Alessandro Bozzon."
            ],
            "title": "Automatic identification of harmful, aggressive, abusive, and offensive language on the web: a survey of technical biases informed by psychology literature",
            "venue": "ACM Transactions on Social",
            "year": 2021
        },
        {
            "authors": [
                "Reuter"
            ],
            "title": "A survey on data augmentation for text",
            "year": 2022
        },
        {
            "authors": [
                "man",
                "Malvina Nissim"
            ],
            "title": "DALC: the Dutch",
            "year": 2021
        },
        {
            "authors": [
                "Vladimir Vapnik"
            ],
            "title": "Vicinal risk minimization",
            "year": 2000
        },
        {
            "authors": [
                "Diyi Yang"
            ],
            "title": "An empirical survey of data",
            "year": 2023
        },
        {
            "authors": [
                "Tonelli",
                "Serena Villata"
            ],
            "title": "Hybrid emoji",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Thomas G Dietterich."
            ],
            "title": "Approximate statistical tests for comparing supervised classification learning algorithms",
            "venue": "Neural computation, 10(7):1895\u20131923.",
            "year": 1998
        },
        {
            "authors": [
                "Juuso Eronen",
                "Michal Ptaszynski",
                "Fumito Masui",
                "Masaki Arata",
                "Gniewosz Leliwa",
                "Michal Wroczynski."
            ],
            "title": "Transfer language selection for zero-shot cross-lingual abusive language detection",
            "venue": "Information Processing & Management,",
            "year": 2022
        },
        {
            "authors": [
                "Steven Y Feng",
                "Varun Gangal",
                "Jason Wei",
                "Sarath Chandar",
                "Soroush Vosoughi",
                "Teruko Mitamura",
                "Eduard Hovy."
            ],
            "title": "A survey of data augmentation approaches for nlp",
            "venue": "arXiv preprint arXiv:2105.03075.",
            "year": 2021
        },
        {
            "authors": [
                "Rodrigo Fernandes de Mello",
                "Moacir Antonelli Ponti",
                "Rodrigo Fernandes de Mello",
                "Moacir Antonelli Ponti."
            ],
            "title": "Statistical learning theory",
            "venue": "Machine Learning: A Practical Approach on the Statistical Learning Theory, pages 75\u2013128.",
            "year": 2018
        },
        {
            "authors": [
                "Anderson Almeida Firmino",
                "Cl\u00e1udio Souza de Baptista",
                "Anselmo Cardoso de Paiva."
            ],
            "title": "Using cross lingual learning for detecting hate speech in portuguese",
            "venue": "Database and Expert Systems Applications: 32nd International Conference, DEXA 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Goran Glava\u0161",
                "Mladen Karan",
                "Ivan Vuli\u0107."
            ],
            "title": "XHate-999: Analyzing and detecting abusive language across domains and languages",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 6350\u20136365, Barcelona,",
            "year": 2020
        },
        {
            "authors": [
                "Archika Jain",
                "Sandhya Sharma."
            ],
            "title": "A survey on identification of hate speech on social media post",
            "venue": "2022 3rd International Conference on Computing, Analytics and Networks (ICAN), pages 1\u20136. IEEE.",
            "year": 2022
        },
        {
            "authors": [
                "Kamil Kanclerz",
                "Alicja Figas",
                "Marcin Gruza",
                "Tomasz Kajdanowicz",
                "Jan Kocon",
                "Daria Puchalska",
                "Przemyslaw Kazienko."
            ],
            "title": "Controversy and conformity: from generalized to personalized aggressiveness detection",
            "venue": "Proceedings of the 59th Annual",
            "year": 2021
        },
        {
            "authors": [
                "Marwa Khairy",
                "Tarek M Mahmoud",
                "Tarek Abd-ElHafeez."
            ],
            "title": "Automatic detection of cyberbullying and abusive language in arabic content on social networks: a survey",
            "venue": "Procedia Computer Science, 189:156\u2013166.",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "SV Kogilavani",
                "S Malliga",
                "KR Jaiabinaya",
                "M Malini",
                "M Manisha Kokila."
            ],
            "title": "Characterization and mechanical properties of offensive language taxonomy and detection techniques",
            "venue": "Materials Today: Proceedings.",
            "year": 2021
        },
        {
            "authors": [
                "Jo\u00e3o Augusto Leite",
                "Diego Silva",
                "Kalina Bontcheva",
                "Carolina Scarton."
            ],
            "title": "Toxic language detection in social media for Brazilian Portuguese: New dataset and multilingual analysis",
            "venue": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Littell",
                "David R. Mortensen",
                "Ke Lin",
                "Katherine Kairis",
                "Carlisle Turner",
                "Lori Levin."
            ],
            "title": "URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors",
            "venue": "Proceedings of the 15th Conference of the European Chap-",
            "year": 2017
        },
        {
            "authors": [
                "Errol Mamani-Condori",
                "Jos\u00e9 Ochoa-Luna."
            ],
            "title": "Aggressive language detection using vgcn-bert for spanish texts",
            "venue": "Intelligent Systems: 10th Brazilian Conference, BRACIS 2021, Virtual Event, November 29\u2013December 3, 2021, Proceedings, Part II 10, pages",
            "year": 2021
        },
        {
            "authors": [
                "Marzieh Mozafari",
                "Reza Farahbakhsh",
                "Noel Crespi."
            ],
            "title": "Cross-lingual few-shot hate speech and offensive language detection using meta learning",
            "venue": "IEEE Access, 10:14880\u201314896.",
            "year": 2022
        },
        {
            "authors": [
                "banie",
                "Zaid Alyafeai",
                "Albert Webson",
                "Edward Raff",
                "Colin Raffel"
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
            "year": 2023
        },
        {
            "authors": [
                "Arianna Muti",
                "Francesco Fernicola",
                "Alberto Barr\u00f3nCede\u00f1o."
            ],
            "title": "Misogyny and aggressiveness tend to come together and together we address them",
            "venue": "Proceedings of the Thirteenth Language Resources and",
            "year": 2022
        },
        {
            "authors": [
                "Matthew A Napierala"
            ],
            "title": "What is the bonferroni correction",
            "venue": "Aaos Now,",
            "year": 2012
        },
        {
            "authors": [
                "Nathan Ng",
                "Kyunghyun Cho",
                "Marzyeh Ghassemi."
            ],
            "title": "SSMBA: Self-supervised manifold based data augmentation for improving out-of-domain robustness",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Debora Nozza."
            ],
            "title": "Exposing the limits of zero-shot cross-lingual hate speech detection",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
            "year": 2021
        },
        {
            "authors": [
                "Endang Wahyu Pamungkas",
                "Valerio Basile",
                "Viviana Patti."
            ],
            "title": "Towards multidomain and multilingual abusive language detection: a survey",
            "venue": "Personal and Ubiquitous Computing, 27(1):17\u201343.",
            "year": 2023
        },
        {
            "authors": [
                "Endang Wahyu Pamungkas",
                "Viviana Patti."
            ],
            "title": "Cross-domain and cross-lingual abusive language detection: A hybrid approach with deep learning and a multilingual lexicon",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Chongyu Pan",
                "Jian Huang",
                "Jianxing Gong",
                "Xingsheng Yuan."
            ],
            "title": "Few-Shot Transfer Learning for Text Classification with Lightweight Word Embedding Based Models",
            "venue": "IEEE Access, 7:53296\u201353304.",
            "year": 2019
        },
        {
            "authors": [
                "Sinno Jialin Pan",
                "Qiang Yang."
            ],
            "title": "A survey on transfer learning",
            "venue": "IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359.",
            "year": 2010
        },
        {
            "authors": [
                "Mei-hong Pana",
                "Hongyi Xin",
                "Hongbin Shen."
            ],
            "title": "Semantic transformation-based data augmentation for few-shot learning",
            "venue": "Available at SSRN 4321351.",
            "year": 2023
        },
        {
            "authors": [
                "Archit Parnami",
                "Minwoo Lee."
            ],
            "title": "Learning from few examples: A summary of approaches to few-shot learning",
            "venue": "arXiv preprint arXiv:2203.04291.",
            "year": 2022
        },
        {
            "authors": [
                "Fabio Poletto",
                "Valerio Basile",
                "Manuela Sanguinetti",
                "Cristina Bosco",
                "Viviana Patti."
            ],
            "title": "Resources and benchmark corpora for hate speech detection: a systematic review",
            "venue": "Language Resources and Evaluation, 55:477\u2013523.",
            "year": 2021
        },
        {
            "authors": [
                "Rahul Pradhan",
                "Ankur Chaturvedi",
                "Aprna Tripathi",
                "Dilip Kumar Sharma."
            ],
            "title": "A review on offensive language detection",
            "venue": "Advances in Data and Information Sciences: Proceedings of ICDIS 2019, pages 433\u2013439.",
            "year": 2020
        },
        {
            "authors": [
                "Michal Ptaszynski",
                "Agata Pieciukiewicz",
                "Pawe\u0142 Dyba\u0142a."
            ],
            "title": "Results of the poleval 2019 shared task 6: First dataset and open shared task for automatic cyberbullying detection in polish twitter",
            "venue": "Warszawa: Institute of Computer Sciences. Polish Academy of",
            "year": 2019
        },
        {
            "authors": [
                "Tharindu Ranasinghe",
                "Marcos Zampieri."
            ],
            "title": "Multilingual offensive language identification with crosslingual embeddings",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5838\u20135844, Online. As-",
            "year": 2020
        },
        {
            "authors": [
                "Julian Risch",
                "Anke Stoll",
                "Lena Wilms",
                "Michael Wiegand."
            ],
            "title": "Overview of the germeval 2021 shared task on the identification of toxic, engaging, and factclaiming comments",
            "venue": "Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, En-",
            "year": 2021
        },
        {
            "authors": [
                "Sebasti\u00e1n E Rodr\u00edguez",
                "H\u00e9ctor Allende-Cid",
                "H\u00e9ctor Allende."
            ],
            "title": "Detecting hate speech in cross-lingual and multi-lingual settings using language agnostic representations",
            "venue": "Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications:",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Rosa",
                "N\u00e1dia Pereira",
                "Ricardo Ribeiro",
                "Paula Costa Ferreira",
                "Joao Paulo Carvalho",
                "Sofia Oliveira",
                "Lu\u00edsa Coheur",
                "Paula Paulino",
                "AM Veiga Sim\u00e3o",
                "Isabel Trancoso."
            ],
            "title": "Automatic cyberbullying detection: A systematic review",
            "venue": "Computers in Human Behavior,",
            "year": 2019
        },
        {
            "authors": [
                "Andy Rosenbaum",
                "Saleh Soltan",
                "Wael Hamza",
                "Marco Damonte",
                "Isabel Groves",
                "Amir Saffari."
            ],
            "title": "CLASP: Few-shot cross-lingual data augmentation for semantic parsing",
            "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Fabian David Schmidt",
                "Ivan Vuli\u0107",
                "Goran Glava\u0161."
            ],
            "title": "Don\u2019t stop fine-tuning: On training regimes for few-shot cross-lingual transfer with multilingual language models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Philip Sedgwick."
            ],
            "title": "Pearson\u2019s correlation coefficient",
            "venue": "Bmj, 345.",
            "year": 2012
        },
        {
            "authors": [
                "Xiayang Shi",
                "Xinyi Liu",
                "Chun Xu",
                "Yuanyuan Huang",
                "Fang Chen",
                "Shaolin Zhu."
            ],
            "title": "Cross-lingual offensive speech identification with transfer learning for low-resource languages",
            "venue": "Computers and Electrical Engineering, 101:108005.",
            "year": 2022
        },
        {
            "authors": [
                "Oleh Shliazhko",
                "Alena Fenogenova",
                "Maria Tikhonova",
                "Vladislav Mikhailov",
                "Anastasia Kozlova",
                "Tatiana Shavrina"
            ],
            "title": "mgpt: Few-shot learners go multilingual",
            "year": 2022
        },
        {
            "authors": [
                "Elena Shushkevich",
                "John Cardiff."
            ],
            "title": "Automatic misogyny detection in social media: A survey",
            "venue": "Computaci\u00f3n y Sistemas, 23(4):1159\u20131164.",
            "year": 2019
        },
        {
            "authors": [
                "Gudbjartur Ingi Sigurbergsson",
                "Leon Derczynski."
            ],
            "title": "Offensive language and hate speech detection for Danish",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 3498\u20133508, Marseille, France. European Language",
            "year": 2020
        },
        {
            "authors": [
                "Levent Soykan",
                "Cihan Karsak",
                "Ilknur Durgar Elkahlout",
                "Burak Aytan."
            ],
            "title": "A comparison of machine learning techniques for Turkish profanity detection",
            "venue": "Proceedings of the Second International Workshop on Resources and Techniques for User Information in",
            "year": 2022
        },
        {
            "authors": [
                "Lukas Stappen",
                "Fabian Brunn",
                "Bj\u00f6rn Schuller."
            ],
            "title": "Cross-lingual zero-and few-shot hate speech detection utilising frozen transformer language models and axel",
            "venue": "arXiv preprint arXiv:2004.13850.",
            "year": 2020
        },
        {
            "authors": [
                "Lichao Sun",
                "Congying Xia",
                "Wenpeng Yin",
                "Tingting Liang",
                "Philip Yu",
                "Lifang He."
            ],
            "title": "Mixuptransformer: Dynamic data augmentation for NLP tasks",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 3436\u2013",
            "year": 2020
        },
        {
            "authors": [
                "JianYuan Wang",
                "KeXin Liu",
                "YuCheng Zhang",
                "Biao Leng",
                "JinHu Lu."
            ],
            "title": "Recent advances of few-shot learning methods and applications",
            "venue": "Science China Technological Sciences, pages 1\u201325.",
            "year": 2023
        },
        {
            "authors": [
                "Yaqing Wang",
                "Quanming Yao",
                "James T Kwok",
                "Lionel M Ni."
            ],
            "title": "Generalizing from a few examples: A survey on few-shot learning",
            "venue": "ACM computing surveys (csur), 53(3):1\u201334.",
            "year": 2020
        },
        {
            "authors": [
                "Genta Winata",
                "Shijie Wu",
                "Mayank Kulkarni",
                "Thamar Solorio",
                "Daniel Preotiuc-Pietro."
            ],
            "title": "Crosslingual few-shot learning on unseen languages",
            "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Wenjie Yin",
                "Arkaitz Zubiaga."
            ],
            "title": "Towards generalisable hate speech detection: a review on obstacles and solutions",
            "venue": "PeerJ Computer Science, 7:e598.",
            "year": 2021
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Ciss\u00e9",
                "Yann N. Dauphin",
                "David Lopez-Paz."
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference",
            "year": 2018
        },
        {
            "authors": [
                "Xin Zhang",
                "Miao Jiang",
                "Honghui Chen",
                "Chonghao Chen",
                "Jianming Zheng."
            ],
            "title": "Cloze-style data augmentation for few-shot intent recognition",
            "venue": "Mathematics, 10(18):3358.",
            "year": 2022
        },
        {
            "authors": [
                "Mengjie Zhao",
                "Yi Zhu",
                "Ehsan Shareghi",
                "Ivan Vuli\u0107",
                "Roi Reichart",
                "Anna Korhonen",
                "Hinrich Sch\u00fctze."
            ],
            "title": "A closer look at few-shot crosslingual transfer: The choice of shots matters",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Jing Zhou",
                "Yanan Zheng",
                "Jie Tang",
                "Li Jian",
                "Zhilin Yang."
            ],
            "title": "FlipDA: Effective and robust data augmentation for few-shot learning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Haris Bin Zia",
                "Ignacio Castro",
                "Arkaitz Zubiaga",
                "Gareth Tyson."
            ],
            "title": "Improving zero-shot crosslingual hate speech detection with pseudo-label finetuning of transformer language models",
            "venue": "Proceedings of the International AAAI Conference on Web",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Few-shot learning (FSL) is a machine learning paradigm that allows models to generalize from a small set of examples (Wang et al., 2020, 2023). Unlike traditional methods, FSL does not require training a model from scratch. Instead, pre-trained models are extended with just a little information, which is useful when training examples are scarce or data annotation is expensive.\n\u2217 Work done while at University of Mannheim\nTransfer learning is popularly used in few-shot learning, where the prior knowledge from a source task is transferred to the few-shot task (Pan and Yang, 2010; Pan et al., 2019). Usually, training data is abundant in the source task, while training data is low in the target task. In natural language processing, few-shot cross-lingual transfer learning (Glava\u0161 et al., 2020; Schmidt et al., 2022; Winata et al., 2022) is the type of few-shot transfer learning in which the source/target tasks are the same but the source/target languages are different. A pretrained multilingual model is first fine-tuned in a high-resource language and then fine-tuned on a few data in a target language (Zhao et al., 2021).\nDue to the limited availability of examples in the target language, naive fine-tuning can lead to overfitting and thus poor generalization performance on the few-shot task (Parnami and Lee, 2022). A strategy usually used to alleviate this problem, not just in the few-shot cross-lingual transfer but in FSL in general, is to increase the number of samples of the few-shot task from prior knowledge. This is the data-level approach (Chen et al., 2023), which can be divided into two categories: 1) transforming samples from the few existing examples (Arthaud et al., 2021; Zhou et al., 2022; Zhang et al., 2022) and 2) transforming samples from external datasets (Antoniou and Storkey, 2019; Rosenbaum et al., 2022; Pana et al., 2023).\nContributions. In this work, we explore abusive language detection in seven topologically diverse languages via few-shot cross-lingual transfer learning at the data-level. Although a number of studies have examined abusive language, we aim to take advantage of resources available for English in other less explored and low-resource languages. We focus on two aspects: 1) considering languages\nthat are typologically distinct from English and 2) with little effort. Previous works focus on languages that are similar to English, such as European languages (Stappen et al., 2020; Nozza, 2021; Rodr\u00edguez et al., 2021; Firmino et al., 2021; Zia et al., 2022; Castillo-L\u00f3pez et al., 2023). In contrast, we analyze languages that are more different from English. \u2018Little effort\u2019 refers to a consistent strategy across all languages, without requiring external resources or ad hoc processing for each particular language. The main contributions of this paper can be summarized as follows:\n- Dataset extension: We rely on a multidomain and multilingual dataset for abusive language detection (Glava\u0161 et al., 2020). This dataset contains texts in 5 languages which have been obtained by translating original English texts. To facilitate a more comprehensive evaluation, we extend the dataset by manually translating it into Spanish.\n- Few-shot cross-lingual transfer learning improvement at data-level: We rely on Vicinal Risk Minimization (VRM) (Chapelle et al., 2000) to generate synthetic samples in the vicinity of the examples to increase the amount of information to fine-tune the model in the target language. In this work we use three VRM-based techniques: 1) SSMBA (Ng et al., 2020), which uses two functions to move randomly through a variety of data, 2) MIXUP (Zhang et al., 2018), which linearly combines pairs of examples to obtain new samples and 3) MIXAG, our variant of MIXUP, which controls the angle between an example and the synthetic data generated in its neighbourhood.\n- Unsupervised language adaptation: We also simulate a fully unsupervised setup, removing the label information from the target languages. In that setup, we examine a strategy to address the lack of information that zero-shot transfer (no example to fine-tune the model) faces. The general idea is to make a domain adaption for abusive terms via masked language modeling (MLM) in the target language before the zero-shot transfer.\nWe aim to answer the following research questions:\nRQ1: What is the role of VRM-based techniques in few-shot cross-lingual abusive language detection?\nRQ2: What is the impact of different languages on few-shot cross-lingual abusive language detection?\nRQ3: How does VRM-based techniques fare against domain specialization for cross-lingual transfer of abusive language detection models?"
        },
        {
            "heading": "2 Background and Related Work",
            "text": "In this section, we discuss the main issue of fewshot learning and how data-based approaches can alleviate it. We take the definitions from Wang et al. (2020), where more details can be found. Then, we provide a brief overview of abusive language and align our work with recent studies focused on fewshot cross-lingual transfer approaches.\nFew-Shot Learning. Few-shot learning deals with a small training set Dtrain = {(xi, yi)} to approximate the optimal function f\u2217 that maps input x to output y, given a joint probability distribution p(x, y). Thus, a FSL algorithm is an optimization strategy that searches in a functions space F to find the set of parameters that determine the best f\n\u2032 \u2208 F . The performance is measured by a loss function l(f(x), y) which defines the expected risk with respect to p(x, y). However, p(x, y) is unknown, hence the empirical risk is used instead (Fernandes de Mello et al., 2018). This is the average of sample losses over Dtrain and can be reduced with a larger number of examples. One major challenge for FSL is then the small size of Dtrain, which can lead to the empirical risk not being a good approximation of the expected risk. To alleviate this problem, an approach that exploits prior knowledge can be used (Wang et al., 2023). Data-level approach involves methods that augment Dtrain with prior knowledge (Feng et al., 2021; Bayer et al., 2022; Dai et al., 2023).\nVicinal Risk Minimization formalizes the data augmentation as an extension of Dtrain by drawing samples from a neighbourhood of the existing samples (Chapelle et al., 2000). The distribution p(x, y) is approximated by a vicinity distribution Dv = {(x\u0302i, y\u0302i)}Nvi=1, whose instances are a function of the instances of Dtrain. Vicinal risk (Rv) is then calculated on Dv as Equation 1.\nRv = 1\nNv Nv\u2211 i=1 l(f(x\u0302i), y\u0302i) (1)\nIn this work, we study three VRM-based techniques that use different strategies to generate the vicinity distribution (see \u00a74).\nAbusive Language. Typically, abusive language refers to a wide range of concepts (Balayn et al., 2021; Poletto et al., 2021), including hate speech (Yin and Zubiaga, 2021; Alkomah and Ma, 2022; Jain and Sharma, 2022), profanity (Soykan et al.,\n2022), aggressive language (Muti et al., 2022; Kanclerz et al., 2021), offensive language (Pradhan et al., 2020; Kogilavani et al., 2021), cyberbullying (Rosa et al., 2019) and misogyny (Shushkevich and Cardiff, 2019). Pamungkas et al. (2023) overview recent research across domains and languages. They identify that English is still the most widely studied language, but abusive language datasets have been extended to other languages, including Italian, Spanish and German (Corazza et al., 2020; Mamani-Condori and Ochoa-Luna, 2021; Risch et al., 2021). In addition, we have found studies for other languages such as Arabic (Khairy et al., 2021), Danish (Sigurbergsson and Derczynski, 2020), Dutch (Caselli et al., 2021), Hindi (Das et al., 2022), Polish (Ptaszynski et al., 2019) and Portuguese (Leite et al., 2020). Regardless, some works like (Stappen et al., 2020) state that there is a need to extend the resources for diverse and low-resource languages. To cover this problem, Glava\u0161 et al. (2020) propose a multidomain and multilingual evaluation dataset. They show that language-adaptive additional pre-training of general-purpose multilingual models can improve the performance in transfer experiments. These are promising results, and although there are works like (Pamungkas et al., 2023) that cite this dataset, we have not found works that exploit it. In this work, we extend the study of the original work (Glava\u0161 et al., 2020) to assess strategies for enhancing the performance of abusive language detection in low-resource languages.\nCross-Lingual Abusive Language Detection. In recent years, cross-lingual abusive language detection has gained increasing attention in zero-shot (Eronen et al., 2022) and few-shot (Mozafari et al., 2022) transfer. Pamungkas and Patti (2019) propose a hybrid approach with deep learning and a multilingual lexicon for cross-lingual abusive content detection. Ranasinghe and Zampieri (2020) use English data for cross-lingual contextual word embeddings and transfer learning to make predictions in languages with fewer resources. More recently, Mozafari et al. (2022) propose an approach based on meta-learning for few-shot hate speech and offensive language detection in low-resource languages. They show that meta-learning models can quickly generalize and adapt to new languages with only a few labelled data points to identify hateful or offensive content. Their meta-learning models are based on optimization-level and metric-level.\nThese are two approaches to improve the problem of poor data availability in few-shot learning. In contrast, we focus on the data-level approach. Unlike other works that are also based on increasing data (Shi et al., 2022), we explore VRM-based strategies for abusive language detection."
        },
        {
            "heading": "3 Dataset and Experimental Setup",
            "text": "XHate-999 (Glava\u0161 et al., 2020) is an available dataset intended to explore several variants of abusive language detection. This dataset includes three different domains: Fox News (GAO), Twitter/Facebook (TRAC), and Wikipedia (WUL). In our work, we define ALL as the set of instances resulting from the union of all three domains. Each domain comprises different amounts of annotated data (abusive/non-abusive) in English for training, validation, and testing (see Appendix A). English test instances are translated into five target languages: Albanian (SQ), Croatian (HR), German (DE), Russian (RU), and Turkish (TR).\nWe extended this dataset with texts in Spanish. To generate the texts, we rely on machine translation and post-editing, following the monitored translation-based approach described in the dataset paper. Thus, slight modifications were made in the Spanish translation to reflect and maintain the level of abuse in the original English instances.\nModels. We rely on mBERT (Devlin et al., 2019) base cased with L = 12 transformer layers, hidden state size of H = 768, and A = 12 selfattention heads (see Appendix A for more details). First, we retrain the model with the XHate-999 training and validation sets, to obtain the model (model_base) that we use in all our experiments. We search the following hyper-parameter grid: training epochs in the set {2, 3, 4} and learning rate in {10\u22124, 10\u22125, 10\u22126}. We train and evaluate in batches of 2 texts, with a maximal length of 512 tokens, and optimize the models with Adam (Kingma and Ba, 2015). We set the random seeds to 7 to facilitate the reproducibility of experiments.\nFine-tuning and Evaluation Details. For each language, we draw 90% of instances from the test set to evaluate model_base. In few-shot crosslingual transfer experiments, we use the remaining 10% of instances to fine-tune model_base before the evaluation. i.e. we use 10 instances to fine-tune model_base in GAO (and 89 to evaluate), while the respective numbers are 30 (270)\nfor TRAC, 60 (540) for WUL, and 100 (899) for ALL (GAO+TRAC+WUL). Notice that for each language, the test set used by Glava\u0161 et al. (2020) is different from the one we use. However, we do not observe a significant difference between the use of the full test set and the use of the subset we rely on (see Appendix C to examine the results).\nStatistical Analysis. In our experiments, we used McNemar\u2019s test as (Dietterich, 1998) recommends. This is a paired non-parametric statistical hypothesis test where the rejection of the null hypothesis suggests that there is evidence to say that the models disagree in different ways. We set the significance level to 0.05 and use \u03b1altered, obtained with the Bonferroni correction (Napierala, 2012)."
        },
        {
            "heading": "4 Few-Shot Cross-lingual Transfer",
            "text": "We first examine the ability of three VRM-based techniques in few-shot cross-lingual transfer learning for abusive language detection to address RQ1."
        },
        {
            "heading": "4.1 SSMBA",
            "text": "Ng et al. (2020) propose SSMBA, a data augmentation method for generating synthetic examples with a pair of corruption and reconstruction functions to move randomly on a data manifold. In the corruption function, we use two strategies: 1) masking a word in each text in a random way (default) or 2) masking the salient abusive words in each text. To identify abusive words, we use HurtLex (Bassignana et al., 2018), a multilingual lexicon with harmful words. For texts that do not contain words in the lexicon, we follow strategy 1. In the reconstruction functions, we use mBERT."
        },
        {
            "heading": "4.2 MIXUP",
            "text": "(Zhang et al., 2018; Sun et al., 2020) is a VRMbased technique that constructs a synthetic example (x\u0302i, y\u0302i) (in the vicinity distribution) from the linear combination of two pairs (xi, yi) and (xj , yj), drawn at random from the training set Dtrain as Equation 2, with \u03bb \u223c \u03b2(\u03b1, \u03b1) where \u03b1 is a hyperparameter1.\nx\u0302i = \u03bbxi + (1\u2212 \u03bb)xj y\u0302i = \u03bbyi + (1\u2212 \u03bb)yj\n(2)\nWe rely on a multilingual GPT model (Shliazhko et al., 2022) (see Appendix A) for the linear\n1We tried some values different from 1 for \u03b1 and MIXUP was not sensitive to variation, so we set it to 0.2.\ncombination of the texts representations (Equation 3): we obtain the embedding Ew of each word of a text xi and concatenate them to generate the vector representation E(xi). Then, we combine two texts xi and xj as the linear combination of their representations E(xi) and E(xj). Note that Ew is a single step of an auto-regressive model. The obtained vector is split into vectors of the same size as the original word embeddings Ew. Finally, we decode those vectors to obtain a sequence T of words, that we use as the new syntectic text x\u0302i. The linear combination of the labels y \u2208 {0, 1}, when yi and yj are different depends on the value of \u03bb. We assign 1 to y\u0302i when the combination is greater than or equal to 0.5. Otherwise, we assign 0.\nx\u0302i = T (\u03bbE(xi) + (1\u2212 \u03bb)E(xj)) (3)\nProcedure. This VRM-based technique is an iterative process. In each iteration, the few-shot set Dtrain is divided into pairs of samples to combine. Thus, the number of instances generated in each iteration is equal to N2 , where N is the number of samples in Dtrain. We make sure not to take the same pairs of examples in different iterations."
        },
        {
            "heading": "4.3 MIXAG",
            "text": "Motivated by the idea of MIXUP, we propose the variant MIXAG: mix vectors with a focus on the AnGle between them. We hypothesize that the distance between an example and the new synthetic examples may be relevant to generate an effective vicinity. As this aspect cannot be easily controlled in the original MIXUP, we propose a particular case which interpolates pairs of instances based on the angle of their representation.\nThe idea is to define a linear combination (Equation 4) with the parameter \u03bb as a function of the angle \u03b1 between the original vectors xi and xj , as well as the angle \u03b8 between the new vector x\u0302 and one of the original vectors (Figure 1).\nx\u0302 = \u03bbxi + xj (4)\nUsing the Law of Sines we express \u03bb as a function (Equation 5) of the cosine of \u03b1, which can be obtained with Equation 6, and the cosine of \u03b8, which is the parameter of MIXAG. || \u00b7 || denotes the norm of a vector. We refer readers to Appendix B for more details.\n\u03bb = ||xj ||(cos(\u03b8)\n\u221a 1\u2212cos(\u03b1)2\u2212cos(\u03b1) \u221a 1\u2212cos(\u03b8)2)\n||xi|| \u221a 1\u2212cos(\u03b8)2 (5)\ncos(\u03b1) = xixj\n||xi||||xj || (6)\nFor MIXAG, we define the combination of texts by Equation 7, following the same representation and processing of texts as in MIXUP. The difference is basically in the parameter \u03bb.\nx\u0302i = T (\u03bbE(xi) + E(xj)) (7)\nIn this work, we set \u03b8 = \u03b12 , thus the parameter of MIXAG is defined by Equation 8. We suggest extending this study to analyze how the parameter cos(\u03b8) can influence the results.\ncos(\u03b8) =\n\u221a 1 + cos(\u03b1)\n2 (8)\nProcedure. This VRM-based technique is also an iterative process. In this case, we randomly select a sample xi from Dtrain and create the pairs with xi and each of the rest of the samples of Dtrain. Therefore, the number of instances generated in each iteration is N \u2212 1, where N is the number of samples in Dtrain."
        },
        {
            "heading": "4.4 Multilingual MIXUP/MIXAG",
            "text": "By default, in MIXUP and MIXAG we use the fewshot set Dtrain of each language to generate new instances for that particular language. Alternatively, we use the union of the Dtrain of all languages. For each pair of original texts xi and xj , we make sure that xi is from the language in the analysis, while xj is a text from any language."
        },
        {
            "heading": "4.5 Multidomain MIXUP/MIXAG",
            "text": "We rely on training data for GAO, TRAC and WUL, as well as ALL (WUL+TRAC+GAO) in all monolingual and multilingual experiments. In short, we analyze performance when training and testing 1) only on a particular domain (for example, when testing on GAO we train only on GAO training data) and 2) on all available data from all three data sets (multidomain setup)."
        },
        {
            "heading": "4.6 Results and Analysis",
            "text": "A summary of cross-lingual transfer results for the variants - few-shot and few-shot with SSMBA, MIXUP and MIXAG - is provided in Figure 2 (we refer readers to Appendix C for all the results).\nAs expected, we observed that VRM-based techniques improve the performance of few-shot crosslingual transfer in most cases. There is no clear difference between the VRM-based techniques, but we can see interesting results that vary depending on the domain. In the GAO domain, all three techniques seem to have similar results across languages. In TRAC, MIXUP seems to be slightly better than MIXAG in most languages. However, the critical result in this domain is that SSMBA fails to improve the few-shot cross-lingual transfer. In contrast, SSMBA seems to be the best technique in WUL. We believe that these results are due to the nature of the texts in each domain. TRAC contains texts from Twitter and Facebook. We speculate that the reconstruction function of SSMBA affects the quality of the vicinity generated for each text by introducing terms that differ from common terms in this domain. On the other hand, WUL contains text from Wikipedia, which supports our assumption.\nMultidomain. Table 1 shows the results for all the variants of the VRM-based techniques. We illustrate and analyze the results for the combination\nof all domains. The results by domain are detailed in Appendix C.\nAll languages except German seem to benefit from few-shot cross-lingual transfer w.r.t. zeroshot cross-lingual transfer. Likewise, the few-shot cross-lingual transfer is improved with VRM-based techniques as in the results by domain.\nSSMBA improves few-shot cross-lingual transfer in all languages except English. In this heterogeneous domain, we do not observe the problem that SSMBA has in TRAC. On the other hand, the use of HurtLex does not seem to be a relevant strategy, since the results are similar to those obtained with the default strategy (random selection). This is an encouraging result, which suggests that we can use SSMBA to improve few-shot cross-lingual transfer learning without relying on external resources.\nMIXUP seems to be better than SSMBA and MIXAG for most languages. However, multilingual MIXAG is significantly the best strategy. This is a good indicator of the benefits of our variant for multidomain and multilingual environments. Note that the multilingual strategies outperform the rest of the variants and that particularly, multilingual MIXAG consistently performs better than multilingual MIXUP. This suggests that our hypothesis about the implication of controlling the angle between the original texts and the new synthetic texts seems to be relevant in multilingual data.\nFinally, we combine MIXUP/MIXAG with SSMBA: First, we augment the data with SSMBA and then augment the new vicinity with MIXUP/MIXAG. The results are also shown in Table 1. This strategy offers some improvement over\nMIXUP/MIXAG in most cases.\nCorrelation Analysis. Thus far, we have observed that the behaviour of the strategies seems quite similar across languages. For instance, the few-shot cross-lingual transfer is outperformed with the VRM-based techniques. This motivates us to investigate RQ2, i.e. we examine if there is a high correlation between the performance of fewshot cross-lingual transfer (and its variants with VRM-based techniques) and the linguistic proximity scores of each language to English.\nWe analyze the correlation between the performance of the strategies that we use for crosslingual transfer learning and the distance between each language and English. We rely on the tool LANG2VEC2 which proves language vectors that encode linguistic features from the URIEL database (Littell et al., 2017). We obtain the vector representation of the languages with 4 features: 1) SYN: encodes syntactic properties, 2) FAM: encodes memberships in language families, 3) INV: denotes the presence of natural classes of sounds and 4) PHO: encodes phonological properties.\nThen, with the vectors from each linguistic feature, we calculate the cosine similarity between each language and English. Finally, we calculate the Pearson correlation coefficients (Sedgwick, 2012) between the cosine similarity and the performance of each cross-lingual strategy across languages and domains.\nTable 2 shows the correlation coefficients for the significant linguistic features with a significance\n2https://github.com/antonisa/lang2vec\nlevel of 0.05 (Appendix C shows the correlation coefficients for all metrics and the similarity scores between each language and English). Coefficients whose magnitude is between 0.5 and 0.7 indicate a moderate correlation, while coefficients between 0.3 and 0.5 indicate a low correlation.\nWe only observe a moderate correlation between the performance of each strategy and the distance between the target languages and English. We consider these results encouraging because they suggest that the strategies are possibly consistent across languages."
        },
        {
            "heading": "4.7 Ablation Studies",
            "text": "MIXAG is a data augmentation method that randomly combines inputs and accordingly combines one-hot-label encodings. This is a variant of MIXUP where the new data is obtained by defining the angle between the inputs and the new instance.\nIn our strategy, we randomly select pairs of inputs and set the angle between the new instance and one of the inputs as \u03b8 = \u03b12 , where \u03b1 is the angle between the original inputs. However, there are other strategies that could be used. For example, selecting data pairs whose latent representations are close neighbors, as well as defining other values for \u03b8. To compare MIXAG with these alternative possibilities, we run a set of ablation study experiments using not only mBERT, but also the XLM-R model (Conneau et al., 2020). We focus on multilingual and multimodal MIXAG (MMA in ALL) as it is the best data augmentation method that we observed in the first experiments.\nOn the one hand, we compare the combination of random pairs of inputs with the combination of nearest neighbors (NN). On the other hand, we set the angle \u03b8 = \u03b13 to evaluate the impact of varying this parameter on the performance of the method. Finally, we use an alternative model for the text rep-\nresentation. Specifically, we used the multilingual generative model mT0 (Muennighoff et al., 2023), instead of mGPT.\nFrom the results of the ablation study in Table 3, we have the following observations. First, there are no significant differences with \u03b1 = .05 between the variants studied, although experiments with XLM-R seem to have shown some improvement. Secondly, we note that the variation of the angle between the inputs and the generated instances does not seem to represent a relevant factor.\nAll five variants obtain very similar results with mBERT. The variation of the factors that we analyze does not seem to influence the performance of the method. However, with XLM-R we observe some interesting findings. Spanish and Russian are the only languages where MMA method is not surpassed by the other variants. In the rest of the languages, we observe the opposite behaviour, where text representation with the alternative model mT0 seems to be the best strategy. Notice that in Albanian the use of mT0 for text representation together with the strategy of selecting the nearest neighbor for interpolation seems to be the best variant."
        },
        {
            "heading": "5 Unsupervised Language Adaptation",
            "text": "In this section, we investigate the scenarios in which there is no information about the target language for the few-shot cross-lingual transfer. In \u00a7 4 we used a small amount of supervised data Dtrain in the target language to fine-tune the pre-trained model. This allowed us to adapt the model to the abusive language of each particular language. In contrast, now we assume that the labels of Dtrain are not available. This is a simulated experiment where we only have an unlabelled set of texts and the set Dtest in which we want to detect abusive language. Previous works have examined this scenario by adjusting a model with unlabelled external data. In this work, we use only a few unlabelled instances from Dtrain.\nBasically, this strategy is a zero-shot crosslingual transfer learning in which the model is adapted to the abusive terms of the target language. As mBERT is pre-trained on general-purpose and multilingual corpora, it is familiar with the target languages. However, it has not been adjusted to the particular case of abusive language. We follow then a two-step methodology: 1) continual pretaining for domain adaptation via masked language modeling (MLM) to make it familiar to the partic-\nular abusive terms, and then 2) employ zero-shot learning to detect abusive language."
        },
        {
            "heading": "5.1 Results and Analysis",
            "text": "Table 4 illustrates the results obtained with the methodology across domains and languages. In most cases, the strategy of prior adaptation to the abusive terms seems to outperform zero-shot crosslingual transfer learning. English is the only language in which the MLM adaptation worsens the results in all domains. Moreover, TRAC also shows no improvement, similar to the behaviour observed with SSMBA in few-shot cross-lingual transfer.\nThese results allow us to answer RQ3: although domain adaptation can improve zero-shot crosslingual transfer, VRM-based techniques seem to be more robust in few-shot cross-lingual transfer.\nError Analysis. In order to deepen the analysis of what happens in the model with the zero-shot cross-lingual transfer adaptation, we also analyze two metrics: Recall and Precision. Recall refers to the true positive rate and is the number of true positives divided by the total number of positive texts. Precision refers to the positive predictive value and is the number of true positives divided by the total number of positive predictions. In this work, positive refers to the class of abusive texts.\nResults across domains and languages are in Appendix C. In all cases we observe an increase in Recall, indicating that adapting the model could improve the proportion of the class of abusive texts that is correctly classified. At first glance, it seems to be a good result, since it is desirable to reduce the number of false negatives in abusive language detection. However, we observe that precision is reduced, suggesting that this strategy favours the\npositive class: while false negatives are reduced, false positives are increased.\nCritical cases are negative texts that can be incorrectly detected as abusive. In order to study this phenomenon, we examine the percentage of texts that are non-abusive and are well-classified with zero-shot transfer learning and misclassified with the MLM adaptation. We investigate two statistics across languages and domains: 1) the percentage of non-abusive texts that are well- classified with zero-shot transfer and misclassified with the MLM adaptation and 2) the percentage of abusive texts that are misclassified with zero-shot transfer and well-classified with the MLM adaptation.\nTable 5 illustrates the statistics across domains and languages. Consistent with the previous results we observe a detriment in the class of non-abusive texts. The number of negative texts well-classified with zero-shot transfer learning and misclassified with the MLM adaptation is large (reaching 100% in a case). However, that amount is surpassed in most cases by the gain in the class of abusive texts. We observe that the number of positive texts that are misclassified with zero-shot transfer learning and well-classified with adaptation via MLM is high (reaching 100% in four cases)."
        },
        {
            "heading": "6 Conclusions and Future Work",
            "text": "In this work, we studied three techniques to improve few-shot cross-lingual transfer learning in abusive language detection. These techniques are concentrated on data-level approach to deal with the problem of data scarcity that can lead to a high estimation error in few-shot learning. Specifically, we focused on vicinal risk minimization techniques to increase the data in the vicinity of the\nfew-shot samples. First, we explored two existing techniques: 1) SSMBA, which is based on a pair of functions to corrupt and reconstruct texts, and 2) MIXUP, which generates new samples from a linear combination of original instances pairs. Then, we proposed MIXAG, a variant of MIXUP, to parameterize the combination of instances with the angle between them. Our experiments were based on the multidomain and multilingual dataset XHATE-999, which allowed us to explore lowresource languages as target languages and English as the base language. This dataset contains six different languages, and we extended it to Spanish, following the same methodology that was used to generate the texts of the other languages. The results showed the effectiveness of VRM-based techniques to improve few-shot cross-lingual transfer learning in most domains and languages. Particularly, we observed that multilingual MIXAG outperforms the other strategies in the heterogeneous set (multidomain) for all target languages. At the same time, we observed that structural language similarity does not seem to be highly correlated with cross-lingual transfer success in none of the strategies. These results are encouraging for abusive language detection in low-resource settings, as\nthe strategies that we have examined appear to be consistent across languages.\nFinally, we evaluated a scenario where it is not possible to perform a few-shot cross-lingual transfer due to the lack of supervised information. We used a strategy based on masked language modeling and saw a degradation in the class of nonabusive texts, but a gain in the class of abusive texts, reducing false negatives.\nIn future work, we aim to further examine our proposed VRM-based technique for data augmentation. MIXAG uses as a parameter the angle between the new instance and one of the original instances being combined. In our experiments, we fixed the angle as half the angle between the original instances, but we consider that the flexibility of varying that parameter must be exploited."
        },
        {
            "heading": "7 Limitations and Ethical Concerns",
            "text": "Our experiments relied on a dataset that only contains English texts in the training and development sets. Only the test set is multilingual. Therefore, we were forced to partition the test set in order to perform the few-shot cross-lingual transfer and domain adaptation experiments. We compared the\nresults obtained in zero-shot cross-lingual transfer with the original test set and with the subset used in our experiments. We did not observe statistical differences. However, this may be a limitation in comparing our results with the original results reported in the dataset paper. Moreover, we observed a limitation in the strategy of domain adaptation. As we discussed in the error analysis, although the class of abusive texts is favoured with this strategy, we observed a detriment in the negative class.\nThis work aims to improve abusive language detection in low-resource languages. While this can be useful for many languages, there are certain ethical implications. Therefore, we strongly recommend not using the proposed strategies as the sole basis for decision-making in abusive language detection. Regarding the issue of privacy, all the data we use in our experiments, both the original dataset and the new texts in Spanish that we generated, are publicly available. It should be noted that the scope of this work is strictly limited to the evaluation of models that are also publicly available, and it is not used to promote abusive language with the information obtained."
        },
        {
            "heading": "Acknowledgements",
            "text": "FairTransNLP research project (PID2021-124361OB-C31) funded by MCIN/AEI/10.13039/501100011033 and by ERDF, EU A way of making Europe."
        },
        {
            "heading": "A Reproducibility",
            "text": "Table 6 provides features and links to the pretrained models that we use, and Table 7 illustrates details of the dataset."
        },
        {
            "heading": "B MIXAG Details",
            "text": "MIXAG is a particular case of MIXUP where the parameter \u03bb of the linear combination (Equation 9) is determined by the angle \u03b1 between the original vectors xi and xj , as well as the angle \u03b8 between the new vector x\u0302 and one of the original vectors. We take xi without loss of generality (see Figure 3). We rely on the cosine of \u03b1, calculated as Equation 10, where || \u00b7 || denotes the norm of a vector. Notice that we only parameterize one of the original vectors, since \u03b1 and \u03b8 are sufficient to determine x\u0302.\nx\u0302 = \u03bbxi + xj (9)\ncos(\u03b1) = xixj\n||xi||||xj || (10)\nThe objective is to express the parameter \u03bb as a function of \u03b8, hence we take advantage of the Law of Sines (Equation 11) that allows relating\nvectors and angles. Then, \u03bb can be expressed in function of \u03b8 as Equation 12. Finally, using the known identities in Equations 13, we can define \u03bb from the cosine of \u03b1, which can be obtained with Equation 10, and the cosine of \u03b8, which is the parameter of MIXAG (Equation 14).\n\u03bb||xi|| sin(\u03b1\u2212\u03b8) = ||xj || sin(\u03b8)\n(11)\n\u03bb = ||xj ||sin(\u03b1\u2212\u03b8) ||xi||sin(\u03b8) (12)\nsin(\u03b1\u2212\u03b8)=sin(\u03b1)cos(\u03b8)\u2212cos(\u03b1)sin(\u03b8) sin(\u03b8)= \u221a 1\u2212cos(\u03b8)2, sin(\u03b1)= \u221a 1\u2212cos(\u03b1)2 sin(\u03b1\u2212\u03b8)= \u221a 1\u2212cos(\u03b1)2cos(\u03b8)\u2212cos(\u03b1) \u221a 1\u2212cos(\u03b8)2\n(13)\n\u03bb= ||xj ||(cos(\u03b8)\n\u221a 1\u2212cos(\u03b1)2\u2212cos(\u03b1) \u221a 1\u2212cos(\u03b8)2)\n||xi|| \u221a 1\u2212cos(\u03b8)2 (14)"
        },
        {
            "heading": "C Results by Language and Domain",
            "text": "We show complete results in this section. Table 8 illustrates that there is no significant difference between using the full test set and using a subset of texts from the test set (the subset that we used in our experiment).\nTable 9 illustrates the cosine similarity between each language and English for five linguistic features. We obtain these features as language vectors from LANG2VEC (Littell et al., 2017).\nTable 10 shows the correlation coefficient and p-value for these linguistic features.\n\u2022 SYN: vectors encode syntactic properties, e.g., if a subject appears before or after a verb.\n\u2022 FAM: vectors encode memberships in language families.\n\u2022 INV: vectors denote the presence or absence of natural classes of sounds.\n\u2022 PHO: vectors encode phonological properties such as the consonant-vowel ratio.\n\u2022 GEO: vectors express orthodromic distances for languages w.r.t. fixed points on the Earth\u2019s surface.\nTable 11 shows the Precision and Recall results across domains and languages for the error analysis of the unsupervised language adaptation.\nTable 12 shows the results for all variants across languages and domains."
        }
    ],
    "title": "Vicinal Risk Minimization for Few-Shot Cross-lingual Transfer in Abusive Language Detection",
    "year": 2023
}