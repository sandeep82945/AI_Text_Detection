{
    "abstractText": "The effectiveness of Chain-of-thought prompting (COT) has been widely recognized, but the underlying mechanisms behind its success, the reason why it just works for a wide range of tasks, remains an open question. To investigate this, we employ a counterfactual prompting approach, systematically manipulating elements of examples used in a few-shot prompt, and testing the consequences on model behavior. This allows us to understand the relative contributions of prompt elements such as symbols (digits, entities) and patterns (equations, sentence structure) on in-context learning. Our experiments with three different large language models (LLMs) reveal several key findings. First, the specific symbols used in the prompt do not significantly impact the model\u2019s performance. However, consistent patterns in examples and specifying text in style frequently found on the web are crucial. Second, our findings suggest that the necessity of accurate few-shot examples depends on their role in communicating task understanding. We identify tasks where inaccurate few-shot examples hurt and, surprisingly, tasks where they improve performance. Additionally, we find that the intermediate steps in COT may not necessarily facilitate learning how to solve a task, but instead efficiently convey task understanding (what) to the model. Furthermore, COT leverages LLMs to fill in missing commonsense information, particularly helping difficult reasoning problems and long-tail questions1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Aman Madaan"
        },
        {
            "affiliations": [],
            "name": "Katherine Hermann"
        },
        {
            "affiliations": [],
            "name": "Amir Yazdanbakhsh"
        }
    ],
    "id": "SP:e187574924ebdf90df53d1d687a31e4e43f0da11",
    "references": [
        {
            "authors": [
                "Aida Amini",
                "Saadia Gabriel",
                "Shanchuan Lin",
                "Rik Koncel-Kedziorski",
                "Yejin Choi",
                "Hannaneh Hajishirzi."
            ],
            "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
            "venue": "ACL.",
            "year": 2019
        },
        {
            "authors": [
                "BIG-bench Collaboration."
            ],
            "title": "Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models",
            "venue": "arXiv preprint arXiv:2206.04615.",
            "year": 2022
        },
        {
            "authors": [
                "Nadia Burkart",
                "Marco F Huber."
            ],
            "title": "A Survey on the Explainability of Supervised Machine Learning",
            "venue": "JAIR.",
            "year": 2021
        },
        {
            "authors": [
                "Morikawa",
                "Alec Radford",
                "Matthew Knight",
                "Miles Brundage",
                "Mira Murati",
                "Katie Mayer",
                "Peter Welinder",
                "Bob McGrew",
                "Dario Amodei",
                "Sam McCandlish",
                "Ilya Sutskever",
                "Wojciech Zaremba"
            ],
            "title": "Evaluating Large Language Models Trained on Code",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Cohen"
            ],
            "title": "A Coefficient of Agreement",
            "year": 1960
        },
        {
            "authors": [
                "Jeremy Ginsberg",
                "Matthew H Mohebbi",
                "Rajan S Patel",
                "Lynnette Brammer",
                "Mark S Smolinski",
                "Larry Brilliant."
            ],
            "title": "Detecting Influenza Epidemics using Search Engine Query Data",
            "venue": "Nature, 457(7232):1012\u2013 1014.",
            "year": 2009
        },
        {
            "authors": [
                "Yash Goyal",
                "Ziyan Wu",
                "Jan Ernst",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee."
            ],
            "title": "Counterfactual Visual Explanations",
            "venue": "ICML.",
            "year": 2019
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The Curious Case of Neural Text Degeneration",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Alon Jacovi",
                "Yoav Goldberg"
            ],
            "title": "Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness",
            "year": 2020
        },
        {
            "authors": [
                "Sarthak Jain",
                "Byron C. Wallace."
            ],
            "title": "Attention is not Explanation",
            "venue": "NAACL.",
            "year": 2019
        },
        {
            "authors": [
                "gory Thorson",
                "Bo Tian",
                "Horia Toma",
                "Erick Tuttle",
                "Vijay Vasudevan",
                "Richard Walter",
                "Walter Wang",
                "Eric Wilcox",
                "Doe Hyun Yoon"
            ],
            "title": "In-Datacenter Performance Analysis of a Tensor Processing Unit",
            "venue": "In ISCA",
            "year": 2017
        },
        {
            "authors": [
                "Michael Kaminski."
            ],
            "title": "Yoda-Speak: A Study of Yoda\u2019s Speaking Pattern and Their Frequencies",
            "venue": "The Secret History of Star Wars.",
            "year": 2011
        },
        {
            "authors": [
                "Vaughan."
            ],
            "title": "Interpreting Interpretability: Understanding Data Scientists\u2019 Use of Interpretability Tools for Machine Learning",
            "venue": "CHI.",
            "year": 2020
        },
        {
            "authors": [
                "Junyeob Kim",
                "Hyuhng Joon Kim",
                "Hyunsoo Cho",
                "Hwiyeol Jo",
                "Sang-Woo Lee",
                "Sang-goo Lee",
                "Kang Min Yoo",
                "Taeuk Kim."
            ],
            "title": "GroundTruth Labels Matter: A Deeper Look into Input-Label Demonstrations",
            "venue": "arXiv preprint arXiv:2205.12685.",
            "year": 2022
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large Language Models are Zero-Shot Reasoners",
            "venue": "arXiv preprint arXiv:2205.11916.",
            "year": 2022
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson."
            ],
            "title": "SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing",
            "venue": "EMNLP-Demo Track.",
            "year": 2018
        },
        {
            "authors": [
                "Thibault Laugel",
                "Marie-Jeanne Lesot",
                "Christophe Marsala",
                "Xavier Renard",
                "Marcin Detyniecki."
            ],
            "title": "The Dangers of Post-hoc Interpretability: Unjustified Counterfactual Explanations",
            "venue": "IJCAI.",
            "year": 2019
        },
        {
            "authors": [
                "Teven Le Scao",
                "Alexander M Rush"
            ],
            "title": "How Many Data Points is a Prompt Worth? In NAACL",
            "year": 2021
        },
        {
            "authors": [
                "Wang Ling",
                "Dani Yogatama",
                "Chris Dyer",
                "Phil Blunsom."
            ],
            "title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
            "venue": "arXiv preprint arXiv:1705.04146.",
            "year": 2017
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "2021a. What Makes Good In-Context Examples for GPT-3? arXiv:2101.06804 [cs",
            "year": 2021
        },
        {
            "authors": [
                "Pengfei Liu",
                "Jinlan Fu",
                "Yang Xiao",
                "Weizhe Yuan",
                "Shuaichen Chang",
                "Junqi Dai",
                "Yixin Liu",
                "Zihuiwen Ye",
                "Graham Neubig."
            ],
            "title": "ExplainaBoard: An Explainable Leaderboard for NLP",
            "venue": "IJCNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
            "venue": "arXiv preprint arXiv:2107.13586.",
            "year": 2021
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming FewShot Prompt Order Sensitivity",
            "venue": "ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Scott M. Lundberg",
                "Su-In Lee."
            ],
            "title": "A Unified Approach to Interpreting Model Predictions",
            "venue": "NeurIPS.",
            "year": 2017
        },
        {
            "authors": [
                "R Thomas McCoy",
                "Ellie Pavlick",
                "Tal Linzen."
            ],
            "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
            "venue": "arXiv preprint arXiv:1902.01007.",
            "year": 2019
        },
        {
            "authors": [
                "Quinn McNemar."
            ],
            "title": "Note on the Sampling Error of the Difference between Correlated Proportions or Percentages",
            "venue": "Psychometrika.",
            "year": 1947
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? arXiv preprint arXiv:2202.12837",
            "year": 2022
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Yejin Choi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Reframing Instructional Prompts to GPTk\u2019s Language",
            "venue": "arXiv preprint arXiv:2109.07830.",
            "year": 2021
        },
        {
            "authors": [
                "Ramaravind K Mothilal",
                "Amit Sharma",
                "Chenhao Tan."
            ],
            "title": "Explaining Machine Learning Classifiers Through Diverse Counterfactual Explanations",
            "venue": "FAT \u030a.",
            "year": 2020
        },
        {
            "authors": [
                "Guoshun Nan",
                "Jiaqi Zeng",
                "Rui Qiao",
                "Zhijiang Guo",
                "Wei Lu."
            ],
            "title": "Uncovering Main Causalities for Long-tailed Information Extraction",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Maxwell Nye",
                "Anders Johan Andreassen",
                "Guy Gur-Ari",
                "Henryk Michalewski",
                "Jacob Austin",
                "David Bieber",
                "David Dohan",
                "Aitor Lewkowycz",
                "Maarten Bosma",
                "David Luan",
                "Charles Sutton",
                "Augustus Odena"
            ],
            "title": "Show your Work: Scratchpads for Interme",
            "year": 2021
        },
        {
            "authors": [
                "ter Welinder",
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training Language Models to Follow Instructions with Human Feedback",
            "venue": "arXiv preprint arXiv:2203.02155",
            "year": 2022
        },
        {
            "authors": [
                "Arkil Patel",
                "Satwik Bhattamishra",
                "Navin Goyal"
            ],
            "title": "Are NLP Models Really Able to Solve Simple Math Word Problems? arXiv preprint arXiv:2103.07191",
            "year": 2021
        },
        {
            "authors": [
                "Gabriel Poesia",
                "Alex Polozov",
                "Vu Le",
                "Ashish Tiwari",
                "Gustavo Soares",
                "Christopher Meek",
                "Sumit Gulwani."
            ],
            "title": "Synchromesh: Reliable Code Generation from Pre-trained Language Models",
            "venue": "ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "Rafael Poyiadzi",
                "Kacper Sokol",
                "Raul Santos-Rodriguez",
                "Tijl De Bie",
                "Peter Flach."
            ],
            "title": "FACE: Feasible and Actionable Counterfactual Explanations",
            "venue": "AAAI.",
            "year": 2020
        },
        {
            "authors": [
                "Danish Pruthi",
                "Mansi Gupta",
                "Bhuwan Dhingra",
                "Graham Neubig",
                "Zachary C. Lipton."
            ],
            "title": "Learning to Deceive with Attention-Based Explanations",
            "venue": "ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Geoffrey K. Pullum"
            ],
            "title": "YODA\u2019S Syntax the Tribune Analyzes; Supply more Details I Will! YODA\u2019S Syntax the Tribune Analyzes; Supply more Details I Will! Accessed: 2022-08-15",
            "year": 2005
        },
        {
            "authors": [
                "Yasaman Razeghi",
                "Robert L Logan IV",
                "Matt Gardner",
                "Sameer Singh."
            ],
            "title": "Impact of Pretraining Term Frequencies on Few-shot Reasoning",
            "venue": "arXiv preprint arXiv:2202.07206.",
            "year": 2022
        },
        {
            "authors": [
                "Laria Reynolds",
                "Kyle McDonell."
            ],
            "title": "Prompt Programming for Large Language Models: Beyond the Few-shot Paradigm",
            "venue": "Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems.",
            "year": 2021
        },
        {
            "authors": [
                "Marco T\u00falio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin."
            ],
            "title": "Why Should I Trust You?\u201d: Explaining the Predictions of Any Classifier",
            "venue": "SIGKDD.",
            "year": 2016
        },
        {
            "authors": [
                "Ohad Rubin",
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Learning to Retrieve Prompts for In-context Learning",
            "venue": "arXiv preprint arXiv:2112.08633.",
            "year": 2021
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176B-Parameter Open-Access",
            "year": 2022
        },
        {
            "authors": [
                "Dylan Slack",
                "Anna Hilgard",
                "Himabindu Lakkaraju",
                "Sameer Singh."
            ],
            "title": "Counterfactual Explanations can be Manipulated",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Ilia Stepin",
                "Jose M Alonso",
                "Alejandro Catala",
                "Mart\u00edn Pereira-Fari\u00f1a."
            ],
            "title": "A Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Artificial Intelligence",
            "venue": "IEEE Access.",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is All you Need",
            "venue": "NeurIPS.",
            "year": 2017
        },
        {
            "authors": [
                "Sahil Verma",
                "John Dickerson",
                "Keegan Hines."
            ],
            "title": "Counterfactual Explanations for Machine Learning: A Review",
            "venue": "arXiv preprint arXiv:2010.10596.",
            "year": 2020
        },
        {
            "authors": [
                "Boshi Wang",
                "Sewon Min",
                "Xiang Deng",
                "Jiaming Shen",
                "You Wu",
                "Luke Zettlemoyer",
                "Huan Sun."
            ],
            "title": "Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters",
            "venue": "arXiv preprint arXiv:2212.10001.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Yuval Pinter."
            ],
            "title": "Attention is not not Explanation",
            "venue": "EMNLP-IJCNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Sang Michael Xie",
                "Aditi Raghunathan",
                "Percy Liang",
                "Tengyu Ma."
            ],
            "title": "An Explanation of In-context Learning as Implicit Bayesian Inference",
            "venue": "ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "Xi Ye",
                "Srinivasan Iyer",
                "Asli Celikyilmaz",
                "Ves Stoyanov",
                "Greg Durrett",
                "Ramakanth Pasunuru."
            ],
            "title": "Complementary Explanations for Effective In-Context Learning",
            "venue": "arXiv preprint arXiv:2211.13892.",
            "year": 2022
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Chi."
            ],
            "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
            "venue": "arXiv preprint",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) have demonstrated remarkable performance in various complex tasks using a small number of examples\u2014a paradigm known as few-shot learning (Brown et al., 2020;\n\u02da Equal Contributions. Work done when Aman was a student researcher at Google Research, Brain Team (now Google DeepMind).\n1Code and data available at https://github.com/ reasoning-machines/prompt-lib/\nChowdhery et al., 2022). This progress has been significantly boosted by chain-of-thought prompting (COT) and its variants (Wei et al., 2022b; Kojima et al., 2022; Zhou et al., 2022), which have been proven to further enhance the LLMs\u2019 capabilities (Ling et al., 2017; Nye et al., 2021; Cobbe et al., 2021; Patel et al., 2021; BIG-bench Collaboration, 2022).\nDespite its demonstrated effectiveness, the underlying mechanisms behind COT still need to be fully understood. A common explanation draws a parallel with human thinking, in which individuals often reflect on a problem before arriving at a solution (Ling et al., 2017; Wei et al., 2022b,a). While this analogy is intuitive, it does not fully explain the reasons for COT\u2019s success, including when and how the COT mechanism operates. Since LLMs are trained to predict the next token in a given context, there might be a more systematic explanation behind the successes and failures of COT. This study aims to explore the mechanism behind COT, providing insights into its operation.\nOur approach involves modifying different components of the examples utilized in the few-shot prompt, and assessing the impact of these changes on the final performance (Figure 1). Specifically, we pinpoint the key elements of an example in fewshot prompting as: Symbols (e.g., digits, dates) and Patterns (e.g., equations, templates, sentence structure). We then apply counterfactual prompting (Goyal et al., 2019) where all components except one are held constant\u2014 for instance, replacing symbols like numbers with Greek letters. The effect of each component is then assessed by comparing the performance differences between prompt variations. Our experimental approach spans four diverse reasoning tasks and is implemented across three major language models\u2014PaLM, GPT-3, and CODEX, yielding several surprising findings:\n1 Our study reveals that the specific symbols employed in the prompt have minimal impact on the\nBa se\nP ro\nm pt\nQuestion (Q) If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? Thought (T) There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5\nAnswer (A) The answer is 5 cars.\nQuestion (Q) If there are \u0251 cars in the parking lot and \u03b2 more cars arrive, how many cars are in the parking lot? Thought (T) There are originally \u0251 cars. \u03b2 more cars arrive. \u0251 + \u03b2 = !\nAnswer (A) The answer is ! cars.\nQuestion (Q) If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? Thought (T) There are originally 3 cars. 2 more cars arrive. 3 + 2 = 7\nAnswer (A)\nThe answer is 7 cars.\nQuestion (Q) If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? Thought (T) Originally 3 cars, there are. 2 more cars arrive. 3 + 2 = 5\nAnswer (A)\nThe answer is 5 cars.\nC ou\nnt er\nfa ct\nua l P\nro m\npt s\nWhat if we don\u2019t have actual numbers? What if the prompt is misleading? What if the syntax of thought is less frequent?\n<latexit sha1_base64=\"yEDpwFZ2LweTXSXVOiXEzcdS7Cs=\">AAACBHicZVC7SgNBFJ31GddX1NJmMQSswq6I2ohBG8sI5gHJEmZnZ5MxszPLzF0hLGmtbbW1tRNbWz9B/AT/wskDScyBC4dz7mHmniDhTIPrflsLi0vLK6u5NXt9Y3NrO7+zW9MyVYRWieRSNQKsKWeCVoEBp41EURwHnNaD3tXQr99TpZkUt9BPqB/jjmARIxiMVGuRUIJu5wtuyR3BmSfehBQuPu3z5OXLrrTzP61QkjSmAgjHWjc9NwE/wwoY4XRgt1JNE0x6uEObhgocU+1no98OnKJRQieSyowAZ6TaxalIhmOt+3FgVmMMXT1nDtU/c9oLgpCJzmAm0EwhOvMzJpIUqCDj96OUOyCdYSNOyBQlwPuGYKKYOcEhXawwAdObbbrx/jcxT2pHJe+kdHzjFsqXaIwc2kcH6BB56BSV0TWqoCoi6A49oif0bD1Yr9ab9T5eXbAmmT00A+vjFxtem0s=</latexit>\u00b7 \u00b7 \u00b7 <latexit sha1_base64=\"UcH4I5LQz1ArN2Sacd9vBRo6CW8=\">AAACEHicZVDLSsNAFJ3UV42vqEs3wbbgqiRF1GXRjcsK9gFtCJPJJB06mYSZSSWE/IRrt/oN7sStf+An+BdO2yCtPXDhcM49zNzjJZQIaVnfWmVjc2t7p7qr7+0fHB4Zxyc9Eacc4S6KacwHHhSYEoa7kkiKBwnHMPIo7nuTu5nfn2IuSMweZZZgJ4IhIwFBUCrJNYx65uZPPGbhyE2gLOquUbOa1hzmOrFLUgMlOq7xM/JjlEaYSUShEEPbSqSTQy4JorjQR6nACUQTGOKhogxGWDj5/OeF2VCKbwYxV8OkOVf1xlIkh5EQWeSp1QjKsVgzZ+qfuex5nk9YWKwEhqkMbpycsCSVmKHF+0FKTRmbs3ZMn3CMJM0UgYgTdYKJxpBDJFWHuurG/t/EOum1mvZV8/KhVWvfli1VwRk4BxfABtegDe5BB3QBAlPwAl7Bm/asvWsf2uditaKVmVOwAu3rF5/1nG0=</latexit>ywrong pat <latexit sha1_base64=\"KHh5kZ7DA5DeX0HkE008Z6dNZ7U=\">AAACDHicZZDLSgMxGIUz9VbrrerSTbAtuCozRdRl0Y3LCvYC7ViSTKYNzWSGJCMMw7yCa7f6DO7Ere/gI/gWZtpBrD0QOJzz/yT5cMSZ0rb9ZZXW1jc2t8rblZ3dvf2D6uFRT4WxJLRLQh7KAUaKciZoVzPN6SCSFAWY0z6e3eR9/5FKxUJxr5OIugGaCOYzgrSJHurJOEVYaYmIzurjas1u2nPBVeMUpgYKdcbV75EXkjigQhOOlBo6dqTdFEnNCKdZZRQrGiEyQxM6NFaggCo3nb86gw2TeNAPpTlCw3laafxZSVGgVBJgMxogPVUrZZ7+ln87jD0mJtnSwjDW/pWbMhHFmgqyuN+POdQhzMlAj0lKNE+MQUQy8wVIpignY/hVDBvnP4lV02s1nYvm+V2r1r4uKJXBCTgFZ8ABl6ANbkEHdAEBEjyDF/BqPVlv1rv1sRgtWcXOMViS9fkDclubTw==</latexit>yabstract <latexit sha1_base64=\"nlT5jYJYRdSsIcX/bCb5HPNaXOE=\">AAACEHicZVDLSsNAFJ3UV62vqEs3wbbgqiRF1GXRjcsK9gFtCZPJpB06mYSZm2II+QnXbvUb3Ilb/8BP8C+cPpDWHrhwOOceZu7xYs4U2Pa3UdjY3NreKe6W9vYPDo/M45O2ihJJaItEPJJdDyvKmaAtYMBpN5YUhx6nHW98N/U7EyoVi8QjpDEdhHgoWMAIBi25pllJ3QzoE/TdNPJxXnHNsl2zZ7DWibMgZbRA0zV/+n5EkpAKIBwr1XPsGAYZlsAIp3mpnygaYzLGQ9rTVOCQqkE2+3luVbXiW0Ek9QiwZmqpuhTJcKhUGnp6NcQwUmvmVP0zlz3P85kY5iuBXgLBzSBjIk6ACjJ/P0i4BZE1bcfymaQEeKoJJpLpEywywhIT0B2WdDfO/ybWSbtec65qlw/1cuN20VIRnaFzdIEcdI0a6B41UQsRNEEv6BW9Gc/Gu/FhfM5XC8Yic4pWYHz9Ap/5nG0=</latexit>ytext yoda <latexit sha1_base64=\"93EgpTtG40gCfIXr+wKZy9BoEq0=\">AAACAXicZVDLSsNAFL3xWeOr6tJNsC24KkkRdVl047KifUAbymQySYdOJmFmIoTQlWu3+g3uxK1f4if4F07bIK09cOFwzj3M3OMljEpl29/G2vrG5tZ2acfc3ds/OCwfHXdknApM2jhmseh5SBJGOWkrqhjpJYKgyGOk641vp373iQhJY/6osoS4EQo5DShGSksP1aw6LFfsuj2DtUqcglSgQGtY/hn4MU4jwhVmSMq+YyfKzZFQFDMyMQepJAnCYxSSvqYcRUS6+eyrE6umFd8KYqGHK2ummrWFSI4iKbPI06sRUiO5Yk7VP3PR8zyf8nCyFOinKrh2c8qTVBGO5+8HKbNUbE3rsHwqCFYs0wRhQfUJFh4hgbDSpZm6G+d/E6uk06g7l/WL+0aleVO0VIJTOINzcOAKmnAHLWgDhhBe4BXejGfj3fgwPuera0aROYElGF+/OaSWNg==</latexit>y"
        },
        {
            "heading": "6 apples and gave half of them away. How many",
            "text": ""
        },
        {
            "heading": "2 Counterfactual Prompting for COT",
            "text": "MATHEMATICAL Solve a grade-school level math reasoning problems Question: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? Thought: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. Symbols: Numbers: 5, 4, 9 Patterns: Equations: 5 + 4 = 9. The equations typically appear at the end of the thought, and are almost always involved in generating the final answer.\nCOMMONSENSE (SPORTS) Verify the accuracy of a statement linking an athlete with a sport. Question: Is the following sentence plausible? \"Jamal Murray was perfect from the line.\"\u2019 Thought: Jamal Murray is a basketball player. Being perfect from the line is part of basketball. Symbols: Person and activity: Jamal Murray, Being perfect from the line Patterns: Consistent sentence structure PERSON belongs to SPORT. ACTIVITY belongs to SPORT, where belongs to is a phrase that connects a sports personality with an activity. The answer is yes if both the person and the activity are associated with the same sport.\nCOMMONSENSE (DATE) Reason about dates Question: It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY? Thought: Today is 04/19/1969. 24 hours later is one day after today, which would be 04/20/1969. The answer is 04/20/1969. Symbols: Dates: 04/19/1969, 04/20/1969 Patterns: Reasoning flows in two steps: initial calculation (Today is 04/19/1969...), followed by generation of output (The answer is...)\nSYMBOLIC (SORTING) Sort integers between 1-9 Question: 3, 1, 2, 7, 8, 5, 6, 9, 4 Thought: 1 < 2 < ... < 9 Symbols: Numbers: 2, 4, 9 Patterns: Smaller number < larger number (1 < 2)\nQuestion / Thought Prompt Type Solve Rate MATHEMATICAL (DIRECT = 10.11%, COT = 27.37%)\nThought: Shawn started with \u03b1 toys. If he got \u03b2 toys each from his mom and dad, then that is \u03bb more toys. \u03b1 + \u03bb = \u03c0. Csymb_absppq (Table 25) 25.70% Thought: Shawn started with 5.5 toys. If he got 2.5 toys each from his mom and dad, then that is 5 more toys. 5.5 + 5 = 10.5. Csymb_oodppq (Table 30) 28.20%\nCOMMONSENSE (SPORTS) (DIRECT = 71.08%, COT = 93.67%) Thought: Jamal Murray is a basketball player. Being ACTIVITY is part of basketball. Csymb_absppq (Table 28) 92.11% Thought: Adair Foster is a basketball player. Juggling the paper cups is part of basketball. Csymb_oodppq (Table 32) 79.72%\nCOMMONSENSE (DATE) (DIRECT = 31.61%, COT = 45.18%) Thought: Today is DATE. 24 hours later is one day after today, which would be DATE. Csymb_absppq (Table 24) 37.41% Thought: Today is 04/30/3069. 24 hours later is one day after today, which would be 04/31/3069. Csymb_oodppq (Table 31) 44.50%\nSYMBOLIC (SORTING) (DIRECT = 46.0%, COT = 60.6%) Thought: \u03c2 < \u03d5 < \u03b3 < \u03b4 < \u03b6 < \u03c7 < \u03f5 < \u03c0 < \u03c5 Csymb_absppq (Table 26) 61.8% Thought: 11 \u0103 23 \u0103 34 \u0103 48 \u0103 56 \u0103 63 \u0103 72 \u0103 85 \u0103 95 Csymb_oodppq (Table 33) 80.0%"
        },
        {
            "heading": "3.2 Results",
            "text": ""
        },
        {
            "heading": "3.1 Method",
            "text": ""
        },
        {
            "heading": "3 Role of Symbols",
            "text": "Q : There are 5 trees the grove . Grove workers will plant trees in grove today . After they are done , there will be 2 1 trees . How many trees did the grove workers plant today ? A : There are 5 trees originally . Then there were 2 1 trees after more were planted . So there must have been 2 - 5 = . The answer is 6 . Q : If there are 3 cars in the parking lot and 2 more cars arrive , how many cars are in the parking lot ? A : There are originally 3 cars . 2 more cars arrive . 3 + 2 = 5 . The answer is 5 . Q : Leah had 3 2 chocolates and her sister had 4 2 . If they at e 3 , how many pieces do they have left total ? A : Originally , Leah had 3 2 chocolates . Her sister had 4 2 . So total they had 3 2 + 4 2 = 7 4 . After eating 3 5 they had 4 - 3 5 = 3 9 . The answer is 3 9 . Q : Jason had 2 0 lollipops . He gave Denny some lollipops . Now Jason has 1 2 lollipops . How many lollipops did Jason give to Denny ? A : Jason started with 2 lollipops . Then he had 2 after giving some to Denny . So he gave Denny 2 0 - 1 2 = 8 . The answer is 8 . Q : Shawn has five toys . For Christmas , he got two toys each from his mom and dad . How many toys does he have now ? A : Shawn started with 5 toys . If he got 2 toys each from his mom and dad , then that is 4 more toys . 5 + 4 = 9 . The answer is 9 . Q : There were nine computers in the server room . Five more computers were installed each day , from monday to thursday . How many computers are now in the server room ? A : There were originally 9 computers . For each of 4 days , 5 more computers were added . So 5 * 4 = 2 0 computers were added . 9 + 2 0 is 2 9 . The answer is 2 9 . Q : Michael had 5 8 golf balls . On tuesday , he lost 2 3 golf balls . On wednesday , he lost 2 more . How many golf balls did he have at the end of wednesday ? A : Michael started with 5 8 golf balls . After losing 2 3 on tuesday , he had 5 8 - 2 3 = 3 5 . After losing 2 more , he had 3 5 - 2 = 3 3 golf balls . The answer is 3 3 . Q : Olivia has $ 2 3 . She bought five bagels for $ 3 each . How much money does she have left ? A : Olivia had 2 3 dollars . 5 bagels for 3 dollars each will be 5 x 3 = 1 5 dollars . So she has 2 3 - 1 5 dollars left . 2 3 - 1 5 is 8 . The answer is 8 .\n(a) Vanilla COT Prompt p.\nQ : There are \u03b1 trees in the grove . Grove workers will plant trees in the grove today . After they are done , there will be \u03b2 one trees . How many trees did the grove workers plant today ? A : There are \u03b1 trees originally . Then there were \u03b2 one trees after some more were planted . So there must have been \u03b2 one - \u03b1 = \u03bb . The answer is \u03bb . Q : If there are \u03b1 cars in the parking lot and \u03b2 more cars arrive , how many cars are the parking lot ? A : There are originally \u03b1 cars . \u03b2 more cars arrive . \u03b1 + \u03b2 = \u03bb . The answer is \u03bb . Q : Leah had \u03b1 chocolates and her sister had \u03b2 . If they at e \u03bb , how many pieces do they have left in total ? A : Originally , Leah had \u03b1 chocolates . Her sister had \u03b2 . So in total they had \u03b1 + \u03b2 = \u03c0 . After eating \u03bb , they had \u03c0 - \u03bb = \u03bc . The answer is \u03bc . Q : Jason had \u03b1 lollipops . He gave Denny some . Now Jason has \u03b2 lollipops . How many lollipops did Jason give to Denny ? A : Jason started with \u03b1 lollipops . Then he had \u03b2 after giving some to Denny . So he gave Denny \u03b1 - \u03b2 = \u03bb . The answer is \u03bb . Q : Shawn has \u03b1 toys . For Christmas , he got \u03b2 toys each from his mom and dad . How many toys does he have now ? A : Shawn started with \u03b1 toys If he got \u03b2 toys each from his mom and dad , then that is \u03bb more toys . \u03b1 + \u03bb = \u03c0 . The answer is \u03c0 . Q : There were \u03b1 computers in the server room . \u03b2 more computers were installed each day , from monday to thursday . How many computers are now in the server room ? A : There were originally \u03b1 computers . For each of four days , \u03b2 more computers were added . So \u03b2 * four = \u03bb computers were added . \u03b1 + \u03bb is \u03c0 . The answer is \u03c0 . Q : Michael had \u03b1 golf balls . On tuesday , he lost \u03b2 golf balls . On wednesday , he lost \u03bb more . How many golf balls did he have at the end of wednesday ? A : Michael started with \u03b1 golf balls . After losing \u03b2 on tuesday , he had \u03b1 - \u03b2 = \u03c0 . After losing \u03bb more , he had \u03c0 - \u03bb = \u03bc golf balls . The answer is \u03bc . Q : Olivia has $ \u03b1 . She bought five bagels for $ \u03b2 each . How much money does she have left ? A : Olivia had \u03b1 dollars . 5 bagels for \u03b2 dollars each will be 5 x \u03b2 = \u03bb dollars . So she has \u03b1 - \u03bb dollars left . \u03b1 - \u03bb is \u03c0 . The answer is \u03c0 .\n(b) Abstract Symbols Csymb_absppq."
        },
        {
            "heading": "4.1 Method",
            "text": ""
        },
        {
            "heading": "4 Role of Patterns",
            "text": "while keeping the input questions unaltered (i.e., no changes are made to the task).\nInconsistent pattern. In Cpat_inconsistentppq we assess the sensitivity of model performance to the usage of inconsistent patterns. For GSM8K, we construct Cpat_inconsistentppq by exclusively removing everything except equations. However, in SPORTS, patterns are implicit in the sentence structure (person is a sport1 player, activity is part of sport2\u201d), making it challenging to create a Cpat_inconsistentppq scenario. To overcome this, we devise a prompt that incorporates multiple variations of thought. For example, in some cases, we phrase the thought by listing activity first: \u201c activity is part of sport2 , person is a sport1 player. \u201d This methodology effectively eliminates the model\u2019s reliance on specific patterns, essentially creating a virtual equivalent of the Cpat_inconsistentppq setup. We apply similar techniques to DATE and SORTING.\nPattern-only. In Cpat_onlyppq prompts, we modify the thoughts by preserving solely the essential information conveyed by the patterns. For example, in GSM-8K, the pattern-only prompts exclusively contain mathematical equations. In SPORTS, the pattern strives to establish a connection between a person and an activity, based on whether they involve the same sport (in affirmative cases) or different sports (in negative cases). The Cpat_onlyppq prompts retain this information by distilling the thought to \u201cboth are part of the same/different sport\u201d. Similarly, in DATE, we construct thoughts that retain the calculation and answer generation. For example, the statement the date today is 04/19/1969, there are 24 hours in a day is transformed into today = 04/19/1969, 24 hours = day, where the second expression only provides the answer equation.\nWrong pattern. In Cpat_wrongppq, we examine prompts that include misleading or incorrect information while following the standard pattern. For instance, we use incorrect equations for GSM-8K, erroneous date calculations for DATE, and improper ordering for SORTING. Similarly, for SPORTS, we associate a sportsperson and activity with a randomly chosen sport, instead of the correct one. The goal of this experiment is to evaluate the role factual information in the prompt plays in model\u2019s ability to generate correct responses."
        },
        {
            "heading": "4.2 Results",
            "text": "Inconsistent pattern. The use of inconsistent patterns in the Cpat_inconsistentppq method had a noticeable impact on performance. For instance, in mathematical tasks, the solve rate was 21.46% (Table 3), significantly lower than the 27.37% achieved by COT. In SPORTS tasks, the solve rate was 79.01%, as compared to COT\u2019s 93.67%. Despite being able to derive relevant facts such as \u201cNick Foles is a football player\u201d and \u201cthe puck is a part of ice hockey,\u201d the model failed to utilize these facts to produce correct answers. Pattern-only. Results from the Cpat_onlyppq method demonstrated that preserving only the patterns in prompts led to a reduced performance. For mathematical tasks, the solve rate was only 10.01% (Table 3), significantly lower than the 27.37% solve rate of COT. Similarly, in SPORTS tasks, the solve rate achieved was 74.13%, as opposed to the 93.67% solve rate of COT. This underscores the importance of the contextual information that accompanies the patterns for optimal performance.\nWrong pattern. Introducing incorrect patterns in the Cpat_wrongppq method led to varying impacts on performance depending on the task. In mathematical tasks, Cpat_wrongppq achieved a solve rate of 24.39%, nearly the same as the 27.37% solve rate of COT (Table 3). Likewise, for DATE tasks, the solve rate achieved by Cpat_wrongppq was 44.84%, closely comparable to COT\u2019s 45.18%. However, for SPORTS tasks, the solve rate sharply declined to 46.02%, which was considerably lower than the 93.67% solve rate of COT. These results indicate that incorrect patterns can greatly skew the model\u2019s understanding, especially for tasks like SPORTS where correct associations between the subject and activity are crucial."
        },
        {
            "heading": "5 Additional Surface-level Manipulations",
            "text": "In addition to symbols and patterns, we delve into surface-level manipulations of text. These manipulations encompass changes to tokens that do not directly contribute to task-specific semantics but may nonetheless impact a language model\u2019s understanding and performance. In this section, we scrutinize the effects of these surface-level alterations in our prompts and examine their influence on the outcomes. Text with altered grammatical style. First, we examine the impact of Yodish, a syntactically valid\nbut non-standard style of English, on the model\u2019s performance (Kaminski, 2011; Pullum, 2005). In Yodish, the XSV sentence structure is prevalent, where X is a phrase that complements the verb V, and S is the subject. For example, the sentence \u201cBryce Harper is a baseball player\u201d would be rearranged in Yodish as \u201cA baseball player, Bryce Harper is\u201d. This style presents a greater challenge for the model, as it is less frequently encountered in typical training data. This makes it a valuable test case for evaluating how textual structure influences model performance. We experiment with three variations of prompts: (a) Ctext_yodathoughtsppq: thoughts, (b) Ctext_yodaquestionsppq: questions, and\n(c) Ctext_yodappq: both questions and thoughts in Yodish. As shown in Table 4, this style has varying effects on model performance, from moderate (GSM-8K) to significantly negative (SPORTS and DATE). For example, in SPORTS, the use of Yodish encourages the model to generate the sport (the object) at the start of the sentence. This structure, while grammatically correct, forces the model to process information in a manner closer to direct prompting, as the model has to output the answer before the reasoning process.\nUsing CODEX as the base model, we performed additional experiments with other forms of less common patterns in standard English grammar: (a) passive voice and (b) nested clause. Specifically, we modified the prompts to rewrite the original thought (e.g., Shawn started with 5 toys. If he got"
        },
        {
            "heading": "2 toys each from his mom and dad, then that is 4",
            "text": "more toys. 5 + 4 = 9.) in passive voice (5 toys were initially had by Shawn. 4 more toys were received by him, 2 each from his mom and dad. 5 + 4 is 9) and using nested clauses (Given that Shawn had 5 toys, and considering he received 4 more from his parents, the total is 9 toys).\nBoth these variations led to a drop in performance. For passive voice the solve rates for GSM-8K, SPORTS, and DATE dropped to 53.0% (- 12.6%), 90.3% (-8.0%), and 65.9% (-3.3%) respec-\ntively. For thoughts written with nested clauses, the solve rates decreased to 55.5% (-10.1%) for GSM-8K, 90.3% (-8.0%) for SPORTS, and 66.4% (-2.8%) for DATE. These results indicate that even within standard English grammar using less common variations cause discernible drops in task solve rates. Shuffled and random thoughts. Finally, we test the impact of altering the order or context of the text. We experiment with random thoughts (Ctext_randppq), where each thought is replaced by a semantically correct but randomly chosen thought from another example, and shuffled thoughts, where we either shuffle the words within a thought (Ctext_inter_shufppq) or across thoughts (Ctext_intra_shufppq). These experiments were designed to assess the model\u2019s dependency on the logical sequence and context of the text. The considerable decrease in performance indicates the model\u2019s reliance on coherent and contextually relevant information. The performance decrease was significant across all models, suggesting a universal dependency on coherent and contextually appropriate text among these models."
        },
        {
            "heading": "6 What makes Chain-of-Thought prompting work?",
            "text": "In this section, we summarize our findings and present some key takeaways.\nTakeaway I. COT helps in reinforcing task understanding.\nSymbols and patterns can be significantly altered as long as they communicate task intent (what has to be done). In some cases (such as SORTING), deviating from standard patterns may be beneficial if they more effectively communicate the task. While few-shot prompting is often called in-context learning, our findings indicate that prompts serve more as a means of reminding the model of the task that needs to be solved.\nTakeaway II. COT helps in eliciting commonsense knowledge.\nExamples in a COT prompt share a key property: they help to fill in the information in the prompt. For instance, in \u27a5Q3 of Appendix-Table 16, the model with COT infuses commonsense knowledge\nTask PaLM62B GPT-3 CODEX PaLM540B\nGSM (Table 73) +6.2% +11.7% -4.7% +5.7%\nDATE (Table 74) +14.8% +12.8% +1.1% +5.7%\nSPORTS (Table 75) +1% +16.6% +0.2% +2.1%\nSORTING (table 23) +9% +273% +268% +24.5%"
        },
        {
            "heading": "2 cats have 4 legs each . . . 10 birds have 2 legs",
            "text": "named CCOT, that retain the essential information while removing unnecessary tokens.\nFor GSM-8K, we randomly select questions from the training set whose thoughts are shorter than COT. For SPORTS, a thought such as Jamal Murray is a basketball player. Being perfect from the line is part of basketball was streamlined to Jamal Murray \u00d1 basketball. perfect from the line \u00d1 basketball. Similarly, in Today is 04/19/1969. 24 hours later is one day after today, which would be 04/20/1969 was converted to Today is 04/19/1969. 24 hours (one day) later is 04/20/1969. Table 5 shows that CCOT outperforms COT while using prompts with fewer tokens. The task solve rate of CCOT remains relatively high as we scale the model to the large version, highlighting the efficiency of CCOT. Additionally, we find that CCOT reduces the input and output tokens by 1.39 and 1.58 times, respectively. We provide additional results and links to each prompt in Table 19."
        },
        {
            "heading": "7 Related Work and Discussion",
            "text": "This paper intersects with a growing body of work on prompting and large language model reasoning (Brown et al., 2020; Chowdhery et al., 2022; Scao et al., 2022; Zhang et al., 2022; Dasgupta et al., 2022). Role of accurate few-shot examples. Min et al. (2022) find that label correctness is not crucial for the success of the models, and even random labels might lead to competitive performance. Building on this work, Kim et al. (2022) find that the role of the correctness of the labels might be taskdependent. A concurrent body of work has also explored the reasons behind the effectiveness of chain-of-thought-prompting and shows that even wrong COT prompts can lead to strong performance (Ye et al., 2022; Wang et al., 2022). Our findings complement and concur with the findings of these works and is mutually reinforcing, but go beyond the notion of the accuracy of examples. Specifically, we manipulate various aspects of symbols and patterns (with correctness being one of the aspects) to examine their role in the success of COT. Further, in addition to comparing the final results (outcome), we also focus on the mechanism (attention patterns). While Wang et al. (2022) primarily evaluates the effectiveness of COT for reasoning and question-answering tasks by introducing drastic changes to prompts to illustrate invalid reasoning, our work adopts a broader approach.\nWe introduce counterfactual studies encompassing both subtle and significant modifications to COT. Specifically, we assess COT\u2019s effectiveness under two scenarios: (1) where the reasoning flow remains but with incorrect symbols and patterns, and (2) where the reasoning flow is intentionally disrupted, such as through prompt shuffling or the introduction of random texts.\nIn a related vein, Ye et al. (2022) investigates the effects of incorrect calculations and word omission/masking in COT. Our work extends this by exploring the influence of out-of-distribution (OOD) symbols, inconsistent patterns, exclusive use of symbols or patterns, and varied grammatical styles. Few-shot learning or few-shot reminding? Our results resonate with the work of Reynolds and McDonell (2021); Ouyang et al. (2022), who found that one of the key roles played by the prompt is to remind the model of the underlying task. Finally, Xie et al. (2021) show that in-context learning enables a large model to infer a shared concept between the examples, possibly leading to better task understanding. Our studies on the role of prompt, especially examples where wrong examples lead to better output (e.g., for SORTING), provide further empirical evidence for these findings. Finally, in concurrence with Razeghi et al. (2022)\u2019s finding that pre-training term frequencies partly account for the success of few-shot methods, our experiments on SPORTS shows that COT method is helps difficult questions involving personalities and activities less commonly found on the web."
        },
        {
            "heading": "8 Conclusions",
            "text": "Our study suggests that the underlying mechanisms behind the effectiveness of Chain-of-thought prompting (COT) may be more complex than previously thought. We find that even large substitutions, like replacing the digits in few-shot examples with Greek letters, do not affect model performance. However, simple grammar or word-order changes can have catastrophic effects. These results, along with other findings, suggest that the effectiveness of COT may stem from its ability to efficiently convey task understanding (what) to the LLM. Our results indicate that a combination of consistent, easy-to-mimic patterns (templates) and a strong LLM that can fill missing commonsense is the recipe for effective COT. We hope to use this research to develop better prompting techniques and more robust language models for various tasks.\nLimitations\nThis work investigates mechanisms that enable the effectiveness of chain of thought techniques in large language models. However, this study does not delve into the underlying interactions between the layers or devise theoretical formulations of the models\u2019 reasoning capability, mainly due to the complexity and depth of these models, which hinder faithful probing. Instead, we leverage counterfactual probing, a tractable approach for understanding the behavior of large language models. Limitations of counterfactual prompting. Counterfactual examples can provide valuable insights into the behavior of language models, as they allow for identifying and collecting prompts that are critical for generating respective outputs. However, it is essential to note that relying solely on counterfactual examples can be misleading (Laugel et al., 2019; Slack et al., 2021). In this work, we focus on counterfactual examples that exhibit consistent and systematic performance divergence to better understand the failure modes and strengths of the model. We also analyze attention patterns to supplement our findings. We neither rely on the results that do not exhibit such characteristics, nor reject prompts that pose contradictory observations. We discuss additional limitations of our approach in Section 8. Spurious Correlations While this approach has its advantages, there are limitations to consider. The counterfactual approach assumes that the model\u2019s behavior can be understood by analyzing its output given a specific input. However, there may be uncharted and baffling artifacts that the model could be exploiting (McCoy et al., 2019; Geirhos et al., 2020), leading to potentially misleading observations. For instance, there is a potential for spurious correlations between symbols, patterns, text, and the outcome, which can lead to false conclusions. Our exhaustive empirical study addresses some of these concerns by providing in-depth analysis and methodical measures to ground our hypotheses. Additionally, the discrete and multiplicative nature of language understanding tasks implies that no study can be completely thorough. Limited Task and Dataset Scope This work is also limited to a subset of common tasks and datasets, including math (Cobbe et al., 2021), commonsense reasoning (BIG-bench Collaboration, 2022), and symbolic reasoning. Our conclusions may not apply to other reasoning tasks. Despite these limitations, we hope that this work sheds light on the ability of large language models to solve complex reasoning tasks. Model availability. In our experiments, we use three different language models: PaLM, GPT-3 (text-davinci-002), and CODEX (code-davinci002). While PaLM is not publicly available at the time of submission, the provided source code is compatible with OpenAI API v0.23.0 and can work with any OpenAI model. However, using closed models like PaLM may limit our results\u2019 reproducibility and hinder our findings\u2019 generalizability to other models. Additionally, our results may not be directly comparable to other studies that use different models, as the behavior of models may vary across architectures and training datasets. This limitation should be considered when interpreting the results of our study."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thanks the anonymous reviewers for their useful comments and suggestions. We would like to extend our gratitude towards Kathy Meier-Hellstern, Denny Zhou, Victor Veitch, Saleem Abdulrasool, Shruthi Sukumar, Milad Hashemi, Douglas Eck, Christian Szegedy, Cliff Young, Yiming Yang, and James Laudon for their invaluable feedback and support. We also thank the PaLM team and our extended team at Google Research, Brain Team (now Google DeepMind) who enabled this research and helped us conduct our experiments."
        },
        {
            "heading": "A Reproducing the Results with Publicly Available Models",
            "text": "We take the following steps to enable the reproducibility of our work. Controlling for randomness due to the order of examples. We run each experiment with multiple random seeds to control for randomness because of the order of examples in the prompt. We report the average and standard deviation of the results across all the random seeds. Additionally, we conduct statistical significance tests (McNemar\u2019s test (McNemar, 1947)) to compare the results across different prompts. Finally, we evaluate the agreement in output generated by different models using Cohen\u2019s kappa (\u03ba) metric. Experiments with publicly available models. We experiment with three different language models: PaLM, GPT-3 (text-davinci-002), and CODEX (code-davinci-002). PaLM is not publicly available as of submission time, but the provided source code is compatible with OpenAI API v0.23.0, and can work with any OpenAI models. Finally, CODEX is free to use as of submission time that further helps with the reproducibility of the results.\nAll the prompts are included in the prompt_lib/prompts/ directory in the code repository."
        },
        {
            "heading": "B Details on Studied Reasoning Tasks",
            "text": "In this work, we evaluate counterfactual prompting on the following reasoning tasks: 1. MATHEMATICAL We experiment with\nGSM-8K (Cobbe et al., 2021) (1319 samples). The dataset contains math word problems geared toward an average middle-school curriculum.\n2. COMMONSENSE We use date understanding (DATE, 349 samples) and sports understanding (SPORTS, 980 samples) as representative tasks for commonsense reasoning, both derived from BIG-bench Collaboration (2022).\n3. SYMBOLIC We experiment with sorting (SORTING, 500 samples) a list of singledigit integers. We do not associate explicit instruction (e.g., sort these numbers) with the questions. Instead, we frame the questions as a challenging setup in which the model should\nfigure out the task and the requisite information to solve it."
        },
        {
            "heading": "C Computational Resources and Models",
            "text": "In this work, we neither train any of the PaLM models, nor performs finetuning. We solely perform inference on PaLM variants using TPU v4 (Jouppi et al., 2021, 2017). For PaLM-62B, we use 4\u02c64\u02c64 TPU v4 configuration, whereas, for PaLM-540B we use 4\u02c64\u02c616 mesh configuration. To account for the variation in results caused by the order of examples in the prompt, we conduct each experiment three times, each with different seeds, and report the average task solve rate. Following Wei et al. (2022b), we evaluate each task using accuracy i.e. fraction of examples where the output matched the expected result. Public large language models. We use OpenAI API3 to conduct experiments with GPT-3 (textdavinci-002) and CODEX (code-davinci-002)."
        },
        {
            "heading": "D FAQ",
            "text": ""
        },
        {
            "heading": "Q: Are the definitions of symbols and patterns",
            "text": "universal? A: It is possible that there can be other ways to define symbols, patterns, and text in the context of Chain-of-thought prompting (CoTp). Our characterization of these components is not meant to be universal, and there may be additional properties and perspectives that future research can explore. However, the goal of our study is not to provide a universal definition but to make practical and reasonable distinctions that allow us to manipulate each component individually while keeping the others fixed. This approach allows us to better understand the underlying structure of prompts in different contexts. We acknowledge that the impact of symbols and patterns on CoTp success is taskdependent, and our results provide a diverse set of insights. Finally, while there is not a universal definition of symbols and patterns, for any given task we have aimed to provide a clear and reasonable characterization.\nQ: What is the TLDR? A: The effectiveness of COT in few-shot learning with large language models is due to two factors: it helps reinforce task understanding and fills in the missing information. The traditional notion of in-context learning may need to be reevaluated as the model may be using the few-shot examples to be reminded of the task (Reynolds and McDonell, 2021).\n3https://openai.com/blog/openai-api/, v0.23.0\nTable 6: Examples of \u201cwhat if\u201d questions that we seek to answer in this work.\nQ1. What if we replace all the symbols in the prompt with abstract placeholders, can the required task still be discerned? Q2. What if the examples in the prompt were incorrect, will it affect the correctness of the outputs? Q3. What if we remove all patterns from the input, will COT continue to be effective? Q4. What if the linguistic style of the prompt was different than that of the questions, will it hamper the performance?"
        },
        {
            "heading": "E Extended Background",
            "text": "plement in-context few-shot prompting with Chain Of Thought (COT) method, improving the performance of LLM in solving several reasoning tasks. In particular, COT additionally prefixes each output with a thought, creating triplets xxi, ti, yiy. The \u201cchain of thought\u201d ti describes the intermediate steps and/or results required to derive the output yi from xi. Therefore, the prompt is assembled in the form of p \u201d xx1\u00a8t1\u00a8y1y}xx2\u00a8t2\u00a8y2y}. . .}xxk\u00a8tk\u00a8yky, where \u201c\u00a8\u201d and \u201c}\u201d are indicator symbols. The role of \u00a8 is to separate elements of an example, whereas } indicates the boundary of an example. The intuition behind chain of thought prompting is that catering the outputs/answers with intermediate steps/results present additional in-context information to the model (Ling et al., 2017; Amini et al., 2019; Chen et al., 2021a; Cobbe et al., 2021; Nye et al., 2021). This additional in-context information presumably improves accuracy in solving various reasoning tasks.\nAt inference time, COT appends an unseen question x\u0302 to the prompt p and supplies the extended prompt to a LLM. The model completes the prompt to generate a relevant thought t\u0302 and an answer y\u0302. To assess the performance of LLM, COT only compares the post-processed generated answer with the ground truth. Gauging the correctness of the generated thought t\u0302 is not straightforward because ground truth thoughts are unavailable. Nonetheless, the generated thought can be further analyzed to infer the possible mechanisms, allowing an analogy with the human thought process, with which the model attains the answer.\nCounterfactual explanation. Counterfactual explanations seek to explain the behavior of a model by conducting \u201cwhat if\u201d analysis on examples for which the expected outputs of the model is known (Mothilal et al., 2020; Stepin et al., 2021; Verma et al., 2020; Poyiadzi et al., 2020; Goyal et al., 2019; Feder et al., 2021). Specifically, let px, yq be a tuple where x is the input to a model M that estimates an output distribution pp\u00a8 | xq, and y \u201e pp\u00a8 | xq. Counterfactual explanations utilize variants Cf px, b, aq of the inputs that differ from the original input x in all except one feature f . Here, b and a denote the before and after values of the feature f in x. For instance, consider an image x of a camel with a brown background labeled correctly by a classifier. A counterfactual Cbgpx, brown, greenq example is an identical image with only a different background color, green,\nin this example. By virtue of comparing pp\u00a8 | xq with pp\u00a8 | Cbgpx, brown, greenqq for a sufficiently large sample of images, one may infer certain facts about the classifier, for example its reliance on the background color."
        },
        {
            "heading": "F Extended Related Work and Discussion",
            "text": "Broadly, this paper intersects with a growing body of work on prompting and large language model reasoning (Brown et al., 2020; Chowdhery et al., 2022; Scao et al., 2022; Zhang et al., 2022). Below, we review the most relevant work in these directions. Least to most prompting. Zhou et al. (2022) help the model generate a chain of thought by first asking the model to generate the sub-questions for the given problem. Next, the model is asked to answer the sub-questions, and finally, the sub-questions, along with sub-answers, are combined to generate the final result. This work is closely related to Kojima et al. (2022), the latter distinguished by generating the rationale from a large language model directly. We posit that Zhou et al. (2022) derives its key strengths from its ability to generate useful sub-steps. This resonates with our finding that the key contribution of CoT is the extraction of meaningful sub-steps. Prompt selection. Several works have recently explored the design of the prompt\u2014a process often called \u201cprompt engineering\u201d (Le Scao and Rush, 2021; Liu et al., 2021c). The methods include dynamically creating prompts based on the question (Liu et al., 2021a; Rubin et al., 2021; Poesia et al., 2021), formatting the prompt as a list or questions (Mishra et al., 2021; Rubin et al., 2021), improving order of examples in the prompt (Lu et al., 2022), and providing instructions in the task (Ouyang et al., 2022). Unlike these techniques, COT is relatively robust to minor changes in the prompt design. Thus, the findings of our work might be more generally applicable. Explaining model behavior using counterfactual prompts and attention. As noted by (Jacovi and Goldberg, 2020), an explanation of a deep learning system typically serves two different purposes: i) plausibility, which aims to provide an interpretation of system outputs that is convincing for humans, and ii) faithfulness, which aims to capture the actual reasoning process of a model. Our study requires both and uses different means to achieve them. We utilize counterfactual prompts\nto interpret the system outputs to aid human understanding. This is similar to using posthoc analysis tools (Ribeiro et al., 2016; Lundberg and Lee, 2017; Liu et al., 2021b), which also focus on analyzing outputs without concern for the details of the model. To get a glimpse of the model\u2019s inner workings, we leverage attention (Vaswani et al., 2017), a ubiquitous mechanism in NLP. While the broader question on the utility of attention for posthoc analysis is still open (Jain and Wallace, 2019; Pruthi et al., 2020), there is some evidence to show that attention can act as an explanation (Wiegreffe and Pinter, 2019). Finally, the utility of any explanation mechanism is closely tied to the users and application domain (Kaur et al., 2020; Burkart and Huber, 2021). As our analysis shows, attention adds intuition and insights to the empirical findings.\nCounterfactual explanations seek to explain the behavior of a model by performing a what if analysis on examples (Mothilal et al., 2020; Stepin et al., 2021; Verma et al., 2020; Poyiadzi et al., 2020; Goyal et al., 2019). While counterfactuals can be misleading due to artifacts (e.g., see (Laugel et al., 2019; Slack et al., 2021)), they offer a tractable solution for probing large models like PaLM and GPT-3. Notably, unlike fine-tuned methods, the most important examples for generating the model output are readily available. Thus, counterfactual inputs that show a consistent and systematic change in the model performance are more likely to reflect the model\u2019s behavior."
        },
        {
            "heading": "G Attention Analysis",
            "text": "While attention mechanisms have proven invaluable for enhancing the performance of deep neural networks, they should be used with caution when interpreting how a model works. The interpretations derived from attention weights are, at best, approximate indicators of the model\u2019s decision process and should not be over-interpreted as a precise description of the underlying mechanisms.\nThe broader question on the utility of attention for posthoc analysis is still open (Jain and Wallace, 2019; Pruthi et al., 2020), with some evidence to show that attention can act as an explanation (Wiegreffe and Pinter, 2019). Finally, the utility of any explanation mechanism is closely tied to the users and application domain (Kaur et al., 2020; Burkart and Huber, 2021). Our analysis shows that attention provides concurring evidence that adds intuition and insights to the empirical findings of this\nwork. Note that while we conduct empirical experiments with PaLM, GPT-3, and CODEX, we only conduct attention-related ablations with PaLM as the GPT-3 and CODEX were only available to us via API. Attention for autoregressive models. Consider a sentence: my dog loved the toy. Modern NLP methods divide each sentence into tokens, a decision dictated by the underlying tokenization library. PaLM uses SentencePiece (Kudo and Richardson, 2018) for tokenization. For simplicity, we assume a tokenizer that divides the sentence into tokens based on the whitespace. This yields the following list of tokens: [my, dog, loves, treats].\nLet BOS be a special beginning of sequence token present in all sentences, and p\u03b8 be a language model with the parameters \u03b8. Decoder-only language models such as PaLM estimate the likelihood of a sequence such as my dog loved the toy using an autoregressive factorization or the chainrule:\np\u03b8pBOS,my, dog, loves, treatsq \u201c p\u03b8pmy | BOSq \u02da p\u03b8pdog | BOS,myq \u02da p\u03b8ploves | BOS,my, dogq \u02da p\u03b8ptreats | BOS,my, dog, lovesq\nEstimating these conditional probabilities (e.g., p\u03b8pmy | BOSq) requires a stack of transformer layers, each containing an attention module. Thus, this factorization also implies that tokens attend to the left (Figure 4), with a token wi at location i attending to all tokens w\u0103i.\nLet ws be the source token (current input to the model). The set of target tokens, or tokens that ws will attend to, thus are: w0, w1, . . . , ws\u00b41. PaLM-62B has 64 layers, each containing the selfattention mechanism with 32 heads. Focusing on a single layer and head, let ast be the attention score from ws to wt, where \u0159s\u00b41 t\u201c0 ast \u201c 1. Analyzing the important components of a COT prompt. We leverage attention scores as an additional signal to help uncover the important components of a prompt. To this end, we calculate the attention scores from the source tokens that are part of the Q\u2019, T\u2019, or A\u2019 to the target prompt question Qi, thought Ti, and answer Ai (Figure 3). Note that the same prompt is used for all the questions in the test set. Thus across questions, the set of target tokens remains the same.\nOur goal in attention analysis is to uncover important tokens and spans used by PaLM to solve a task. Since the distribution of attention scores as is typically long-tailed, recording the attention score between every pair of source-target tokens might lead to noise and spurious patterns (Nan et al., 2021). To remedy this, we take inspiration from nucleus sampling (Holtzman et al., 2019) and set all values below the kth largest attention value to 0 (we use k \u201c 10).\nLet Q1j be the j th question in the test set Q of the questions to be evaluated. Recall that the same prompt is used for all the questions, and we calculate the attention scores from the source tokens (tokens in the inference question) to the target tokens (those in the prompt). Let ast be the attention from token ws to wt.\nWe calculate the attention importance It of a token wt in the prompt as the average max attention it has received across the set Q of inference questions.\nIt \u201c \u0159|Q| j\u201c1max |Q1j |`|A1j |`|T 1j | s\u201c1 ast\n|Q| (1)\n1. The spectrum plots show a comparison of It for all tokens in the prompt for two different prompts: vanilla COT prompt and Csymb_absppq prompt.\n2. The pattern vs. text prompts group the target tokens by their type: the tokens that belong to a pattern vs. tokens belonging to the text. The attention importance values are then shown.\n3. The bos by layer plots investigate the total attention importance for the BOS token.\nG.1 Per-layer Attention Analysis The main draft provides spectrum plots averaged over heads and layers. Figure 11 shows the same question for three different datasets averaged across layers. Figure 5, Figure 6, and Figure 7 provide\nthe same plots, per layer. We find that the spectrum of Is values is identical between COT(p) and Csymb_absppq across layers, showing that averaging is not leading to spurious correlations.\nG.2 Specialized Attention Heads\nFine-tuned models can be expected to learn attention patterns that facilitate solving a task. Does the same hold for few-shot models? To our knowledge, the question of attention in a few-shot setup has not been explored. Surprisingly, we find that the model consistently uses certain heads and layers for attending over certain semantic parts of the inputs. We find such specialized head-layer pairs manually, and plot the average It for 100 questions for them in Figure 9. The It values show a clear tendency for the head to favor either past tense (would, yesterday) or future tense (will). Analyzing a largelanguage model\u2019s attention patterns in detail is an interesting future work.\nSymbiosis in attention scores We have explored different semantic components of prompts, namely patterns (including symbols) and text. A logical next question is whether patterns or text confer differential importance. While importance can be measured via various approaches, we use attention scores as a reasonable proxy. For GSM-8K (where the distinction between patterns and text is clear), we calculate attention mass on patterns and text across several layers and average it over their attention heads. Figure 11 compares these average scores, normalized between patterns and text. Our findings show that the model pays approximately equal attention to both, indicating similar importance. These results concur with our findings that text and patterns contribute equally to the success of COT."
        },
        {
            "heading": "H Results on CODEX, GPT-3, PaLM-540B and Statistical Significance Test",
            "text": "We show results from four models: CODEX (Chen et al., 2021b), GPT-3 (Brown et al., 2020), and two variants of PaLM (Chowdhery et al., 2022) (PaLM62B and PaLM-540B). Note that we could not get results on all variations of prompts for GPT-3 because of usage limits by OpenAI. Such cases are indicated with a hyphen (-). Similarly, due to the rate limitations, we experimented with two seeds for all variations on CODEX and had to use a single seed for some variations. The findings are shown in Table 8 (GSM-8K), Table 9 (DATE), Table 10 (SPORTS), and Table 11 (SORTING). We find that all the findings hold across models: correctness of patterns is immaterial, abstract and OOD symbols are still helpful, and the sensitivity to text is proportional to the degree of randomness. Finally, CCOT matches or outperforms COT despite being 20% shorter.\nH.1 Significance tests for PaLM-62B\nIn this section, we present detailed results for experiments on PaLM-62B. Each experiment was repeated thrice using three different values of the random seed. We use McNemar\u2019s test (McNemar, 1947) to calculate the statistical significance of differences in the performance of a given Counterfactual prompt with COT(p), and Cohen\u2019s kappa (Cohen, 1960) to measure the degree of agreement between the outputs generated by a counterfactual prompt and COT(p)."
        },
        {
            "heading": "I Additional Experiments",
            "text": "I.1 Constructing Effective Intermediate Thoughts\nHeeding our preceding findings, this section underscores few concrete venues in which the symbiosis of patterns and text contribute to the construction of effective thoughts, consequently leading to the success of COT. To enable a systematic analysis, we first identify samples in which COT(p) yields\ncorrect answer, whereas both Cpat_inconsistentppq and Cpat_onlyppq are wrong. Analyzing these samples assist us in identifying probable systematic differences across these methods.\nCOT is more effective in solving questions with\nmore patterns. In general, questions with more patterns require more intermediate steps to arrive at correct answers. Thus, COT is expected to help more for such cases. We test this hypothesis by glancing into the GSM-8K dataset. The num-\nber of GSM-8K questions that the model exclusively solve using COT(p) is 140. Note that these are the questions that neither Cpat_inconsistentppq nor Cpat_onlyppq can solve. We observe that this is roughly twice and four-times the num-\nber of questions that are exclusively solved by Cpat_inconsistentppq (71) and Cpat_onlyppq (33), respectively. Further, the average number of numerical entities in these questions is 3.98 as compared to the overall average of 3.62, a statistically sig-"
        },
        {
            "heading": "Q : 2 0 1 5 is coming in 3 6 hours . What is the date one week from today in MM / DD / YYYY ? A : If 2 0 1 5 is coming in",
            "text": ""
        },
        {
            "heading": "3 6 hours , then it is coming in 2 days . 2 days before 0 1 / 0 1 / 2 0 1 5 is 1 2 / 3 0 / 2 0 2 1 , so today is 1 2 / 3 0 / 2 0 2 1 . So one week from today will be 0 1 / 0 5 / 2 0 1 5 . So the answer is 0 1 / 0 5 / 2 0 1 5 . Q : The first day of 2 0 1 9 is a Tuesday , and today is the first Monday of 2 0 1 9 . What is the date today in MM / DD / YYYY ? A : If the first day of 2 0",
            "text": ""
        },
        {
            "heading": "1 9 was Tuesday , then 0 1 / 0 1 / 2 0 1 9 was a Tuesday . Today is the first monday , would be six days later . So today is",
            "text": "CODEX GPT-3 PaLM-62B PaLM-540B\nPrompt Avg. SD Avg. SD Avg. SD Avg. SD DIRECT 100% - 50% - 46.87% 2.288 99.8% - COT(p) (Table 23) 100% - 50% - 61.87% 3.151 99.8% - Csymb_oodppq (Table 33) 100% - 50% - 57.2% 3.441 99% - Csymb_absppq (Table 26) 100% - 50% - 81.47% 10.804 91.4% - Cpat_wrongppq (Table 47) 99.9% 0.1 50% - 61.47% 4.014 99.0% - Cpat_inconsistentppq (Table 43) 45.2% 4 24.6% - 79.27% 9.116 84.2% -\nFigure 15-a\nPattern Text C/A\n0 0.56340131 0.436598691.29043289158747\n7 0.57312423 0.426875771.34260192374001 15 0.56156426 0.438435741.28083595557242 23 0.46547819 0.534521810.870831051776914 31 0.64273462 0.357265381.79903975022713 0 0.58325325 0.416746751.39953880864098 7 0.58644751 0.413552491.41807273364501 15 0.57141153 0.428588471.33324055591136 23 0.45298008 0.547019920.828086991786332 31 0.64524827 0.354751731.81887279309392\n0.00\n0.20\n0.40\n0.60\n0.80\n1.00 Pattern Text\nN or\nm al\niz ed\nF ra\nct io\nn of\nAt\nte nt\nio n\nM as\ns\nCoT CoT-Symbolic\nLa ye\nr 0\nLa ye\nr 7\nLa ye\nr 1 5\nLa ye\nr 2 3\nLa ye\nr 3 1\nLa ye\nr 0\nLa ye\nr 7\nLa ye\nr 1 5\nLa ye\nr 2 3\nLa ye\nr 3 1\nFigure 11: Visualizing the normalized fraction of attention mass between text and patterns across multiple layers of the PaLM-62B model layers with the vanilla COT (left-side) and symbolic COT (right-side). In general, patterns receive slightly higher attention across most of the layers, with the topmost layer paying the largest attention. Strikingly, the attention patterns closely match for vanilla and symbolic COT, implying that few-shot models leverage patterns to a larger extent.\nSolve Rate\nPrompt S0 S1 S2 Avg. SD Mcnemar\u2019s p-value Cohen\u2019s \u03ba DIRECT 29.51% 32.09% 33.24% 31.61% 1.558 0.000002 0.4888 COT(p) (Table 21) 44.70% 44.99% 45.85% 45.18% 0.487 \u2014 \u2014\nCsymb_absppq (Table 24) 37.54% 36.96% 35.24% 36.58% 0.974 0.001335 0.5844 Csymb_oodppq (Table 31) 42.69% 46.13% 44.70% 44.51% 1.410 0.520219 0.7895 Cpat_onlyppq (Table 31) 33.52% 32.38% 33.52% 33.14% 0.540 0.000001 0.5931 Cpat_wrongppq (Table 36) 38.68% 45.56% 44.41% 42.88% 3.008 0.433173 0.8060 Cpat_inconsistentppq (Table 44) 37.54% 36.10% 34.67% 36.10% 1.170 0.000263 0.6212 Ctext_randppq (Table 60) 21.78% 28.37% 18.05% 22.73% 4.265 \u01030.00001 0.4094 Ctext_yodathoughtsppq (Table 54) 28.94% 32.95% 30.37% 30.75% 1.660 0.000004 0.4426 Ctext_yodappq (Table 57) 34.10% 32.09% 33.24% 33.14% 0.822 0.000010 0.5023 Ctext_yodaquestionsppq (Table 57) 44.13% 48.14% 42.12% 44.79% 2.502 0.358008 0.7609 Ctext_intra_shufppq (Table 63) 26.36% 25.50% 24.64% 25.50% 0.702 \u01030.00001 0.4428 Ctext_inter_shufppq (Table 66) 25.21% 23.78% 23.50% 24.16% 0.752 \u01030.00001 0.4332\nI.2 Commonsense Extraction\nWe conjecture that COT unlocks a golden opportunity to bring forth supplementary commonsense knowledge from the question and generate correct intermediate steps. Consequently, the generated commonsense knowledge assists the model to attain a factual answer. For example, we observe that in \u27a5Q3 of Table 16 the model with COT infuses commonsense knowledge about animals (e.g., \u201c5 dogs have 4 legs each . . . 2 cats have 4 legs each . . . 10 birds have 2 legs each\u201d). In addition, reviewing the generated thoughts in DATE underscores the ability of the model to recast commonsense knowledge about dates into a coherent format in thoughts (\u27a5Q6- T vCOTw in Table 16). In this example, the model articulates the exact date for \u201cChrist-\nmas Eve\u201d in the generated thought. Evidently, the model conditions on \u201cChristmas Eve\u201d and \u201cToday is\u201d to form the exact date \u201c12/24/1937\u201d. This is undeniably an arduous task for the DIRECT setup, as it warrants the creation of these two fragments of information in one step.\nExtracting rare commonsense knowledge. To better understand the ability of the model to extract rare commonsense knowledge, we resort to the number of Google search results, which we refer to as \u201cPopularity Metric\u201d, as a proxy to gauge the rarity of an entity. Employing this metric is germane to PaLM\u2019s training dataset, which is a web-based corpus (Chowdhery et al., 2022). We use this metric in the SPORTS dataset because the model is required to reason about factual commonsense knowledge to arrive at the correct conclu-"
        },
        {
            "heading": "J Complete List of Counterfactual Prompts",
            "text": "This section includes all the counterfactual prompts used in our experiments. Please note that the prompts are added here for quick reference, and can also be located in the accompanying repository (https://github.com/ reasoning-machines/prompt-lib).\nText and Patterns: For Effective Chain of Thought, It Takes Two to Tango\nSource Code 1: Python code to reproduce Sort dataset.\nI. COMPLETE LIST OF COUNTERFACTUAL PROMPTS\nThis section includes all the counterfactual prompts used in our experiments.\n59\nTable 20: GSM-8K prompts used by Wei et al. (2022b).\nqPrompt 1 \u27a5Q \u00de\u00d1 Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? T \u00de\u00d1 Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. VA \u00de\u00d1 9 qPrompt 2 \u27a5Q \u00de\u00d1 If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? T \u00de\u00d1 There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. VA \u00de\u00d1 5 qPrompt 3 \u27a5Q \u00de\u00d1 Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny? T \u00de\u00d1 Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. VA \u00de\u00d1 8 qPrompt 4 \u27a5Q \u00de\u00d1 There were nine computers in the server room. Five more computers were installed each day, from Monday to Thursday. How many computers are now in the server room? T \u00de\u00d1 There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. VA \u00de\u00d1 29 qPrompt 5 \u27a5Q \u00de\u00d1 There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? T \u00de\u00d1 There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. VA \u00de\u00d1 6 qPrompt 6 \u27a5Q \u00de\u00d1 Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? T \u00de\u00d1 Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. VA \u00de\u00d1 39 qPrompt 7 \u27a5Q \u00de\u00d1 Olivia has $23. She bought five bagels for $3 each. How much money does she have left? T \u00de\u00d1 Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. VA \u00de\u00d1 8 qPrompt 8 \u27a5Q \u00de\u00d1 Michael had 58 golf balls. On Tuesday, he lost 23 golf balls. On Wednesday, he lost 2 more. How many golf balls did he have at the end of Wednesday? T \u00de\u00d1 Michael started with 58 golf balls. After losing 23 on Tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. VA \u00de\u00d1 33\nTable 21: DATE prompts used by Wei et al. (2022b).\nqPrompt 1 \u27a5Q \u00de\u00d1 It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 Today is 04/19/1969. 24 hours later is one day after today, which would be 04/20/1969. VA \u00de\u00d1 04/20/1969 qPrompt 2 \u27a5Q \u00de\u00d1 The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY? T \u00de\u00d1 One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. VA \u00de\u00d1 05/23/1943 qPrompt 3 \u27a5Q \u00de\u00d1 The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY? T \u00de\u00d1 If the first day of 2019 was Tuesday, then 01/01/2019 was a Tuesday. Today is the first Monday, would be six days later. So today is 01/07/2019. VA \u00de\u00d1 01/07/2019 qPrompt 4 \u27a5Q \u00de\u00d1 Jane was born on the last day of February in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY? T \u00de\u00d1 The last day of February is the 28th, so Jane was born on 02/28/2001. Today is her 16-year old birthday, so today is 02/28/2017. So yesterday was 02/27/2017. VA \u00de\u00d1 02/27/2017 qPrompt 5 \u27a5Q \u00de\u00d1 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY? T \u00de\u00d1 If 2015 is coming in 36 hours, then it is coming in 2 days. 2 days before 01/01/2015 is 12/30/2014, so today is 12/30/2014. So one week from today will be 01/05/2015. VA \u00de\u00d1 01/05/2015 qPrompt 6 \u27a5Q \u00de\u00d1 Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 Today is 03/12/2002. So the date 24 hours later will be 03/13/2002. VA \u00de\u00d1 03/13/2002\nTable 22: SPORTS prompts used by Wei et al. (2022b).\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJamal Murray was perfect from the line.\u201d T \u00de\u00d1 Jamal Murray is a basketball player. Being perfect from the line is part of basketball. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJoao Moutinho caught the screen pass in the NFC championship.\u201d T \u00de\u00d1 Joao Moutinho is a soccer player. The NFC championship is part of American football, not soccer. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJonas Valanciunas beat the buzzer.\u201d T \u00de\u00d1 Jonas Valanciunas is a basketball player. Beating the buzzer is part of basketball. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cSam Darnold passed the puck.\u201d T \u00de\u00d1 Sam Darnold is a American football player. Passing the puck is part of hockey, not American football. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cKyle Palmieri was called for slashing.\u201d T \u00de\u00d1 Kyle Palmieri is a hockey player. Being called for slashing is part of hockey. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cCarson Wentz set the pick and roll.\u201d T \u00de\u00d1 Carson Wentz is an American football player. Pick and roll is part of basketball, not football. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cMalcolm Brogdon banked the shot in.\u201d T \u00de\u00d1 Malcolm Brogdon is a basketball player. Banking the shot in is part of basketball. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cDraymond Green threw a touchdown.\u201d T \u00de\u00d1 Draymond Green is an basketball player. Throwing a touchdown is part of football, not basketball. VA \u00de\u00d1 no\nTable 23: SORTING prompts using code snippet 12.\nqPrompt 1 \u27a5Q \u00de\u00d1 7 , 8 , 4 , 1 , 2 , 9 , 3 , 6 , 5 T \u00de\u00d1 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 < 9 VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 qPrompt 2 \u27a5Q \u00de\u00d1 5 , 9 , 3 , 1 , 8 , 4 , 6 , 2 T \u00de\u00d1 1 < 2 < 3 < 4 < 5 < 6 < 8 < 9 VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 8 , 9 qPrompt 3 \u27a5Q \u00de\u00d1 6 , 5 , 7 , 4 , 3 , 2 , 8 , 1 T \u00de\u00d1 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 4 \u27a5Q \u00de\u00d1 1 , 6 , 4 , 8 , 5 , 3 , 7 , 2 T \u00de\u00d1 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 5 \u27a5Q \u00de\u00d1 5 , 2 , 1 , 4 , 3 , 7 T \u00de\u00d1 1 < 2 < 3 < 4 < 5 < 7 VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 7 qPrompt 6 \u27a5Q \u00de\u00d1 3 , 8 , 2 , 5 , 6 , 4 , 7 , 1 T \u00de\u00d1 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 7 \u27a5Q \u00de\u00d1 8 , 6 , 1 , 2 , 9 , 7 , 4 T \u00de\u00d1 1 < 2 < 4 < 6 < 7 < 8 < 9 VA \u00de\u00d1 1 , 2 , 4 , 6 , 7 , 8 , 9 qPrompt 8 \u27a5Q \u00de\u00d1 7 , 6 , 8 , 1 T \u00de\u00d1 1 < 6 < 7 < 8 VA \u00de\u00d1 1 , 6 , 7 , 8\nTable 24: DATE with abstract dates.\nqPrompt 1 \u27a5Q \u00de\u00d1 It is DATE today. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 Today is DATE. 24 hours later is one day after today, which would be DATE. VA \u00de\u00d1 04/20/1969 qPrompt 2 \u27a5Q \u00de\u00d1 The concert was scheduled to be on DATE, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY? T \u00de\u00d1 One day after DATE is DATE, so today is DATE. 10 days before today is 05/23/1943. VA \u00de\u00d1 05/23/1943 qPrompt 3 \u27a5Q \u00de\u00d1 The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY? T \u00de\u00d1 If the first day of 2019 was Tuesday, then DATE was a Tuesday. Today is the first Monday, would be six days later. So today is 01/07/2019. VA \u00de\u00d1 01/07/2019 qPrompt 4 \u27a5Q \u00de\u00d1 Jane was born on the last day of February in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY? T \u00de\u00d1 The last day of February is the 28th, so Jane was born on DATE. Today is her 16-year old birthday, so today is DATE. So yesterday was 02/27/2017. VA \u00de\u00d1 02/27/2017 qPrompt 5 \u27a5Q \u00de\u00d1 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY? T \u00de\u00d1 If 2015 is coming in 36 hours, then it is coming in 2 days. 2 days before DATE is DATE, so today is DATE. So one week from today will be 01/05/2015. VA \u00de\u00d1 01/05/2015 qPrompt 6 \u27a5Q \u00de\u00d1 Jane thought today is DATE, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 Today is DATE. So the date 24 hours later will be 03/13/2002. VA \u00de\u00d1 03/13/2002\nTable 25: GSM-8K after replacing numbers with Greek letters.\nqPrompt 1 \u27a5Q \u00de\u00d1 Shawn has \u03b1 toys. For Christmas, he got \u03b2 toys each from his mom and dad. How many toys does he have now? T \u00de\u00d1 Shawn started with \u03b1 toys. If he got \u03b2 toys each from his mom and dad, then that is \u03bb more toys. \u03b1 + \u03bb = \u03c0. VA \u00de\u00d1 \u03c0 qPrompt 2 \u27a5Q \u00de\u00d1 If there are \u03b1 cars in the parking lot and \u03b2 more cars arrive, how many cars are in the parking lot? T \u00de\u00d1 There are originally \u03b1 cars. \u03b2 more cars arrive. \u03b1 + \u03b2 = \u03bb. VA \u00de\u00d1 \u03bb qPrompt 3 \u27a5Q \u00de\u00d1 Jason had \u03b1 lollipops. He gave Denny some lollipops. Now Jason has \u03b2 lollipops. How many lollipops did Jason give to Denny? T \u00de\u00d1 Jason started with \u03b1 lollipops. Then he had \u03b2 after giving some to Denny. So he gave Denny \u03b1 - \u03b2 = \u03bb. VA \u00de\u00d1 \u03bb qPrompt 4 \u27a5Q \u00de\u00d1 There were \u03b1 computers in the server room. \u03b2 more computers were installed each day, from Monday to Thursday. How many computers are now in the server room? T \u00de\u00d1 There were originally \u03b1 computers. For each of four days, \u03b2 more computers were added. So \u03b2 * four = \u03bb computers were added. \u03b1 + \u03bb is \u03c0. VA \u00de\u00d1 \u03c0 qPrompt 5 \u27a5Q \u00de\u00d1 There are \u03b1 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be \u03b2 trees. How many trees did the grove workers plant today? T \u00de\u00d1 There are \u03b1 trees originally. Then there were \u03b2 trees after some more were planted. So there must have been \u03b2 - \u03b1 = \u03bb. VA \u00de\u00d1 \u03bb qPrompt 6 \u27a5Q \u00de\u00d1 Leah had \u03b1 chocolates and her sister had \u03b2. If they ate \u03bb, how many pieces do they have left in total? T \u00de\u00d1 Originally, Leah had \u03b1 chocolates. Her sister had \u03b2. So in total they had \u03b1 + \u03b2 = \u03c0. After eating \u03bb, they had \u03c0 - \u03bb = \u00b5. VA \u00de\u00d1 \u00b5 qPrompt 7 \u27a5Q \u00de\u00d1 Olivia has \u03b1. She bought five bagels for \u03b2 each. How much money does she have left? T \u00de\u00d1 Olivia had \u03b1 dollars. 5 bagels for \u03b2 dollars each will be 5 x \u03b2 = \u03bb dollars. So she has \u03b1 - \u03bb dollars left. \u03b1 - \u03bb is \u03c0. VA \u00de\u00d1 \u03c0 qPrompt 8 \u27a5Q \u00de\u00d1 Michael had \u03b1 golf balls. On Tuesday, he lost \u03b2 golf balls. On Wednesday, he lost \u03bb more. How many golf balls did he have at the end of Wednesday? T \u00de\u00d1 Michael started with \u03b1 golf balls. After losing \u03b2 on Tuesday, he had \u03b1 - \u03b2 = \u03c0. After losing \u03bb more, he had \u03c0 - \u03bb = \u00b5 golf balls. VA \u00de\u00d1 \u00b5\nTable 26: SORTING after replacing numbers with Greek letters.\nqPrompt 1 \u27a5Q \u00de\u00d1 \u03c5 , \u03b4 , \u03b6 , \u03d5 , \u03c0 , \u03b3 , \u03c2, \u03f5 , \u03c7 T \u00de\u00d1 \u03c2 < \u03d5 < \u03b3 < \u03b4 < \u03b6 < \u03c7 < \u03f5 < \u03c0 < \u03c5 VA \u00de\u00d1 \u03c2 , \u03d5 , \u03b3 , \u03b4 , \u03b6 , \u03c7 , \u03f5 , \u03c0 , \u03c5 qPrompt 2 \u27a5Q \u00de\u00d1 \u03bd , \u03b9 , \u03bb , \u03b6 , \u03be , \u03b3 , \u03c8 , \u03c0 T \u00de\u00d1 \u03be < \u03b6 < \u03bb < \u03c0 < \u03c8 < \u03b3 < \u03b9 < \u03bd VA \u00de\u00d1 \u03be , \u03b6 , \u03bb , \u03c0 , \u03c8 , \u03b3 , \u03b9 , \u03bd qPrompt 3 \u27a5Q \u00de\u00d1 \u03be , \u03c4 , \u03d5 , \u03b8 , \u03b6 , \u03b7 , \u03b9 , \u03b4 T \u00de\u00d1 \u03be < \u03b7 < \u03b8 < \u03d5 < \u03b6 < \u03b4 < \u03b9 < \u03c4 VA \u00de\u00d1 \u03be , \u03b7 , \u03b8 , \u03d5 , \u03b6 , \u03b4 , \u03b9 , \u03c4 qPrompt 4 \u27a5Q \u00de\u00d1 \u03b6 , \u03c0 , \u03b2 , \u03c3 , \u03d5 , \u03f5 , \u03bb , o T \u00de\u00d1 \u03f5 < \u03b6 < \u03c0 < \u03bb < \u03d5 < \u03b2 < o < \u03c3 VA \u00de\u00d1 \u03f5 , \u03b6 , \u03c0 , \u03bb , \u03d5 , \u03b2 , o , \u03c3 qPrompt 5 \u27a5Q \u00de\u00d1 \u03c9 , o , \u03b4 , \u03b9 , \u03c5 , \u03bd T \u00de\u00d1 \u03c9 < o < \u03bd < \u03b4 < \u03b9 < \u03c5 VA \u00de\u00d1 \u03c9 , o , \u03bd , \u03b4 , \u03b9 , \u03c5 qPrompt 6 \u27a5Q \u00de\u00d1 \u00b5 , \u03b6 , \u03c0 , \u03c7 , \u03bb , \u03c2 , \u03b1 , \u03b3 T \u00de\u00d1 \u03c2 < \u03c0 < \u03b3 < \u03bb < \u03b6 < \u03c7 < \u00b5 < \u03b1 VA \u00de\u00d1 \u03c2 , \u03c0 , \u03b3 , \u03bb , \u03b6 , \u03c7 , \u00b5 , \u03b1 qPrompt 7 \u27a5Q \u00de\u00d1 \u03c4 , \u03b3 , \u03bb , \u00b5 , \u03b1 , \u03c2 , \u03ba T \u00de\u00d1 \u00b5 < \u03bb < \u03b1 < \u03c4 < \u03ba < \u03b3 < \u03c2 VA \u00de\u00d1 \u00b5 , \u03bb , \u03b1 , \u03c4 , \u03ba , \u03b3 , \u03c2 qPrompt 8 \u27a5Q \u00de\u00d1 \u03ba , \u03be , \u03bd , \u03b4 T \u00de\u00d1 \u03ba < \u03be < \u03b4 < \u03bd VA \u00de\u00d1 \u03ba , \u03be , \u03b4 , \u03bd\nTable 27: SPORTS with abstract sports person.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPERSON was perfect from the line.\u201d T \u00de\u00d1 PERSON is a basketball player. Being perfect from the line is part of basketball. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPERSON caught the screen pass in the NFC championship.\u201d T \u00de\u00d1 PERSON is a soccer player. The NFC championship is part of American football, not soccer. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPERSON beat the buzzer.\u201d T \u00de\u00d1 PERSON is a basketball player. Beating the buzzer is part of basketball. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPERSON passed the puck.\u201d T \u00de\u00d1 PERSON is a American football player. Passing the puck is part of hockey, not American football. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPERSON was called for slashing.\u201d T \u00de\u00d1 PERSON is a hockey player. Being called for slashing is part of hockey. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPERSON set the pick and roll.\u201d T \u00de\u00d1 PERSON is an American football player. Pick and roll is part of basketball, not football. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPERSON banked the shot in.\u201d T \u00de\u00d1 PERSON is a basketball player. Banking the shot in is part of basketball. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPERSON threw a touchdown.\u201d T \u00de\u00d1 PERSON is an basketball player. Throwing a touchdown is part of football, not basketball. VA \u00de\u00d1 no\nTable 28: SPORTS with abstract sports activity.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJamel Murray was ACTIVITY.\u201d T \u00de\u00d1 Jamal Murray is a basketball player. Being ACTIVITY is part of basketball. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJoao Moutinho caught ACTIVITY.\u201d T \u00de\u00d1 Joao Moutinho is a soccer player. The ACTIVITY is part of American football, not soccer. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJonas Valanciunas ACTIVITY.\u201d T \u00de\u00d1 Jonas Valanciunas is a basketball player. ACTIVITY is part of basketball. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cSam Darnold ACTIVITY.\u201d T \u00de\u00d1 Sam Darnold is a American football player. ACTIVITY is part of hockey, not American football. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cKyle Palmieri was ACTIVITY.\u201d T \u00de\u00d1 Kyle Palmieri is a hockey player. Being ACTIVITY is part of hockey. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cCarson Wentz set the ACTIVITY.\u201d T \u00de\u00d1 Carson Wentz is an American football player. ACTIVITY is part of basketball, not football. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cMalcolm Brogdon ACTIVITY.\u201d T \u00de\u00d1 Malcolm Brogdon is a basketball player. ACTIVITY is part of basketball. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cDraymond Green ACTIVITY.\u201d T \u00de\u00d1 Draymond Green is an basketball player. ACTIVITY is part of football, not basketball. VA \u00de\u00d1 no\nTable 29: SPORTS with abstract sport.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"Jamel Murray was perfect from the line.\" T \u00de\u00d1 Jamal Murray is a SPORT1 player. Being perfect from the line is part of SPORT1. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"Joao Moutinho caught the screen pass in the NFC championship.\" T \u00de\u00d1 Joao Moutinho is a SPORT2 player. The NFC championship is part of SPORT3, not SPORT2. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"Jonas Valanciunas beat the buzzer.\" T \u00de\u00d1 Jonas Valanciunas is a SPORT1 player. Beating the buzzer is part of SPORT1. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"Sam Darnold passed the puck.\" T \u00de\u00d1 Sam Darnold is a SPORT3 player. Passing the puck is part of SPORT4, not SPORT3. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"Kyle Palmieri was called for slashing.\" T \u00de\u00d1 Kyle Palmieri is a SPORT4 player. Being called for slashing is part of SPORT4. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"Carson Wentz set the pick and roll.\" T \u00de\u00d1 Carson Wentz is an SPORT3 player. Pick and roll is part of SPORT1, not SPORT3. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"Malcolm Brogdon banked the shot in.\" T \u00de\u00d1 Malcolm Brogdon is a SPORT1 player. Banking the shot in is part of SPORT1. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"Draymond Green threw a touchdown.\" T \u00de\u00d1 Draymond Green is an SPORT1 player. Throwing a touchdown is part of SPORT3, not SPORT1. VA \u00de\u00d1 no\nTable 30: GSM-8K with decimal numbers.\nqPrompt 1 \u27a5Q \u00de\u00d1 Shawn has five and a half toys. For Christmas, he got two and a half toys each from his mom and dad. How many toys does he have now? T \u00de\u00d1 Shawn started with 5.5 toys. If he got 2.5 toys each from his mom and dad, then that is 5 more toys. 5.5 + 5 = 10.5. VA \u00de\u00d1 10.5 qPrompt 2 \u27a5Q \u00de\u00d1 If there are 3.3 cars in the parking lot and 2.8 more cars arrive, how many cars are in the parking lot? T \u00de\u00d1 There are originally 3.3 cars. 2.8 more cars arrive. 3.3 + 2.8 = 6.1. VA \u00de\u00d1 6.1 qPrompt 3 \u27a5Q \u00de\u00d1 Jason had 20.2 lollipops. He gave Denny some lollipops. Now Jason has 15.5 lollipops. How many lollipops did Jason give to Denny? T \u00de\u00d1 Jason started with 20.2 lollipops. Then he had 15.5 after giving some to Denny. So he gave Denny 20.2 - 15.5 = 4.7. VA \u00de\u00d1 4.7 qPrompt 4 \u27a5Q \u00de\u00d1 There were nine and a quarter computers in the server room. Five and three quarters more computers were installed each day, from Monday to Thursday. How many computers are now in the server room? T \u00de\u00d1 There were originally 9.25 computers. For each of 4 days, 5.75 more computers were added. So 5.75 * 4 = 23 computers were added. 9.25 + 23 is 32.25. VA \u00de\u00d1 32.25 qPrompt 5 \u27a5Q \u00de\u00d1 There are 15.3 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 16.5 trees. How many trees did the grove workers plant today? T \u00de\u00d1 There are 15.3 trees originally. Then there were 16.5 trees after some more were planted. So there must have been 16.5 - 15.3 = 1.2. VA \u00de\u00d1 1.2 qPrompt 6 \u27a5Q \u00de\u00d1 Leah had 3.2 chocolates and her sister had 4.2. If they ate 3.5, how many pieces do they have left in total? T \u00de\u00d1 Originally, Leah had 3.2 chocolates. Her sister had 4.2. So in total they had 3.2 + 4.2 = 7.4. After eating 3.5, they had 7.4 - 3.5 = 3.9. VA \u00de\u00d1 3.9 qPrompt 7 \u27a5Q \u00de\u00d1 Olivia has $2.3. She bought five bagels for $0.3 each. How much money does she have left? T \u00de\u00d1 Olivia had 2.3 dollars. 5 bagels for .3 dollars each will be 5 x .3 = 1.5 dollars. So she has 2.3 - 1.5 dollars left. 2.3 - 1.5 is 0.8. VA \u00de\u00d1 0.8 qPrompt 8 \u27a5Q \u00de\u00d1 Michael had 5.8 golf balls. On Tuesday, he lost 2.3 golf balls. On Wednesday, he lost 0.2 more. How many golf balls did he have at the end of Wednesday? T \u00de\u00d1 Michael started with 5.8 golf balls. After losing 2.3 on Tuesday, he had 5.8 - 2.3 = 3.5. After losing 0.2 more, he had 3.5 - 0.2 = 3.3 golf balls. VA \u00de\u00d1 3.3\nTable 31: DATE with future dates.\nqPrompt 1 \u27a5Q \u00de\u00d1 It is 4/30/3069 today. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 Today is 04/30/3069. 24 hours later is one day after today, which would be 04/31/3069. VA \u00de\u00d1 04/31/3069 qPrompt 2 \u27a5Q \u00de\u00d1 The concert was scheduled to be on 06/01/3043, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY? T \u00de\u00d1 One day after 06/01/3043 is 06/02/3043, so today is 06/02/3043. 10 days before today is 05/23/3043. VA \u00de\u00d1 05/23/3043 qPrompt 3 \u27a5Q \u00de\u00d1 The first day of 3130 is a Tuesday, and today is the first Monday of 3130. What is the date today in MM/DD/YYYY? T \u00de\u00d1 If the first day of 3130 was Tuesday, then 01/01/3130 was a Tuesday. Today is the first Monday, would be six days later. So today is 01/07/3130. VA \u00de\u00d1 01/07/3130 qPrompt 4 \u27a5Q \u00de\u00d1 Jane was born on the last day of February in 3101. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY? T \u00de\u00d1 The last day of February is the 28th, so Jane was born on 02/28/3101. Today is her 16-year old birthday, so today is 02/28/3117. So yesterday was 02/27/3117. VA \u00de\u00d1 02/27/3117 qPrompt 5 \u27a5Q \u00de\u00d1 3115 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY? T \u00de\u00d1 If 3115 is coming in 36 hours, then it is coming in 2 days. 2 days before 01/01/3115 is 12/30/3114, so today is 12/30/3114. So one week from today will be 01/05/3115. VA \u00de\u00d1 01/05/3115 qPrompt 6 \u27a5Q \u00de\u00d1 Jane thought today is 3/11/3102, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 Today is 03/12/3102. So the date 24 hours later will be 03/13/3102. VA \u00de\u00d1 03/13/3102\nTable 32: SPORTS with non-sport names and activities.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cAdair Foster was juggling the paper cups.\u201d T \u00de\u00d1 Adair Foster is a basketball player. Juggling the paper cups is part of basketball. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cScottie Colby caught the hot potato in the NFC championship.\u201d T \u00de\u00d1 Scottie Colby is a soccer player. The NFC championship is part of American football, not soccer. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cChas Jase beat the buzzer.\u201d T \u00de\u00d1 Chas Jase is a basketball player. Beating the pillow is part of basketball. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cDenny Dillan passed the soda.\u201d T \u00de\u00d1 Denny Dillan is a American football player. Passing the soda is part of hockey, not American football. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cMyron Storm was called for trashing.\u201d T \u00de\u00d1 Myron Storm is a hockey player. Being called for trashing is part of hockey. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cCliff Kristopher set the tick and floor.\u201d T \u00de\u00d1 Cliff Kristopher is an American football player. Tick and floor is part of basketball, not football. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cCelestine Holden banked the pot in.\u201d T \u00de\u00d1 Celestine Holden is a basketball player. Banking the pot in is part of basketball. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPeter Abraham threw a tantrum.\u201d T \u00de\u00d1 Peter Abraham is an basketball player. Throwing a tantrum is part of football, not basketball. VA \u00de\u00d1 no\nTable 33: SORTING with out-of-distribution numbers.\nqPrompt 1 \u27a5Q \u00de\u00d1 72 , 85 , 48 , 11 , 23 , 95 , 34 , 63 , 56 T \u00de\u00d1 11 < 23 < 34 < 48 < 56 < 63 < 72 < 85 < 95 VA \u00de\u00d1 11 , 23 , 34 , 48 , 56 , 63 , 72 , 85 , 95 qPrompt 2 \u27a5Q \u00de\u00d1 56 , 95 , 34 , 11 , 85 , 48 , 63 , 23 T \u00de\u00d1 11 < 23 < 34 < 48 < 56 < 63 < 85 < 95 VA \u00de\u00d1 11 , 23 , 34 , 48 , 56 , 63 , 85 , 95 qPrompt 3 \u27a5Q \u00de\u00d1 63 , 56 , 72 , 48 , 34 , 23 , 85 , 11 T \u00de\u00d1 11 < 23 < 34 < 48 < 56 < 63 < 72 < 85 VA \u00de\u00d1 11 , 23 , 34 , 48 , 56 , 63 , 72 , 85 qPrompt 4 \u27a5Q \u00de\u00d1 11 , 63 , 48 , 85 , 56 , 34 , 72 , 23 T \u00de\u00d1 11 < 23 < 34 < 48 < 56 < 63 < 72 < 85 VA \u00de\u00d1 11 , 23 , 34 , 48 , 56 , 63 , 72 , 85 qPrompt 5 \u27a5Q \u00de\u00d1 56 , 23 , 11 , 48 , 34 , 72 T \u00de\u00d1 11 < 23 < 34 < 48 < 56 < 72 VA \u00de\u00d1 11 , 23 , 34 , 48 , 56 , 72 qPrompt 6 \u27a5Q \u00de\u00d1 34 , 85 , 23 , 56 , 63 , 48 , 72 , 11 T \u00de\u00d1 11 < 23 < 34 < 48 < 56 < 63 < 72 < 85 VA \u00de\u00d1 11 , 23 , 34 , 48 , 56 , 63 , 72 , 85 qPrompt 7 \u27a5Q \u00de\u00d1 85 , 63 , 11 , 23 , 95 , 72 , 48 T \u00de\u00d1 11 < 23 < 48 < 63 < 72 < 85 < 95 VA \u00de\u00d1 11 , 23 , 48 , 63 , 72 , 85 , 95 qPrompt 8 \u27a5Q \u00de\u00d1 72 , 63 , 85 , 11 T \u00de\u00d1 11 < 63 < 72 < 85 VA \u00de\u00d1 11 , 63 , 72 , 85\nTable 34: GSM-8K with negative numbers.\nqPrompt 1 \u27a5Q \u00de\u00d1 Shawn has five toys. For Christmas, he got minus two toys each from his mom and dad. How many toys does he have now? T \u00de\u00d1 Shawn started with 5 toys. If he got -2 toys each from his mom and dad, then that is -4 more toys. 5 + -4 = 1. VA \u00de\u00d1 1 qPrompt 2 \u27a5Q \u00de\u00d1 If there are 3 cars in the parking lot and minus 2 more cars arrive, how many cars are in the parking lot? T \u00de\u00d1 There are originally 3 cars. -2 more cars arrive. 3 - 2 = 1. VA \u00de\u00d1 1 qPrompt 3 \u27a5Q \u00de\u00d1 Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has -2 lollipops. How many lollipops did Jason give to Denny? T \u00de\u00d1 Jason started with 20 lollipops. Then he had -2 after giving some to Denny. So he gave Denny 20 - -2 = 22. VA \u00de\u00d1 22 qPrompt 4 \u27a5Q \u00de\u00d1 There were fifteen computers in the server room. Two computers were uninstalled each day, from Monday to Thursday. How many computers are now in the server room? T \u00de\u00d1 There were originally 15 computers. For each of 4 days, 2 computers were removed. So 2 * 4 = 8 computers were removed. 15 - 8 is 7. VA \u00de\u00d1 7 qPrompt 5 \u27a5Q \u00de\u00d1 There are 15 trees in the grove. Grove workers will uproot trees in the grove today. After they are done, there will be 10 trees. How many trees did the grove workers uproot today? T \u00de\u00d1 There are 15 trees originally. Then there were 10 trees after some were uprooted. So the number that is uprooted must have been 10, as 15 - 5 = 10. VA \u00de\u00d1 5 qPrompt 6 \u27a5Q \u00de\u00d1 Leah had 32 chocolates and her sister had 42. If they ate 75, how many pieces of chocolate did they have to borrow in total? T \u00de\u00d1 Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. Since they ate 75, they had to borrow 75 - 74 = 1. VA \u00de\u00d1 1 qPrompt 7 \u27a5Q \u00de\u00d1 Olivia has $23. She wants to buy 10 bagels for $3 each. How much more money does she need? T \u00de\u00d1 Olivia has 23 dollars. 10 bagels for 3 dollars each will be 10 x 3 = 30 dollars. So she is short of 23 - 30 = -7 dollars. 30 - 23 is 7. VA \u00de\u00d1 7 qPrompt 8 \u27a5Q \u00de\u00d1 Michael reserved 58 golf balls. On Tuesday, he lost 57 golf balls. On Wednesday, he lost 2 more. How many more golf balls did he lose than his reserved quota at the end of Wednesday? T \u00de\u00d1 Michael started with 58 golf balls. After losing 57 on Tuesday, he had 58 - 57 = 1. After losing 2 more, he had 1 - 2 = -1 golf balls. So he lost 2 - 1 = 1 more balls. VA \u00de\u00d1 1\nTable 35: GSM-8K with larger numbers.\nqPrompt 1 \u27a5Q \u00de\u00d1 Shawn has three fifty nine toys. For Christmas, he got seventy one toys each from his mom and dad. How many toys does he have now? T \u00de\u00d1 Shawn started with 359 toys. If he got 71 toys each from his mom and dad, then that is 142 more toys. 359 + 142 = 501. VA \u00de\u00d1 501 qPrompt 2 \u27a5Q \u00de\u00d1 If there are 987 cars in the parking lot and 659 more cars arrive, how many cars are in the parking lot? T \u00de\u00d1 There are originally 987 cars. 659 more cars arrive. 987 + 659 = 1646. VA \u00de\u00d1 1646 qPrompt 3 \u27a5Q \u00de\u00d1 Jason had 813 lollipops. He gave Denny some lollipops. Now Jason has 582 lollipops. How many lollipops did Jason give to Denny? T \u00de\u00d1 Jason started with 813 lollipops. Then he had 582 after giving some to Denny. So he gave Denny 813 - 582 = 231. VA \u00de\u00d1 231 qPrompt 4 \u27a5Q \u00de\u00d1 There were six hundred seventy seven computers in the server room. Five hundred twenty two more computers were installed each day, from Monday to Thursday. How many computers are now in the server room? T \u00de\u00d1 There were originally 677 computers. For each of 4 days, 522 more computers were added. So 522 * 4 = 2088 computers were added. 677 + 2088 is 29. VA \u00de\u00d1 2765 qPrompt 5 \u27a5Q \u00de\u00d1 There are 715 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 803 trees. How many trees did the grove workers plant today? T \u00de\u00d1 There are 715 trees originally. Then there were 803 trees after some more were planted. So there must have been 803 - 715 = 88. VA \u00de\u00d1 88 qPrompt 6 \u27a5Q \u00de\u00d1 Leah had 732 chocolates and her sister had 642. If they ate 35, how many pieces do they have left in total? T \u00de\u00d1 Originally, Leah had 732 chocolates. Her sister had 642. So in total they had 32 + 42 = 1374. After eating 35, they had 1374 - 35 = 1339. VA \u00de\u00d1 1339 qPrompt 7 \u27a5Q \u00de\u00d1 Olivia has $323. She bought five bagels for $39 each. How much money does she have left? T \u00de\u00d1 Olivia had 323 dollars. 5 bagels for 39 dollars each will be 5 x 3 = 195 dollars. So she has 323 - 195 dollars left. 323 - 195 is 128. VA \u00de\u00d1 128 qPrompt 8 \u27a5Q \u00de\u00d1 Michael had 958 golf balls. On Tuesday, he lost 323 golf balls. On Wednesday, he lost 259 more. How many golf balls did he have at the end of Wednesday? T \u00de\u00d1 Michael started with 958 golf balls. After losing 323 on Tuesday, he had 958 - 323 = 665. After losing 259 more, he had 665 - 259 = 406 golf balls. VA \u00de\u00d1 406\nTable 36: DATE with wrong answers.\nqPrompt 1 \u27a5Q \u00de\u00d1 It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 Today is 04/19/1969. 24 hours later is one day after today, which would be 03/20/1969. VA \u00de\u00d1 04/20/1969 qPrompt 2 \u27a5Q \u00de\u00d1 The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY? T \u00de\u00d1 One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/12/1943. VA \u00de\u00d1 05/23/1943 qPrompt 3 \u27a5Q \u00de\u00d1 The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY? T \u00de\u00d1 If the first day of 2019 was Tuesday, then 01/01/2019 was a Tuesday. Today is the first Monday, would be six days later. So today is 01/07/2009. VA \u00de\u00d1 01/07/2019 qPrompt 4 \u27a5Q \u00de\u00d1 Jane was born on the last day of February in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY? T \u00de\u00d1 The last day of February is the 28th, so Jane was born on 02/28/2001. Today is her 16-year old birthday, so today is 02/28/2017. So yesterday was 03/27/2017. VA \u00de\u00d1 02/27/2017 qPrompt 5 \u27a5Q \u00de\u00d1 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY? T \u00de\u00d1 If 2015 is coming in 36 hours, then it is coming in 2 days. 2 days before 01/01/2015 is 12/30/2014, so today is 12/30/2014. So one week from today will be 02/05/2015. VA \u00de\u00d1 01/05/2015 qPrompt 6 \u27a5Q \u00de\u00d1 Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 Today is 03/12/2002. So the date 24 hours later will be 04/13/2002. VA \u00de\u00d1 03/13/2002\nTable 37: GSM-8K with wrong math but correct answer.\nqPrompt 1 \u27a5Q \u00de\u00d1 Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? T \u00de\u00d1 Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 7. VA \u00de\u00d1 9 qPrompt 2 \u27a5Q \u00de\u00d1 If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? T \u00de\u00d1 There are originally 3 cars. 2 more cars arrive. 3 + 2 = 7. VA \u00de\u00d1 5 qPrompt 3 \u27a5Q \u00de\u00d1 Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny? T \u00de\u00d1 Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 2. VA \u00de\u00d1 8 qPrompt 4 \u27a5Q \u00de\u00d1 There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room? T \u00de\u00d1 There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 22 computers were added. 9 + 20 is 49. VA \u00de\u00d1 29 qPrompt 5 \u27a5Q \u00de\u00d1 There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? T \u00de\u00d1 There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 9. VA \u00de\u00d1 6 qPrompt 6 \u27a5Q \u00de\u00d1 Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? T \u00de\u00d1 Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 50. After eating 35, they had 74 - 35 = 25. VA \u00de\u00d1 39 qPrompt 7 \u27a5Q \u00de\u00d1 Olivia has $23. She bought five bagels for $3 each. How much money does she have left? T \u00de\u00d1 Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 29 dollars. So she has 23 - 15 dollars left. 23 - 15 is 18. VA \u00de\u00d1 8 qPrompt 8 \u27a5Q \u00de\u00d1 Michael had 58 golf balls. On Tuesday, he lost 23 golf balls. On Wednesday, he lost 2 more. How many golf balls did he have at the end of Wednesday? T \u00de\u00d1 Michael started with 58 golf balls. After losing 23 on Tuesday, he had 58 - 23 = 15. After losing 2 more, he had 35 - 2 = 17 golf balls. VA \u00de\u00d1 33\nTable 38: SPORTS with non-sport names.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJamal Murray was perfect from the line.\u201d T \u00de\u00d1 Adair Foster is a basketball player. Being perfect from the line is part of basketball. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJoao Moutinho caught the screen pass in the NFC championship.\u201d T \u00de\u00d1 Scottie Colby is a soccer player. The NFC championship is part of American football, not soccer. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJonas Valanciunas beat the buzzer.\u201d T \u00de\u00d1 Chas Jase is a basketball player. Beating the buzzer is part of basketball. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cSam Darnold passed the puck.\u201d T \u00de\u00d1 Denny Dillan is a American football player. Passing the puck is part of hockey, not American football. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cKyle Palmieri was called for slashing.\u201d T \u00de\u00d1 Myron Storm is a hockey player. Being called for slashing is part of hockey. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cCarson Wentz set the pick and roll.\u201d T \u00de\u00d1 Cliff Kristopher is an American football player. Pick and roll is part of basketball, not football. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cMalcolm Brogdon banked the shot in.\u201d T \u00de\u00d1 Celestine Holden is a basketball player. Banking the shot in is part of basketball. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cDraymond Green threw a touchdown.\u201d T \u00de\u00d1 Peter Abraham is an basketball player. Throwing a touchdown is part of football, not basketball. VA \u00de\u00d1 no\nTable 39: GSM-8K with no equations.\nqPrompt 1 \u27a5Q \u00de\u00d1 Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? T \u00de\u00d1 Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. VA \u00de\u00d1 9 qPrompt 2 \u27a5Q \u00de\u00d1 If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? T \u00de\u00d1 There are originally 3 cars. 2 more cars arrive. VA \u00de\u00d1 5 qPrompt 3 \u27a5Q \u00de\u00d1 Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny? T \u00de\u00d1 Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny VA \u00de\u00d1 8 qPrompt 4 \u27a5Q \u00de\u00d1 There were nine computers in the server room. Five more computers were installed each day, from Monday to Thursday. How many computers are now in the server room? T \u00de\u00d1 There were originally 9 computers. For each of 4 days, 5 more computers were added. So computers were added. VA \u00de\u00d1 29 qPrompt 5 \u27a5Q \u00de\u00d1 There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? T \u00de\u00d1 There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been. VA \u00de\u00d1 6 qPrompt 6 \u27a5Q \u00de\u00d1 Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? T \u00de\u00d1 Originally, Leah had 32 chocolates. Her sister had 42. So in total they had. After eating 35, they had. VA \u00de\u00d1 39 qPrompt 7 \u27a5Q \u00de\u00d1 Olivia has $23. She bought five bagels for $3 each. How much money does she have left? T \u00de\u00d1 Olivia had 23 dollars. 5 bagels for 3 dollars each will be dollars. So she has dollars left. VA \u00de\u00d1 8 qPrompt 8 \u27a5Q \u00de\u00d1 Michael had 58 golf balls. On Tuesday, he lost 23 golf balls. On Wednesday, he lost 2 more. How many golf balls did he have at the end of Wednesday? T \u00de\u00d1 Michael started with 58 golf balls. After losing 23 on Tuesday, he had. After losing 2 more, he had golf balls. VA \u00de\u00d1 33\nTable 40: GSM-8K with only equations in thoughts.\nqPrompt 1 \u27a5Q \u00de\u00d1 Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? T \u00de\u00d1 5 + (2 * 2) = 9. VA \u00de\u00d1 9 qPrompt 2 \u27a5Q \u00de\u00d1 If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? T \u00de\u00d1 3 + 2 = 5. VA \u00de\u00d1 5 qPrompt 3 \u27a5Q \u00de\u00d1 Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny? T \u00de\u00d1 20 - 12 = 8. VA \u00de\u00d1 8 qPrompt 4 \u27a5Q \u00de\u00d1 There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room? T \u00de\u00d1 9 + (5 * 4) = 29. VA \u00de\u00d1 29 qPrompt 5 \u27a5Q \u00de\u00d1 There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? T \u00de\u00d1 21 - 15 = 6. VA \u00de\u00d1 6 qPrompt 6 \u27a5Q \u00de\u00d1 Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? T \u00de\u00d1 32 + 42 - 35 = 39. VA \u00de\u00d1 39 qPrompt 7 \u27a5Q \u00de\u00d1 Olivia has $23. She bought five bagels for $3 each. How much money does she have left? T \u00de\u00d1 23 - (5 * 3) = 8. VA \u00de\u00d1 8 qPrompt 8 \u27a5Q \u00de\u00d1 Michael had 58 golf balls. On Tuesday, he lost 23 golf balls. On Wednesday, he lost 2 more. How many golf balls did he have at the end of Wednesday? T \u00de\u00d1 58 - 23 - 2 = 33. VA \u00de\u00d1 33\nTable 41: SPORTS with only patterns in thoughts.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJamal Murray was perfect from the line.\u201d T \u00de\u00d1 Both are a part of the same sport. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJoao Moutinho caught the screen pass in the NFC championship.\u201d T \u00de\u00d1 Both are a part of different sports. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJonas Valanciunas beat the buzzer.\u201d T \u00de\u00d1 Both are a part of the same sport. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cSam Darnold passed the puck.\u201d T \u00de\u00d1 Both are a part of different sports. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cKyle Palmieri was called for slashing.\u201d T \u00de\u00d1 Both are a part of the same sport. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cCarson Wentz set the pick and roll.\u201d T \u00de\u00d1 Both are a part of different sports. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cMalcolm Brogdon banked the shot in.\u201d T \u00de\u00d1 Both are a part of the same sport. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cDraymond Green threw a touchdown.\u201d T \u00de\u00d1 Both are a part of different sports. VA \u00de\u00d1 no\nTable 42: DATE with pattern only thoughts.\nqPrompt 1 \u27a5Q \u00de\u00d1 It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 Today = 04/19/1969. 24 hours = 1 day. 04/19/1969 + 1 = 04/20/1969. VA \u00de\u00d1 04/20/1969 qPrompt 2 \u27a5Q \u00de\u00d1 The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY? T \u00de\u00d1 06/01/1943 + 1 day = 06/02/1943. Today = 06/02/1943. Today - 10 days = 05/23/1943. VA \u00de\u00d1 05/23/1943 qPrompt 3 \u27a5Q \u00de\u00d1 The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY? T \u00de\u00d1 First day of 2019 = 01/01/2019. 01/01/2019 = Tuesday. Today = first Monday. Tuesday - Monday = 6 days. So Today 01/01/2019 + 6 days = 01/07/2019. VA \u00de\u00d1 01/07/2019 qPrompt 4 \u27a5Q \u00de\u00d1 Jane was born on the last day of February in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY? T \u00de\u00d1 Last day of February = 28th. 16-year old birthday => +16 years. Today = 02/28/2001 + 16 years = 02/28/2017. Yesterday = 02/28/2017 - 1 = 02/27/2017. VA \u00de\u00d1 02/27/2017 qPrompt 5 \u27a5Q \u00de\u00d1 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY? T \u00de\u00d1 36 hours = 2 days. 2 days before = 01/01/2015 - 2 = 12/30/2014. Today = 12/30/2014. Today + 1 week = 01/05/2015. VA \u00de\u00d1 01/05/2015 qPrompt 6 \u27a5Q \u00de\u00d1 Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 Today = 03/12/2002. 24 hours = 1 day. 3/12/2002 + 1 day = 03/13/2002. VA \u00de\u00d1 03/13/2002\nTable 43: SORTING with no patterns.\nqPrompt 1 \u27a5Q \u00de\u00d1 72 , 85 , 48 , 11 , 23 , 95 , 34 , 63 , 56 T \u00de\u00d1 11 < 23 < 34 < 48 < 56 < 63 < 72 < 85 < 95 VA \u00de\u00d1 11 , 23 , 34 , 48 , 56 , 63 , 72 , 85 , 95 qPrompt 2 \u27a5Q \u00de\u00d1 \u03bd , \u03b9 , \u03bb , \u03b6 , \u03be , \u03b3 , \u03c8 , \u03c0 T \u00de\u00d1 \u03be < \u03b6 < \u03bb < \u03c0 < \u03c8 < \u03b3 < \u03b9 < \u03bd VA \u00de\u00d1 \u03be , \u03b6 , \u03bb , \u03c0 , \u03c8 , \u03b3 , \u03b9 , \u03bd qPrompt 3 \u27a5Q \u00de\u00d1 6 , 5 , 7 , 4 , 3 , 2 , 8 , 1 T \u00de\u00d1 8 > 7 > 6 > 5 > 4 > 3 > 2 > 1 VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 4 \u27a5Q \u00de\u00d1 1 , 6 , 4 , 8 , 5 , 3 , 7 , 2 T \u00de\u00d1 def list_sort(array): return sorted(array) VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 5 \u27a5Q \u00de\u00d1 5 , 2 , 1 , 4 , 3 , 7 T \u00de\u00d1 First number is 1. Next we have 2, since 1 is less than 2. Next we have 3, since 2 is less than 3. Next we have 4, since 3 is less than 4. Next we have 5, since 4 is less than 5. Next we have 7, since 5 is less than 7. VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 7 qPrompt 6 \u27a5Q \u00de\u00d1 3 , 8 , 2 , 5 , 6 , 4 , 7 , 1 T \u00de\u00d1 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 7 \u27a5Q \u00de\u00d1 8 , 6 , 1 , 2 , 9 , 7 , 4 T \u00de\u00d1 Last number is 9. Next largest is 8, since 9 is more than 8. Next largest is 7, since 8 is more than 7. Next largest is 6, since 7 is more than 6. Next largest is 4, since 6 is more than 4. Next largest is 2, since 4 is more than 2. Next largest is 1, since 2 is more than 1. VA \u00de\u00d1 1 , 2 , 4 , 6 , 7 , 8 , 9 qPrompt 8 \u27a5Q \u00de\u00d1 7 , 6 , 8 , 1 T \u00de\u00d1 8 > 7 > 6 > 1 VA \u00de\u00d1 1 , 6 , 7 , 8\nTable 44: DATE with no patterns.\nqPrompt 1 \u27a5Q \u00de\u00d1 It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 Today is 04/19/1969. VA \u00de\u00d1 04/20/1969 qPrompt 2 \u27a5Q \u00de\u00d1 The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY? T \u00de\u00d1 10 days before today is 05/23/1943. VA \u00de\u00d1 05/23/1943 qPrompt 3 \u27a5Q \u00de\u00d1 The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY? T \u00de\u00d1 If the first day of 2019 was Tuesday, then 01/01/2019 was a Tuesday. VA \u00de\u00d1 01/07/2019 qPrompt 4 \u27a5Q \u00de\u00d1 Jane was born on the last day of February in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY? T \u00de\u00d1 Today is her 16-year old birthday, so today is 02/28/2017. So yesterday was 02/27/2017. VA \u00de\u00d1 02/27/2017 qPrompt 5 \u27a5Q \u00de\u00d1 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY? T \u00de\u00d1 If 2015 is coming in 36 hours, then it is coming in 2 days. VA \u00de\u00d1 01/05/2015 qPrompt 6 \u27a5Q \u00de\u00d1 Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 So the date 24 hours later will be 03/13/2002. VA \u00de\u00d1 03/13/2002\nTable 45: SPORTS with no patterns.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJamal Murray was perfect from the line.\u201d T \u00de\u00d1 Jamal Murray is a basketball player. Being perfect from the line is part of basketball. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJoao Moutinho caught the screen pass in the NFC championship.\u201d T \u00de\u00d1 Joao Moutinho and the NFC championship are not both part of American football. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJonas Valanciunas beat the buzzer.\u201d T \u00de\u00d1 Beating the buzzer, Jonas Valanciunas, and basketball player are related. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cSam Darnold passed the puck.\u201d T \u00de\u00d1 Both are a part of different sports. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cKyle Palmieri was called for slashing.\u201d T \u00de\u00d1 Being called for slashing is Ice hockey too. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cCarson Wentz set the pick and roll.\u201d T \u00de\u00d1 One of them is related to American football player, the other to basketball. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cMalcolm Brogdon banked the shot in.\u201d T \u00de\u00d1 They seem to be related. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cDraymond Green threw a touchdown.\u201d T \u00de\u00d1 The person and the activity are unrelated to each other. VA \u00de\u00d1 no\nTable 46: SPORTS with wrong thoughts.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJamal Murray was perfect from the line.\u201d T \u00de\u00d1 Jamal Murray is a soccer player. Being perfect from the line is part of soccer. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJoao Moutinho caught the screen pass in the NFC championship.\u201d T \u00de\u00d1 Joao Moutinho is a basketball player. The NFC championship is part of American football, not basketball. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJonas Valanciunas beat the buzzer.\u201d T \u00de\u00d1 Jonas Valanciunas is an American football player. Beating the buzzer is part of American football. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cSam Darnold passed the puck.\u201d T \u00de\u00d1 Sam Darnold is a basketball player. Passing the puck is part of American football, not basketball. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cKyle Palmieri was called for slashing.\u201d T \u00de\u00d1 Kyle Palmieri is an American football player. Being called for slashing is part of American football. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cCarson Wentz set the pick and roll.\u201d T \u00de\u00d1 Carson Wentz is a hockey player. Pick and roll is part of football, not hockey. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cMalcolm Brogdon banked the shot in.\u201d T \u00de\u00d1 Malcolm Brogdon is a hockey player. Banking the shot in is part of hockey. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cDraymond Green threw a touchdown.\u201d T \u00de\u00d1 Draymond Green is an American football player. Throwing a touchdown is part of hockey, not American football. VA \u00de\u00d1 no\nTable 47: SORTING with wrong thoughts.\nqPrompt 1 \u27a5Q \u00de\u00d1 7 , 8 , 4 , 1 , 2 , 9 , 3 , 6 , 5 T \u00de\u00d1 1 < 2 < 3 < 4 < 7 < 6 < 5 < 8 < 9 VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 qPrompt 2 \u27a5Q \u00de\u00d1 5 , 9 , 3 , 1 , 8 , 4 , 6 , 2 T \u00de\u00d1 1 < 2 < 3 < 4 < 5 < 8 < 6 < 9 VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 8 , 9 qPrompt 3 \u27a5Q \u00de\u00d1 6 , 5 , 7 , 4 , 3 , 2 , 8 , 1 T \u00de\u00d1 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 4 \u27a5Q \u00de\u00d1 1 , 6 , 4 , 8 , 5 , 3 , 7 , 2 T \u00de\u00d1 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 5 \u27a5Q \u00de\u00d1 5 , 2 , 1 , 4 , 3 , 7 T \u00de\u00d1 1 < 2 < 3 < 4 < 5 < 7 VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 7 qPrompt 6 \u27a5Q \u00de\u00d1 3 , 8 , 2 , 5 , 6 , 4 , 7 , 1 T \u00de\u00d1 6 < 2 < 3 < 4 < 5 < 1 < 7 < 8 VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 7 \u27a5Q \u00de\u00d1 8 , 6 , 1 , 2 , 9 , 7 , 4 T \u00de\u00d1 1 < 2 < 4 < 8 < 7 < 6 < 9 VA \u00de\u00d1 1 , 2 , 4 , 6 , 7 , 8 , 9 qPrompt 8 \u27a5Q \u00de\u00d1 7 , 6 , 8 , 1 T \u00de\u00d1 8 < 6 < 7 < 1 VA \u00de\u00d1 1 , 6 , 7 , 8\nTable 48: SORTING when all the examples start with number two.\nqPrompt 1 \u27a5Q \u00de\u00d1 7 , 8 , 4 , 2 , 9 , 3 , 6 , 5 T \u00de\u00d1 2 < 3 < 4 < 5 < 6 < 7 < 8 < 9 VA \u00de\u00d1 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 qPrompt 2 \u27a5Q \u00de\u00d1 5 , 9 , 3 , 8 , 4 , 6 , 2 T \u00de\u00d1 2 < 3 < 4 < 5 < 6 < 8 < 9 VA \u00de\u00d1 2 , 3 , 4 , 5 , 6 , 8 , 9 qPrompt 3 \u27a5Q \u00de\u00d1 6 , 5 , 7 , 4 , 3 , 2 , 8 T \u00de\u00d1 2 < 3 < 4 < 5 < 6 < 7 < 8 VA \u00de\u00d1 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 4 \u27a5Q \u00de\u00d1 1 , 6 , 4 , 8 , 5 , 3 , 7 T \u00de\u00d1 2 < 3 < 4 < 5 < 6 < 7 < 8 VA \u00de\u00d1 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 5 \u27a5Q \u00de\u00d1 5 , 2 , 4 , 3 , 7 T \u00de\u00d1 2 < 3 < 4 < 5 < 7 VA \u00de\u00d1 2 , 3 , 4 , 5 , 7 qPrompt 6 \u27a5Q \u00de\u00d1 3 , 8 , 2 , 5 , 6 , 4 , 7 T \u00de\u00d1 2 < 3 < 4 < 5 < 6 < 7 < 8 VA \u00de\u00d1 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 7 \u27a5Q \u00de\u00d1 8 , 6 , 2 , 9 , 7 , 4 T \u00de\u00d1 2 < 4 < 6 < 7 < 8 < 9 VA \u00de\u00d1 2 , 4 , 6 , 7 , 8 , 9 qPrompt 8 \u27a5Q \u00de\u00d1 7 , 6 , 8 , 2 T \u00de\u00d1 2 < 6 < 7 < 8 VA \u00de\u00d1 2 , 6 , 7 , 8\nTable 49: SORTING when some of the examples start with number two.\nqPrompt 1 \u27a5Q \u00de\u00d1 7 , 8 , 4 , 2 , 9 , 3 , 6 , 5 T \u00de\u00d1 2 < 3 < 4 < 5 < 6 < 7 < 8 < 9 VA \u00de\u00d1 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 qPrompt 2 \u27a5Q \u00de\u00d1 5 , 9 , 3 , 8 , 4 , 6 , 2 T \u00de\u00d1 2 < 3 < 4 < 5 < 6 < 8 < 9 VA \u00de\u00d1 2 , 3 , 4 , 5 , 6 , 8 , 9 qPrompt 3 \u27a5Q \u00de\u00d1 6 , 5 , 7 , 4 , 3 , 2 , 8 T \u00de\u00d1 2 < 3 < 4 < 5 < 6 < 7 < 8 VA \u00de\u00d1 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 4 \u27a5Q \u00de\u00d1 1 , 6 , 4 , 8 , 5 , 3 , 7 , 2 T \u00de\u00d1 1 < 2 < 3 < 4 < 5 < 6 < 7 < 8 VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 5 \u27a5Q \u00de\u00d1 5 , 2 , 1 , 4 , 3 , 7 T \u00de\u00d1 1 < 2 < 3 < 4 < 5 < 7 VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 7 qPrompt 6 \u27a5Q \u00de\u00d1 3 , 8 , 2 , 5 , 6 , 4 , 7 T \u00de\u00d1 2 < 3 < 4 < 5 < 6 < 7 < 8 VA \u00de\u00d1 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 7 \u27a5Q \u00de\u00d1 8 , 6 , 1 , 2 , 9 , 7 , 4 T \u00de\u00d1 1 < 2 < 4 < 6 < 7 < 8 < 9 VA \u00de\u00d1 1 , 2 , 4 , 6 , 7 , 8 , 9 qPrompt 8 \u27a5Q \u00de\u00d1 7 , 6 , 8 , 1 T \u00de\u00d1 1 < 6 < 7 < 8 VA \u00de\u00d1 1 , 6 , 7 , 8\nTable 50: GSM-8K with replacing names and objects with different entities of same type.\nqPrompt 1 \u27a5Q \u00de\u00d1 Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? T \u00de\u00d1 Teddy started with 5 cookies. If he got 2 cookies each from his Jenna and Rehan, then that is 4 more cookies. 5 + 4 = 9. VA \u00de\u00d1 9 qPrompt 2 \u27a5Q \u00de\u00d1 If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? T \u00de\u00d1 There are originally 3 trains. 2 more trains arrive. 3 + 2 = 5. VA \u00de\u00d1 5 qPrompt 3 \u27a5Q \u00de\u00d1 Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny? T \u00de\u00d1 Peet started with 20 ice creams. Then he had 12 after giving some to Andrei. So he gave Andrei 20 - 12 = 8. VA \u00de\u00d1 8 qPrompt 4 \u27a5Q \u00de\u00d1 There were nine computers in the server room. Five more computers were installed each day, from Monday to Thursday. How many computers are now in the server room? T \u00de\u00d1 There were originally 9 cars. For each of 4 days, 5 more cars were added. So 5 * 4 = 20 cars were added. 9 + 20 is 29. VA \u00de\u00d1 29 qPrompt 5 \u27a5Q \u00de\u00d1 There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? T \u00de\u00d1 There are 15 wires originally. Then there were 21 wires after some more were planted. So there must have been 21 - 15 = 6. VA \u00de\u00d1 6 qPrompt 6 \u27a5Q \u00de\u00d1 Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? T \u00de\u00d1 Originally, Christy had 32 apples. Her neighbor had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. VA \u00de\u00d1 39 qPrompt 7 \u27a5Q \u00de\u00d1 Olivia has $23. She bought five bagels for $3 each. How much money does she have left? T \u00de\u00d1 Lulu had 23 dollars. 5 pillows for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. VA \u00de\u00d1 8 qPrompt 8 \u27a5Q \u00de\u00d1 Michael had 58 golf balls. On Tuesday, he lost 23 golf balls. On Wednesday, he lost 2 more. How many golf balls did he have at the end of Wednesday? T \u00de\u00d1 Norton started with 58 cars. After losing 23 on Tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 cars. VA \u00de\u00d1 33\nTable 51: SPORTS after replacing sports person with a random name and sports activity with a non-sport activity.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJamal Murray was perfect from the line.\u201d T \u00de\u00d1 Adair Foster is a basketball player. Juggling the paper cups is part of basketball. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJoao Moutinho caught the screen pass in the NFC championship.\u201d T \u00de\u00d1 Scottie Colby is a soccer player. The NFC championship is part of American football, not soccer. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJonas Valanciunas beat the buzzer.\u201d T \u00de\u00d1 Chas Jase is a basketball player. Beating the pillow is part of basketball. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cSam Darnold passed the puck.\u201d T \u00de\u00d1 Denny Dillan is a American football player. Passing the soda is part of hockey, not American football. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cKyle Palmieri was called for slashing.\u201d T \u00de\u00d1 Myron Storm is a hockey player. Being called for trashing is part of hockey. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cCarson Wentz set the pick and roll.\" T \u00de\u00d1 Cliff Kristopher is an American football player. Tick and floor is part of basketball, not football. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cMalcolm Brogdon banked the shot in.\u201d T \u00de\u00d1 Celestine Holden is a basketball player. Banking the pot in is part of basketball. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cDraymond Green threw a touchdown.\u201d T \u00de\u00d1 Peter Abraham is an basketball player. Throwing a tantrum is part of football, not basketball. VA \u00de\u00d1 no\nTable 52: GSM-8K with examples in YodaSpeak.\nqPrompt 1 \u27a5Q \u00de\u00d1 Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? T \u00de\u00d1 With 5 toys, Shawn started. 2 toys each from his mom and dad, if he got, then that is 4 more toys. 5 + 4 = 9. VA \u00de\u00d1 9 qPrompt 2 \u27a5Q \u00de\u00d1 If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? T \u00de\u00d1 Originally 3 cars, there are. 2 more cars arrive. 3 + 2 = 5. VA \u00de\u00d1 5 qPrompt 3 \u27a5Q \u00de\u00d1 Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny? T \u00de\u00d1 With 20 lollipops, Jason started. 12 after giving some to Denny, then he had. So he gave Denny 20 - 12 = 8. VA \u00de\u00d1 8 qPrompt 4 \u27a5Q \u00de\u00d1 There were nine computers in the server room. Five more computers were installed each day, from Monday to Thursday. How many computers are now in the server room? T \u00de\u00d1 Originally 9 computers, there were. For each of 4 days, added, 5 more computers were. So 5 * 4 = 20 computers were added. 9 + 20 is 29. VA \u00de\u00d1 29 qPrompt 5 \u27a5Q \u00de\u00d1 There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? T \u00de\u00d1 15 trees originally, there are. 21 trees after some more were planted, then there were. Been 21 - 15 = 6, so there must have. VA \u00de\u00d1 6 qPrompt 6 \u27a5Q \u00de\u00d1 Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? T \u00de\u00d1 Originally,32 chocolates, Leah had. 42, her sister had. 32 + 42 = 74, so in total they had. After eating 35,74 - 35 = 39, they had. VA \u00de\u00d1 39 qPrompt 7 \u27a5Q \u00de\u00d1 Olivia has $23. She bought five bagels for $3 each. How much money does she have left? T \u00de\u00d1 23 dollars, Olivia had. 5 x 3 = 15 dollars, 5 bagels for 3 dollars each will be. 23 - 15 dollars left, so she has. 23 - 15 is 8. VA \u00de\u00d1 8 qPrompt 8 \u27a5Q \u00de\u00d1 Michael had 58 golf balls. On Tuesday, he lost 23 golf balls. On Wednesday, he lost 2 more. How many golf balls did he have at the end of Wednesday? T \u00de\u00d1 With 58 golf balls, Michael started. After losing 23 on Tuesday,58 - 23 = 35, he had. After losing 2 more,35 - 2 = 33 golf balls, he had. VA \u00de\u00d1 33\nTable 53: SPORTS with examples in YodaSpeak.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJamal Murray was perfect from the line.\u201d T \u00de\u00d1 A basketball player Jamal Murray is. Perfect from the line is part of basketball being. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJoao Moutinho caught the screen pass in the NFC championship.\u201d T \u00de\u00d1 A soccer player Joao Moutinho is. Part of American football the NFC championship is, not soccer. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJonas Valanciunas beat the buzzer.\u201d T \u00de\u00d1 A basketball player Jonas Valanciunas is. The buzzer is part of basketball beating. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cSam Darnold passed the puck.\u201d T \u00de\u00d1 An American football player Sam Darnold is. Part of hockey passing the puck is, not American football. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cKyle Palmieri was called for slashing.\u201d T \u00de\u00d1 A hockey player Kyle Palmieri is. Called for slashing is part of hockey being. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cCarson Wentz set the pick and roll.\u201d T \u00de\u00d1 An American football player Carson Wentz is. Part of basketball pick and roll is, not football. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cMalcolm Brogdon banked the shot in.\u201d T \u00de\u00d1 A basketball player Malcolm Brogdon is. In is part of basketball banking the shot. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cDraymond Green threw a touchdown.\u201d T \u00de\u00d1 An basketball player Draymond Green is. A touchdown is part of football throwing, not basketball. VA \u00de\u00d1 no\nTable 54: DATE with thoughts in YodaSpeak.\nqPrompt 1 \u27a5Q \u00de\u00d1 It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 04/19/1969, today is. Later is one day after today, 24 hours, 04/20/1969, which would be. VA \u00de\u00d1 04/20/1969 qPrompt 2 \u27a5Q \u00de\u00d1 The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY? T \u00de\u00d1 06/02/1943, one day after 06/01/1943 is, 06/02/1943, so today is. 05/23/1943, 10 days before today is. VA \u00de\u00d1 05/23/1943 qPrompt 3 \u27a5Q \u00de\u00d1 The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY? T \u00de\u00d1 Tuesday, if the first day of 2019 was, a Tuesday, then 01/01/2019 was. The first Monday, today is, six days later, would be. 01/07/2019, so today is. VA \u00de\u00d1 01/07/2019 qPrompt 4 \u27a5Q \u00de\u00d1 Jane was born on the last day of February in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY? T \u00de\u00d1 The 28th, the last day of February is, Born on 02/28/2001, so Jane was. Her 16-year old birthday, today is,02/28/2017, so today is. 02/27/2017, so yesterday was. VA \u00de\u00d1 02/27/2017 qPrompt 5 \u27a5Q \u00de\u00d1 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY? T \u00de\u00d1 Coming in 36 hours, if 2015 is, coming in 2 days, then it is. 12/30/2014, 2 days before 01/01/2015 is, 12/30/2014, so today is. 01/05/2015, so one week from today will be. VA \u00de\u00d1 01/05/2015 qPrompt 6 \u27a5Q \u00de\u00d1 Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 03/12/2002, today is. Later will be 03/13/2002, so the date 24 hours. VA \u00de\u00d1 03/13/2002\nTable 55: GSM-8K with examples in YodaSpeak.\nqPrompt 1 \u27a5Q \u00de\u00d1 Five toys, shawn has. For Christmas, two toys each from his mom and dad, he got. He have now, how many toys does? T \u00de\u00d1 With 5 toys, Shawn started. 2 toys each from his mom and dad, if he got, then that is 4 more toys. 5 + 4 = 9. VA \u00de\u00d1 9 qPrompt 2 \u27a5Q \u00de\u00d1 3 cars in the parking lot and 2 more cars arrive, if there are,In the parking lot, how many cars are? T \u00de\u00d1 Originally 3 cars, there are. 2 more cars arrive. 3 + 2 = 5. VA \u00de\u00d1 5 qPrompt 3 \u27a5Q \u00de\u00d1 20 lollipops, Jason had. Denny some lollipops, he gave. 12 lollipops, now Jason has. Jason give to Denny, how many lollipops did? T \u00de\u00d1 With 20 lollipops, Jason started. 12 after giving some to Denny, then he had. So he gave Denny 20 - 12 = 8. VA \u00de\u00d1 8 qPrompt 4 \u27a5Q \u00de\u00d1 Nine computers in the server room, there were. Installed each day, five more computers were, from Monday to Thursday. Now in the server room, how many computers are? T \u00de\u00d1 Originally 9 computers, there were. For each of 4 days, added, 5 more computers were. So 5 * 4 = 20 computers were added. 9 + 20 is 29. VA \u00de\u00d1 29 qPrompt 5 \u27a5Q \u00de\u00d1 15 trees in the grove, there are. Grove workers will plant trees in the grove today. Done, after they are,21 trees, there will be. The grove workers plant today, how many trees did? T \u00de\u00d1 15 trees originally, there are. 21 trees after some more were planted, then there were. Been 21 - 15 = 6, so there must have. VA \u00de\u00d1 6 qPrompt 6 \u27a5Q \u00de\u00d1 32 chocolates and her sister had 42, Leah had. 35, if they ate, they have left in total, how many pieces do? T \u00de\u00d1 Originally, 32 chocolates, Leah had. 42, her sister had. 32 + 42 = 74, so in total they had. After eating 35,74 - 35 = 39, they had. VA \u00de\u00d1 39 qPrompt 7 \u27a5Q \u00de\u00d1 $23, Olivia has. Five bagels for $3 each, she bought. She have left, how much money does? T \u00de\u00d1 23 dollars, Olivia had. 5 x 3 = 15 dollars, 5 bagels for 3 dollars each will be. 23 - 15 dollars left, so she has. 23 - 15 is 8. VA \u00de\u00d1 8 qPrompt 8 \u27a5Q \u00de\u00d1 58 golf balls, Michael had. On Tuesday,23 golf balls, he lost. On Wednesday,2 more, he lost. He have at the end of Wednesday, how many golf balls did? T \u00de\u00d1 With 58 golf balls, Michael started. After losing 23 on Tuesday,58 - 23 = 35, he had. After losing 2 more,35 - 2 = 33 golf balls, he had. VA \u00de\u00d1 33\nTable 56: SPORTS with examples in YodaSpeak.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPerfect from the line Jamal Murray was.\u201d T \u00de\u00d1 A basketball player Jamal Murray is. Perfect from the line is part of basketball being. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cThe screen pass in the NFC championship Joao Moutinho caught.\u201d T \u00de\u00d1 A soccer player Joao Moutinho is. Part of American football the NFC championship is, not soccer. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cThe buzzer Jonas Valanciunas beat.\u201d T \u00de\u00d1 A basketball player Jonas Valanciunas is. The buzzer is part of basketball beating. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPassed the puck, Sam Darnold did.\u201d T \u00de\u00d1 An American football player Sam Darnold is. Part of hockey passing the puck is, not American football. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cCalled for slashing Kyle Palmieri was.\u201d T \u00de\u00d1 A hockey player Kyle Palmieri is. Called for slashing is part of hockey being. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cThe pick and roll Carson Wentz set.\u201d T \u00de\u00d1 An American football player Carson Wentz is. Part of basketball pick and roll is, not football. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 is the following sentence plausible? In \u201cMalcolm Brogdon banked the shot.\u201d T \u00de\u00d1 A basketball player Malcolm Brogdon is. In is part of basketball banking the shot. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 is the following sentence plausible? A touchdown \u201cDraymond Green threw.\u201d T \u00de\u00d1 An basketball player Draymond Green is. A touchdown is part of football throwing, not basketball. VA \u00de\u00d1 no\nTable 57: DATE with examples in YodaSpeak.\nqPrompt 1 \u27a5Q \u00de\u00d1 4/19/1969 today it is. The date 24 hours later in MM/DD/YYYY what is? T \u00de\u00d1 04/19/1969, today is. Later is one day after today, 24 hours, 04/20/1969, which would be. VA \u00de\u00d1 04/20/1969 qPrompt 2 \u27a5Q \u00de\u00d1 Scheduled to be on 06/01/1943 the concert was, delayed by one day to today but was. The date 10 days ago in MM/DD/YYYY what is? T \u00de\u00d1 06/02/1943, one day after 06/01/1943 is, 06/02/1943, so today is. 05/23/1943, 10 days before today is. VA \u00de\u00d1 05/23/1943 qPrompt 3 \u27a5Q \u00de\u00d1 A Tuesday the first day of 2019 is, the first Monday of 2019 and today is. The date today in MM/DD/YYYY what is? T \u00de\u00d1 Tuesday, if the first day of 2019 was, a Tuesday, then 01/01/2019 was. The first Monday, today is, six days later, would be. 01/07/2019, so today is. VA \u00de\u00d1 01/07/2019 qPrompt 4 \u27a5Q \u00de\u00d1 Born on the last day of February in 2001 Jane was. Her 16-year-old birthday today is. The date yesterday in MM/DD/YYYY what is? T \u00de\u00d1 The 28th, the last day of February is, Born on 02/28/2001, so Jane was. Her 16-year old birthday, today is,02/28/2017, so today is. 02/27/2017, so yesterday was. VA \u00de\u00d1 02/27/2017 qPrompt 5 \u27a5Q \u00de\u00d1 Coming in 36 hours 2015 is. The date one week from today in MM/DD/YYYY what is? Yes. T \u00de\u00d1 Coming in 36 hours, if 2015 is, coming in 2 days, then it is. 12/30/2014, 2 days before 01/01/2015 is, 12/30/2014, so today is. 01/05/2015, so one week from today will be. VA \u00de\u00d1 01/05/2015 qPrompt 6 \u27a5Q \u00de\u00d1 Today is 3/11/2002 Jane thought, in fact mar 12 but today is, 1 day later which is. The date 24 hours later in MM/DD/YYYY what is? T \u00de\u00d1 03/12/2002, today is. Later will be 03/13/2002, so the date 24 hours. VA \u00de\u00d1 03/13/2002\nTable 58: GSM-8K with random thoughts.\nqPrompt 1 \u27a5Q \u00de\u00d1 Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? T \u00de\u00d1 Initially, Steve is 66 inches tall. After growing 6 inches, Steve is 66 + 6 = 72 inches tall. VA \u00de\u00d1 9 qPrompt 2 \u27a5Q \u00de\u00d1 If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? T \u00de\u00d1 Sandra eats 3 beignets every morning and there are 7 days in a week so she eats 3 * 7 = 21 beignets in a week. VA \u00de\u00d1 5 qPrompt 3 \u27a5Q \u00de\u00d1 Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny? T \u00de\u00d1 The quarter of the number is 1, thus the number is 1 * 4 = 4. VA \u00de\u00d1 8 qPrompt 4 \u27a5Q \u00de\u00d1 There were nine computers in the server room. Five more computers were installed each day, from Monday to Thursday. How many computers are now in the server room? T \u00de\u00d1 Half of 10 is 10 / 2 = 5. Five more than 5 is 5 + 5 = 10. VA \u00de\u00d1 29 qPrompt 5 \u27a5Q \u00de\u00d1 There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? T \u00de\u00d1 Angie\u2019s age minus 4 is 20 - 4 = 16. Thus, Angie\u2019s age is 16. VA \u00de\u00d1 6 qPrompt 6 \u27a5Q \u00de\u00d1 Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? T \u00de\u00d1 A total of 20 passengers are in two buses. Thus, one bus can fit 20 passengers / 2 buses = 10 passengers. VA \u00de\u00d1 39 qPrompt 7 \u27a5Q \u00de\u00d1 Olivia has $23. She bought five bagels for $3 each. How much money does she have left? T \u00de\u00d1 2 bags weight 2 pounds, so one bag weights 2 pounds / 2 = 1 pound. So 8 bags of oranges would weigh 8 * 1 pound = 8 pounds. VA \u00de\u00d1 8 qPrompt 8 \u27a5Q \u00de\u00d1 Michael had 58 golf balls. On Tuesday, he lost 23 golf balls. On Wednesday, he lost 2 more. How many golf balls did he have at the end of Wednesday? T \u00de\u00d1 John takes 2 pills a day. In a week, John therefore takes 2 * 7 = 14 pills. VA \u00de\u00d1 33\nTable 59: SPORTS with random thoughts.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJamal Murray was perfect from the line.\u201d T \u00de\u00d1 Sam Darnold is a American football player. Passing the puck is part of hockey, not American football. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJoao Moutinho caught the screen pass in the NFC championship.\u201d T \u00de\u00d1 Draymond Green is an basketball player. Throwing a touchdown is part of football, not basketball. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJonas Valanciunas beat the buzzer.\u201d T \u00de\u00d1 Carson Wentz is an American football player. Pick and roll is part of basketball, not football. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cSam Darnold passed the puck.\u201d T \u00de\u00d1 Jamal Murray is a basketball player. Being perfect from the line is part of basketball. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cKyle Palmieri was called for slashing.\u201d T \u00de\u00d1 Joao Moutinho is a soccer player. The NFC championship is part of American football, not soccer. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cCarson Wentz set the pick and roll.\u201d T \u00de\u00d1 Malcolm Brogdon is a basketball player. Banking the shot in is part of basketball. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cMalcolm Brogdon banked the shot in.\u201d T \u00de\u00d1 Jonas Valanciunas is a basketball player. Beating the buzzer is part of basketball. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cDraymond Green threw a touchdown.\u201d T \u00de\u00d1 Kyle Palmieri is a hockey player. Being called for slashing is part of hockey. VA \u00de\u00d1 no\nTable 60: DATE with random thoughts.\nqPrompt 1 \u27a5Q \u00de\u00d1 It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 The last day of February is the 28th, so Jane was born on 02/28/2001. Today is her 16-year old birthday, so today is 02/28/2017. So yesterday was 02/27/2017. VA \u00de\u00d1 04/20/1969 qPrompt 2 \u27a5Q \u00de\u00d1 The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY? T \u00de\u00d1 Today is 03/12/2002. So the date 24 hours later will be 03/13/2002. VA \u00de\u00d1 05/23/1943 qPrompt 3 \u27a5Q \u00de\u00d1 The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY? T \u00de\u00d1 Today is 04/19/1969. 24 hours later is one day after today, which would be 04/20/1969. VA \u00de\u00d1 01/07/2019 qPrompt 4 \u27a5Q \u00de\u00d1 Jane was born on the last day of February in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY? T \u00de\u00d1 One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. VA \u00de\u00d1 02/27/2017 qPrompt 5 \u27a5Q \u00de\u00d1 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY? T \u00de\u00d1 If the first day of 2019 was Tuesday, then 01/01/2019 was a Tuesday. Today is the first Monday, would be six days later. So today is 01/07/2019. VA \u00de\u00d1 01/05/2015 qPrompt 6 \u27a5Q \u00de\u00d1 Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 If 2015 is coming in 36 hours, then it is coming in 2 days. 2 days before 01/01/2015 is 12/30/2014, so today is 12/30/2014. So one week from today will be 01/05/2015. VA \u00de\u00d1 03/13/2002\nTable 61: GSM-8K with shuffled tokens within each sentence. The location of numbers is fixed.\nqPrompt 1 \u27a5Q \u00de\u00d1 Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? T \u00de\u00d1 with Shawn toys 5 started. dad, from more 2 his toys then is toys he mom got that each 4 and If. 5 + 4 = 9 VA \u00de\u00d1 9 qPrompt 2 \u27a5Q \u00de\u00d1 If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? T \u00de\u00d1 originally cars There 3 are. 2 arrive more cars. 3 + 2 = 5 VA \u00de\u00d1 5 qPrompt 3 \u27a5Q \u00de\u00d1 Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny? T \u00de\u00d1 lollipops with started 20 Jason. had after to 12 Denny Then some giving he. he gave Denny So 20 - 12 = 8 VA \u00de\u00d1 8 qPrompt 4 \u27a5Q \u00de\u00d1 There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room? T \u00de\u00d1 were There originally 9 computers. more For each 4 computers 5 of added days, were. computers 5 * 4 = 20 were added So. 9 + 20 is 29 VA \u00de\u00d1 29 qPrompt 5 \u27a5Q \u00de\u00d1 There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? T \u00de\u00d1 are There 15 originally trees. planted were some 21 more Then after there trees were. must So there been have 21 - 15 = 6 VA \u00de\u00d1 6 qPrompt 6 \u27a5Q \u00de\u00d1 Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? T \u00de\u00d1 Originally, had chocolates 32 Leah. Her sister had 42. total had they in So 32 + 42 = 74. eating had 35, After they 74 - 35 = 39 VA \u00de\u00d1 39 qPrompt 7 \u27a5Q \u00de\u00d1 Olivia has $23. She bought five bagels for $3 each. How much money does she have left? T \u00de\u00d1 had Olivia 23 dollars. 5 dollars be 3 each dollars bagels for 5 x 3 = 15 will. dollars So she 23 - 15 has left. 23 - 15 is 8 VA \u00de\u00d1 8 qPrompt 8 \u27a5Q \u00de\u00d1 Michael had 58 golf balls. On Tuesday, he lost 23 golf balls. On Wednesday, he lost 2 more. How many golf balls did he have at the end of Wednesday? T \u00de\u00d1 Michael started balls 58 with golf. had After 23 losing Tuesday, he on 58 - 23 = 35. golf losing 2 balls more, he 35 - 2 = 33 After had VA \u00de\u00d1 33\nTable 62: SPORTS with shuffled tokens within each sentence.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJamal Murray was perfect from the line.\u201d T \u00de\u00d1 is a player Jamal basketball Murray. from line Being perfect part is the basketball of VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJoao Moutinho caught the screen pass in the NFC championship.\u201d T \u00de\u00d1 Moutinho player is soccer a Joao. NFC American soccer of championship The not is part football, VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJonas Valanciunas beat the buzzer.\u201d T \u00de\u00d1 is a Valanciunas Jonas basketball player. part buzzer basketball the Beating of is VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cSam Darnold passed the puck.\u201d T \u00de\u00d1 a Sam player football is Darnold American. of hockey, puck the football American is Passing part not VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cKyle Palmieri was called for slashing.\u201d T \u00de\u00d1 Palmieri player hockey a is Kyle. called Being of part slashing hockey for is VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cCarson Wentz set the pick and roll.\u201d T \u00de\u00d1 football Carson is American player Wentz an. roll and not basketball, part is Pick football of VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cMalcolm Brogdon banked the shot in.\u201d T \u00de\u00d1 player basketball Brogdon Malcolm a is. the Banking shot in of basketball part is VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cDraymond Green threw a touchdown.\u201d T \u00de\u00d1 an is Draymond player basketball Green. Throwing football, a of touchdown part not is basketball VA \u00de\u00d1 no\nTable 63: DATE with shuffled tokens of each sentence.\nqPrompt 1 \u27a5Q \u00de\u00d1 It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 Today 04/19/1969 is. 24 after one which later be would hours 04/20/1969 day today, is VA \u00de\u00d1 04/20/1969 qPrompt 2 \u27a5Q \u00de\u00d1 The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY? T \u00de\u00d1 is 06/02/1943, so today 06/02/1943 day 06/01/1943 One is after. 10 today days is before 05/23/1943 VA \u00de\u00d1 05/23/1943 qPrompt 3 \u27a5Q \u00de\u00d1 The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY? T \u00de\u00d1 was Tuesday the day was 2019 first If then 01/01/2019 of Tuesday, a. days Today six later is Monday, be would first the. today So 01/07/2019 is VA \u00de\u00d1 01/07/2019 qPrompt 4 \u27a5Q \u00de\u00d1 Jane was born on the last day of February in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY? T \u00de\u00d1 last day Jane February on was The of 28th, born is so the 02/28/2001. Today today is birthday, old so 02/28/2017 16-year is her. 02/27/2017 yesterday So was VA \u00de\u00d1 02/27/2017 qPrompt 5 \u27a5Q \u00de\u00d1 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY? T \u00de\u00d1 days 2015 is then coming 36 If it hours, coming is in 2 in. 2 is 12/30/2014, days 01/01/2015 so 12/30/2014 is today before. from week will So 01/05/2015 be today one VA \u00de\u00d1 01/05/2015 qPrompt 6 \u27a5Q \u00de\u00d1 Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 03/12/2002 Today is. the hours later 24 be will So date 03/13/2002 VA \u00de\u00d1 03/13/2002\nTable 64: GSM-8K with sentences within each thought are shuffled. The location of numbers is fixed.\nqPrompt 1 \u27a5Q \u00de\u00d1 Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? T \u00de\u00d1 dad, he got 5 toys. then started mom 2 each is more that from If his and toys. toys 4 with Shawn 5 + 4 = 9. VA \u00de\u00d1 9 qPrompt 2 \u27a5Q \u00de\u00d1 If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot? T \u00de\u00d1 arrive. more are 3 cars 2 originally cars. There 3 + 2 = 5. VA \u00de\u00d1 5 qPrompt 3 \u27a5Q \u00de\u00d1 Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny? T \u00de\u00d1 giving So started 20 some gave to Denny 12 Jason Denny. Then after had he with he lollipops. 20 - 12 = 8. VA \u00de\u00d1 8 qPrompt 4 \u27a5Q \u00de\u00d1 There were nine computers in the server room. Five more computers were installed each day, from Monday to Thursday. How many computers are now in the server room? T \u00de\u00d1 originally were were 9 computers added. is For 4 each 5 computers. added. of days, more 5 * 4 = 20 computers were There 9 + 20 So 29. VA \u00de\u00d1 29 qPrompt 5 \u27a5Q \u00de\u00d1 There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? T \u00de\u00d1 So were 15 there after more have planted. 21 trees originally. must There are there were trees some Then been 21 - 15 = 6. VA \u00de\u00d1 6 qPrompt 6 \u27a5Q \u00de\u00d1 Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total? T \u00de\u00d1 After Originally, total 32 they sister had had 42. had eating in Her So 32 + 42 = 74. had they Leah 35, chocolates. 74 - 35 = 39. VA \u00de\u00d1 39 qPrompt 7 \u27a5Q \u00de\u00d1 Olivia has $23. She bought five bagels for $3 each. How much money does she have left? T \u00de\u00d1 bagels dollars 23 has 5 Olivia will 3 is dollars. left. for 5 had 3 = 15 dollars. So be x 23 - 15 dollars each 23 - 15 she 8. VA \u00de\u00d1 8 qPrompt 8 \u27a5Q \u00de\u00d1 Michael had 58 golf balls. On Tuesday, he lost 23 golf balls. On Wednesday, he lost 2 more. How many golf balls did he have at the end of Wednesday? T \u00de\u00d1 he golf Michael 58 golf After started with 23 more, losing had balls. 58 - 23 = 35. losing After 2 on Tuesday, he 35 - 2 = 33 balls. had VA \u00de\u00d1 33\nTable 65: SPORTS with sentences within each thought are shuffled.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJamal Murray was perfect from the line.\u201d T \u00de\u00d1 perfect player. basketball. a Murray part basketball is of Being Jamal is the from line VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJoao Moutinho caught the screen pass in the NFC championship.\u201d T \u00de\u00d1 American soccer. Moutinho not soccer Joao is part player. NFC The football, a championship is of VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJonas Valanciunas beat the buzzer.\u201d T \u00de\u00d1 Valanciunas of player. the basketball Jonas Beating is buzzer is part a basketball. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cSam Darnold passed the puck.\u201d T \u00de\u00d1 football is puck the hockey, player. not football. Darnold part a American of American Passing Sam is VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cKyle Palmieri was called for slashing.\u201d T \u00de\u00d1 hockey. of Palmieri slashing Kyle Being a player. called part hockey is is for VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cCarson Wentz set the pick and roll.\u201d T \u00de\u00d1 is football not football. Carson Pick roll basketball, and part Wentz American an of player. is VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cMalcolm Brogdon banked the shot in.\u201d T \u00de\u00d1 player. is of a in Brogdon basketball Banking shot the basketball. part Malcolm is VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cDraymond Green threw a touchdown.\u201d T \u00de\u00d1 of an a is touchdown football, Green basketball Draymond basketball. not player. is part Throwing VA \u00de\u00d1 no\nTable 66: DATE with sentences within each thought are shuffled.\nqPrompt 1 \u27a5Q \u00de\u00d1 It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 later is 04/19/1969. 24 day after which would be today, hours Today 04/20/1969. one is VA \u00de\u00d1 04/20/1969 qPrompt 2 \u27a5Q \u00de\u00d1 The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY? T \u00de\u00d1 today today days One 05/23/1943. 06/01/1943 06/02/1943, 06/02/1943. is is 10 is so day before after VA \u00de\u00d1 05/23/1943 qPrompt 3 \u27a5Q \u00de\u00d1 The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY? T \u00de\u00d1 01/01/2019 today Monday, would Tuesday, 2019 first is Tuesday. day of later. then So was 01/07/2019. first be days Today the a six was is If the VA \u00de\u00d1 01/07/2019 qPrompt 4 \u27a5Q \u00de\u00d1 Jane was born on the last day of February in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY? T \u00de\u00d1 last was her 16-year is 02/28/2001. So of so 02/27/2017. is birthday, is The February on yesterday was old today Today 02/28/2017. so day born 28th, the Jane VA \u00de\u00d1 02/27/2017 qPrompt 5 \u27a5Q \u00de\u00d1 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY? T \u00de\u00d1 in 2015 one hours, 01/01/2015 36 so it days. today 12/30/2014, then 2 week 2 is 12/30/2014. is be days in is 01/05/2015. So from today coming coming before is will If VA \u00de\u00d1 01/05/2015 qPrompt 6 \u27a5Q \u00de\u00d1 Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 later So date Today 03/13/2002. hours 24 is 03/12/2002. the be will VA \u00de\u00d1 03/13/2002\nTable 67: GSM-8K with questions in YodaSpeak.\nqPrompt 1 \u27a5Q \u00de\u00d1 Five toys, Shawn has.For Christmas,Two toys each from his mom and dad, he got.He have now, how many toys does? T \u00de\u00d1 Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. VA \u00de\u00d1 9 qPrompt 2 \u27a5Q \u00de\u00d1 3 cars in the parking lot and 2 more cars arrive, if there are,In the parking lot, how many cars are? T \u00de\u00d1 There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. VA \u00de\u00d1 5 qPrompt 3 \u27a5Q \u00de\u00d1 20 lollipops, Jason had. Denny some lollipops, he gave. 12 lollipops, now Jason has. Jason give to Denny, how many lollipops did? T \u00de\u00d1 Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. VA \u00de\u00d1 8 qPrompt 4 \u27a5Q \u00de\u00d1 Nine computers in the server room, there were. Installed each day, five more computers were, from Monday to Thursday. Now in the server room, how many computers are? T \u00de\u00d1 There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. VA \u00de\u00d1 29 qPrompt 5 \u27a5Q \u00de\u00d1 15 trees in the grove, there are. Grove workers will plant trees in the grove today. Done, after they are,21 trees, there will be. The grove workers plant today, how many trees did? T \u00de\u00d1 There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. VA \u00de\u00d1 6 qPrompt 6 \u27a5Q \u00de\u00d1 32 chocolates and her sister had 42, Leah had. 35, if they ate, they have left in total, how many pieces do? T \u00de\u00d1 Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. VA \u00de\u00d1 39 qPrompt 7 \u27a5Q \u00de\u00d1 $23, Olivia has. Five bagels for $3 each, she bought. She have left, how much money does? T \u00de\u00d1 Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. VA \u00de\u00d1 8 qPrompt 8 \u27a5Q \u00de\u00d1 58 golf balls, Michael had. On Tuesday, 23 golf balls, he lost. On Wednesday, 2 more, he lost. He have at the end of Wednesday, how many golf balls did? T \u00de\u00d1 Michael started with 58 golf balls. After losing 23 on Tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. VA \u00de\u00d1 33\nTable 68: SORTING with verbose chain of thought.\nqPrompt 1 \u27a5Q \u00de\u00d1 7 , 8 , 4 , 1 , 2 , 9 , 3 , 6 , 5 T \u00de\u00d1 First number is 1. Next we have 2, since 1 is less than 2. Next we have 3, since 2 is less than 3. Next we have 4, since 3 is less than 4. Next we have 5, since 4 is less than 5. Next we have 6, since 5 is less than 6. Next we have 7, since 6 is less than 7. Next we have 8, since 7 is less than 8. Next we have 9, since 8 is less than 9. VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 qPrompt 2 \u27a5Q \u00de\u00d1 5 , 9 , 3 , 1 , 8 , 4 , 6 , 2 T \u00de\u00d1 First number is 1. Next we have 2, since 1 is less than 2. Next we have 3, since 2 is less than 3. Next we have 4, since 3 is less than 4. Next we have 5, since 4 is less than 5. Next we have 6, since 5 is less than 6. Next we have 8, since 6 is less than 8. Next we have 9, since 8 is less than 9. VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 8 , 9 qPrompt 3 \u27a5Q \u00de\u00d1 6 , 5 , 7 , 4 , 3 , 2 , 8 , 1 T \u00de\u00d1 First number is 1. Next we have 2, since 1 is less than 2. Next we have 3, since 2 is less than 3. Next we have 4, since 3 is less than 4. Next we have 5, since 4 is less than 5. Next we have 6, since 5 is less than 6. Next we have 7, since 6 is less than 7. Next we have 8, since 7 is less than 8. VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 4 \u27a5Q \u00de\u00d1 1 , 6 , 4 , 8 , 5 , 3 , 7 , 2 T \u00de\u00d1 First number is 1. Next we have 2, since 1 is less than 2. Next we have 3, since 2 is less than 3. Next we have 4, since 3 is less than 4. Next we have 5, since 4 is less than 5. Next we have 6, since 5 is less than 6. Next we have 7, since 6 is less than 7. Next we have 8, since 7 is less than 8. VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 5 \u27a5Q \u00de\u00d1 5 , 2 , 1 , 4 , 3 , 7 T \u00de\u00d1 First number is 1. Next we have 2, since 1 is less than 2. Next we have 3, since 2 is less than 3. Next we have 4, since 3 is less than 4. Next we have 5, since 4 is less than 5. Next we have 7, since 5 is less than 7. VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 7 qPrompt 6 \u27a5Q \u00de\u00d1 3 , 8 , 2 , 5 , 6 , 4 , 7 , 1 T \u00de\u00d1 First number is 1. Next we have 2, since 1 is less than 2. Next we have 3, since 2 is less than 3. Next we have 4, since 3 is less than 4. Next we have 5, since 4 is less than 5. Next we have 6, since 5 is less than 6. Next we have 7, since 6 is less than 7. Next we have 8, since 7 is less than 8. VA \u00de\u00d1 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 qPrompt 7 \u27a5Q \u00de\u00d1 8 , 6 , 1 , 2 , 9 , 7 , 4 T \u00de\u00d1 First number is 1. Next we have 2, since 1 is less than 2. Next we have 4, since 2 is less than 4. Next we have 6, since 4 is less than 6. Next we have 7, since 6 is less than 7. Next we have 8, since 7 is less than 8. Next we have 9, since 8 is less than 9. VA \u00de\u00d1 1 , 2 , 4 , 6 , 7 , 8 , 9 qPrompt 8 \u27a5Q \u00de\u00d1 7 , 6 , 8 , 1 T \u00de\u00d1 First number is 1. Next we have 6, since 1 is less than 6. Next we have 7, since 6 is less than 7. Next we have 8, since 7 is less than 8. VA \u00de\u00d1 1 , 6 , 7 , 8\nTable 69: GSM-8K with verbalized numbers.\nqPrompt 1 \u27a5Q \u00de\u00d1 Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now? T \u00de\u00d1 Shawn started with five toys. If he got two toys each from his mom and dad, then that is four more toys. five + four = 9. VA \u00de\u00d1 9 qPrompt 2 \u27a5Q \u00de\u00d1 If there are three cars in the parking lot and two more cars arrive, how many cars are in the parking lot? T \u00de\u00d1 There are originally three cars. two more cars arrive. three + two = five. VA \u00de\u00d1 5 qPrompt 3 \u27a5Q \u00de\u00d1 Jason had twenty lollipops. He gave Denny some lollipops. Now Jason has twelve lollipops. How many lollipops did Jason give to Denny? T \u00de\u00d1 Jason started with twenty lollipops. Then he had twelve after giving some to Denny. So he gave Denny twenty - twelve = 8. VA \u00de\u00d1 8 qPrompt 4 \u27a5Q \u00de\u00d1 There were nine computers in the server room. Five more computers were installed each day, from Monday to Thursday. How many computers are now in the server room? T \u00de\u00d1 There were originally nine computers. For each of four days, five more computers were added. So five * four = twenty computers were added. nine + twenty is 29. VA \u00de\u00d1 29 qPrompt 5 \u27a5Q \u00de\u00d1 There are fifteen trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be twenty one trees. How many trees did the grove workers plant today? T \u00de\u00d1 There are fifteen trees originally. Then there were twenty one trees after some more were planted. So there must have been twenty one - fifteen = 6. VA \u00de\u00d1 6 qPrompt 6 \u27a5Q \u00de\u00d1 Leah had thirty two chocolates and her sister had 42. If they ate thirty five,how many pieces do they have left in total? T \u00de\u00d1 Originally, Leah had thirty two chocolates. Her sister had 42. So in total they had thirty two + forty two = 74. After eating thirty five, they had seventy four - thirty five = 39. VA \u00de\u00d1 39 qPrompt 7 \u27a5Q \u00de\u00d1 Olivia has twenty three. She bought five bagels for three dollars each. How much money does she have left? T \u00de\u00d1 Olivia had twenty three dollars. five bagels for three dollars each will be five x three = fifteen dollars. So she has twenty three - fifteen dollars left. twenty three - fifteen is 8. VA \u00de\u00d1 8 qPrompt 8 \u27a5Q \u00de\u00d1 Michael had fifty eight golf balls. On Tuesday, he lost twenty three golf balls. On Wednesday, he lost two more. How many golf balls did he have at the end of Wednesday? T \u00de\u00d1 Michael started with fifty eight golf balls. After losing twenty three on Tuesday, he had fifty eight - twenty three = 3five. After losing two more, he had thirty five - two = thirty three golf balls. VA \u00de\u00d1 33\nTable 70: SPORTS with abstract sports person, sport, and sport activity.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"PERSON was involved in ACTIVITY.\" T \u00de\u00d1 PERSON is a SPORT1 player. Being ACTIVITY is part of SPORT1. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"PERSON does ACTIVITY.\" T \u00de\u00d1 PERSON is a SPORT2 player. The ACTIVITY is part of SPORT3, not SPORT2. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"PERSON finishes ACTIVITY.\" T \u00de\u00d1 PERSON is a SPORT1 player. ACTIVITY is part of SPORT1. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"PERSON performs ACTIVITY.\" T \u00de\u00d1 PERSON is a SPORT3 player. ACTIVITY is part of SPORT4, not SPORT3. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"PERSON was seen doing ACTIVITY.\" T \u00de\u00d1 PERSON is a SPORT4 player. Being ACTIVITY is part of SPORT4. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"PERSON complete ACTIVITY.\" T \u00de\u00d1 PERSON is an SPORT3 player. ACTIVITY is part of SPORT1, not SPORT3. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"PERSON executes ACTIVITY.\" T \u00de\u00d1 PERSON is a SPORT1 player. ACTIVITY is part of SPORT1. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"PERSON carries out ACTIVITY.\" T \u00de\u00d1 PERSON is an SPORT1 player. ACTIVITY is part of SPORT3, not SPORT1. VA \u00de\u00d1 no\nTable 71: SPORTS with abstract sports person and sport.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPERSON was perfect from the line.\u201d T \u00de\u00d1 PERSON is a SPORT1 player. Being perfect from the line is part of SPORT1. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPERSON caught the screen pass in the NFC championship.\u201d T \u00de\u00d1 PERSON is a SPORT3 player. The NFC championship is part of SPORT2, not SPORT3. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPERSON beat the buzzer.\u201d T \u00de\u00d1 PERSON is a SPORT1 player. Beating the buzzer is part of SPORT1. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPERSON passed the puck.\u201d T \u00de\u00d1 PERSON is a SPORT2 player. Passing the puck is part of SPORT4, not SPORT2. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPERSON was called for slashing.\u201d T \u00de\u00d1 PERSON is a SPORT4 player. Being called for slashing is part of SPORT4. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPERSON set the pick and roll.\u201d T \u00de\u00d1 PERSON is an SPORT2 player. Pick and roll is part of SPORT1, not SPORT2. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPERSON banked the shot in.\u201d T \u00de\u00d1 PERSON is a SPORT1 player. Banking the shot in is part of SPORT1. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cPERSON threw a touchdown.\u201d T \u00de\u00d1 PERSON is an SPORT1 player. Throwing a touchdown is part of SPORT2, not SPORT1. VA \u00de\u00d1 no\nTable 72: SPORTS with the order of clauses switched.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"Jamel Murray was perfect from the line.\" T \u00de\u00d1 Being perfect from the line is part of basketball. Jamal Murray is a basketball player. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"Joao Moutinho caught the screen pass in the NFC championship.\" T \u00de\u00d1 The NFC championship is part of American football, not soccer. Joao Moutinho is a soccer player. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"Jonas Valanciunas beat the buzzer.\" T \u00de\u00d1 Beating the buzzer is part of basketball. Jonas Valanciunas is a basketball player. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"Sam Darnold passed the puck.\" T \u00de\u00d1 Passing the puck is part of hockey, not American football. Sam Darnold is a American football player. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"Kyle Palmieri was called for slashing.\" T \u00de\u00d1 Being called for slashing is part of hockey. Kyle Palmieri is a hockey player. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"Carson Wentz set the pick and roll.\" T \u00de\u00d1 Pick and roll is part of basketball, not football. Carson Wentz is an American football player. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"Malcolm Brogdon banked the shot in.\" T \u00de\u00d1 Banking the shot in is part of basketball. Malcolm Brogdon is a basketball player. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \"Draymond Green threw a touchdown.\" T \u00de\u00d1 Throwing a touchdown is part of football, not basketball. Draymond Green is an basketball player. VA \u00de\u00d1 no\nTable 73: CCOT prompt for GSM-8K.\nqPrompt 1 \u27a5Q \u00de\u00d1 What is fifteen more than a quarter of 48? T \u00de\u00d1 A quarter of 48 is 48 / 4 = 12. 15 more than 12 is 12 + 15 = 27. VA \u00de\u00d1 27 qPrompt 2 \u27a5Q \u00de\u00d1 Twice Angie\u2019s age, plus 4, is 20. How old is Angie? T \u00de\u00d1 Angie\u2019s age minus 4 is 20 - 4 = 16. 16 is twice Angie\u2019s age. Thus, Angie\u2019s age is 16 / 2 = 8. VA \u00de\u00d1 8 qPrompt 3 \u27a5Q \u00de\u00d1 Steve is 5\u20196\u201d. He grows 6 inches. How tall is he in inches? T \u00de\u00d1 One feet has 12 inches. Initially, Steve is 5 * 12 + 6 = 66 inches tall. After growing 6 inches, Steve is 66 + 6 = 72 inches tall. VA \u00de\u00d1 72 qPrompt 4 \u27a5Q \u00de\u00d1 198 passengers fit into 9 buses. How many passengers fit in 5 buses? T \u00de\u00d1 Capacity of one bus is 198 passengers / 9 buses = 22 passengers in one bus. Thus, 5 buses can fit 22 * 5 = 110 passengers. VA \u00de\u00d1 110 qPrompt 5 \u27a5Q \u00de\u00d1 Fifteen more than a quarter of a number is 27. What is the number? T \u00de\u00d1 Fifteen less than 27 is 27 - 15 = 12. The quarter of the number is thus 12, and the number is 12 * 4 = 48. VA \u00de\u00d1 48 qPrompt 6 \u27a5Q \u00de\u00d1 If 12 bags of oranges weigh 24 pounds, how much do 8 bags weigh? T \u00de\u00d1 12 bags weight 24 pounds, so one bag weights 24 pounds / 12 = 2 pounds. So 8 bags of oranges would weigh 8 * 2 pounds = 16 pounds. VA \u00de\u00d1 16 qPrompt 7 \u27a5Q \u00de\u00d1 Sandra eats 3 beignets every morning. How many beignets will she eat in 16 weeks? T \u00de\u00d1 Sandra eats 3 beignets every morning and there are 7 days in a week so she eats 3 * 7 = 21 beignets in a week. Sandra eats 21 beignets in a week, so in 16 weeks she will eat 21 * 16 = 336 beignets. VA \u00de\u00d1 336 qPrompt 8 \u27a5Q \u00de\u00d1 John takes a pill every 6 hours. How many pills does he take a week? T \u00de\u00d1 There are 24 hours in a day. So John takes 24 / 6 = 4 pills a day. In a week, John therefore takes 4 * 7 = 28 pills. VA \u00de\u00d1 28\nTable 74: CCOT prompt for DATE.\nqPrompt 1 \u27a5Q \u00de\u00d1 It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 Today is 04/19/1969. 24 hours (one day) later is 04/20/1969. VA \u00de\u00d1 04/20/1969 qPrompt 2 \u27a5Q \u00de\u00d1 The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY? T \u00de\u00d1 Today is 06/02/1943 (one day after 06/01/1943). 10 days before today is 05/23/1943. VA \u00de\u00d1 05/23/1943 qPrompt 3 \u27a5Q \u00de\u00d1 The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY? T \u00de\u00d1 01/01/2019 was a Tuesday (first day of 2019). Today is the first Monday, 01/07/2019. (six days later). VA \u00de\u00d1 01/07/2019 qPrompt 4 \u27a5Q \u00de\u00d1 Jane was born on the last day of February in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY? T \u00de\u00d1 Jane was born on 02/28/2001. So today is 02/28/2017 and yesterday was 02/27/2017. VA \u00de\u00d1 02/27/2017 qPrompt 5 \u27a5Q \u00de\u00d1 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY? T \u00de\u00d1 2015 is coming in 2 days (36 hours). So today is 12/30/2021, and one week from today will be 01/05/2015. VA \u00de\u00d1 01/05/2015 qPrompt 6 \u27a5Q \u00de\u00d1 Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY? T \u00de\u00d1 Today is 03/12/2002. So the date 24 hours later will be 03/13/2002. VA \u00de\u00d1 03/13/2002\nTable 75: CCOT prompt for SPORTS.\nqPrompt 1 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJamel Murray was perfect from the line.\u201d T \u00de\u00d1 Jamal Murray -> basketball. perfect from the line -> basketball. VA \u00de\u00d1 yes qPrompt 2 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJoao Moutinho caught the screen pass in the NFC championship.\u201d T \u00de\u00d1 Joao Moutinho -> soccer. NFC championship -> American football. VA \u00de\u00d1 no qPrompt 3 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cJonas Valanciunas beat the buzzer.\u201d T \u00de\u00d1 Jonas Valanciunas -> basketball. beating the buzzer -> basketball. VA \u00de\u00d1 yes qPrompt 4 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cSam Darnold passed the puck.\u201d T \u00de\u00d1 Sam Darnold -> American football. passing the puck -> hockey. VA \u00de\u00d1 no qPrompt 5 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cKyle Palmieri was called for slashing.\u201d T \u00de\u00d1 Kyle Palmieri -> hockey. called for slashing -> hockey. VA \u00de\u00d1 yes qPrompt 6 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cCarson Wentz set the pick and roll.\u201d T \u00de\u00d1 Carson Wentz is -> American football. pick and roll -> basketball. VA \u00de\u00d1 no qPrompt 7 \u27a5Q \u00de\u00d1 Is the following sentence plausible? Malcolm Brogdon banked the shot in. T \u00de\u00d1 th \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cMalcolm Brogdon banked the shot in.\u201d T \u00de\u00d1 Malcolm Brogdon -> basketball. banking the shot in -> basketball. VA \u00de\u00d1 yes qPrompt 8 \u27a5Q \u00de\u00d1 Is the following sentence plausible? \u201cDraymond Green threw a touchdown.\u201d T \u00de\u00d1 Draymond Green -> basketball. throwing a touchdown -> football. VA \u00de\u00d1 no"
        }
    ],
    "title": "What makes Chain-of-Thought Prompting Effective? A Counterfactual Study",
    "year": 2023
}