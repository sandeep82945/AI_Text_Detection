{
    "abstractText": "The need for grounding in language understanding is an active research topic. Previous work has suggested that color perception and color language appear as a suitable test bed to empirically study the problem, given its cognitive significance and showing that there is considerable alignment between a defined color space and the feature space defined by a language model. To further study this issue, we collect a large scale source of colors and their descriptions, containing almost a 1 million examples , and perform an empirical analysis to compare two kinds of alignments: (i) inter-space, by learning a mapping between embedding space and color space, and (ii) intra-space, by means of prompting comparatives between color descriptions. Our results show that while color space alignment holds for monolexemic, highly pragmatic color descriptions, this alignment drops considerably in the presence of examples that exhibit elements of real linguistic usage such as subjectivity and abstractedness, suggesting that grounding may be required in such cases.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pablo Loyola"
        },
        {
            "affiliations": [],
            "name": "Edison Marrese-Taylor"
        },
        {
            "affiliations": [],
            "name": "Andres Hoyos-Idobro"
        }
    ],
    "id": "SP:1ac5f60c8ccd18c519c725020f23913be2e2e0a0",
    "references": [
        {
            "authors": [
                "Mostafa Abdou",
                "Artur Kulmizev",
                "Daniel Hershcovich",
                "Stella Frank",
                "Ellie Pavlick",
                "Anders S\u00f8gaard."
            ],
            "title": "Can language models encode perceptual structure without grounding? a case study in color",
            "venue": "Proceedings of the 25th Conference on Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Piotr Bojanowski",
                "Edouard Grave",
                "Armand Joulin",
                "Tomas Mikolov."
            ],
            "title": "Enriching word vectors with subword information",
            "venue": "Transactions of the association for computational linguistics, 5:135\u2013146.",
            "year": 2017
        },
        {
            "authors": [
                "Marc Brysbaert",
                "Amy Beth Warriner",
                "Victor Kuperman."
            ],
            "title": "Concreteness ratings for 40 thousand generally known english word lemmas",
            "venue": "Behavior research methods, 46:904\u2013911.",
            "year": 2014
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Edward Gibson",
                "Richard Futrell",
                "Julian Jara-Ettinger",
                "Kyle Mahowald",
                "Leon Bergen",
                "Sivalogeswaran Ratnasingam",
                "Mitchell Gibson",
                "Steven T Piantadosi",
                "Bevil R Conway."
            ],
            "title": "Color naming across languages reflects color use",
            "venue": "Proceedings of the Na-",
            "year": 2017
        },
        {
            "authors": [
                "Yoav Goldberg."
            ],
            "title": "Assessing bert\u2019s syntactic abilities",
            "venue": "arXiv preprint arXiv:1901.05287.",
            "year": 2019
        },
        {
            "authors": [
                "Xudong Han",
                "Philip Schulz",
                "Trevor Cohn."
            ],
            "title": "Grounding learning of modifier dynamics: An application to color naming",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Kazuya Kawakami",
                "Chris Dyer",
                "Bryan R Routledge",
                "Noah A Smith."
            ],
            "title": "Character sequence models for colorful words",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1949\u20131954.",
            "year": 2016
        },
        {
            "authors": [
                "Nikolaus Kriegeskorte",
                "Marieke Mur",
                "Peter A Bandettini."
            ],
            "title": "Representational similarity analysisconnecting the branches of systems neuroscience",
            "venue": "Frontiers in systems neuroscience, page 4.",
            "year": 2008
        },
        {
            "authors": [
                "Delwin T. Lindsey",
                "Angela M. Brown."
            ],
            "title": "The color lexicon of American English",
            "venue": "Journal of Vision,",
            "year": 2014
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692",
            "year": 2019
        },
        {
            "authors": [
                "Vittorio Loreto",
                "Animesh Mukherjee",
                "Francesca Tria."
            ],
            "title": "On the origin of the hierarchy of color names",
            "venue": "Proceedings of the National Academy of Sciences, 109(18):6819\u20136824.",
            "year": 2012
        },
        {
            "authors": [
                "Steven Loria"
            ],
            "title": "Textblob documentation",
            "venue": "Release 0.15, 2(8).",
            "year": 2018
        },
        {
            "authors": [
                "Rebecca Marvin",
                "Tal Linzen."
            ],
            "title": "Targeted syntactic evaluation of language models",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1192\u20131202.",
            "year": 2018
        },
        {
            "authors": [
                "Brian McMahan",
                "Matthew Stone."
            ],
            "title": "A bayesian model of grounded color semantics",
            "venue": "Transactions of the Association for Computational Linguistics, 3:103\u2013 115.",
            "year": 2015
        },
        {
            "authors": [
                "Will Monroe",
                "Noah Goodman",
                "Christopher Potts."
            ],
            "title": "Learning to generate compositional color descriptions",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2243\u20132248.",
            "year": 2016
        },
        {
            "authors": [
                "Will Monroe",
                "Robert XD Hawkins",
                "Noah D Goodman",
                "Christopher Potts."
            ],
            "title": "Colors in context: A pragmatic neural model for grounded language understanding",
            "venue": "Transactions of the Association for Computational Linguistics, 5:325\u2013338.",
            "year": 2017
        },
        {
            "authors": [
                "Will Monroe",
                "Jennifer Hu",
                "Andrew Jong",
                "Christopher Potts."
            ],
            "title": "Generating bilingual pragmatic color references",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
            "year": 2018
        },
        {
            "authors": [
                "Randall Munroe."
            ],
            "title": "Color survey results",
            "venue": "Online at http://blog.xkcd.com/2010/05/03/ color-survey-results).",
            "year": 2010
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Gabriel Peyr\u00e9",
                "Marco Cuturi",
                "Justin Solomon."
            ],
            "title": "Gromov-wasserstein averaging of kernel and distance matrices",
            "venue": "International conference on machine learning, pages 2664\u20132672. PMLR.",
            "year": 2016
        },
        {
            "authors": [
                "Gabriel Peyr\u00e9",
                "Marco Cuturi"
            ],
            "title": "Computational optimal transport: With applications to data science",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Luc Steels",
                "Tony Belpaeme"
            ],
            "title": "Coordinating perceptually grounded categories through language: A case study for colour",
            "venue": "Behavioral and brain sciences,",
            "year": 2005
        },
        {
            "authors": [
                "Colin R Twomey",
                "Gareth Roberts",
                "David H Brainard",
                "Joshua B Plotkin."
            ],
            "title": "What we talk about when we talk about colors",
            "venue": "Proceedings of the National Academy of Sciences, 118(39):e2109237118.",
            "year": 2021
        },
        {
            "authors": [
                "Olivia Winn",
                "Smaranda Muresan."
            ],
            "title": "lighter\u2019can still be dark: Modeling comparative color descriptions",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 790\u2013795.",
            "year": 2018
        },
        {
            "authors": [
                "Noga Zaslavsky",
                "Charles Kemp",
                "Naftali Tishby",
                "Terry Regier."
            ],
            "title": "Color naming reflects both perceptual structure and communicative need",
            "venue": "Topics in cognitive science, 11(1):207\u2013219.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "One of the most interesting aspects of Large Language Models (LLMs) is that while they are usually trained without explicit linguistic supervision, or knowledge injection, there is a relevant body of work that shows that both linguistic structures and relational knowledge emerge even without the need for fine-tuning (Petroni et al., 2019; Goldberg, 2019; Marvin and Linzen, 2018). This has generated an ongoing discussion on how these models are learning and if the current training objectives and text-only data are enough to cover a wide range of downstream tasks.\nOne of the perspectives that have been used recently to study this phenomenon is color language (Abdou et al., 2021). Color naming is a relevant task (Steels et al., 2005), well understood in physiological (Loreto et al., 2012) and sociocultural\nterms (Gibson et al., 2017) and has an inherent intent from the communicative point of view (Zaslavsky et al., 2019; Twomey et al., 2021). In a recent work, Abdou et al. (2021) proposed to quantify the alignment, understood as the structural correspondence between a point in the color space (e.g. RGB or CIELAB) and its associated name in natural language represented as the feature vector obtained from a LLM. For such empirical study, they make use of the Color Lexicon of American English (Lindsey and Brown, 2014), based on the Munsell Chart of color chips (Munsell et al., 1915), finding that in general alignment is present across the color spectrum. While this work provides valuable insights on the actual grounding requirements, most of the color descriptions are monolexemic.\nThe above findings sparked our interest in the issue and led us to run preliminary experiments to test to what degree this alignment exists for less pragmatic color descriptions. We observed that such alignment drops significantly in the presence of more complex and subjective ways to describe specific colors, for example, when the color names contain multiple nouns or NPs, as well as terms with other parts-of-speech.\nTo further study these issues, we construct a more challenging test scenario by using and processing data from ColorNames1, an online service where users can collaboratively generate (color, color description) pairs. Given its free-form structure, the COLORNAMES dataset represents a rich\n1https://colornames.org\nand heterogeneous corpus of color descriptions. This can be appreciated clearly in Table 1.\nUsing this new, more challenging dataset, we conducted two experiments. The first one complements the work of Abdou et al. (2021) in assessing the inter-space alignment between color and LLM spaces, with two key new additions: (i) we propose a more fine-grained color name segmentation, considering metrics associated to subjectivity and concreteness, and (ii) we adopt an Optimal Transport-based metric to complement the existing alignment methods. For the second experiment, we focus on the representations obtained from the LLMs and their ability to ground color comparatives as a way to structure color descriptions. Critically, we do this without the need of accessing underlying points in color space. Concretely, we assess to what extent the LLM is able to discover a comparative-based relationship between a pair of color names without the need for explicit underlying color information, following a few shot learning configuration. For example, what would be the correct comparative associated to the pair of color names (e.g. between blood red wine and funeral roses)? If the model succeeds on that task, it could mean that such relationships are somehow encoded during pretraining, without the need of an explicit color signal.\nThe results of the proposed experiments show in general the alignment scores between spaces on the proposed dataset are low, contrasting with the results provided on (Abdou et al., 2021) on the Munsell dataset. This means that the complexity of the color descriptions, exemplified by the subjectivity and concreteness metrics, really impact on the perceptual structure the language models achieve. On the other hand, the results of the second experiment on comparative prediction, show that all language models are able to perform surprisingly well, even in scenarios of high subjectivity. This discrepancy leads to think that somehow the models retain certain structure learned thought language, but are not able to translate into color modality."
        },
        {
            "heading": "2 Related Work",
            "text": "Color language, as a medium to study grounding, has been used in several works, mainly trying to learn a mapping between the descriptions and their associated points in color space, such as Kawakami et al. (2016), based on character level model, Monroe et al. (2016); McMahan and Stone (2015) which\ntake as input a color representation and generates a natural language description, Monroe et al. (2017), who incorporates the idea of contextual information to guide the generation, and in Monroe et al. (2018) tested it in a bilingual setting. Winn and Muresan (2018); Han et al. (2019) proposed to model comparatives between colors. In most of these works, the source of color names come from Munroe (2010), which compresses the results of an online survey where participants were asked to provide free-form labels in natural language to various RGB samples. This data was subsequently filtered by McMahan and Stone (2015), leading to a total number of samples of over 2 million instances, but with a number of unique color names constrained to only 829. This suggests a reduction in the complexity of modeling tasks as proposed by previous work, as the vocabulary is fairly small, and with a homogeneous frequency. In contrast, the empirical study we propose does not have such constraint, allowing us to work with unique, subjective descriptions which are richer in vocabulary. In terms of using color to understand perceptual structure in LLMs, our direct inspiration was the work by Abdou et al. (2021), where authors perform experiments to quantify the alignment between points in color space and embeddings obtained by encoding the associated color names with LLMs."
        },
        {
            "heading": "3 Experimental Setting",
            "text": "Data: We use data from ColorNames2, which crowdsources (color, color name) pairs. Table 1 presents some examples. The extracted data was filtered to standardize the comparison. Only English sentences were kept, spam entries were removed using predefined rules and only color names with a maximum of five words were kept. The resulting dataset consists of 953,522 pair instances, with a total vocabulary size of 111,531 tokens. As seen in Table 2, words with the highest frequencies correspond to color words. In terms of POS patterns, the data presents a total of 3,809 combinations, extracted using Spacy3 but the most frequent patterns represent ways to modify nouns, by using an adjective (e.g. dark apple) or a sequence of nouns. We computed concreteness scores for the descriptions based on Brysbaert et al. (2014), which provides a defined set of lemmas with a ranking varying from 1 to 5. For example, red pine brown gets a score\n2colornames.org 3https://spacy.io/usage/linguistic-features\nof 4.3, while mysterious skyscape gets a score of 1.9. In this sense, we make the assumption that lower concreteness means higher abstractedness. Additionally, subjectivity scores were computed based on TextBlob (Loria et al., 2018), a rule-based approach that provides a score between 0 and 1, where a description like thick and creamy gets a score of 0.47, and mathematically perfect purple gets a 1.0. Figure 3 shows the correspondence between the scores and the expected usage of three sample words, ranging from ugly, a term associated with subjectivity to apple, which is commonly used to represent the reds spectrum. In the case of rich, it could have mixed connotations, which is reflected in its histogram having most frequent values at the center.\nLanguage Models: For all experiments, we used three language models, namely, BERT (Devlin et al., 2019), Roberta (Liu et al., 2019), T5 (Raffel et al., 2020), all of them in their large type, based on their availability on HuggingFace4. As a control baseline, we use FastText word embeddings (Bojanowski et al., 2017). The selection of such models is based on the need to explicitly extract embeddings from the LLM, which means that in principle only white-box models can be used. This ruled out API-based systems like GPT-3 and other similar. Moreover, even for competitive white-box models such as LLaMa, given the size of our introduced dataset (around million examples), its usage is left for future work. Finally, we note that our selection of LLMs lies within the masked-language modelling domain (MLM). This is a deliberate and critical decision, as it allows for our experiments to be performed in a controlled in-filling setting, limiting what the LLM can output and allowing us to parse their generations automatically. More\n4https://huggingface.co/models"
        },
        {
            "heading": "Word Frequency POS Pattern Frequency",
            "text": "up-to-date models are all causal LMs (with a few exceptions), which means that our capacity to control is more limited, as these models cannot in-fill text. Moreover, it has been shown that the output of these models is highly dependent on how they are prompted, usually requiring a huge amount of work into prompt construction in order to control the output, which adds further complications."
        },
        {
            "heading": "4 Experiments",
            "text": "Experiment I: Inter-space Alignment This first experiment is directly inspired by Abdou et al. (2021). In this case, we want to assess the alignment between color and LM feature spaces. For measuring such alignment, we replicated the settings proposed by (Abdou et al., 2021) for the case of (i) Linear Mapping (LMap), where given a set of n (color, color name) pairs, the alignment is measured as the fitness of the regressor W \u2208 RdLM\u00d73 that minimizes ||XW \u2212Y||22 + \u03b1||W||1, with \u03b1 regularization parameter, X \u2208 Rn\u00d7dLM the color names embeddings and Y \u2208 Rn\u00d73 the vectors coming from the color space, and (ii) Representational Similarity Analysis (RSA) (Kriegeskorte et al., 2008), the non-parametric method, whose score is operationalized via the mean Kendall\u2019s \u03c4 between both modalities. In addition, we propose to model alignment as an Optimal Transport (OT) (Peyr\u00e9 et al., 2019) problem, where the goal is to find a transport matrix that minimizes the cost of moving all text samples onto their corresponding color samples. We rely on Gromov-Wasserstein distance (GW) (Peyr\u00e9 et al., 2016), which extends the OT setting from samples to metric spaces. GW finds a mapping T \u2208 Rn\u00d7n+ that minimizes the GW cost, \u2211 i,j,k,l \u2225CTEXTik \u2212CCOLORjl \u222522Tij Tkl\nsubject to 0 \u2264 Tij \u2264 1, \u2211 iTij = 1 n , and\u2211\nj Tij = 1 n , where C TEXT = cos(X,X) \u2208 Rn\u00d7n\nand CCOLOR = cos(Y,Y) \u2208 Rn\u00d7n are the withindomain similarity matrices. Therefore, Tij denotes the probability of assigning a sample i in the text space to a sample j in the color space.\nWe grouped the examples into uniform segments associated to the subjectivity and concreteness scores, computing alignment scores per segment, per LM. In general, results show the alignment is low, and it decreases as subjectivity increases. Similarly to (Abdou et al., 2021), FastText behaves as a strong baseline, we hypothesize that given the uniqueness of the descriptions, its n-gram based tokenizer could be more stable than the tokenizers implemented within the LMs. Figure 2 show the results for all models on all segments using LMap and OT. (We omitted RSA as its behavior is very similar to LMap).\nExperiment II: Perceptual Structure via Comparative Identification The objective of this experiment is to determine if a LM can structure relationships between color descriptions without accessing the associated points in color space. To that end we design a task where, given two color descriptions, the LM has to determine the correct comparative that relates both descriptions (e.g. darker or lighter). To this end, we firstly match the dataset provided by Winn and Muresan (2018), which consists of tuples ( [ reference color points ], comparative, [target color points]) against COLORNAMES , by sampling (color, color description) pairs (ci, cdi), (cj , cdj) and retrieving the comparative that minimizes simultaneously the distance between [reference color points] and ci, and [target color points] and cj . After this step, we have, for any pair of descriptions in the COLORNAMES dataset, a ranking with the most suitable comparatives, based on explicit grounding. Table 3 provides matched examples.\nWe operationalize the task as a few shot inference. Firstly, we select randomly from the matched dataset, K tuples of the form (description i, comparative, description j ), from which K \u2212 1 are used to construct the labeled part of a prompt, following the template \"descriptioni is [compar-\native] than descriptionj\". The remaining k-th tuple is appended as \"descriptioni is [MASK] than descriptionj\", i.e., the comparative has been masked. The resulting prompt is passed through the LM and the ranking of the most likely tokens for [MASK] is retrieved. As evaluation metric, we chose the Mean Reciprocal Rank (MRR), as it encodes the position of the correct answer among the raking provided by the LM. We experimented using a total set of 81 comparatives and varying parameter K from 5 to 20. We performed the same uniform segmentation in terms of subjectivity and concreteness. Results in general showed surprisingly good results in terms of MRR, in several cases, LM outputs the correct comparative at position 1. There was a natural decay in performance when K was smaller, but for K > 10 results were consistent across models and segments. Figure 3 presents the results for K = 10, showing uniformity that led us to speculate that for this task, subjectivity may not be relevant as a control factor. From a qualitative point of view, Figure 4 shows the result of constructing a graph based on the comparative relationships correctly inferred by the LM. As it can be seen, (a) there is color coherence in terms of the neighboring relationships, and (b) when sampling paths form the graphs, transitions are consistent. Further investigation of these structures is left for future work. Additionally, trying to assess the impact on the language model selection, we experimented considering ChatGPT (web-based query) and Llama-2 (llama-2-13b-chat) as language models tasked to predict the comparatives. We found that, in several cases (with different prompts) even when using a k-shot setting, these models often created new types of comparatives (e.g. \"SIMILAR IN LIGHTNESS\"). As such new comparatives are not present in the ground truth of our dataset, our evaluation framework becomes increasingly complicated since we would need to further post-process the model outputs. Such a more complex experiment is left for future work.\nImpact of inner context Finally, differently from (Abdou et al., 2021), as our descriptions are not appended to any additional context at encoding time, we want to assess if their complexity acts as a natural source of context. For example, for the description mustard taxi yellow, we can see how the generic yellow color word is being conditioned by two nouns, mustard and taxi. An analysis of our data showed that out of 900K instances, around 390K (43 %) contain a color word. Based on this, we split the data into two chunks and re-run the two experiments described above. The results show that for the case of the alignment task, the mean R-scores from the regressor associated with the set of descriptions that have and do not have a color word are 0.401 and 0.381 respectively (using BERTlarge). We can see that there is in fact a difference, although the ranges are within the results reported.\nMoreover, given the full set of (color, color description) pairs, we cluster them using the color representations using k-means. From the resulting set of clusters, we choose the one that groups the yellows, as an example. Within that cluster, we performed a new grouping, this time using the embeddings of the color descriptions. From this, we now obtained 22 subgroups (again, using standard k-means) that are semantically close (inner context) but that are globally constrained by the color spectrum chosen (yellow). We now study the alignment within semantically-close groups and in-between\ngroups. We selected three distinct groups: a group with semantics about taxis, one about bananas and one about sun/sunset/sunrise. We computed the alignment score and the MRR associated to comparative prediction of each group independently, for pairs of groups and for the combination of all of them, as seen in Table 4.\nAs we can see, in general the alignment scores are reasonable for single groups, but as we start combining them, the scores drop. This is expected as in most cases there is a token that becomes an anchor which is slightly modified by the rest of the description. On the other hand, in the prediction of the comparative among pairs of descriptions, we can see that the accuracies (measured with MRR) dropping as we combine the sets, but still remain mostly in the same ballpark. This, while not conclusive evidence, helps us approximate to the notion that alignment can indeed be influenced by the semantics of the descriptions, but it does not seem to play a big role on how the LM structure the information using comparatives."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "We studied how LLMs encode perceptual structure in terms of the alignment between color and text embedding spaces and the inference of comparative relationships in a new, challenging dataset that encapsulates elements of real language usage, such abstractedness and subjectivity. The results show that LMs perform in mixed way, which provides additional evidence on the need for actual grounding. In terms of future work, we are considering the need for additional contextual information, which could be attacked by considering color palettes instead of single colors, and also considering a multilingual approach, to cover the sociocultural aspects of color naming, specially in scenarios of low resource translation."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank the anonymous reviewers for their insightful and constructive feedback."
        },
        {
            "heading": "Limitations",
            "text": "One of the key limitations of the current work is its focus solely on English language, which, in terms of color language, naturally compresses the cultural aspects associated to English-speaking societies. This is clear when we analyze in detail the vocabulary, we can find cultural archetypes that are probably not transferable. In that, there is an inherent challenge on how to learn color naming conventions in a more broad way. For example, for applications related to low resource languages, understanding the use of language for color description could be helpful for anchoring linguistic patterns."
        },
        {
            "heading": "Ethics Statement",
            "text": "Our main objective is to understand how language is used to communicate color. This has several applications, for example in e-commerce, such as search, product reviews (where color is an important attribute). While directly our study tries to abstract from specific user information, it is certain that language usage identification could be used for prospecting or targeting individuals from a specific cultural background."
        }
    ],
    "title": "Perceptual Structure in the Absence of Grounding for LLMs: The Impact of Abstractedness and Subjectivity in Color Language",
    "year": 2023
}