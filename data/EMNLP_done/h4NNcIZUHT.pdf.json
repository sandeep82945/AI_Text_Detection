{
    "abstractText": "Dialogue State Tracking (DST), a key component of task-oriented conversation systems, represents user intentions by determining the values of pre-defined slots in an ongoing dialogue. Existing approaches use hand-crafted templates and additional slot information to fine-tune and prompt large pre-trained language models and elicit slot values from the dialogue context. Significant manual effort and domain knowledge is required to design effective prompts, limiting the generalizability of these approaches to new domains and tasks. In this work, we propose DiSTRICT, a generalizable in-context tuning approach for DST that retrieves highly relevant training examples for a given dialogue to fine-tune the model without any hand-crafted templates. Experiments with the MultiWOZ benchmark datasets show that DiSTRICT outperforms existing approaches in various zeroshot and few-shot settings using a much smaller model, thereby providing an important advantage for real-world deployments that often have limited resource availability.",
    "authors": [
        {
            "affiliations": [],
            "name": "Praveen Venkateswaran"
        },
        {
            "affiliations": [],
            "name": "Evelyn Duesterwald"
        },
        {
            "affiliations": [],
            "name": "Vatche Isahagian"
        }
    ],
    "id": "SP:c809df74c34e8d63ac0ab8fc2af7fed76b8a699a",
    "references": [
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Pawe\u0142 Budzianowski",
                "Tsung-Hsien Wen",
                "Bo-Hsiang Tseng",
                "I\u00f1igo Casanueva",
                "Stefan Ultes",
                "Osman Ramadan",
                "Milica Gasic."
            ],
            "title": "Multiwoz-a largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling",
            "venue": "Proceedings of the",
            "year": 2018
        },
        {
            "authors": [
                "Jie Cao",
                "Yi Zhang."
            ],
            "title": "A comparative study on schema-guided dialogue state tracking",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 782\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Yanda Chen",
                "Ruiqi Zhong",
                "Sheng Zha",
                "George Karypis",
                "He He."
            ],
            "title": "Meta-learning via language model in-context tuning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 719\u2013730.",
            "year": 2022
        },
        {
            "authors": [
                "Mihail Eric",
                "Rahul Goel",
                "Shachi Paul",
                "Adarsh Kumar",
                "Abhishek Sethi",
                "Peter Ku",
                "Anuj Kumar Goyal",
                "Sanchit Agarwal",
                "Shuyang Gao",
                "Dilek Hakkani-Tur"
            ],
            "title": "Multiwoz 2.1: A consolidated multi-domain dialogue dataset with state corrections and state",
            "year": 2019
        },
        {
            "authors": [
                "Shuyang Gao",
                "Sanchit Agarwal",
                "Di Jin",
                "Tagyoung Chung",
                "Dilek Hakkani-Tur."
            ],
            "title": "From machine reading comprehension to dialogue state tracking: Bridging the gap",
            "venue": "Proceedings of the 2nd Workshop on Natural Language Processing for Con-",
            "year": 2020
        },
        {
            "authors": [
                "Ishaan Gulrajani",
                "David Lopez-Paz."
            ],
            "title": "In search of lost domain generalization",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Raghav Gupta",
                "Harrison Lee",
                "Jeffrey Zhao",
                "Abhinav Rastogi",
                "Yuan Cao",
                "Yonghui Wu."
            ],
            "title": "Show, don\u2019t tell: Demonstrations outperform descriptions for schema-guided task-oriented dialogue",
            "venue": "arXiv preprint arXiv:2204.04327.",
            "year": 2022
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Ana Marasovi\u0107",
                "Swabha Swayamdipta",
                "Kyle Lo",
                "Iz Beltagy",
                "Doug Downey",
                "Noah A Smith."
            ],
            "title": "Don\u2019t stop pretraining: Adapt language models to domains and tasks",
            "venue": "Proceedings of the 58th Annual Meeting of the",
            "year": 2020
        },
        {
            "authors": [
                "Yushi Hu",
                "Chia-Hsuan Lee",
                "Tianbao Xie",
                "Tao Yu",
                "Noah A Smith",
                "Mari Ostendorf."
            ],
            "title": "In-context learning for few-shot dialogue state tracking",
            "venue": "arXiv preprint arXiv:2203.08568.",
            "year": 2022
        },
        {
            "authors": [
                "Vojt\u011bch Hude\u010dek",
                "Ond\u0159ej Du\u0161ek",
                "Zhou Yu."
            ],
            "title": "Discovering dialogue slots with weak supervision",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "L\u00e9o Jacqmin",
                "Lina M Rojas Barahona",
                "Benoit Favre."
            ],
            "title": "do you follow me?\u201d: A survey of recent approaches in dialogue state tracking",
            "venue": "Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 336\u2013350.",
            "year": 2022
        },
        {
            "authors": [
                "Jeff Johnson",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou."
            ],
            "title": "Billion-scale similarity search with GPUs",
            "venue": "IEEE Transactions on Big Data, 7(3):535\u2013547.",
            "year": 2019
        },
        {
            "authors": [
                "Adarsh Kumar",
                "Peter Ku",
                "Anuj Goyal",
                "Angeliki Metallinou",
                "Dilek Hakkani-Tur."
            ],
            "title": "Ma-dst: Multiattention-based scalable dialog state tracking",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8107\u20138114.",
            "year": 2020
        },
        {
            "authors": [
                "Chia-Hsuan Lee",
                "Hao Cheng",
                "Mari Ostendorf."
            ],
            "title": "Dialogue state tracking with a language model using schema-driven prompting",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4937\u20134949.",
            "year": 2021
        },
        {
            "authors": [
                "Shuyang Li",
                "Jin Cao",
                "Mukund Sridhar",
                "Henghui Zhu",
                "Shang-Wen Li",
                "Wael Hamza",
                "Julian McAuley."
            ],
            "title": "Zero-shot generalization in dialog state tracking through generative question answering",
            "venue": "Proceedings of the 16th Conference of the European",
            "year": 2021
        },
        {
            "authors": [
                "Zhaojiang Lin",
                "Bing Liu",
                "Andrea Madotto",
                "Seungwhan Moon",
                "Zhenpeng Zhou",
                "Paul A Crook",
                "Zhiguang Wang",
                "Zhou Yu",
                "Eunjoon Cho",
                "Rajen Subba"
            ],
            "title": "Zero-shot dialogue state tracking via crosstask transfer",
            "venue": "In Proceedings of the 2021 Conference",
            "year": 2021
        },
        {
            "authors": [
                "Zhaojiang Lin",
                "Bing Liu",
                "Seungwhan Moon",
                "Paul A Crook",
                "Zhenpeng Zhou",
                "Zhiguang Wang",
                "Zhou Yu",
                "Andrea Madotto",
                "Eunjoon Cho",
                "Rajen Subba."
            ],
            "title": "Leveraging slot descriptions for zero-shot cross-domain dialogue statetracking",
            "venue": "Proceedings",
            "year": 2021
        },
        {
            "authors": [
                "Haokun Liu",
                "Derek Tam",
                "Mohammed Muqeeth",
                "Jay Mohta",
                "Tenghao Huang",
                "Mohit Bansal",
                "Colin Raffel."
            ],
            "title": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
            "venue": "arXiv preprint arXiv:2205.05638.",
            "year": 2022
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Andrea Madotto",
                "Zhaojiang Lin",
                "Genta Indra Winata",
                "Pascale Fung."
            ],
            "title": "Few-shot bot: Promptbased learning for dialogue systems",
            "venue": "arXiv preprint arXiv:2110.08118.",
            "year": 2021
        },
        {
            "authors": [
                "Fei Mi",
                "Yasheng Wang",
                "Yitong Li."
            ],
            "title": "Cins: Comprehensive instruction for few-shot learning in task-oriented dialog systems",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 11076\u201311084.",
            "year": 2022
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Abhinav Rastogi",
                "Xiaoxue Zang",
                "Srinivas Sunkara",
                "Raghav Gupta",
                "Pranav Khaitan."
            ],
            "title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
            "year": 2020
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Darsh Shah",
                "Raghav Gupta",
                "Amir Fayazi",
                "Dilek Hakkani-Tur."
            ],
            "title": "Robust zero-shot cross-domain slot filling with example values",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5484\u20135490.",
            "year": 2019
        },
        {
            "authors": [
                "Jamin Shin",
                "Hangyeol Yu",
                "Hyeongdon Moon",
                "Andrea Madotto",
                "Juneyoung Park."
            ],
            "title": "Dialogue summaries as dialogue states (ds2), template-guided summarization for few-shot dialogue state tracking",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Praveen Venkateswaran",
                "Vatche Isahagian",
                "Vinod Muthusamy",
                "Nalini Venkatasubramanian."
            ],
            "title": "Fedgen: Generalizable federated learning for sequential data",
            "venue": "2023 IEEE 16th International Conference on Cloud Computing (CLOUD), pages 308\u2013318.",
            "year": 2023
        },
        {
            "authors": [
                "Praveen Venkateswaran",
                "Vinod Muthusamy",
                "Vatche Isahagian",
                "Nalini Venkatasubramanian."
            ],
            "title": "Environment agnostic invariant risk minimization for classification of sequential datasets",
            "venue": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge",
            "year": 2021
        },
        {
            "authors": [
                "Praveen Venkateswaran",
                "Vinod Muthusamy",
                "Vatche Isahagian",
                "Nalini Venkatasubramanian."
            ],
            "title": "Robust and generalizable predictive models for business processes",
            "venue": "International Conference on Business Process Management, pages 105\u2013122. Springer.",
            "year": 2021
        },
        {
            "authors": [
                "Tsung-Hsien Wen",
                "David Vandyke",
                "Nikola Mrk\u0161i\u0107",
                "Milica Gasic",
                "Lina M Rojas Barahona",
                "Pei-Hao Su",
                "Stefan Ultes",
                "Steve Young."
            ],
            "title": "A network-based end-to-end trainable task-oriented dialogue system",
            "venue": "Proceedings of the 15th Conference of the Euro-",
            "year": 2017
        },
        {
            "authors": [
                "Jason D Williams",
                "Matthew Henderson",
                "Antoine Raux",
                "Blaise Thomson",
                "Alan Black",
                "Deepak Ramachandran."
            ],
            "title": "The dialog state tracking challenge series",
            "venue": "AI Magazine, 35(4):121\u2013124.",
            "year": 2014
        },
        {
            "authors": [
                "Chien-Sheng Wu",
                "Steven CH Hoi",
                "Caiming Xiong."
            ],
            "title": "Improving limited labeled dialogue state tracking with self-supervision",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4462\u20134472.",
            "year": 2020
        },
        {
            "authors": [
                "Chien-sheng Wu",
                "Andrea Madotto",
                "Ehsan Hosseini-asl",
                "Ciaming Xiong",
                "Richard Socher",
                "Pascale Ngan Fung."
            ],
            "title": "Transferable multi-domain state generator for task-oriented dialogue systems",
            "venue": "Proceedings of the 57th Annual Meeting of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Tianbao Xie",
                "Chen Henry Wu",
                "Peng Shi",
                "Ruiqi Zhong",
                "Torsten Scholak",
                "Michihiro Yasunaga",
                "Chien-Sheng Wu",
                "Ming Zhong",
                "Pengcheng Yin",
                "Sida I Wang"
            ],
            "title": "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text",
            "year": 2022
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "International Conference on Machine Learning, pages 12697\u201312706. PMLR.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Task-oriented dialogue systems are increasingly used to enable users to perform tasks through multiturn conversations in various domains such as travel reservations, banking transactions, or online shopping. Dialogue state tracking (DST) is a critical component of these systems that tracks user requirements by determining key information at each turn in the dialogue (Jacqmin et al., 2022). Given a predefined schema of task parameters (i.e., slots), DST models identify and represent the dialogue state as pairs of slots and their corresponding values as shown in Figure 1.\nIn real-world deployments, new task domains and slots are frequently added to improve user functionality. Hence, periodic updates to the models may be required, even when the new domains offer little to no dialogue data for training. To address\nthese challenges, DST solutions need to be generalizable to new zero-shot and few-shot settings with minimal overhead while also maintaining a small model footprint to enable compute-efficient and cost-effective deployments.\nRecent advances in DST have leveraged pretrained language models (LMs) to elicit slot values from dialogues using primarily two methods \u2013 fine-tuning and in-context learning. However, both methods suffer from several shortcomings \u2013 Fine-tuning LMs \u2013 Most existing approaches condition LMs for DST by fine-tuning their parameters using prompts derived from historical dialogue data. However, they typically rely on handcrafted prompt templates that include slot-specific questions, value based functions, or even text-tocode snippets (Lin et al., 2021b; Cao and Zhang, 2021). In addition to the manual effort required, these templates are customized to specific domains and slots, and hence have low generalizability to new domains. Some approaches also extend finetuning to first train models on additional natural language tasks using external datasets which is expensive and requires access to significantly more data. They also often rely on additional information\nfrom the schema such as slot descriptions and task instructions (Mi et al., 2022). However, real-world datasets may not always have this necessary information, which again limits their generalizability. In-context learning \u2013 LMs have shown remarkable performance through in-context learning of new tasks (Brown et al., 2020), where a raw LM (i.e. pre-trained LM without fine-tuning on taskspecific data) is prompted during inference using input-output task examples to condition the generated output. For DST, approaches have leveraged this to craft prompts containing examples of dialogue history and slot values (Hu et al., 2022). However, similar to fine-tuning approaches, these prompt examples are hand-crafted, customized, and require significant effort. Additionally, as a result of using raw LMs, these approaches have to rely on extremely large models limiting their practical use. Importantly, it has been shown that prompting raw LMs without fine-tuning is often oversensitive to example choices and instruction wording (Chen et al., 2022), and can demonstrate undesirable biases that significantly reduce performance (Zhao et al., 2021; Liu et al., 2021).\nIn this work, we address these challenges and present DiSTRICT, a generalizable approach for dialogue state tracking that fine-tunes a LM using relevant examples (i.e., in-context tuning). For a given input dialogue and slot to be tracked, DiSTRICT retrieves semantically similar dialogues and slots from available historical data in zero-shot and fewshot settings, and concatenates them into a prompt with no hand-crafted template requirements. We first fine-tune the language model using in-context examples and input dialogues from the training set, and subsequently perform similar inference on test inputs as shown in Figure 2. Specifically, we make the following contributions \u2013\n\u2022 To the best of our knowledge, DiSTRICT is the first DST approach to use in-context tuning by fine-tuning a LM with in-context examples.\n\u2022 DiSTRICT avoids shortcomings of prior approaches by leveraging relevant existing dialogue and slot examples without requiring hand-crafted prompts or external datasets, thereby improving generalizability and avoiding manual overhead.\n\u2022 Our evaluation shows that DiSTRICT outperforms existing approaches on most zero-shot\nand few-shot settings, while using a much smaller model, thus demonstrating its practicality and applicability to real-world deployments."
        },
        {
            "heading": "2 Related Work",
            "text": "Dialogue state tracking (DST) is a critical yet challenging task for task-oriented dialogue systems (Williams et al., 2014), and several multi-domain benchmark conversation datasets have been proposed (Budzianowski et al., 2018; Eric et al., 2019; Rastogi et al., 2020) to evaluate research efforts.\nA majority of state-of-the-art approaches finetune language models using hand-crafted templates containing descriptions or questions related to the dialogue slots. Shah et al. (2019) used slot descriptions and examples of slot values to create templates while Lin et al. (2021b) and Lee et al. (2021) provided different types of manually annotated slot descriptions to the model. Mi et al. (2022) extended this by also including task instructions and other constraints. In contrast, our approach does not require hand-crafted templates for fine-tuning and is hence more easily generalizable.\nAnother set of approaches aim to improve zeroshot performance by exploiting external knowledge and datasets from other natural language tasks before fine-tuning a model for DST. For instance, Gao et al. (2020); Li et al. (2021); Lin et al. (2021a) pre-train models on reading comprehension data, Shin et al. (2022) reformulate DST as a dialogue summarization task with external annotated data, and Hudec\u030cek et al. (2021) use semantic analysis and named entity recognition to identify slots. In contrast, our approach does not require any extra datasets or training efforts .\nIn-context learning (ICL) for DST has been explored as part of a larger set of few-shot generative tasks (Madotto et al., 2021; Xie et al., 2022), but the lack of a task-specific prompt design resulted in low performance. Hu et al. (2022) and Gupta et al. (2022) achieved improved performance using extremely large models, where the former reformulated DST as a text-to-SQL task by using semantic matching to identify relevant examples that were subsequently crafted into SQL queries, and the latter manually created example dialogues containing combinations of slots in the schema.\nRecent efforts have shown that the shortcomings of ICL (Liu et al., 2022; Min et al., 2022) can be overcome through in-context tuning of LMs. Gu-\nrurangan et al. (2020) and Liu et al. (2022) demonstrate the improved performance of models finetuned with examples compared to ICL over a variety of language tasks. In this work, we leverage this concept specifically for DST."
        },
        {
            "heading": "3 Approach",
            "text": "We first present the background and some definitions for dialogue state tracking before describing our approach."
        },
        {
            "heading": "3.1 DST Background",
            "text": "A task-oriented dialogue consists of a multi-turn conversation between a user U and the system A. Given a dialogue context Ct as the sequence of utterances until turn t, (i.e.) Ct = [A1, U1, ..., At, Ut], the goal of DST is to predict the dialogue state yt, defined as a set of (slot, value) pairs:\nyt = {(sit, vit) | Ct , \u2200si \u2208 S}\nwhere S denotes the set of possible slots predefined in an ontology or schema. In a multi-domain setting, the schema can comprise of different domains or topics, each corresponding to a service such as restaurant booking or banking. The slots associated with each domain can be either categorical with a set of candidate values (e.g. restaurant-open = \u2018True\u2019 / \u2018False\u2019), or non-categorical, where the value is a span in the dialogue context (e.g. hotelname = \u2018Courtyard Marriott\u2019)."
        },
        {
            "heading": "3.2 In-Context Retriever",
            "text": "The key concept behind our approach is the identification of the most semantically relevant in-context\nexamples from the available training set of dialogues. Intuitively, historical labeled dialogues contain information about slots and their values under different conversational contexts. Hence, for an input dialogue and given query slot, conditioning the model during fine-tuning using example dialogues that are semantically similar and additionally contain the same or similar slots, enables the model to better learn the association between slots, their values, and the context.\nAs shown in Figure 2, the retriever in DiSTRICT performs semantic matching of the input dialogue and slot with single-turn training set conversations as examples (i.e. one pair of user-system utterances). This design choice is due to the fact that large prompts require additional memory and compute, significantly increasing the training time of the model. Furthermore, real-world dialogues can be lengthy, and the context needed to find the value of a particular slot can often be limited to a single sentence. Hence, constraining the in-context examples to single-turn conversations reduces the prompt size, enables the addition of more examples, and removes irrelevant dialogue context.\nFormally, we define a dataset D = {(ej , sij , vij)} consisting of single-turn dialogue examples ej , containing an observed slot sij and its corresponding value vij . For a given input dialogue context Ct and query slot sq, we retrieve the k most relevant examples Ek \u2208 D based on the similarity between their text embeddings \u2013\nEk = max k {sim[(Ct \u2295 sq)emb , Demb]}\nwhere \u2295 denotes concatenation."
        },
        {
            "heading": "3.3 Applicability to Zero-shot and Few-shot",
            "text": "To illustrate the generalizability of the retriever to zero-shot and few-shot settings, we use the example shown in Figure 2. Given a test input dialogue from the restaurant domain and the query slot restaurant-people, the retriever identifies the k most relevant single-turn in-context examples derived from the training set.\nIn a zero-shot setting (Figure 2-(1)), dialogue and slot examples from the restaurant domain would not be available. Hence, the retriever identifies semantically similar examples from other domains. For instance, conversations about hotel reservations have similar contexts, and the slot hotel-people is semantically similar to the query slot. The retrieved example thus conditions the model to look for the number of people mentioned in the dialogue.\nIn few-shot and full-shot settings (Figure 2-(2)) the set of available examples would include other dialogues from the same domain which could also contain the query slot. Hence, the most similar examples retrieved would demonstrate the values of the query slot when used in similar restaurant reservation contexts (e.g. restaurant-people: 8). We note that our approach requires no changes for the different settings, and can be easily extended to include additional information like slot descriptions to further enhance the semantic retrieval."
        },
        {
            "heading": "3.4 In-context Tuning",
            "text": "We fine-tune the language model by retrieving incontext single-turn dialogue examples for each dialogue in the training set. As shown in Figure 2, to create the input to the model, we annotate prefixes to each of the k in-context examples Ek, dialogue context Ct, and the query slot sq to enable the model to distinguish between them, and then concatenate them into a single input sequence. We then fine-tune an encoder-decoder based language model, where the input is passed to the encoder, and the decoder generates the corresponding value for the query slot. The model in-context tuning objective L is to minimize the negative log-likelihood loss \u2013\nL(\u03b8) = \u2212 n\u2211\ni=0\nlog p(vi | Ek \u2295 Ct \u2295 sq)\nwhere n is the total number of slots in the ontology and \u2295 denotes concatenation."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets and Evaluation",
            "text": "MultiWOZ (Budzianowski et al., 2018) is a multidomain task-oriented dialogue benchmark dataset that consists of around 10k multi-turn dialogues over 7 domains. The dataset has been refined and erroneous annotations have been corrected over multiple versions. To enable comparisons with most existing approaches, we use the MultiWOZ 2.0 and MultiWOZ 2.1 (Eric et al., 2019) datasets in our evaluation, and follow the same data preprocessing and domain selection steps as prior work (Wu et al., 2019; Gao et al., 2020; Lin et al., 2021b). To make consistent comparisons to prior work in zero-shot and few-shot settings, we use the same Joint Goal Accuracy (JGA) metric to evaluate our approach. For a given turn, JGA compares predicted dialogue states to the corresponding ground truth states and considers the prediction as accurate if and only if all predicted values match the ground truth values (Wu et al., 2019; Wen et al., 2017; Kumar et al., 2020)."
        },
        {
            "heading": "4.2 Implementation",
            "text": "DiSTRICT uses T5-small (60M parameters) (Raffel et al., 2020) which is one of the smallest pretrained models available1. It has 6 encoder-decoder layers and the size of each layer is 512. We fine-tune using an AdamW optimizer (Loshchilov and Hutter, 2018) with an initial learning rate of 1e\u22124. For the retriever, we use Sentence-BERT (Reimers and Gurevych, 2019) and perform semantic search with cosine similarity using the FAISS library (Johnson et al., 2019). Unless specified otherwise, we set the number of in-context examples to be k = 3. We use a single NVIDIA V100 GPU\n1https://huggingface.co/transformers/ v2.9.1/pretrained_models.html\nfor our experiments and provide further details in the appendix."
        },
        {
            "heading": "4.3 Comparison Baselines",
            "text": "We evaluate DiSTRICT against existing DST baselines as shown in Table 1. The table shows that, except for T5DST which also uses T5-small, prior DST approaches use models that are significantly larger compared to DiSTRICT.\nTRADE (Wu et al., 2019) uses slot and domain embeddings as well as a copy mechanism to track slot values across domains.\nSTARC (Gao et al., 2020) prompts two different instances of the RoBERTa-large (Liu et al., 2019) model with separate natural language questions for categorical and non-categorical slots.\nT5-DST (Lin et al., 2021b) fine-tunes a T5-small model (Raffel et al., 2020) with multiple hand-crafted prompts that include questions and different descriptions of slots.\nTransferQA (Lin et al., 2021a) represents DST as a QA task, where the model is pre-trained on six external QA datasets and individual questions are manually crafted for each slot in the ontology to use in the prompt.\nDS2 (Shin et al., 2022) treats DST as a dialogue summarization task, and fine-tunes T5-large and BART models with synthetic summary templates.\nIC-DST (Hu et al., 2022) reformulates DST as a text-to-SQL task and transforms relevant in-context examples to SQL queries and prompts a Codex model without any fine-tuning."
        },
        {
            "heading": "4.4 Experimental Settings",
            "text": "Zero-shot \u2013 Similar to prior work (Lin et al., 2021a; Wu et al., 2019), the retriever and model have access to training data from all domains except from one \u2018unseen\u2019 domain, on which the model is evaluated. We note that our retriever does not result in any information leakage since no examples and slots are included from the unseen domain. Cross-domain few-shot \u2013 We include three fewshot settings, where the retriever and model additionally have access to 1%, 5%, and 10% of training data from the unseen domain, similar to Shin et al. (2022); Wu et al. (2019). Multi-domain few-shot \u2013 We follow the multidomain scenario from Shin et al. (2022); Wu et al. (2020), where 1%, 5%, and 10% of the entire training data are sampled for model training and retrieval. Note: We do not include the zero-shot results from prior in-context learning work (Hu et al., 2022) (ICDST) since their prompt examples are designed to include information from the \u2018unseen\u2019 domain, which results in information leakage to the model, and hence does not reflect the traditional zero-shot learning setting. Additionally, we include results from the other comparison approaches where available."
        },
        {
            "heading": "4.5 Results",
            "text": "Zero-shot DST\nTable 2 shows the dialogue state tracking performance of DiSTRICT in zero-shot settings along with the available baseline results for TRADE, T5DST, and TransferQA. We observe that DiSTRICT outperforms the baseline approaches in most domains across both datasets. DiSTRICT achieves an 8% improvement in JGA on average\nover the next best approach on both datasets (i.e, T5DST in MultiWoz 2.0 and TransferQA in MultiWoz 2.1), and obtains improvements up to 23% on the \u2018Train\u2019 domain.\nBoth TransferQA and T5DST use hand-crafted prompts, where the former annotates all slots in the form of questions, and the latter uses individual slot descriptions. In the zero-shot setting, this implies that the query-slot is not truly \"unseen\", since semantic information about the slot is being provided to the model in the hand-crafted prompt. Furthermore, the addition of new domains and slots would first require crafting new prompts, thereby limiting generalizability.\nIn contrast, DiSTRICT does not possess any additional information about the unseen query slot and instead relies on identifying other semantically similar slots and dialogues from the data available from other domains to enable model reasoning. The improved performance hence reflects the effectiveness of our retriever driven approach in zero-shot settings, and also demonstrates the generalizability of our solution.\nPer-domain few-shot DST Tables 3 and 4 show the per-domain few-shot performance on MultiWOZ 2.0 and MultiWOZ 2.1 respectively. DiSTRICT outperforms the baseline approaches across across all domains and across both datasets. For MultiWOZ 2.1, DiSTRICT achieves a JGA improvement of over 15% in the best-case (\"Attraction\" domain at 1%) and 9% on average, compared to the next best approach, DS2-T5.\nAdditionally, the availability of even a few labeled examples significantly improves the performance of our retriever, as evidenced by a 46% improvement in JGA on average across all domains over the zero-shot setting from Table 2 with just 1% of available few-shot data in MultiWOZ 2.1, compared to a 34% improvement by TransferQA. This improvement stems from the increased relevance of available in-context examples, since the retriever now has access to a few (i.e. 1%-5%-10%) dialogues from the few-shot domain.\nCross-domain few-shot and full-shot DST From Table 5, we see that DiSTRICT achieves the best performance in the full-shot (100%) setting, obtaining \u223c 10% improvement in JGA on average over the other approaches. However, we observe a significant drop in performance in the cross-domain few-shot setting, when the total available training data is reduced. DiSTRICT suffers from a 75% drop in JGA when only 1% of training data is available in MultiWOZ 2.1, compared to a 35% drop for DS2-T5 which suffered the smallest drop in performance.\nThis performance drop can be attributed to the limited diversity of in-context examples arising from the unavailability of training data. For instance, the 1% setting in MultiWOZ 2.1 translates to the availability of only 84 training examples. Hence, the retriever is limited to repeatedly using the same examples, restricting the model\u2019s reasoning capabilities. In contrast, the hand-crafted prompts used by the baseline approaches appear\nto provide sufficient additional information to the model, reducing performance drop in limited data settings."
        },
        {
            "heading": "4.6 Additional Experiments",
            "text": "Impact of number of in-context examples We study the effect of varying the retrieved number of in-context examples used in the prompt on DiSTRICT\u2019s performance in all our experimental settings. From Figure 3, we observe that using no examples (i.e.) model fine-tuning and inference using only input dialogues, results in very poor performance that is worse than all the baseline comparison approaches. This shows that the addition of relevant examples has a significant impact on conditioning the model for dialogue state tracking.\nWe also observe an improvement in performance as the number of in-context examples increases, highlighting the potential of using a larger number of examples as part of future work. However, this involves a trade-off, since the improvement is not linear and has diminishing returns, and using a larger number of examples would require more memory, compute resources, and increased training time.\nRetriever design As described in Section 3.2, DiSTRICT uses the entire dialog context until the current turn as the query to the retriever to identify relevant examples. We compare the performance of this design choice against using only the utterances of the current turn to obtain examples.\nTable 6 shows that using the entire context performs better than the single-turn approach for various settings in the MultiWOZ 2.1 dataset. We observed several examples, where the system\u2019s response in the current turn was incorrect (e.g. recommending italian restaurants instead of indian as asked by the user in prior turns). Hence, using only the current turn resulted in low-relevancy examples being retrieved (involving italian restaurants), whereas using the entire dialog context ensured the retrieval of more relevant examples (involving indian restaurants), thereby demonstrating the importance of using the dialog context.\nRetriever performance We examined the effectiveness of the retriever by analyzing the domains and slots that were selected\nfor the input dialogues in the test set. Figure 4 shows the heatmaps for zero-shot and per-domain few-shot settings, depicting the relative number of examples picked from each domain for test inputs across all domains.\nIn the zero-shot setting, since data from the unseen test domain is unavailable, the main diagonal is empty and we observed that examples were relatively evenly picked across the other available domains. In particular, as illustrated by the examples in Table 7, we found that the retriever identified examples containing semantically similar slots or having similar dialogue contexts, thereby demonstrating the effectiveness of our approach.\nIn the few-shot setting, we observed that the majority of examples were selected from the same domain as the input (i.e. darker diagonals), reflecting the higher semantic and contextual match between intra-domain dialogues. We also studied the distribution of examples at an individual slot level, shown in the appendix, and observed the same patterns. In particular, for the few-shot setting, the retriever prioritized examples containing the same slot, followed by the same domain, validating the\nuse of semantic matching.\nImpact of model size\nFinally, we studied the effectiveness of using larger models for DST. We evaluate the performance of DiSTRICT and T5DST, as both approaches use the T5-small model (Table 1), with multiple sizes of T5 in the zero-shot setting with the \u2018Taxi\u2019 domain. As shown in Table 8, in both approaches the larger T5-base and T5-large models achieve modest improvements over the T5-small model. However, these improvements may be too limited to justify the potentially significant increase in compute resources required to support larger model sizes in real-world deployments.\nAblation study\nWe compare the performance of DiSTRICT against variants that use the same model, but have differences in the in-context tuning pipeline shown in Figure 2. We evaluate the variants on the zero-shot setting, using the MultiWOZ 2.1 dataset. We define three variants \u2013 the first does not perform any finetuning (i.e.) in-context learning, using examples from the same retriever as DiSTRICT. The second and third variants perform in-context fine-tuning, but instead use a random retriever (i.e.) select k examples randomly, and a non-parametric BM25 retriever respectively.\nThe results (Table 9) show that smaller models like \u2019T5-small\u2019 cannot learn to perform DST without any fine-tuning, further highlighting the tradeoffs between performance, model size, and resource requirements. The performance drop with the use of random examples can be attributed to both biasing the model towards incorrect answers corresponding to other domains/slots, and also withholding valuable information that would have been present in the relevant examples used in DiSTRICT. Furthermore, identifying relevant dialogue examples is heavily reliant on their semantic meaning. Hence, functions like BM25 which is based on bag-of-words retrieval, also perform poorly since they ignore semantic similarity and instead rely on word frequency that often does not accurately reflect the meaning of the dialogue. This serves to show that DiSTRICT\u2019s retriever driven in-context tuning approach (Figure 2) plays a big role in enabling effective dialogue state tracking."
        },
        {
            "heading": "5 Conclusion",
            "text": "We present DiSTRICT, a novel approach for dialogue state tracking using in-context tuning of language models (LMs). For an input dialogue instance and slots, DiSTRICT retrieves the most relevant examples from the training data through semantic matching, and uses these examples as part of the input to the LM to obtain the dialogue state. The fully automated prompt construction, with-\nout requiring hand-crafted templates or additional schema information, overcomes drawbacks of prior DST approaches and also reflects the high generalizability of DiSTRICT to new task domains and slots. Our experiments show that DiSTRICT outperforms existing baselines in different zero-shot and few-shot experiments despite using a smaller and lower-resource model. We also demonstrate the effectiveness of our semantic-search based retriever for the DST task and highlight several tradeoffs between model performance and resource requirements that impact real-world use. As part of future work, we intend to improve robustness to dialogue quality and distribution-drifts."
        },
        {
            "heading": "6 Limitations",
            "text": "The performance of DiSTRICT hinges on the effective retrieval of relevant in-context examples from the training data. This results in our approach being sensitive to issues with data quantity and quality. As shown in our results, when the amount of training data is limited, the retriever often has to select from a pool of examples that have low diversity and semantic similarity to the input, thereby adversely impacting performance.\nAdditionally, data quality issues such as poorly named slots (i.e. not sufficiently descriptive) and incorrect/mislabeled slot values would also impact the semantic matching and performance of our approach. Also, our zero-shot learning relies on semantic relationships between the unseen samples and the known data. However, if the new task domains are highly disparate from the existing domains, this relationship may not hold, presenting a challenge for zero-shot learning.\nRecently, research efforts have studied domain generalization in the context of model robustness under data distribution shifts (i.e.) out-ofdistribution (OOD) generalization (Gulrajani and Lopez-Paz, 2020; Venkateswaran et al., 2021a,b, 2023) which can also occur in real-world task oriented dialogue systems. We did not address this as part of our work, and intend to explore OOD model robustness as part of future work."
        },
        {
            "heading": "B Retriever Performance",
            "text": "We analyzed the distribution of in-context examples at a slot level for different test inputs. Figures 5 and 6 show the heatmap depicting the slots within the in-context examples that were picked for each test query slot for zero-shot and few-shot settings.\nFor the zero-shot setting, we observed that whenever possible, the retriever prioritized examples containing slots that had a similar semantic meaning as the query slot (e.g.) restaurant-area and attraction-area, hotel-price range and restaurant-price range, train-arrive by and taxi-arrive by. In cases where the query slot had no similar example slots (e.g.) hotel-internet, the retriever picked examples based on the dialogue context similarity.\nFor the few-shot setting, we observed that the retriever prioritized examples containing the same slot as the query, reflected by the dark diagonal in the heatmap. Additionally, the retriever also typically picked examples from the same domain as the test input, which is shown clearly by the clusters within the heatmap. This serves to show that identifying examples using semantic matching is a viable and effective approach."
        }
    ],
    "title": "DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context Tuning",
    "year": 2023
}