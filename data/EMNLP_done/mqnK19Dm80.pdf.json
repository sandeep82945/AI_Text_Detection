{
    "abstractText": "Emotion Cause Triplet Extraction in Conversations (ECTEC) aims to simultaneously extract emotion utterances, emotion categories, and cause utterances from conversations. However, existing studies mainly decompose the ECTEC task into multiple subtasks and solve them in a pipeline manner. Moreover, since conversations tend to contain many informal and implicit expressions, it often requires external knowledge and reasoning-based inference to accurately identify emotional and causal clues implicitly mentioned in the context, which are ignored by previous work. To address these limitations, in this paper, we propose a commonSense knowledge-enHanced generAtive fRameworK named SHARK, which formulates the ECTEC task as an index generation problem and generates the emotion-causecategory triplets in an end-to-end manner with a sequence-to-sequence model. Furthermore, we propose to incorporate both retrieved and generated commonsense knowledge into the generative model via a dual-view gate mechanism and a graph attention layer. Experimental results show that our SHARK model consistently outperforms several competitive systems on two benchmark datasets. Our source codes are publicly released at https://github.com/ NUSTM/SHARK.",
    "authors": [
        {
            "affiliations": [],
            "name": "Fanfan Wang"
        },
        {
            "affiliations": [],
            "name": "Jianfei Yu"
        },
        {
            "affiliations": [],
            "name": "Rui Xia"
        }
    ],
    "id": "SP:c1787e6349cc93e4e3c5e4a4003a649b5634084a",
    "references": [
        {
            "authors": [
                "Carlos Busso",
                "Murtaza Bulut",
                "Chi-Chun Lee",
                "Abe Kazemzadeh",
                "Emily Mower",
                "Samuel Kim",
                "Jeannette N Chang",
                "Sungbok Lee",
                "Shrikanth S Narayanan."
            ],
            "title": "Iemocap: Interactive emotional dyadic motion capture database",
            "venue": "Language resources",
            "year": 2008
        },
        {
            "authors": [
                "Shunjie Chen",
                "Xiaochuan Shi",
                "Jingye Li",
                "Shengqiong Wu",
                "Hao Fei",
                "Fei Li",
                "Donghong Ji."
            ],
            "title": "Joint alignment of multi-task feature and label spaces for emotion cause pair extraction",
            "venue": "Proceedings of the 29th International Conference on Computational",
            "year": 2022
        },
        {
            "authors": [
                "Xinhong Chen",
                "Qing Li",
                "Jianping Wang."
            ],
            "title": "A unified sequence labeling model for emotion cause pair extraction",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 208\u2013218.",
            "year": 2020
        },
        {
            "authors": [
                "Zifeng Cheng",
                "Zhiwei Jiang",
                "Yafeng Yin",
                "Na Li",
                "Qing Gu."
            ],
            "title": "A unified target-oriented sequenceto-sequence model for emotion-cause pair extraction",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:2779\u20132791.",
            "year": 2021
        },
        {
            "authors": [
                "Zifeng Cheng",
                "Zhiwei Jiang",
                "Yafeng Yin",
                "Cong Wang",
                "Shiping Ge",
                "Qing Gu"
            ],
            "title": "A consistent dualmrc framework for emotion-cause pair extraction",
            "year": 2023
        },
        {
            "authors": [
                "Zixiang Ding",
                "Rui Xia",
                "Jianfei Yu"
            ],
            "title": "2020a. ECPE2D: Emotion-cause pair extraction based on joint two-dimensional representation, interaction and prediction",
            "venue": "In Association for Computational Linguistics (ACL),",
            "year": 2020
        },
        {
            "authors": [
                "Zixiang Ding",
                "Rui Xia",
                "Jianfei Yu."
            ],
            "title": "End-toend emotion-cause pair extraction based on sliding window multi-label learning",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3574\u20133583.",
            "year": 2020
        },
        {
            "authors": [
                "Paul Ekman"
            ],
            "title": "Basic emotions",
            "venue": "Handbook of cognition and emotion, 98(45-60):16.",
            "year": 1999
        },
        {
            "authors": [
                "Chuang Fan",
                "Hongyu Yan",
                "Jiachen Du",
                "Lin Gui",
                "Lidong Bing",
                "Min Yang",
                "Ruifeng Xu",
                "Ruibin Mao."
            ],
            "title": "A knowledge regularized hierarchical approach for emotion cause analysis",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Chuang Fan",
                "Chaofa Yuan",
                "Jiachen Du",
                "Lin Gui",
                "Min Yang",
                "Ruifeng Xu."
            ],
            "title": "Transition-based directed graph construction for emotion-cause pair extraction",
            "venue": "Association for Computational Linguistics (ACL), pages 3707\u20133717.",
            "year": 2020
        },
        {
            "authors": [
                "Chuang Fan",
                "Chaofa Yuan",
                "Lin Gui",
                "Yue Zhang",
                "Ruifeng Xu."
            ],
            "title": "Multi-task sequence tagging for emotion-cause pair extraction via tag distribution refinement",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:2339\u20132350.",
            "year": 2021
        },
        {
            "authors": [
                "Mauajama Firdaus",
                "Hardik Chauhan",
                "Asif Ekbal",
                "Pushpak Bhattacharyya."
            ],
            "title": "Meisd: A multimodal multi-label emotion, intensity and sentiment dialogue dataset for emotion recognition and sentiment analysis in conversations",
            "venue": "Proceedings of the 28th",
            "year": 2020
        },
        {
            "authors": [
                "Deepanway Ghosal",
                "Navonil Majumder",
                "Alexander Gelbukh",
                "Rada Mihalcea",
                "Soujanya Poria."
            ],
            "title": "Cosmic: Commonsense knowledge for emotion identification in conversations",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Herbert P Grice."
            ],
            "title": "Logic and conversation",
            "venue": "Speech acts, pages 41\u201358. Brill.",
            "year": 1975
        },
        {
            "authors": [
                "Xiaojie Gu",
                "Renze Lou",
                "Lin Sun",
                "Shangxin Li."
            ],
            "title": "Page: A position-aware graph-based model for emotion cause entailment in conversation",
            "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages",
            "year": 2023
        },
        {
            "authors": [
                "Lin Gui",
                "Jiannan Hu",
                "Yulan He",
                "Ruifeng Xu",
                "Qin Lu",
                "Jiachen Du."
            ],
            "title": "A question answering approach for emotion cause extraction",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1593\u20131602.",
            "year": 2017
        },
        {
            "authors": [
                "Lin Gui",
                "Dongyin Wu",
                "Ruifeng Xu",
                "Qin Lu",
                "Yu Zhou"
            ],
            "title": "Event-driven emotion cause extraction with corpus construction",
            "venue": "In EMNLP,",
            "year": 2016
        },
        {
            "authors": [
                "Lin Gui",
                "Ruifeng Xu",
                "Qin Lu",
                "Dongyin Wu",
                "Yu Zhou."
            ],
            "title": "Emotion cause extraction, a challenging task with corpus construction",
            "venue": "Chinese National Conference on Social Media Processing, pages 98\u2013109.",
            "year": 2016
        },
        {
            "authors": [
                "Jena D Hwang",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Jeff Da",
                "Keisuke Sakaguchi",
                "Antoine Bosselut",
                "Yejin Choi."
            ],
            "title": "comet-) atomic 2020: on symbolic and neural commonsense knowledge graphs",
            "venue": "Proceedings of the AAAI Conference on Artificial",
            "year": 2021
        },
        {
            "authors": [
                "Dongjin Jeong",
                "Jinyeong Bak."
            ],
            "title": "Conversational emotion-cause pair extraction with guided mixture of experts",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 3280\u20133290.",
            "year": 2023
        },
        {
            "authors": [
                "Sophia Yat Mei Lee",
                "Ying Chen",
                "Chu-Ren Huang."
            ],
            "title": "A text-driven rule-based system for emotion cause detection",
            "venue": "NAACL HLT Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 45\u201353.",
            "year": 2010
        },
        {
            "authors": [
                "Sophia Yat Mei Lee",
                "Ying Chen",
                "Shoushan Li",
                "Chu-Ren Huang"
            ],
            "title": "Emotion cause events: Corpus construction and analysis",
            "year": 2010
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jiangnan Li",
                "Fandong Meng",
                "Zheng Lin",
                "Rui Liu",
                "Peng Fu",
                "Yanan Cao",
                "Weiping Wang",
                "Jie Zhou."
            ],
            "title": "Neutral utterances are also causes: Enhancing conversational causal emotion entailment with social commonsense knowledge",
            "venue": "IJCAI, pages 4209\u20134215.",
            "year": 2022
        },
        {
            "authors": [
                "Wei Li",
                "Yang Li",
                "Vlad Pandelea",
                "Mengshi Ge",
                "Luyao Zhu",
                "Erik Cambria."
            ],
            "title": "Ecpec: emotioncause pair extraction in conversations",
            "venue": "IEEE Transactions on Affective Computing.",
            "year": 2022
        },
        {
            "authors": [
                "Yanran Li",
                "Hui Su",
                "Xiaoyu Shen",
                "Wenjie Li",
                "Ziqiang Cao",
                "Shuzi Niu."
            ],
            "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing, pages 986\u2013995.",
            "year": 2017
        },
        {
            "authors": [
                "Navonil Majumder",
                "Soujanya Poria",
                "Devamanyu Hazarika",
                "Rada Mihalcea",
                "Alexander Gelbukh",
                "Erik Cambria."
            ],
            "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
            "venue": "Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Soujanya Poria",
                "Devamanyu Hazarika",
                "Navonil Majumder",
                "Gautam Naik",
                "Erik Cambria",
                "Rada Mihalcea."
            ],
            "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
            "venue": "Proceedings of the 57th Annual Meeting of the Associ-",
            "year": 2019
        },
        {
            "authors": [
                "Soujanya Poria",
                "Navonil Majumder",
                "Devamanyu Hazarika",
                "Deepanway Ghosal",
                "Rishabh Bhardwaj",
                "Samson Yu Bai Jian",
                "Pengfei Hong",
                "Romila Ghosh",
                "Abhinaba Roy",
                "Niyati Chhaya"
            ],
            "title": "Recognizing emotion cause in conversations",
            "year": 2021
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Maarten Sap",
                "Ronan Le Bras",
                "Emily Allaway",
                "Chandra Bhagavatula",
                "Nicholas Lourie",
                "Hannah Rashkin",
                "Brendan Roof",
                "Noah A Smith",
                "Yejin Choi."
            ],
            "title": "Atomic: An atlas of machine commonsense for ifthen reasoning",
            "venue": "Proceedings of the AAAI confer-",
            "year": 2019
        },
        {
            "authors": [
                "Xiaohui Song",
                "Longtao Huang",
                "Hui Xue",
                "Songlin Hu."
            ],
            "title": "Supervised prototypical contrastive learning for emotion recognition in conversation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5197\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Fanfan Wang",
                "Zixiang Ding",
                "Rui Xia",
                "Zhaoyu Li",
                "Jianfei Yu."
            ],
            "title": "Multimodal emotion-cause pair extraction in conversations",
            "venue": "IEEE Transactions on Affective Computing, pages 1\u201312.",
            "year": 2022
        },
        {
            "authors": [
                "Penghui Wei",
                "Jiahao Zhao",
                "Wenji Mao."
            ],
            "title": "Effective inter-clause modeling for end-to-end emotioncause pair extraction",
            "venue": "Association for Computational Linguistics (ACL), pages 3171\u20133181.",
            "year": 2020
        },
        {
            "authors": [
                "Rui Xia",
                "Zixiang Ding."
            ],
            "title": "Emotion-cause pair extraction: A new task to emotion analysis in texts",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1003\u2013 1012.",
            "year": 2019
        },
        {
            "authors": [
                "Chaofa Yuan",
                "Chuang Fan",
                "Jianzhu Bao",
                "Ruifeng Xu."
            ],
            "title": "Emotion-cause pair extraction as sequence labeling based on a novel tagging scheme",
            "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Duzhen Zhang",
                "Zhen Yang",
                "Fandong Meng",
                "Xiuyi Chen",
                "Jie Zhou."
            ],
            "title": "Tsam: A two-stream attention model for causal emotion entailment",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 6762\u20136772.",
            "year": 2022
        },
        {
            "authors": [
                "Weixiang Zhao",
                "Yanyan Zhao",
                "Zhuojun Li",
                "Bing Qin."
            ],
            "title": "Knowledge-bridged causal interaction network for causal emotion entailment",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.",
            "year": 2023
        },
        {
            "authors": [
                "Xiaopeng Zheng",
                "Zhiyue Liu",
                "Zizhen Zhang",
                "Zhaoyang Wang",
                "Jiahai Wang."
            ],
            "title": "Ueca-prompt: Universal prompt for emotion cause analysis",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 7031\u20137041.",
            "year": 2022
        },
        {
            "authors": [
                "Changzhi Zhou",
                "Dandan Song",
                "Jing Xu",
                "Zhijing Wu."
            ],
            "title": "A multi-turn machine reading comprehension framework with rethink mechanism for emotioncause pair extraction",
            "venue": "Proceedings of the 29th International Conference on Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Lixing Zhu",
                "Gabriele Pergola",
                "Lin Gui",
                "Deyu Zhou",
                "Yulan He."
            ],
            "title": "Topic-driven and knowledgeaware transformer for dialogue emotion detection",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Emotion understanding is a key component of human-like artificial intelligence, as emotions are intrinsic to humans and significantly influence our cognition, decision-making, and social interactions. Conversations, as a fundamental form of human communication, are replete with diverse emotions. Beyond mere emotion recognition, delving into the triggers behind these emotions in conversations is a more intricate and less explored task. A comprehensive understanding of both the speaker\u2019s emotions and their causes facilitates many applications\n\u2217Corresponding authors.\nAs a result, others feel confused.\nCharlie Ross\nAs a result, Charlie feels sad.\n\ud83d\ude14Uh, well... Joey and I broke up.U1\nOh my God, wh... what happened? U2\ud83d\ude32\nJoey is a great guy, but we are just... so different! I mean, during your speech he kept laughing at homo erectus! U3 \ud83d\ude14\nI knew that was him! U4\ud83d\ude20\nAs a result, Charlie feels sad.\nAs a result, others feel annoyed.\nECTEC output (EmoAon, Cause, Category)\nsuch as customer support, mental health care, and human-computer interaction. Therefore, Emotion Cause Analysis in Conversations (ECAC) has been gaining increasing attention from both academia and industry in recent years.\nMost existing studies on ECAC primarily focus on Causal Emotion Entailment (CEE) and EmotionCause Pair Extraction in Conversations (ECPEC). The former line of work on CEE assumes that the emotion utterances are given and formulates the ECAC task as an utterance classification problem (Poria et al., 2021; Li et al., 2022a; Zhang et al., 2022; Zhao et al., 2023; Gu et al., 2023), which aims to predict whether each utterance in a conversation is the cause of the given emotion utterance. The latter line of work on ECPEC focuses on designing different multi-task learning architectures to jointly extract the emotion utterances and\ntheir corresponding causes in a pipeline manner (Li et al., 2022b; Jeong and Bak, 2023).\nDue to the importance of emotion categories in ECAC, Wang et al. (2022) recently explored a task named Emotion Cause Triplet Extraction in Conversations (ECTEC), which aims to simultaneously extract emotion utterances, emotion categories, and cause utterances from conversations. For example, given the conversation in Figure 1, it is expected to identify four emotion-cause-category triplets. To tackle the task, they further proposed a two-step approach, which first extracts emotion utterances with emotion category and cause utterances separately, followed by pairing them to obtain valid emotion-cause-category triplets.\nHowever, all the aforementioned studies on ECAC still suffer from two limitations. First, existing works on CEE and ECPEC primarily decompose ECTEC into several subtasks and only focus on tackling one or two subtasks. Although Wang et al. (2022) attempted to address the ECTEC task, their pipeline approach still suffers from the error propagation issue. To the best of our knowledge, there is still a lack of an end-to-end approach to generate all the emotion-cause-category triplets in one shot. Second, since interlocutors usually rely on the dialogue history and commonsense knowledge (CSK) to make sense of others\u2019 utterances and respond succinctly rather than explicitly (Grice, 1975), it often requires external knowledge and reasoning-based inference to accurately identify emotional and causal clues in the conversation. For example, in Figure 1, we can find that the CSK reasoned from U3 not only indicates Charlie\u2019s sadness in U3 and Ross\u2019s anger in U4, but also implies that U3 contains the causes behind their emotions. With the clues inferred from CSK, the two triplets \u201c(U3,U3,surprise)\u201d and \u201c(U4,U3,anger)\u201d can be easily inferred. Despite the importance of CSK, it is still under-explored how to utilize CSK to facilitate the ECTEC task.\nTo address these limitations, in this paper, we propose a commonSense knowledge-enHanced generAtive fRameworK named SHARK, which incorporates CSK into a pre-trained sequence-tosequence model BART (Lewis et al., 2020) to generate all the emotion-cause-category triplets in an end-to-end manner. Specifically, SHARK formulates the ECTEC task as an index generation problem, which linearizes each emotion-cause-category triplet into a position index triplet. It then em-\nploys BART to encode the input conversation, followed by decoding a set of triplets containing the indexes of emotion utterances, cause utterances, and emotion categories. Moreover, to incorporate CSK into the BART-based framework, SHARK feeds each utterance to a pre-trained neural knowledge model COMET-ATOMIC2020 (Hwang et al., 2021) for commonsense knowledge generation and a widely-used knowledge base ATOMIC (Sap et al., 2019) for commonsense knowledge retrieval. Next, SHARK integrates both generated and retrieved knowledge via a dual-view gate mechanism to obtain the knowledge representation, and then introduces a knowledge-aware graph attention layer to capture the intra-speaker and inter-speaker interactions in the conversation. Finally, the knowledgeenhanced utterance representation is used to generate the position index sequence.\nOur contributions are summarized as follows:\n\u2022 We formulate the ECTEC task as an index generation problem by linearizing all emotion-causecategory triplets into a position index sequence, and employ a BART-based encoder-decoder framework to generate the index sequence from the input conversation.\n\u2022 We further introduce a dual-view gate mechanism to integrate both retrieved and generated knowledge to obtain the knowledge representation, and then incorporate it into the BARTbased framework via a knowledge-aware graph attention layer.\n\u2022 Experimental results demonstrate that our generative framework consistently performs better than a number of competitive systems on two benchmark datasets. Further in-depth analysis shows the importance of the commonsense knowledge for the ECTEC task."
        },
        {
            "heading": "2 Methodology",
            "text": ""
        },
        {
            "heading": "2.1 Task Formulation and Model Overview",
            "text": "Given a conversation containing n utterances D = [U1, . . . , Ui, . . . , UN ], in which each utterance Ui corresponds to a speaker Si, the goal of ECTEC is to extract all the emotion cause triplets:\nP = { . . . , (U ej , U c k , y e), . . . } , (1)\nwhere U ej is an emotion utterance with certain emotion ye, U ck is the corresponding cause utterance. The emotion category ye is one of the six basic\nemotions defined by Ekman et al. (1999), including Anger, Disgust, Fear, Joy, Sadness and Surprise.\nAs illustrated in Figure 2, our proposed commonSense knowledge-enHanced generAtive fRameworK (SHARK) formulates the ECTEC task as an index generation problem, which linearizes all the emotion-cause-category triplets into their corresponding position indexes to obtain a position index sequence, and employs a pre-trained sequence-tosequence model BART as the backbone to generate the position index sequence in an end-to-end manner. To incorporate CSK into the generative model, SHARK utilizes an external knowledge base and a pre-trained model to both retrieve and generate emotion-oriented CSK for each utterance, as shown in the left of Figure 2. Next, SHARK further introduces a dual-view gate mechanism to fuse the CSK representation, and then leverages it to enhance the utterance representation via a graph attention layer in the middle of Figure 2.\nIn the next subsections, we will first present the BART-based index generation framework, and then describe the details of the dual-view gate mechanism and the knowledge-aware graph attention layer."
        },
        {
            "heading": "2.2 Index Generation Framework",
            "text": "Given a conversation, we use the pre-trained BART model as the backbone to generate the output index sequence of the emotion-cause-category triplets.\nEncoder. The encoder is a multi-layer bidirectional Transformer, which encodes the input sequence into the hidden representation. Specifically, we concatenate all the utterances in a conversation, and add several special tokens before\neach utterance to obtain the input sequence: X = <s><U1>S1 : U1 . . .<Un>Sn : Un</s>, where <s> and </s> refer to the start and end tokens, <Ui> is a special token to indicate the start of the i-th utterance, and Si and Ui denote the speaker\u2019s name and the token sequence of the i-th utterance. The input sequence X is then fed to the BART encoder to obtain the hidden representation:\nX = Encoder(X), (2)\nwhere X \u2208 RL\u00d7d, L is the length of X , and d is the hidden dimension. The representation of the special token <Ui> in X is regarded as the utterance representation of Ui, i.e., ui. Based on this, we further incorporate CSK to obtain the knowledgeenhanced representation X \u2032 via a dual-view gate mechanism (Section 2.3) and a knowledge-aware graph attention layer (Section 2.4).\nDecoder. To map all the emotion-cause-category triplets to a position index sequence, we use six indexes (i.e., 1 to 6) to denote six emotion categories, and then use L indexes starting from 7 to denote each word in the input sequence X , in which the index of the special token <Ui> refers to the emotion or cause utterance. Formally, let us use Y = [eu1, cu1, ec1, . . . , eum, cum, ecm] to denote the output index sequence, where eu and cu represent the position indexes of the emotion and cause utterances, respectively, ec denotes the index of the emotion category, and m is the number of triplets. At the t-th time step, the decoder takes the knowledge-enhanced representation X \u2032 and the previous decoder outputs Y<t as inputs to predict the output probability distribution. Since Y<t is an index sequence, we first need to convert the indexes\ninto tokens as follows:\ny\u0302t = { Cyt , yt \u2264 |C|, Xyt\u2212|C|, yt > |C|,\n(3)\nwhere C is the token list of the emotion categories, |C| = 6.\nNext, we can obtain the last hidden state of the BART decoder and predict the probability distribution as follows:\nhdt = Decoder(X \u2032 ; Y\u0302<t), (4)\nX\u0304 = MLP(X \u2032 ), (5)\nH\u0304eutt = (E e utt + X\u0304utt)/2, (6)\nP (yt|X,Y<t) = Softmax([Ce; H\u0304eutt] \u00b7 hdt ), (7)\nwhere MLP refers to a multi-layer perceptron, Eeutt = TokenEmbed(Xutt) and X\u0304utt respectively denote the embeddings of <Ui> and the hidden representations of <Ui>, Ce = TokenEmbed(C) refers to the embeddings of six emotion categories; [\u00b7; \u00b7] is the concatenation operation, and \u201c\u00b7\u201d denotes the dot product; P (yt|X,Y<t) \u2208 R(|C|+L) is the final distribution on all indexes."
        },
        {
            "heading": "2.3 Dual-view Gate Mechanism",
            "text": "For each utterance, we separately obtain its retrieved and generated knowledge, and then integrate both of them through a gate mechanism.\nKnowledge Retrieval. We utilize ATOMIC20201 (Hwang et al., 2021), a widely-used commonsense knowledge graph covering social, physical, and eventive aspects of everyday inferential knowledge, as the external knowledge source. ATOMIC-2020 contains a large number of commonsense knowledge tuples composed of a head phrase, a relation type, and a tail phrase, e.g., (\u201cPersonX affords a car\u201d, xReact, \u201cproud\u201d). In this paper, we focus on two social-interaction relations that are highly correlated with emotion and cause: xReact and oReact, which denote how the subject and the object feel after the input event occurs. Specifically, we use SBERT (Reimers and Gurevych, 2019) to calculate the similarity between each utterance in the dataset and all head phrases in ATOMIC-2020, and obtain the tail phrases under the two relations of the top-3 similar head phrases. We then concatenate these tail phrases as our retrieved knowledge for each utterance.\nKnowledge Generation. A pre-trained neural knowledge model COMET-ATOMIC2020\n2 is used to 1https://allenai.org/data/atomic-2020 2https://github.com/allenai/comet-atomic-2020\ngenerate novel commonsense knowledge tuples. Taking each utterance and the selected relation type as inputs, the model would automatically generate several tail phrases (we set the beam size to 3) as the CSK for the utterance. For example, given the utterance \u201cUh, well... Joey and I broke up.\u201d and the relation type xReact, the following phrases {sad, regretful, upset} could be generated.\nKnowledge Encoding. We first convert the knowledge phrases for each utterance into a complete sentence, which incorporates speaker\u2019s name, e.g., \u201c[Charlie] feels [sad, different, upset].\u201d for xReact and \u201cOthers feel [annoyed, sad, confused].\u201d for oReact. Next, we concatenate the knowledge sentences of all utterances in the conversation and feed them to the BART encoder. The retrieved knowledge and generated knowledge under the two relation types are encoded separately in the same way as utterance encoding described in Section 2.2. Finally, for each utterance Ui, we obtain four knowledge representations: rxRi , r oR i , g xR i , g oR i .\nKnowledge Fusion. Since the CSK under xReact and oReact types would independently influence the representation of utterances from the same speaker and other speakers, we employ a dualview gate mechanism to perform knowledge fusion from two different views, thereby obtaining viewspecific knowledge representations. Specifically, for each relation type, we calculate the knowledge weight through a linear layer, and then use the weight to integrate retrieved knowledge and generated knowledge. The formula is shown as follows:\n\u03b1xRi = \u03c3(W[ui, r xR i , g xR i ]), \u03b1oRi = \u03c3(W[ui, r oR i , g oR i ]),\n(8)\nhxRi = \u03b1 xR i r xR i + (1\u2212 \u03b1xRi )gxRi , hoRi = \u03b1 oR i r oR i + (1\u2212 \u03b1oRi )goRi ,\n(9)\nwhere W \u2208 R3d is a trainable weight, \u03c3 denotes the sigmoid function."
        },
        {
            "heading": "2.4 Knowledge-aware Graph Attention Layer",
            "text": "To better model the conversation context and incorporate CSK effectively, we employ a graph attention layer to capture the dynamics and dependencies among speakers in the conversation.\nAs shown in Figure 2, we construct a graph G = (V, E) for each conversation, where V is the set of nodes and E is the set of edges between two nodes.\n\u2022 Nodes: Each utterance in the conversation is regarded as a node in the graph, and we initialize each node with the utterance representation ui obtained from the encoder.\n\u2022 Edges: The utterance nodes are linked in the temporal order of the conversation, and each edge is assigned an attention weight eij (i > j \u2265 1) representing the relationship between two utterances. We introduce CSK under two relation types to enrich the semantic dependencies among utterances. Specifically, given an utterance node, for each of its neighbor nodes, if their speakers are the same, the knowledge representation hxRi is used to calculate the edge weight between them; otherwise, hoRi is utilized.\nFor a target node ui, the edge weight from neighbor nodes uj can be computed as follows:\nu \u2032 j = W hhj , (10)\nh\u0304 xR j = u \u2032 j +W khxRj , (11) h\u0304 oR j = u \u2032 j +W khoRj , (12)\nexRij = LeakyReLU(a T 1 [W hui; h\u0304 xR j ]), (13)\neoRij = LeakyReLU(a T 2 [W hui; h\u0304 oR j ]), (14)\neij =\n{ exRij , Si = Sj ,\neoRij , Si \u0338= Sj , (15)\nwhere a1,a2 \u2208 R2d and Wh,Wk \u2208 Rd\u00d7d are learnable parameters.\nNext, we normalize the edge weights across all choices of j using the Softmax function. Finally, the knowledge-enhanced utterance representation hi is obtained by aggregating neighbor nodes according to the normalized edge weights:\n\u03b1ij = Softmaxj(eij), (16) hi = \u2211 j\u2208Ni \u03b1iju \u2032 j , (17)\nwhere Ni is the set of neighbor nodes of utterance node ui in the graph.\nTo facilitate our model to capture emotional dynamics, we add an auxiliary task of emotion recognition in conversations (ERC). As shown in Figure 2, an emotion classifier is applied to predict the emotion category of each utterance based on the knowledge-enhanced utterance representation.\nP (yemoi ) = Softmax(MLP(hi)), (18)\nwhere P (yemoi ) \u2208 R7 is the probability distribution over all emotion categories including Neutral."
        },
        {
            "heading": "2.5 Model Training",
            "text": "During the training phase, we use the teacher forcing strategy to train our model and the negative log-likelihood loss to optimize the model. The loss function is defined as follows:\nL = Lgen + Laux, (19)\nLgen = \u2212 1 M M\u2211 t=1 logP (y\u2217t |X,Y<t), (20)\nLaux = \u2212 1 N N\u2211 i=1 logP (yemo\u2217i ), (21)\nwhere M and N refer to the length of the output index sequence and the number of utterances in a conversation, respectively; y\u2217t and y emo\u2217 i denote the ground truth label of index and emotion, respectively. Moreover, during the inference, we use the beam search to obtain the target index sequence in an autoregressive manner."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Experimental Settings",
            "text": "Datasets. We conduct experiments on two benchmark datasets. ECF (Wang et al., 2022) is a multimodal conversational emotion cause dataset containing multi-party conversations from the sitcom Friends, which is closer to real-world scenarios. In this paper, we only consider the textual input. RECCON (Poria et al., 2021) includes dyadic conversations and is built for the task of emotion cause recognition in conversations. We use the subset RECCON-DD derived from DailyDialog (Li et al., 2017). Both datasets are divided into training, validation and test sets. The basic statistics of the two datasets are shown in Table 1.\nImplementation Details. We utilize the pretrained BART-base3 model to initial the parameters in the index generation framework. During training, we use the Adam optimizer with linear warm up and a weight decay of 1e-2 for parameter tuning. The batch size and initial learning rate are set to 16 and 2e-5, respectively. We use beam search to\n3https://huggingface.co/facebook/bart-base\ngenerate the index sequence and set the beam size to 4. The results on the test set come from the best checkpoint in the validation set. We repeat all the experiments five times with different seeds and report the average results. All experiments are conducted on an Nvidia RTX-3090 GPU.\nEvaluation Metrics. Similar to (Wang et al., 2022), we separately evaluate the emotion-cause pairs of each emotion category in the triplets with F1 score and further calculate a weighted average of F1 scores across different emotion categories. Considering the imbalance of emotion categories in the two datasets, we also report the weighted average F1 score of the four main emotion categories except Disgust and Fear."
        },
        {
            "heading": "3.2 Compared Methods",
            "text": "Since less work has been done on the ECPEC or ECTEC tasks, we compare our framework with several representative methods for ECPE in news articles: (1) ECPE-2steps (Xia and Ding, 2019) is the first pipeline framework proposed for ECPE, which individually extracts the emotion set and cause set, followed by emotion-cause pairing and filtering. Wang et al. (2022) has adapted it to the ECTEC task. (2) ECPE-2D (Ding et al., 2020a) is a joint end-to-end (E2E) framework using the crossroad 2D transformers to model the interactions of different emotion-cause pairs. (3) UECA-Prompt (Zheng et al., 2022) is a universal prompt method that decomposes ECPE into multiple objectives and converts them into sub-prompts. (4) BART, which only utilizes the index generation framework to generate the triplets, without two knowledgeenhanced modules and the auxiliary task in our SHARK model.\nIt should be noted that ECPE-2D and UECAPrompt are designed for ECPE and require modifications to extend them to the ECTEC task. Specifically, we adapt their emotion recognition module from binary classification to multi-class classification. Moreover, we have explored simply incorporating CSK into these methods, which is denoted as \u201c\u00b7 + CSK\u201d in the tables of experimental results. For ECPE-2steps and ECPE-2D, we feed the generated knowledge to their encoder to obtain the knowledge representation for each utterance, followed by concatenating the knowledge representation and the original utterance feature as the final utterance representation. For UECA-Prompt, we place the commonsense knowledge after each utterance, and the whole conversation is then fed into the model together. All the models are initialized with parameters from the pre-trained BERT-base4 or BART-base for a fair comparison."
        },
        {
            "heading": "3.3 Main Results",
            "text": "In Table 2, we report the results of different methods on the ECTEC task. To better compare these methods, we also report the F1 scores of two sub-\n4https://github.com/google-research/bert or https://huggingface.co/bert-base-cased\ntasks including emotion extraction and cause extraction in Table 3. Note that the weighted F1 for emotion extraction is evaluated based on the emotion categories in the predicted triplets, rather than the predictions from the auxiliary task.\nResults on ECTEC. First, it is clear that the E2E methods significantly outperform ECPE-2steps, which shows the superiority of the joint E2E framework in addressing the ECTEC task compared to the two-step pipeline framework. Second, BART achieves comparable performance to other baseline methods, indicating the feasibility of the index generation framework to solve ECTEC. Third, the introduction of CSK on the baseline methods does not lead to significant improvements, and ECPE-2D + CSK, UECA-Prompt + CSK even perform worse on the two datasets. This suggests that simply concatenating knowledge may bring much noise, and it is necessary to explore more effective approaches for knowledge fusion. Finally, we can clearly observe that our proposed SHARK obtains the best results and performs much better than BART, which demonstrates the effectiveness of our framework and reveals the benefits of incorporating CSK into the ECTEC task.\nResults on two subtasks. Similar to the conclusions drawn on ECTEC, incorporating knowledge into the baseline methods through simple concatenation may lead to a performance decline. However, SHARK greatly outperforms other methods on both subtasks, further validating the effectiveness of our framework and the necessity of knowledge fusion. Furthermore, given that the relation types of CSK we selected (xReact and oReact) are directly related to emotions, SHARK shows significant improvement over BART primarily on the emotion extraction subtask."
        },
        {
            "heading": "3.4 Ablation Study",
            "text": "To investigate the impact of individual modules on the overall performance, we conduct an ablation study of SHARK for the ECTEC task, and the\nexperimental results are shown in Table 4. We can see that removing different modules from SHARK leads to varying degrees of performance degradation. Specifically, discarding the auxiliary task has the largest impact on performance, indicating that ERC is helpful for triplet extraction. Moreover, removing the generated CSK has a greater negative effect compared to removing the retrieved CSK. This suggests the ability of the pre-trained knowledge model to generate relevant and informative commonsense knowledge, which is beneficial to emotion-cause understanding. Furthermore, the significant performance drop when both sources of external knowledge are disregarded highlights the importance of incorporating knowledge into the ECTEC task."
        },
        {
            "heading": "3.5 In-Depth Analysis",
            "text": "Cross-dataset Analysis. To compare the generalization of each method, we conduct cross-dataset testing between ECF and RECCON, and report the results in Table 5.\nAn obvious observation is that the performance of all the methods significantly deteriorates when tested on a different dataset. This indicates that training a model on one dataset does not ensure good adaptation to other datasets. However, in comparison with other methods, BART and SHARK show great advantages across different datasets, especially SHARK outperforms others by at least 7% on 4 avg. F1 under the ECF\u2192RECCON setting, which demonstrates the strong generalization abil-\nMethod Ang. Dis. Fear Joy Sad. Sur. 6 Avg. 4 Avg.\nity and high robustness of our framework. Moreover, we find that models trained on ECF can adapt to RECCON well, while models trained on RECCON perform poorly on ECF. We conjecture the reason is that the conversations in ECF come from TV series that are close to the real world, which involve informal text and complex scenes; while RECCON consists of human-generated conversations that are simpler and more formal, thus weakening the generalization of the model.\nThe Impact of Conversation Length. We conduct further analysis to explore the impact of different conversation lengths, i.e., separately evaluating the predictions for conversations with varying numbers of utterances from the test set of ECF. As shown in Table 6, we can observe that our generative models perform much better than encoderbased methods on conversations with more than 10 utterances, which account for about 42.65% in the ECF dataset. In these long conversations, encoderbased methods often fail to fully consider the contextual information and ignore a number of triplets, while our generative models tend to comprehensively capture the context, effectively capturing a broader range of triplets.\nComparison with ChatGPT. Considering the remarkable performance of large language models in various NLP tasks, we further apply ChatGPT to the ECTEC task under zero-shot and few-shot settings. Specifically, we randomly selected 50 conversations from the test set of ECF, and then fed each test conversation and a task-instruction prompt into ChatGPT to obtain the predicted emotion-cause triplets. Table 7 presents the 0-shot and 5-shot results of ChatGPT. It is obvious that SHARK performs significantly better than ChatGPT in both zero-shot and few-shot settings. We conjecture that\ndue to the complexity of extracting three elements (i.e., emotion, cause, and category) in the ECTEC task, the performance of ChatGPT is not satisfactory.\nSensitivity Analysis of Beam Size. In order to investigate the impact of beam size on our SHARK model, we conduct experiments with different beam sizes, and the results are shown in Figure 3. The F1 curves exhibit slightly different trends on the two datasets, but a beam size of 4 is the optimal choice for both datasets."
        },
        {
            "heading": "3.6 Case Study",
            "text": "To show the advantage of our SHARK model, we compare its predictions with the output of three baseline systems on a test sample. As shown in Figure 4, ECPE-2D and ECPE-2D + CSK only correctly identify two emotion-cause-category triplets, i.e., \u201c(U2,U1,surprise)\u201d and \u201c(U2,U2,surprise)\u201d. Moreover, our base model BART can further identify another triplet \u201c(U1,U1,surprise)\u201d, but still ignores \u201c(U4,U1,surprise)\u201d due to the long-term dependency between U4 and U1. In contrast, with the help of the CSK, SHARK correctly extracts all the four triplets. These observations show the effectiveness of the proposed generative model and CSK for the ECTEC task."
        },
        {
            "heading": "4 Related Work",
            "text": ""
        },
        {
            "heading": "4.1 Emotion Cause Analysis",
            "text": "Emotion Cause Analysis (ECA) has attracted increasing attention in recent years. It contains two representative subtasks: emotion cause extraction (ECE) and emotion-cause pair extraction (ECPE). ECE aims to extract the potential causes given the emotions (Lee et al., 2010a,b; Gui et al., 2016a,b, 2017; Fan et al., 2019); ECPE was proposed to jointly extract the emotions and the corresponding causes in pairs, thereby solving the problem of ECE\u2019s emotion annotation dependency (Xia and\nDing, 2019). Much research has been conducted on the ECPE task. Xia and Ding (2019) first proposed a two-step framework ECPE-2steps, which first extracts an individual emotion set and cause set, and then pairs the corresponding emotions and causes. Following their work, many end-to-end approaches have been proposed to address the limitations of the pipeline architecture. One line of work focuses on multi-task learning using a joint modeling framework (Ding et al., 2020a,b; Wei et al., 2020; Fan et al., 2020; Chen et al., 2022). Another line of work transforms ECPE into a unified sequence labeling problem and designs novel tagging schemes (Yuan et al., 2020; Chen et al., 2020; Cheng et al., 2021; Fan et al., 2021). More recently, several studies attempted to address the ECPE task with prompt-based methods (Zheng et al., 2022) and machine reading comprehension-based methods (Zhou et al., 2022; Cheng et al., 2023)."
        },
        {
            "heading": "4.2 Emotion Analysis in Conversations",
            "text": "Emotion recognition in conversations (ERC) is a hot-spot task in sentiment analysis, which aims to assign emotion labels to all the utterances in a conversation. Due to the increasing amount of public conversational data (Busso et al., 2008; Li et al., 2017; Poria et al., 2019; Firdaus et al., 2020), ERC has received continuous attention in the field of af-\nfective computing (Majumder et al., 2019; Ghosal et al., 2020; Zhu et al., 2021; Song et al., 2022).\nRecent years have witnessed a shift from ERC to ECAC. Poria et al. (2021) introduced an interesting task of recognizing emotion cause in conversations, aiming to find the causes behind the given emotions in the conversation, and constructed a new dataset RECCON. Several works for the CEE subtask have subsequently emerged (Li et al., 2022a; Zhang et al., 2022; Zhao et al., 2023; Gu et al., 2023). Furthermore, Li et al. (2022b) and Jeong and Bak (2023) attempted to extract emotion and causes in conversations simultaneously, and Wang et al. (2022) introduced the ECTEC task. However, the aforementioned encoder-only models for ECPEC mainly solve the task in a pipeline manner (independently predicting emotions and causes before matching, or first predicting emotions and then using the emotion predictions to infer causes), which suffer from error propagation. In contrast, our proposed generative framework enables endto-end triplet generation, which can extract all the emotion-cause-category triplets from a conversation in one shot."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we proposed a commonSense knowledge-enHanced generAtive fRameworK named SHARK for the Emotion Cause Triplet Extraction in Conversations (ECTEC) task. Specifically, we formulated the ECTEC task as an index generation problem and employed a BART-based model to generate all the emotion-cause-category triplets in one shot. Moreover, we designed a dualview gate mechanism and a graph attention layer to incorporate both the retrieved and generated commonsense knowledge. Experimental results on two benchmark datasets show the superiority of SHARK over a number of comparison systems and the usefulness of commonsense knowledge.\nLimitations\nAlthough the proposed SHARK model has obtained state-of-the-art performance on two benchmark datasets for the ECTEC task, our work still suffers from the following limitations.\nFirst, we only consider the commonsense knowledge under two relation types, i.e., xReact and oReact, and thus design a dual-view gate mechanism to better capture the intra-speaker and inter-speaker interactions. However, there are other causal rela-\ntions that may help cause inference, such as xWant and oWant. We plan to incorporate the other related causal relations into our generative framework in the future. Second, it might be interesting to explore the potential of incorporating commonsense knowledge for different modalities to boost the performance of the Multimodal ECTEC task.\nEthics Statement\nThis paper does not involve any data collection and release, and thus there are no privacy issues. All the experiments are conducted on two publicly available datasets, namely ECF (Wang et al., 2022) and RECCON (Poria et al., 2021), which do not include personal information or contain any objectionable content that could potentially harm individuals or communities."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors would like to thank the anonymous reviewers for their insightful comments. This work was supported by the Natural Science Foundation of China (62076133 and 62006117), and the Natural Science Foundation of Jiangsu Province for Young Scholars (BK20200463) and Distinguished Young Scholars (BK20200018)."
        }
    ],
    "title": "Generative Emotion Cause Triplet Extraction in Conversations with Commonsense Knowledge",
    "year": 2023
}