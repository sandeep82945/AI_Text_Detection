{
    "abstractText": "Entity-centric summarization is a form of controllable summarization that aims to generate a summary for a specific entity given a document. Concise summaries are valuable in various reallife applications, as they enable users to quickly grasp the main points of the document focusing on an entity of interest. This paper presents ENTSUMV2, a more abstractive version of the original entity-centric ENTSUM summarization dataset. In ENTSUMV2 the annotated summaries are intentionally made shorter to benefit more specific and useful entity-centric summaries for downstream users. We conduct extensive experiments on this dataset using multiple abstractive summarization approaches that employ supervised fine-tuning or large-scale instruction tuning. Additionally, we perform comprehensive human evaluation that incorporates metrics for measuring crucial facets. These metrics provide a more fine-grained interpretation of the current state-of-the-art systems and highlight areas for future improvement.",
    "authors": [
        {
            "affiliations": [],
            "name": "Dhruv Mehra"
        },
        {
            "affiliations": [],
            "name": "Lingjue Xie"
        },
        {
            "affiliations": [],
            "name": "Ella Hofmann-Coyle"
        },
        {
            "affiliations": [],
            "name": "Mayank Kulkarni"
        },
        {
            "affiliations": [],
            "name": "Daniel Preo\u0163iuc-Pietro"
        }
    ],
    "id": "SP:70d4aac15f156cfb2e8e52ef9c3d5477b5dc8ddb",
    "references": [
        {
            "authors": [
                "Ojas Ahuja",
                "Jiacheng Xu",
                "Akshay Gupta",
                "Kevin Horecka",
                "Greg Durrett."
            ],
            "title": "ASPECTNEWS: Aspect-oriented summarization of news documents",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Reinald Kim Amplayo",
                "Stefanos Angelidis",
                "Mirella Lapata."
            ],
            "title": "Aspect-controllable opinion summarization",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6578\u20136593, Online and Punta Cana, Dominican",
            "year": 2021
        },
        {
            "authors": [
                "Ziqiang Cao",
                "Furu Wei",
                "Wenjie Li",
                "Sujian Li."
            ],
            "title": "Faithful to the original: Fact aware neural abstractive summarization",
            "venue": "Thirty-second AAAI Conference on Artificial Intelligence, AAAI.",
            "year": 2018
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Zi-Yi Dou",
                "Pengfei Liu",
                "Hiroaki Hayashi",
                "Zhengbao Jiang",
                "Graham Neubig."
            ],
            "title": "GSum: A general framework for guided neural abstractive summarization",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Jesse Dunietz",
                "Daniel Gillick."
            ],
            "title": "A new entity salience task with millions of training examples",
            "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, volume 2: Short Papers, pages 205\u2013209,",
            "year": 2014
        },
        {
            "authors": [
                "Alexander Fabbri",
                "Irene Li",
                "Tianwei She",
                "Suyi Li",
                "Dragomir Radev."
            ],
            "title": "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Alexander R Fabbri",
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher",
                "Dragomir Radev."
            ],
            "title": "Summeval: Re-evaluating summarization evaluation",
            "venue": "Transactions of the Association for Computational Linguistics, 9:391\u2013409.",
            "year": 2021
        },
        {
            "authors": [
                "sander Wawer"
            ],
            "title": "SAMSum corpus: A human",
            "year": 2019
        },
        {
            "authors": [
                "Fei Liu"
            ],
            "title": "Analyzing sentence fusion in abstrac",
            "year": 2019
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2019
        },
        {
            "authors": [
                "Chin-Yew Lin",
                "Eduard Hovy."
            ],
            "title": "Automatic evaluation of summaries using n-gram co-occurrence statistics",
            "venue": "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguis-",
            "year": 2003
        },
        {
            "authors": [
                "Mounica Maddela",
                "Mayank Kulkarni",
                "Daniel Preotiuc-Pietro."
            ],
            "title": "EntSUM: A data set for entitycentric extractive summarization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Bowen Zhou",
                "Cicero dos Santos",
                "\u00c7a\u011flar Gul\u00e7ehre",
                "Bing Xiang."
            ],
            "title": "Abstractive text summarization using sequence-to-sequence RNNs and beyond",
            "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural Lan-",
            "year": 2016
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B. Cohen",
                "Mirella Lapata."
            ],
            "title": "Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
            "year": 2018
        },
        {
            "authors": [
                "Ani Nenkova",
                "Kathleen McKeown"
            ],
            "title": "Automatic summarization. Foundations and Trends\u00ae in Information Retrieval, 5(2\u20133):103\u2013233",
            "year": 2011
        },
        {
            "authors": [
                "talia Loukachevitch",
                "Evgeniy Kotelnikov",
                "Nuria Bel",
                "Salud Mar\u00eda"
            ],
            "title": "Jim\u00e9nez-Zafra, and G\u00fcl\u015fen Eryi\u011fit",
            "venue": "In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016),",
            "year": 2016
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Alexander M. Rush",
                "Sumit Chopra",
                "Jason Weston."
            ],
            "title": "A neural attention model for abstractive sentence summarization",
            "venue": "Proceedings of the 2015",
            "year": 2015
        },
        {
            "authors": [
                "Andrew Turpin",
                "Yohannes Tsegay",
                "David Hawking",
                "Hugh E Williams."
            ],
            "title": "Fast generation of result snippets in web search",
            "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 127\u2013134.",
            "year": 2007
        },
        {
            "authors": [
                "Ramakrishna Varadarajan",
                "Vagelis Hristidis."
            ],
            "title": "A system for query-specific document summarization",
            "venue": "Proceedings of the 15th ACM international conference on Information and knowledge management, pages 622\u2013631.",
            "year": 2006
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with BERT",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Controllable summarization is a rapidly expanding field of research that deals with creating summaries tailored to different elements (Fan et al., 2018; He et al., 2020; Hofmann-Coyle et al., 2022). The controllable elements include entities (Maddela et al., 2022), aspects (Amplayo et al., 2021; Ahuja et al., 2022), users\u2019 preferred style (Fan et al., 2018) and length (Kikuchi et al., 2016; Dou et al., 2021). Controllable summarization has the promise to increase the utility and usability of summarization systems by enabling users to obtain summaries that align with their specific needs and preferences (Maddela et al., 2022). Further, controllable summaries can be used in downstream applications like search (Varadarajan and Hristidis, 2006; Turpin et al., 2007), entity salience (Gamon et al., 2013; Dunietz\n\u2217The authors contributed equally \u2020 Work done while at Bloomberg\nand Gillick, 2014), aspect-based sentiment classification (Pontiki et al., 2016) or question answering.\nAbstractive summarization methods aim to produce new summaries (Nenkova et al., 2011), which can be obtained through selection, compression and reformulation of the given source document. Compared to extractive summarization, abstractive summarization can produce concise summaries that capture the essence of the source text using fewer words, making them more efficient for users to consume. However, abstractive summarization is prone to suffer from issues in consistency with the source document (or factual errors), coherence or fluency (Cao et al., 2018; Kryscinski et al., 2019; Lebanoff et al., 2019). To evaluate abstractive summaries, automatic metrics have been proposed, although their correlation with human evaluation on the desirable facts for a summary are not always high or consistent (Fabbri et al., 2021).\nIn this paper, we focus on the task of abstractive entity-centric summarization. Past research on this topic was limited by the ability to comprehensively evaluate models, relying either on singlefaceted human quality judgments (Fan et al., 2018; He et al., 2020; Goyal et al., 2023) or reference entity-centric summaries which were very extractive (Maddela et al., 2022). To this end, we release an updated version of the ENTSUM dataset (Maddela et al., 2022), named ENTSUMV2, where summaries are deliberately made shorter and more abstractive. Moreover, we enhance the evaluation process of entity-centric summarization methods by incorporating a comprehensive multi-faceted human evaluation, specifically designed for this task. This human evaluation complements the standard automatic metrics, including ROUGE and BERTScore. By incorporating both automatic metrics and human evaluation, we aim to provide a thorough and robust evaluation of summarization model performance and show the path forward to improving models for this task.\nSeparately, we explore training several model architectures on this task and propose several improvements to the training process, which substantially outperform the existing state-of-the-art (+2.5 BERTScore, +4.4 Rouge-L), instructiontuned models and the strong entity-centric Lead3 heuristic."
        },
        {
            "heading": "2 Data",
            "text": "In this paper, we introduce the ENTSUMV2 dataset which contains more compressed abstractive summaries when compared to the original ENTSUM dataset. We build the ENTSUM dataset on top of The New York Times\u2019 summarization corpus (hereafter referred to as NYT) which is available to use via the LDC.1 The dataset shares the same set of documents as ENTSUM, but with a stricter length constraint of up to 60 words, half of ENTSUM\u2019s. The annotations are performed by annotators trained over multiple rounds on a proprietary annotation platform. The annotators are presented with the original document, the target entity and the salient sentences for the target entity as annotated in the original ENTSUM dataset. A diagram with the annotation process is presented in Appendix A. For quality control, we additionally calculated the inter-annotator agreement for the EntSUMv2 dataset in the final training round using ROUGE-[1,2,L] and BERTScore between the abstractive summary and the proxy summary (entity salient sentences) provided to annotators at annotation time. The EntSUMv2 Krippendorff\u2019s alpha for ROUGE-[1,2,L] and BERTscore are 0.75, 0.84, 0.85 and 0.81 respectively, indicating a high overlap. We collect a single summary for each document and entity pair.\nTable 1 displays summary statistics for the newly introduced ENTSUMV2 dataset in comparison to ENTSUM and other public datasets for summarization. There is a notable increase in the occurrence of novel n-grams compared to ENTSUM, albeit still less than other datasets like NYT or CNN/Daily Mail (Nallapati et al., 2016). Moreover, the average summary length in ENTSUMV2 is significantly shorter, with an average of 46 words compared to the 81 words in ENTSUM. This stricter length constraint presents a challenge for the model to effectively select the most essential information within the summarized output.\n1https://catalog.ldc.upenn.edu/LDC2008T19"
        },
        {
            "heading": "3 Methods",
            "text": "We experiment with several methods for abstractive summarization as follows:"
        },
        {
            "heading": "3.1 Heuristics",
            "text": "Lead3ovr is a generic summarization approach that disregards the target entity and simply selects the first three sentences from the document. Lead3ent selects the first three sentences in the document specifically mentioning the given entity following entity detection and coreference resolution, as described in ENTSUM (Maddela et al., 2022)."
        },
        {
            "heading": "3.2 GSum",
            "text": "We start with GSument\u2212sent, an entity-centric summarization version of GSum (Dou et al., 2021) which obtained the best performance on abstractive summarization on ENTSUM (Maddela et al., 2022). GSum is a summarization framework that incorporates two encoders: one for the source document and another for the guidance signal. Our GSum setup closely follows the setup outlined in ENTSUM (Maddela et al., 2022), where the model weights are initialized with BART (Lewis et al., 2019) with a few modifications which we find lead to improved performance. First, we incorporate a dropout layer (p=0.5) into the guidance signal encoder stack of the model architecture. Second, we experiment with a two step training process (two\u2013step) motivated by an analysis on the GSum results which showed the entity-centric model\u2019s ability to select key information from the source document could be improved. In Step 1, we train the GSum model to generate generic document summaries by providing only the source document and an empty guidance signal input. In Step 2, we load the best generic GSum summarization checkpoint and fine-tune with the entity guidance signal and proxy entity-centric summaries, as described in (Maddela et al., 2022), to produce entity-centric summaries."
        },
        {
            "heading": "3.3 T5",
            "text": "T5 (Raffel et al., 2020) is a transformer-based encoder-decoder model that is pretrained using text with dropped token sequences as input and the dropped out tokens delimited by their sentinel tokens as output. We fine-tune two base versions of the T5 model for entity-centric summarization. The first is T5-base, trained with a combination\nof supervised tasks including summarization. The second is T5-v1.1-base, pretrained solely on the unsupervised objective, allowing us to assess its performance independently of mixed task fine-tuning or other summarization data. In addition, we investigate two training setups: we experiment with finetuning the model with proxy entity-centric summarization only (proxy), or train it in two steps (two\u2013step), wherein we initially train the model to generate generic summaries and subsequently fine-tune it for proxy entity-centric summarization. The second approach aims to provide the models with additional contextual understanding through the first step of training."
        },
        {
            "heading": "3.4 Flan-T5",
            "text": "Large-scale instruction tuning using diverse NLP tasks has emerged as an alternative to single-task fine-tuning. We examine the efficacy of Flan-T5 (Chung et al., 2022), an enhanced version of the T5 model, which has undergone instruction-tuning using a wide range of tasks and instructions, including several summarization datasets such as CNN/Daily Mail (Nallapati et al., 2016), Gigaword (Rush et al., 2015), MultiNews (Fabbri et al., 2019), SamSum (Gliwa et al., 2019) and XSum (Narayan et al., 2018). To facilitate zero-shot entity-centric summarization, we employ prompt engineering techniques to guide the model in generating entity-centric summaries. We develop entity-centric summarization prompt templates, inspired by the Flan Collection templates2 and explore two input strategies, as the model was not originally trained for the entitycentric summarization task. In the first, we provide the complete source document as input, and in the second only sentences containing the entity and its coreference are provided. In Appendix C, we present the performance evaluation of the Flan-T5 model across different prompts.\n2https://github.com/google-research/FLAN"
        },
        {
            "heading": "4 Experimental Setup",
            "text": "Training Data We employ proxy summaries from the NYT corpus during the fine-tuning process of GSum and T5 models for entity-centric summarization. The original corpus comprised 44,382 training and 5,523 validation pairs (document, summary) for generic summary. To generate entitycentric summaries, we select the first three sentences that mention the target entity. This selection is based on the entity recognition and coreference resolution methods as described in (Maddela et al., 2022). Given that each document in the corpus contained multiple entities, the training set expanded to 464,339 pairs, while the validation set grows to 58,991 pairs. Test Data We use the ENTSUMV2 dataset for evaluation only, following (Maddela et al., 2022). We conduct experiments by splitting this dataset into training and test sets. However, training on this dataset, even when combined with the additional proxy summaries, does not result in any performance improvements. Therefore, to ensure more accurate and dependable evaluations of model performance, we utilize the entire ENTSUMV2 dataset exclusively for testing purposes. Implementation Details The T5-base 3, T5-v1.1 4, and Flan-T5-base 5 models are obtained from the HuggingFace model repository. We use the GSum implementation provided by the authors.6 In our GSum experiments, we adhere to the hyperparameters and implementation details outlined in the GSum framework. We conduct fine-tuning of the T5 and T5 v1.1 models for 2 epochs with a learning rate of 2e-5. The batch size is set to 32, and the experiments are performed on Nvidia Tesla V100 GPUs. During inference, we impose a constraint\n3https://huggingface.co/t5-base 4https://huggingface.co/docs/transformers/\nmodel_doc/t5v1.1 5https://huggingface.co/google/flan-t5-base 6https://github.com/neulab/guided_ summarization\non the T5 models to limit the generated output to a maximum of 60 tokens."
        },
        {
            "heading": "5 Results",
            "text": "We evaluate all models using both automatic and human evaluation for a more comprehensive view on model performance."
        },
        {
            "heading": "5.1 Automatic Evaluation",
            "text": "The results of automatic evaluation are reported in Table 2. We employ the same set of automated metrics used in ENTSUM, namely ROUGE-1, ROUGE-2, ROUGE-L (Lin and Hovy, 2003) and BERTScore (Zhang et al., 2020). The results show: \u2022 GSum and T5 based methods perform similarly\nin their best configurations, with GSum slightly better on BERTScore. \u2022 The best performing summarization model outperforms the strong Lead3 entity-centric baseline on R-1 (+2.2), R-2 (+3), R-L (+2.2) and BERTScore (+0.9). \u2022 Two step training on generic, then entity-centric summaries is beneficial, improving results on GSum. Since GSum takes in 2 inputs (source document and guidance signal) as opposed T5 which only takes a single input, we suspect that the two step training process acts like a curriculum based learning approach which helps the model learn more effectively. The GSum model first learns to summarize the overall key information from the provided source document. Then, it uses the additional provided signal to summarize the information relevant to the provided entity. \u2022 Instruction-tuned models obtain decent results but only as part of a pipeline that selects the entity related sentences a priori. Otherwise, their performance is similar or lower to the Lead3 generic summary heuristic, showing they can not perform the entity control aspect. \u2022 Both T5 and T5-v1.1 achieve comparable performance after being fine-tuned on proxy entitycentric summarization, despite T5-base being initially fine-tuned with multiple supervised tasks, including summarization. This shows that further training on out-of-domain summaries provides diminishing gains. \u2022 We also compare the summarization models to oracle extractive summarization models that rely on identifying the Lead3 salient sentences (Lead3ent Salient) and Lead3 sentences used to write the summary (Lead3ent Sum-\nmary) (Hofmann-Coyle et al., 2022). Despite evaluating on abstractive summaries, there remains a gap compared to these oracle extractive methods, highlighting that abstractive methods still need to be further enhanced to identify key entity information."
        },
        {
            "heading": "5.2 Faceted Human Evaluation",
            "text": "We conduct human evaluation of three top-performing methods of each type (GSumtwo\u2212step+dropout, T5-v1.1-basetwo\u2212step, Flan-T5-basep2\u2212entity) for a more comprehensive assessment. T5-v1.1-base is selected for a fair comparison with GSum as T5-base is trained with additional summarization datasets. The T5v1.1 output is restricted to the first 60 tokens for a fair evaluation, as it tends to produce longer summaries. Three independent raters evaluated all 480 summaries each. Summaries are ranked on a Likert scale of 1 to 5, with a focus on crucial aspects: entity-specificity (or relevance), factuality (or consistency), and fluency aligning with previous work on human evaluation for summarization (Kryscinski et al., 2019; Fabbri et al., 2021). We also include completeness, specifically for measuring the controllability aspect and an overall quality score. The evaluation guidelines are provided in Appendix E. The trained annotators achieve a Krippendorf Alpha (Krippendorff, 2011) of 0.48 with the authors on a random subset of 100 annotations. The inter-annotator agreement between annotators on the five aspects is 0.73. The agreement numbers are in line to past research (Fabbri et al., 2021). The facet based scores indicate that:\n\u2022 GSum and T5 demonstrate divergent facet-level performance, notably on overall quality, despite similar overall ROUGE and BERTScore results. The different architectures of these models lead to distinct summary patterns, with GSum excelling in factuality and completeness. \u2022 Despite the 10-point R-1 score difference between T5 and Flan-T5, the performance gap narrows in human evaluation. Flan-T5 is trained on a larger corpus and diverse tasks, which aid in sentence fluency but inhibit its performance in other areas due to the generic nature of its pretrained tasks. Additionally, both T5 and Flan-T5 struggle more with factuality, generating inaccurate or fictional information. \u2022 All models are able to obtain controllability, al-\nthough Flan-T5 lags behind the other models, even if fed with sentences that contain the entity."
        },
        {
            "heading": "6 Conclusions",
            "text": "This paper presents a comprehensive analysis of abstractive entity-centric summarization. We introduce a new dataset - ENTSUMV2 with summaries that are more abstractive and almost half the length of the summaries in ENTSUM, posing additional challenges to summarization models. We explore different model types, improving upon previous top-performing models through data insights and training techniques, as well as surpassing the strong Lead3 entity-centric baseline. Finally, we conduct the first multi-faceted human evaluation on entitycentric summarization, revealing detailed insights into model behavior and trade-offs, suggesting potential avenues for further enhancement.\nLimitations\nWe only study the task of entity-centric summarization in English, as this is a relatively new task and there are no other datasets to build on with relevant and salient entity sentences selected, which we use\nas base for writing our summaries. Thus, the paper does not test the generalizability of our models and findings to other languages.\nWe train the model for a predetermined number of epochs without task specific validation as a validation dataset for entity-centric summarization is not available and we only use the entire ENTSUMV2 dataset for evaluation.\nWe limit our experimentation to the T5-base model due to its comparable number of parameters with the GSum model and due to limited compute resources. However, exploring the training of larger T5 models can provide valuable insights into the impact of model size on task performance.\nWe only use arguably the most popular metrics for automatic summarization (ROUGE, BERTScore). Using more metrics could provide a more complete picture of model performance."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank Rajarshi Bhowmik and the many members of Bloomberg AI group who provided invaluable feedback on this paper. We are grateful to our annotators for their diligence in per-\nforming this annotation task and human evaluation of the summaries."
        },
        {
            "heading": "A Dataset Annotation Process",
            "text": "In Figure 1, we illustrate an example showcasing the multiple stages of annotation implemented in the ENTSUM dataset. The example encompasses four distinct stages of annotation. In this paper, our experiments focus on two particular stages: salient sentences and entity-centric summary. We use the entity-centric summary to evaluate the model performance. During the evaluation process, we compare the models\u2019 performance when provided with either the entire article or only the salient sentences as input."
        },
        {
            "heading": "B Qualitative Comparison of EntSUM and EntSUMv2",
            "text": "In Table 4, we illustrate the qualitative difference between the ENTSUM and ENTSUMV2 datasets. In ENTSUMV2 the abstractive entity-centric summaries are constrained to 60 tokens, resulting more abstractive and specific summaries. Entitycentric summaries in ENTSUMV2 are on average 33% shorter than the cooresponding summary in ENTSUM."
        },
        {
            "heading": "C Prompt comparison for Flan-T5",
            "text": "Table 5 compares the performance of the FlanT5 model when selecting different prompts for entity-centric summarization. The evaluation of the prompts is conducted on the NYT validation dataset using proxy entity-centric summaries. Prompt 1 and Prompt 2 adopt the summarization prompts employed in Flan-T5 instruction-tuning,\naccompanied by additional entity-related information. Prompt 3 introduces an explicit word constraint. Prompt 4 adopts a question-answering style prompt, utilizing the 5W1H framework. The results show that the design of the prompts has a significant impact on the performance of the model. Prompts that closely resemble the task-specific prompts used during model training yield more accurate and relevant summaries. Prompt 2 is the selected prompt for the following evaluations."
        },
        {
            "heading": "D Extended Human Evaluation Results",
            "text": "The results of the authors human evaluation results can be found in Table 6. The histograms of the trained annotators and author Likert scores for each facet are included in Figure 2."
        },
        {
            "heading": "E Human Evaluation Guidelines",
            "text": "Entity-Specificity: for this metric we are determining to what extent the content pertains to the entity and is salient (relevant) in a summary about the entity. Please note the following: \u2022 Please do not penalize the score for the entity\nname not being mentioned so long as the content\nstill pertains to the entity. \u2022 If all of the content pertains to the entity, but is\nnot factually correct according to the source text, please score this metric 4 (All content is about the entity but the sentences may not be salient)\nScale anchors: 1. None of the content is about the entity 2. Most of the content is not about the entity 3. Some but not all of the content is about the entity 4. All content is about the entity but the sentences\nmay not be salient 5. All content is about the entity and is salient Fluency: this metric measures whether the summary is grammatically correct and easy to understand. Please do not penalize the score if the summary about the entity is incomplete (i.e., should include more details from the source text). The completeness metric measures this instead.\nScale anchors: 1. The summary is incomprehensible 2. Disfluent 3. Understandable 4. Good 5. Flawless\nFactuality: this metric measures whether the summary is true to the source text. Please penalize the score if the summary introduces new facts that were not present in the source text.\nScale anchors: 1. Very untrue of the source text 2. Mostly untrue of the source text 3. Somewhat true of the source text 4. Mostly true of the source text 5. Very true of the source text Completeness: this metric measures whether the summary includes a comprehensive overview of the source text that pertains to the entity.\nScale anchors: 1. Does not capture entity-specific or overall im-\nportant information 2. Captures overall important information, but does\nnot capture entity-specific information 3. Captures some entity-specific information 4. Mostly captures the entity-specific information 5. Completely captures the entity-specific informa-\ntion\nOverall Quality: this measures, from a reader\u2019s point of view, whether a reader would be able to gain an overview of the essential information from the original source text that pertains to the entity if they did not have access to the original source and the entity name.\nScale anchors: 1. Poor 2. Fair 3. Good 4. Very good 5. Excellent"
        }
    ],
    "title": "ENTSUMV2: Data, Models and Evaluation for More Abstractive Entity-Centric Summarization",
    "year": 2023
}