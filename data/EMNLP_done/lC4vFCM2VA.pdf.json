{
    "abstractText": "The rapid growth of web pages and the increasing complexity of their structure poses a challenge for web mining models. Web mining models are required to understand semistructured web pages, particularly when little is known about the subject or template of a new page. Current methods migrate language models to web mining by embedding the XML source code into the transformer or encoding the rendered layout with graph neural networks. However, these approaches do not take into account the relationships between text nodes within and across pages. In this paper, we propose a new approach, ReXMiner, for zero-shot relation extraction in web mining. ReXMiner encodes the shortest relative paths in the Document Object Model (DOM) tree of the web page which is a more accurate and efficient signal for key-value pair extraction within a web page. It also incorporates the popularity of each text node by counting the occurrence of the same text node across different web pages. We use contrastive learning to address the issue of sparsity in relation extraction. Extensive experiments on public benchmarks show that our method, ReXMiner, outperforms the state-ofthe-art baselines in the task of zero-shot relation extraction in web mining.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zilong Wang"
        },
        {
            "affiliations": [],
            "name": "Jingbo Shang"
        }
    ],
    "id": "SP:f0be533d2313f31741b2d749106972fb97c95a09",
    "references": [
        {
            "authors": [
                "Mirko Bronzi",
                "Valter Crescenzi",
                "Paolo Merialdo",
                "Paolo Papotti."
            ],
            "title": "Extraction and integration of partially overlapping web sources",
            "venue": "Proceedings of the VLDB Endowment, 6(10):805\u2013816.",
            "year": 2013
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Jiaoyan Chen",
                "Yuxia Geng",
                "Zhuo Chen",
                "Ian Horrocks",
                "Jeff Z Pan",
                "Huajun Chen."
            ],
            "title": "Knowledgeaware zero-shot learning: Survey and perspective",
            "venue": "arXiv preprint arXiv:2103.00070.",
            "year": 2021
        },
        {
            "authors": [
                "Lei Cui",
                "Furu Wei",
                "Ming Zhou."
            ],
            "title": "Neural open information extraction",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 407\u2013 413.",
            "year": 2018
        },
        {
            "authors": [
                "Oren Etzioni",
                "Michele Banko",
                "Stephen Soderland",
                "Daniel S Weld."
            ],
            "title": "Open information extraction from the web",
            "venue": "Communications of the ACM, 51(12):68\u201374.",
            "year": 2008
        },
        {
            "authors": [
                "Anthony Fader",
                "Stephen Soderland",
                "Oren Etzioni."
            ],
            "title": "Identifying relations for open information extraction",
            "venue": "Proceedings of the 2011 conference on empirical methods in natural language processing, pages 1535\u20131545.",
            "year": 2011
        },
        {
            "authors": [
                "Xavier Glorot",
                "Yoshua Bengio."
            ],
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "venue": "Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256. JMLR Workshop and Conference",
            "year": 2010
        },
        {
            "authors": [
                "Jiuxiang Gu",
                "Jason Kuen",
                "Vlad I Morariu",
                "Handong Zhao",
                "Rajiv Jain",
                "Nikolaos Barmpalios",
                "Ani Nenkova",
                "Tong Sun."
            ],
            "title": "Unidoc: Unified pretraining framework for document understanding",
            "venue": "Advances in Neural Information Processing Systems, 34:39\u201350.",
            "year": 2021
        },
        {
            "authors": [
                "Qiang Hao",
                "Rui Cai",
                "Yanwei Pang",
                "Lei Zhang."
            ],
            "title": "From one tree to a forest: a unified solution for structured web data extraction",
            "venue": "Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 775\u2013",
            "year": 2011
        },
        {
            "authors": [
                "William Hogan",
                "Jiacheng Li",
                "Jingbo Shang."
            ],
            "title": "Fine-grained contrastive learning for relation extraction",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1083\u20131095.",
            "year": 2022
        },
        {
            "authors": [
                "Yupan Huang",
                "Tengchao Lv",
                "Lei Cui",
                "Yutong Lu",
                "Furu Wei."
            ],
            "title": "Layoutlmv3: Pre-training for document ai with unified text and image masking",
            "venue": "Proceedings of the 30th ACM International Conference on Multimedia, pages 4083\u20134091.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin Ming-Wei Chang Kenton",
                "Lee Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of NAACL-HLT, pages 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Dongyang Li",
                "Taolin Zhang",
                "Nan Hu",
                "Chengyu Wang",
                "Xiaofeng He."
            ],
            "title": "Hiclre: A hierarchical contrastive learning framework for distantly supervised relation extraction",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 2567\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Junlong Li",
                "Yiheng Xu",
                "Lei Cui",
                "Furu Wei."
            ],
            "title": "Markuplm: Pre-training of text and markup language for visually rich document understanding",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2022
        },
        {
            "authors": [
                "Zimeng Li",
                "Bo Shao",
                "Linjun Shou",
                "Ming Gong",
                "Gen Li",
                "Daxin Jiang."
            ],
            "title": "Wiert: web information extraction via render tree",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 13166\u201313173.",
            "year": 2023
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Ying Sheng",
                "Nguyen Vo",
                "Sandeep Tata."
            ],
            "title": "Freedom: A transferable neural architecture for structured information extraction on web documents",
            "venue": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery",
            "year": 2020
        },
        {
            "authors": [
                "Colin Lockard",
                "Xin Luna Dong",
                "Arash Einolghozati",
                "Prashant Shiralkar."
            ],
            "title": "Ceres: Distantly supervised relation extraction from the semi-structured web",
            "venue": "Proceedings of the VLDB Endowment, 11(10).",
            "year": 2018
        },
        {
            "authors": [
                "Colin Lockard",
                "Prashant Shiralkar",
                "Xin Luna Dong."
            ],
            "title": "Openceres: When open information extraction meets the semi-structured web",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2019
        },
        {
            "authors": [
                "Colin Lockard",
                "Prashant Shiralkar",
                "Xin Luna Dong",
                "Hannaneh Hajishirzi."
            ],
            "title": "Zeroshotceres: Zeroshot relation extraction from semi-structured webpages",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Mausam Mausam."
            ],
            "title": "Open information extraction systems and downstream applications",
            "venue": "Proceedings of the twenty-fifth international joint conference on artificial intelligence, pages 4074\u20134077.",
            "year": 2016
        },
        {
            "authors": [
                "Dat Quoc Nguyen",
                "Karin Verspoor."
            ],
            "title": "Endto-end neural relation extraction using deep biaffine attention",
            "venue": "European conference on information retrieval, pages 729\u2013738. Springer.",
            "year": 2019
        },
        {
            "authors": [
                "Han Peng",
                "Ge Li",
                "Wenhan Wang",
                "Yunfei Zhao",
                "Zhi Jin."
            ],
            "title": "Integrating tree path in transformer for code representation",
            "venue": "Advances in Neural Information Processing Systems, 34:9343\u20139354.",
            "year": 2021
        },
        {
            "authors": [
                "Han Peng",
                "Ge Li",
                "Yunfei Zhao",
                "Zhi Jin."
            ],
            "title": "Rethinking positional encoding in tree transformer for code representation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3204\u20133214.",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Peng Su",
                "Yifan Peng",
                "K Vijay-Shanker."
            ],
            "title": "Improving bert model using contrastive learning for biomedical relation extraction",
            "venue": "Proceedings of the 20th Workshop on Biomedical Language Processing, pages 1\u201310.",
            "year": 2021
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Qifan Wang",
                "Yi Fang",
                "Anirudh Ravula",
                "Fuli Feng",
                "Xiaojun Quan",
                "Dongfang Liu."
            ],
            "title": "Webformer: The web-page transformer for structure information extraction",
            "venue": "Proceedings of the ACM Web Conference 2022, pages 3124\u20133133.",
            "year": 2022
        },
        {
            "authors": [
                "Zilong Wang",
                "Jiuxiang Gu",
                "Chris Tensmeyer",
                "Nikolaos Barmpalios",
                "Ani Nenkova",
                "Tong Sun",
                "Jingbo Shang",
                "Vlad Morariu."
            ],
            "title": "Mgdoc: Pre-training with multi-granular hierarchy for document image understanding",
            "venue": "Proceedings of the 2022 Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-ofthe-art natural language processing",
            "year": 2019
        },
        {
            "authors": [
                "Chenhao Xie",
                "Wenhao Huang",
                "Jiaqing Liang",
                "Chengsong Huang",
                "Yanghua Xiao."
            ],
            "title": "Webke: Knowledge extraction from semi-structured web with pre-trained markup language model",
            "venue": "Proceedings of the 30th ACM International Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Yang Xu",
                "Yiheng Xu",
                "Tengchao Lv",
                "Lei Cui",
                "Furu Wei",
                "Guoxin Wang",
                "Yijuan Lu",
                "Dinei Florencio",
                "Cha Zhang",
                "Wanxiang Che"
            ],
            "title": "Layoutlmv2: Multi-modal pre-training for visually-rich document understanding",
            "venue": "In Proceedings of the 59th Annual",
            "year": 2021
        },
        {
            "authors": [
                "Yiheng Xu",
                "Minghao Li",
                "Lei Cui",
                "Shaohan Huang",
                "Furu Wei",
                "Ming Zhou."
            ],
            "title": "Layoutlm: Pre-training of text and layout for document image understanding",
            "venue": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data",
            "year": 2020
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Yichao Zhou",
                "Ying Sheng",
                "Nguyen Vo",
                "Nick Edmonds",
                "Sandeep Tata."
            ],
            "title": "Simplified dom trees for transferable attribute extraction from the web",
            "venue": "arXiv preprint arXiv:2101.02415.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "The rapid growth of web pages and the increasing complexity of their structure poses a challenge for web mining models. Web mining models are required to understand semistructured web pages, particularly when little is known about the subject or template of a new page. Current methods migrate language models to web mining by embedding the XML source code into the transformer or encoding the rendered layout with graph neural networks. However, these approaches do not take into account the relationships between text nodes within and across pages. In this paper, we propose a new approach, ReXMiner, for zero-shot relation extraction in web mining. ReXMiner encodes the shortest relative paths in the Document Object Model (DOM) tree of the web page which is a more accurate and efficient signal for key-value pair extraction within a web page. It also incorporates the popularity of each text node by counting the occurrence of the same text node across different web pages. We use contrastive learning to address the issue of sparsity in relation extraction. Extensive experiments on public benchmarks show that our method, ReXMiner, outperforms the state-ofthe-art baselines in the task of zero-shot relation extraction in web mining."
        },
        {
            "heading": "1 Introduction",
            "text": "The internet is a vast repository of semi-structured web pages that are characterized by the use of HTML/XML markup language. Compared to plain text in traditional natural language understanding tasks, these web pages possess additional multimodal features such as the semi-structured visual and layout elements from the HTML/XML source code. These features can be effectively generalized across different websites and provide a richer understanding of the web pages (Lockard et al., 2018, 2019, 2020).\n\u2217 Jingbo Shang is the corresponding author.\nThe dynamic nature of the modern internet poses significant challenges for web mining models due to its rapid pace of updates. It is infeasible to annotate emerging web pages and train targeted models for them. Modern web mining models are expected to perform zero-shot information extraction tasks with little prior knowledge of emerging subjects or templates (Lockard et al., 2020; Chen et al., 2021). In this context, the multimodal features extracted from the HTML/XML source code as well as the textual contents are crucial for dealing with zeroshot information extraction task on the countless emerging web pages.\nPrevious approaches to the problem of zero-shot web mining have primarily focused on creating rich representations through large-scale multimodal pretraining, utilizing XML Paths of text nodes1 (Lin et al., 2020; Zhou et al., 2021; Li et al., 2022b). As shown in Figure 1, XML Paths are sequences of tags (e.g., div, span, li) indicating the location of\n1https://en.wikipedia.org/wiki/XPath\nthe text node in the DOM Tree2 of the page. These pre-training approaches extend vanilla language models by embedding the absolute XML Paths but fail to take into account the relative local relationship expressed by the relative XML Paths. The related nodes tend to be close to each other in the DOM tree, which results in a long common prefix in their XML Paths, as shown in Figure 1. Such local relation is more common than the absolute XML Path patterns. Therefore, it is easy to transfer the relative XML Paths to new web pages, and the relative XML Paths serve as a more efficient and meaningful signal in predicting the relation between text nodes.\nAdditionally, existing web mining approaches tend to treat each web page separately and focus on memorizing their various templates, ignoring the fact that the relevance across different web pages of the same website is also meaningful to identify the related text nodes (Zhou et al., 2021; Li et al., 2022b; Lockard et al., 2020). Intuitively, a text node is more likely to be a key word if it appears frequently in a collection of web pages and its surrounding words are not fixed. For example, in web pages about NBA players, the statistics about the height, age are common text fields in the player introduction, so the text nodes, such as \u201cHeight:\u201d and \u201cAge:\u201d should appear more frequently than other text nodes and the surrounding text contents should be different.\nIn light of the aforementioned challenges in web mining, we propose a web mining model with Relative XML Path, ReXMiner, for tackling the zero-shot relation extraction task from semistructured web pages. Our approach aims to learn the local relationship within each web page by exploiting the potential of the DOM Tree. Specifically, we extract the shortest path between text nodes in the DOM Tree as the relative XML Path, which removes the common prefix in the XML Paths. Inspired by the relative position embedding in T5 (Raffel et al., 2020), we then embed the relative XML Paths as attention bias terms in the multi-layered Transformer. Additionally, we incorporate the popularity of each text node by counting the number of times it occurs across different web pages, and embed the occurrence logarithmically in the embedding layer. Furthermore, we address the data sparsity issues in the relation extraction task by\n2https://en.wikipedia.org/wiki/Document_ Object_Model\nadopting contrastive learning during training which is widely used in related works (Su et al., 2021; Hogan et al., 2022; Li et al., 2022a). We randomly generate negative cases and restrict their ratio to the positive ones, allowing the model to properly discriminate related node pairs from others.\nBy learning from the relationships between text nodes within and across pages, ReXMiner is able to effectively transfer knowledge learned from existing web pages to new ones. We validate our approach on web pages from three different verticals from the SWDE dataset (Hao et al., 2011), including Movie, University, and NBA. The relation labels are annotated by Lockard et al. (2019). We summarize our contribution as follows. \u2022 We propose a novel multimodal framework,\nReXMiner, that effectively exploit the relative local relationship within each web page and incorporate the popularity of text nodes across different web pages in the relation extraction task. \u2022 We represent the relative local relation and the popularity of text nodes in the language models through relative XML Paths in the DOM Tree and the occurrence number of text nodes across different web pages. \u2022 Extensive experiments on three different verticals from SWDE dataset demonstrate the effectiveness of ReXMiner in the zero-shot relation extraction task in web mining. Reproducibility. The code will be released on Github.3."
        },
        {
            "heading": "2 Related Work",
            "text": "Information Extraction in Web Mining How to efficiently and automatically gathering essential information from the internet is always a hot topic in the academia of natural language processing and data mining due to the enormous scale and vast knowledge within the internet. The open information extraction task in web mining is originally proposed by Etzioni et al. (2008) and further developed by following works, including Fader et al. (2011); Bronzi et al. (2013); Mausam (2016) which rely on the syntactic constraints or heuristic approaches to identify relation patterns, and Cui et al. (2018); Lockard et al. (2018, 2019); Xie et al. (2021); Li et al. (2023) which introduce neural networks to solve the task under supervision or distant supervision settings. Our proposed method follows the task formulation of the zero-\n3github.com/zlwang-cs/ReXMiner-release\nshot relation extraction in web mining proposed by ZeroShotCeres (Lockard et al., 2020) where the models are required to transfer relation knowledge from the existing verticals to the unseen ones. ZeroShotCeres adopts the graph neural network to understand the textual contents and model the layout structure. It finally produces rich multimodal representation for each text node and conduct binary classification to extract related pairs.\nLayout-aware Multimodal Transformers The pre-trained language models, such as BERT (Kenton and Toutanova, 2019), XLNet (Yang et al., 2019), GPT (Brown et al., 2020), T5 (Raffel et al., 2020), are revolutionary in the academia of natural language processing. It achieves state-of-the-art performance in text-only tasks. To further deal with multimodal scenarios, various features are extracted and incorporated into the Transformer framework. Recent study has shown that it is beneficial to incorporate multimodal features, such as bounding box coordinates and image features,into pre-trained language models to enhance overall performance in understanding visually-rich documents (Xu et al., 2020, 2021; Huang et al., 2022; Gu et al., 2021; Wang et al., 2022b). Similarly, web pages are rendered with HTML/XML markup language and also represent layout-rich structures. Multimodal features from the DOM Tree or rendered web page images are incorporated in the pretrained language models to solve the tasks in the semi-structured web pages (Lin et al., 2020; Zhou et al., 2021; Li et al., 2022b; Wang et al., 2022a)."
        },
        {
            "heading": "3 Problem Formulation",
            "text": "The zero-shot relation extraction in web mining is to learn knowledge of related pairs in the existing web pages and transfer the knowledge to the unseen\nones (Lockard et al., 2020). The unseen web pages should be orthogonal to the existing ones with regard to vertical, topic, and template. The zero-shot setting requires the web mining models to extract relevant pairs based on both the textual content and the DOM Tree structure of web pages. Specifically, each web page is denoted as a sequence of text nodes, P = [x1, x2, ..., xn], where n is the number of nodes in the page. Each node involves textual contents and the XML Path extracted from the DOM Tree, xi = (wi, xpathi). The goal of the zero-shot relation extraction task is to train a model using related pairs, (xi \u2192 xj), from a set of web pages, and subsequently extract related pairs from unseen ones. For example, as shown in Figure 2, one of our tasks is to train models with web pages from Movie and NBA verticals and test the models with web pages from the University vertical."
        },
        {
            "heading": "4 Methodology",
            "text": "We extend the text-only language models with multimodal features and propose a novel framework, ReXMiner, for zero-shot relation extraction task in web mining. Figure 3 shows the components in our framework. We adopt the absolute XML Path embedding in MarkupLM (Li et al., 2022b), and further extend it with popularity embedding and relative XML Path attention. To cope with the sparsity issue in the relation extraction task, we adopt the contrastive learning strategy where we conduct negative sampling to control the ratio between positive cases and negative cases."
        },
        {
            "heading": "4.1 Absolute XML Path Embedding",
            "text": "We follow the idea in MarkupLM and embed the absolute XML Paths in the embedding layer. We introduce it in this section for self-contained purpose. The XML Path is a sequence of tags from HTML/XML markup language (e.g., div, span, li). Both of the tag names and the order of tags are important to the final representation. Therefore, in the embedding layer, we first embed each tag as a embedding vector, and all these tag embeddings are concatenated. To be more specific, we pad or truncate the XPath to a tag sequence of fixed length, [t1, ..., tn], and embed the tags as Emb(t1), ...,Emb(tn) where ti is the i-th tag and Emb(ti) \u2208 Rs is its embedding. We further concatenate the vectors as Emb(t1) \u25e6 ... \u25e6 Emb(tn) \u2208 Rn\u00b7s to explicitly encode the ordering information, where \u25e6 is the operation of vector concatenation.\nTo fit in with the hyperspace of other embedding layers \u2208 Rd, a linear layer is used to convert the concatenation into the right dimension.\nAbsXPathEmb(xpathi)\n=Proj(Emb(t1) \u25e6 ... \u25e6 Emb(tn)) \u2208 Rd\nwhere Proj(\u00b7) is a linear layer with parameters W \u2208 Rns\u00d7d and b \u2208 Rd."
        },
        {
            "heading": "4.2 Popularity Embedding",
            "text": "We propose Popularity Embedding to incorporate the occurrence of the text nodes into the pre-trained framework. Web pages from the same website use similar templates. The popularity of a certain text node across different web pages of the same website is meaningful in the relation extraction task in the web mining. Intuitively, a text node is more likely to be a key word if it appears frequently and its neighboring words are not fixed.\nIn details, given a text node (w, xpath) and N web pages P1, ..., PN from the same website, we iterate through all the text nodes in each web page and compare their textual contents with w, regardless of their XML Paths. We count the web pages that involves nodes with the same text and define the number of these web pages as the popularity of w. Thus, higher popularity of a text node means that the same textual contents appears more fre-\nquently in the group of web pages.\n\u03c3(w,P ) = { 1, if \u2203xpath\u2032, s.t.(w, xpath\u2032) \u2208 P 0, otherwise\npop(w) = N\u2211 i=1 \u03c3(w,Pi)\nwhere pop(w) is the popularity of w. Then we normalize it logarithmically and convert the value into indices ranging from 0 to \u03c4 . Each index corresponds to an embedding vector.\nPopEmb(w) = Emb (\u230a\n\u03c4 \u00b7 log pop(w) logN\n\u230b) \u2208 Rd\nwhere Emb(\u00b7) is the embedding function; \u03c4 is the total number of popularity embeddings; d is the dimension of embedding layers.\nFormally, along with the absolute XML Path embedding, the embedding of the i-th text node, (wi, xpathi), is as follows.\nei =PopEmb(wi) + AbsXPathEmb(xpathi)\n+WordEmb(wi) + PosEmb(i)"
        },
        {
            "heading": "4.3 Self-Attention with Relative XML Paths",
            "text": "The local relation within each web page is essential to the zero-shot relation extraction since the related nodes are more likely to be close in the DOM Tree. As shown in Figure 4, they present a long common\nprefix in their XML Paths, and the rest parts of their XML Paths compose the relative XML Paths between them. The relative XML Paths can be seen as the shortest path between text nodes in the DOM Tree. Therefore, the relative XML Paths are useful signals and could be well transferred into unseen web pages. Enlightened by Yang et al. (2019); Raffel et al. (2020); Peng et al. (2021, 2022), we model the relative XML Paths as bias terms and incorporate them into the multi-layer self-attention of Transformer. Specifically, we embed the common prefix length in the first \u03b1 layers of self-attention and embed the relative XML Paths tags in the next \u03b2 layers of self-attention, where (\u03b1+ \u03b2) equals to the total number of layers. In the case of Figure 4, we embed the common prefix length 4 as well as the relative XML Paths [t4, t3, t5, t6].\nExtracting Relative XML Paths Given a pair of text nodes, xi and xj , we first extract the common prefix of their XML Paths which shows the path from the root to the lowest common ancestor of these two nodes in the DOM Tree (e.g. [t0, t1, t2, t3] in Figure 4). We denote the prefix length as dij . The rest parts in the XML Paths shows the path from the lowest common ancestor to the text node. We denote them as xpath\u2212i and xpath\u2212j which are the XML Paths without the common prefix (e.g. [t5, t6] and [t4, ] in Figure 4). They also compose the shortest path between these\nnodes in the DOM Tree:\nRelXPath(xi \u21d2 xj) = [rev(xpath\u2212i ); t;xpath \u2212 j ] RelXPath(xj \u21d2 xi) = [rev(xpath\u2212j ); t;xpath \u2212 i ]\nwhere rev(\u00b7) is to reverse the tag sequence; t is the lowest common ancestor of xi and xj (e.g. t3 in Figure 4). In the case of Figure 4, rev(xpath\u2212j ) equals [t6, t5], the lowest common ancestor is t3, and xpath\u2212i equals [t4, ], so RelXPath(xj \u21d2 xi) equals [t6, t5, t3, t4].\nAdding Bias Terms In the first \u03b1 layers of the self-attention, we embed the common prefix length dij as bias terms. The attention weight between xi and xj is computed as\nA\u03b1ij = 1\u221a d (WQei) \u22a4(WKej) + b pre(dij)\nwhere the common prefix length dij is a bounded integer and each integer is mapped to a specific bias term by bpre(\u00b7).\nIn the next \u03b2 layers of the self-attention, we embed the relative XML Paths as bias terms. Following the absolute XML Path embedding (introduced in Section 4.1), we project the embedding of tags in RelXPath(xi \u21d2 xj) into bias terms. Specifically, we split the relative XML Path at the lowest common ancestor tag and embed each part separately. When embedding RelXPath(xi \u21d2 xj), the two sub-sequences of tags are [rev(xpath\u2212i ); t] and [t;xpath \u2212 j ].\nIn the equation, tm is the lowest common ancestor (e.g. t3 in Figure 4); [t1, ..., tm] is the path from xi to the lowest common ancestor (e.g. [t4, t3] in Figure 4); [tm, ..., tn] is the path from the lowest common ancestor to xj (e.g. [t3, t5, t6] in Figure 4). The bias term is as follows,\nbxpath(xi, xj) = b(Emb(t1) \u25e6 ... \u25e6 Emb(tm)) + b\u2032(Emb\u2032(tm) \u25e6 ... \u25e6 Emb\u2032(tn)) \u2208 R\nwhere \u25e6 is the operation of vector concatenation; Emb is the embedding function; b is a linear layer projecting embedding to R. We also use two sets of modules to differentiate the two sub-sequences of tags, (b,Emb) and (b\u2032,Emb\u2032). Thus, the attention weight between xi and xj is computed as\nA\u03b2ij = 1\u221a d (WQei) \u22a4(WKej) + b xpath(xi, xj)"
        },
        {
            "heading": "4.4 Contrastive Learning",
            "text": "We observe the sparsity issues in the relation extraction task, where only a small proportion of nodes are annotated as related pairs so the negative cases are much more than the positive ones. To tackle this issue, we adopt the contrastive learning and conduct negative sampling to control the ratio between the positive cases and negative ones.\nNegative Sampling The number of positive cases and negative cases in the sampling should follow,\n#Pos + #Neg = \u03b7 ; #Pos : #Neg = \u00b5\nwhere we denote the number of related pairs in the groundtruth as #Pos and the number of negative samples as #Neg; \u03b7 and \u00b5 are two hyperparameters.\nLoss Function To distinguish the positive samples from the negative ones, we train our model with cross-entropy loss. First, we define the probability of a related pair, (xi \u2192 xj) using the Biaffine attention (Nguyen and Verspoor, 2019) and the sigmoid function \u03c3.\nBiaffine(u, v) = u\u22a4Mv +W (u \u25e6 v) + b P(xi \u2192 xj) = \u03c3(Biaffine(hi, hj))\nwhere hi and hj are the hidden states from ReXMiner corresponding to xi and xj ; M,W, b are trainable parameters; \u25e6 is the vector concatenation. During training, we reduce the cross entropy of training samples against the labels.\nL = \u2211\n(xi,xj)\nCrossEntropy(P(xi \u2192 xj), L(xi, xj))\nwhere L(xi, xj) is the label of (xi, xj), either positive or negative, indicating whether these two nodes are related or not."
        },
        {
            "heading": "5 Experiments",
            "text": "We conduct experiments and ablation study of zeroshot relation extraction on the websites of different verticals from the SWDE dataset following the problem settings proposed in Lockard et al. (2020)."
        },
        {
            "heading": "5.1 Datasets",
            "text": "Our experiments are conducted on the SWDE dataset (Hao et al., 2011). As shown in Figure 2, the SWDE dataset includes websites of three\ndifferent verticals, Movie, NBA, and University, and each vertical includes websites of the corresponding topic. For example, http://imdb.com and http://rottentomatoes.com are collected in the Movie vertical, and http://espn.go.com and http://nba.com are collected in the NBA vertical. Then the SWDE dataset collects web pages in each website and extracts their HTML source code for web mining tasks. Based on the original SWDE dataset, Lockard et al. (2019, 2020) further annotates the related pairs in the web pages, and propose the zero-shot relation extraction task in web mining. The statistics of the SWDE dataset is shown in Table 1, where we report the total number of websites in each vertical, the total number of web pages in each vertical, and the average number of annotated pairs in each web page."
        },
        {
            "heading": "5.2 Experiment Setups",
            "text": "The zero-shot relation extraction task requires that the unseen web pages in the testing set and the existing web pages in the training set are of different verticals. Therefore, we follow the problem settings, and design three tasks based on the SWDE dataset, where we train our model on web pages from two of the three verticals and test our model on the third one. We denote the three tasks as, \u2022 Movie+NBA\u21d2Univ: Train models with the\nMovie and NBA verticals, and test them on the University vertical; \u2022 NBA+Univ\u21d2Movie: Train models with the NBA and University verticals, and test them on the Movie vertical; \u2022 Univ+Movie\u21d2NBA: Train models with the University and Movie verticals, and test them on the NBA vertical.\nWe report the precision, recall, and F-1 score."
        },
        {
            "heading": "5.3 Compared Methods",
            "text": "We evaluate ReXMiner against several baselines.\nColon Baseline The Colon Baseline is a heuristic method proposed in Lockard et al. (2020). It identifies all text nodes ending with a colon (\u201c:\u201d)\nas the relation strings and extracts the closest text node to the right or below as the object. The Colon Baseline needs no training data, so it satisfies the requirement of the zero-shot relation extraction.\nZeroshotCeres ZeroshotCeres (Lockard et al., 2020) is a graph neural network-based approach that learns the rich representation for text nodes and predicts the relationships between them. It first extracts the visual features of text nodes from the coordinates and font sizes, and the textual features by inputting the text into a pre-trained BERT model (Kenton and Toutanova, 2019). Then the features are fed into a graph attention network (GAT) (Velic\u030ckovic\u0301 et al., 2018), where the graph is built based on the location of text nodes in the rendered web page to capture the layout relationships. The relation between text nodes is predicted as a binary classification on their feature concatenation.\nMarkupLM MarkupLM (Li et al., 2022b) is a pre-trained transformer framework that jointly models text and HTML/XML markup language in web pages. It embeds absolute XML Paths in the embedding layer of the BERT framework and proposes new pre-training tasks to learn the correlation between text and markup language. These tasks include matching the title with the web page, predicting the location of text nodes in the DOM Tree, and predicting the masked word in the input sequence. We use MarkupLM as a backbone model and append it with the contrastive learning module of ReXMiner to solve relation extraction task."
        },
        {
            "heading": "5.4 Experimental Results",
            "text": "We report the performance of ReXMiner in Table 2 and compare it with baseline models. From the result, we can see that our proposed model, ReXMiner, achieves the state-of-the-art perfor-\nmance in the zero-shot relation extraction task in all three verticals of the SWDE dataset. Specifically, ReXMiner surpasses the second-best model, MarkupLM, by 5.88 in the average F-1 score. In each task, we can observe a remarkable improvement of 2.93, 10.62 and 4.11 in F-1 score when the Movie, NBA, or University verticals are considered as the unseen vertical, respectively.\nZeroshotCeres is the previous state-of-art model proposed to solve the zero-shot relation extraction which leverages the graph neural network to model the structural information. We copy its performance from Lockard et al. (2020). In the comparison with MarkupLM and ReXMiner, we observe that directly modeling the XML Path information using Transformer framework achieves better performance, where MarkupLM and ReXMiner surpass ZeroshotCeres by 7.17 and 13.05 in average F-1 score. The multimodal attention mechanism with absolute XML Path embedding from MarkupLM enhance the performance in each task, and ReXMiner achieves the state-of-the-art overall performance after incorporating the relative XML Paths and the popularity of text nodes.\nThough the performance of ReXMiner varies in different verticals, we can safely come to the conclusion that our proposed model, ReXMiner, is superior to the baselines in solving zero-shot relation extraction task in web mining. Further analysis is conducted in Ablation Study and Case Study to study the multimodal features."
        },
        {
            "heading": "5.5 Ablation Study",
            "text": "In the ablation study, we aim at studying the role of multimodal features proposed in ReXMiner, including the Relative XML Path Attention and the Popularity Embedding. We introduce three ablation versions of ReXMiner by removing certain\nfeatures in Table 3. From Table 3, we compare the performance of all ablation models. We find that using the Popularity Embedding enhances F-1 score by 2.84 and 2.66 in Movie+NBA\u21d2Univ task and Univ+Movie\u21d2NBA task, respectively. After incorporating the Relative XML Path Attention, the F-1 score are further improved in all three tasks. Thus, the ablation model with all multimodal features achieve the highest F-1 score. We conclude that the Relative XML Path contributes to the high precision while the popularity embedding enhances recall leading to the best performance in F-1 score."
        },
        {
            "heading": "5.6 Case Study",
            "text": "In Table 4, we show the extraction results of the ablation models on Quiz Show.html in NBA+Univ\u21d2Movie. We select one relative XML Path pattern, [span;div;ul,li,a] and list the corresponding extracted pairs into two groups, true positive extractions and false positive extractions. From the results, we can see that ReXMiner with all proposed features shows the best performance, which is also demonstrated in the ablation study. Specifically, by incorporating the Popularity Em-\nbedding, ReXMiner (w/o RelXPath, w/ PopEmb) depends on the frequency when predicting the related pairs so it intends to extract more text node pairs and contributes to a higher recall. After adding the Relative XML Path Attention, the extracted pairs are further filtered by the relative XML Path patterns in ReXMiner (w/ RelXPath + PopEmb) so it can extract similar number of true positive pairs and largely reduce the number of false positive cases, but it leads to the missing extraction of (From book, Remembering America)."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "In this paper, we present ReXMiner, a web mining model to solve the zero-shot relation extraction task from semi-structured web pages. It benefits from the proposed features, the relative XML Paths extracted from the DOM Tree and the popularity of text nodes among web pages from the same website. Specifically, based on MarkupLM, and we further incorporate the relative XML Paths into the attention layers of Transformer framework as bias terms and embed the popularity of text nodes in the embedding layer. To solve the relation ex-\ntraction task, we append the backbone model with the contrastive learning module and use the negative sampling to solve the sparsity issue of the annotation. In this way, ReXMiner can transfer the knowledge learned from the existing web pages to the unseen ones and extract the related pairs from the unseen web pages. Experiments demonstrate that our method can achieve the state-of-the-art performance compared with the strong baselines.\nFor future work, we plan to explore the new problem settings with limited supervision, such as few-shot learning and distant supervision, and further study the topological structure information in the DOM Tree to explore more meaningful signals in understanding the semi-structured web pages in web mining tasks."
        },
        {
            "heading": "7 Limitations",
            "text": "We build ReXMiner based on MarkupLM and incorporate new features, including the relative XML Paths, the popularity of text nodes, and the contrastive learning. After initializing our model with the pre-trained weights of MarkupLM, the additional modules are finetuned on the datasets of downstream tasks without large-scale pre-training, due to the limited computing resource. We believe more promising results can be achieved if it is possible to pre-train our proposed framework enabling all parameters to be well converged."
        },
        {
            "heading": "8 Ethics Statement",
            "text": "Our work focus on the relation extraction in web mining under zero-shot settings. We build our framework using Transformers repository by Huggingface (Wolf et al., 2019), and conduct experiments on the SWDE datasets (Hao et al., 2011). All resources involved in our paper are from open source and widely used in academia. We also plan to release our code publicly. Thus, we do not anticipate any ethical concerns."
        },
        {
            "heading": "Acknowledgments",
            "text": "We want to thank the anonymous reviewers for their insightful comments. Our work is sponsored in part by NSF CAREER Award 2239440, NSF Proto-OKN Award 2333790, NIH Bridge2AI Center Program under award 1U54HG012510-01, Cisco-UCSD Sponsored Research Project, as well as generous gifts from Google, Adobe, and Teradata. Any opinions, findings, and conclusions or recommendations expressed herein are those of the\nauthors and should not be interpreted as necessarily representing the views, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for government purposes not withstanding any copyright annotation hereon."
        },
        {
            "heading": "B Zero-shot Relation Extraction on Unseen Websites",
            "text": "In this paper, we propose ReXMiner to solve the zero-shot relation extraction task where web pages in the training set and the testing set are from different verticals. Here, we report the results of the additional experiments for the zero-shot relation extraction on unseen websites. To be more specific, in these additional experiments, the web pages in the training set and the testing set are from the same vertical but different websites. For each vertical in the SWDE dataset, we select a subset of websites as the testing set and train the model with the rest websites. We select \u201crottentomatoes\" and \u201cyahoo\" from Movie Vertical, \u201cyahoo\u201d from NBA Vertical, and \u201cecampustours\" and \u201cusnews\" from University Vertical. We report the results in Table 5."
        }
    ],
    "title": "Towards Zero-shot Relation Extraction in Web Mining: A Multimodal Approach with Relative XML Path",
    "year": 2023
}