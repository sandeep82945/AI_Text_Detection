{
    "abstractText": "Distantly supervised named entity recognition (DS-NER) aims to locate entity mentions and classify their types with only knowledge bases or gazetteers and unlabeled corpus. However, distant annotations are noisy and degrade the performance of NER models. In this paper, we propose a noise-robust prototype network named MProto for the DS-NER task. Different from previous prototype-based NER methods, MProto represents each entity type with multiple prototypes to characterize the intra-class variance among entity representations. To optimize the classifier, each token should be assigned an appropriate ground-truth prototype and we consider such token-prototype assignment as an optimal transport (OT) problem. Furthermore, to mitigate the noise from incomplete labeling, we propose a novel denoised optimal transport (DOT) algorithm. Specifically, we utilize the assignment result between Other class tokens and all prototypes to distinguish unlabeled entity tokens from true negatives. Experiments on several DS-NER benchmarks demonstrate that our MProto achieves state-of-the-art performance. The source code is now available on Github1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shuhui Wu"
        },
        {
            "affiliations": [],
            "name": "Yongliang Shen"
        },
        {
            "affiliations": [],
            "name": "Zeqi Tan"
        },
        {
            "affiliations": [],
            "name": "Wenqi Ren"
        },
        {
            "affiliations": [],
            "name": "Jietian Guo"
        },
        {
            "affiliations": [],
            "name": "Shiliang Pu"
        },
        {
            "affiliations": [],
            "name": "Weiming Lu"
        }
    ],
    "id": "SP:a6be1cc9a55c31cfa61263e5db5e96b7af7831d7",
    "references": [
        {
            "authors": [
                "Marco Cuturi."
            ],
            "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
            "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held Decem-",
            "year": 2013
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Ning Ding",
                "Xiaobin Wang",
                "Yao Fu",
                "Guangwei Xu",
                "Rui Wang",
                "Pengjun Xie",
                "Ying Shen",
                "Fei Huang",
                "Hai-Tao Zheng",
                "Rui Zhang."
            ],
            "title": "Prototypical representation learning for relation extraction",
            "venue": "9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Fritzler",
                "Varvara Logacheva",
                "Maksim Kretov."
            ],
            "title": "Few-shot classification in named entity recognition task",
            "venue": "Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing, SAC \u201919, pages 993\u20131000, New York, NY, USA.",
            "year": 2019
        },
        {
            "authors": [
                "Octavian-Eugen Ganea",
                "Thomas Hofmann."
            ],
            "title": "Deep joint entity disambiguation with local neural attention",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2619\u20132629, Copenhagen, Denmark. Associa-",
            "year": 2017
        },
        {
            "authors": [
                "Zhiheng Huang",
                "Wei Xu",
                "Kai Yu."
            ],
            "title": "Bidirectional LSTM-CRF Models for Sequence Tagging",
            "venue": "ArXiv preprint, abs/1508.01991.",
            "year": 2015
        },
        {
            "authors": [
                "Guillaume Lample",
                "Miguel Ballesteros",
                "Sandeep Subramanian",
                "Kazuya Kawakami",
                "Chris Dyer."
            ],
            "title": "Neural architectures for named entity recognition",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang."
            ],
            "title": "BioBERT: A pre-trained biomedical language representation model for biomedical text mining",
            "venue": "Bioinformatics (Oxford, England), 36(4):1234\u20131240.",
            "year": 2020
        },
        {
            "authors": [
                "Qi Li",
                "Heng Ji."
            ],
            "title": "Incremental joint extraction of entity mentions and relations",
            "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 402\u2013412, Baltimore, Maryland. Association",
            "year": 2014
        },
        {
            "authors": [
                "Yangming Li",
                "Lemao Liu",
                "Shuming Shi."
            ],
            "title": "Empirical analysis of unlabeled entity problem in named entity recognition",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.",
            "year": 2021
        },
        {
            "authors": [
                "Yangming Li",
                "Lemao Liu",
                "Shuming Shi."
            ],
            "title": "Rethinking negative sampling for handling missing entity annotations",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7188\u20137197,",
            "year": 2022
        },
        {
            "authors": [
                "Chen Liang",
                "Yue Yu",
                "Haoming Jiang",
                "Siawpeng Er",
                "Ruijia Wang",
                "Tuo Zhao",
                "Chao Zhang"
            ],
            "title": "BOND: bert-assisted open-domain named entity recognition",
            "year": 2020
        },
        {
            "authors": [
                "Yaojie Lu",
                "Qing Liu",
                "Dai Dai",
                "Xinyan Xiao",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun",
                "Hua Wu."
            ],
            "title": "Unified structure generation for universal information extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Tingting Ma",
                "Huiqiang Jiang",
                "Qianhui Wu",
                "Tiejun Zhao",
                "Chin-Yew Lin."
            ],
            "title": "Decomposed metalearning for few-shot named entity recognition",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1584\u20131596, Dublin, Ire-",
            "year": 2022
        },
        {
            "authors": [
                "Yu Meng",
                "Yunyi Zhang",
                "Jiaxin Huang",
                "Xuan Wang",
                "Yu Zhang",
                "Heng Ji",
                "Jiawei Han."
            ],
            "title": "Distantlysupervised named entity recognition with noiserobust learning and language model augmented selftraining",
            "venue": "Proceedings of the 2021 Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Makoto Miwa",
                "Mohit Bansal."
            ],
            "title": "End-to-end relation extraction using LSTMs on sequences and tree structures",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1105\u20131116, Berlin,",
            "year": 2016
        },
        {
            "authors": [
                "Minlong Peng",
                "Xiaoyu Xing",
                "Qi Zhang",
                "Jinlan Fu",
                "Xuanjing Huang."
            ],
            "title": "Distantly supervised named entity recognition using positive-unlabeled learning",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2409\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Jingbo Shang",
                "Liyuan Liu",
                "Xiaotao Gu",
                "Xiang Ren",
                "Teng Ren",
                "Jiawei Han."
            ],
            "title": "Learning named entity tagger using domain-specific dictionary",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2054\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Yongliang Shen",
                "Xinyin Ma",
                "Zeqi Tan",
                "Shuai Zhang",
                "Wen Wang",
                "Weiming Lu."
            ],
            "title": "Locate and label: A two-stage identifier for nested named entity recognition",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Yongliang Shen",
                "Xinyin Ma",
                "Yechun Tang",
                "Weiming Lu"
            ],
            "title": "2021b. A Trigger-Sense Memory Flow",
            "year": 2021
        },
        {
            "authors": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Dongsheng Li",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "DiffusionNER: Boundary diffusion for named entity recognition",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2023
        },
        {
            "authors": [
                "Yongliang Shen",
                "Zeqi Tan",
                "Shuhui Wu",
                "Wenqi Zhang",
                "Rongsheng Zhang",
                "Yadong Xi",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "PromptNER: Prompt locating and typing for named entity recognition",
            "venue": "Proceedings of the 61st Annual Meeting of the As-",
            "year": 2023
        },
        {
            "authors": [
                "Yongliang Shen",
                "Xiaobin Wang",
                "Zeqi Tan",
                "Guangwei Xu",
                "Pengjun Xie",
                "Fei Huang",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "Parallel instance query network for named entity recognition",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Jake Snell",
                "Kevin Swersky",
                "Richard S. Zemel."
            ],
            "title": "Prototypical networks for few-shot learning",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,",
            "year": 2017
        },
        {
            "authors": [
                "Mohammad Golam Sohrab",
                "Makoto Miwa."
            ],
            "title": "Deep exhaustive model for nested named entity recognition",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2843\u20132849, Brussels, Belgium. Associa-",
            "year": 2018
        },
        {
            "authors": [
                "Zeqi Tan",
                "Yongliang Shen",
                "Shuai Zhang",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "A Sequence-to-Set Network for Nested Named Entity Recognition",
            "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, pages 3936\u20133942,",
            "year": 2021
        },
        {
            "authors": [
                "Erik F. Tjong Kim Sang."
            ],
            "title": "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition",
            "venue": "COLING-02: The 6th Conference on Natural Language Learning 2002 (CoNLL-2002).",
            "year": 2002
        },
        {
            "authors": [
                "Meihan Tong",
                "Shuai Wang",
                "Bin Xu",
                "Yixin Cao",
                "Minghui Liu",
                "Lei Hou",
                "Juanzi Li."
            ],
            "title": "Learning from miscellaneous other-class words for few-shot named entity recognition",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Jue Wang",
                "Lidan Shou",
                "Ke Chen",
                "Gang Chen."
            ],
            "title": "Pyramid: A layered model for nested named entity recognition",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5918\u20135928, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Chih-Hsuan Wei",
                "Yifan Peng",
                "Robert Leaman",
                "Allan Peter Davis",
                "Carolyn J. Mattingly",
                "Jiao Li",
                "Thomas C. Wiegers",
                "Zhiyong Lu"
            ],
            "title": "Assessing the state of the art in biomedical relation extraction: Overview of the BioCreative V chemical-disease",
            "year": 2016
        },
        {
            "authors": [
                "Shuhui Wu",
                "Yongliang Shen",
                "Zeqi Tan",
                "Weiming Lu."
            ],
            "title": "Propose-and-Refine: A Two-Stage Set Prediction Network for Nested Named Entity Recognition",
            "venue": "Thirty-First International Joint Conference on Artificial Intelligence, volume 5, pages 4418\u20134424.",
            "year": 2022
        },
        {
            "authors": [
                "Yiquan Wu",
                "Kun Kuang",
                "Yating Zhang",
                "Xiaozhong Liu",
                "Changlong Sun",
                "Jun Xiao",
                "Yueting Zhuang",
                "Luo Si",
                "Fei Wu."
            ],
            "title": "De-biased court\u2019s view generation with causality",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Yiquan Wu",
                "Yifei Liu",
                "Weiming Lu",
                "Yating Zhang",
                "Jun Feng",
                "Changlong Sun",
                "Fei Wu",
                "Kun Kuang."
            ],
            "title": "Towards interactivity and interpretability: A rationale-based legal judgment prediction framework",
            "venue": "Proceedings of the 2022 Conference on Empiri-",
            "year": 2022
        },
        {
            "authors": [
                "Yiquan Wu",
                "Weiming Lu",
                "Yating Zhang",
                "Adam Jatowt",
                "Jun Feng",
                "Changlong Sun",
                "Fei Wu",
                "Kun Kuang."
            ],
            "title": "Focus-aware response generation in inquiry conversation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 12585\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Hang Yan",
                "Tao Gui",
                "Junqi Dai",
                "Qipeng Guo",
                "Zheng Zhang",
                "Xipeng Qiu."
            ],
            "title": "A unified generative framework for various NER subtasks",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Juntao Yu",
                "Bernd Bohnet",
                "Massimo Poesio."
            ],
            "title": "Named entity recognition as dependency parsing",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6470\u2013 6476, Online. Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Wenkai Zhang",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun"
            ],
            "title": "2021a. De-biasing distantly supervised named entity",
            "year": 2021
        },
        {
            "authors": [
                "Wenkai Zhang",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun",
                "Huidan Liu",
                "Zhicheng Wei",
                "Nicholas Yuan."
            ],
            "title": "Denoising Distantly Supervised Named Entity Recognition via a Hypergeometric Probabilistic Model",
            "venue": "Proceedings of the AAAI Conference on Arti-",
            "year": 2021
        },
        {
            "authors": [
                "Xinghua Zhang",
                "Bowen Yu",
                "Tingwen Liu",
                "Zhenyu Zhang",
                "Jiawei Sheng",
                "Xue Mengge",
                "Hongbo Xu."
            ],
            "title": "Improving distantly-supervised named entity recognition with self-collaborative denoising learning",
            "venue": "Proceedings of the 2021 Conference",
            "year": 2021
        },
        {
            "authors": [
                "Kang Zhou",
                "Yuepei Li",
                "Qi Li."
            ],
            "title": "Distantly supervised named entity recognition via confidencebased multi-class positive and unlabeled learning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Named Entity Recognition (NER) is a fundamental task in natural language processing and is essential in various downstream tasks (Li and Ji, 2014; Miwa and Bansal, 2016; Ganea and Hofmann, 2017; Shen et al., 2021b). Most state-of-the-art NER models are based on deep neural networks and require massive annotated data for training. However, human annotation is expensive, time-consuming, and often unavailable in some specific domains. To mitigate the reliance on human annotation, distant supervision is widely applied to automatically generate annotated data with the aid of only knowledge bases or gazetteers and unlabeled corpus.\n\u2020 Corresponding author. 1https://github.com/XiPotatonium/mproto\nDespite its easy accessibility, the distantlyannotated data is rather noisy, severely degrading the performance of NER models. We observe two types of issues in the distantly-supervised named entity recognition task. The first issue is incomplete labeling. Due to the limited coverage of dictionaries, entities not presented in dictionaries are mistakenly labeled as Other class (denoted as O class). For instance, in Figure 1, \u201cMicheal\u201d is not covered in the dictionary and thus cannot be correctly labeled by distant supervision. These unlabeled entities will misguide NER models to overfit the label noise, particularly hurting recall. The second issue is intra-class variance. As illustrated in Figure 1, tokens of the same class (e.g., \u201cMainStay\u201d and \u201cFunds\u201d) may fall into different sub-clusters in feature space due to semantic differences. Traditional single-prototype classifiers do not consider the semantic difference within the same entity type. They set only one prototype for each type, which suffers from intra-class variance.\nVarious methods have been proposed to mitigate the noise of incomplete labeling. For example, AutoNER (Shang et al., 2018) tries to modify the standard CRF-based classifier to adapt to the noisy NER datasets. Self-learning-based methods (Liang\net al., 2020; Meng et al., 2021; Zhang et al., 2021c) learn by soft labels generated from a teacher model for denoising. Positive-unlabeled learning methods treat tokens of O class as unlabeled samples and optimize with an unbiasedly estimated empirical risk (Peng et al., 2019; Zhou et al., 2022). Some other studies utilize negative sampling to avoid NER models overfitting label noise in O class (Li et al., 2021, 2022). Despite these efforts, the issue of intra-class variance has not yet been studied in previous DS-NER research.\nUnlike previous methods, we propose a novel prototype-based network named MProto for distantly-supervised named entity recognition. In MProto, each class is represented by multiple prototypes so that each prototype can represent a certain sub-cluster of an entity type. In this way, our MProto can characterize the intra-class variance. To optimize our multiple-prototype classifier, we assign each input token with an appropriate ground-truth prototype. We formulate such tokenprototype assignment problem as an optimal transport (OT) problem. Moreover, we specially design a denoised optimal transport (DOT) algorithm for tokens labeled as O class to alleviate the noise of incomplete labeling. Specifically, we perform the assignment between O tokens and all prototypes and regard O tokens assigned to O prototypes as true negatives while tokens assigned to prototypes of entity classes as label noise. Based on our observation, before overfitting, unlabeled entity tokens tend to be assigned to prototypes of their actual classes in our similarity-based token-prototype assignment. Therefore, unlabeled entity tokens can be discriminated from clean samples by our DOT algorithm so as not to misguide the training.\nOur main contributions are three-fold:\n\u2022 We present MProto for the DS-NER task. MProto represents each entity type with multiple prototypes for intra-class variance. And we model the token-prototype assignment problem as an optimal transport problem.\n\u2022 To alleviate the noise of incomplete labeling, we propose the denoised optimal transport algorithm. Tokens labeled as O but assigned to non-O prototypes are considered as label noise so as not to misguide the training.\n\u2022 Experiments on various datasets show that our method achieves SOTA performance. Further analysis validates the effectiveness of our\nmultiple-prototype classifier and denoised optimal transport."
        },
        {
            "heading": "2 Method",
            "text": "In this section, we first introduce the task formulation in Section 2.1. Then, we present our MProto network in Section 2.2. To compute the crossentropy loss, we assign each token with a groundtruth prototype, and the token-prototype assignment will be discussed in Section 2.3. Besides, to mitigate the incomplete labeling noise in O tokens, we propose the denoised optimal transport algorithm which will be specified in Section 2.4. Figure 2 illustrates the overview of our MProto."
        },
        {
            "heading": "2.1 Task Formulation",
            "text": "Following the tagging-based named entity recognition paradigm, we denote the input sequence as X = [x1, \u00b7 \u00b7 \u00b7 , xL] and the corresponding distantlyannotated tag sequence as Y = [y1, . . . , yL]. For each token-label pair (xi, yi), xi is the input token and yi \u2208 C = {c1, . . . , cK} is the class of the token. Here we let c1 be O class and others be predefined entity classes. We denote the human-annotated tag sequence as Y\u0303 = [y\u03031, . . . , y\u0303L]. Human annotations can be considered as true labels. In the distantlysupervised NER task, only distant annotations can be used for training while human annotations are only available in evaluation."
        },
        {
            "heading": "2.2 Multi-Prototype Network",
            "text": "Token Encoding. For each input token sequence X , we generate its corresponding features with a pretrained language model:\nH = f\u03b8(X) (1)\nwhere f\u03b8 is the pretrained language model parameterized by \u03b8 and H = [h1, . . . ,hL] is the features of the token sequence.\nPrototype-based Classification. MProto is a prototype-based classifier where predictions are made based on token-prototype similarity. Visualization in Figure 4 shows that tokens of the same class will form multiple sub-clusters in the feature space due to the semantic difference. However, previous prototype-based classifiers which represent each entity type with only a single prototype cannot characterize such intra-class variance. To this end, we represent each entity type with multiple prototypes. Specifically, for each class c, let\nPc = {pc,1, . . . ,pc,M} be the set of M prototypes representing class c. For the classification, we compare the similarity between token features and all prototypes, and the class of the most similar prototype is chosen as the prediction:\nc\u0302i = argmax c s(hi,pc,m) (2)\nwhere s(h,p) = h\u00b7p||h||\u00b7||p|| is the similarity metric and here we adopt cosine similarity. In the inference process, consecutive tokens of the same type are considered a singular entity.\nLoss. To update the parameters of our MProto, we calculate the loss between token features and their corresponding ground-truth prototypes. First, we should assign each token with an appropriate ground-truth prototype. Specifically, for each token i and its annotation yi = ci, one of the prototypes of class ci will be assigned as the ground truth based on the similarity between the token feature and prototypes. Such token-prototype assignment is considered as an optimal transport problem, and the detail of solving the token-prototype assignment will be discussed later (in Section 2.3). Based on the assigned ground-truth prototype pci,m for token i, we can compute the cross-entropy loss:\n\u2113CE = \u2212 \u2211 i log exp(s(hi,pci,m))\u2211 c\u2032,m\u2032 exp(s(hi,pc\u2032,m\u2032)) (3)\nHowever, optimizing the model only through CE loss guides tokens to be relatively close to their ground-truth prototypes in feature space, while the compactness of the token features within the same sub-cluster is not considered. To this end, we further optimize the absolute distance between token features and ground-truth prototypes as follows:\n\u2113c = \u2211 i d2(hi,pci,m) = \u2211 i (1\u2212 s(hi,pci,m))2\n(4) here we define the distance based on cosine similarity as d(hi,pci,m) = 1\u2212 s(hi,pci,m). The overall loss can be calculated as follows:\n\u2113 = \u2113CE + \u03bbc\u2113c (5)\nwhere \u03bbc is the weight of the compactness regularization.\nPrototype Updating. We update each prototype with the token features assigned to that prototype. For convenience, we denote the set of tokens assigned to prototype pc,m as T . At each training step t, we update prototypes with exponential moving average (EMA):\nptc,m = \u03b1p t\u22121 c,m + (1\u2212 \u03b1)\n\u2211 i\u2208T hi\n|T | (6)\nwhere \u03b1 is the EMA updating ratio. With EMA updating, the learned prototypes can be considered\nas the representative of a certain sub-cluster in the feature space."
        },
        {
            "heading": "2.3 Token-Prototype Assignment for Entity Tokens",
            "text": "In our MProto, each class is represented by multiple prototypes. Therefore, how to assign a token with an appropriate ground-truth prototype is an essential issue for optimization. First, we denote the set of tokens labeled as class c as Tc. We aim to compute the assignment matrix \u03b3c \u2208 R|Tc|\u00d7M between Tc and Pc (prototypes of class c). Then the assigned prototype pci,m for token i can be obtained by m = argmaxj \u03b3 c i,j . We consider such token-prototype assignment problem as an optimal transport problem:\n\u03b3\u0302c = argmin \u03b3c \u2211 i\u2208Tc M\u2211 j=1 \u03b3ci,jC c i,j ,\ns.t. \u03b3c1 = a, \u03b3c\u22a41 = b\n(7)\nwhere Cci,j = d(hi,pc,j) is the cost matrix which is composed of the distances between features of the token set Tc and prototype set Pc, a = 1 \u2208 R|Tc| is the assignment constraint for each token which guarantees that each token is expected to be assigned to exactly one prototype, b \u2208 RM is the assignment constraint for each prototype which prevents the model from the trivial solution where all tokens are assigned to a single prototype. The constraint can be set based on prior knowledge. However, to keep our method simple, we simply choose even distribution (b = |Tc|M 1). The experiments also show that such simple constraint can already achieve satisfactory performance.\nBy optimizing Equation 7, we aim to obtain an assignment where each token is assigned a similar prototype. The token-prototype assignment problem can be solved by the sinkhorn-knopp algorithm (Cuturi, 2013), which is detailed in Appendix A."
        },
        {
            "heading": "2.4 Token-Prototype Assignment for O Tokens",
            "text": "Incomplete labeling noise is a common issue in the distantly supervised named entity recognition task. If we directly assign all O tokens to O prototypes, features of unlabeled entity tokens will be pulled closer to O prototypes and farther to prototypes of their actual entity type, which leads to overfitting. To mitigate the noise of incomplete labeling, we specially design the denoised optimal transport algorithm for O tokens. Specifically, we allow all\nprototypes to participate in the assignment of O tokens. Here we denote the set of tokens labeled as O class as To and the assignment matrix between To and all prototypes as \u03b3o \u2208 R|To|\u00d7KM . The denoised optimal transport is formulated as follows:\n\u03b3\u0302o = argmin \u03b3o \u2211 i\u2208To KM\u2211 j=1 \u03b3oi,jC all i,j ,\ns.t. \u03b3o1 = a, \u03b3o\u22a41 = b\n(8)\nwhere Call = [Cc1 , . . . ,CcK ] \u2208 R|To|\u00d7KM is the cost matrix composed of the distances between features of the token set To and all prototypes, and [\u00b7] is the concatenation operation. The first constraint a = 1 \u2208 R|To| indicates that each token is expected to be assigned to exactly one prototype. The second constraint is formulated as:\nb = [ \u03b2|To| M 1, (1\u2212 \u03b2)|To| (K \u2212 1)M 1, . . . , (1\u2212 \u03b2)|To| (K \u2212 1)M\n1\ufe38 \ufe37\ufe37 \ufe38 K\u22121 ]\n(9) where \u03b2 is the assignment ratio for O prototypes. It indicates that: (1) we expect \u03b2|To| tokens to be assigned to O prototypes, (2) the remaining tokens are assigned to non-O prototypes, (3) the token features are evenly assigned to prototypes of the same type.\nIntuitively, before the model overfits the incomplete labeling noise, unlabeled entity tokens are similar to prototypes of their actual entity type in feature space. So these unlabeled entity tokens tend to be assigned to prototypes of their actual entity type in our similarity-base token-prototype assignment. Therefore, tokens assigned to O prototypes can be considered as true negatives while others can be considered as label noise. We then modify the CE loss in Equation 3 as follows:\n\u2113CE =\u2212 \u2211 i\u2208T\u00aco log exp(s(hi,pci,m))\u2211 c\u2032,m\u2032 exp(s(hi,pc\u2032,m\u2032))\n\u2212 \u2211 i\u2208To wi log exp(s(hi,pi))\u2211 c\u2032,m\u2032 exp(s(hi,pc\u2032,m\u2032))\n(10) where T\u00aco = \u22c3M i=2 Tci is the set of tokens not labeled as O, wi = 1(pi \u2208 Pc1) is the indicator with value 1 when the assigned prototype pi for token i is of class c1 (O class). The first term of Equation 10 is the loss for entity tokens which is identical to traditional CE loss. The second term is the loss for O tokens where only these true negative\nsamples are considered. In this way, unlabeled entity tokens will not misguide the training of our MProto, and the noise of incomplete annotation can be mitigated."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Settings",
            "text": "NER Dataset. The distantly-annotated data is generated from two wildly used flat NER datasets: (1) CoNLL03 (Tjong Kim Sang, 2002) is an opendomain English NER dataset that is annotated with four entity types: PER, LOC, ORG and MISC. It is split into 14041/3250/3453 train/dev/test sentences. (2) BC5CDR (Wei et al., 2016) is a biomedical domain English NER dataset which consists of 1500 PubMed articles with 4409 annotated Chemicals and 5818 Diseases. It is split into 4560/4579/4797 train/dev/test sentences.\nDistant Annotation. We generate four different distantly-annotated datasets based on CoNLL03 and BC5CDR for the main experiment: (1) CoNLL03 (Dict) is generated by dictionary matching with the dictionary released by Peng et al. (2019). (2) CoNLL03 (KB) is generated by KB matching follows (Liang et al., 2020). (3) BC5CDR (Big Dict) is generated by dictionary matching with the dictionary released by Shang et al. (2018). (4) BC5CDR (Small Dict) is generated by the first 20% of the dictionary used in BC5CDR (Big Dict). We follow the dictionary matching algorithm presented in (Zhou et al., 2022) and the KB matching algorithm presented in (Liang et al., 2020).\nEvaluation Metric. We train our MProto with the distantly-annotated train set. Then the model is evaluated on the human-annotated dev set. The best checkpoint on the dev set is tested on the humanannotated test set, and the performance on the test set is reported as the final result. Entity prediction is considered correct when both span and category are correctly predicted.\nImplementation Details. For a fair comparison, we use the BERT-base-cased (Devlin et al., 2019) for the CoNLL03 dataset, and BioBERT-basecased (Lee et al., 2020) for the BC5CDR dataset. We set M = 3 prototypes per class and the hyperparameter search experiment can be found in Appendix E. More detailed hyperparameters can be found in Appendix B."
        },
        {
            "heading": "3.2 Baselines",
            "text": "Fully-Supervised. We implement a fullysupervised NER model for comparison. The model is composed of a BERT encoder and a linear layer as the classification head. Fully-supervised model is trained with human-annotated data, and the performance can be viewed as the upper bound of the DS-NER models.\nAutoNER. AutoNER (Shang et al., 2018) is a DS-NER method that classifies two adjacent tokens as break or tie. To mitigate the noise of incomplete labeling, the unknown type is not considered in loss computation. For a fair comparison, we reimplement their method to use BERT-base-cased as the encoder for the CoNLL03 dataset and use BioBERT-base-cased for the BC5CDR dataset.\nEarly Stoping. Liang et al. (2020) apply early stopping to prevent the model from overfitting the label noise. For a fair comparison, we reimplement their method to use BERT-base-cased as the encoder for the CoNLL03 dataset and use BioBERT-base-cased for the BC5CDR dataset.\nNegative Sampling. Li et al. (2021, 2022) use negative sampling to eliminate the misguidance of the incomplete labeling noise. We re-implement their method and set the negative sampling ratio to be 0.3 as recommended in (Li et al., 2021).\nMPU. Multi-class positive-unlabeled (MPU) learning (Zhou et al., 2022) considers the negative samples as unlabeled data and optimizes with the unbiasedly estimated task risk. Conf-MPU incorporates the estimated confidence score for tokens being an entity token into the MPU risk estimator to alleviate the impact of annotation imperfection."
        },
        {
            "heading": "3.3 Overall Performance",
            "text": "Table 1 shows the overall performance of our MProto compared with various baselines in four distantly-annotated datasets. We observe that our MProto achieves state-of-the-art performance on all four datasets. Specifically, our MProto achieves +1.48%, +0.39%, +0.28%, and +0.60% improvement in F1 score on BC5CDR (Big Dict), BC5CDR (Small Dict), CoNLL03 (KB), and CoNLL03 (Dict) compared with previous state-of-the-art methods. We also notice that our MProto achieves consistent improvement on all datasets. In contrast, previous SOTA methods usually achieve promising results on some datasets while performing poorly in other\ncases. We attribute this superior performance to two main factors: (1) The denoised optimal transport algorithm significantly mitigates the noise of incomplete labeling in O tokens by leveraging the similarity-based token-prototype assignment result, leading to a performance improvement across varying annotation quality (with differences in dictionary coverage or Distant Supervision matching algorithms). (2) Our multiple prototype classifier can characterize the intra-class variance and can better model rich semantic entity classes, which improves the robustness across diverse data domains."
        },
        {
            "heading": "3.4 Ablation Study",
            "text": "We conduct the ablation study in the following three aspects. The results are shown in Table 2.\nMultiple Prototypes. By representing each entity type with multiple prototypes, the F1 score of MProto improves by +0.95% on BC5CDR (Big Dict) and by +1.26% on CoNLL03 (Dict). It confirms that the multiple-prototype classifier can greatly help the MProto characterize the intra-class variance of token features.\nCompactness Loss. By applying compactness loss as elaborated in Equation 4, the F1 score of MProto improves by +2.23% on BC5CDR and by +0.29%. Simply optimizing the Cross-Entropy loss in Equation 3 only encourages the token features to be relatively close to the assigned ground-truth prototypes. While the compactness loss directly optimizes the distance between the token feature and the prototype to be small, making the token features of the same prototype more compact.\nDenoised Optimal Transport. Compared with assigning all O tokens to O prototypes, the denoised optimal transport elaborated in Section 2.4 improves the F1 score of MProto by +21.89% on BC5CDR and by +34.14% on CoNLL03. The improvement indicates that the denoised optimal transport significantly mitigates the incomplete labeling\nnoise in the DS-NER task."
        },
        {
            "heading": "3.5 Experiments on Dictionary Coverage",
            "text": "To analyze the performance of our method with different coverage of dictionaries, we generate distant annotations with dictionaries of different proportions following (Zhou et al., 2022). The distant annotation quality can be found in Table 3 and the experiment result is shown in Figure 3.\nThe result shows that the performance of Negative Sampling and Early Stopping drops significantly when the coverage of the dictionary decreases. When the dictionary size drops from 100% to 20%, the F1 score of the Negative Sampling drops by -54.48% on the BC5CDR dataset and by -47.44% on the CoNLL03 dataset. The F1 score of the Early Stopping drops by -56.13% on the BC5CDR dataset and by -51.02% on the CoNLL03 dataset. This indicates that these methods suffer severely from incomplete labeling noise. However, our MProto only suffers from a slight performance drop compared with Negative Sampling and Early Stopping. The F1 score drops only by -7.17% on BC5CDR and by -9.29% on CoNLL03. We can conclude that with multiple prototypes and denoised optimal transport, our MProto is more robust to the incomplete labeling noise and can achieve better performance, especially on the DS-\nNER datasets with low-coverage dictionaries."
        },
        {
            "heading": "3.6 Visualization of Tokens and Prototypes",
            "text": "We get the token features and the embedding of the prototypes from the best checkpoint on the dev set. To visualize high-dimensional features, we convert token features and prototype embeddings to 2-dimensions by t-SNE.\nAs illustrated in Figure 4, we can conclude that: (1) The intra-class variance is a common phenomenon in the NER task. The token features of Disease class form two sub-clusters. Representing each entity type with only a single prototype cannot cover token features in all sub-clusters. Therefore, single-prototype networks cannot characterize the intra-class variance and the performance will suffer. (2) Our MProto can model the intra-class variance in the NER task. For each sub-cluster of Disease, there exists at least one prototype to represent that sub-cluster. In this way, the performance of the NER method can benefit from the multiple-prototype network.\nMore visualizations on different datasets and MProto with different prototypes can be found in Appendix C."
        },
        {
            "heading": "3.7 Effectiveness of Denoised Optimal Transport",
            "text": "To validate the efficacy of our DOT algorithm, we present the similarity between tokens and prototypes over training steps in Figure 5. The tokenprototype similarity of each entity type is obtained by:\nsimc = 1 |\u03c7c| \u2211 x\u2208\u03c7c max m s(x,pc,m) (11)\nwhere \u03c7c is the set of tokens whose actual class is\nc (the actual class of token is obtained from human annotation of CoNLL03) and s denotes the cosine similarity function. As can be observed from the figure, when the model is trained using the DOT, the similarity between entity tokens and their actual prototypes exhibits a consistent increase throughout the training, thereby bolstering the entity token classification performance. In contrast, the model trained without DOT exhibits a decrease in similarity, suggesting that the model overfits the incomplete labeling noise. Regarding O tokens, the similarity between O tokens and O prototypes is higher when trained without DOT. This implies the network tends to predict the O class, which leads to false negative. While with our DOT algorithm, this issue can be mitigated. Based on these observations, we can conclude that the DOT algorithm plays a important role in alleviating the noise of incomplete labeling, and thereby significantly enhancing the performance in the DS-NER task."
        },
        {
            "heading": "4 Related Work",
            "text": "Named entity recognition is a fundamental task in natural language processing and has been applied to various downstream tasks (Li and Ji, 2014; Miwa and Bansal, 2016; Ganea and Hofmann, 2017; Wu et al., 2020; Shen et al., 2021b; Wu et al., 2022b, 2023). NER methods can be divided into two main categories: tagging-based and span-based. Tagging-based methods (Lample et al., 2016; Huang et al., 2015) predict a label for each token, which perform well at detecting flat named entities while failing at detecting nested named entities. Span-based methods (Sohrab and Miwa, 2018; Yu et al., 2020; Wang et al., 2020; Shen et al., 2021a) perform classification over span representations, which performs well on the nested NER task but is inferior in computational complexity. Tan et al. (2021); Shen et al. (2022); Wu et al. (2022a) design a query-based NER framework that optimizes entity queries using bipartite graph matching. Recently, some generative NER methods (Yan et al., 2021; Shen et al., 2023a,b; Lu et al., 2022) have been proposed with superior performance on various NER tasks. These supervised NER methods require a large amount of annotated data for training.\nDS-NER. To mitigate the need for human annotations, distant supervision is widely used. The main challenge of the DS-NER task is the label noise, of which the most widely studied is the incomplete\nlabeling noise. Various methods have been proposed to address the noise in distant annotations. AutoNER (Shang et al., 2018) design a new tagging scheme that classifies two adjacent tokens to be tied, broken, or unknown. Token pairs labeled unknown are not considered in loss computation for denoising. Negative sampling methods (Li et al., 2021, 2022) sample O tokens for training to mitigate the incomplete labeling noise. Positive-unlabeled learning (PU-learning) (Peng et al., 2019; Zhou et al., 2022) treats tokens labeled with O class as unlabeled samples and optimizes with an unbiasedly estimated empirical risk. Self-learning-based methods (Liang et al., 2020; Zhang et al., 2021c; Meng et al., 2021) train a teacher model with distant annotations and utilize the soft labels generated by the teacher to train a new student. Other studies also adopt causal intervention (Zhang et al., 2021a) or hyper-geometric distribution (Zhang et al., 2021b) for denoising. In our MProto, we propose a novel denoised optimal transport algorithm for the DSNER task. Experiments show that the denoised optimal transport can significantly mitigate the noise of incomplete labeling.\nPrototypical Network. Our work is also related to prototypical networks (Snell et al., 2017). Prototypical networks learn prototype for each class and make predictions based on the similarity between samples and prototypes. It is widely used in many tasks such as relation extraction (Ding et al., 2021) and the few-shot named entity recognition (FS-NER) task (Fritzler et al., 2019; Ma et al., 2022). Previous prototypical networks for the NER task are mostly single-prototype networks, which do not consider the semantic difference within the same entity type. Tong et al. (2021) try to incorporate multiple prototypes for O class in the FS-NER task. However, they do not consider the intra-class variance in entity classes and their clustering strategy will introduce huge computational complexity when training data is large. In our work, we utilize multiple prototypes to represent each entity type and solve the token-prototype assignment efficiently with the sinkhorn-knopp algorithm. Experiments confirm that our MProto can successfully describe the intra-class variance."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we present MProto, a prototype-based network for the DS-NER task. In MProto, each category is represented with multiple prototypes to\nmodel the intra-class variance. We consider the token-prototype assignment problem as an optimal transport problem and we apply the sinkhornknopp algorithm to solve the OT problem. Besides, to mitigate the noise of incomplete labeling, we propose a novel denoised optimal transport algorithm for O tokens. Experiments show that our MProto has achieved SOTA performance on various DS-NER benchmarks. Visualizations and detailed analysis have confirmed that our MProto can successfully characterize the intra-class variance, and the denoised optimal transport can mitigate the noise of incomplete labeling.\nLimitations\nThe intra-class variance is actually a common phenomenon in the NER task. Currently, we only try to utilize the MProto on the distantly supervised NER task. Applying MProto to the few-shot NER task and the fully supervised NER task can be further investigated."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by the Fundamental Research Funds for the Central Universities (No. 226- 2023-00060), National Natural Science Foundation of China (No. 62376245), Key Research and Development Program of Zhejiang Province (No. 2023C01152), National Key Research and Development Project of China (No. 2018AAA0101900), Joint Project DH-2022ZY0013 from Donghai Lab, and MOE Engineering Research Center of Digital Library."
        },
        {
            "heading": "A Sinkhorn-Knopp Algorithm",
            "text": "We apply the sinkhorn-knopp algorithm (Cuturi, 2013) to solve the optimal transport and denoised optimal transport elaborated in equation 7 and equation 8. In order to apply the sinkhorn-knopp algorithm, we add an entropy regularizer as follows:\n\u03b3\u0302 = argmin \u03b3 \u2211 i \u2211 j \u03b3i,jCi,j + \u03bb r H(\u03b3),\ns.t. \u03b31 = a, \u03b3\u22a41 = b (12)\nwhere \u03bbr is the weight of the regularization and H(\u03b3) = \u2211 i,j \u03b3i,j log(\u03b3i,j) is the entropy of the assignment matrix. We specify the pseudo-code for the sinkhornknopp algorithm in Algorithm 1. Here the \u2298 corresponds to the element-wise division, a and b are vectors that represent the weights of each sample in the source and target distributions, C is the cost matrix, \u03bbr is the regularization weight, and \u03b3 is the assignment matrix.\nThe sinkhorn-knopp algorithm mainly consists of several matrix operations which can be easily accelerated by GPU devices. And we empirically find that the sinkhorn-knopp algorithm can obtain satisfying results in a few iterations in our work. Therefore, applying the sinkhorn-knopp algorithm to solve the token-prototype assignment problem only has a slight impact on the speed of the training.\nAlgorithm 1 Sinkhorn-Knopp Algorithm Require: a,b,C, \u03bbr\nu0 = 1,K = exp(\u2212C/\u03bbr) for i in 1, . . . , n do vi = b\u2298K\u22a4ui\u22121 ui = a\u2298Kvi end for return \u03b3 = diag(un)Kdiag(vn)\nB Implementation Details and Hyperparameters\nWe implement our MProto with Pytorch2 and Huggingface Transformers3. All experiments are carried out on a single RTX-3090 with 24G graphical memory. And the training of the model can be finished in approximately 2 hours.\nWe report the hyperparameters used in different datasets in Table 4. When training, we use an\n2https://pytorch.org/ 3https://huggingface.co/transformers\nAdamW optimizer with weight decay 0.0001 and maximum gradient norm 1.0. The maximum learning is 0.0001, and the learning rate is warmed up linearly in the first 100 steps and decayed linearly afterward. The batch size is set to 32. We train the model for 10 epochs in all experiments.\nFor the sinkhorn-knopp algorithm used in computing the token-prototype assignment, we set the regularization weight \u03bbr = 0.001 and the number of iterations to 100."
        },
        {
            "heading": "C Feature Visualization",
            "text": "We further visualize the token features and the embedding of the prototypes in different datasets and model setups in Figure 6.\nAs shown in Figure 6a and Figure 6c, we can see that even when the model is trained with only a single prototype per entity type (M = 1), token features of the same entity type still tend to form different sub-clusters due to the semantic difference. Therefore, it can be confirmed that the intra-class variance is a common phenomenon in the NER task regardless of the model.\nOur MProto represents each entity type with multiple prototypes. As shown in Figure 4 and Figure 6b, for each sub-cluster of token features, there exists at least one prototype to represent this subcluster. In other words, our MProto can successfully model the intra-class variance of the entity token features. The visualizations show that representing each entity type with multiple prototypes rather than a single prototype is beneficial and can significantly improve the performance in the DSNER task."
        },
        {
            "heading": "D Transport Plan of Unlabeled Entities",
            "text": "To analyze the effectiveness of our denoised optimal transport algorithm, we obtain the transport plan by counting the assignment result of the unlabeled entity tokens in the train set with a checkpoint of MProto at the early stage of training.\nAs shown in Figure 7, most unlabeled entity\n(a) MProto (M = 1) on BC5CDR (Big Dict) (b) MProto (M = 3) on CoNLL03 (Dict) (c) MProto (M = 1) on CoNLL03 (Dict)\nFigure 6: The t-SNE visualization of token features and prototypes. The visualization for MProto (M = 3) on BC5CDR (Big Dict) can be found in Figure 4.\nM BC5CDR (Big Dict) CoNLL03 (Dict)\nPrec. Rec. F1 Prec. Rec. F1\n1 75.74 85.94 80.52 84.39 79.27 81.75 2 77.36 83.43 80.28 83.42 79.27 81.29 3 77.53 85.84 81.47 84.27 80.79 82.49 4 76.78 84.71 80.55 84.53 80.10 82.25 5 73.52 84.14 78.47 82.84 77.96 80.32 6 76.09 85.49 80.52 84.53 79.55 81.97\nTable 5: Experiment with the different number of prototypes.\ntokens are assigned to prototypes of their actual classes, and only a few tokens are mistakenly assigned to the O prototypes. With this observation, we can confirm that before the model overfits the label noise, unlabeled entity tokens tend to be similar to prototypes of their actual classes in the feature space. Therefore, they tend to be assigned to prototypes of their actual classes in our similaritybased token-prototype assignment. In our denoised optimal transport, O tokens assigned to entity prototypes are considered as label noise, and only O tokens assigned to O prototypes are considered as true negative samples. In this way, we can discriminate unlabeled entity tokens from clean samples. These unlabeled entity tokens are ignored in loss computation so as not to misguide the training of the model. And we can conclude that the denoised optimal transport can effectively mitigate the incomplete labeling noise in the DS-NER task.\n(a) CoNLL03 (Dict) (b) BC5CDR (Big Dict)\nFigure 7: The transport plan for unlabeled entities. The y-axis represents the actual entity type of the unlabeled entity tokens. The x-axis represents the class of the prototypes that are assigned to the unlabeled entity tokens."
        },
        {
            "heading": "E Experiment on Different Number of Prototypes",
            "text": "We try different M (the number of prototypes per type) on BC5CDR (Big Dict) and CoNLL03 (Dict) datasets. The results are reported in Table 5. It shows that representing each entity type with 3 prototypes is the optimal choice on both BC5CDR (Big Dict) and CoNLL03 (Dict) datasets. Intuitively, there exists an ideal M for each entity type based on the semantic complexity of that category. And a too-large M or a too-small M will hurt the performance of MProto. Besides, setting a toolarge M will reduce the number of tokens assigned to each prototype. In this case, there might be no enough token features for learning representative prototypes, which leads to underfitting."
        },
        {
            "heading": "F Similarity Curve of Different Entity Types",
            "text": "To better analyze the effectiveness of denoised optimal transport algorithm, we additionally report the similarity curve of each entity class in Figure 8. The similarity is obtained by Equation 11."
        }
    ],
    "title": "MProto: Multi-Prototype Network with Denoised Optimal Transport for Distantly Supervised Named Entity Recognition",
    "year": 2023
}