{
    "abstractText": "Today\u2019s language models can be remarkably intelligent yet still produce text that contains trivial commonsense errors. Therefore, we seek a retrospective verification approach that can reflect on the commonsense plausibility of the machine text, and introduce VERA, a general-purpose model that learns to estimate the commonsense plausibility of declarative statements. To support diverse commonsense domains, VERA is trained on \u223c7M commonsense statements that are automatically converted from 19 QA datasets and two commonsense knowledge bases, and using a combination of three training objectives. When applied to solving commonsense problems in the verification format, VERA substantially outperforms existing models that can be repurposed for commonsense verification, even including GPT-3.5/ChatGPT/GPT-4, and it further exhibits generalization capabilities to unseen tasks and provides well-calibrated outputs. We find that VERA excels at filtering machinegenerated commonsense knowledge and is useful in detecting erroneous commonsense statements generated by models like ChatGPT in real-world settings.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiacheng Liu"
        },
        {
            "affiliations": [],
            "name": "Wenya Wang"
        },
        {
            "affiliations": [],
            "name": "Dianzhuo Wang"
        },
        {
            "affiliations": [],
            "name": "Noah A. Smith"
        },
        {
            "affiliations": [],
            "name": "Yejin Choi"
        },
        {
            "affiliations": [],
            "name": "Hannaneh Hajishirzi"
        }
    ],
    "id": "SP:c091333543bda1f6e382a9eb87f75b2dd8dddbdd",
    "references": [
        {
            "authors": [
                "Stephane T Aroca-Ouellette",
                "Cory Paik",
                "Alessandro Roncone",
                "Katharina Kann."
            ],
            "title": "Prost: Physical reasoning about objects through space and time",
            "venue": "Findings.",
            "year": 2021
        },
        {
            "authors": [
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Chaitanya Malaviya",
                "Keisuke Sakaguchi",
                "Ari Holtzman",
                "Hannah Rashkin",
                "Doug Downey",
                "Scott Yih",
                "Yejin Choi."
            ],
            "title": "Abductive commonsense reasoning",
            "venue": "ArXiv, abs/1908.05739.",
            "year": 2019
        },
        {
            "authors": [
                "Chandra Bhagavatula",
                "Jena D. Hwang",
                "Doug Downey",
                "Ronan Le Bras",
                "Ximing Lu",
                "Keisuke Sakaguchi",
                "Swabha Swayamdipta",
                "Peter West",
                "Yejin Choi."
            ],
            "title": "I2d2: Inductive knowledge distillation with neurologic and self-imitation",
            "venue": "ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Sumithra Bhakthavatsalam",
                "Chloe Anastasiades",
                "Peter Clark."
            ],
            "title": "Genericskb: A knowledge base of generic statements",
            "venue": "ArXiv, abs/2005.00660.",
            "year": 2020
        },
        {
            "authors": [
                "Yonatan Bisk",
                "Rowan Zellers",
                "Ronan Le Bras",
                "Jianfeng Gao",
                "Yejin Choi."
            ],
            "title": "Piqa: Reasoning about physical commonsense in natural language",
            "venue": "ArXiv, abs/1911.11641.",
            "year": 2019
        },
        {
            "authors": [
                "Ali Borji."
            ],
            "title": "A categorical archive of chatgpt failures",
            "venue": "ArXiv, abs/2302.03494.",
            "year": 2023
        },
        {
            "authors": [
                "Kaj Bostrom",
                "Zayne Sprague",
                "Swarat Chaudhuri",
                "Greg Durrett."
            ],
            "title": "Natural language deduction through search over statement compositions",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2022
        },
        {
            "authors": [
                "Jifan Chen",
                "Eunsol Choi",
                "Greg Durrett"
            ],
            "title": "Can nli models verify qa systems",
            "venue": "predictions? ArXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Chen",
                "Mike D\u2019Arcy",
                "Alisa Liu",
                "Jared Fernandez",
                "Doug Downey"
            ],
            "title": "Codah: An adversarially authored question-answer dataset for common sense. arXiv preprint arXiv:1904.04365",
            "year": 2019
        },
        {
            "authors": [
                "Peter Clark",
                "Isaac Cowhey",
                "Oren Etzioni",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Carissa Schoenick",
                "Oyvind Tafjord."
            ],
            "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
            "venue": "ArXiv, abs/1803.05457.",
            "year": 2018
        },
        {
            "authors": [
                "Ernest Davis."
            ],
            "title": "Benchmarks for automated commonsense reasoning: A survey",
            "venue": "ArXiv, abs/2302.04752.",
            "year": 2023
        },
        {
            "authors": [
                "Mor Geva",
                "Daniel Khashabi",
                "Elad Segal",
                "Tushar Khot",
                "Dan Roth",
                "Jonathan Berant"
            ],
            "title": "Did aristotle use a laptop? a question answering benchmark with",
            "year": 2021
        },
        {
            "authors": [
                "Andrew S. Gordon",
                "Zornitsa Kozareva",
                "Melissa Roemmele."
            ],
            "title": "Semeval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
            "venue": "International Workshop on Semantic Evaluation.",
            "year": 2011
        },
        {
            "authors": [
                "Sylvain Gugger",
                "Lysandre Debut",
                "Thomas Wolf",
                "Philipp Schmid",
                "Zachary Mueller",
                "Sourab Mangrulkar."
            ],
            "title": "Accelerate: Training and inference at scale made simple, efficient and adaptable",
            "venue": "https:// github.com/huggingface/accelerate.",
            "year": 2022
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q. Weinberger."
            ],
            "title": "On calibration of modern neural networks",
            "venue": "International Conference on Machine Learning.",
            "year": 2017
        },
        {
            "authors": [
                "Jena D. Hwang",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Jeff Da",
                "Keisuke Sakaguchi",
                "Antoine Bosselut",
                "Yejin Choi."
            ],
            "title": "Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs",
            "venue": "AAAI Conference on Artificial Intelligence.",
            "year": 2020
        },
        {
            "authors": [
                "Albert Qiaochu Jiang",
                "Sean Welleck",
                "Jin Peng Zhou",
                "Wenda Li",
                "Jiacheng Liu",
                "Mateja Jamnik",
                "Timoth\u00e9e Lacroix",
                "Yuhuai Wu",
                "Guillaume Lample."
            ],
            "title": "Draft, sketch, and prove: Guiding formal theorem provers with informal proofs",
            "venue": "ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Jaehun Jung",
                "Lianhui Qin",
                "Sean Welleck",
                "Faeze Brahman",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "Maieutic prompting: Logically consistent reasoning with recursive explanations",
            "venue": "Conference on Empirical Methods in Natural Lan-",
            "year": 2022
        },
        {
            "authors": [
                "Benjamin Mann",
                "Sam McCandlish",
                "Christopher Olah",
                "Jared Kaplan."
            ],
            "title": "Language models (mostly) know what they know",
            "venue": "ArXiv, abs/2207.05221.",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Yeganeh Kordi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Unifiedqa-v2: Stronger generalization via broader cross-format training",
            "venue": "ArXiv, abs/2202.12359.",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Sewon Min",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Oyvind Tafjord",
                "Peter Clark",
                "Hannaneh Hajishirzi."
            ],
            "title": "Unifiedqa: Crossing format boundaries with a single qa system",
            "venue": "Findings.",
            "year": 2020
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "Chen Wang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan."
            ],
            "title": "Supervised contrastive learning",
            "venue": "Advances in Neural Information Processing Systems, volume 33.",
            "year": 2020
        },
        {
            "authors": [
                "Tushar Khot",
                "Peter Clark",
                "Michal Guerquin",
                "Peter Alexander Jansen",
                "Ashish Sabharwal."
            ],
            "title": "Qasc: A dataset for question answering via sentence composition",
            "venue": "ArXiv, abs/1910.11473.",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "CoRR, abs/1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Hector J. Levesque",
                "Ernest Davis",
                "L. Morgenstern."
            ],
            "title": "The winograd schema challenge",
            "venue": "International Conference on Principles of Knowledge Representation and Reasoning.",
            "year": 2011
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Seyeon Lee",
                "Rahul Khanna",
                "Xiang Ren."
            ],
            "title": "Birds have four legs?! numersense: Probing numerical commonsense knowledge of pretrained language models",
            "venue": "ArXiv, abs/2005.00683.",
            "year": 2020
        },
        {
            "authors": [
                "Stephanie Lin",
                "Jacob Hilton",
                "Owain Evans."
            ],
            "title": "TruthfulQA: Measuring how models mimic human falsehoods",
            "venue": "ACL, pages 3214\u20133252.",
            "year": 2022
        },
        {
            "authors": [
                "Alisa Liu",
                "Swabha Swayamdipta",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Wanli: Worker and ai collaboration for natural language inference dataset creation",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Liu",
                "Skyler Hallinan",
                "Ximing Lu",
                "Pengfei He",
                "Sean Welleck",
                "Hannaneh Hajishirzi",
                "Yejin Choi."
            ],
            "title": "Rainier: Reinforced knowledge introspector for commonsense question answering",
            "venue": "Conference on Empirical Methods in Natural Language Process-",
            "year": 2022
        },
        {
            "authors": [
                "Jiacheng Liu",
                "Alisa Liu",
                "Ximing Lu",
                "Sean Welleck",
                "Peter West",
                "Ronan Le Bras",
                "Yejin Choi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Generated knowledge prompting for commonsense reasoning",
            "venue": "ArXiv, abs/2110.08387.",
            "year": 2021
        },
        {
            "authors": [
                "Xiao Liu",
                "Da Yin",
                "Yansong Feng",
                "Dongyan Zhao."
            ],
            "title": "Things not written in text: Exploring spatial commonsense from visual signals",
            "venue": "ArXiv, abs/2203.08075.",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "ArXiv, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Nicholas Lourie",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi."
            ],
            "title": "Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark",
            "venue": "AAAI Conference on Artificial Intelligence.",
            "year": 2021
        },
        {
            "authors": [
                "Gary Marcus",
                "Ernest Davis."
            ],
            "title": "Chatgpt/llm errors (public)",
            "venue": "https://docs.google. com/spreadsheets/d/1kDSERnROv5FgHbVN8z_ bXH9gak2IXRtoqz0nwhrviCw/edit.",
            "year": 2023
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Peter Clark",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2018
        },
        {
            "authors": [
                "N. Mostafazadeh",
                "Nathanael Chambers",
                "Xiaodong He",
                "Devi Parikh",
                "Dhruv Batra",
                "Lucy Vanderwende",
                "Pushmeet Kohli",
                "James F. Allen."
            ],
            "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
            "venue": "North American Chapter of the",
            "year": 2016
        },
        {
            "authors": [
                "Mahdi Pakdaman Naeini",
                "Gregory F. Cooper",
                "Milos Hauskrecht."
            ],
            "title": "Obtaining well calibrated probabilities using bayesian binning",
            "venue": "Proceedings of the ... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence, 2015:2901\u2013",
            "year": 2015
        },
        {
            "authors": [
                "Yasumasa Onoe",
                "Michael J.Q. Zhang",
                "Eunsol Choi",
                "Greg Durrett."
            ],
            "title": "Creak: A dataset for commonsense reasoning over entity knowledge",
            "venue": "ArXiv, abs/2109.01653.",
            "year": 2021
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "Technical report, OpenAI.",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Keisuke Sakaguchi",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi."
            ],
            "title": "Winogrande: An adversarial winograd schema challenge at scale",
            "venue": "Commun. ACM, 64:99\u2013106.",
            "year": 2019
        },
        {
            "authors": [
                "Maarten Sap",
                "Hannah Rashkin",
                "Derek Chen",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "Social iqa: Commonsense reasoning about social interactions",
            "venue": "ArXiv, abs/1904.09728.",
            "year": 2019
        },
        {
            "authors": [
                "Shikhar Singh",
                "Nuan Wen",
                "Yu Hou",
                "Pegah Alipoormolabashi",
                "Te-Lin Wu",
                "Xuezhe Ma",
                "Nanyun Peng."
            ],
            "title": "Com2sense: A commonsense reasoning benchmark with complementary sentences",
            "venue": "Findings.",
            "year": 2021
        },
        {
            "authors": [
                "Zayne Sprague",
                "Kaj Bostrom",
                "Swarat Chaudhuri",
                "Greg Durrett."
            ],
            "title": "Natural language deduction with incomplete information",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2022
        },
        {
            "authors": [
                "Oyvind Tafjord",
                "Peter Clark."
            ],
            "title": "Generalpurpose question-answering with macaw",
            "venue": "ArXiv, abs/2109.02593.",
            "year": 2021
        },
        {
            "authors": [
                "Oyvind Tafjord",
                "Peter Clark",
                "Matt Gardner",
                "Wen tau Yih",
                "Ashish Sabharwal."
            ],
            "title": "Quarel: A dataset and models for answering questions about qualitative relationships",
            "venue": "ArXiv, abs/1811.08048.",
            "year": 2018
        },
        {
            "authors": [
                "Oyvind Tafjord",
                "Bhavana Dalvi",
                "Peter Clark."
            ],
            "title": "Entailer: Answering questions with faithful and truthful chains of reasoning",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2022
        },
        {
            "authors": [
                "Oyvind Tafjord",
                "Matt Gardner",
                "Kevin Lin",
                "Peter Clark."
            ],
            "title": "Quartz: An open-domain dataset of qualitative relationship questions",
            "venue": "ArXiv, abs/1909.03553.",
            "year": 2019
        },
        {
            "authors": [
                "Alon Talmor",
                "Jonathan Herzig",
                "Nicholas Lourie",
                "Jonathan Berant."
            ],
            "title": "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
            "venue": "ArXiv, abs/1811.00937.",
            "year": 2019
        },
        {
            "authors": [
                "Alon Talmor",
                "Ori Yoran",
                "Ronan Le Bras",
                "Chandrasekhar Bhagavatula",
                "Yoav Goldberg",
                "Yejin Choi",
                "Jonathan Berant"
            ],
            "title": "Commonsenseqa 2.0: Exposing the limits of ai through gamification",
            "venue": "ArXiv, abs/2201.05320",
            "year": 2021
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "Fever: a large-scale dataset for fact extraction and verification",
            "venue": "ArXiv, abs/1803.05355.",
            "year": 2018
        },
        {
            "authors": [
                "Giuseppe Venuto."
            ],
            "title": "chatgpt-failures",
            "venue": "https:// github.com/giuven95/chatgpt-failures.",
            "year": 2023
        },
        {
            "authors": [
                "David Wadden",
                "Kyle Lo",
                "Lucy Lu Wang",
                "Shanchuan Lin",
                "Madeleine van Zuylen",
                "Arman Cohan",
                "Hannaneh Hajishirzi."
            ],
            "title": "Fact or fiction: Verifying scientific claims",
            "venue": "ArXiv, abs/2004.14974.",
            "year": 2020
        },
        {
            "authors": [
                "Cunxiang Wang",
                "Shuailong Liang",
                "Yili Jin",
                "Yilong Wang",
                "Xiaodan Zhu",
                "Yue Zhang."
            ],
            "title": "Semeval2020 task 4: Commonsense validation and explanation",
            "venue": "International Workshop on Semantic Evaluation.",
            "year": 2020
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Huai hsin Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "ArXiv, abs/2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Johannes Welbl",
                "Nelson F. Liu",
                "Matt Gardner."
            ],
            "title": "Crowdsourcing multiple choice science questions",
            "venue": "ArXiv, abs/1707.06209.",
            "year": 2017
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz",
                "Jamie Brew."
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "venue": "ArXiv,",
            "year": 2019
        },
        {
            "authors": [
                "Kaiyu Yang",
                "Jia Deng",
                "Danqi Chen."
            ],
            "title": "Generating natural language proofs with verifier-guided search",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2022
        },
        {
            "authors": [
                "Rowan Zellers",
                "Yonatan Bisk",
                "Roy Schwartz",
                "Yejin Choi."
            ],
            "title": "Swag: A large-scale adversarial dataset for grounded commonsense inference",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2018
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ari Holtzman",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Yejin Choi"
            ],
            "title": "Hellaswag: Can a machine really finish your sentence? In Annual Meeting of the Association for Computational Linguistics",
            "year": 2019
        },
        {
            "authors": [
                "Sheng Zhang",
                "Rachel Rudinger",
                "Kevin Duh",
                "Benjamin Van Durme."
            ],
            "title": "Ordinal common-sense inference",
            "venue": "Transactions of the Association for Computational Linguistics, 5:379\u2013395.",
            "year": 2017
        },
        {
            "authors": [
                "West"
            ],
            "title": "2021), we replace the name placeholders with random person names, and convert into natural language statements using templates",
            "year": 2020
        },
        {
            "authors": [
                "ECE. C"
            ],
            "title": "Details on Baseline Models SKD Critic. West et al. (2021) trained a critic model that filters incorrect commonsense knowledge generated by their symbolic knowledge distil",
            "year": 2021
        },
        {
            "authors": [
                "Aroca-Ouellette"
            ],
            "title": "https://huggingface.co/datasets/corypaik/prost yes SpatialCS",
            "venue": "Spatial Commonsense Liu et al",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "We introduce VERA, a general-purpose commonsense statement verification model. This model is designed to estimate the plausibility of declarative, natural language statements based on commonsense knowledge.\nWe build VERA in response to the absence of good detectors of commonsense errors in text generated by language models (LMs). LMs have been advancing rapidly and have demonstrated remarkable success in various tasks, including question answering, natural language inference, sequence classification, and text generation. Yet these models still make simple commonsense mistakes. As shown in Figure 1, as of February 23, 2023, ChatGPT (OpenAI, 2022a) reportedly output the text\n\u201csince the density of a marble is much less than the density of mercury, the marble would sink to the bottom of the bowl if placed in it\u201d, which is clearly incorrect. This kind of failure raises concerns about the reliability and trustworthiness of these models (Lin et al., 2022).\nVERA estimates a plausibility score for a commonsense statement based on its commonsense knowledge about the world. It contrasts with fact verification methods (Thorne et al., 2018; Wadden et al., 2020), which verify the correctness of claims based on evidence from a text corpus. VERA enables plausibility estimation where direct evidence is often not retrievable from some corpus, and usually some implicit, fuzzy reasoning is needed. It operates solely with the commonsense knowledge stored in its model parameters, and does not have a retrieval component.\nVERA is built on top of T5 (Raffel et al., 2020), a generic pretrained LM, by finetuning on a vast collection of correct and incorrect commonsense statements sourced from knowledge bases (KBs) and question answering (QA) datasets. The 21 data sources (Table 5, appendix) amount to \u223c7M statements encompassing a wide spectrum of domains, including general, scientific, physical, and social commonsense, as well as quantitative (reasoning about numbers) and qualitative (reasoning about\nqualitative relationships such as smaller) commonsense. We propose a novel two-stage training process that takes into account the scale and quality of data from different sources. In addition to the standard multiple-choice binary classification objectives, we adopt a supervised contrastive loss (Khosla et al., 2020) to magnify the distinction between similar statements with different correctness labels. Furthermore, we propose an automatic way of augmenting the training data by eliciting LMs to generate incorrect answers to commonsense questions and empirically find it helps generalization.\nWe evaluate VERA in the following applications:\n\u2022 Excelling in commonsense problems over GPT-series when repurposed for verification (\u00a75.1). VERA can be applied to solve multiple-choice and boolean commonsense problems when expressed in the verification format, by scoring and ranking candidate hypotheses. It substantially outperforms existing models repurposed for commonsense verification (including GPT-3.5, ChatGPT and GPT4), improving upon the best existing baseline, Flan-T5, with absolute improvement of 6% on seen benchmarks and 4% on unseen ones. \u2022 Filtering LM-generated commonsense knowledge (\u00a75.2). VERA can filter noisy commonsense knowledge statements generated by other LMs, improving the effectiveness of LM-generated knowledge in downstream knowledge-augmented inferences. VERA is well-calibrated, enabling filtering at customized thresholds. \u2022 Detecting commonsense errors in ChatGPT outputs (\u00a75.3). Through a preliminary analysis, we find that VERA can identify commonsense errors made by ChatGPT in-the-wild, with a precision of 91% and a recall of 74%. An example of VERA in action is shown in Figure 1.\nWe hope VERA can be a useful tool for improving the commonsense correctness of existing generative LM output and inspire more effort toward general-purpose and robust verification methods."
        },
        {
            "heading": "2 Problem Definition and Scope",
            "text": "Our goal is to build a model that can estimate the plausibility of any given commonsense statement. The model takes as input a statement that (1) is expressed in natural language; (2) is declarative,\nas opposed to interrogative questions; (3) is selfcontained without requiring additional context to comprehend; (4) has an objective, binary correctness label; and (5) in principle can be labeled using widely-held commonsense knowledge about the world. Encyclopedic knowledge (e.g., Ljubljana is the capital of Slovenia.) is out of scope. Moving forward, unless explicitly noted, we use commonsense statement to refer to statements within the above scope. Though somewhat strict, this scope covers a broad range of potential applications.\nFor an input commonsense statement x, the model should output a real-valued score s \u2208 [0, 1] that represents its estimated plausibility of x. While the gold correctness label is binary, we let the model output a score to reflect its confidence. A score of 1.0 means that it is completely confident that x is correct, and a score of 0.0 means it is completely confident that x is incorrect. When predicting correctness label from the score, we use 0.5 as the threshold."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we describe the whole pipeline to build VERA. We start from curating large-scale training data including both correct and incorrect statements from diverse commonsense tasks (\u00a73.1). Next, we learn a scoring model that takes a statement and returns a continuous score by finetuning a LM via 3 training objectives (\u00a73.2). An additional post hoc calibration strategy is applied to make the output scores well-calibrated (\u00a73.3)."
        },
        {
            "heading": "3.1 Data Construction",
            "text": "Labeled commonsense statements usually do not appear in text in the wild, while some commonsense question answering (QA) datasets and commonsense knowledge bases (KBs) are good sources for this kind of statements. We collect correct and incorrect commonsense statements from the above two types of data source. Table 1 shows some examples on how these statements can be converted from QA problems and KB entries. In total, we obtain \u223c7M statements (for training) from 19 QA datasets (\u00a73.1.1) and two KBs (\u00a73.1.2) that encompass a wide spectrum of commonsense domains. Table 5 (appendix) lists these datasets with statistics. All datasets we use are publicly available."
        },
        {
            "heading": "3.1.1 From Commonsense QA Datasets",
            "text": "Numerous commonsense reasoning datasets have been published in recent years (Davis, 2023), and\nmany of them are in the format of multiple-choice QA (selecting the correct answer out of a set of choices) or boolean (yes/no) QA. These can be easily converted to correct and incorrect commonsense statements. From multiple-choice QA problems, we combine the question and each answer choice to form declarative statements, which are correct when using the correct answer, and incorrect otherwise. From boolean QA problems, we convert the question into a declarative statement, and keep the original label as the correctness label. Concrete examples can be found in Table 1.\nStatement groups. We refer to statements originating from the same problem as a statement group. Note that statement groups originating from multiple-choice problems contain at least two statements, of which one and only one is correct; statement groups originating from boolean problems contain only one statement, and it can be either correct or incorrect. We do conversion to declarative statements automatically. From QA datasets, we create declarative statements from QA problems using the following method:\n\u2022 If the problem contains a question, we convert the question and choice into a declarative statement using the question conversion model created by Chen et al. (2021).\n\u2022 If the question is cloze-style, we replace the blank with the choice.\n\u2022 If the question is an incomplete sentence and\nthe choice is a continuation to it, we concatenate the question and the choice.\n\u2022 If there is no question and the problem only asks to choose between some choices, we use the choice as the declarative statement.\n\u2022 For boolean problems, we always use yes as the choice and create a single declarative statement for each problem. We use the original label as the correctness label of this statement.\nIn total, 19 commonsense QA datasets contribute \u223c200k statement groups and \u223c400k statements to the training set of VERA.\nLM-augmented falsehoods. Existing commonsense QA datasets are mostly manually constructed or assembled from standard school exams. A model trained on these datasets might overfit specific annotation patterns from humans which may limit generalization. Therefore, we augment QA problems with LM-generated answers and construct additional incorrect statements. Specifically, for a multiple-choice question, we use a small LM to sample 50 possible answers to the question, and select the 3 least probable answers with generation probability less than 0.15 (making these unlikely to be correct answers). This threshold is chosen based on manual inspection over a small portion of examples. We observe generated answers with probability larger than 0.15 are more likely to be plausible. We create LM-augmented falsehoods for the training set of 9 commonsense QA datasets, as noted in Table 5 (appendix)."
        },
        {
            "heading": "3.1.2 From Commonsense KBs",
            "text": "Commonsense KBs (e.g., Atomic2020 in Hwang et al. (2020), and GenericsKB in Bhakthavatsalam et al. (2020)) contain a large number of correct commonsense statements. To create incorrect statements, we automatically perturb KB entries by replacing the subject with three random subjects that appear in the KB. Table 1 shows how to convert an entry in GenericsKB to a statement group containing four statements, three of which are augmented via perturbations. The perturbed statements are relatively easy to identify and may contain false negatives. As noted in \u00a73.2.4, we use these KBconstructed statements in a separate training stage that precedes training with QA-constructed statements. In total, two commonsense KBs contribute \u223c1.6M statement groups and \u223c6M statements to the training set of VERA."
        },
        {
            "heading": "3.2 Model Training",
            "text": ""
        },
        {
            "heading": "3.2.1 Model Architecture",
            "text": "Given a statement x, VERA outputs a real-valued score s \u2208 [0, 1]. As we will use a transformerbased LM as the backbone of VERA, we first extract the input representation h by selecting the last hidden state corresponding to the EOS input token. We choose EOS because it is capable to encode the entire input in both bidirectional encoder models (e.g., T5\u2019s encoder) and left-to-right decoder models (e.g., LLaMA). Then a linear layer projects h to a scalar logit z, followed by a sigmoid function \u03c3(\u00b7) that transforms the logit into a score s. Formally,\nh = fLM(x), z = flinear(h), s = \u03c3(z).\nFor brevity, we use h(x), z(x) and s(x) to refer to the representation, logit and score of an arbitrary input x."
        },
        {
            "heading": "3.2.2 Batching",
            "text": "The data we construct consists of statements belonging to different statement groups. For reasons we will describe in \u00a73.2.3, we put all statements belonging to the same statement group into the same batch. Each batch may contain multiple complete statement groups. We denote by BG the number of statement groups and BS the number of statements in total within a single batch. We denote the statement groups as {Xj}BGj=1, and the statements as {xi}BSi=1. {Xj} BG j=1 is a partition of {xi}BSi=1. yi \u2208 {0, 1} is the correctness label of xi."
        },
        {
            "heading": "3.2.3 Training Objectives",
            "text": "The model is trained with a linear combination of three losses: a binary classification loss, a multiclass loss, and a supervised contrastive loss, L = \u03b1Lbin + \u03b2Lmc + \u03b3Lctr, which we describe below.\nBinary classification loss. Naively, commonsense statement verification can be viewed as a binary classification task. Under this setting, the loss is\nLbin = \u2212yi log s(xi)\u2212 (1\u2212 yi) log(1\u2212 s(xi)).\nMulti-class loss. We expect the model to be robust against nuances in commonsense statements. Ideally, the model should be able to recognize opposite correctness labels for a group of seemingly similar statements in surface forms, such as statements created from different choices of the same question, or perturbed from the same piece of knowledge in\na KB. To achieve this goal, we treat each statement group as a multi-class classification problem, maximizing the log-likelihood of the single correct statement in the statement group after passing the logits through a softmax. Formally,\nLmc = \u2212 log exp z(xj\u2217)\u2211Cj c=1 exp z(xjc) ,\nwhere xj\u2217 is the correct statement in Xj . Note that the multi-class loss is not applicable to statement groups with only one statement (i.e., statement groups from boolean QA datasets). We empirically find that the multi-class loss indeed improves generalization towards unseen multiple-choice QA datasets as indicated in Figure 3 (appendix).\nSupervised contrastive loss. It has been shown (Khosla et al., 2020) that supervised contrastive learning helps to improve model robustness and generalization against input variations. In light of this, we further adopt supervised contrastive learning on top of the input representations h. We show in Figure 3 (appendix) that the contrastive loss indeed improve generalization to unseen datasets. For each anchor statement xi in a batch, the contrastive loss aims to maximize the similarity between xi and each other statement xp that has the same correctness label as xi (i.e., positive example). At the same time, we push apart xi and other statements xn that has opposite correctness label as xi (i.e., negative example). The supervised contrastive loss is\nLctr = \u2212 log \u2211\nk\u2208P(i) exp[cos(h(xi),h(xk))/\u03c4 ]\u2211 k\u2208P(i)\u222aN (i) exp[cos(h(xi),h(xk))/\u03c4 ] ,\nwhere \u03c4 is a temperature hyperparameter, cos(\u00b7, \u00b7) refers to cosine similarity, P(i) \u2286 [BS ] is the index set of statements that are positive examples for xi, and N (i) \u2286 [BS ] is the index set of statements that are negative examples for xi. Formally,\nP(i) = { k | 1 \u2264 k \u2264 BS , yk = yi, k \u0338= i } ,\nN (i) = { k | 1 \u2264 k \u2264 BS , yk \u0338= yi } ."
        },
        {
            "heading": "3.2.4 Two-Stage Training",
            "text": "Since data sourced from KBs are larger in scale but more noisy than data sourced from QA datasets, we take a two-stage training approach. In training stage A, we start from a pre-trained LM and train with data sourced from KBs. In training stage B,\nwe start from the model obtained in stage A and train with data sourced from QA datasets. During experiments we found that this setting is better than single-stage training with either data source or a mixture of the two."
        },
        {
            "heading": "3.3 Inference and Calibration",
            "text": "An ideal plausibility estimation model should be calibrated, that is, its confidence in its predictions should be approximately equal to the actual frequency of correctness. During early experiments, we found that VERA tends to be overconfident. Therefore, we apply a post hoc calibration on VERA\u2019s output. Following the temperature scaling method introduced in Guo et al. (2017), during inference we divide the model-predicted logit by a temperature T before computing the score, that is,\nh = fLM(x), z = flinear(h), z\u0303 = z/T, s = \u03c3(z\u0303).\nNote that no temperature scaling is applied during model training.\nWith predictions on a validation set D = {(xi, yi)}Di=1, we estimate T that gives the minimal expected calibration error (ECE) (Naeini et al., 2015) on this validation set. Equation 1 in \u00a7C.1 shows how ECE is computed. In practice, we use the combined development sets of the seen datasets (\u00a74.2) to estimate T , and the optimal T becomes a parameter of VERA. Note that temperature scaling does not change the relative ordering of prediction scores, and thus the other performance metrics (e.g., accuracy) are not affected (see detailed explanation in \u00a7B.2)."
        },
        {
            "heading": "4 Experimental Setup",
            "text": "In this section, we provide more details of model training, the evaluation protocol and metrics, and describe the baseline models we benchmark."
        },
        {
            "heading": "4.1 Training Details",
            "text": "Datasets. For training stage A, we use the \u223c1.6M statement groups (\u223c6M statements) sourced from two commonsense KBs; for training stage B, we use the \u223c200k statement groups (\u223c400k statements) sourced from 19 commonsense QA datasets. For each training stage, we mix the training sets of all datasets together, without any re-weighting.\nModels. We use two types of pretrained LMs as the backbone of VERA: (1) the encoder of T5 (Raffel et al., 2020), which is a bidirectional encoder model; (2) LLaMA (Touvron et al.,\n2023), which is a left-to-right decoder model. For the T5 encoder, we start from the pretrained T5-v1.1-XXL1 whose encoder has about 5B parameters, and refer to the resulting model as VERAT5. (During experiments we found that starting from Flan-T5-XXL2 performs slightly worse than starting from T5-v1.1-XXL.) For LLaMA, we start from the pretrained LLaMA-7B and refer to the resulting model as VERA-LLaMA. As we will see, VERA-T5 has better performance than VERALLaMA, so unless explicitly specified, when we say VERA we mean VERA-T5. See Table 8 (appendix) for the complete hyperparameter settings and \u00a7C for the implementation details."
        },
        {
            "heading": "4.2 Evaluation and Baselines",
            "text": "Evaluation protocol. We divide our evaluation into two parts: (1) Seen benchmarks, whose training set is used for model training. (2) Unseen benchmarks, whose training set is not used for model training. We futher divide up the unseen benchmarks into type 1 and type 2, where in type 1 benchmarks the task is similar to those in the seen benchmarks, while type 2 benchmarks are a bit further away in terms of the nature of the task. Examples of type 2 unseen benchmarks include HellaSwag which is contextualized with event descriptions, and CREAK which involves reasoning among different entities. Depending on the nature of the evaluation benchmark, we use different metrics to evaluate our model\u2019s performance. Unless explicitly said otherwise, we report performance on the development set, where the gold labels are available, and we do not use the development sets of unseen datasets for model selection. The overall metric reported over multiple benchmarks is the unweighted average of the metric over all these benchmarks, which accounts for the differently-sized evaluation sets.\nMetrics. We report accuracy for multiple-choice and balanced boolean benchmarks. For those unbalanced boolean benchmarks (e.g., LM-generated knowledge filtering datasets), we report area under the ROC curve (AUROC) and average precision (AP). To measure how well the model-predicted scores reflect confidence, we measure the ECE (Naeini et al., 2015) on the boolean benchmarks, following Equation 1.\n1https://huggingface.co/google/t5-v1_1-xxl 2https://huggingface.co/google/flan-t5-xxl\nBaseline Models. We compare VERA with the best publicly available models that can be directly used or repurposed for commonsense statement verification. Roughly in increasing order of performance, these models are: SKD Critic (West et al., 2021), I2D2 Critic (Bhagavatula et al., 2022), UnifiedQA-v2 (Khashabi et al., 2022), Entailer (Tafjord et al., 2022), GPT-3.5 (OpenAI, 2022b), ChatGPT (OpenAI, 2022a), GPT-4 (OpenAI, 2023), and Flan-T5 (Chung et al., 2022). See more details in \u00a7C.2."
        },
        {
            "heading": "5 Evaluation Results",
            "text": "In this section, we evaluate the ability of VERA to estimate the plausibility of commonsense statements and compare it with the baseline models. We show the effectiveness of VERA in three scenarios: solving commonsense problems, filtering LM-generated commonsense knowledge, and detecting commonsense errors in ChatGPT outputs."
        },
        {
            "heading": "5.1 Solving Multiple-Choice and Boolean Commonsense Problems",
            "text": "The output plausibility scores from VERA can be used for solving multiple-choice and boolean commonsense problems. We first convert the problems into the statement group format (\u00a73.1). For multiple-choice problems, we choose the statement with the highest score in the statement group. For boolean problems, we use s = 0.5 as the threshold\nto predict correctness labels of statements. Table 2 reports the results when VERA is applied to solve commonsense problems. See Figure 5 and Table 9, 10, 11 (appendix) for full results including AUROC and AP. On seen benchmarks (16 multiplechoice and one boolean), VERA outperforms the best baseline, Flan-T5, by 6% on (absolute) accuracy and 9% on AUROC. VERA beats Flan-T5 by 4% accuracy and 5% AUROC on type 1 unseen benchmarks (four multiple-choice and one boolean), and by 4% accuracy and 6% AUROC on type 2 unseen benchmarks (five multiple-choice and two boolean), demonstrating good generalization. VERA-T5 has better performance than VERALLaMA across the board, which may be due to its bidirectional connectivity. Aside from performance, VERA also has good calibration, with ECE no higher than 3% on seen and unseen benchmarks. The post hoc calibration method improves calibration across all three parts.\nTypically we may need to choose a threshold for binary classification in boolean datasets. However, we notice that a zero logit (z = 0) is generally close to the optimal decision threshold between correct and incorrect commonsense statements. Therefore we do not estimate a model-specific threshold, and simply use the default threshold: z = 0, or equivalently, s = 0.5."
        },
        {
            "heading": "5.2 Filtering LM-generated Commonsense Knowledge",
            "text": "Figure 2 reports the results when VERA is applied to filter LM-generated commonsense knowledge. On the two seen benchmarks, SKD_anno and I2D2_anno, VERA is a better knowledge filter than all baseline models, in terms of both AUROC and AP. In particular, on I2D2_anno it outperforms the I2D2 critic model by 2% AUROC, which is specifically trained on the I2D2_anno dataset and does not generalize well to other benchmarks. On the unseen benchmark, Rainier_anno, VERA is also comparable with the best baselines like Flan-T5 and GPT-3.5. As for calibration, the ECE is no higher than 8% on all three benchmarks.\nWe find that filtering commonsense knowledge using VERA can greatly improve the performance of knowledge-augmented reasoning methods. In the Generated Knowledge Prompting framework (Liu et al., 2021), when solving a commonsense QA problem, first a knowledge model generates several commonsense knowledge statements rele-\nvant to the question, and then a QA model makes predictions based on them. A big problem that hinders the effectiveness of this framework is that model-generated knowledge is not always factual, and incorrect knowledge statements can mislead the QA model. We introduce VERA to filter these statements before passing them to the QA model. In particular, we keep those statements that receive a score higher than 0.5 from VERA.\nFollowing Liu et al. (2022b), we use UnifiedQA-large as the QA model, and consider two knowledge models: few-shot GPT-3 (davinci) (Brown et al., 2020) and Rainier-large (Liu et al., 2022b). We follow the evaluation settings as in Liu et al. (2022b), and for few-shot GPT-3 (davinci), we use the same task-specific few-shot prompts and same process to generate silver knowledge as in Liu et al. (2022b). Results are shown in Table 3. Applying knowledge filtering with VERA increases the\nusefulness of GPT-3\u2019s and Rainier\u2019s knowledge by 46% and 233%, respectively. VERA can effectively supervise and improve the quality of commonsense knowledge generated by a much larger model, GPT-3 (davinci). Detailed results (Table 12, appendix) show that there is increased effectiveness in every individual benchmark."
        },
        {
            "heading": "5.3 Preliminary Study on Detecting Commonsense Errors made by ChatGPT",
            "text": "VERA can be useful in detecting commonsense mistakes made by generative LMs in-the-wild. We collected 27 anecdotes from the Internet where people reported ChatGPT making commonsense errors, and manually rewrote them into their correct versions, obtaining 54 statements in total.\nWhen detecting incorrect commonsense statements in this dataset, VERA has a precision of 91% and a recall of 74%, amounting to an F1 score of 82%. Table 4 shows how VERA scores some of these these erroneous commonsense statements and their manually corrected version. In 7 out of the 9 cases, VERA assigns a low score to the original, incorrect statement, and a high score to the corrected statement. For example, \u201csince the density of a marble is much less than the density of mercury, the marble would sink to the bottom of the bowl if placed in it\u201d receives a score of 0.04 and is identified as an incorrect statement, whereas \u201csince the density of a marble is much less than the density of mercury, the marble would float if placed in mercury\u201d receives a score of 0.96 and is identified as a correct statement. Meanwhile, there are also some failure cases. VERA believes that \u201cit is possible for a solar eclipse to be followed by a lunar eclipse the next day\u201d, and fails to reject that \u201cit is possible to draw a diagonal line in a triangle\u201d."
        },
        {
            "heading": "5.4 Analysis",
            "text": "Ablations. We conduct an ablation study by incrementally removing the following components from the training process: contrastive loss (\u00a73.2.3), training stage A (\u00a73.2.4), LM-augmented falsehoods (\u00a73.1), multi-class loss or binary loss (\u00a73.2.3). Since at least one of the multi-class loss and the binary loss is needed, we remove them separately and observe the effect of training with a single loss. Results are shown in Figure 3. Overall, the ablated components have more impact on unseen benchmarks than seen ones. Removing the contrastive loss hurts performance mostly on unseen datasets, implying that the contrastive objective is beneficial for generalization. Removing training stage A hurts performance across the board, emphasizing the importance of training with largescale commonsense knowledge. LM-augmented falsehoods are most helpful on unseen benchmarks, with a little sacrifice in the performance on seen benchmarks. The multi-class loss is most helpful on multiple-choice benchmarks, while removing the binary loss substantially hurts performance on boolean benchmarks. Scaling Trends of VERA. We trained variants of VERA that are based on smaller versions of the T5 encoder, and show the results in Figure 4. Model performance increases steadily with size, and does not show evidence of saturation at 5B parameters, suggesting that better commonsense plausibility estimation models might be yielded from larger pretrained LMs.\nFormat: Verification vs. QA. In this paper, we focus on the verification format to solve commonsense problems. A comprehensive discussion on how this format compares with the QA format is provided in \u00a7E and Figure 7."
        },
        {
            "heading": "6 Related Work",
            "text": "Commonsense verifiers. Prior work has explored the idea of verifying commonsense statements. SYMBOLIC KNOWLEDGE DISTILLATION (West et al., 2021) and I2D2 (Bhagavatula et al., 2022) train models to classify the acceptability of model-generated commonsense statements. The ENTAILER (Tafjord et al., 2022) model is partially trained to score the validity of a given hypothesis. These models are trained on relatively small-scale, domain-specific data and do not generalize well to broader commonsense domains. Some other work uses pretrained LMs with few-shot prompting to verify commonsense statements (Kadavath et al., 2022; Jung et al., 2022). In this work, we develop a general-purpose commonsense statement verifier that works out-of-the-box in zero-shot setting.\nVerification in other tasks. Beyond commonsense statements, the problem of verification has been extensively studied on various NLP tasks. NLI (Liu et al., 2019, 2022a; Zhang et al., 2017) can be viewed as an entailment verification task. Chen et al. (2021) presents a method for QA verification by transforming the context passage and question-answer pair into a premise-hypothesis format as in NLI. Some work build models to perform reasoning verification \u2013 classifying whether a premise supports or refutes a hypothesis (Bostrom et al., 2022; Sprague et al., 2022; Yang et al., 2022; Tafjord et al., 2022). On the other hand, fact verification (Thorne et al., 2018; Wadden et al., 2020) requires judging the validity of claims against a corpus of evidence (e.g., Wikipedia). These tasks feature context-sensitive or knowledge-intensive hy-\npotheses to verify and are typically complemented with additional context. In contrast, we focus on verifying standalone commonsense statements where no context is required or provided.\nGeneration vs. verification. With the rapid progress in generative LMs, researchers have been largely building general-purpose problem-solving methods with a generative approach (Khashabi et al., 2020, 2022; Lourie et al., 2021; Tafjord and Clark, 2021; Wei et al., 2022). However, current generative LMs are still prone to hallucination errors and lack an intrinsic mechanism to express confidence level on their outputs. Verification, on the other hand, shows promise to complement these shortcomings and has been adopted to improve the outcome of generation (Chen et al., 2021; Jiang et al., 2022). In this work, we take a pure verification approach and build a general-purpose verifier for commonsense statements, which to our best knowledge is the first of its kind."
        },
        {
            "heading": "7 Conclusion and Future Work",
            "text": "We introduced VERA, a general-purpose verification model for commonsense statements and an early step toward tools for mitigating commonsense errors in text generated by language models. VERA achieves state-of-the-art performance when solving commonsense problems in the verification format, excels at filtering LM-generated commonsense knowledge statements, and is found useful in detecting erroneous commonsense statements from generative LMs. Furthermore, the scores produced by VERA are well-calibrated; and could be used for plausibility score estimation for declarative statements if needed. As VERA mainly targets on singlesentence statements, future work may consider verification of multi-sentence or long-form statements, or contextualized/defeasible commonsense statements.\nLimitations\nVERA aims, and is trained, to predict the plausibility of statements based on objective commonsense knowledge of our world. It is not intended to handle text outside the scope of commonsense statements (e.g., encyclopedic facts, reading comprehension with fictional worlds). It is not trained or evaluated on moral commonsense data, so its capability of making moral predictions is unknown. It gives a prediction even if the input falls out of its intended scope, which could be mitigated by an additional scope guard to determine its applicability. In addition, it is not trained to handle very long and compositional input. Although greatly outperforming existing systems, VERA is not perfect and may make incorrect predictions. It is not very robust under syntactic variations of the input, such as paraphrases and negations. As the training data may contain bias or toxicity, VERA may also make predictions that are perceived as ethically problematic. The output of VERA does not reflect the authors\u2019 view. VERA is a research prototype, and it is not designed for making real-world decisions."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank Sean Welleck, Peter West, Alisa Liu, Jaehun Jung, Chandra Bhagavatula, Ram Pasunuru, Asli Celikyilmaz, and members of the H2lab, Xlab and ARK lab for their discussion and constructive feedback. This work was funded in part by the DARPA MCS program through NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, and ONR N00014-18-1-2826. We thank OpenAI for offering access to their API."
        },
        {
            "heading": "A More Details on Datasets",
            "text": "Table 6 shows more dataset statistics, and Table 7 shows the dataset citations and links from which we retrieved the datasets.\nA.1 Dataset-Specific Special Handling\nFor some datasets, we pre-process them into a unified multiple-choice or boolean format. We provide the details below.\nCom2Sense (paired). Com2Sense contains true and false statements that can be paired into complements. To utilize this pairing information, we place the two statements in each pair into the same statement group, and treat this as a multiple-choice dataset. Some statements in the dev set are not paired, so we discarded these examples.\nCycIC (mc). CycIC contains both multiplechoice and boolean QA problems. To keep consistency in evaluation, we use only the multiplechoice problems, which is the dominant problem type in this dataset.\nComVE (task A). ComVE contains data for three tasks. Task A is assigning true/false labels to paired statements, similar to Com2Sense (paired). Task B and C are about choosing and generating explanations to a given statement being against commonsense. We use the data for task A.\nSKD (annotated). The annotated dataset of Symbolic Knowledge Distillation (SKD) contains LM-generated, semi-structured knowledge triples, where the head and tail events are connected by relations, such as\n(PersonX doesn\u2019t like to wait, xIntent, to get the job done).\nFollowing West et al. (2021), we replace the name placeholders with random person names, and convert into natural language statements using templates adapted from Hwang et al. (2020). For example, the triple in the above example becomes\nArnold doesn\u2019t like to wait. Because\nArnold wanted to get the job done.\nWe set the correctness label to be true iff the valid field has a positive value.\nI2D2 (annotated). The annotated dataset of I2D2 contains LM-generated commonsense statements with human-annotated correctness labels. We use the combination of annotated data in \u201cIter0\u201d and \u201cIter2\u201d, because the data of \u201cIter1\u201d is missing from the website.\nA.2 Conversion to Declarative Statements From QA datasets, we create declarative statements from QA problems using the following method:\n\u2022 If the problem contains a question, we convert the question and choice into a declarative statement using the question conversion model created by Chen et al. (2021).\n\u2022 If the question is cloze-style, we replace the blank with the choice.\n\u2022 If the question is an incomplete sentence and the choice is a continuation to it, we concatenate the question and the choice.\n\u2022 If there is no question and the problem only asks to choose between some choices, we use the choice as the declarative statement.\n\u2022 For boolean problems, we always use yes as the choice and create a single declarative statement for each problem. We use the original label as the correctness label of this statement."
        },
        {
            "heading": "B More Details on Method",
            "text": "B.1 Training Objectives Binary classification loss. We defined the binary classification loss as\nLbin(xi, yi) = \u2212 yi log s(xi)\u2212 (1\u2212 yi) log(1\u2212 s(xi)).\nTo account for the fact that there are usually more incorrect statements than correct ones in the data produced from multiple-choice datasets, we divide this loss by the number of statements with the same correctness label in the same statement group. Therefore, the binary classification loss for the whole batch is\nLbin =\n1\nBG BG\u2211 j=1 \u2211 y\u2208{0,1} \u2211Cj c=1 I[yjc = y]Lbin(xjc, yjc)\u2211Cj c=1 I[yjc = y] ,\nwhere Cj is the number of statements in statement group Xj , xjc is the cth statement in Xj , and I is the indicator function.\nMulti-class loss. We defined the multi-class loss as\nLmc(Xj) = \u2212 log exp z(xj\u2217)\u2211Cj c=1 exp z(xjc) .\nThe multi-class loss for the whole batch is\nLmc = 1\nBG BG\u2211 j=1 Lmc(Xj).\nSupervised contrastive loss. We defined the supervised contrastive loss as Lctr(xi, yi) = \u2212 log \u2211 k\u2208P(i) e cos(h(xi),h(xk))\n\u03c4\u2211 k\u2208P(i)\u222aN (i) e cos(h(xi),h(xk)) \u03c4 .\nThe supervised contrastive loss for the whole batch is\nLctr = 1\nBS BS\u2211 i=1 Lctr(xi, yi).\nB.2 Calibration\nOur calibration is a post-hoc strategy and does not affect the task performance metrics we report in \u00a75. This is because applying our calibration method \u2013 temperature scaling \u2013 does not affect the relative order of plausibility scores assigned to a given set of statements:\n\u2022 For tasks with multiple-choice questions (\u00a75.1), calibration does not affect the argmax prediction for the above reason.\n\u2022 For commonsense knowledge filtering (\u00a75.2), calibration does not affect the TPR/FPR numbers at each corresponding decision point, again for the above reason, so the ROC curves are valid.\n\u2022 For True/False judgment problems (\u00a75.1 and \u00a75.3), calibration does not move the plausibility scores across the decision boundary. We use logit z = 0.0 (or equivalently, plausibility score s = 0.5) as the True/False boundary. A positive (or negative) logit remains positive (or negative) after applying the temperature."
        },
        {
            "heading": "C More Details on Experimental Setup",
            "text": "Table 8 shows the hyperparamter settings for training VERA. These values are obtained from some\nmoderate hyperparameter tuning, and we did not do extensive search due to training cost.\nFor tokenization, the T5 tokenizer tokenizes input so that it ends with the EOS token </s> (token ID = 1). We manually configured the LLaMA tokenizer so that its output ends with the EOS token </s> (token ID = 2), and does not contain the BOS token <s> (token ID = 1). Models are trained for S = 50k steps with BG = 64 statement groups per batch, using the Adam optimizer (Kingma and Ba, 2014) with learning rate \u03b7 = 1 \u00d7 10\u22125 for T5 encoder and \u03b7 = 2 \u00d7 10\u22126 for LLaMA. We train models with the Huggingface Transformers and Accelerate libraries (Wolf et al., 2019; Gugger et al., 2022). For memory efficiency, during training, each statement is truncated to 128 tokens (which can accommodate more than 99% of the statements; see Table 6) and each statement group is capped to four statements.\nC.1 Definition of Metrics Multiple-choice accuracy. For multiple-choice benchmarks, we report the multiple-choice accuracy:\nAccmc = 1 |D| \u2211 Xj\u2208D I[xj\u2217 = argmax xjc\u2208Xj s(xjc)].\nBoolean accuracy. The boolean accuracy is defined as\nAccbool = 1 |D| \u2211\n(xi,yi)\u2208D\nI [ yi = I[z(xi) > 0] ] .\nBoolean accuracy is applicable to balanced boolean benchmarks where there are roughly equal true and false statements (e.g., CommonsenseQA 2.0, Spatial Commonsense, StrategyQA, CREAK). Generally it is not a good metric for multiplechoice benchmarks and unbalanced boolean benchmarks.\nAUROC and AP. For unbalanced boolean benchmarks (e.g., LM-generated knowledge filtering datasets), accuracy may not faithfully capture the model\u2019s performance. Instead, the metrics we use are the area under the ROC curve (AUROC) and the average precision (AP) for selecting the True statements. Statements are ranked based on their assigned raw scores, so that different score thresholds can be selected to construct the ROC and PrecisionRecall curves. Aside from unbalanced boolean benchmarks, AUROC and AP are also applicable\nto multiple-choice and balanced boolean benchmarks.\nCalibration. To measure how well the verifierpredicted score reflects its confidence, we measure the ECE (Naeini et al., 2015) on the boolean benchmarks. ECE is computed as\nECE = M\u2211\nm=1\n|Bm| |D|\n\u00b7 \u2223\u2223\u2223Acc(Bm)\u2212 Score(Bm)\u2223\u2223\u2223\n= M\u2211\nm=1\n|Bm| |D| \u00b7 \u2223\u2223\u2223 1|Bm| \u2211\n(xi,yi)\u2208Bm\nI[yi = 1]\n\u2212 1 |Bm| \u2211 (xi,yi)\u2208Bm s(xi) \u2223\u2223\u2223, (1)\nwhere M is the number of bins which bucket data points with similar predictions, and Bm \u2286 D is the subset of data points that fall into the m-th bin. We use M = 10 equal-sized bins when computing ECE.\nC.2 Details on Baseline Models SKD Critic. West et al. (2021) trained a critic model that filters incorrect commonsense knowledge generated by their symbolic knowledge distillation (SKD) method. This critic model is based on RoBERTa-large (Liu et al., 2019) and is finetuned on 8k GPT-3-generated commonsense knowledge sentences with human-annotated true/false labels. The model predicts a [0, 1] score s which we use as the final score, and we let the logit z = \u03c3\u22121(s).\nI2D2 Critic. Bhagavatula et al. (2022) trained a critic model that filters incorrect commonsense knowledge generated by their I2D2 method. This critic model is based on RoBERTa-large (Liu et al., 2019) and is finetuned on 12k I2D2-generated commonsense knowledge sentences with humanannotated true/false labels. Given an input statement, the model predicts two logits: t for the True label and f for the False label. We let the logit z = t\u2212 f and the score s = \u03c3(t\u2212 f). We use the critic model trained in the final iteration (i.e., \u201cIter 2\u201d in I2D23).\nUnifiedQA-v2. UnifiedQA-v2 (Khashabi et al., 2022) is a general-purpose QA model trained on datasets with a variety of input formats, including boolean datasets. When the input is a declarative statement, the model is trained to output either \u201cyes\u201d\n3https://gengen.apps.allenai.org/\nor \u201cno\u201d. We use this feature of the model and make it act as a commonsense statement verifier. For an input statement, we compute the logits received by \u201cyes\u201d and \u201cno\u201d in the decoder, denoted as t and f , respectively. We let the logit z = t \u2212 f and the score s = \u03c3(t\u2212 f). We use the largest version of this model, UnifiedQA-v2-11b.4\nEntailer. Entailer (Tafjord et al., 2022) is a model trained to construct proof trees for scientific commonsense hypotheses. This multi-angle model can be used in three ways: (1) given a hypothesis, generate a set of premises that may entail it; (2) given a hypothesis, predict a score that reflects the model\u2019s belief in it; (3) given a hypothesis and set of premises, predict a score that reflects whether there is a valid entailment between them. We use (2) as a commonsense statement verifier. The model predicts a [0, 1] score s which we use as the final score, and we let the logit z = \u03c3\u22121(s). We use the largest version of this model, Entailer-11b.5\nGPT-3.5. GPT-3.5 (OpenAI, 2022b) is a series of general-purpose autoregressive decoder-only LMs. To make it act as a commonsense verifier, we use the following input prompt:\nQuestion: Based on commonsense knowledge, is the following\nstatement correct? Please answer yes or no.\nStatement: {statement}\nAnswer:\nWe query the OpenAI Completions API6 with this prompt and compute the logits received by \u201c Yes\u201d and \u201c No\u201d in the next-token prediction, denoted as t and f , respectively. We let the logit z = t \u2212 f and the score s = \u03c3(t\u2212 f). We experimented with several prompt formats and found the one presented above to have the best performance, and in most cases, \u201c Yes\u201d and \u201c No\u201d together receive most of the probability mass during next-token prediction. We also experimented with several models in the GPT-3 (Brown et al., 2020) and GPT-3.5 series, and found GPT-3.5 (text-davinci-002) to work the best. Additionally, we report a baseline where the (negated) language modeling perplexity is used for\n4https://huggingface.co/allenai/ unifiedqa-v2-t5-11b-1251000\n5https://huggingface.co/allenai/entailer-11b 6https://platform.openai.com/docs/\napi-reference/completions\ncommonsense plausibility. Note that the plausibility scores derived this way are not normalized, and we only use them for ranking purposes. For this baseline, we use GPT-3.5 (text-davinci-002) as the base model, and name it as \u201cPPL (GPT-3.5)\u201d.\nChatGPT and GPT-4. ChatGPT (OpenAI, 2022a) and GPT-4 (OpenAI, 2023) are optimized for chat. To make them act as a commonsense verifier, we use the same input prompt as for GPT-3.5, without the \u201cAnswer:\u201d line. We query the OpenAI Chat API7 with this prompt in a user message, and obtain the first token of the assistant message in the response. Besides this zero-shot setting, we additionally report a few-shot chain-of-thought (Wei et al., 2022) setting with 5 in-domain examples, formatted as additional user-assistant message pairs prior to the query user message.\nSince the API does not provide token logits, we let the score s = 1.0 when this token is \u201cYes\u201d, and s = 0.0 when this token is \u201cNo\u201d. In the unlikely case that this token is neither, we let s = 0.5. We add a small random noise to the score. This is to arbitrate potentially multiple positive predictions within statement groups from multiple-choice QA problems, and to enable plotting the ROC and precision-recall curves. Note that this is not an ideal solution and may cause under-estimation of ChatGPT and GPT-4\u2019s performance.\nFlan-T5. Flan-T5 (Chung et al., 2022) is a series of sequence-to-sequence LMs instruction-finetuned on massive number of tasks. To make it act as a commonsense verifier, we use the same input prompt as for GPT-3.5. We compute the logits received by \u201cyes\u201d and \u201cno\u201d in the first token prediction in the decoder, denoted as t and f , respectively. We let the logit z = t\u2212f and the score s = \u03c3(t\u2212f). We experimented with several prompt formats and found the one presented above to have the best performance, and in most cases, \u201cyes\u201d and \u201cno\u201d together receive most of the probability mass during the token prediction. We use the largest version of this model, Flan-T5-XXL.8 Note that some unseen benchmarks are in the training data of Flan-T5; see Table 7 for details on data contamination.\n7https://platform.openai.com/docs/ api-reference/chat\n8https://huggingface.co/google/flan-t5-xxl"
        },
        {
            "heading": "D More Evaluation Results",
            "text": "Figure 5 is an expansion of Table 2 and additionally shows the precision-recall curves on problemsolving benchmarks. Table 9, Table 10, and Table 11 show the per-dataset breakdown of the accuracy numbers in Figure 5. Figure 6 is an expansion of Figure 2 and additionally shows the precisionrecall curves on knowledge-filtering benchmarks. Table 12 shows the per-dataset breakdown of the accuracy numbers in Table 3."
        },
        {
            "heading": "E Further Analysis",
            "text": "Format: Verification vs. QA. In this paper, we have been using the verification format to approach problem-solving tasks. But do we lose something when compared to using the QA format? In Figure 7 we compare how well existing models can solve problems in the verification format and the QA format. Verification format does fall behind QA format, especially with models trained exclusively in QA format (i.e., UnifiedQA-v2). We also trained a sequence-to-sequence model in QA format on the same multiple-choice data as VERA. It leads VERA by 1.5% on seen multiple-choice benchmarks. We hypothesize that this is because verification models only see one option at a time, whereas QA models can see all choices of a problem at the same time and thus can do comparative ranking. In addition to the performance loss, a verification model does lose the generative capability possessed by some QA models that are generative (e.g., UnifiedQA in Khashabi et al. (2020)), and it has to run C times to solve a C-way multiple-choice problem, whereas QA models (e.g., UnifiedQA in Khashabi et al. (2020), Unicorn in Lourie et al. (2021)) need to run only once. However, verification models can perform some tasks that generative QA models cannot cover. They can classify the correctness of declarative statements, without having to convert them into questions in the first place. They can also reflect on the answer produced by a generative QA model, and provide a level of confidence. We argue that verification models and generative QA models have different best-application scenarios and are sometimes complementary to each other."
        }
    ],
    "title": "VERA: A General-Purpose Plausibility Estimation Model for Commonsense Statements",
    "year": 2023
}