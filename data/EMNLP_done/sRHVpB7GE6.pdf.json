{
    "abstractText": "Generative AI models exhibit remarkable potential; however, hallucinations across various tasks present a significant challenge, particularly for longer inputs that current approaches struggle to address effectively. We introduce SCALE (Source Chunking Approach for Large-scale inconsistency Evaluation), a task-agnostic model for detecting factual inconsistencies using a novel chunking strategy. Specifically, SCALE is a Natural language inference (NLI) based model that uses large text chunks to condition over long texts. This approach achieves state-of-the-art performance in factual inconsistency detection for diverse tasks and long inputs. Additionally, we leverage the chunking mechanism and employ a novel algorithm to explain SCALE\u2019s decisions through relevant source sentence retrieval. Our evaluations reveal that SCALE outperforms existing methods on both standard benchmarks and a new long-form dialogue dataset ScreenEval we constructed. Moreover, SCALE surpasses competitive systems in efficiency and model explanation evaluations. We have released our code and data publicly to GitHub1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Barrett Martin Lattimer"
        },
        {
            "affiliations": [],
            "name": "Patrick Chen"
        },
        {
            "affiliations": [],
            "name": "Xinyuan Zhang"
        },
        {
            "affiliations": [],
            "name": "Yi Yang"
        }
    ],
    "id": "SP:f7da64cc345b44c97e21e9fa3902153bc5253c50",
    "references": [
        {
            "authors": [
                "Ziqiang Cao",
                "Furu Wei",
                "Wenjie Li",
                "Sujian Li."
            ],
            "title": "Faithful to the original: Fact aware neural abstractive summarization",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
            "year": 2018
        },
        {
            "authors": [
                "Mingda Chen",
                "Zewei Chu",
                "Sam Wiseman",
                "Kevin Gimpel."
            ],
            "title": "Summscreen: A dataset for abstractive screenplay summarization",
            "venue": "arXiv preprint arXiv:2104.07091.",
            "year": 2021
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Ido Dagan",
                "Oren Glickman",
                "Bernardo Magnini."
            ],
            "title": "The pascal recognising textual entailment challenge",
            "venue": "Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment: First PASCAL",
            "year": 2006
        },
        {
            "authors": [
                "Mingkai Deng",
                "Bowen Tan",
                "Zhengzhong Liu",
                "Eric Xing",
                "Zhiting Hu."
            ],
            "title": "Compression, transduction, and creation: A unified framework for evaluating natural language generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Deutsch",
                "Dan Roth."
            ],
            "title": "Understanding the extent to which content quality metrics measure the information quality of summaries",
            "venue": "Proceedings of the 25th Conference on Computational Natural Language Learning, pages 300\u2013309.",
            "year": 2021
        },
        {
            "authors": [
                "Esin Durmus",
                "He He",
                "Mona Diab."
            ],
            "title": "Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization",
            "venue": "arXiv preprint arXiv:2005.03754.",
            "year": 2020
        },
        {
            "authors": [
                "Nouha Dziri",
                "Hannah Rashkin",
                "Tal Linzen",
                "David Reitter."
            ],
            "title": "Evaluating attribution in dialogue systems: The begin benchmark",
            "venue": "Transactions of the Association for Computational Linguistics, 10:1066\u2013 1083.",
            "year": 2022
        },
        {
            "authors": [
                "Ori Ernst",
                "Ori Shapira",
                "Ramakanth Pasunuru",
                "Michael Lepioshkin",
                "Jacob Goldberger",
                "Mohit Bansal",
                "Ido Dagan."
            ],
            "title": "Summary-source proposition-level alignment: Task, datasets and supervised baseline",
            "venue": "arXiv preprint arXiv:2009.00590.",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Fabbri",
                "Chien-Sheng Wu",
                "Wenhao Liu",
                "Caiming Xiong."
            ],
            "title": "QAFactEval: Improved QA-based factual consistency evaluation for summarization",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Alexander R Fabbri",
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher",
                "Dragomir Radev."
            ],
            "title": "Summeval: Re-evaluating summarization evaluation",
            "venue": "Transactions of the Association for Computational Linguistics, 9:391\u2013409.",
            "year": 2021
        },
        {
            "authors": [
                "Alexander R Fabbri",
                "Chien-Sheng Wu",
                "Wenhao Liu",
                "Caiming Xiong."
            ],
            "title": "Qafacteval: Improved qa-based factual consistency evaluation for summarization",
            "venue": "arXiv preprint arXiv:2112.08542.",
            "year": 2021
        },
        {
            "authors": [
                "Tobias Falke",
                "Leonardo FR Ribeiro",
                "Prasetya Ajie Utama",
                "Ido Dagan",
                "Iryna Gurevych."
            ],
            "title": "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference",
            "venue": "Proceedings of the 57th Annual",
            "year": 2019
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q Weinberger."
            ],
            "title": "On calibration of modern neural networks",
            "venue": "International conference on machine learning, pages 1321\u20131330. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Prakhar Gupta",
                "Chien-Sheng Wu",
                "Wenhao Liu",
                "Caiming Xiong."
            ],
            "title": "Dialfact: A benchmark for fact-checking in dialogue",
            "venue": "arXiv preprint arXiv:2110.08222.",
            "year": 2021
        },
        {
            "authors": [
                "Or Honovich",
                "Roee Aharoni",
                "Jonathan Herzig",
                "Hagai Taitelbaum",
                "Doron Kukliansy",
                "Vered Cohen",
                "Thomas Scialom",
                "Idan Szpektor",
                "Avinatan Hassidim",
                "Yossi Matias."
            ],
            "title": "True: Re-evaluating factual consistency evaluation",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Or Honovich",
                "Leshem Choshen",
                "Roee Aharoni",
                "Ella Neeman",
                "Idan Szpektor",
                "Omri Abend"
            ],
            "title": "q2: Evaluating factual consistency",
            "year": 2021
        },
        {
            "authors": [
                "Kung-Hsiang Huang",
                "Siffi Singh",
                "Xiaofei Ma",
                "Wei Xiao",
                "Feng Nan",
                "Nicholas Dingwall",
                "William Yang Wang",
                "Kathleen McKeown."
            ],
            "title": "Swing: Balancing coverage and faithfulness for dialogue summarization",
            "venue": "arXiv preprint arXiv:2301.10483.",
            "year": 2023
        },
        {
            "authors": [
                "Tushar Khot",
                "Ashish Sabharwal",
                "Peter Clark."
            ],
            "title": "Scitail: A textual entailment dataset from science question answering",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
            "year": 2018
        },
        {
            "authors": [
                "Yuta Koreeda",
                "Christopher D Manning."
            ],
            "title": "Contractnli: A dataset for document-level natural language inference for contracts",
            "venue": "arXiv preprint arXiv:2110.01799.",
            "year": 2021
        },
        {
            "authors": [
                "Kalpesh Krishna",
                "Erin Bransom",
                "Bailey Kuehl",
                "Mohit Iyyer",
                "Pradeep Dasigi",
                "Arman Cohan",
                "Kyle Lo."
            ],
            "title": "Longeval: Guidelines for human evaluation of faithfulness in long-form summarization",
            "venue": "arXiv preprint arXiv:2301.13298.",
            "year": 2023
        },
        {
            "authors": [
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Evaluating the factual consistency of abstractive text summarization",
            "venue": "arXiv preprint arXiv:1910.12840.",
            "year": 2019
        },
        {
            "authors": [
                "Wojciech Kryscinski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Evaluating the factual consistency of abstractive text summarization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Philippe Laban",
                "Tobias Schnabel",
                "Paul N Bennett",
                "Marti A Hearst."
            ],
            "title": "Summac: Re-visiting nlibased models for inconsistency detection in summarization",
            "venue": "Transactions of the Association for Computational Linguistics, 10:163\u2013177.",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Joshua Maynez",
                "Shashi Narayan",
                "Bernd Bohnet",
                "Ryan McDonald."
            ],
            "title": "On faithfulness and factuality in abstractive summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906\u20131919, On-",
            "year": 2020
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela."
            ],
            "title": "Adversarial nli: A new benchmark for natural language understanding",
            "venue": "arXiv preprint arXiv:1910.14599.",
            "year": 2019
        },
        {
            "authors": [
                "Artidoro Pagnoni",
                "Vidhisha Balachandran",
                "Yulia Tsvetkov."
            ],
            "title": "Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the",
            "year": 2021
        },
        {
            "authors": [
                "Tal Schuster",
                "Sihao Chen",
                "Senaka Buthpitiya",
                "Alex Fabrikant",
                "Donald Metzler."
            ],
            "title": "Stretching sentence-pair nli models to reason over long documents and clusters",
            "venue": "arXiv preprint arXiv:2204.07447.",
            "year": 2022
        },
        {
            "authors": [
                "Tal Schuster",
                "Adam Fisch",
                "Regina Barzilay."
            ],
            "title": "Get your vitamin c! robust fact verification with contrastive evidence",
            "venue": "arXiv preprint arXiv:2103.08541.",
            "year": 2021
        },
        {
            "authors": [
                "Tal Schuster",
                "Adam Fisch",
                "Regina Barzilay."
            ],
            "title": "Get your vitamin C! robust fact verification with contrastive evidence",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Scialom",
                "Paul-Alexis Dray",
                "Patrick Gallinari",
                "Sylvain Lamprier",
                "Benjamin Piwowarski",
                "Jacopo Staiano",
                "Alex Wang."
            ],
            "title": "Questeval: Summarization asks for fact-based evaluation",
            "venue": "arXiv preprint arXiv:2103.12693.",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Scialom",
                "Paul-Alexis Dray",
                "Sylvain Lamprier",
                "Benjamin Piwowarski",
                "Jacopo Staiano",
                "Alex Wang",
                "Patrick Gallinari."
            ],
            "title": "QuestEval: Summarization asks for fact-based evaluation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Scialom",
                "Sylvain Lamprier",
                "Benjamin Piwowarski",
                "Jacopo Staiano."
            ],
            "title": "Answers unite! unsupervised metrics for reinforced summarization models",
            "venue": "arXiv preprint arXiv:1909.01610.",
            "year": 2019
        },
        {
            "authors": [
                "Thibault Sellam",
                "Dipanjan Das",
                "Ankur Parikh."
            ],
            "title": "BLEURT: Learning robust metrics for text generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881\u20137892, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "Fever: a large-scale dataset for fact extraction and verification",
            "venue": "arXiv preprint arXiv:1803.05355.",
            "year": 2018
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Oana Cocarascu",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "The fact extraction and VERification (FEVER) shared task",
            "venue": "Proceedings of the First Workshop on Fact Extraction and VERification",
            "year": 2018
        },
        {
            "authors": [
                "Prasetya Ajie Utama",
                "Joshua Bambrick",
                "Nafise Sadat Moosavi",
                "Iryna Gurevych."
            ],
            "title": "Falsesum: Generating document-level nli examples for recognizing factual inconsistency in summarization",
            "venue": "arXiv preprint arXiv:2205.06009.",
            "year": 2022
        },
        {
            "authors": [
                "Alex Wang",
                "Kyunghyun Cho",
                "Mike Lewis."
            ],
            "title": "Asking and answering questions to evaluate the factual consistency of summaries",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008\u20135020, On-",
            "year": 2020
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "arXiv preprint arXiv:1704.05426.",
            "year": 2017
        },
        {
            "authors": [
                "Weizhe Yuan",
                "Graham Neubig",
                "Pengfei Liu."
            ],
            "title": "Bartscore: Evaluating generated text as text generation",
            "venue": "Advances in Neural Information Processing Systems, 34:27263\u201327277.",
            "year": 2021
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "arXiv preprint arXiv:1904.09675.",
            "year": 2019
        },
        {
            "authors": [
                "Yuan Zhang",
                "Jason Baldridge",
                "Luheng He."
            ],
            "title": "PAWS: Paraphrase adversaries from word scrambling",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2019
        },
        {
            "authors": [
                "Ming Zhong",
                "Yang Liu",
                "Da Yin",
                "Yuning Mao",
                "Yizhu Jiao",
                "Pengfei Liu",
                "Chenguang Zhu",
                "Heng Ji",
                "Jiawei Han."
            ],
            "title": "Towards a unified multidimensional evaluator for text generation",
            "venue": "arXiv preprint arXiv:2210.07197.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Generative AI models exhibit remarkable potential; however, hallucinations across various tasks present a significant challenge, particularly for longer inputs that current approaches struggle to address effectively. We introduce SCALE (Source Chunking Approach for Large-scale inconsistency Evaluation), a task-agnostic model for detecting factual inconsistencies using a novel chunking strategy. Specifically, SCALE is a Natural language inference (NLI) based model that uses large text chunks to condition over long texts. This approach achieves state-of-the-art performance in factual inconsistency detection for diverse tasks and long inputs. Additionally, we leverage the chunking mechanism and employ a novel algorithm to explain SCALE\u2019s decisions through relevant source sentence retrieval. Our evaluations reveal that SCALE outperforms existing methods on both standard benchmarks and a new long-form dialogue dataset ScreenEval we constructed. Moreover, SCALE surpasses competitive systems in efficiency and model explanation evaluations. We have released our code and data publicly to GitHub1."
        },
        {
            "heading": "1 Introduction",
            "text": "Large Language Models (LLMs) have shown immense promise in various applications, but deploying them in real-time presents certain challenges such as hallucinations (Cao et al., 2018; Falke et al., 2019; Krys\u0301cin\u0301ski et al., 2019; Fabbri et al., 2021a; Honovich et al., 2022). Hallucinations, or factual inconsistencies generated by a model relative to a source document, can mislead the user and undermine trust in LLMs. Thus, detecting factual inconsistency in LLM generations is crucial for the future of LLMs, especially with the growing popularity of platforms like ChatGPT.\n\u2217Work done while at ASAPP. 1https://github.com/asappresearch/scale-score\nPrior research on inconsistency detection has predominantly dealt with short documents in offline settings (Laban et al., 2022; Schuster et al., 2022; Utama et al., 2022) and relied heavily on sentence-level text matching techniques. Consequently, these methods exhibit slow performance in processing longer documents and suffer from poor calibration. Such characteristics pose substantial challenges in implementing them in realworld online environments, where incorporating inconsistency detection could potentially result in a substantial increase in latency. Additionally, the absence of well-calibrated scores complicates the balancing act between mitigating the risk of incorporating hallucinations and excluding pertinent information from the model output. Given the exponential growth in context sizes (maximum allowed tokens of an input) of contemporary large language models (LLMs),2 there is an increasing urgency to develop efficient and effective approaches for inconsistency detection in lengthy documents.\nIn addressing the challenges, we introduce SCALE (Source Chunking Approach for Largescale inconsistency Evaluation), a method designed for efficient detection of factual inconsistencies in generated sentences by identifying related source text snippets. SCALE consists of two crucial components. First, it builds on a Natural language inference (NLI) based method, integrating a novel chunking mechanism for rapid and accurate online performance in diverse natural language generation (NLG) tasks. Second, model explanation is essential for real-time deployment of inconsistency detection systems, facilitating swift human inspection to determine model configurations. We show that our chunking mechanism improves calibration scores and enables the use of a binary search tree algorithm for rapidly locating relevant source text snippets for a target sentence, ultimately enhancing\n2For instance, OpenAI GPT-4 and Anthropic Claude support context sizes up to 32k and 100k tokens, respectively.\nthe explanation of model behaviors. Current benchmark datasets for factual inconsistency detection predominantly feature short documents. In order to evaluate SCALE using a more realistic dataset with long documents, we introduce ScreenEval \u2014 a novel dataset designed to assess the factual inconsistency of summary sentences generated by humans, Longformer, and GPT4 in comparison to actual long-form dialogues. ScreenEval encompasses 52 dialogues, averaging over 6,000 tokens per dialogue. The use of dialogue in this dataset poses a considerable unique challenges such as long-distance coreference resolution and significant noise between utterances. To the best of our knowledge, ScreenEval is the longest dialogue based dataset for factual inconsistency detection presently available.\nIn our experiments, we first show that SCALE outperforms and is better calibrated than baseline methods across various NLG tasks on the standard factual inconsistency detection benchmark TRUE (Honovich et al., 2022). We then assess accuracy, speed, and model explanation (via relevant text retrieval evaluation) on the new ScreenEval dataset for long document factual inconsistency detection. Our findings indicate that SCALE surpasses strong competitors in the majority of tests. The key contributions of this paper are:\n\u2022 We introduce SCALE, a reference-free, NLI based factual inconsistency detection method with a novel chunking strategy for versatility across domains and extended documents.\n\u2022 We show SCALE\u2019s broad applicability in NLG domains by attaining state-of-the-art performance on the TRUE benchmark.\n\u2022 We build ScreenEval, a novel dataset designed for factual inconsistency detection in long dialogues, and then demonstrate SCALE\u2019s superiority in accuracy, efficiency, and model explanation evaluations on the dataset."
        },
        {
            "heading": "2 Related Work",
            "text": "Factual Inconsistency Detection There are two main directions in factual inconsistency detection: Natural language inference (NLI) based and question answering (QA) based methods. In NLI based methods, pretrained NLI models can be utilized to determine whether a given \"premise\" factually entails a \"hypothesis.\" Although initial attempts\nencountered challenges (Khot et al., 2018), recent advancements have shown that NLI models can effectively assess the factual consistency of generated text (hypothesis) with respect to a source (premise) (Utama et al., 2022). This progress can largely be attributed to addressing the granularity problem, which arises from the abundance of current NLI datasets predominantly comprised of short, singlesentence premises and hypotheses (Williams et al., 2017; Nie et al., 2019; Thorne et al., 2018a; Schuster et al., 2021a).\nSCALE is an NLI based method and our findings indicate that utilizing larger premise chunks enhances efficiency and outperforms sentence decomposition. Although SeNtLI (Schuster et al., 2022) extended NLI based methods to longer documents, it adhered to the sentence decomposition assumption and focused solely on summarization tasks. SummaC (Laban et al., 2022) investigated various aggregation techniques for NLI scores obtained from sentence decomposition to generate overall summary scores. Meanwhile, SWING (Huang et al., 2023) developed a loss function to train models for improved NLI performance, yielding mixed outcomes.\nIn QA based methods, a question is first generated based on a summary sentence, and a QA system is used to give an answer. A summary is considered factually consistent if the generated answer significantly overlaps with the original summary (Durmus et al., 2020). Prior research focused on using different question generation strategies (Scialom et al., 2019) or overlap measures (Deutsch and Roth, 2021). In the experiments, we consider the most competitive QuestEval (Scialom et al., 2021a) and QAFactEval (Fabbri et al., 2021b).\nDetecting Factual Inconsistencies in Long Documents Prior work on factual inconsistency performance in long source documents has been limited in scope. For example, ContractNLI (Koreeda and Manning, 2021) concentrates on legal documents, which differ significantly from dialogues in terms of challenges. Likewise, LongEval (Krishna et al., 2023) emphasizes human evaluation strategies for scoring, without considering dialogues. To our knowledge, this paper presents the first dataset for evaluating factual inconsistency in long-form dialogues, ScreenEval, thus addressing a substantial gap in the literature."
        },
        {
            "heading": "3 SCALE",
            "text": "In this section we elaborate on our approach taken for our inconsistency detection model SCALE. Firstly, we formally define the use of chunks and NLI in SCALE, aiming at improving the accuracy and efficiency of the system. Secondly, we propose to explain the model output by retrieving the relevant source sentence for a target, and show how the relevant sentence retrieval can be improved through the use of chunks."
        },
        {
            "heading": "3.1 Chunking Mechanism for NLI based Model",
            "text": "Our approach uses NLI (Dagan et al., 2006) as a building block for factual inconsistency detection. An NLI model M provides the relationship between a premise p and a hypothesis h, M(p, h) with probabilities of three labels: entailed, neutral, and contradictory. For example, given an NLI model M(p, h), source document D with a set of facts FD, and a generated text G with a set of facts FG, if FG \u2286 FD we would expect M(D,G) to produce high entailment probability.\nWe define factual consistency between a generated text and source document as FG \u2286 FD. Canonical NLI models cannot be properly used for factual inconsistency detection because both p and h are commonly single sentences in NLI models, however in the factual inconsistency task their equivalents D and G almost always contain multiple sentences which M cannot effectively handle, leading to an issue known as the granularity prob-\nlem (Utama et al., 2022). To bypass the granularity problem, a natural generalization is to split both D and G into sentences and run M pairwise on each of those sentences then using an aggregation function f to generate the final entailment probability. Numerous papers have used this approach to generate competitive results (Schuster et al., 2022; Laban et al., 2022) however this generalization is hindered by a few shortcomings.\nFirst, the sentence decomposition of D and G does not properly capture the context provided in D. By decomposing D and G into single sentences D = (d1, d2, . . . , di, . . . , d|D|) and G = (g1, g2, . . . , gj , . . . , g|G|) and put into the model to evaluate as M(di, gj), the context and long term dependencies present in D that may have factually supported gj very likely could not be represented in di. Multiple sentences (e.g., \u222ai\u2208{1,3,6}di) together in unison may be needed to support a single claim gj . However, evaluating gj against each sentence individually M(d1, gj), M(d3, gj), M(d6, gj) would likely lead to artificially low scores. Second, evaluating pairwise sentences of D and G is slow. It requires |D| \u00b7 |G| model runs to obtain a final score for one sentence gj .\nSCALE poses a different solution to the granularity problem by decomposing D into much larger chunks, which can be visualized in Figure 1. Formally, SCALE decomposes document D into a set of N chunks C = c1, c2, . . . , cN such that \u222ac\u2208C = D. SCALE can handle chunks of arbitrary length only limited by memory requirements, thus drastically increasing the context win-\ndow provided to the model through the premise. The generated text G is broken into sentences G = (g1, g2, . . . , gj , . . . , g|G|). We propose that decomposing D using chunks rather than sentences does not negatively affect the granularity problem but rather enables superior contextual capture in model M , boosts accuracy, and requires significantly less model runs.\nSCALE uses Flan-T5 (Chung et al., 2022) as a backbone NLI model M . SCALE obtains the probability that a chunk ci entails a generated sentence gj through the following steps. First, logits are obtained by prompting M with the following: logits = M(\u201c{ci} Question: does this imply \u2018{gj}\u2019? Yes or no?\u201d). The entailment probability between gj and ci is then calculated by\nPentail = SoftMax(logits[\"Yes\"], logits[\"No\"])[0].\nTo obtain the overall entailment score for a generated sentence gj , the results are aggregated over all possible ci by,\nSCALE(C, gj) = max i=1...N (Pentail(ci, gj))\nto obtain a final measure of factual consistency."
        },
        {
            "heading": "3.2 Model Explanation via Relevant Source Text Retrieval",
            "text": "To produce explainable and interpretable scores for factual consistency and empower necessary human inspection, it is important to justify the score by retrieving relevant text from the source. Formally, the retrieval task involves finding the most relevant sentence di in document D with respect\nto a hypothesis h. Using a new search tree approach enabled by chunking, SCALE is able to retrieve di in context while using far fewer model runs than previous approaches. We use a greedy search tree approach that evaluates a hypothesis using SCALE against progressively smaller chunks to find the highly relevant text from the source document. For the following example, assume we use a binary search tree (BST) at each level, dividing the text into two chunks. This process can be visualized in Figure 2. Given a hypothesis h, we want to find the most relevant utterance di in the source text. We begin by dividing the entire source document into two large chunks. SCALE is used to calculate the score between both chunks and the hypothesis h and then use the higher scoring chunk as the new source text. The new source text is then divided into two chunks, and continues to descend in this manner until the chunk size becomes a single sentence or utterance di. The best scoring chunk is then chosen to be the supporting proof of the hypothesis from the source document.\nThis retrieval approach is able to significantly reduce the number of model calls needed to find the relevant text from a source document. Previous approaches commonly break the source document down by sentence which requires O(n) model calls for a source document with n sentences. Whereas our BST approach only needs O(log(n)) model calls in order to find the most relevant utterance in the same source document of n sentences.\nNotice that we proposed the binary search scheme due to its simplicity and its connection to the popular binary search tree. In practice, diving the source text into only two chunks might cause out of GPU memory issues. In this case, we could generalize the proposed approach into different chunk splits. For example, we could divide the remaining tokens into three chunks or larger for the search of each block until the model fits the chunk. We could also use different chunk sizes for different layers so long as it fits in the memory."
        },
        {
            "heading": "4 ScreenEval Dataset",
            "text": "We introduce a novel dataset for evaluating inconsistency detection on long form dialogues called ScreenEval. This dataset uses TV scripts and summaries pulled from the SummScreen (Chen et al., 2021) dataset. In addition to the provided human summaries, we generate summaries using Longformer and GPT-4 on 52 scripts from the Summ-\nScreen test set. We then hire human annotators to classify the factual inconsistency of each summary sentence and identify relevant supporting utterances for factually consistent summary sentences. ScreenEval is released publicly. Details of how we use Longformer and GPT-4 and collect human annotation can be found in the Appendix A.\nThe SummScreen dataset is comprised of 2 sub datasets pulled from different sources ForeverDreaming and TVMegaSite. We use the ForeverDreaming subset of SummScreen, (SummScreenFD) to create ScreenEval due to its manageable summary size and diversity of shows and genres, spanning a total of 21 genres. SummScreenFD uses human-written gold summaries from Wikipedia and TVMaze. Table 1 shows statistics related to ScreenEval. Notably, the average number of tokens in a source document is 6,073, which, to the best of our knowledge, makes ScreenEval the longest dialogue based inconsistency detection dataset created. We provide 52 documents with an associated 624 summary sentences, 455 of which are artificially generated using Longformer and GPT-4. Summaries are kept at a high level, covering major plot points and character developments. This leads to shorter and more concise summaries that on average only run 101 tokens."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Datasets",
            "text": "TRUE The TRUE benchmark contains 11 datasets from 4 different NLG tasks. We compare our approach to others on this dataset to show how SCALE performs across a multitude of NLG tasks on inconsistency detection. Notably, the average number of tokens per example in the TRUE benchmark is small, generally less than 512. Each dataset\nin the TRUE benchmark is condensed down into a source document, generated text, and factual inconsistency label. The datasets are distributed across different tasks as shown in Table 2.\nSummarization datasets are from FRANK (Pagnoni et al., 2021), SummEval (Fabbri et al., 2021a), MNBM (Maynez et al., 2020), QAGSCNNDM (Wang et al., 2020), and QAGS-XSum (Wang et al., 2020). Dialogue datasets include BEGIN (Dziri et al., 2022), Q2 (Honovich et al., 2021), and DialFact (Gupta et al., 2021). Fact Verification datasets are FEVER (Thorne et al., 2018b) and VitaminC (Schuster et al., 2021b). The Paraphrasing dataset is PAWS (Zhang et al., 2019b).\nScreenEval Our datasest ScreenEval compares the inconsistency detection ability of methods on long form dialogue. Details can be found in Sec. 4"
        },
        {
            "heading": "5.2 Competitive Systems",
            "text": "TRUE provides 9 inconsistency detection baselines from 4 different inconsistency detection styles, namely n-gram based methods (token level F1), model based methods (BERTScore (Zhang et al., 2019a), BLEURT (Sellam et al., 2020), FactCC (Kryscinski et al., 2020), BARTScore (Yuan et al., 2021), CTC (Deng et al., 2021)), NLI based methods (ANLI (Honovich et al., 2022), SummaC (Laban et al., 2022)), and question answering (QA) based methods (Q2 (Honovich et al., 2021), QuestEval (Scialom et al., 2021b)).\nFor ScreenEval we compare SCALE to 8 models which use NLI, QA, modern GPT systems, and older n-gram and semantic similarity methods. The baseline models consist of two NLI based sentence decomposition approaches seNtLI (Schuster et al., 2022), and SummaCconv (Laban et al., 2022), a state-of-the-art QA based model QAFactEval (Fabbri et al., 2022), a multidimensional QA model UniEval (Zhong et al., 2022), a semantic similarity model based method BERTScore (Zhang et al., 2019a), an n-gram overlap method ROUGE (Lin, 2004), and the two recent OpenAI models ChatGPT, and GPT-4.\nWe also compare the performance of SCALE\u2019s search tree based relevant utterance retrieval with other recent retrieval models. We compare SCALE to the retrieval performance of SuperPal (Ernst et al., 2020) which was shown to have superior retrieval capabilities in LongEval. We also compare against seNtLI (Schuster et al., 2022) which was designed to perform retrieval to identify factual inconsistencies over long documents.\nFor SCALE , we include three variants of FlanT5 as the backbone, namely base, XL, and XXL."
        },
        {
            "heading": "5.3 Metrics",
            "text": "Accuracy Evaluation We compare the performance of methods primarily using four metrics, ROC_AUC score, Pearson correlation, Kendall_Tau correlation, and F1_Macro score. We employ the ROC_AUC score to quantify the ability of different methods in accurately identifying true positives and true negatives. Pearson and Kendall_Tau correlations show the relationship between methods and labels by measuring the correlations between the two. Finally the F1_Macro score is used to compare the continuous outputs of SCALE to the discrete outputs of GPT-4 and ChatGPT. To obtain an F1 score for SCALE we use the optimal threshold to convert its continuous output into discrete values.\nEfficiency Evaluation We measure wall clock time in seconds for all of our experiments on ScreenEval. Wall clock time demonstrates how SCALE can be used efficiently in an online setting especially when compared to other models.\nModel Explanation Evaluation We evaluate Calibration and Relevant Source Text Retrieval for model explanation.\nCalibration is the measure of how close the pseudo-probability outputs of a model are to the actual probability of a correct prediction. For example, a well calibrated model that produces a score of 0.2 would have a 20% probability of being classified as 1. In this paper, we use Expected Calibration Error (ECE) (Guo et al., 2017) to compare the calibration of SCALE to other commonly used models.\nGiven model outputs spanning from 0 to 1, ECE separates the outputs into K equally sized bins Bk between 0 and 1 and takes the difference between accuracy acc and confidence conf in each one. The accuracy of a bin Bk is the average amount\nof predicted labels that match true class labels in a bin, formally defined as\nacc(Bk) = 1 |Bk| \u2211 i\u2208Bk 1(y\u0302i = yi), (1)\nwhere y\u0302i and yi are the predicted and true class labels for sample i. Confidence in a bin Bk shows the average predicted score in a bin, formally defined as\nconf(Bk) = 1 |Bk| \u2211 i\u2208Bk p\u0302i, (2)\nwhere p\u0302i is the model output score for sample i. Then the following equation is used to calculate ECE,\nECE = K\u2211 k=1 |Bk| n |acc(Bk)\u2212 conf(Bk)|, (3)\nusing equation (1) and (2). A lower ECE indicates a better calibration.\nRelevant Source Text Retrieval tests if each model could return the correct utterance identified as relevant by human labelers. We report the recall of retrieval results."
        },
        {
            "heading": "6 Results",
            "text": ""
        },
        {
            "heading": "6.1 TRUE",
            "text": "We first evaluate SCALE on the TRUE benchmark to confirm SCALE is NLG task agnostic and generalizes well to the factual inconsistency detection.\nAccuracy Evaluation Results For the TRUE benchmark as shown in Table 3, SCALEXXL provides superior performance in 10 out of the 11 datasets, and SCALEXL achieves superior performance in 8 datasets compared to other nonSCALE models. Notably, other models were not previously able to perform well across all tasks, with Q2metric having superior performance across 3 datasets and ANLI having superior performance across 5. These results demonstrate SCALE\u2019s ability to perform well across domains and against a large variety of model types.\nModel Explanation Evaluation Results Not only does SCALE provide superior performance on the TRUE benchmark, but it is also highly calibrated across NLG tasks. Table 4 shows the ECE of multiple methods across the TRUE benchmark datasets. Note that a lower ECE is better. SCALElarge provides the best calibration on\nMetric FRANK SummEval MNBM QAGS-C QAGS-X BEGIN Q2ds DialFact PAWS FEVER VitC Avgw/o VitC,FEVER\naverage and SCALEXL outperforms other nonSCALE models in calibration on over half of the datasets in the TRUE benchmark.\nA visual example of the calibration results can be analyzed with the calibration curves in Figure 3. While most models are severely uncalibrated and underestimate the fraction of positives in Figure 3, SCALE is capable of sticking extremely close to the perfectly calibrated line. The closest model SummaCConv can be seen overestimating positive examples before scores reach 0.4. We hypothesize that the large context window is the key to better calibration in SCALE as it includes more information. This makes the underlying NLI model less likely to be biased toward a specific range of tokens which leads to extreme confidence based on certain short text. To empirically justify this, we perform further experiments on the proposed ScreenEval dataset shown in Figure 4. We can observe that for chunk size < 400, the calibration score (the lower the better) is much higher than larger chunk size 500 to 1000. This shows that a larger chunk size could enable the NLI model to extract more useful information to provide appropriate confidence when making the prediction. We also use this knowledge to support our decision to use 512 tokens as our chunk size for all experiments in this paper. The enhanced calibration achieved by SCALE allows it to be more interpretable as a probability, making it a valuable tool for comparison tasks."
        },
        {
            "heading": "6.2 ScreenEval",
            "text": "We then evaluate the performance of SCALE\u2019s chunking capabilities against other models in a long form dialogue setting using ScreenEval. We evaluate the factual inconsistency detection and relevant utterance retrieval of SCALE compared with other models and explore the unique problems posed by long form dialogue evaluation.\nAccuracy and Efficiency Evaluation Results We compare the factual inconsistency detection performance of multiple models on ScreenEval in Table 5. SCALE significantly outperforms other methods across all measures. While the state-ofthe-art QA model QAFactEval was able to perform well on ScreenEval, SCALElarge still showed superior performance across all metrics. Notably, even\nSummaCconv and seNtLI, which are designed to deal with long documents, have poor performance on ScreenEval.\nAlong with its superior performance, SCALE is able to run faster than other LLM based methods on ScreenEval also shown in Table 5. For a fair comparison, we set the batch size to 1 for all models and run with all other default settings. We do not include BERTScore due to it\u2019s truncation of the document, making timing not comparable. Most notably QAFactEval, which was closest in performance to SCALElarge, was 6 times slower than SCALElarge in wall clock time. Even faster though was SCALEbase which was 17 times faster than QAFactEval while only achieving slightly worse performance across all metrics on ScreenEval, and outperforming all non-SCALE methods other than QAFactEval. The SCALEbase model running at 1.1 seconds per score for long documents could realistically be used in an online setting to more accurately evaluate factual inconsistency.\nChunk size proves to have a large effect on the ability of SCALE\u2019s performance and time as seen in Figure 5. SCALElarge sees a sharp increase in performance up until the chunk size is 1000 tokens\nlong. Similarly, there is a sharp decrease in model run time up until 1000 tokens. Figure 5 substantiates our approach to the granularity problem by illustrating that a larger number of tokens in the premise leads to a more effective method.\nWe additionally compare SCALE with ChatGPT and GPT-4 on ScreenEval in Table 6. Due to the discrete nature of GPT-4 and ChatGPT\u2019s output, we choose an ideal threshold for SCALE and compare macro F1-Scores on the ScreenEval dataset. While GPT-4 is able to outperform SCALEXL in macro F1-Score, SCALE shows to be significantly better in terms of time and cost. ChatGPT is more comparable in terms of time and cost to SCALE; however, there is a significant performance drop in macro F1-Score. ChatGPT is also limited by its 4096 token length limit at the time of writing and must use truncated conversations from ScreenEval. These results help us conclude that while GPT-4 has superior performance, SCALE is able to provide a faster, more affordable model that can be used locally in an online setting to produce continuous scores.\nModel Explanation Evaluation Results We now compare SCALE\u2019s BST retrieval approach with SuperPal and seNtLI. SCALE outperforms both in terms of time and performance as shown in Table 7. SCALEXL identifies the most relevant utterances correctly 47% of the time compared to 34% for seNtLI and 41% for SuperPal. SCALE\u2019s BST retrieval requires significantly fewer model calls, allowing it to pinpoint relevant utterances without having to score each one individually like the other methods. This results in higher retrieval recall for both SCALElarge and SCALEXL. Moreover, because SCALElarge requires far less model calls it is able to provide faster results than SuperPal or seNtLI without comprimising effectiveness. This enhanced performance shows how SCALE could be used in an online setting for fast and accurate results."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we introduce a cutting-edge NLI based factual inconsistency detection method called SCALE. We show that SCALE is an NLG task agnostic factual inconsistency detection method by achieving state-of-the-art results across four distinct NLG tasks and 11 datasets within the TRUE benchmark. We show that across NLG tasks SCALE is also superior in calibration, providing scores that are interpretable and enables accurate comparison of scores. We introduce a new dataset called ScreenEval that is the first of its kind for long form dialogue factual inconsistency evaluation. SCALE is shown to significantly outperform all models other than GPT-4 in factual inconsistency detection on this dataset. However, we show that SCALE is significantly more cost effective and faster than GPT-4 for evaluation on ScreenEval. Moreover, we introduce a new retrieval strategy enabled by SCALE that significantly decreases the time to retrieve relevant utterances from a long document with increased recall accuracy."
        },
        {
            "heading": "8 Limitations",
            "text": "While SCALE performs well at inconsistency detection there are some limitations to this approach.\nSCALE only uses the \"Yes\" and \"No\" logits to compute it\u2019s entailment score, however only using those two logits specifically could lead to loss of accuracy due to other information possibly flowing to similar tokens such as \u201cyes\u201d, \u201cno\u201d. Using logits for scoring purposes may cause a loss of information to other similar logits.\nFinally, even though SCALE is able to achieve better calibration in the aggregate, it still struggles with calibration on certain tasks and this can even vary by model size. Consistent calibration of scoring methods across NLG tasks should be a goal for future methods."
        },
        {
            "heading": "A Details of Construction of ScreenEval Dataset",
            "text": "We create summaries for ScreenEval using two summarization models, GPT-4 and Longformer, as well as human generated summaries. The models that we use for summarization are designed to have a large token context window, giving them both the ability to globally attend over the long source dialogues we provide. Below, we will first explain how to leverage Longformer, GPT-4 to generate summaries, and then explain the process and the cost of human labeling.\nA.1 Building the Dataset\nWe first generated longformer summaries for each script in the test set. To keep the annotation task reasonable, and to filter out any rambling summaries, we filter out any TV scripts with a longformer or human summary that had more than 6 sentences or only 1 sentence. We still preserve 52 of the TV scripts by doing this, as the median number of summary sentences in a longformer summary was 4 and a human summary was 3. In order to meet GPT-4\u2019s token limit requirements, from the remaining TV scripts we chose all that had less than 8,100 tokens. Our final dataset consists of 52 TV scripts that have an average length of 6073 tokens with 624 summary sentences.\nA.2 Longformer\nWe use the same baseline model as in SummScreen to generate summaries in ScreenEval, a Longformer model finetuned on SummScreenFD\u2019s training set. This model uses a transformer based sequence to sequence architecture to globally attend over the entire dialogue. Longformer is able to take as many as 16384 tokens as input.\nA.3 GPT-4\nGPT-4 is a large language model that has shown near human level performance in a wide variety of tasks, including summarization. We task GPT-4 to summarize each document in ScreenEval using the prompt \u201cSummarize in 5 sentences or less: \u201d. To accommodate for the roughly 8k token limit on the GPT-4 model, we specifically select documents in ScreenEval that are under 8k tokens.\nA.4 Human Annotation We lable ScreenEval using workers from amazon mechanical turks with the promtp shown in Figure 6 and Figure 7. For each task, a worker is presented with a TV script from ScreenEval along with a highlighted summary sentence. The worker is instructed to first read through the source dialogue. Then, the worker is instructed to click either a Yes or No button to indicate whether the highlighted summary sentence is consistent with the source document. Each utterance in the TV script will be presented alongside a check box where if the worker chose \u201cYes\u201d to the consistency question, they will be asked to select the relevant utterances that led to their answer.\nWe had 3 human annotators label each instance, and 61% of the time all three annotators agreed. Workers were paid 0.27 per task. We ensured the quality of annotators through a number of methods. First, we filtered annotators to just those located in the US and Canada to increase the chances of high fluency in English on our reading comprehension task. Additionally the workers had to have an MTurk \u201cMaster\u201d qualification, greater than a 95% task approval rate, and greater than 5000 tasks approved. The dataset was labeled in batches of 30 at a time and closely monitored by the authors. Workers were only rejected if they did not list relevant utterances as instructed or listed non existent utterances, and these workers were able to dispute rejection via email."
        },
        {
            "heading": "B Prompts Used for GPT",
            "text": "B.1 ChatGPT/GPT-4\n{Dialogue} Question: does the previous conversation factually imply \"{Summary Sentence}\"? Answer Yes or No."
        }
    ],
    "title": "Fast and Accurate Factual Inconsistency Detection Over Long Documents",
    "year": 2023
}