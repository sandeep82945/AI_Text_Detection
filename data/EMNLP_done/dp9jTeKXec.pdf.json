{
    "abstractText": "Document-level Event Argument Extraction (EAE) requires the model to extract arguments of multiple events from a single document. Considering the underlying dependencies between these events, recent efforts leverage the idea of \u201cmemory\u201d, where the results of already predicted events are cached and can be retrieved to help the prediction of upcoming events. These methods extract events according to their appearance order in the document, however, the event that appears in the first sentence does not mean that it is the easiest to extract. Existing methods might introduce noise to the extraction of upcoming events if they rely on an incorrect prediction of previous events. In order to provide more reliable memory, we propose a simple-to-complex progressive framework for document-level EAE. Specifically, we first calculate the difficulty of each event and then, we conduct the extraction following a simple-to-complex order. In this way, the memory will store the most certain results, and the model could use these reliable sources to help the prediction of more difficult events. Experiments on WIKIEVENTS show that our model outperforms SOTA by 1.4% in F1, indicating the proposed simple-to-complex framework is useful in the EAE task. The code is available at https://github.com/zhangyx0417/ simple_to_complex.",
    "authors": [
        {
            "affiliations": [],
            "name": "Quzhe Huang"
        },
        {
            "affiliations": [],
            "name": "Yanxi Zhang"
        },
        {
            "affiliations": [],
            "name": "Dongyan Zhao"
        }
    ],
    "id": "SP:7c4055070798ae0f17dc72c2b64dd72b66a01217",
    "references": [
        {
            "authors": [
                "Yangyi Chen",
                "Lifan Yuan",
                "Ganqu Cui",
                "Zhiyuan Liu",
                "Heng Ji."
            ],
            "title": "A close look into the calibration of pre-trained language models",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Morris H. Degroot",
                "Stephen E. Fienberg."
            ],
            "title": "The comparison and evaluation of forecasters",
            "venue": "Journal of the Royal Statistical Society: Series D (The Statistician), 32(1-2):12\u201322.",
            "year": 1983
        },
        {
            "authors": [
                "Shrey Desai",
                "Greg Durrett."
            ],
            "title": "Calibration of pre-trained transformers",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 295\u2013302, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Xinya Du",
                "Claire Cardie."
            ],
            "title": "Event extraction by answering (almost) natural questions",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 671\u2013683, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Xinya Du",
                "Heng Ji."
            ],
            "title": "Retrieval-augmented generative question answering for event argument extraction",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4649\u20134666, Abu Dhabi, United Arab Emirates.",
            "year": 2022
        },
        {
            "authors": [
                "Xinya Du",
                "Sha Li",
                "Heng Ji."
            ],
            "title": "Dynamic global memory for document-level argument extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5264\u20135275, Dublin, Ireland. As-",
            "year": 2022
        },
        {
            "authors": [
                "Seth Ebner",
                "Patrick Xia",
                "Ryan Culkin",
                "Kyle Rawlins",
                "Benjamin Van Durme."
            ],
            "title": "Multi-sentence argument linking",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8057\u20138077, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q. Weinberger."
            ],
            "title": "On calibration of modern neural networks",
            "venue": "Proceedings of the 34th International Conference on Machine Learning - Volume 70, page 1321\u20131330, Sydney, NSW, Australia. JMLR.org.",
            "year": 2017
        },
        {
            "authors": [
                "Rujun Han",
                "I-Hung Hsu",
                "Jiao Sun",
                "Julia Baylon",
                "Qiang Ning",
                "Dan Roth",
                "Nanyun Peng."
            ],
            "title": "ESTER: A machine reading comprehension dataset for reasoning about event semantic relations",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Norman Mu",
                "Ekin Dogus Cubuk",
                "Barret Zoph",
                "Justin Gilmer",
                "Balaji Lakshminarayanan"
            ],
            "title": "Augmix: A simple method to improve robustness and uncertainty under data shift",
            "venue": "In International Conference on Learning Representa-",
            "year": 2020
        },
        {
            "authors": [
                "Kuan-Hao Huang",
                "I-Hung Hsu",
                "Prem Natarajan",
                "KaiWei Chang",
                "Nanyun Peng."
            ],
            "title": "Multilingual generative language models for zero-shot crosslingual event argument extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Ruihong Huang",
                "Ellen Riloff."
            ],
            "title": "Modeling textual cohesion for event extraction",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 26(1):1664\u20131670.",
            "year": 2021
        },
        {
            "authors": [
                "Heng Ji",
                "Ralph Grishman."
            ],
            "title": "Refining event extraction through cross-document inference",
            "venue": "Proceedings of ACL-08: HLT, pages 254\u2013262, Columbus, Ohio. Association for Computational Linguistics.",
            "year": 2008
        },
        {
            "authors": [
                "Jaeyoung Kim",
                "Dongbin Na",
                "Sungchul Choi",
                "Sungbin Lim."
            ],
            "title": "Bag of tricks for in-distribution calibration of pretrained transformers",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 551\u2013563, Dubrovnik, Croatia. Associa-",
            "year": 2023
        },
        {
            "authors": [
                "Qi Li",
                "Heng Ji",
                "Yu Hong",
                "Sujian Li."
            ],
            "title": "Constructing information networks using one single model",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1846\u20131851, Doha, Qatar. Associa-",
            "year": 2014
        },
        {
            "authors": [
                "Qi Li",
                "Heng Ji",
                "Liang Huang."
            ],
            "title": "Joint event extraction via structured prediction with global features",
            "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 73\u201382, Sofia, Bulgaria.",
            "year": 2013
        },
        {
            "authors": [
                "Sha Li",
                "Heng Ji",
                "Jiawei Han."
            ],
            "title": "Document-level event argument extraction by conditional generation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Alexandru Niculescu-Mizil",
                "Rich Caruana."
            ],
            "title": "Predicting good probabilities with supervised learning",
            "venue": "Proceedings of the 22nd International Conference on Machine Learning, page 625\u2013632, New York, NY, USA. Association for Computing Machinery.",
            "year": 2005
        },
        {
            "authors": [
                "Mahdi Pakdaman Naeini",
                "Gregory Cooper",
                "Milos Hauskrecht."
            ],
            "title": "Obtaining well calibrated probabilities using bayesian binning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 29(1).",
            "year": 2015
        },
        {
            "authors": [
                "Seo Yeon Park",
                "Cornelia Caragea."
            ],
            "title": "On the calibration of pre-trained language models using mixup guided by area under the margin and saliency",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2022
        },
        {
            "authors": [
                "Gabriel Pereyra",
                "George Tucker",
                "Jan Chorowski",
                "Lukasz Kaiser",
                "Geoffrey Hinton"
            ],
            "title": "Regularizing neural networks by penalizing confident output distributions",
            "year": 2017
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Peng Shi",
                "Jimmy Lin."
            ],
            "title": "Simple BERT Models for Relation Extraction and Semantic Role Labeling",
            "venue": "arXiv e-prints, page arXiv:1904.05255.",
            "year": 2019
        },
        {
            "authors": [
                "Beth M. Sundheim."
            ],
            "title": "Overview of the fourth Message Understanding Evaluation and Conference",
            "venue": "Fourth Message Understanding Conference (MUC4): Proceedings of a Conference Held in McLean, Virginia, June 16-18, 1992.",
            "year": 1992
        },
        {
            "authors": [
                "MeiHan Tong",
                "Bin Xu",
                "Shuai Wang",
                "Meihuan Han",
                "Yixin Cao",
                "Jiangqi Zhu",
                "Siyu Chen",
                "Lei Hou",
                "Juanzi Li."
            ],
            "title": "DocEE: A large-scale and finegrained benchmark for document-level event extraction",
            "venue": "Proceedings of the 2022 Conference of the",
            "year": 2022
        },
        {
            "authors": [
                "Xi Xiangyu",
                "Wei Ye",
                "Shikun Zhang",
                "Quanxiu Wang",
                "Huixing Jiang",
                "Wei Wu."
            ],
            "title": "Capturing event argument interaction via a bi-directional entity-level recurrent decoder",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Runxin Xu",
                "Peiyi Wang",
                "Tianyu Liu",
                "Shuang Zeng",
                "Baobao Chang",
                "Zhifang Sui"
            ],
            "title": "A two-stream",
            "year": 2022
        },
        {
            "authors": [
                "Yuqing Yang",
                "Qipeng Guo",
                "Xiangkun Hu",
                "Yue Zhang",
                "Xipeng Qiu",
                "Zheng Zhang."
            ],
            "title": "An AMRbased link prediction approach for document-level event argument extraction",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational",
            "year": 2023
        },
        {
            "authors": [
                "Xi Ye",
                "Greg Durrett."
            ],
            "title": "Can explanations be useful for calibrating black box models? In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6199\u20136212, Dublin, Ireland",
            "venue": "Association for",
            "year": 2022
        },
        {
            "authors": [
                "Shujian Zhang",
                "Chengyue Gong",
                "Eunsol Choi."
            ],
            "title": "Knowing more about questions can help: Improving calibration in question answering",
            "venue": "Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 1958\u20131970, Online. Association",
            "year": 2021
        },
        {
            "authors": [
                "Tianran Zhang",
                "Muhao Chen",
                "Alex A.T. Bui."
            ],
            "title": "Diagnostic prediction with sequence-of-sets representation learning for clinical events",
            "venue": "Artificial Intelligence in Medicine, pages 348\u2013358, Cham. Springer International Publishing.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Document-level Event Argument Extraction (EAE) aims at identifying the participants of multiple events from a document and classifying them into proper roles (Li et al., 2021; Du et al., 2022; Xu et al., 2022; Huang et al., 2022; Yang et al., 2023). Understanding events in documents is crucial for a line of downstream tasks, such as machine reading comprehension (Han et al., 2021) and dialogue systems (Zhang et al., 2020).\n\u2217 Equal Contribution.\nGeneration-based document-level EAE methods are widely used in recent works (Li et al., 2021; Du et al., 2022; Du and Ji, 2022; Huang et al., 2022). Among them, one line of studies (Li et al., 2021; Huang et al., 2022) treats each event independently and ignores the underlying correlations between events in real-world documents. Other works (Du et al., 2022; Du and Ji, 2022) start to consider inter-event dependencies and model them by introducing the idea of \u201cmemory\u201d, where event predictions (e.g., arguments, roles) are cached and can be retrieved to help the prediction of the upcoming events in a document. However, since these methods still use front-to-back prediction\u2014extracting events according to their appearance order in a document, an event can only rely on the predictions of events that appeared before it. Besides, the prediction of an event is cached regardless of its quality, whereas false predictions may be cached first, misleading the prediction of the following events.\nIn general, using current retrieval-based methods to model inter-event dependencies faces two main challenges: (1) front-to-back prediction only partially models inter-event dependencies, where the dependency links from an event to all the events that appeared after it are ignored; (2) incorrect predictions may be cached first and retrieved by the upcoming events.\nConsidering the challenges, we propose the simple-to-complex framework. First, we calculate the difficulty of each event, where the difficulty is defined as the average probability that the model assigns to the arguments of an event. We also calibrate the argument probabilities before using them to ensure they truly reflect how certain the model is on each argument. Second, we reorder events in a document from simple to complex and predict them accordingly. In this way, the model could use the simple instance to help the prediction of the difficult ones, no matter whether the simple events appear before or after the difficult ones in\nthe original document. We conduct experiments on widely used benchmarks WIKIEVENTS(Li et al., 2021), and our proposed simple-to-complex framework outperforms the previous SOTA by 1.4% in F1, illustrating the effectiveness of our method. Further analyses show the calibration of probability is very important when calculating the difficulty of different events and the success of our framework relies on the better usage of dependency between an event and the events that appear after it in the document."
        },
        {
            "heading": "2 Task Definition",
            "text": "In this work, we focus on document-level Informative Argument Extraction1 (IAE) (Li et al., 2021), where informative arguments are far more distant than local/uninformative ones and provide more useful information about an event. We formulate document-level IAE as a generative template-filling task following Li et al. (2021) and Du et al. (2022). Given a document D with triggers marked (using a special token <tgr>), our goal is to extract all the arguments of E to fill in the slots of the event template T .\nEach event consists of (1) an event trigger, which has a specific type E (we use E to represent an event); (2) a series of event arguments, each corresponding to a specific role. In the event ontology, event types and argument roles are pre-defined, and event templates depicting the relationship between the argument roles of an event are also provided. For example, the template for E = Attack in the KAIROS ontology2 is:\n<arg1> attacked <arg2> using\n<arg3> at <arg4> place\nwhere each slot <argx> is a placeholder for arguments with a specific role. We replace all the <argx>s in a template with a special token <arg> before using them as input. If the model extracts an argument, then <arg> will be replaced by the argument. If no, the placeholder <arg> remains."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we illustrate our framework based on simple-to-complex prediction (Figure 1). First, we introduce our memory-enhanced IAE model\n1Name mentions are more informative than nominal mentions, and pronouns are the least informative.\n2https://www.ldc.upenn.edu/collaborations/ current-projects\n(Section 3.1). Here, we use retrieval to augment model input and apply constrained decoding to improve model output, both leveraging inter-event dependencies to benefit prediction. Second, we elaborate on how to define and calculate the difficulty of an event, and how to reorder events in a document from simple to complex for simple-tocomplex prediction (Section 3.2)."
        },
        {
            "heading": "3.1 Memory-Enhanced IAE Model",
            "text": "Our memory-enhanced IAE model is based on a generative model. When calculating the difficulty of each event as well as generating (the arguments of) events in a document from simple to complex, we use the same generative model. In this study, we assume that each event has a prediction order and events in a document are predicted according to that order. After reordering, the prediction order of an event may change and further change the retrieved prediction of that event.\nModel Input & Output The generation of an event E in a document D is conditioned on the (1) prediction order o \u2208 {1, 2, . . . , ne}: the order of predicting (the arguments of) E, where ne denotes the number of events in D; (2) event context c: the concatenation of E\u2019s context words (a continuous span in D close to E\u2019s trigger) and E\u2019s template; (3) retrieved prediction mR: the prediction of an event appeared before E retrieved from the document memory, a data structure that caches the predictions of already predicted events in D. To sum up, the input of event E for the model is:\n<s> mR </s> <s> T </s> x1, x2, . . . , xn [EOS]\nwhere x1, x2, . . . , xn are the context words and T is E\u2019s unfilled template, and these two parts form the event context c. The prediction of E is a filled template, with each <arg> placeholder replaced by the predicted argument (or not), as shown in Figure 1. If there are multiple arguments for the same slot, the arguments are connected with \u201cand\u201d.\nRetrieval-Augmented Generation In the input stage (both for training and testing), we augment our model with similarity-based retrieval following Du et al. (2022) to make it capable of finding argument mentions beyond the context of an event, especially informative ones (Li et al., 2021). When predicting the i-th event Ei in a document D, the snapshot of the document memory is m = {m1,m2, . . . ,mi\u22121}, where mk denotes the prediction of the k-th event. We calculate the cosine\nsimilarity between Ei\u2019s context ci and each prediction in m using S-BERT (Reimers and Gurevych, 2019) embeddings, and select the prediction with the highest score as additional input to help the prediction of Ei:\nscore(mj |ci) = exp f(ci,mj)\u2211\nmj\u2208m exp f(ci,mj) (1)\nf(ci,mj) = SBERT(ci) TSBERT(mj) (2)\nmRi = argmax mj score(mj |ci) (3)\nwhere SBERT() denotes S-BERT encoding, mRi denotes the retrieved prediction that Ei relies on.\nConstrained Decoding In the output stage, we introduce argument pair constraints following Du et al. (2022) to constrain the decoding of arguments with conflicting roles. For example, if the DETAINEE of event Ea is \u201cMike\u201d, then \u201cMike\u201d can not be decoded as the ATTACKER of another event Eb (happened after Ea), since \u201cMike\u201d is already in jail. Here, \u201cMike\u201d as DETAINEE and \u201cMike\u201d as ATTACKER is an argument pair constraint. However, once an incorrect prediction is used to constrain\nanother, it may cause more errors (Du et al., 2022). In the example above, \u201cMike\u201d will never be decoded as the ATTACKER of Eb once it is decoded as the DETAINEE of Ea, even if the prediction of DETAINEE is incorrect. To alleviate such error propagation, we disable the constraints when the model is not certain about the prediction of an argument. The certainty of an argument can be measured by the calibrated probability of decoding it. Low probability intuitively implies the model is not confident about this prediction, while we have also found that high probability (e.g. \u2265 0.8) corresponds to low prediction accuracy, which is shown in Figure 3. Therefore, we set both lower and upper bounds for argument probabilities to exclude possibly incorrect constraints, and we refer to our pruned constraints as bounded constraints. The heuristics of selecting bounds are discussed in Appendix A.3."
        },
        {
            "heading": "3.2 Simple-to-Complex Reordering",
            "text": "Since an event usually contains multiple arguments, we reckon the difficulty of an event lies in the average difficulty of predicting its arguments. In this\nsection, we will elaborate on how to calculate the difficulty of an event as well as how to reorder events in a document by the difficulty and predict them from simple to complex.\nSuppose the set of prediction orders of events in a document D be o = {o1, o2, . . . , one}, where oi represents the i-th appeared event is the oi-th to be predicted, then front-to-back prediction satisfies oi = i, i = 1, 2, . . . , ne. Suppose the probabilities of decoding the arguments of the i-th event Ei in a document D be:\npargi = { p (1) i , p (2) i , . . . , p (na) i } , (4)\nwhere na denotes the number of predicted arguments of Ei, and p (j) i denotes the probability that the generative model assigns to the j-th argument of the i-th event. The argument probability reflects the certainty of the generative model on predicting an argument, inversely proportional to our desired difficulty. Thus, we define the difficulty of predicting the arguments of Ei as:\ndargi = { d (1) i , d (2) i , . . . , d (na) i } , (5)\nd (j) i = 1\u2212 p (j) i , j = 1, 2, . . . , ne, (6)\nwhere d(j)i denotes the difficulty of predicting the j-th argument of the i-th event. The difficulty of Ei is defined as the average difficulty of predicting its arguments, so we take the average of dargi and obtain the difficulty of Ei:\ndevti = mean(d arg i ). (7)\nIf no arguments of Ei are predicted, then devti = 2. That means Ei will be placed to the rear, since it provides no arguments/roles that might benefit prediction. After calculating the difficulty of events in D, we obtain a new set of prediction orders o\u2032 = {o\u20321, o\u20322, . . . , o\u2032ne}. Then, we can predict (the arguments of) events in D from simple to complex according to o\u2032.\nThe method described above assumes that the model providing the probabilities is well-calibrated, where confidence (the probability that a model assigns to a prediction) equals or nearly equals accuracy (the real correctness of a prediction). In other words, high confidence corresponds to high accuracy, and vice versa. However, some studies reveal that current Deep Neural Networks (DNNs) are prone to over-confidence, which implies that the model\u2019s confidence is not reliable (Guo et al.,\n2017). We have also found similar problems in our model, which is discussed in Section 5.1. Therefore, we should calibrate these probabilities before using them for our simple-to-complex reordering. Specifically, we adopt temperature scaling (Guo et al., 2017; Desai and Durrett, 2020), a simple and effective method for calibration. In this work, the temperature T is selected by minimizing the Expected Calibration Error (ECE) (Pakdaman Naeini et al., 2015) on the validation set, and we denote the temperature with the lowest ECE as T \u2032.\nAccounting for calibration, there should be a revision on the calculation of p(j)i . Suppose the logits vector of the j-th predicted argument of the i-th event be z(j)i , then:\np (j) i = max\nk softmax\n( z (j) i\nT \u2032 ) k , (8)\nwhere k traverses each dimension of z(j)i ."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Dataset and Evaluation Metrics",
            "text": "We evaluate our framework on WIKIEVENTS (Li et al., 2021) as it annotates all the events in a document (averagely 16 events per document), while existing document-level datasets such as DocEE (Tong et al., 2022), RAMS (Ebner et al., 2020) and MUC-4 (Sundheim, 1992) only annotate at most 3 events per document. Also, it provides complete coreference annotation for document-level IAE. Its statistics are shown in Table 1.\nWe measure the Argument Identification (Arg-I) and Argument Classification (Arg-C) capabilities of our model following Li et al. (2013). If an argument span matches any of the gold informative arguments of the event, the argument is correctly identified. If the semantic role also matches, the\nModels Argument Identification (Arg-I) Argument Classification (Arg-C)\nHead Match Coref Match Head Match Coref Match\nP R F1 P R F1 P R F1 P R F1\nBERT-CRF - - 52.71\u2020 - - 58.12\u2020 - - 43.29\u2020 - - 47.70\u2020\nBART-Gen 58.62 55.64 57.09\u2020 62.84 59.64 61.19\u2020 54.02 51.27 52.61\u2020 57.47 54.55 55.97\u2020\nw/ M 60.38 57.97 59.15 64.72 62.14 63.40 54.53 52.36 53.42 58.11 55.80 56.93 w/ M+C 61.79 58.88 60.30 66.16 63.04 64.56 55.70 53.08 54.36 59.32 56.52 57.88\nargument is considered correctly classified. Following previous studies on document-level IAE (Li et al., 2021; Du et al., 2022), we adopt Head Word Match (Head F1) (Huang and Riloff, 2021) and Coreferential Match (Coref F1) (Ji and Grishman, 2008) to judge whether the predicted argument span matches the gold argument span. Head Word Match demands the first word of the predicted argument to match that of the gold argument, while Coreferential Match only needs the extracted argument to be coreferential with the gold argument. We report the micro-P/R/F1 averaged on three different seeds."
        },
        {
            "heading": "4.2 Baselines",
            "text": "We compare our framework with a series of competitive baselines: (1) BERT-CRF (Shi and Lin, 2019), a simple BERT-based model without incorporating lexical or syntactic features for argument identification and classification. (2) BART-Gen (Li et al., 2021), a conditional neural text generation model that generates a filled template for each event given the event template and context words. (3) BART-Gen (w/ M+C) (Du et al., 2022), a framework based on BART-Gen, which utilizes retrieval to augment model input and constructs argument pair constraints for decoding. It is the SOTA model on document-level IAE, but still extracts events according to their appearance order in the document. We also report the results of BARTGen (w/ M) for comparison."
        },
        {
            "heading": "4.3 Main Results",
            "text": "The main results for document-level IAE are presented in Table 2. From the results, we can conclude that:\n\u2022 Our S2C-CD model outperforms all previous\nmethods on WIKIEVENTS as to documentlevel IAE, with an average gain of 1.4% in F1 on all four settings.\n\u2022 All models augmented with retrieval (i.e., w/ M) perform better compared with BERT-CRF and raw BART-Gen, showing the importance of modeling inter-event dependencies.\n\u2022 Compared to BART-Gen (w/ M), the addition of simple-to-complex reordering (S2C model) greatly improves F1, where F1 on average increases by 1.34 in Arg-I and 1.42 in Arg-C. This improvement can be mainly attributed to our simple-to-complex prediction paradigm, since it allows more inter-event dependency links, i.e., from an event to all the events that appeared after it.\n\u2022 After applying our bounded constraints (S2CCD model), there is an additional improvement in P and F1, which shows that incorrect constraints are effectively pruned."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Is Calibration Necessary?",
            "text": "What to Calibrate? For each argument, we focus on its first token probability, and this probability is what we aim to calibrate. The reason for using the first token probabilities is that the generation of the remaining tokens is highly dependent on the first token. As shown in Figure 2, 87% of the non-first token probabilities are \u2265 0.9, while first token probabilities are better distributed, with only 53% of them \u2265 0.9.\nWhy to Calibrate? Modern DNNs are prone to over-confidence, which implies that the model\u2019s\nconfidence is not reliable (Guo et al., 2017). We have also found similar problems in our model. As shown in Figure 2, the first and non-first token probability distribution both exhibit a severe overconfidence phenomenon before calibration, with most probabilities \u2265 0.9. This suggests that the model tends to assign a high (i.e., \u2192 1) probability to nearly all of the arguments, which can not truly reflect how sure the model is of each argument. Also, over-confidence leads to miscalibration, which is reflected in the reliability diagram in Figure 3. The reliability diagram plots the relation between confidence and accuracy, and its definition is discussed in Appendix A.2. As shown in Figure 3, most points on the orange curve (before calibration) are far from the zero error curve where confidence exactly equals accuracy, demonstrating that uncalibrated probabilities are unreliable. After calibration, the first token probability distribution becomes flat (Figure 2) and calibrated (Figure 3). Therefore, we should calibrate probabilities (con-\nfidence) to align them with accuracy before using them for our simple-to-complex reordering.\nInfluence of Uncalibrated Probabilities We conduct an ablation study to further explore what will happen if we reorder events using uncalibrated probabilities. As shown in Table 3, if we order events by uncalibrated probabilities, F1 is only comparable to excluding simple-to-complex reordering from our S2C model. The performance is maximized only when the probabilities are calibrated. Therefore, calibration is essential."
        },
        {
            "heading": "5.2 Difficulty Calculation Needs Memory?",
            "text": "In this section, we present two possible ways of calculating the difficulty of an event to explore their impact on performance. In Figure 4, we define the first inference as step 1-2 (calculating the difficulty), and the second inference as step 3 (predicting the arguments of reordered events).\nThe first way is utilized in our framework, where we use the same model for both inferences. When calculating the difficulty at the first inference, we also use retrieval. There are two reasons for this. On the one hand, the model input is augmented with retrieval during training, so the input/prompt\nformat should be consistent during testing. On the other hand, the model is trained to predict the arguments of an upcoming event conditioned on the prediction of an already predicted event. Therefore, we should calculate \u201cthe difficulty of an event conditioned on the prediction of an earlier predicted event\u201d. However, the retrieved predictions of the same event at both inferences are usually different.\nThe second way is to use separate models for each inference. At the first inference, we use a model trained without retrieval to calculate the \u201craw\u201d difficulty of an event (i.e., do not condition on the prediction of an earlier predicted event). At the second inference, we train a retrieval-augmented model. This way removes the possible influence of retrieval on calculating the difficulty of an event, but the training overhead doubles.\nWe conduct an experiment to compare these two ways, as shown in Table 4. R1 and R2 represent one model and two models, respectively. R1 is comparable to R2 in Arg-I, while R1 is notably better than R2 in Arg-C. The results suggest that using one model is generally better as to performance, so we should calculate the difficulty of an event conditioned on the prediction of an earlier predicted event. Besides, using one model is more time-efficient."
        },
        {
            "heading": "5.3 Influence of Bounded Constraints",
            "text": "In this section, we first compare our bounded constraints with those presented in Du et al. (2022), then analyze the impact of the lower and upper bounds individually.\nIn Table 5, we observe that when applying the original constraints (Du et al., 2022), the model\nperforms only comparably with our S2C model. This implies the number of correct and incorrect constraints is nearly equal when we predict events from simple to complex. By contrast, our bounded constraints perform well, suggesting that the number of incorrect constraints is indeed reduced and the correct constraints are (mostly) kept.\nUsing the steps presented in Appendix A.3, we obtain the lower bound 0.5 and the upper bound 0.8, so we will disable a constraint if the probability of decoding the argument is \u2264 0.5 or \u2265 0.8. Based on this, we individually analyze the influence of the lower and upper bounds, as shown in Table 6. We have found that whether we remove the lower bound or the upper bound, the performance drops, indicating that both bounds are useful for reducing the number of incorrect constraints."
        },
        {
            "heading": "5.4 Case Study",
            "text": "In the case presented in Figure 5, we would like to predict the arguments of event E = Damage, which describes the mental damage that Dzhokhar Tsarnaev brought to the victims as a bomber. In\nfront-to-back prediction, E can only access the predictions of earlier appeared events and retrieves E1\u2019s prediction as additional input. The death of Dzhokhar Tsarnaev in E1 happened after E, wrongly restricting the prediction of \u201cDzhokhar Tsarnaev\u201d as the DAMAGER of E. By contrast, with our simple-to-complex prediction, E has the chance to rely on the INJURER argument of a later appeared event E2 and obtain the correct DAMAGER argument \u201cDzhokhar Tsarnaev\u201d. E2 describes that Dzhokhar Tsarnaev injured 264 people in the bombing as the INJURER, thus bringing mental damage to them as the DAMAGER.\nComparing these two prediction paradigms, we find that simple-to-complex prediction is better, mainly because it allows more inter-event dependency links. In this example, it is intuitive that E is more similar to E2, since they respectively depict the physical damage and mental damage Dzhokhar Tsarnaev brought to the victims. However, the dependency link from E to E2 is disabled when predicting events from front to back."
        },
        {
            "heading": "5.5 Error Analysis",
            "text": "Table 7 summarizes the error types of our S2C-CD model. The errors mainly come from the inability to recognize an argument span (around half), while only about 8% of identified arguments are assigned incorrect semantic roles. Therefore, identifying the argument span is more important than assigning a more accurate role to already extracted arguments."
        },
        {
            "heading": "6 Related Work",
            "text": ""
        },
        {
            "heading": "6.1 Document-level EAE",
            "text": "Unlike sentence-level EAE (Li et al., 2014; Du and Cardie, 2020; Xiangyu et al., 2021), events\nand their participants usually spread across the document in document-level EAE. We focus on document-level IAE (Li et al., 2021) in this work, which is more practical but more challenging. Li et al. (2021) constructed the WIKIEVENTS dataset and pioneered the research on document-level IAE. Compared with existing document-level EAE datasets such as DocEE (Tong et al., 2022), RAMS (Ebner et al., 2020) and MUC-4 (Sundheim, 1992) that only annotate at most 3 events per document, WIKIEVENTS annotates all the events in a document, with an average of 16 events per document. Also, it provides complete coreference annotation for evaluating document-level IAE.\nRecently, generation-based methods have been proposed for document-level EAE. Among them, template generation-based approaches (Li et al., 2021; Huang et al., 2022; Du et al., 2022) are widely utilized. BART-Gen (Li et al., 2021) conditioned generation on event templates and context words but considered each event independently. Further, Du et al. (2022); Du and Ji (2022) introduced the idea of \u201cmemory\u201d to document-level EAE, where predictions of already predicted events were utilized as additional input. Although these methods can model inter-event dependencies to some extent, they ignore the dependency links from an event to all the events that appeared after it in a document. Besides, uncertain/false event predic-\ntions may be cached first and retrieved by future events, misleading their prediction."
        },
        {
            "heading": "6.2 Confidence Calibration",
            "text": "Studies on the calibration of natural language models have been drawing attention recently (Desai and Durrett, 2020; Park and Caragea, 2022; Kim et al., 2023). Among modern calibration approaches, temperature scaling is a simple and effective method (Desai and Durrett, 2020) which can produce low ECE (Guo et al., 2017; Chen et al., 2023). Due to its low time overhead and low ECE property, we adopt it in our work. Other works focus on methods such as label smoothing (Pereyra et al., 2017) and data augmentation (Hendrycks* et al., 2020), but these methods cannot produce as low ECE as temperature scaling (Chen et al., 2023). More recent studies started to treat calibration as an additional task, which needs collecting data and training extra models (Ye and Durrett, 2022; Zhang et al., 2021). In order to reduce time overhead, we do not consider these approaches."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we propose the idea of simple-tocomplex prediction for events in a document, where events in a document are reordered from simple to complex and predicted accordingly. Besides, we introduce retrieval to augment model input and apply constrained decoding to improve model output. Empirical results and analysis demonstrate that our best model outperforms prior methods by a notable margin and our simple-to-complex prediction is beneficial since it allows more inter-event dependency links, i.e., from an event to all the events appeared after it.\nLimitations\nFirstly, our framework requires two inference processes, where the first inference is to calculate the difficulty of events in a document and the second inference is to predict the arguments of these events from simple to complex. Secondly, the way of setting lower/upper bounds is a hard pruning strategy that disables constraints where the argument probability is too low/high. However, this strategy rigidly excludes constraints for which the model is not sufficiently certain or less reliable, without really taking into account the wrong constraints caused by the incorrectly predicted arguments. We leave the problems for future work."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by the National Key R&D Program of China (No. 2021YFC3340304). We would like to thank the anonymous reviewers for their helpful comments. We would like to express appreciation to Yansong Feng for his insightful suggestions on our idea."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Hyperparameters The hyperparameters used in our experiments are shown in Table 8.\nA.2 Confidence Calibration Confidence & Accuracy Confidence is defined as the probability that a model assigns to a prediction, while accuracy is the real correctness of a prediction. In the classification task, \u201cprediction\u201d means the predicted class of a specific instance. In our work, \u201cprediction\u201d means a specific argument.\nConfidence Calibration The goal of confidence calibration is to align the model\u2019s posterior probabilities (confidence) with empirical likelihoods (accuracy) (Guo et al., 2017). For example, if we take 100 instances where the model\u2019s prediction receives a posterior probability of 0.8, the model should get 80 of the instances correct.\nReliability Diagrams Usually, calibration is visualized by reliability diagrams (Degroot and Fienberg, 1983; Niculescu-Mizil and Caruana, 2005). Reliability diagrams treat expected accuracy as a function of model confidence. To draw the reliability diagram, we usually partition predictions into k disjoint, equally-sized bins {B1, B2, . . . , Bk}, and calculate the average confidence/accuracy in each bin as an approximation. In this paper, we set k = 10.\nEvaluation Metrics We use Expected Calibration Error (ECE) (Pakdaman Naeini et al., 2015) in this work. ECE is the weighted average of all the bins\u2019 confidence/accuracy difference in the reliability diagram:\nECE = k\u2211\ni=1\n|Bi| n |acc(Bi)\u2212 conf(Bi)| (9)\nwhere n denotes the number of predictions. ECE measures how calibrated the model is. The smaller the ECE, the more calibrated the model is.\nA.3 The Heuristics of Selecting Bounds We select the lower and upper bounds of the argument probabilities according to the probability distribution (Figure 2) and the reliability diagram (Figure 3). The steps are as follows:\n1. Firstly, calculate the median Nmed of the number of probabilities in each interval (e.g., [0.9, 1.0]) of the calibrated probability distribution (Figure 2). If the number of probabilities in an interval is \u2265 Nmed, then it is selected as a candidate interval.\n2. Secondly, merge all candidate intervals to form a continuous interval I . If there are multiple intervals, then prune intervals with poorer calibration (e.g., [0.8, 1.0]) according to the reliability diagram (Figure 3), and only keep one (denoted as I).\n3. Finally, prune the less calibrated part of I .\nFollowing these steps, we present the process of selecting bounds in our work below. First, we can calculate Nmed = 60.5 according to Figure 2. After merging candidate intervals, we obtain a continuous interval I = [0.3, 0.8]. Then, we should prune [0.3, 0.5] because probabilities in this interval are less calibrated with low accuracy (Figure 3). To sum up, our final interval is [0.5, 0.8]."
        }
    ],
    "title": "From Simple to Complex: A Progressive Framework for Document-level Informative Argument Extraction",
    "year": 2023
}