{
    "abstractText": "News articles are driven by the informational sources journalists use in reporting. Modeling when, how and why sources get used together in stories can help us better understand the information we consume and even help journalists with the task of producing it. In this work, we take steps toward this goal by constructing the largest and widest-ranging annotated dataset, to date, of informational sources used in news writing. We first show that our dataset can be used to train high-performing models for information detection and source attribution. Then, we introduce a novel task, source prediction, to study the compositionality of sources in news articles \u2013 i.e. how they are chosen to complement each other. We show good modeling performance on this task, indicating that there is a pattern to the way different sources are used together in news storytelling. This insight opens the door for a focus on sources in narrative science (i.e. planningbased language generation) and computational journalism (i.e. a source-recommendation system to aid journalists writing stories).1",
    "authors": [
        {
            "affiliations": [],
            "name": "Alexander Spanghera"
        },
        {
            "affiliations": [],
            "name": "Nanyun Pengc"
        },
        {
            "affiliations": [],
            "name": "Jonathan Maya"
        },
        {
            "affiliations": [],
            "name": "Emilio Ferraraa"
        }
    ],
    "id": "SP:8ac8bcf9bf9de1dcf556098b3a51d4a726816c17",
    "references": [
        {
            "authors": [
                "Ali Haif Abbas."
            ],
            "title": "Politicizing the pandemic: A schemata analysis of covid-19 news in two selected newspapers",
            "venue": "International Journal for the Semiotics of Law-Revue internationale de S\u00e9miotique juridique, 35(3):883\u2013902.",
            "year": 2022
        },
        {
            "authors": [
                "Eran Amsalem",
                "Yair Fogel-Dror",
                "Shaul R Shenhav",
                "Tamir Sheafer."
            ],
            "title": "Fine-grained analysis of diversity levels in the news",
            "venue": "Communication Methods and Measures, 14(4):266\u2013284.",
            "year": 2020
        },
        {
            "authors": [
                "David Bamman",
                "Brendan O\u2019Connor",
                "Noah A Smith"
            ],
            "title": "Learning latent personas of film characters",
            "venue": "In Proceedings of the 51st Annual Meeting",
            "year": 2013
        },
        {
            "authors": [
                "Bettina Berendt",
                "\u00d6zg\u00fcr Karadeniz",
                "Stefan Mertens",
                "Leen d\u2019Haenens"
            ],
            "title": "Fairness beyond \u201cequal\u201d: The diversity searcher as a tool to detect and enhance the representation of socio-political actors in news media",
            "venue": "In Companion Proceedings of the Web Con-",
            "year": 2021
        },
        {
            "authors": [
                "Ryan L Boyd",
                "Alexander Spangher",
                "Adam Fourney",
                "Besmira Nushi",
                "Gireeja Ranade",
                "James Pennebaker",
                "Eric Horvitz"
            ],
            "title": "Characterizing the internet research agency\u2019s social media operations during the 2016 us presidential election using linguistic analy",
            "year": 2018
        },
        {
            "authors": [
                "Dallas Card",
                "Justin Gross",
                "Amber Boydstun",
                "Noah A. Smith."
            ],
            "title": "Analyzing framing through the casts of characters in the news",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1410\u20131420, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "David Caswell."
            ],
            "title": "Structured journalism and the semantic units of news",
            "venue": "Digital Journalism, 7(8):1134\u20131156.",
            "year": 2019
        },
        {
            "authors": [
                "Prafulla Kumar Choubey",
                "Aaron Lee",
                "Ruihong Huang",
                "Lu Wang."
            ],
            "title": "Discourse as a function of event: Profiling discourse structure in news articles around the main event",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Costanza Conforti",
                "Mohammad Taher Pilehvar",
                "Nigel Collier."
            ],
            "title": "Towards automatic fake news detection: cross-level stance detection in news articles",
            "venue": "Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 40\u201349.",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Steffen Eger",
                "Johannes Daxenberger",
                "Iryna Gurevych."
            ],
            "title": "Neural end-to-end learning for computational argumentation mining",
            "venue": "arXiv preprint arXiv:1704.06104.",
            "year": 2017
        },
        {
            "authors": [
                "David K Elson",
                "Kathleen R McKeown."
            ],
            "title": "Automatic attribution of quoted speech in literary narrative",
            "venue": "Twenty-fourth AAAI conference on artificial intelligence.",
            "year": 2010
        },
        {
            "authors": [
                "James T Hamilton."
            ],
            "title": "All the news that\u2019s fit to sell",
            "venue": "All the News That\u2019s Fit to Sell. Princeton University Press.",
            "year": 2011
        },
        {
            "authors": [
                "Momchil Hardalov",
                "Arnav Arora",
                "Preslav Nakov",
                "Isabelle Augenstein."
            ],
            "title": "A survey on stance detection for mis-and disinformation identification",
            "venue": "arXiv preprint arXiv:2103.00242.",
            "year": 2022
        },
        {
            "authors": [
                "Tiancheng Hu",
                "Manoel Horta Ribeiro",
                "Robert West",
                "Andreas Spitz."
            ],
            "title": "Quotatives indicate decline in objectivity in us political news",
            "venue": "arXiv preprint arXiv:2210.15476.",
            "year": 2022
        },
        {
            "authors": [
                "Haozhe Ji",
                "Minlie Huang."
            ],
            "title": "Discodvt: Generating long text with discourse-aware discrete variational transformer",
            "venue": "arXiv preprint arXiv:2110.05999.",
            "year": 2021
        },
        {
            "authors": [
                "Armand Joulin",
                "Edouard Grave",
                "Piotr Bojanowski",
                "Tomas Mikolov."
            ],
            "title": "Bag of tricks for efficient text classification",
            "venue": "arXiv preprint arXiv:1607.01759.",
            "year": 2016
        },
        {
            "authors": [
                "Yuval Kirstain",
                "Ori Ram",
                "Omer Levy."
            ],
            "title": "Coreference resolution without span representations",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Andrea Masini",
                "Peter Van Aelst",
                "Thomas Zerback",
                "Carsten Reinemann",
                "Paolo Mancini",
                "Marco Mazzoni",
                "Marco Damiani",
                "Sharon Coen"
            ],
            "title": "Measuring and explaining the diversity of voices and viewpoints in the news: A comparative study",
            "year": 2018
        },
        {
            "authors": [
                "Jesse Mu",
                "Xiang Lisa Li",
                "Noah Goodman."
            ],
            "title": "Learning to compress prompts with gist tokens",
            "venue": "arXiv preprint arXiv:2304.08467.",
            "year": 2023
        },
        {
            "authors": [
                "Grace Muzny",
                "Michael Fang",
                "Angel Chang",
                "Dan Jurafsky."
            ],
            "title": "A two-stage sieve approach for quote attribution",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Pa-",
            "year": 2017
        },
        {
            "authors": [
                "Edward Newell",
                "Drew Margolin",
                "Derek Ruths."
            ],
            "title": "An attribution relations corpus for political news",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).",
            "year": 2018
        },
        {
            "authors": [
                "Joakim Nivre."
            ],
            "title": "Dependency parsing",
            "venue": "Language and Linguistics Compass, 4(3):138\u2013152.",
            "year": 2010
        },
        {
            "authors": [
                "Shon Otmazgin",
                "Arie Cattan",
                "Yoav Goldberg."
            ],
            "title": "Lingmess: Linguistically informed multi expert scorers for coreference resolution",
            "venue": "arXiv preprint arXiv:2205.12644.",
            "year": 2022
        },
        {
            "authors": [
                "Tim O\u2019Keefe",
                "James R Curran",
                "Peter Ashwell",
                "Irena Koprinska"
            ],
            "title": "An annotated corpus of quoted opinions in news articles",
            "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics",
            "year": 2013
        },
        {
            "authors": [
                "Tim O\u2019Keefe",
                "Silvia Pareti",
                "James R Curran",
                "Irena Koprinska",
                "Matthew Honnibal"
            ],
            "title": "A sequence labelling approach to quote attribution",
            "year": 2012
        },
        {
            "authors": [
                "Sebastian Pad\u00f3",
                "Andr\u00e9 Blessing",
                "Nico Blokker",
                "Erenay Dayan\u0131k",
                "Sebastian Haunss",
                "Jonas Kuhn."
            ],
            "title": "Who sides with whom? towards computational construction of discourse networks for political debates",
            "venue": "Proceedings of the 57th Annual",
            "year": 2019
        },
        {
            "authors": [
                "Silvia Pareti",
                "Tim O\u2019keefe",
                "Ioannis Konstas",
                "James R Curran",
                "Irena Koprinska"
            ],
            "title": "Automatically detecting and attributing indirect quotations",
            "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2013
        },
        {
            "authors": [
                "Dario Pavllo",
                "Tiziano Piccardi",
                "Robert West."
            ],
            "title": "Quootstrap: Scalable unsupervised extraction of quotation-speaker pairs from large news corpora via bootstrapping",
            "venue": "Twelfth International AAAI Conference on Web and Social Media.",
            "year": 2018
        },
        {
            "authors": [
                "Jeroen Peperkamp",
                "Bettina Berendt."
            ],
            "title": "Diversity checker: Toward recommendations for improving journalism with respect to diversity",
            "venue": "Adjunct Publication of the 26th Conference on User Modeling, Adaptation and Personalization, pages 35\u201341.",
            "year": 2018
        },
        {
            "authors": [
                "Will Radford",
                "Daniel Tse",
                "Joel Nothman",
                "Ben Hachey",
                "George Wright",
                "James R Curran",
                "Will Cannings",
                "Tim O\u2019Keefe",
                "Matthew Honnibal",
                "David Vadas"
            ],
            "title": "The computable news project: research in the newsroom",
            "year": 2015
        },
        {
            "authors": [
                "Evan Sandhaus."
            ],
            "title": "The new york times annotated corpus",
            "venue": "Linguistic Data Consortium, Philadelphia, 6(12):e26752.",
            "year": 2008
        },
        {
            "authors": [
                "Niraj Sitaula",
                "Chilukuri K Mohan",
                "Jennifer Grygiel",
                "Xinyi Zhou",
                "Reza Zafarani."
            ],
            "title": "Credibilitybased fake news detection",
            "venue": "Disinformation, Misinformation, and Fake News in Social Media, pages 163\u2013182. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Spangher",
                "Divya Choudhary."
            ],
            "title": "If it bleeds, it leads: A computational approach to covering crime in los angeles",
            "venue": "arXiv preprint arXiv:2206.07115.",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Spangher",
                "Xinyu Hua",
                "Yao Ming",
                "Nanyun Peng."
            ],
            "title": "Sequentially controlled text generation",
            "venue": "arXiv preprint arXiv:2301.02299.",
            "year": 2023
        },
        {
            "authors": [
                "Alexander Spangher",
                "Nanyun Peng",
                "Jonathan May",
                "Emilio Ferrara."
            ],
            "title": "Don\u2019t quote me on that\u201d: Finding mixtures of sources in news articles",
            "venue": "arXiv preprint arXiv:2104.09656.",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Spangher",
                "Gireeja Ranade",
                "Besmira Nushi",
                "Adam Fourney",
                "Eric Horvitz."
            ],
            "title": "Characterizing search-engine traffic to internet research agency web properties",
            "venue": "Proceedings of The Web Conference 2020, pages 2253\u20132263.",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Spangher",
                "Xiang Ren",
                "Jonathan May",
                "Nanyun Peng."
            ],
            "title": "Newsedits: A news article revision dataset and a novel document-level reasoning challenge",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Kobie Van Krieken."
            ],
            "title": "Story character, news source or vox pop? representations and roles of citizen perspectives in crime news narratives",
            "venue": "Journalism, 23(9):1975\u20131994.",
            "year": 2022
        },
        {
            "authors": [
                "Timote Vaucher",
                "Andreas Spitz",
                "Michele Catasta",
                "Robert West."
            ],
            "title": "Quotebank: a corpus of quotations from a decade of news",
            "venue": "Proceedings of the 14th ACM International Conference on Web Search and Data Mining, pages 328\u2013336.",
            "year": 2021
        },
        {
            "authors": [
                "Lili Yao",
                "Nanyun Peng",
                "Ralph Weischedel",
                "Kevin Knight",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Planand-write: Towards better automatic storytelling",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7378\u20137385.",
            "year": 2019
        },
        {
            "authors": [
                "Caleb Ziems",
                "William Held",
                "Omar Shaikh",
                "Jiaao Chen",
                "Zhehao Zhang",
                "Diyi Yang"
            ],
            "title": "Can large language models transform computational social science? arXiv preprint arXiv:2305.03514",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Journalism informs our worldviews; news itself is informed by the sources reporters use. Identifying sources of information in a news article is relevant to many tasks in NLP: misinformation detection (Hardalov et al., 2022), argumentation (Eger et al., 2017) and news discourse (Choubey et al., 2020).\nAttributing information to sources is challenging: as shown in Table 1, while some attributions are identified via lexical cues (e.g. \u201csaid\u201d), others are deeply implicit (e.g. one would have to know that ordering a \u201ccurfew\u201d creates a public record that can be retrieved/verified). Previous modeling work, we show, has focused on the \u201ceasy\u201d cases:\n1All data and model code can be found at https://gi thub.com/alex2awesome/source-exploration.\nidentifying attributions via quotes,2 resulting in high-precision, low recall techniques (Pado\u0301 et al., 2019; Vaucher et al., 2021).\nIn the first part of this paper we address source attribution. We define the concept of \u201csource\u201d broadly to capture different information-gathering techniques used by journalists, introducing 16 categories of sourcing (some shown in Tables 1, 2, and 3). We apply this schema to construct the largest source-attribution dataset, to our knowledge, with 28,000 source-attributions in 1,304 news articles. Then, we train high-performing models, achieving an overall attribution accuracy of 83% by fine-tuning GPT3. We test numerous baselines and show that previous lexical approaches (Muzny et al., 2017), bootstrapping (Pavllo et al., 2018), and distant supervision\n2By quote, we mean information derived from a person or a document \u2013 verbatim or paraphrased. Sourced information is broader and includes actions by the journalist to uncover information: first-person observations, analyses or experiments.\n(Vaucher et al., 2021) fail. In the second part of this paper, with sourceattribution models in hand, we turn to a fundamental question in news writing: when and why are sources used together in an article? Sources tend to be used in canonical ways: an article covering local crime, for instance, will likely include quotes from both a victim and a police officer (Van Krieken, 2022; Spangher and Choudhary, 2022), and an article covering a political debate will include voices from multiple political parties (Hu et al., 2022). However, until now, the tools have not existed to study the compositionality of sources in news, i.e., why a set of sources was selected during the article\u2019s generative process.\nTo establish compositionality, we must show that a certain set of sources is needed in an article. We introduce a new task, source prediction: does this document need another source? We implement this task in two settings: (1) ablating news documents where all sentences attributable to a source are removed or (2) leveraging a dataset with news edit history (Spangher et al., 2022) where updates add sources. We show that large language models achieve up to 81.5% accuracy in some settings, indicating a degree of predictability. In doing so, we pave the way for downstream applications based on assumptions of compositionality in news, like source-based generative planning (Yao et al., 2019) and source recommendation engines (Spangher et al., 2021; Caswell, 2019).\nIn sum, our contributions are three-fold:\n1. We introduce the first comprehensive corpus of news article source attribution, covering 1,304 news articles across an expansive set of 16 information channels.\n2. We build state-of-the-art source attribution models, showing that they can be used to achieve 83% accuracy. Additionally, we validate that this task is challenging and requires in-depth comprehension by showing that fine-tuning GPT3 outperforms zero- and few-shot prompting, echoing results from (Ziems et al., 2023).\n3. We open the door to further work in document-level planning and sourcerecommendation by showing news articles use sources in a compositional way. We show that models can predict when an article is missing a source 81.5% of the time.\nA roadmap to the rest of the paper is as follows. In the next part, Section 2, we address our approach to source attribution: we define more precisely the ways in which a sentence is attributable to a source, the way we built our dataset, and our results. In Section 3, we discuss how we study compositionality in news writing by constructing a prediction problem. We close by discussing the implications and outlining future work."
        },
        {
            "heading": "2 Source Attribution",
            "text": ""
        },
        {
            "heading": "2.1 Problem Definition",
            "text": "We model a news article as a set of sentences, S = {s1, ...sn} and a set of informational sources Q = {q1, ...qk}. We define an attribution function a that maps each sentence to a subset of sources:3\na(s) \u2286 Q for s \u2208 S\nA sentence is attributable to a source if there is an explicit or implicit indication that the facts in it came from that source. A sentence is not attributable to any source if the sentence does not convey concrete facts (i.e. it conveys journalistprovided analysis, speculation, or context), or if it cannot be determined where the facts originated.\nSources are people or organizations and are usually explicitly mentioned. They may be named entities (e.g. \u201cLaurent Lamothe,\u201d in Table 1), or canonical indicators (e..g \u201ccommission,\u201d \u201cauthorities\u201d) and they are not pronouns. In some cases, a sentence\u2019s source is not mentioned in the article but can still be determined if (1) the information can only have come from a small number of commonly-used sources4 or (2) the information is based on an eye-witness account by the journalist. See Table 2 for examples of these latter two categories. In the first two rows, we give examples of sourced information that a knowledgeable journalist could look up quickly. The third row shows a scene that could only have been either directly observed, either in-person or via recording, and thus must be sourced directly to the journalist.\nWe formulate this attribution task with two primary principles in mind: we wish to attribute as many sentences as possible to informational\n3Most sentences are attributed to only one source in the article, but some are attributed to several.\n4Examples in this category include \u201cthe stock market,\u201d \u201clegislative/executive records,\u201d \u201ccourt filings.\u201d Trained journalists can tell with relative accuracy where this information came from.\nsources used, and we wish to identify when the same source informed multiple sentences.5 We allow for an expansive set of information channels to be considered (see Table 3 for some of the top channels) and design a set of 16 canonical information channels that journalists rely on.6"
        },
        {
            "heading": "2.2 Corpus Creation and Annotation",
            "text": "We select 1,304 articles from the NewsEdits corpus (Spangher et al., 2022) and deduplicate across versions. In order to annotate sentences with their attributions, we recruit two annotators. One an-\n5For example, in Table 1, two sentences are attributable to the commission, despite the information coming from two separate channels (a published document and statement).\n6These 16 categories are formulated both in conversation with journalists and after extensive annotation and schema expansion.\nnotator is a trained journalist with over 4 years of experience working in a major newsroom, and the other is an undergraduate assistant. The senior annotator checks and mentors the junior annotator until they have a high agreement rate. Then, they collectively annotate 1,304 articles including 50 articles jointly. From these 50, we calculate an agreement rate of more than \u03ba = .85 for source detection, \u03ba = .82 for attribution. Categories shown in Table 3 are developed early in the annotation process and expanded until a reasonable set captures all further observations.7 Categories are refined and adjusted following conversations with experienced journalists and journalism professors. For a full list of categories, see appendix8."
        },
        {
            "heading": "2.3 Source Attribution Modeling",
            "text": "We split Source Attribution into two steps: detection (is the sentence attributable?) and retrieval (what is that attribution?) because, in early trials, we find that using different models for each step is more effective than modeling both jointly.\nPrior work in Source Attribution primarily used hand-crafted rules (Peperkamp and Berendt, 2018), bootstrapping (Pavllo et al., 2018) and distance-supervision (Vaucher et al., 2021) approaches (see Section 4). Although such work has shown impressive performance on curated datasets, they typically define a source\u2019s informational contribution rather narrowly (i.e. only direct or indirect quotes, compared with the 16 channels listed in Table 3). So, we test several variations of\n7We find \u03ba = .45 agreement for quote-type categories 8 Note: we do not perform modeling on these categories in the present work, but use them for illustration and evaluation.\nmethods introduced in prior work on our dataset to confirm that these categories are not implicitly attributed. For detection, a binary classification task, F1-score is used. For retrieval, we use accuracy, or precision@1.\nBaseline Methods Rules 1 (R1): Co-Occurrence: We identify sentences where a source entity candidate co-occurs with a speaking verb. For detection, any sentence that contains such a co-occurence is considered a detected sentence. For attribution, we consider the identity of the source entity. We use a list of 538 speaking verbs from Peperkamp and Berendt (2018) along with ones identified during annotation. We extract PERSON Named Entities and noun-phrase signifiers using a lexicon (n=300) (e.g. \u201cauthorities\u201d, \u201cwhite house official\u201d) extracted from Newell et al. (2018)\u2019s dataset.\nRules 2 (R2): Governance: Expanding on R1, we parse syntactic dependencies in sentences (Nivre, 2010) to introduce additional heuristics. Specifically, we identify sentences where the name\nis an nsubj dependency to a speaking verb governor. nsubj is a grammatical part-of-speech, and a governor is a higher node in a syntactic parse tree.\nQuootstrap: Pavllo et al. (2018) created a bootstrapping algorithm to discover lexical patterns indicative of sourcing. Contrasting with previous baselines, which hand-crafted lexical rules, bootstrapping allowed researchers to learn large numbers of highly specific patterns. Although the small size of our dataset compared with theirs prevents us from extracting novel lexical patterns tailored to us, we use a set of 1,000 lexical patterns provided by the authors9. Similary to R1 and R2, for detection, we consider all sentences that match these 1,000 lexical rules to be \u201cdetected\u201d sentences. For attribution, we examine the entities these rules extract.\nQuoteBank: In Vaucher et al. (2021), authors train a BERT-based entity-extraction model on distantly-supervised data scored from (Pavllo\n9https://github.com/epfl-dlab/Quoteban k/blob/main/quootstrap/resources/seedPat terns.txt\net al., 2018). This method is less lexically focused, and thus more generalizable, than previous methods. They use their model to score and release a large corpus of documents. We examine this corpus and select articles that are both in their corpus and in our annotation set, finding 139 articles, and limit our evaluation to these articles. For detection, we examine all sentences with an attribution, and for attribution, we match the name of that source with our gold-labels.\nDetection Methods Sentence: We adapt a binary sentence classifier where each token in each sentence is embedded using the BigBird-base transformer architecture (Zaheer et al., 2020). Tokens are combined via self attention to yield a sentence embedding and again to yield a document embedding. Thus, each sentence is independent of the others.\nFull-Doc: We use a similar architecture to the Sentence approach, but instead of embedding tokens in each sentence separately, we embed tokens in the whole document, then split into sentences and combine using self-attention. Thus, the sentences are not embedded independently and are allowed to share information.\nRetrieval Methods Sequence Labeling: predicts whether each token in a document is a source-token or not. We pass each document through BigBird-base to obtain token embeddings and then use a tokenlevel classifier. We experiment with inducing a curriculum by training on shorter-documents first, and freezing layers 0-4 of the architecture.\nSpan Detection: predicts start and stop tokens of the sentence\u2019s source. We use BigBird-base, and separate start/stop-token classifiers (Devlin et al., 2018). We experiment with inducing decaying reward around start/stop positions to reward near-misses, and expand the objective to induce source salience as in Kirstain et al. (2021), but find no improvement.\nGeneration: We formulate retrieval as openended generation and fine-tune GPT3 models to generate source-names. We use with the following prompt: \u201c<article>To which source can we attribute the sentence <sentence>?\u201d. We need to include the whole article in order to capture cases where a source is mentioned in another sentence. We experiment with fine-tuning Babbage and Curie models, and testing zero- and few-shot for DaVinci models.\nBecause our prompt-query as it contains an entire article/source pair, we have limited additional token-budget; so, for our few-shot setting, we give examples of sentence/source pairs where the source is mentioned in the sentence.\nFor +coref variations, we evaluate approaches on articles after resolving all coreferences using LingMess (Otmazgin et al., 2022). For +Nones variations, we additionally train our models to detect when sentences do not contain sources. We use this as a further corrective to eliminate false positives introduced during detection."
        },
        {
            "heading": "2.4 Source Attribution Results",
            "text": "As shown in Table 4, we find that the GPT3 Curie source-retrieval model paired with the Full-Doc detection module in a pipeline performed best, achieving an attribution accuracy of 83%. In the +None setting, both GPT3 Babbage and Curie can identify false positives introduced by the detection stage and outperform their counterparts. Overall, we find that resolving coreference does not improve performance, despite similarities between\nthe tasks. The poor performance of both rules-based approaches and QuoteBank, which also uses heuristics,10 indicates that simple lexical cues are insufficient. Although QuoteBank authors reported it outperformed similar baselines as we tested (Vaucher et al., 2021), we observe low performance from Quotebank (Vaucher et al., 2021), even in categories it is trained to detect.\nGPT3 DaVinci zero-shot and few-shot greatly underperform fine-tuned models in almost all categories (except \u201cOther\u201d). Further, we see very little improvement in the use of a few-shot setup vs. zero-shot. This might be because the examples we give GPT3 are sentence/source pairs, which do not correctly mimic our document-level sourceattribution task. We face shortcomings due to the document-level nature of our task: the tokenbudget required to ask a document-level question severely limits our ability to do effective few-shot document-level prompting. Approaches that condense prompts (Mu et al., 2023) might be helpful to explore in future work."
        },
        {
            "heading": "2.5 Insights from Source Analysis",
            "text": "Having built an attribution pipeline that performs reasonably well, we run our best-performing attribution model across 9051 unlabeled documents from NewsEdits and extract all sources. In this section, we explain derive insights into how sources are used in news articles. For statistics guiding these insights, see in Table 5, which shows statistics calculated on both our annotated dataset (\u201cGold Train\u201d and \u201cGold Test\u201d columns) and the 9051 documents we just described (\u201cSilver\u201d column). We ask two primary questions: how much an article is sourced? and when are sources used in the reporting and writing process?\nInsight #1: \u223c 50% of sentences are sourced, and sources are used unevenly. Most articles, we find, attribute roughly half the information in their sentences to sources. This indicates that the percentage of sources used is fairly consistent between longer and shorter documents. So, as a document grows, it adds roughly an equal amount of sourced and unsourced content (e.g. explanations, analysis, predictions).11 We also find that sources\n10Quotebank\u2019s algorithm condenses input data to a BERT span-classifier by (1) looking for double-quotes (2) identifying candidate speakers through a lookup table.\n11For more details, see the appendix.\nare used unevenly. The most-used source in each article contributes \u223c 35% of sourced sentences, whereas the least-used source contributes \u223c 5%. This shows a hierarchy between major and minor sources used in reporting and suggests future work analyzing the differences between these sources.\nInsight #2: Sources begin and end documents, and are added while reporting Next we examine when sources are used in the reporting process. We find that articles early in their publication cycle tend to have fewer sources, and add on average two sources per subsequent version. This indicates an avenue of future work: understanding which kinds of sources get added in later versions can help us recommend sources as the journalist is writing. Finally, we also find, in terms of narrative structure, that journalists tend to lead their stories with sourced information: the most likely position for a source is the first sentence, the least likely position is the second. The second-most likely position is the end of the document.12 A caveat to Table 5: many gold-labeled documents were parsed so the first sentence got split over several sentences, which is why we observe the last sentences having highest sourcing.13"
        },
        {
            "heading": "3 Source Compositionality",
            "text": ""
        },
        {
            "heading": "3.1 Problem Definition",
            "text": "Our formulation in Section 2 for quotation attribution aims to identify the set of sources a journalist used in their reporting. Can we reason about why certain groups of sources were chosen in tandem? Can we determine if an article is missing sources?\nWe create two probes for this question:\n1. Ablation: Given an article (S,Q) with attribution a(s)\u2200s \u2208 S, choose one source q \u2208 Q. To generate positive examples, remove all sentences s where q \u2208 a(s). To generate negative, remove an equal number of sentences where a(s) = {} (i.e. no source).\n2. NewsEdits: Sample article-versions from NewsEdits, a corpus of news articles with their updates across time (Spangher et al.,\n12The sources might be used for different purposes: Spangher et al. (2023) performed an analysis on news articles\u2019 narrative structure, and found that sentences conveying the Main Idea lead the article while sentences conveying Evaluations or Predictions.\n13E.g. sents=[\u2019BAGHDAD\u2019, \u2019--\u2019, \u2019Yesterday, the American military said\u2019]. See appendix, Figure 4.\n2022). Identify articles at time t where the update at t+ 1 either adds a source or not.\nEach probe tests source usage in different ways. Ablation assumes that the composition of sources in an article is cohesively balanced, and induces reasoning about this balance. NewsEdits relaxes this assumption and probes if this composition might change, either due to the article\u2019s completeness, changing world events that necessitate new sources, or some other factor.14"
        },
        {
            "heading": "3.2 Dataset Construction and Modeling",
            "text": "We use our Source Attribution methods discussed in Section 2 to create large silver-standard datasets in the following manner for our two primary experimental variants: Ablation and NewsEdits. To interpret results in each variant better, we train a classifier to categorize articles into four topics plus\n14Spangher et al. (2022) found that many news updates were factual and tied to event changes, indicating a breaking news cycle.\none \u201cother\u201d topic15, based on articles in the New York Times Annotated Corpus (Sandhaus, 2008) with keyword sets corresponding to each topic.\nAblation We take 9051 silver-standard documents (the same ones explored in Section 2.5) and design three variations of this task. As shown in Table 5, articles tend to use sources lopsidedly: one source is usually primary. Thus, we design Easy (Top Source, in Table 1), Medium (Secondary) and Hard (Any Source) variations of our task. For Easy, we choose the source with the most sentences attributed to it. For Medium, we randomly choose among the top 3 sources. And for Hard, we randomly choose any of the sources. Then, we create a y = 1 example by removing all sentences attributed to the chosen source, and we create a y = 0 example from the same document by removing an equal number of sentences that are\n15These four have been identified as especially socially valuable topics, or \u201cbeats,\u201d due to their impact on government responsiveness (Hamilton, 2011)\nnot attributed to any sources.\nNewsEdits We sample an additional 40, 000 articles from the NewsEdits corpora and perform attribution on them. We sample versions pairs that have roughly the same number of added, deleted and edited sentences in between versions in order to reduce possible confounders, as Spangher et al. (2022) showed that these edit-operations were predictable. We identify article-version pairs where 2 or more sources were added between version t and t + 1 and label these as y = 1, and 0 or 1 sources added as y = 0.\nModeling We use three models: (1) FastText (Joulin et al., 2016) for sentence classification, (2) A BigBird-based model: we use BigBird with self-attention for document classification, similar to Spangher et al. (2022).16 Finally, (3) we fine-tune GPT3 Babbage to perform promptcompletion for binary classification. For each model, we test two setups. First, we train on the vanilla text of the document. Then, in the +Source-Attribution variants, we train by appending each sentence\u2019s source attribution to the end of it.17 The source annotations are obtained from our attribution pipeline."
        },
        {
            "heading": "3.3 Results and Discussion",
            "text": "The results in Table 6 show that we are broadly able to predict when major sources (Top, Secondary) are removed from articles, indicating that there is indeed compositionality, or intention, in the way sources are chosen to appear together in news articles. The primary source (Top)\u2019s absence is the easiest to detect, indicating that many stories revolve around a single source that adds crucial information. Secondary sources (Second) are still predictable, showing that they serve an important role. Minor sources (Any)\u2019s absence are the hardest to predict and the least crucial to a story. Finally, source-addition across versions (NewsEdits) is the hardest to detect, indicating that versions contain balanced compositions.\n16Concretely, we obtain token embeddings of the entire document, which we combine for each sentence using selfattention. We contextualize each sentence embedding using a shallow transformer architecture. We finally combine these sentence embeddings using another self-attention layer to obtain a document embedding for classification. We utilize curriculum learning based on document length, a linear loss-decay schedule.\n17Like so: <sent 1>. SOURCE: <source 1>. <sent 2> SOURCE: <source 2>... <sent n> SOURCE: <source n>.\nOverall, we find that our experiments are statistically significant from random (50% accuracy) with t-test p < .01, potentially allowing us to reject the null hypothesis that positive documents are indistinguishable from negative in both settings. Statistical significance does not preclude confounding, and both the Ablation and the NewsEdits setups contain possible confounders. In the Ablation set up, we might be inadvertently learning stylistic differences rather than source-based differences. To reduce this risk, we investigate several factors. First, we consider whether lexical confounders, such as speaking verbs, might be artificially removed in the ablated documents. We use lexicons defined in our rules-based methods to measure the number of speaking verbs in our dataset. We find a mean of n = [34, 32] speaking verbs per document in y = [0, 1] classes in the Top case, n = [35, 34] in the Medium, and n = [35, 37] in Hard. None of these differences are statistically significant. We also do not find statistically significant differences between counts of named entities or source signifiers (defined in Section 4). Finally, we create secondary test sets where y = 0 is non-ablated documents. This changes the nature of the stylistic differences between y = 1 and y = 0 while not affecting sourcing differences18. We rerun trials in the Top grouping, as this would show us the greatest confounding effect, and find that the accuracy of our classifiers differs by within -/+3 points.\nIn the NewsEdits setup, we have taken care to balance our dataset along axes where prior work have found predictability. For instance, Spangher et al. (2022) found that an edit-operations19 could be predicted. So, we balance for length, version number and edit operations.\nHaving attempted to address confounding in various ways in both experiments, we take them together to indicate that, despite each probing different questions around sourcing, there are patterns to the way sources are during the journalistic reporting process. To illustrate, we find in Table 6 that Election coverage is the most easily predictable across all tasks. This might be because of efforts to include both left-wing and right-wing voices. It also might be because the cast of charac-\n18We do not want to train on such datasets, because there are statistically significant length differences and other stylistic concerns ablated and non-ablated articles.\n19E.g. Whether a sentence would be added in a subsequent version.\nters (e.g. campaign strategists, volunteers, voters) stays relatively consistent across stories.\nTwo additional findings are that (1) the tasks we expect are harder do yield lower accuracies and, (2) larger GPT3-based language models generally perform better. Although not especially surprising, it further confirms our intuitions about what these tasks are probing. We were surprised to find that, in general, adding additional information in both stages of this project, whether coreference in the attribution stage or source information in the prediction stage, did not improve the models\u2019 performance. (In contrast, adding source information to smaller language model, BigBird, helped with harder tasks like the Medium, Hard and NewsEdits). We had hypothesized that the signal introduced by this labeling would not harm the GPT3-based models, but this was not the case. It could be that the larger models are already incorporating a notion of coreference and attribution, and adding this information changed English grammar in a way that harmed performance."
        },
        {
            "heading": "4 Related Work",
            "text": "Quote Attribution Prior work in quote attribution has also been aimed at identifying which sources contributed information in news articles. Early work explored rules-based methods (Elson and McKeown, 2010; O\u2019Keefe et al., 2012) and statistical classifiers (Pareti et al., 2013) to attribute sources to quotes. More recent work has extended these ideas by using bootstrapping to discover new patterns, Quootstrap (Pavllo et al., 2018) and training BERT-based models on perturbations on these patterns, or QuoteBERT (Vaucher et al., 2021). One upside of Quootstrap and QuoteBERT is that they might adapt better to new domains by learning and generalizing from new patterns. However, the method by which patterns are learned, finding quotes that repeat across outlets, might bias this method towards discovering quotes by oft-quoted figures. These quotes, in turn, may be contextualized differently than other quotes, introducing fundamental biases in which sources get discovered. We urge more consideration of these potential biases, not only for performance considerations but fairness. Overall, our work differs from previous work in this field because we defined information more broadly. Prior work is quote-focused, whereas we include a larger set of information channels (Table 3).\nPersona Modeling A second area that our work draws inspiration from is the study of narrative characters and how they are used in fiction. Work by Bamman et al. (2013) and Card et al. (2016) used custom topic models to model characters by latent \u201cpersonas\u201d generated from latent document-level distributions.Earlier work extended this topic-modeling approach to news sources (Spangher et al., 2021). We see potential for future work merging this with our dataset and framework, using methods like discrete variational autoencoders, which have been applied to document-planning (Ji and Huang, 2021).\nDownstream Applications : Diversity An interesting downstream applications of our work is to improve analysis of diversity in sourcing. Source-diversity has been studied in news articles (Peperkamp and Berendt, 2018; Masini et al., 2018; Berendt et al., 2021; Amsalem et al., 2020), where authors have constructed ontologies to further explore the role of sources from different backgrounds. Opinion Mining Another strain focuses on characterizing voices in a text by opinion (O\u2019Keefe et al., 2013). Such work has been applied in computational platforms for journalists (Radford et al., 2015) and in fake news detection (Conforti et al., 2018)."
        },
        {
            "heading": "5 Conclusions",
            "text": "We have offered a more expansive definition of sourcing in journalism and introduced the largest attribution dataset capturing this notion. We have developed strong models to identify and attribute information in news articles. We have used these attribution models to create a large silver standard dataset that we used to probe whether source inclusion in news writing follows predictable patterns. Overall, we intend this work to serve as a starting point for future inquiries into the nature of source inclusion in news articles. We hope to improve various downstream tasks in NLP and, ultimately, take steps towards building a source recommendation engine that can help journalists in the task of reporting."
        },
        {
            "heading": "6 Acknowledgments",
            "text": "Alexander Spangher would like to thank Bloomberg News for a generous 4 year fellowship that has funded this work and his other work in the areas of computational journalism, computational law, nuclear fusion, and linguistics."
        },
        {
            "heading": "7 Limitations",
            "text": "A central limitation to our work is that the datasets we used to train our models are all in English. As mentioned previously, we used English language sources from Spangher et al. (2022)\u2019s NewsEdits dataset, which consists of sources such as nytimes.com, bbc.com, washingtonpost.com, etc.\nThus, we must view our work in source extraction and prediction with the important caveat that non-Western news outlets may not follow the same source-usage patterns and discourse structures in writing their news articles as outlets from other regions. We might face extraction biases if we were to attempt to do such work in other languages, such as only extracting sources that present in patterns similar to those observed in Western sources, which should be considered as a fairness issue."
        },
        {
            "heading": "8 Ethics Statement",
            "text": ""
        },
        {
            "heading": "8.1 Risks",
            "text": "Since we constructed our datasets on well-trusted news outlets, we assumed that every informational sentence was factual, to the best of the journalist\u2019s ability, and honestly constructed. We have no guarantees that such an attribution system would work in a setting where a journalist was acting adversarially.\nThere is a risk that, if such a work were used in a larger news domain, it could fall prey to attributing misinformation or disinformation. Thus, any downstream tasks that might seek to gather sourced sentences might be poisoned by such a dataset. This risk is acute in the news domain, where fake news outlets peddle false stories that attempt to look true (Boyd et al., 2018; Spangher et al., 2020). We have not experimented how our classifiers would function in such a domain. There is work using discourse-structure to identify misinformation (Abbas, 2022; Sitaula et al., 2020), and this could be useful in a source-attribution pipeline to mitigate such risks.\nWe used OpenAI Finetuning to train the GPT3 variants. We recognize that OpenAI is not transparent about its training process, and this might reduce the reproducibility of our process. We also recognize that OpenAI owns the models we finetuned, and thus we cannot release them publicly. Both of these thrusts are anti-science and antiopenness and we disagree with them on principle. However, their models are still useful in a black-\nbox sense for giving strong baselines for predictive problems and drawing scientific conclusions about hypotheses."
        },
        {
            "heading": "8.2 Licensing",
            "text": "The dataset we used, NewsEdits (Spangher et al., 2022), is released academically. Authors claim that they received permission from the publishers to release their dataset, and it was published as a dataset resource in NAACL 2023. We have had lawyers at a major media company ascertain that this dataset was low risk for copyright infringement."
        },
        {
            "heading": "8.3 Computational Resources",
            "text": "The experiments in our paper required computational resources. We used 8 40GB NVIDIA V100 GPUs, Google Cloud Platform storage and CPU capabilities. We designed all our models to run on 1 GPU, so they did not need to utilize model or data-parallelism. However, we still need to recognize that not all researchers have access to this type of equipment.\nWe used Huggingface Bigbird-base models for our predictive tasks, and will release the code of all the custom architectures that we constructed. Our models do not exceed 300 million parameters."
        },
        {
            "heading": "8.4 Annotators",
            "text": "We recruited annotators from our educational institutions. They consented to the experiment in exchange for mentoring and acknowledgement in the final paper. One is an undergraduate student, and the other is a former journalist. Both annotators are male. Both identify as cis-gender. The annotation conducted for this work was deemed exempt from review by our Institutional Review Board."
        },
        {
            "heading": "A Exploratory Data Analysis",
            "text": "We show more data analysis around source usage in news articles. Figure 1 shows the distribution over the amount of sentences in each article that are attributable to sources. Although most articles have around 50% of their sentences as source sentences, a small number of articles source < 10% of their sentences (a manual analysis shows that that these are mainly opinion pieces) or > 90% of their sentences (a manual analysis shows that these are mainly short, one-paragraph breaking news excerpts).\nFigure 2 shows how articles grow over time, through versions. We find that, on average, two sources are added per version. This is surprisingly linear, with earlier versions containing the least number of sources.\nIn Figure 3, we show that the percent of sourcing is consistent the longer a document gets. This means that when more sentences are added to the document, the journalists adds a consistent amount of sourced and non-sourced sentences. The only exception is when articles are very short. Manual inspection reveals that these are usually breaking news paragraphs that are entirely composed of a reference to a press release, a statement or a quote.\nIn Figure 4, we show the likelihood of a source being present in each sentence-position of our document. This indicates where in the document sources are used. The likeliest spot for a source is the first sentence, and the least likely is the second sentence. As can be seen, the likelihood of a source increases further throughout the document."
        },
        {
            "heading": "B Annotation Definitions",
            "text": "1. Quote: A statement or a paraphrase of a statement given by the source to the reporter in an interview.\n2. Background: A sentence giving non-event information the source (i.e. descriptions, role, status, etc.), that does not contain a\nquote. Does not have to contain any external source of information.\n3. Narrative: A sentence giving narrative information about the source\u2019s role in events that does not contain a quote. Does not have to contain any external source of information.\n(a) For \u201cBackground\u201d and \u201cNarrative,\u201d these usually don\u2019t explicitly reference external sources of information. It\u2019s typically implied that the journalist learned this information by talking to the sources, but it does not have to be the case.\n4. Direct Observation: A sentence where it\u2019s clear that the journalist is either (1) literally witnessing the events (2) conducting their own analysis, investigation or experiment, i.e. the journalist is their own source of observation.\n(a) The difference between \u201cNarrative\u201d and \u201cDirect Observation\u201d can be hazy. Unless it is very clear that the journalist\nis literally observing events unfold, do NOT use \u201cDirect Observation.\u201d\n(b) When \u201cDirect Observation\u201d is selected, the source head is \u201cjournalist,\u201d source type is \u201cNamed Individual,\u201d affiliation is \u201cMedia,\u201d role is \u201cinformational\u201d and status is \u201ccurrent,\u201d UNLESS the journalist abundantly defines themselves as otherwise (i.e. \u201cIn my years as a diplomat...,\u201d \u201cI am a professor...\u201d).\n5. Public Speech: Remarks made by the source in a public setting to an open crowd.\n6. Communication: Remarks made by the source in a private setting or to a closed, select group. Can be interpreted broadly to include written communications.\n7. Published Work: Work written by the source, usually distributed via academic journals or government publications.\n8. Statement: A prepared quote given by a source. Usually distributed in a press conference or in writing.\n9. Lawsuit: Any information given during the course of a court proceeding including claims, defense, rulings or other court-related procedures.\n10. Price Signal: Any information about a company\u2019s stock price, the price of goods, etc. that was obtained through analyzing market data.\n11. Vote/Poll: Information given about voting decisions, whether as a result of an actual vote or a electoral or opinion poll.\n12. Document: A more generic category of information distributed via writing (Published Work is a subset of this class).\n13. Press Report: Information obtained from a media source, whether it\u2019s a news article, a television report or a radio report.\n14. Social Media Post: Information posted on a social media platform (i.e. Twitter, Facebook, blog comments, etc.).\n15. Proposal/Order/Law: Information codified in text by officials resulting, or intending to\nresult in, policy changes (i.e. executive order, legislative text, etc.).\n16. Declined Comment: A special category of quote where the source does not comment. Also includes when a source \u201ccould not be reached for comment.\u201d"
        }
    ],
    "title": "Identifying Informational Sources in News Articles",
    "year": 2023
}