{
    "abstractText": "Figures of speech such as metaphors, similes, and idioms are integral parts of human communication. They are ubiquitous in many forms of discourse, allowing people to convey complex, abstract ideas and evoke emotion. As figurative forms are often conveyed through multiple modalities (e.g., both text and images), understanding multimodal figurative language is an important AI challenge, weaving together profound vision, language, commonsense and cultural knowledge. In this work, we develop the Image Recognition of Figurative Language (IRFL) dataset. We leverage human annotation and an automatic pipeline we created to generate a multimodal dataset, and introduce two novel tasks as a benchmark for multimodal figurative language understanding. We experimented with state-of-the-art vision and language models and found that the best (22%) performed substantially worse than humans (97%). We release our dataset, benchmark, and code1, in hopes of driving the development of models that can better understand figurative language.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ron Yosef"
        },
        {
            "affiliations": [],
            "name": "Yonatan Bitton"
        },
        {
            "affiliations": [],
            "name": "Dafna Shahaf"
        }
    ],
    "id": "SP:66dc4a007b22dc2e5973e71c5e78f93f6247c3bf",
    "references": [
        {
            "authors": [
                "Arjun R. Akula",
                "Brendan Driscoll",
                "Pradyumna Narayana",
                "Soravit Changpinyo",
                "Zhiwei Jia",
                "Suyash Damle",
                "Garima Pruthi",
                "Sugato Basu",
                "Leonidas Guibas",
                "William T. Freeman",
                "Yuanzhen Li",
                "Varun Jampani"
            ],
            "title": "Metaclue: Towards comprehensive",
            "year": 2022
        },
        {
            "authors": [
                "Anas Awadalla",
                "Irena Gao",
                "Josh Gardner",
                "Jack Hessel",
                "Yusuf Hanafy",
                "Wanrong Zhu",
                "Kalyani Marathe",
                "Yonatan Bitton",
                "Samir Gadre",
                "Shiori Sagawa"
            ],
            "title": "Openflamingo: An open-source framework for training large autoregressive vision-language models",
            "year": 2023
        },
        {
            "authors": [
                "Yonatan Bitton",
                "Nitzan Bitton Guetta",
                "Ron Yosef",
                "Yuval Elovici",
                "Mohit Bansal",
                "Gabriel Stanovsky",
                "Roy Schwartz."
            ],
            "title": "Winogavil: Gamified association benchmark to challenge vision-and-language models",
            "venue": "ArXiv, abs/2207.12576.",
            "year": 2022
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Yejin Choi",
                "Vered Shwartz."
            ],
            "title": "It\u2019s not Rocket Science: Interpreting Figurative Language in Narratives",
            "venue": "Transactions of the Association for Computational Linguistics, 10:589\u2013 606.",
            "year": 2022
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Arkadiy Saakyan",
                "Olivia Winn",
                "Artemis Panagopoulou",
                "Yue Yang",
                "Marianna Apidianaki",
                "Smaranda Muresan"
            ],
            "title": "2023. I spy a metaphor: Large language models and diffusion models co-create visual metaphors",
            "year": 2023
        },
        {
            "authors": [
                "Ju Chun Cheng",
                "Arbee L.P. Chen."
            ],
            "title": "Multimodal time-aware attention networks for depression detection",
            "venue": "Journal of Intelligent Information Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Wenliang Dai",
                "Junnan Li",
                "Dongxu Li",
                "Anthony Meng Huat Tiong",
                "Junqi Zhao",
                "Weisheng Wang",
                "Boyang Li",
                "Pascale Fung",
                "Steven Hoi"
            ],
            "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning",
            "year": 2023
        },
        {
            "authors": [
                "Abhishek Das",
                "Japsimar Singh Wahi",
                "Siyao Li."
            ],
            "title": "Detecting hate speech in multi-modal memes",
            "venue": "CoRR, abs/2012.14891.",
            "year": 2020
        },
        {
            "authors": [
                "Susan R Fussell",
                "Mallie M Moss."
            ],
            "title": "Figurative language in emotional communication",
            "venue": "Social and cognitive approaches to interpersonal communication, pages 113\u2013141.",
            "year": 1998
        },
        {
            "authors": [
                "Hessel Haagsma",
                "Johan Bos",
                "Malvina Nissim."
            ],
            "title": "MAGPIE: A large corpus of potentially idiomatic expressions",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages",
            "year": 2020
        },
        {
            "authors": [
                "hen Liu",
                "Gregor Geigle",
                "Robin Krebs",
                "Iryna Gurevych"
            ],
            "title": "Figmemes: A dataset for figurative language identification in politically-opinionated memes",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Process-",
            "year": 2022
        },
        {
            "authors": [
                "Robert R. Hoffman",
                "Susan Kemper"
            ],
            "title": "What could reaction-time studies be telling us about metaphor comprehension",
            "venue": "Metaphor and Symbol,",
            "year": 1987
        },
        {
            "authors": [
                "Leila Khalatbari",
                "Maria Ryskina",
                "Rita Frieske",
                "Ryan Cotterell",
                "Zhijing Jin"
            ],
            "title": "State-of-the-art generalisation research in nlp: A taxonomy and review",
            "year": 2023
        },
        {
            "authors": [
                "EunJeong Hwang",
                "Vered Shwartz"
            ],
            "title": "Memecap: A dataset for captioning and interpreting memes",
            "year": 2023
        },
        {
            "authors": [
                "Anya Ji",
                "Noriyuki Kojima",
                "Noah Rush",
                "Alane Suhr",
                "Wai Keen Vong",
                "Robert D. Hawkins",
                "Yoav Artzi."
            ],
            "title": "Abstract visual reasoning with tangram shapes",
            "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for",
            "year": 2022
        },
        {
            "authors": [
                "Wonjae Kim",
                "Bokyung Son",
                "Ildoo Kim."
            ],
            "title": "Vilt: Vision-and-language transformer without convolution or region supervision",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "year": 2014
        },
        {
            "authors": [
                "George Lakoff",
                "Mark Johnson."
            ],
            "title": "Conceptual metaphor in everyday language",
            "venue": "The Journal of Philosophy, 77(8):453\u2013486.",
            "year": 1980
        },
        {
            "authors": [
                "Bo Li",
                "Yuanhan Zhang",
                "Liangyu Chen",
                "Jinghao Wang",
                "Jingkang Yang",
                "Ziwei Liu"
            ],
            "title": "2023a. Otter: A multimodal model with in-context instruction tuning",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597.",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi."
            ],
            "title": "Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation",
            "venue": "International Conference on Machine Learning, pages 12888\u201312900. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee."
            ],
            "title": "Visual instruction tuning",
            "venue": "arXiv preprint arXiv:2304.08485.",
            "year": 2023
        },
        {
            "authors": [
                "Melanie Mitchell."
            ],
            "title": "Abstraction and analogymaking in artificial intelligence",
            "venue": "CoRR, abs/2102.10717.",
            "year": 2021
        },
        {
            "authors": [
                "Anthony M. Paul."
            ],
            "title": "Figurative language",
            "venue": "Philosophy and Rhetoric, 3(4):225\u2013248.",
            "year": 1970
        },
        {
            "authors": [
                "G. Philip."
            ],
            "title": "Colouring Meaning: Collocation and connotation in figurative language",
            "venue": "Studies in Corpus Linguistics. John Benjamins Publishing Company.",
            "year": 2011
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever."
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "International Conference on Machine Learning, pages 8821\u20138831. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Antonio Reyes",
                "Paolo Rosso",
                "Davide Buscaldi."
            ],
            "title": "From humor recognition to irony detection: The figurative language of social media",
            "venue": "Data and Knowledge Engineering, 74:1\u201312. Applications of Natural Language to Information Systems.",
            "year": 2012
        },
        {
            "authors": [
                "Richard M. Roberts",
                "Roger J. Kreuz"
            ],
            "title": "Why do people use figurative language",
            "venue": "Psychological Science,",
            "year": 1994
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer."
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages",
            "year": 2022
        },
        {
            "authors": [
                "Rossano Schifanella",
                "Paloma de Juan",
                "Joel R. Tetreault",
                "Liangliang Cao."
            ],
            "title": "Detecting sarcasm in multimodal social platforms",
            "venue": "CoRR, abs/1608.02289.",
            "year": 2016
        },
        {
            "authors": [
                "Dafna Shahaf",
                "Eric Horvitz",
                "Robert Mankoff."
            ],
            "title": "Inside jokes: Identifying humorous cartoon captions",
            "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201915, page 1065\u20131074, New York, NY,",
            "year": 2015
        },
        {
            "authors": [
                "Vered Shwartz",
                "Ido Dagan."
            ],
            "title": "Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition",
            "venue": "Transactions of the Association for Computational Linguistics, 7:403\u2013419.",
            "year": 2019
        },
        {
            "authors": [
                "Mohammad Soleymani",
                "David Garcia",
                "Brendan Jou",
                "Bj\u00f6rn Schuller",
                "Shih-Fu Chang",
                "Maja Pantic."
            ],
            "title": "A survey of multimodal sentiment analysis",
            "venue": "Image and Vision Computing, 65:3\u201314. Multimodal",
            "year": 2017
        },
        {
            "authors": [
                "Shweta Yadav",
                "Jainish Chauhan",
                "Joy Prakash Sain",
                "Krishnaprasad Thirunarayan",
                "Amit Sheth",
                "Jeremiah Schumm."
            ],
            "title": "Identifying depressive symptoms from tweets: Figurative language enabled multitask learning framework",
            "venue": "Proceedings of the 28th Inter-",
            "year": 2020
        },
        {
            "authors": [
                "Barry Menglong Yao",
                "Aditya Shah",
                "Lichao Sun",
                "Jin-Hee Cho",
                "Lifu Huang"
            ],
            "title": "End-to-end multimodal fact-checking and explanation generation: A challenging dataset and models",
            "year": 2022
        },
        {
            "authors": [
                "Jiahui Yu",
                "Zirui Wang",
                "Vijay Vasudevan",
                "Legg Yeung",
                "Mojtaba Seyedhosseini",
                "Yonghui Wu."
            ],
            "title": "Coca: Contrastive captioners are image-text foundation models",
            "venue": "arXiv preprint arXiv:2205.01917.",
            "year": 2022
        },
        {
            "authors": [
                "Dongyu Zhang",
                "Minghao Zhang",
                "Heting Zhang",
                "Liang Yang",
                "Hongfei Lin."
            ],
            "title": "MultiMET: A multimodal dataset for metaphor understanding",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Inter-",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "In this work, we develop the Image Recognition of Figurative Language (IRFL) dataset. We leverage human annotation and an automatic pipeline we created to generate a multimodal dataset, and introduce two novel tasks as a benchmark for multimodal figurative language understanding. We experimented with state-of-the-art vision and language models and found that the best (22%) performed substantially worse than humans (97%). We release our dataset, benchmark, and code1, in hopes of driving the development of models that can better understand figurative language."
        },
        {
            "heading": "1 Introduction",
            "text": "Figures of speech such as metaphors, similes, and idioms are integral parts of human communication. They are ubiquitous in many forms of discourse, allowing people to convey complex, abstract ideas, compare situations, provke thought and evoke emotion (Lakoff and Johnson, 1980; Hoffman and Kemper, 1987; Roberts and Kreuz, 1994; Fussell and Moss, 1998). Figurative language research often focuses on text alone; however, figurative language is often conveyed through multiple modalities (usually text and images) \u2013 for example, in areas such as social media, advertising, and news.\nFigure 1 shows two social media posts that require multimodal figurative understanding. In the\n1https://irfl-dataset.github.io/\nleft image, the caption reads \u201cJumped off the sinking ship just in time\u201d, and the image shows a soccer player who has just left his struggling club. The right image, captioned \u201cA performing clown\u201d, shows a famous YouTuber losing a boxing match to a professional boxer.\nDue to its integral part in human communication, detecting and understanding multimodal figurative language could play an important role in various multimodal challenges, such as hate-speech detection in memes (Das et al., 2020), fact checking (Yao et al., 2022), sentiment analysis (Soleymani et al., 2017), humor recognition (Reyes et al., 2012; Shahaf et al., 2015; Schifanella et al., 2016), and tasks focusing on the mental state of social media users (Yadav et al., 2020; Cheng and Chen, 2022).\nVision and Language Pre-Trained Models\u2019 (VLPTMs) understanding of figurative language combined with images has not been thoroughly explored, partly due to the lack of large-scale datasets. In this work, we introduce the IRFL dataset (Image Recognition of Figurative Language) of idioms, metaphors, and similes with matching images \u2013 both figurative and literal. We developed a pipeline to collect candidate figurative and literal images and annotated them via crowdsourcing.\nNext, we used the dataset to design two novel\ntasks, multimodal figurative language detection and multimodal figurative language retrieval, to assess the figurative-language capabilities of VLPTMs. The detection task is to choose the image that best visualizes the figurative phrase. See Figure 2 for an example for an idiom, a metaphor, and a simile, with the correct answers highlighted.\nAs we noticed that VL-PTMs tend to select images containing objects that appear in the figurative phrase, we designed a second task targeting this behavior. In the multimodal figurative language retrieval task, the goal is to rank figurative images higher than images with objects from the phrase.\nWe experiment with several VL-PTMs and find that the best model (22% accuracy) fails to reach human performance (97%). We also find that generative models have difficulties generating figurative images for idiomatic phrases.\nWe hope our dataset and benchmarks will drive the development of multimodal models that can better understand figurative language, closing the gap with human performance. More broadly, metaphorical reasoning is strongly tied to problem-solving and creativity; we believe such models, able to see analogies between situations that share very little on the surface, could greatly advance the field."
        },
        {
            "heading": "2 Background",
            "text": "We start with a short introduction to the main types of figurative language (Lakoff and Johnson, 1980; Paul, 1970; Philip, 2011).\nA metaphor is a comparison between concepts that makes us think of the target concept in terms of the source concept. For example, in \u201cYou\u2019re a peach!\u201d, a person is equated with a peach, suggesting that they are pleasing or delightful.\nA simile also compares two things, often introduced by \u201clike\u201d or \u201cas\u201d. A simile is open when the shared properties are not explicitly revealed (\u201cHer heart is like a stone\u201d), and closed when they are (\u201cHer heart is hard as stone\u201d).\nAn idiom is a group of words with a non-literal meaning that can not be understood by looking at its individual words. E.g., \u201cWe\u2019re on the same page\u201d means \u201cAgreeing about something\u201d.\nUnderstanding figurative language requires commonsense, general knowledge, and the ability to map between domains. Understanding idioms, in particular, requires profound language and cultural knowledge (Paul, 1970; Philip, 2011)."
        },
        {
            "heading": "3 The IRFL Dataset",
            "text": "Our goal is to create a dataset with idioms, metaphors, and similes paired with figurative and literal images. This dataset can then serve as a benchmark to evaluate Vision and Language models on multimodal figurative language.\nLabels. Initially, we intended to have our annotators label images \u201cliteral\u201d or \u201cfigurative\u201d. However, after initial experimentation with the data generated by our pipeline, we realized the necessity of a more nuanced classification system. Hence, we introduced two additional categories.\nThe first new category, \u201cFigurative+Literal,\u201d encompasses images that express the figurative meaning of an expression while also maintaining some aspects of the literal interpretation. The second, \u201cPartial Literal,\u201d includes images that visualize some (literal) elements or objects from the expression.\nTable 1 illustrates our categories for the expression \u201cTouch wood\u201d. For example, an image of someone literally touching wood while crossing his fingers for luck is classified as Figurative+Literal. This distinction also allows us to later perform a richer analysis of model performance."
        },
        {
            "heading": "3.1 Pipeline: Idioms",
            "text": "We collected 628 idioms from the MAGPIE corpus (Haagsma et al., 2020) of idiomatic expressions. The MAGPIE corpus contains 56, 622\nIdiom: Touch wood Definitions: 1) Hopefully 2) Said while touching something wooden,\ncrowdsourced potentially idiomatic expressions, covering 1, 756 unique idioms that appear in at least two of the following dictionaries: Wiktionary, Oxford Dictionary of English Idioms, and UsingEnglish.com. After collecting the idioms, we feed them into our pipeline.\nOur pipeline consists of four main steps, illustrated in Figure 3. Given an idiom, we first get its definitions from online dictionaries and parse them into search queries (\u00a73.1.1). Second, we search for candidate images using the queries. Third, we filter the images and select the best k literal and figurative candidates for annotation (\u00a73.1.3). Lastly, we annotate the different images via crowdworkers."
        },
        {
            "heading": "3.1.1 Searching for Images",
            "text": "Our goal is to find literal and figurative images for each idiom from the MAGPIE dataset. Searching for an idiom using image search often results in\nliteral images. To find figurative images, we need to understand the meaning of the idiom; however, the MAGPIE dataset does not contain idiom definitions, so we crawl them from online dictionaries (Wiktionary definitions tagged with \u2018figurative\u201d or \u201cidiomatic\u201d2; if no such definitions exist, we try the Oxford Dictionary).\nFor example, in Figure 3, the idiom \u201cwhite hat\u201d nd is defined as \u201cA good person; a hero\u201d (tagged with \u201cidiomatic\u201d), and also as \u201ca sailor\u201d and \u201cA well-meaning hacker\u201d (tagged with \u201cslang\").\nWe split concatenated definitions (e.g., \u201cA good person; a hero\u201d is split into two definitions). In some rare cases, a definition may be another idiom, and then we replace that idiom with its definitions.\nWe then searched Google images for the idioms and their definitions, taking up to 20 images per search query. Images were searched with \u201cSafeSearch\u201d flag \u201con\u201d, and in \u201cUnited States\u201d region."
        },
        {
            "heading": "3.1.2 Image Filtering",
            "text": "We noted that many of the retrieved images contained the search query in textual form. We used optical character recognition (OCR) tool EasyOCR to extract text from the images, and TextBlob to correct spelling errors the OCR made. We then filtered images that contained objects or entities from the idiom or its definitions in textual form (50% of the images). Such images are problematic because they may cause the model to select an image solely based on its textual signal. Following this filter, 15% of the resulting images contained mostly\n2We also construct search queries from untagged definitions. Even though untagged definitions are rare (<3%), they are typically idiomatic.\ntext. To tackle this problem, we used OCR (See Appendix A.2) to remove images with more than a couple of words, as well as images with more than 30% of their space containing text.\nFor the remaining images, we calculated the matching score of each image with its phrase and search query using ViLT. Top-k images with a high \u201cphrase-image\u201d score (that passed a threshold, see Appendix A.3) were tagged as potentially literal. We chose the top k images with the highest \u201cdefinition-image\u201d score as Figurative candidates."
        },
        {
            "heading": "3.1.3 Human Annotation",
            "text": "We hired Amazon Mechanical Turk (AMT) workers to annotate the relation between each idiom and its candidate images using the user interface seen in Appendix A.1 (Figure 6). Five workers annotated each image in batches of five images per sample. They received a payment of $0.15 per sample, which resulted in an average hourly wage of $15. We created a qualification test3 to select quality annotators and provided them with an interactive training platform4 to understand the task and the different categories better.\nWe split the annotation process into batches with an average size of 60 idioms per batch. After each batch, we provided each worker with a personal profile page (Appendix A.1, Figure 7) to view their statistics and some examples where their choice was different from the majority of workers.\nFull annotation results and statistics are presented in Table 2. Despite the subjective nature of the task, in 94% of the instances, there was a majority of 3 workers or more out of 5 compared to a random chance of 29%.\n3https://irfl-dataset.github.io/mturk/image/qualification 4https://irfl-dataset.github.io/mturk/image/train"
        },
        {
            "heading": "3.2 Pipeline: Metaphors and Similes",
            "text": "We collected 35 textual metaphors and 142 textual similes, compiled from online lists. Generating search queries from definitions (to find figurative images) is a central part of our pipeline for idioms (Section 3.1). However, idioms are fixed expressions, but metaphors and similes are much more flexible, as the number of possible comparisons between two things is vast.\nFor this reason, we had to adapt our pipeline. For metaphors, we asked three expert annotators to agree upon definitions. For similes, we use the simile itself and the target concept with the shared property (\u201cfast\u201d) as search queries to find figurative images. For literal images that serve as distractors, we use the source and target without the shared property. In some cases, the target concept images are inadequate literal distractors (an image of a car might still be considered figurative for the simile \"The car is as fast as a cheetah\"). To solve this problem, we include the antonym of the property (\u201cA slow car\u201d).\nAnnotation. As the number of images was relatively small, we had two experts from our team manually annotate images. We obtained 1107 figurative and 1816 partial literal images for similes, 333 figurative and 729 partial literal for metaphors (the other categories were less relevant for the specific data generated by our pipeline)."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Multimodal Figurative Language Detection Task",
            "text": "The Multimodal Figurative Language Detection Task evaluates VL-PTMs\u2019 ability to choose the image that best visualizes the meaning of a figurative expression. Figure 2 shows an example of the task for an idiom, a metaphor, and a simile.\nOur goal was to create a difficult and diverse task representing the richness of our dataset (Section 3). We constructed 810 \u201cmixed\u201d task instances for idioms, metaphors, and similes. Each \u201cmixed\u201d instance contains four candidates: one is the correct answer, partially literal distractors, and random images.\nIdiom instances have 1-2 partially literal distractors. Simile instances contain two literal distractors, one of the target concept without the compared property or with its antonym visualized, and one of the source concept. Metaphor \u201cmixed\u201d instances\nconsist of between 1-3 partially literal distractors.\nZero-Shot Baselines. We evaluate several state-ofthe-art vision-and-language models. We use four versions of CLIP models (Radford et al., 2021): RN50, ViT-B/32, ViT-L/14, and RN50x64/14 with 100M, 150M, 430M, and 620M parameters, respectively. We use the official implementations of ViLT (Kim et al., 2021), BLIP (Li et al., 2022), CoCa ViT-L-14 (Yu et al., 2022), and BLIP2 (Li et al., 2023b). We evaluate all models with their default hyper-parameters, except ViLT on idioms, due to its maximum sequence length of 40 tokens.\nThe models encode the figurative phrase and the image, producing a matching score for each pair. We choose the image with the highest score as the one best matches the expression.\nWe also experimented with multimodal chatbot models, including LLaVA (Liu et al., 2023), InstructBLIP (Dai et al., 2023), OpenFlamingo (Awadalla et al., 2023), and Otter (Li et al., 2023a). We found that the first two do not support our setting, as they can not handle questions about multiple images; the latter two do support the setting, but did not seem to understand the task, returning mostly nonsense answers.\nSupervised Models. We train a supervised model for figurative classification of idioms. We add a binary classifier on top of pre-trained embeddings to classify whether a given image is figurative. We use CLIP (VIT-B/32) model, concatenating the textual idiom embedding to the visual image embedding, followed by a classifier that produces a matching score. A score above 0.5 is labeled \u201cFigurative\u201d. We use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.001, batch size of 12, and train for 7 epochs. We run the fine-tuned model on the multimodal figurative language detection (\u00a74.1) task using the model\u2019s matching score. We train the binary classifier on 4790 images, making sure the training data does not contain any of the images or idioms that appear in the task. We repeat five experiments with different random seeds for each task and take the mean score and std."
        },
        {
            "heading": "4.1.1 Human Evaluation",
            "text": "We asked annotators that did not work on previous IRFL tasks to solve the multimodal figurative language detection task. Each instance of the \u201cmixed\u201d multimodal detection task was annotated by 5 annotators, and the correct answer was chosen by the majority. We find that human performance on\nthe data sampled for all figures of speech ranges between 90%\u2212 100% (Table 3). Additionally, in 82% \u2212 99% of the instances, there was an agreement between at least four annotators compared to a random chance of 6%. Samples from the validation process are presented in Appendix A.5."
        },
        {
            "heading": "4.1.2 Results and Model Analysis",
            "text": "Zero-shot results on the \u201cmixed\u201d multimodal figurative language detection task are presented in Table 3. The best model achieved 22%, 30%, and 66% accuracy on the idioms5, metaphors, and similes tasks compared to a random chance of 25%. These results suggest that models do not understand the connection between a figurative phrase and an image as humans do. We next conduct a finegrained analysis to examine if models failed because they do not see any connection to the figurative images or rather because they prioritize literal connections over figurative ones.\nModels prefer partially literal images over figurative ones. We analyze the models\u2019 choices on the \u201cmixed\u201d multimodal figurative language detection task and found that in all models, a partially literal distractor was selected in 92%\u2212100% of the instances where the models failed across all figures of speech (idioms, metaphors, and similes). This shows that models prefer partially literal images\n5Idioms were passed along with their definitions as input.\nover figurative ones. We find the case of idioms to be particularly interesting. Models receive a relatively long prompt (idiom+definitions), and often choose an image that is a literal interpretation of only 1-2 words from the prompt.\nModels partially understand the figurative connection between idioms and images. To examine whether models can comprehend a figurative connection between an image and an idiom, we experiment with random candidates and several configurations of the multimodal figurative language detection task (Table 4). When provided with an idiom and its definitions as input, the accuracy on the Figurative category ranges between 75%\u221287% with 2 candidates and 58% \u2212 71% with 4 candidates. These results are above chance level but still below human performance on the \u201cmixed\u201d task.\nWhen given the idiom alone as input, the accuracy ranges between 56%\u2212 67% with 2 candidates and 25%\u2212 46% with 4 candidates. These results suggest that models partially understand the figurative connection between idioms and images. We see a significant performance drop with all models when the number of candidates increases.\nIn the Figurative+Literal category, with only the idiom as input, models registered an accuracy of 65%\u2212 78% with 4 candidates. This performance significantly exceeds the accuracy recorded on the Figurative category with 2 and 4 candidates. The performance increase can be explained by the fact\nthat Figurative+Literal images have both a literal and figurative connection to the phrase.\nModels understand metaphors but fail to reach human performance. Table 5 shows the models\u2019 performance on metaphors with random candidates. The accuracy of all models on the Figurative category with 2 candidates is 72%\u2212 88%, and 53% \u2212 76% with 4 candidates. We see a significant performance drop with all models when the number of candidates increases. The results suggest that models can understand metaphors but fail to reach human performance.\nModels understand similes well. Table 5 shows the models\u2019 performance on the similes with random candidates. The accuracy of all models on the Figurative category with 2 candidates is 95% \u2212 99%, and 88% \u2212 98% with 4 candidates. Models\u2019 performance is competitive with that of humans, and the models maintain their performance when increasing the number of candidates. In contrast to the multimodal figurative language detection task with random images, the \u201cmixed\u201d task shows a performance gap between closed and open similes due to open similes concealing the compared property, making it harder for the model to choose the figurative image. Analyzing the \u201cmixed\u201d task results on closed similes, we found that figurative images scored higher than source concept images in 52%\u2212 74% of cases across all models.\nAdditionally, source concept images scored higher than target concept distractor images in 51% \u2212 70% of cases. This pattern suggests a model prioritization for simile images: firstly, target concept images with the compared property, then source concept images, and finally, target con-\ncept images lacking the compared property. Fine-tuning improves figurative understanding and reduces literal preference. The supervised model results are presented in Table 6. Previously we did not display the models\u2019 performance on the \u201cmixed\u201d task when taking the idiom alone as input due to their poor performance (5%\u2212 7% accuracy). However, when training on idioms alone, the supervised model scored a mean accuracy of 46.2%, 9\u00d7 the zero-shot score of 5%. This large performance increase might suggest that VL-PTMs representation of an idiom encodes its definitions.\nTraining and testing with the idiom and its definitions as input resulted in a mean accuracy of 58% compared to 16% in the Zero-shot configuration. After analyzing the supervised model results, we found that its literal preference has improved significantly. In 41% \u00b1 4.3 of the instances where the model failed, a partially literal distractor was selected compared to 96% in the zero-shot configuration. Along with this improvement in literal preference, Figurative+Literal category accuracy raised from 41% in zero-shot to 49%. These results show that models can improve their preference for partially literal images and recognize idiomatic figurative connections better via training. Moreover, the results suggest that the data is a useful training signal for our task.\nWe have discovered that VL-PTMs tend to prefer partially literal images. In the next section, we design a task to tackle this issue."
        },
        {
            "heading": "4.2 Multimodal Figurative Language Retrieval Task",
            "text": "The Multimodal Figurative Retrieval Task examines VL-PTMs\u2019 preference for figurative images. Given a set of figurative and partially literal\nimages, the task is to rank the images using the model-matching score such that the figurative images are ranked higher, and calculate the precision at k, where k is the number of figurative images in the input.\nFigure 4 shows an example of the task for the idiom \u201cruffle someone\u2019s feathers\u201d. We wish to have images of people causing discomfort ranked higher than pictures of birds and feathers. This task provides an opportunity for a deeper probe into how the model comprehends figurative language in terms of its preferences.\nIn this task, we use the same baselines and training methods mentioned in the previous task. We train the supervised model on 3802 images, making sure the training data does not contain any of the images or idioms that appear in the task."
        },
        {
            "heading": "4.2.1 Results and Model Analysis",
            "text": "Zero-shot results are presented in Table 7. We evaluate all figurative phrases that have both Figurative and Partial Literal images. Models\u2019 scores on the preference task are low (<61%). We expect models with proper figurative preference to achieve better results. Models\u2019 success in the Figurative+Literal category can be attributed to the literal connections of the Figurative+Literal images.\nThe supervised model achieved a score of 68\u00b1 3.8 in the Figurative category, almost double the zero-shot score of CLIP-ViT-B/32 (36). Additionally, the score in the Figurative+Literal category was improved by 10 \u00b1 2.25 points. These results align well with the observation that the multimodal figurative language detection task supervised model, which was trained using the same method on a different training set, also showed\nsubstantially moderate literal preference. Table 8 shows the fine-tuned model results."
        },
        {
            "heading": "4.3 Generative Models Analysis",
            "text": "In our work so far, we focused on finding existing images matching a figurative expression. We now explore the question of whether generative models can generate figurative images. We sampled 15 idioms from the IRFL dataset and experimented with the idioms and their definitions as input to Dall-E (Ramesh et al., 2021) and Stable Diffusion (Rombach et al., 2022). We annotated 345 generated images and found that generative models failed to generate figurative images for given idioms, generating literal images instead. When provided with the definitions as input, the models had some more success in creating figurative images. Statistics on the generated images can be seen in Table 9. We also included the percentage of images from each category found by our pipeline.\nThe results show that our pipeline extracted more Figurative, Figurative+Literal, and Literal images and fewer None images than the generative models managed to generate."
        },
        {
            "heading": "5 Related Work",
            "text": "Idioms. Several papers have examined pre-trained LMs\u2019 ability to represent idioms. Shwartz and Dagan (2019) found that LMs\u2019 representation of idiomatic expressions was of lower quality than that of literal ones. Chakrabarty et al. (2022) introduced a narrative understanding benchmark focused on interpreting figurative language and found that pre-trained LMs struggle to perform well in zero-shot and few-shot settings. To the best of our knowledge, Vision and Language Pre-trained models (VL-PTMs) understanding of idioms has not been investigated until this work.\nMetaphors and Similes. Recently there have been several works exploring the ability of VL-PTMs to understand similes and metaphors, and several datasets have been introduced (Zhang et al., 2021; hen Liu et al., 2022; Chakrabarty et al., 2023; Hwang and Shwartz, 2023). These datasets often focus on different types of images (memes, politics, advertising), sometimes containing syntethic images (Akula et al., 2022). In contrast, we use natural images from a search engine. In addition, our tasks introduce the new aspect of retrieval.\nCommonsense. Commonsense is a topic of increasing interest. Particularly relevant lines of work deal with abstractions, associations, and analogies (Mitchell, 2021; Ji et al., 2022; Bitton et al., 2022), all required for understanding figurative language. For example, understanding \u201cas stubborn as a mule\u201d\nrequires the commonsense (false) association between mules and stubbornness."
        },
        {
            "heading": "6 Conclusions and Future Work",
            "text": "In this work we introduced IRFL, a dataset of Figurative and Literal images for idioms, metaphors, and similes. We developed two novel tasks as a benchmark for multimodal figurative language understanding. Our experiments demonstrate that the tasks are easy for humans and challenging for stateof-the-art vision and language models. We publish our dataset, benchmark, and code.\nIn the future, we hope to extend this work to other modalities and different forms of figurative speech. In addition, there are interesting crosscultural connections between figurative expressions. For example, the English expression \u201ccost an arm and a leg\u201d (meaning expensive) has a corresponding expression in French: \u201cCo\u00fbter les yeux de la t\u00eate\u201d (literally, cost the eyes of the head). Adapting our ideas to languages other than English, taking advantage of such connections, is another promising direction.\nWe believe that multimodal figurative language is an essential aspect of human communication that is under-explored in AI; we hope that this work will encourage the development of multimodal models that can better understand figurative language.\nMore broadly, metaphorical reasoning is strongly tied to problem-solving and creativity; we believe that models that can see analogies between situations that share very little on the surface could find many potential applications."
        },
        {
            "heading": "7 Limitations",
            "text": "Our dataset focuses on English idioms. As translation of figurative expressions is a particularly delicate task, it is not straightforward to expand our dataset to other languages, and further research is needed to explore the effectiveness of our pipeline to other languages. In addition, our method heavily relies on sources of figurative expressions, their definitions, and the image search engine."
        },
        {
            "heading": "Acknowledgements",
            "text": "We want to thank Nir Sweed for his valuable feedback. We would also like to thank Nissim Barzilay for his contribution to the collection of figurative and literal images for metaphors and similes. This work was supported by the European Research\nCouncil (ERC) under the European Union\u2019s Horizon 2020 research and innovation program (grant no. 852686, SIAM, Shahaf).\nIn memory of the more than one thousand victims of the horrific massacre carried out by Hamas terrorists on October 7th, 2023."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Annotation UI\nA.2 Documents Filter In an effort to minimize the number of images dominated by text, we filtered out images containing more than a few words, which accounted for 15% of the total. Despite this, certain images like documents, books, and contracts managed to bypass our OCR-based filters, representing 2% of the total images. To address this issue, we developed a filter using the ViLT model (Kim et al., 2021). This filter calculates an image\u2019s matching score with the prompts \"a document\", \"a page of a book\", or \"a contract\" and removes it if the total score surpasses a set \"document\" threshold. To find this threshold, we conducted a grid search on 20 sampled images at each point in the distribution of \u221230,\u221225,\u221220,\u221215,\u221210,\u22125, 0, 5, 10, 15, 20, 25 , 30 categorizing each as a \"document\" or \"nondocument\". The (20, 15) range showed the best results, so we conducted a more dense grid search within this range and found the best threshold to be 18.77 with a TPR of 100% and an FPR of 1%.\nA.3 Literal Threshold We conducted two grid searches on images that passed the OCR filters and had a \"phrase-image\" score higher than the \"search-query\" score to find a literal threshold. We sampled 20 images from each point in the distribution of \u221210,\u22128,\u22126,\u22124,\u22122, 0, 2, 4, 6, 8, 10, and annotated them as \"literal\" or \"non-literal\". This distribution aligns with the normal distribution of the images that stand the two criteria mentioned\nabove (Figure 9). We found the (\u22122, 2) range to result in the best thresholds, so we conducted a more dense grid search in this range. We sampled\n30 images from each point in the distribution of \u22125,\u22124,\u22122,\u22121, 0, 1, 2, 4, 5, and annotated them as \"literal\" or \"non-literal\". We chose the threshold of 1.150353 with a TPR of 86% and FPR of 18%.\nWe observed that when the \"phrase-image\" score is high, we can say that the image is literal with a high probability. However, the reverse is not true, there can be multiple \u201cliteral\u201d images with a very low literal score (Figure 10).\nA.4 GenBench Evaluation Card\nA.5 Understanding Task Samples"
        }
    ],
    "title": "IRFL: Image Recognition of Figurative Language",
    "year": 2023
}