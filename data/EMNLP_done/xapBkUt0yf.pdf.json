{
    "abstractText": "While many languages possess processes of joining two or more words to create compound words, previous studies have been typically limited only to languages with excessively productive compound formation (e.g., German, Dutch) and there is no public dataset containing compound and non-compound words across a large number of languages. In this work, we systematically study decompounding, the task of splitting compound words into their constituents, at a wide scale. We first address the data gap by introducing a dataset of 255k compound and noncompound words across 56 diverse languages obtained from Wiktionary. We then use this dataset to evaluate an array of Large Language Models (LLMs) on the decompounding task. We find that LLMs perform poorly, especially on words which are tokenized unfavorably by subword tokenization. We thus introduce a novel methodology to train dedicated models for decompounding. The proposed two-stage procedure relies on a fully self-supervised objective in the first stage, while the second, supervised learning stage optionally fine-tunes the model on the annotated Wiktionary data. Our self-supervised models outperform the prior best unsupervised decompounding models by 13.9% accuracy on average. Our fine-tuned models outperform all prior (language-specific) decompounding tools. Furthermore, we use our models to leverage decompounding during the creation of a subword tokenizer, which we refer to as CompoundPiece. CompoundPiece tokenizes compound words more favorably on average, leading to improved performance on decompounding over an otherwise equivalent model using SentencePiece tokenization.",
    "authors": [
        {
            "affiliations": [],
            "name": "Benjamin Minixhofer"
        },
        {
            "affiliations": [],
            "name": "Jonas Pfeiffer"
        },
        {
            "affiliations": [],
            "name": "Ivan Vuli\u0107"
        }
    ],
    "id": "SP:51e305b24804c319d6c5805f83716b0518219ccc",
    "references": [
        {
            "authors": [
                "Martine Adda-Decker",
                "Gilles Adda."
            ],
            "title": "Morphological decomposition for asr in german",
            "venue": "Workshop on Phonetics and Phonology in ASR, Saarbr\u00fccken, Germany, pages 129\u2013143.",
            "year": 2000
        },
        {
            "authors": [
                "Enrique Alfonseca",
                "Slaven Bilac",
                "Stefan Pharies."
            ],
            "title": "Decompounding query keywords from compounding languages",
            "venue": "Proceedings of ACL-08: HLT, Short Papers, pages 253\u2013256, Columbus, Ohio. Association for Computational Linguistics.",
            "year": 2008
        },
        {
            "authors": [
                "Duygu Altinok."
            ],
            "title": "Demorphy, german language morphological analyzer",
            "venue": "arXiv preprint arXiv:1803.00902.",
            "year": 2018
        },
        {
            "authors": [
                "Khuyagbaatar Batsuren",
                "G\u00e1bor Bella",
                "Fausto Giunchiglia."
            ],
            "title": "MorphyNet: a large multilingual database of derivational and inflectional morphology",
            "venue": "Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology,",
            "year": 2021
        },
        {
            "authors": [
                "Florian Holz"
            ],
            "title": "ASV toolbox: a modular collec",
            "year": 2008
        },
        {
            "authors": [
                "Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Ryan Cotterell",
                "Tim Vieira",
                "Hinrich Sch\u00fctze."
            ],
            "title": "A joint model of orthography and morphological segmentation",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2016
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Miguel Domingo",
                "Mercedes Garc\u00eda-Mart\u00ednez",
                "Alexandre Helle",
                "Francisco Casacuberta",
                "Manuel Herranz"
            ],
            "title": "How much does tokenization affect neural machine translation",
            "venue": "In Computational Linguistics and Intelligent Text Processing:",
            "year": 2023
        },
        {
            "authors": [
                "Manaal Faruqui",
                "Yulia Tsvetkov",
                "Graham Neubig",
                "Chris Dyer."
            ],
            "title": "Morphological inflection generation using character sequence to sequence learning",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Omer Goldman",
                "David Guriel",
                "Reut Tsarfaty."
            ],
            "title": "un)solving morphological inflection: Lemma overlap artificially inflates models\u2019 performance",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short",
            "year": 2022
        },
        {
            "authors": [
                "Harald Hammarstr\u00f6m."
            ],
            "title": "Measuring prefixation and suffixation in the languages of the world",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Yijie Han",
                "Bhagirath Narahari",
                "H-A Choi."
            ],
            "title": "Mapping a chain task to chained processors",
            "venue": "Information Processing Letters, 44(3):141\u2013148.",
            "year": 1992
        },
        {
            "authors": [
                "Verena Henrich",
                "Erhard Hinrichs."
            ],
            "title": "Determining immediate constituents of compounds in GermaNet",
            "venue": "Proceedings of the International Conference Recent Advances in Natural Language Processing 2011, pages 420\u2013426, Hissar, Bulgaria. Associa-",
            "year": 2011
        },
        {
            "authors": [
                "Valentin Hofmann",
                "Janet Pierrehumbert",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Superbizarre is not superb: Derivational morphology improves BERT\u2019s interpretation of complex words",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Philipp Koehn",
                "Kevin Knight."
            ],
            "title": "Empirical methods for compound splitting",
            "venue": "10th Conference of the European Chapter of the Association for Computational Linguistics, Budapest, Hungary. Association for Computational Linguistics.",
            "year": 2003
        },
        {
            "authors": [
                "Maria Koliopoulou."
            ],
            "title": "Issues of modern greek and german compounding: a contrastive approach",
            "venue": "Journal of Greek Linguistics, 14(1):117\u2013125.",
            "year": 2014
        },
        {
            "authors": [
                "Taku Kudo."
            ],
            "title": "Subword regularization: Improving neural network translation models with multiple subword candidates",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66\u201375,",
            "year": 2018
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson."
            ],
            "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2018
        },
        {
            "authors": [
                "Mikko Kurimo",
                "Sami Virpioja",
                "Ville Turunen",
                "Krista Lagus."
            ],
            "title": "Morpho challenge 2005-2010: Evaluations and results",
            "venue": "Proceedings of the 11th Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 87\u2013",
            "year": 2010
        },
        {
            "authors": [
                "Stefan Langer."
            ],
            "title": "Zur morphologie und semantik von nominalkomposita",
            "venue": "Tagungsband der 4. Konferenz zur Verarbeitung nat\u00fcrlicher Sprache (KONVENS), pages 83\u201397. Bonn.",
            "year": 1998
        },
        {
            "authors": [
                "Vladimir I Levenshtein"
            ],
            "title": "Binary codes capable of correcting deletions, insertions, and reversals",
            "venue": "Soviet physics doklady, volume 10, pages 707\u2013710. Soviet Union.",
            "year": 1966
        },
        {
            "authors": [
                "Constantine Lignos."
            ],
            "title": "Learning from unseen data",
            "venue": "Proceedings of the Morpho Challenge 2010 Workshop, pages 35\u201338. Helsinki.",
            "year": 2010
        },
        {
            "authors": [
                "Krister Lind\u00e9n",
                "Tommi Pirinen."
            ],
            "title": "Weighted finite-state morphological analysis of Finnish compounding with HFST-LEXC",
            "venue": "Proceedings of the 17th Nordic Conference of Computational Linguistics (NODALIDA 2009), pages 89\u201395, Odense, Denmark.",
            "year": 2009
        },
        {
            "authors": [
                "Klaus Macherey",
                "Andrew Dai",
                "David Talbot",
                "Ashok Popat",
                "Franz Och."
            ],
            "title": "Language-independent compound splitting with morphological operations",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human",
            "year": 2011
        },
        {
            "authors": [
                "Fredrik Manne",
                "Tor Sorevik."
            ],
            "title": "Optimal partitioning of sequences",
            "venue": "Journal of Algorithms, 19(2):235\u2013249.",
            "year": 1995
        },
        {
            "authors": [
                "Austin Matthews",
                "Graham Neubig",
                "Chris Dyer."
            ],
            "title": "Using morphological knowledge in openvocabulary neural language models",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2018
        },
        {
            "authors": [
                "Sabrina J Mielke",
                "Zaid Alyafeai",
                "Elizabeth Salesky",
                "Colin Raffel",
                "Manan Dey",
                "Matthias Gall\u00e9",
                "Arun Raja",
                "Chenglei Si",
                "Wilson Y Lee",
                "Beno\u00eet Sagot"
            ],
            "title": "Between words and characters: A brief history of open-vocabulary modeling and tokenization in nlp",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Minixhofer",
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107."
            ],
            "title": "Where\u2019s the point? self-supervised multilingual punctuation-agnostic sentence segmentation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2023
        },
        {
            "authors": [
                "Christof Monz",
                "Maarten De Rijke."
            ],
            "title": "Shallow morphological analysis in monolingual information retrieval for dutch, german, and italian",
            "venue": "Evaluation of Cross-Language Information Retrieval Systems: Second Workshop of the Cross-Language Evaluation",
            "year": 2002
        },
        {
            "authors": [
                "Yirong Pan",
                "Xiao Li",
                "Yating Yang",
                "Rui Dong."
            ],
            "title": "Morphological word segmentation on agglutinative languages for neural machine translation",
            "venue": "arXiv preprint arXiv:2001.01589.",
            "year": 2020
        },
        {
            "authors": [
                "Ben Peters",
                "Andre F.T. Martins."
            ],
            "title": "Beyond characters: Subword-level morpheme segmentation",
            "venue": "Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 131\u2013138, Seattle, Washing-",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Pollatsek",
                "Jukka Hy\u00f6n\u00e4",
                "Raymond Bertram."
            ],
            "title": "The role of morphological constituents in reading finnish compound words",
            "venue": "Journal of Experimental Psychology: Human perception and performance, 26(2):820.",
            "year": 2000
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Martin Riedl",
                "Chris Biemann."
            ],
            "title": "Unsupervised compound splitting with distributional semantics rivals supervised methods",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2016
        },
        {
            "authors": [
                "Phillip Rust",
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Sebastian Ruder",
                "Iryna Gurevych."
            ],
            "title": "How good is your tokenizer? on the monolingual performance of multilingual language models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Jonne Saleva",
                "Constantine Lignos."
            ],
            "title": "The effectiveness of morphology-aware segmentation in low-resource neural machine translation",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725,",
            "year": 2016
        },
        {
            "authors": [
                "Naomi Tachikawa Shapiro."
            ],
            "title": "Splitting compounds with ngrams",
            "venue": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 630\u2013640, Osaka, Japan. The COLING 2016 Organizing Committee.",
            "year": 2016
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Oriol Vinyals",
                "Quoc V Le."
            ],
            "title": "Sequence to sequence learning with neural networks",
            "venue": "Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc.",
            "year": 2014
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Vinh Q Tran",
                "Xavier Garcia",
                "Dara Bahri",
                "Tal Schuster",
                "Huaixiu Steven Zheng",
                "Neil Houlsby",
                "Donald Metzler."
            ],
            "title": "Unifying language learning paradigms",
            "venue": "arXiv preprint arXiv:2205.05131.",
            "year": 2022
        },
        {
            "authors": [
                "Yi Tay",
                "Vinh Q. Tran",
                "Sebastian Ruder",
                "Jai Gupta",
                "Hyung Won Chung",
                "Dara Bahri",
                "Zhen Qin",
                "Simon Baumgartner",
                "Cong Yu",
                "Donald Metzler."
            ],
            "title": "Charformer: Fast character transformers via gradientbased subword tokenization",
            "venue": "International Con-",
            "year": 2022
        },
        {
            "authors": [
                "Don Tuggener."
            ],
            "title": "Incremental coreference resolution for German",
            "venue": "Ph.D. thesis, University of Zurich.",
            "year": 2016
        },
        {
            "authors": [
                "Menno van Zaanen",
                "Gerhard van Huyssteen",
                "Suzanne Aussems",
                "Chris Emmery",
                "Roald Eiselen."
            ],
            "title": "The development of Dutch and Afrikaans language resources for compound boundary analysis",
            "venue": "Proceedings of the Ninth International Conference on",
            "year": 2014
        },
        {
            "authors": [
                "P\u00e4ivi Johanna Virkkunen",
                "Juraj Simko",
                "Heini Henriikka Kallio",
                "Martti Tapani Vainio."
            ],
            "title": "Prosodic features of finnish compound words",
            "venue": "Proceedings of the 9th International Conference on Speech Prosody 2018. International Speech Communications",
            "year": 2018
        },
        {
            "authors": [
                "Sami Virpioja",
                "Ville T. Turunen",
                "Sebastian Spiegler",
                "Oskar Kohonen",
                "Mikko Kurimo."
            ],
            "title": "Empirical comparison of evaluation methods for unsupervised learning of morphology",
            "venue": "Traitement Automatique des Langues, 52(2):45\u201390.",
            "year": 2011
        },
        {
            "authors": [
                "Irene Vogel",
                "Sergio Scalise."
            ],
            "title": "Crossdisciplinary issues in compounding",
            "venue": "CrossDisciplinary Issues in Compounding, pages 1\u2013390.",
            "year": 2010
        },
        {
            "authors": [
                "Linting Xue",
                "Aditya Barua",
                "Noah Constant",
                "Rami AlRfou",
                "Sharan Narang",
                "Mihir Kale",
                "Adam Roberts",
                "Colin Raffel."
            ],
            "title": "ByT5: Towards a token-free future with pre-trained byte-to-byte models",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Giulio Zhou."
            ],
            "title": "Morphological zero-shot neural machine translation",
            "venue": "Ph.D. thesis, Master\u2019s thesis, University of Edinburgh.",
            "year": 2018
        },
        {
            "authors": [
                "Patrick Ziering",
                "Lonneke van der Plas."
            ],
            "title": "Towards unsupervised and language-independent compound splitting using inflectional morphological transformations",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Asso-",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Decompounding is the task of separating compound words into their single word constituents. Decompounding is used in user-facing tools such as dictionaries and morphological analyzers (Altinok,\n\u2020Equal senior authorship.\n0 2 4 8 16 0\n20\n40\n60\n80\n100\nOurs (S1)\n#shots\nA cc\nur ac\ny (%\n)\nOurs (S1+S2)\nEasy Compounds\n0 2 4 8 16 0\n20\n40\n60\n80\n100\nOurs (S1)\nFLAN UL2 20B FLAN T5 XXL FLAN T5 XL FLAN T5 Large\n#shots\nOurs (S1+S2)\nHard Compounds\n0.3 1.4 1.8\n2.9 2.8\nFigure 1: In-context learning performance of LLMs on compound segmentation vs. our method (\u00a75).\nConstituents Subwords\nHard Compound\nEasy Compound\nwind surfing\nspell binding\nwinds fur ing\nspell ingbind\nFigure 2: Examples of easy and hard compounds w.r.t. the T5 tokenizer (also used by FLAN UL2 20B).\n2018). Historically, it has also been widely used as a preprocessing step for other NLP tasks, e.g. for information retrieval (Monz and De Rijke, 2002; Braschler and Ripplinger, 2004), automatic speech recognition (Adda-Decker and Adda, 2000) and machine translation (Koehn and Knight, 2003).\nDecompounding can come in two similar yet different task formats: (i) compound segmentation and (ii) compound normalization (Ziering and van der Plas, 2016). Compound segmentation is the task of segmenting a word into its compound constituents, while preserving its surface form (e.g. bridesmaid \u2192 brides + maid). Compound normalization is the task of recovering the base form of each compound constituent (e.g. bridesmaid\u2192 bride + maid).1\nMost prior work on decompounding has focused on the few languages with excessively productive\n1In morphological segmentation, segmentation and normalization are also referred to as surface-level segmentation and canonical segmentation, respectively (Cotterell et al., 2016).\ncompound formation such as Finnish, German and Swedish (Koehn and Knight, 2003; Shapiro, 2016; Riedl and Biemann, 2016). However, compound words occur in a large, diverse number of languages (Vogel and Scalise, 2010). Yet, datasets which annotate compounds with their segmented or normalized form sparsely exist, even in languages with high compound usage. As the first contribution of this work, we aim to address this issue by introducing a dataset of 255k compound words and their normalized form as well as non-compound words covering 56 languages obtained from Wiktionary (www.wiktionary.org).\nUsing our dataset, we then find that large language models (LLMs), which typically rely on subword-based tokenization (Sennrich et al., 2016; Kudo and Richardson, 2018), struggle with decompounding, as illustrated in Figure 1. Performance is especially low for compounds where subword boundaries do not coincide with compound constituent boundaries; we term compounds with this property \u2018hard\u2019 compounds (Figure 2).\nIn order to create a more effective decompounding model, we then formulate compound segmentation and normalization as a sequence-to-sequence learning task (Sutskever et al., 2014) and train a byte-level ByT5 model (Xue et al., 2022) using a two-stage framework. In the first stage, we use a novel self-supervised hyphen-prediction objective to learn compound segmentation without any labeled data. In the second stage, we turn the model into a compound normalization model via supervised training on our Wiktionary data. In addition, we introduce a procedure to predict the segmentation of any compound word based on its normalized form, effectively making compound segmentation a subtask of normalization. Finally, we demonstrate that decompounding has real-world applications by investigating compound segmentation for language model tokenization. We apply compound segmentation as pretokenization during training of a SentencePiece tokenizer (Kudo and Richardson, 2018), which results in fewer hard compounds while incurring no extra cost during training and inference of the language model (i.e. the only extra cost occurs during creation of the tokenizer).\nOur Stage 1 models outperform the best prior unsupervised models by 13.9% accuracy on average, while our (supervised) Stage 2 models outperform all prior language-specific decompounding tools. Furthermore, a model trained with a Com-\npoundPiece tokenizer achieves a 5.5% improved performance on compound normalization over an otherwise equivalent SentencePiece model.\nContributions. 1) We introduce a dataset for decompounding of 255k words across 56 languages obtained from Wiktionary. 2) We show that a byte-level language model can efficiently decompound words via a two-stage training framework, whereas current subword-based LLMs fall short. 3) We present a way to improve subword tokenization by performing compound segmentation during creation of the tokenizer. 4) We make our code, models and dataset publicly available at github.com/bminixhofer/compoundpiece."
        },
        {
            "heading": "2 Related Work",
            "text": "Decompounding. Early work in decompounding used word frequency lists along with manually specified suffixes (e.g., a connective -s-) to segment and normalize German compounds (Langer, 1998; Koehn and Knight, 2003). Subsequently, multiple submissions to the Morpho Challenge in morphological segmentation (Kurimo et al., 2010) explicitly or implicitly made use of compound segmentation (Lignos, 2010; Virpioja et al., 2011). Later work replaced the fixed list of suffixes used in Koehn and Knight (2003) by learned morphological operations from parallel corpora (Macherey et al., 2011) or from pre-lemmatized corpora of non-compound words (Ziering and van der Plas, 2016). Another branch of work added more linguistic knowledge in the form of black- and white-lists to the paradigm of Koehn and Knight (2003), resulting in JWordSplitter2 (German) and nl-splitter3 (Dutch); this has only been done for a couple of languages due to its knowledge-intensive nature. CharSplit (Tuggener, 2016) achieves high performance for German by relying on the frequency of character n-grams appearing within the compound.\nWhile the approaches above use (at most) light supervision, there exist supervised approaches which learn directly from an annotated corpus of compounds and their constituents, along with optional auxiliary signals (Biemann et al., 2008; Alfonseca et al., 2008). In contrast, SECOS (Riedl and Biemann, 2016) is a fully unsupervised and language-agnostic method achieving competitive performance by using word embeddings along with word frequencies for semantic compound segmen-\n2github.com/danielnaber/jwordsplitter 3github.com/bminixhofer/ilps-nl-splitter\ntation. Our method improves over SECOS in the unsupervised case and provides a unified alternative to prior language-specific decompounding tools via additional training on labelled data.\nRelation to Morphological Segmentation. Decompounding can be seen as a special case of morphological segmentation (Batsuren et al., 2022a). However, a large amount of work in morphological segmentation focuses on derivational and inflectional morphology (Cotterell et al., 2016; Faruqui et al., 2016; Cotterell et al., 2018; McCarthy et al., 2019; Goldman et al., 2022), which is reflected by datasets such as UniMorph (Batsuren et al., 2022b) and MorphyNet (Batsuren et al., 2021) annotating inflectional and derivational affixes, but not compound constituents. The SIGMORPHON-2022 Shared Task (Batsuren et al., 2022a, SMST 2022) breaks this pattern by providing a dataset for segmentation into compound constituents in addition to inflectional and derivational affixes. We improve on the SMST 2022 dataset by broadening coverage from 9 to 56 languages, as well as handling negatives (i.e., non-compounds) more carefully (\u00a73.1).\nDecompounding Datasets. Besides the SMST 2022 dataset, datasets for decompounding include AuCoPro (van Zaanen et al., 2014) for Dutch and Afrikaans, and the GermaNet dataset for German (Henrich and Hinrichs, 2011). Although there is a significant amount of work studying compound\nterms in languages with highly productive compound formation beyond German and Dutch, such as Finnish and Greek (Pollatsek et al., 2000; Lind\u00e9n and Pirinen, 2009; Koliopoulou, 2014; Shapiro, 2016; Virkkunen et al., 2018), to the best of our knowledge there exist no public datasets for decompounding in these languages (and beyond).\nLinguistically Informed Tokenization. Various studies have tried augmenting or replacing the \u2018linguistically uninformed\u2019 subword-tokenizers used in contemporary LMs (Devlin et al., 2019; Raffel et al., 2020, inter alia) such as SentencePiece (Kudo and Richardson, 2018) and BPE (Sennrich et al., 2016) with linguistic knowledge. Using manually constructed morphological analyzers before applying BPE (Pan et al., 2020) or after generation (Matthews et al., 2018) has led to improvements, but is limited by the availability (and quality) of morphological analyzers across many languages. Unsupervised morphological segmentation has not shown consistent improvements (Zhou, 2018; Saleva and Lignos, 2021; Domingo et al., 2023); see Mielke et al. (2021) for additional discussion."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Dataset Construction",
            "text": "We use words categorized as compound terms on Wiktionary to create a dataset for decompounding. The information on Wiktionary allows associating compound terms with their corresponding normalized constituents. Since Wiktionary only annotates the top-level split,4 we recursively split constituents into their smallest parts by checking if the top-level constituents are themselves compound words. Many prior decompounding tools do not evaluate performance on negative examples (i.e. non-compound words; Koehn and Knight, 2003; Riedl and Biemann, 2016; Tuggener, 2016) since most prior datasets do not contain any (Henrich\n4For instance, highwayman is segmented into highway + man instead of high + way + man.\nand Hinrichs, 2011; van Zaanen et al., 2014). It is not trivial to obtain negative examples from Wiktionary since a large amount of compound words are not categorized as such, leading to many false negatives. We solve this issue by using all normalized compound constituents as negative examples, since by definition the compound constituents can also appear on their own as non-compound words. Note that this way of obtaining negative examples is biased against words which never occur inside compounds; however, we found this to be a rather weak bias (Appendix E). We include every language with at least 100 words, leading to a dataset which covers 56 languages. The number of training examples is shown in Figure 3, example words in Figure 4. We select up to 1,000 words (but at most 50% of total words) in every language as evaluation data. See Appendix A for further details concerning the dataset."
        },
        {
            "heading": "3.2 Two-Stage Training",
            "text": "To overcome the problem of data scarcity in lowresource languages, we introduce a two-stage training procedure for creating dedicated decompounding models. In Stage 1, we train on the selfsupervised objective of restoring hyphenation in words extracted from a large-scale Web corpus, leading to a self-supervised compound segmentation model. In Stage 2, we fine-tune the model on compounds and their normalized constituents from an annotated corpus in a supervised fashion, turning it into a compound normalization model.\nStage 1: Self-Supervised Compound Segmentation. This stage is motivated by the fact that hyphen characters can be seen as a high-precision, lowrecall indicator of compound constituent boundaries, in the same way that newline characters are a high-precision, low-recall indicator of sentence boundaries (Minixhofer et al., 2023). We use this natural segmentation into compound constituents to create a compound segmentation model without requiring any labeled data. First, we obtain all words containing a hyphen plus an equivalent amount of non-hyphenated words from a corpus of unannotated text. Hyphens primarily have two uses: (1) as a compound boundary and (2) to indicate the word continues on the next line. We only want to retain hyphens when they function as compound boundaries, so we filter the instances of (2) by discarding all words where the hyphenated form of the word occurs x \u2264 e\u22126 times less frequent\nWord : akiratis (horizon)x Norm. constituents : {akis (eye), ratas (circle)}c\nInput\nthan the non-hyphenated form.5\nWe strip all words of hyphens and train a seq2seq LM to predict the original (hyphenated) form of each word. We introduce a logit bias b added to the logit of the token representing a hyphen to skew generation towards or away from hyphenation at inference time. Training on this data enables effective compound segmentation without relying on human annotations, as demonstrated later in \u00a75.\nStage 2: Supervised Compound Normalization. In the second stage, we improve upon the Stage 1 model by additional training on labeled data, where the inputs are individual compounds, and the target is to predict the normalized constituents of each compound, separated by a hyphen. Training exclusively on compound normalization allows using data from the collected Wiktionary dataset, which contains compound terms along with their normalized constituents across many languages, but does not contain compound segmentation annotations."
        },
        {
            "heading": "3.3 Turning Normalization into Segmentation",
            "text": "Considering the scarcity of annotated compound segmentation data, it is infeasible to train a multilingual model directly on segmentation. Thus, we introduce a method to predict a segmentation given the normalized constituents. Let x be a word of length n. In addition, we have k normalized com-\n5Consider for example the hyphen-as-compound-boundary in side-experiments and the hyphen-as-newline-indicator in experi-ments. #experi-ments\n#experiments will be considerably lower than\n#side-experiments #sideexperiments . x was chosen from {e\u22124, e\u22125, e\u22126, e\u22127} by manual inspection in preliminary experiments.\npound constituents c = {c1, ..., ck} (e.g. predicted by the Stage 2 model). Our aim is to find boundaries r = {r0, ..., rk}, r0 = 0, rk = n giving rise to the segmentation s = {x[r0 : r1], ...,x[rk\u22121 : rk]}. We approach this problem by minimizing the edit distance of each segment to its corresponding normalized constituent. This leads to an optimization problem where the cost C(s) indicates the total edits needed to turn all segments into their corresponding normalized constituents:\nC(s) = k\u2211\ni=1\nL(si, ci).\nHere, L is an edit distance metric such as Levenshtein distance (Levenshtein et al., 1966). The optimal segmentation s\u22c6 is the segmentation with the minimal cost: s\u22c6 = argminsC(s).\nIn case of ties, we prefer segmentations with higher edit cost for segments with lower indices due to the preference for languages in our training set for suffixation over prefixation (Hammarstr\u00f6m, 2021).6 There is a total of ( n k\u22121 )\npossible segmentations, so solving the optimization problem via enumeration of all solutions is only feasible for short words (Figure 5). We introduce a more efficient search algorithm which is capable of quickly finding the optimal segmentation of long words by enumerating candidates in order of a lower bound on the edit distance in Appendix B. This method can be used to turn the normalization predictions of a model into segmentation. We also use it on the ground-truth normalization from Wiktionary, making it possible to approximate compound segmentation performance by comparing the segmentation corresponding to the ground-truth normalization to the segmentation produced by the model normalization predictions."
        },
        {
            "heading": "3.4 Reducing Hard Compounds",
            "text": "We define hard compounds relative to a particular tokenizer as compound words where the constituent boundaries do not coincide with token boundaries set by the tokenizer. More formally, a compound word made up of k constituents and l subwords is hard if the constituent boundaries r = {r0, ..., rk} are not a subset of the token boundaries t = {t0, ..., tl} i.e. r \u0338\u2282 t.\n6E.g., given x = bridesmaid, c = {bride, maid}, we prefer the segmentation {brides, maid} over {bride, smaid} although their cost is equal.\nWe hypothesize that hard compounds may impair language model performance due to the nontrivial relation of subwords to the compound word. In contrast, in easy compounds the word is naturally decomposed into its constituents. The increased difficulty of hard compounds is apparent on the sequence-to-sequence compound segmentation task: for an easy compound, all tokens can be copied to the output (only the special separator tokens must be inserted). On the other hand, for hard compounds, the tokens change, requiring knowledge of the characters within each token.\nTokenizers where every possible constituent boundary is a token boundary trivially do not give rise to any hard compounds. This includes character-level (Clark et al., 2022; Tay et al., 2022b) as well as byte-level tokenizers (Xue et al., 2022). However, many contemporary language models use subword-based tokenizers to increase efficiency (Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020). We propose a modification to subword tokenization to reduce the number of hard compounds while keeping the efficiency advantages.\nSubword tokenizers typically segment text into pre-tokens (e.g. by splitting on whitespace) before applying their subword tokenization algorithm (Mielke et al., 2021). We propose modifying pretokenization by applying compound segmentation in addition to splitting on whitespace. This modification is only done during creation of the tokenizer, thus incurring no additional cost once the tokenizer has been created. We refer to tokenizers created in this way as CompoundPiece tokenizers. The modified pretokenization tries to create more subwords which do not span compound constituent boundaries, thus decreasing the fraction of hard compounds (Figure 6). It aims to turn the dual-route model for computing the meaning of complex (compound) words proposed by Hofmann et al. (2021) into a single-route model which always computes the meaning of compounds from their constitutent subwords, and never stores a compound word as a single subword."
        },
        {
            "heading": "4 Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1 Data",
            "text": "We obtain Stage 1 data by selecting all words containing a hyphen from a subset of the mC4 corpus (Xue et al., 2021) which results in ~25M hyphenated words. As negative examples, we choose the n most common words from mC4 such that there is an equivalent amount of non-hyphenated and hyphenated words in every language. Regarding the Stage 2 data, see Section \u00a73.1 before."
        },
        {
            "heading": "4.2 Training",
            "text": "We train a decompounding model using a two-stage framework (\u00a73) covering 56 languages. We use ByT5 (Xue et al., 2022) as our main pretrained model and the main starting point since it directly ingests Unicode bytes instead of using subword tokenization, leading to zero hard compounds. We compare our approach against the subword-based T5 (Raffel et al., 2020), Flan-T5 (Chung et al., 2022) and mT5 (Xue et al., 2021) trained with the same two-stage framework. We use t5x (Roberts et al., 2022) for training with a batch size of 512 and a maximum sequence length of 64 tokens, otherwise matching T5 pretraining (Raffel et al., 2020). The setup is the same for Stage 1 and Stage 2."
        },
        {
            "heading": "4.3 Evaluation",
            "text": "Metric. We measure performance via averaged accuracy, i.e., the ratio of examples which are entirely correctly segmented or normalized.\nDatasets. Besides our new Wiktionary evaluation subset, we use the established datasets for particular languages: GermaNet (Henrich and Hinrichs, 2011), AuCoPro for Dutch (van Zaanen et al., 2014) as well the subset containing compound-only words across 6 languages from the SIGMORPHON 2022 Shared Task (Batsuren et al., 2022a).7\nBaselines. We use SECOS as the main unsupervised baseline, as well as CharSplit, JWS and nlsplitter as baselines using different amounts of supervision. For the SIGMORPHON 2022 Shared Task dataset, we compare against the task winner, DeepSPIN-3 (Peters and Martins, 2022).\n7We do not include words containing derivational or inflectional affixes since the type of morpheme is not specified, so it is not possible to distinguish between derivational/inflectional affixes and compound constituents. We also do not include root words since we found from manual inspection that >10% of root words are mislabeled, likely due to the difficulty of obtaining negative examples from Wiktionary (\u00a73.1).\nLanguages. For clarity of presentation, we present results on Danish, German, English, Spanish, Estonian, Greek, Persian, Finnish, Hungarian, Kazakh, Latvian, Dutch, Polish and Swedish as a linguistically diverse subset of languages with productive compound formation in the main paper. For the full evaluation across all languages, see Appendix C."
        },
        {
            "heading": "5 Results and Discussion",
            "text": "Main compound segmentation results are shown in Table 1. For the self-supervised models, we choose the logit bias b = 3 to bias generation towards hyphenated words.8 ByT5 outperforms subwordbased models by a large margin with an absolute 8.9% improvement over the best subword-based model after Stage 1 training, and a 3.7% improvement after Stage 2 training. Comparing models not trained on any annotated data, the self-supervised ByT5 outperforms SECOS on 13 out of 14 languages, and by 13.9% on average.\nWe further compare against language-specific and supervised methods in Table 2. Our ByT5based model outperforms all prior methods on every dataset. Since GermaNet tests compound head segmentation (i.e., even if a word contains multiple constituents, it is only split into a head and a modifier) we count an example as correctly segmented if either the first constituent matches the modifier or the last constituent matches the head.\nEvaluating LLMs on Decompounding. We also evaluate in-context learning performance of multiple LLMs on compound segmentation. We use T5 models with 770M, 3B and 11B parameters (Raffel et al., 2020) as well as the UL2 model with 20B parameters (Tay et al., 2022a) since all of them use the same tokenizer, enabling performance comparisons on hard compounds across LLMs. We use the model versions fine-tuned on the Flan dataset collection (Chung et al., 2022), matching our prompt to the style of instructions in the Flan collection (Appendix D). Zero- to 16-shot results are shown in Figure 7. Although the LLMs perform non-trivially well on easy compounds, performance is close to zero (<3%) on hard compounds. Intriguingly, UL2 20B performs worse than Flan T5 XXL (11B), reversing the trend seen on other tasks (Tay et al., 2022a). All the LLMs perform considerably worse than our ByT5-based model; see Figure 1.\n8Chosen among the set {0, 1, 2, 3, 4} to maximize performance on the English validation data.\nReducing Hard Compounds via CompoundPiece. To evaluate our method of reducing the number of hard compounds in subword-based language models (\u00a73.4), we train CompoundPiece models in two configurations: (i) multilingual tokenizers across all 56 languages and (ii) separate monolingual tokenizers for every language. For the multilingual tokenizers, we sample languages with p(L) \u221d |L|\u03b1 where p(L) is the probability of sampling text from a language L with |L| texts as in prior work (Conneau et al., 2020). We use a subsample of 10M texts from the mC4 corpus (Xue et al., 2021) with \u03b1 = 0.2. The vocabulary size is 250k for the multilingual and 32k for the monolin-\ngual tokenizers, following prior work (Rust et al., 2021; Conneau et al., 2020).\nWe use our fine-tuned ByT5 model for traintime pretokenization into compound constituents and SentencePiece (Kudo and Richardson, 2018) with Unigram LM (Kudo, 2018) as the subword tokenization applied after pretokenization. As a baseline, we train SentencePiece tokenizers with pretokenization into words (split by whitespace) on the same data. Table 3 shows the percentage of hard compounds for every tokenizer. CompoundPiece reduces the number of hard compounds from 27.1%\u2192 9.7% on average in the monolingual case. In the multilingual case, there is a less marked\nimprovement of 23.2% \u2192 16.5%. This may be because tokens from different languages interfere with the segmentation of any given word. We test this hypothesis by computing plausible token origins for tokens in the multilingual tokenizer. This is done by checking which monolingual tokenizers also contain the token in their vocabulary, and ordering the result by unigram token probability. Examples are shown in Table 4. Interference from\ncommon tokens in other languages is likely the lead cause for the increased number of hard compounds in the multilingual tokenizers. It could potentially be solved by adjusting token probability based on the input language; we leave this to future work.\nTo more thoroughly evaluate our tokenization, we train multilingual T5 models using SentencePiece and CompoundPiece. We use the same sampling ratio (\u03b1 = 0.2) of mC4 as for creating the tokenizer, but instead use a subset of 500M texts. We match the architecture and the pretraining setup of the mT5-base model, but train for a total of\n~65.5B tokens.9 We evaluate the model on the decompounding task. Results are shown in Table 5.\nAblation Studies. We quantify the impact of the most significant design choices of our model in Table 6. Although filtering hyphens-as-newlineindicator (\u00a74.1) removes only 300k words (<1%) from the pretraining data, it increases performance on negatives by a large margin. Removing Stage 1 training (i.e., fine-tuning directly on the Wiktionary data instead) consistently decreases performance."
        },
        {
            "heading": "6 Conclusion",
            "text": "We systematically investigated word decompounding tasks of compound segmentation and normalization on a wide scale and in multilingual contexts. To this end, we introduced a dataset of 255k words including compounds and non-compounds across 56 languages from Wiktionary, which allowed us to evaluate performance of LLMs on decompounding. We found that current LLMs\u2019 performance is limited due to hard compounds which arise when subword token boundaries do not coincide with compound constituent boundaries. We then introduced dedicated models for decompounding which use byte-level tokenization to entirely avoid hard compounds. Finally, we used our decompounding models to create novel CompoundPiece tokenizers, keeping the efficiency advantages of subword tokenization while strongly decreasing the amount of hard compounds; this increases the performance of CompoundPiece models over comparable SentencePiece models on the decompounding tasks.\nLimitations\nAlthough self-supervised training in Stage 1 allows for decompounding without any annotated training data, Stage 2 training is limited to languages with sufficient entries in Wiktionary: this excludes extremely low-resource languages. Furthermore, due to computational constraints we have not trained larger models using CompoundPiece tokenization; hence we are unable to report on its benefits at larger scales and on tasks besides decompounding."
        },
        {
            "heading": "Acknowledgements",
            "text": "Ivan Vulic\u0301 is supported by a personal Royal Society University Research Fellowship \u2018Inclusive\n9This equates to 1 16 of mT5\u2019s ~1T tokens, chosen in line with our computational resources.\nand Sustainable Language Technology for a Truly Multilingual World\u2019 (no 221137; 2022\u2013).\nResearch supported with Cloud TPUs from Google\u2019s TPU Research Cloud (TRC).\nWe thank Sebastian Ruder and Srini Narayanan for helpful feedback on a draft of this paper."
        },
        {
            "heading": "A Dataset Statistics",
            "text": "Statistics for the training and validation splits of the Wiktionary dataset are shown in Table 7."
        },
        {
            "heading": "B Efficient Segmentation Algorithm",
            "text": "Pseudocode of the brute-force algorithm to turn normalization into segmentation is shown in Algorithm 1. Since enumerating all possible segmentations is only feasible for short words (\u00a73.3) we introduce a more efficient algorithm (Algorithm 2) where candidate segmentations are ordered such that segmentations with constituents closest in length to the corresponding normalized constituents appear first. Assuming insertions and deletions both have a cost of one (as is the case in standard Levenshtein distance), constituents are thus sorted in increasing order of a lower bound on edit distance. The procedure can stop once the lower bound on edit distance reaches the cost of the best solution found so far since by that point it is impossible for a better solution to be found.\nNote that the normalization-to-segmentation problem is related to sequence partitioning (Manne and Sorevik, 1995; Han et al., 1992) where the aim is to find a partition of a sequence such that the maximum cost across partitions of some cost function is minimized. However, since our goal is to find the partitioning with the minimum aggregated cost, algorithms for conventional sequence partitioning are not applicable."
        },
        {
            "heading": "C Results for All Languages",
            "text": "Segmentation accuracy for all languages is shown in Tables 8-11."
        },
        {
            "heading": "D LLM Prompts",
            "text": "The prompt used for LLM evaluations (\u00a75) is shown in Figure 8. The prompt was chosen among 10 prompts to maximize performance on Flan T5 Large. For 2- to 16-shot results, we provide 50% positive (compound) and 50% negative (noncompound) examples in a random order."
        },
        {
            "heading": "E Quantifying Negative Collection Bias",
            "text": "We conduct an experiment to measure the extent of the bias against words which do not occur inside compounds in our data collection methodology (\u00a73.1). In particular, we quantify the bias against long non-compound words, which usually would not occur inside compounds. We took a\nZero-shot: {word}\nHyphenate the above word. Ans :\nn-shot: {example_0}\nHyphenate the above word. Ans : {example_0_hyphenated}\n...\n{example_n}\nHyphenate the above word. Ans : {example_n_hyphenated}\n{word}\nHyphenate the above word. Ans :\nFigure 8: Prompts used to evaluate LLM in-context learning compound segmentation performance.\nrandom sample of 500 words each from word frequency lists in English and German (Speer, 2022), manually removed compound words, and compared the length statistics of this (unbiased) sample of non-compounds to our non-compound dataset.\nWhile words in our non-compound dataset are indeed shorter on average (6.0 vs. 6.7 chars for English, 6.7 vs. 7.1 chars for German), with less than one character length difference on average, there is only a weak length bias in data collection.\nWe also found qualitatively that our noncompound dataset contains a wide variety of words since compounding is typically a process that can occur for many different root words.\nData: Compound x, norm. constituents c. Result: Optimal segmentation s\u22c6. k \u2190 \u2225c\u2225, n\u2190 \u2225x\u2225 r0 \u2190 0, rn \u2190 n best_cost\u2190\u221e for r1, ..., rn\u22121 \u2208 ( [n] k\u22121 ) do\nCompute s, C(s) /* see \u00a73.3 */ if C(s) < best_cost then\nsbest \u2190 s best_cost\u2190 C(s)\nend end\ns\u22c6 \u2190 sbest Algorithm 1: Na\u00efve brute-force segmentation.\nData: Compound x, norm. constituents c. Result: Optimal segmentation s\u22c6. k \u2190 |c|, n\u2190 |x| r0 \u2190 0, rk \u2190 n best_cost\u2190\u221e\n/* \u2206 is the total difference in length of the normalized constituents to the word. */\n\u2206 = n\u2212 \u2211\ni |ci| lower_bound\u2190 |\u2206|\nwhile lower_bound < best_cost do offsets = {x | |x| = k,\u2211\ni |xi| = lower_bound,\u2211 i xi = \u2206}\nlower_bound\u2190 lower_bound + 1 for o1, ..., ok \u2208 offsets do\nr1, ..., rk\u22121 = |c1|+ o1, ..., \u2211n\u22121 i=1 |ci|+ oi Compute s, C(s) /* see \u00a73.3 */ if C(s) < best_cost then\nsbest \u2190 s best_cost\u2190 C(s)\nend end\nend\ns\u22c6 \u2190 sbest Algorithm 2: Segmentation by enumerating candidates in order of increased lower bound on edit distance."
        }
    ],
    "title": "CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models",
    "year": 2023
}