{
    "abstractText": "Neural language models (LMs) can be used to evaluate the truth of factual statements in two ways: they can be either queried for statement probabilities, or probed for internal representations of truthfulness. Past work has found that these two procedures sometimes disagree, and that probes tend to be more accurate than LM outputs. This has led some researchers to conclude that LMs \u201clie\u201d or otherwise encode non-cooperative communicative intents. Is this an accurate description of today\u2019s LMs, or can query\u2013probe disagreement arise in other ways? We identify three different classes of disagreement, which we term confabulation, deception, and heterogeneity. In many cases, the superiority of probes is simply attributable to better calibration on uncertain answers rather than a greater fraction of correct, high-confidence answers. In some cases, queries and probes perform better on different subsets of inputs, and accuracy can further be improved by ensembling the two.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Kevin Liu"
        },
        {
            "affiliations": [],
            "name": "Stephen Casper"
        },
        {
            "affiliations": [],
            "name": "Dylan Hadfield-Menell"
        },
        {
            "affiliations": [],
            "name": "Jacob Andreas"
        }
    ],
    "id": "SP:8a193078aa8270fcf422069a0db934b074adb05e",
    "references": [
        {
            "authors": [
                "Gavin Abercrombie",
                "Amanda Cercas Curry",
                "Tanvi Dinkar",
                "Zeerak Talat."
            ],
            "title": "Mirages: On anthropomorphism in dialogue systems",
            "venue": "arXiv preprint arXiv:2305.09800.",
            "year": 2023
        },
        {
            "authors": [
                "Amanda Askell",
                "Yuntao Bai",
                "Anna Chen",
                "Dawn Drain",
                "Deep Ganguli",
                "Tom Henighan",
                "Andy Jones",
                "Nicholas Joseph",
                "Ben Mann",
                "Nova DasSarma"
            ],
            "title": "A general language assistant as a laboratory",
            "year": 2021
        },
        {
            "authors": [
                "Amos Azaria",
                "Tom Mitchell."
            ],
            "title": "The internal state of an LLM knows when it\u2019s lying",
            "venue": "arXiv preprint arXiv:2304.13734.",
            "year": 2023
        },
        {
            "authors": [
                "Yonatan Belinkov."
            ],
            "title": "Probing classifiers: Promises, shortcomings, and advances",
            "venue": "Computational Linguistics, 48(1):207\u2013219.",
            "year": 2022
        },
        {
            "authors": [
                "Collin Burns",
                "Haotian Ye",
                "Dan Klein",
                "Jacob Steinhardt."
            ],
            "title": "Discovering latent knowledge in language models without supervision",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova."
            ],
            "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "Proceedings of the Conference of the North American Chapter of",
            "year": 2019
        },
        {
            "authors": [
                "Benj Edwards."
            ],
            "title": "Why ChatGPT and Bing Chat are so good at making things up",
            "venue": "Ars Technica.",
            "year": 2023
        },
        {
            "authors": [
                "Yanai Elazar",
                "Shauli Ravfogel",
                "Alon Jacovi",
                "Yoav Goldberg."
            ],
            "title": "Amnesic probing: Behavioral explanation with amnesic counterfactuals",
            "venue": "Transactions of the Association for Computational Linguistics, 9:160\u2013 175.",
            "year": 2021
        },
        {
            "authors": [
                "Evan Hernandez",
                "Jacob Andreas."
            ],
            "title": "The lowdimensional linear geometry of contextualized word representations",
            "venue": "Proceedings of the Conference on Natural Language Learning.",
            "year": 2021
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Computing Surveys, 55(12):1\u201338.",
            "year": 2023
        },
        {
            "authors": [
                "Saurav Kadavath",
                "Tom Conerly",
                "Amanda Askell",
                "Tom Henighan",
                "Dawn Drain",
                "Ethan Perez",
                "Nicholas Schiefer",
                "Zac Hatfield Dodds",
                "Nova DasSarma",
                "Eli Tran-Johnson"
            ],
            "title": "Language models (mostly) know what they know",
            "year": 2022
        },
        {
            "authors": [
                "Stephanie Lin",
                "Jacob Hilton",
                "Owain Evans."
            ],
            "title": "TruthfulQA: Measuring how models mimic human falsehoods",
            "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "James Edwin Mahon"
            ],
            "title": "The definition of lying and deception",
            "year": 2008
        },
        {
            "authors": [
                "Samuel Marks",
                "Max Tegmark."
            ],
            "title": "The geometry of truth: Emergent linear structure in large language model representations of true/false datasets",
            "venue": "arXiv preprint arXiv:2310.06824.",
            "year": 2023
        },
        {
            "authors": [
                "Marianna Martindale",
                "Marine Carpuat",
                "Kevin Duh",
                "Paul McNamee."
            ],
            "title": "Identifying fluently inadequate output in neural and statistical machine translation",
            "venue": "Proceedings of Machine Translation Summit XVII: Research Track.",
            "year": 2019
        },
        {
            "authors": [
                "Sabrina J Mielke",
                "Arthur Szlam",
                "Emily Dinan",
                "YLan Boureau."
            ],
            "title": "Reducing conversational agents\u2019 overconfidence through linguistic calibration",
            "venue": "Transactions of the Association for Computational Linguistics, 10:857\u2013872.",
            "year": 2022
        },
        {
            "authors": [
                "Yasumasa Onoe",
                "Michael JQ Zhang",
                "Eunsol Choi",
                "Greg Durrett."
            ],
            "title": "CREAK: A dataset for commonsense reasoning over entity knowledge",
            "venue": "NeurIPS Datasets And Benchmarks.",
            "year": 2021
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Shauli Ravfogel",
                "Michael Twiton",
                "Yoav Goldberg",
                "Ryan D Cotterell."
            ],
            "title": "Linear adversarial concept erasure",
            "venue": "Proceedings of the International Conference on Machine Learning.",
            "year": 2022
        },
        {
            "authors": [
                "Abhilasha Ravichander",
                "Yonatan Belinkov",
                "Eduard Hovy"
            ],
            "title": "Probing the probing paradigm: Does probing accuracy entail task relevance",
            "venue": "In Proceedings of the Annual Meeting of the European Association for Computational Linguistics",
            "year": 2020
        },
        {
            "authors": [
                "Maarten Sap",
                "Ronan Le Bras",
                "Daniel Fried",
                "Yejin Choi."
            ],
            "title": "Neural theory-of-mind? On the limits of social intelligence in large lms",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing.",
            "year": 2022
        },
        {
            "authors": [
                "Murray Shanahan."
            ],
            "title": "Talking about large language models",
            "venue": "arXiv preprint arXiv:2212.03551.",
            "year": 2022
        },
        {
            "authors": [
                "Elena Voita",
                "Ivan Titov."
            ],
            "title": "Information-theoretic probing with minimum description length",
            "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
            "year": 2020
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki."
            ],
            "title": "GPT-J6B: A 6 Billion Parameter Autoregressive Language Model",
            "venue": "https://github.com/kingoflolz/ mesh-transformer-jax.",
            "year": 2021
        },
        {
            "authors": [
                "Andy Zou",
                "Long Phan",
                "Sarah Chen",
                "James Campbell",
                "Phillip Guo",
                "Richard Ren",
                "Alexander Pan",
                "Xuwang Yin",
                "Mantas Mazeika",
                "Ann-Kathrin Dombrowski"
            ],
            "title": "Representation engineering: A topdown approach to AI transparency",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Text generated by neural language models (LMs) often contains factual errors (Martindale et al., 2019). These errors limit LMs\u2019 ability to generate trustworthy content and serve as knowledge sources in downstream applications (Ji et al., 2023).\nSurprisingly, even when LM outputs are factually incorrect, it is sometimes possible to assess the truth of statements by probing models\u2019 hidden states. In the example shown in Fig. 1A, a language model assigns high probability to the incorrect answer yes when prompted to answer the question Is Sting a police officer? However, a linear knowledge probe trained on the LM\u2019s hidden representations (Fig. 1B) successfully classifies no as the more likely answer. Knowledge probes of this kind\n*Equal contribution. 1Code at github.com/lingo-mit/lm-truthfulness.\nhave consistently been found to be slightly more reliable at question answering and fact verification than direct queries to LMs (Burns et al., 2022), suggesting a mismatch between LMs\u2019 (internal) representations of factuality and their (external) expressions of statement probability.\nHow should we understand this behavior? One possible interpretation, suggested by several previous studies, is that it is analogous to \u201cdeception\u201d in human language users (Kadavath et al., 2022; Azaria and Mitchell, 2023; Zou et al., 2023). In this framing, if LM representations encode truthful-\nness more reliably than outputs, then mismatches between LM queries and probes must result from situations in which LMs \u201cknow\u201d that a statement is untrue and nonetheless assign it high probability.\nBut knowledge is not a binary phenomenon in LMs or humans. While probes are better than queries on average, they may not be better on every example. Moreover, both LMs and probes are probabilistic models: LM predictions can encode uncertainty over possible answers, and this uncertainty can be decoded from their internal representations (Mielke et al., 2022). There are thus a variety of mechanisms that might underlie query\u2013probe disagreement, ranging from differences in calibration to differences in behavior in different input subsets.\nIn this paper, we seek to understand mismatches between internal and external expressions of truthfulness by understanding the distribution over predictions, taking into account uncertainty in answers produced by both queries and probes. In doing so, we hope to provide a formal grounding of several terms that are widely (but vaguely and inconsistently) used to describe factual errors in LMs. LMs and probing techniques are rapidly evolving, so we do not aim to provide a definitive answer to the question of why LMs assign false statements high probability. However, in a widely studied probe class, two LMs, and three question-answering datasets, we identify three qualitatively different reasons that probes may outperform queries, which we call confabulation (queries produce high-confidence answers when probes are low-confidence), heterogeneity (probes and queries improve performance on different data subsets), and (in a small number of instances) what past work would characterize as deception (Fig. 1C\u2013D, in which queries and probes disagree confidently on answer probabilities). Most mismatches occur when probes or queries are uncertain about answers. By combining probe and query predictions, it is sometimes possible to obtain better accuracy than either alone.\nOur results paint a nuanced picture of the representation of factuality in LMs: even when probes outperform LMs, they do not explain all of LMs\u2019 ability to make factual predictions. Today, many mismatches between internal and external representations of factuality appear attributable to different prediction pathways, rather than an explicit representation of a latent intent to generate outputs \u201cknown\u201d to be incorrect."
        },
        {
            "heading": "2 Preliminaries",
            "text": "Extracting answers from LMs We study autoregressive LMs that compute a next-token distribution pLM(xi | x<i) by first mapping the input x<i to some hidden representation hLM(x<i), then using this representation to predict xi. There are two standard procedures for answering questions using such an LM:\n1. Querying: Provide the question q as input to the LM, and rank answers a (e.g. yes/no) according to pLM(a | q) (Petroni et al., 2019).\n2. Probing: Extract the hidden representation h([q, a]) from the LM, then use question/answer pairs labeled for correctness (or unsupervised clustering methods) to train a binary classifier that maps from hLM to a distribution pprobe(correct | h) (Burns et al., 2022).\nCauses of disagreement Past work has shown that this probing procedure is effective; recent work has shown that it often produces different, and slightly better, answers than direct querying (Burns et al., 2022). Why might this mismatch occur? In this paper, we define three possible cases:\n1. Model Confabulation (Edwards, 2023): Disagreements occurring when probe confidence is low, and the completions from queries are incorrect (mid-left of Figure 1). In these cases probes may be slightly more accurate if their prediction confidence is better calibrated to the probability of correctness. In LMs exhibiting confabulation, a large fraction of disagreements will occur on inputs with large probe entropy Hprobe(correct | (q, a)) and large query confidence pquery(a | q) for any a.\n2. \u201cDeception\u201d (Azaria and Mitchell, 2023), which we define as the set of disagreements in which the probe is confidently correct and the query completion is confidently incorrect (upper-left of Figure 1). In these cases, a large fraction of disagreements will occur on questions to which queries and probes both assign high confidence, but to different outputs.2\n2We use this terminology for consistency with past work, and do not intend any claims about the presence of specific communicative intentions in LMs (q.v. Abercrombie et al., 2023; Shanahan, 2022). In humans, deception involves models of other agents\u2019 mental states (Mahon, 2008) of a kind that are\n3. Heterogeneity: Disagreements resulting from probes and queries exhibiting differential accuracy on specific input subsets (upper-mid of Figure 1). In these cases, probes may outperform queries if the subset of inputs on which probes are more effective is larger than the (disjoint) subset on which queries are more effective.\nThese three categories (along with cases of query\u2013probe agreement, and probe confabulation and error) are visualized in Fig. 1C. Behaviors may occur simultaneously in a single model: we are ultimately interested in understanding what fraction of predictions corresponds to each of these categories.\nDatasets We evaluate predictions on three datasets: BoolQ, a dataset of general-topic yes\u2013no questions derived from search queries (Clark et al., 2019); SciQ, a dataset of crowdsourced multiplechoice science exam questions, and CREAK, a dataset of crowdsourced (true and false) factual assertions (Onoe et al., 2021). We evaluate model behavior on all three datasets via a binary question answering task. SciQ contains multiple wrong answers for each question; we retain the correct answer and select a single distractor.\nModels As the base LM (and implementation of pquery), we use the GPT2-XL (Radford et al., 2019) and GPT-J LMs (Wang and Komatsuzaki, 2021). We query LMs by evaluating the probability they assign to correct answers. In BoolQ and CREAK we re-normalize their output distributions over the strings {true, false}; in SciQ we use provided correct and incorrect answers.3\nProbes While the space of probe designs is large, many recent studies have used linear probes in LMs (Hernandez and Andreas, 2021; Ravfogel et al., 2022; Burns et al., 2022; Marks and Tegmark, 2023).4 We train a linear model (using a logistic\nnot exhibited by the LMs we study (Sap et al., 2022). What we call \u201cdeception\u201d is necessary but insufficient for an LM to \u201cbelieve\u201d one thing but choose to say another.\n3Just as different behavior may be exhibited by different models, it may be induced by different prompts or query formats (Lin et al., 2022). We experimented with different query formatting strategies but found no striking changes in results. However, future work may more systematically study the distribution of query\u2013probe disagreements induced by different prompts.\n4The probing paradigm has limitations. A successful probe does not indicate that the LM necessarily uses the feature being probed for, and an unsuccessful probe does not indicate that the LM does not use the feature being probed for (Ravichander et al., 2020; Elazar et al., 2021; Belinkov, 2022).\nregression objective) to classify (question, answer) pairs as correct or incorrect using the training split of each dataset described above. The input to this linear model is the LM\u2019s final hidden state in the final layer (we did not find substantial differences in probe accuracy using different layers or representation pooling techniques). As in past work, we obtain a distribution over answers by normalizing over pprobe(correct | q, a). Our main results train an ordinary linear model; additional results with a sparse probing objective (as in e.g. Voita and Titov, 2020) are in Appendix A."
        },
        {
            "heading": "3 The success of probes over queries can largely be explained by better calibration",
            "text": "By evaluating accuracy of probes and queries on held-out data, we replicate the finding that probes are more accurate than queries (Table 1). In some cases the difference is small (less than 1% for GPTJ on BoolQ); in other cases, it is substantial (more\nthan 20% on CREAK). Another striking difference between probes and queries is visualized in Fig. 2, which plots the calibration of predictions from GPT-J (e.g. when a model predicts an answer with 70% probability, is it right 70% of the time?). Probes are well calibrated, while queries are reasonably well calibrated on SciQ but poorly calibrated on other datasets. This aligns with the finding by Mielke et al. (2022) that it is possible to obtain calibrated prediction of model errors (without assigning probabilities to specific answers) using a similar probe.\nIt is important to emphasize that the main goal of this work is to understand representational correlates of truthfulness, not to build more accurate models. Indeed, the superior calibration and accuracy of probes over queries might not seem surprising given that probes are trained many-shot while queries use the model zero-shot. To contextualize these results, we also finetuned GPT2-XL on true question/answer pairs and found it performed better overall than probing the pretrained model. Probe predictions exhibited 1.7% better accuracy than fine-tuned model queries on BoolQ. However, querying the fine-tuned model had 6.9% and 1.9% better accuracy on SciQ and CREAK."
        },
        {
            "heading": "4 Most disagreements result from confabulation and heterogeneity",
            "text": "Next, we study the joint distribution of query and probe predictions. Results are visualized in Fig. 3. GPT2-XL and GPT-J exhibit a similar pattern of errors on each dataset, but that distribution varies\nsubstantially between datasets. In BoolQ and SciQ, the query is correct and the probe is incorrect nearly as often as the opposite case; on both datasets, examples of highly confident but contradictory predictions by probes and queries are rare.\nFig. 1D shows the fraction of examples in each of the nine categories depicted in Fig. 1C. Only in CREAK do we observe a significant number of instances of \u201cdeception\u201d; however, we also observe many instances of probe errors. In all other cases (even SciQ, where probes are substantially more accurate than queries), query\u2013probe mismatches predominantly represent other types of disagreement."
        },
        {
            "heading": "5 Queries and probes are complementary",
            "text": "Another observation in Fig. 1 is that almost all datasets exhibit a substantial portion of heterogeneity: there are large subsets of the data in which probes have low confidence, but queries assign high probability to the correct answer (and vice-versa).\nWe can exploit this fact by constructing an ensemble model of the form:\npensemble(a | q) = \u03bb \u00b7 pprobe(a | q) + (1\u2212 \u03bb) \u00b7 pquery(a | q) (1)\nWe select \u03bb using 500 validation examples from each dataset, and evaluate accuracy on the test split. Results are shown at the bottom of Table 1; in 4/6 cases, this ensemble model is able to improve over the probe. While improvements on BoolQ and CREAK are small, on SciQ they are substantial\u2014 nearly as large as the improvement of the probe\nover the base LM. These results underscore the fact that, even when probes significantly outperform queries, query errors are not the source of all mismatches, and heterogeneity in probing and querying pathways can be exploited."
        },
        {
            "heading": "6 Conclusion",
            "text": "We studied mismatches between language models\u2019 factual assertions and probes trained on their internal states, and showed that these mismatches reflect a diverse set of situations including confabulation, heterogeneity, and a small number of instances of deception. In current models, disagreements between internal and external representations of truthfulness appear predominantly attributable to different prediction pathways, rather than a latent intent to produce incorrect output. A variety of other model interventions are known to decrease truthfulness, including carefully designed prompts (Lin et al., 2022), and future models may exhibit more complex relationships between internal representations and generated text (especially for openended generation). Even in these cases, we expect the taxonomy in Fig. 1 to remain useful: not all mismatches between model and probe behavior involve deception, and not all model behaviors are (currently) reducible to easily decodable properties of their internal states."
        },
        {
            "heading": "7 Limitations",
            "text": "As seen in Fig. 1, there is significant heterogeneity in the distribution of disagreement types across datasets. These specific findings may thus not predict the distribution of disagreements in future datasets. As noted in Section 2, our experiments use only a single prompt template for each experiment; we believe it is likely (especially in CREAK) that better prompts exist that would substantially alter the distribution of disagreements\u2014our goal in this work has been to establish a taxonomy of errors for future models. Finally, we have presented only results on linear probes. The success of ensembling methods means that some information must be encoded non-linearly in model representations."
        },
        {
            "heading": "8 Ethical Considerations",
            "text": "Our work is motivated by ethical concerns raised in past work (e.g., Askell et al., 2021; Evans et al., 2021) that LMs might (perhaps unintentionally) mislead users. The techniques presented here might be used to improve model truthfulness or detect\nerrors. However, better understanding of modelinternal representations of factuality and truthfulness might enable system developers to steer models toward undesirable or harmful behaviors.\nFinally, while we have presented techniques for slightly improving the accuracy of models on question answering tasks, models continue to make a significant number of errors, and are not suitable for deployment in applications where factuality and reliability are required."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported by a grant from the OpenPhilanthropy foundation."
        },
        {
            "heading": "A Sparse Probing Results",
            "text": ""
        }
    ],
    "title": "Cognitive Dissonance: Why Do Language Model Outputs Disagree with Internal Representations of Truthfulness?",
    "year": 2023
}