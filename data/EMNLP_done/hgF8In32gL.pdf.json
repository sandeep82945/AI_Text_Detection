{
    "abstractText": "Performant vision-language (VL) models like CLIP represent captions using a single vector. How much information about language is lost in this bottleneck? We first curate CompPrompts, a set of increasingly compositional image captions that VL models should be able to capture (e.g., single object, to object+property, to multiple interacting objects). Then, we train text-only recovery probes that aim to reconstruct captions from single-vector text representations produced by several VL models. This approach does not require images, allowing us to test on a broader range of scenes compared to prior work. We find that: 1) CLIP\u2019s text encoder falls short on more compositional inputs, including object relationships, attribute-object association, counting, and negations; 2) some text encoders work significantly better than others; and 3) text-only recovery performance predicts multimodal matching performance on ControlledImCaps: a new evaluation benchmark we collect and release consisting of fine-grained compositional images and captions. Specifically, our results suggest textonly recoverability is a necessary (but not sufficient) condition for modeling compositional factors in contrastive VL models. We release our datasets and code.",
    "authors": [
        {
            "affiliations": [],
            "name": "Amita Kamath"
        },
        {
            "affiliations": [],
            "name": "Jack Hessel"
        },
        {
            "affiliations": [],
            "name": "Kai-Wei Chang"
        }
    ],
    "id": "SP:792450d175f73acd13b820d68b3082eb6fd4d797",
    "references": [
        {
            "authors": [
                "Peter Anderson",
                "Qi Wu",
                "Damien Teney",
                "Jake Bruce",
                "Mark Johnson",
                "Niko S\u00fcnderhauf",
                "Ian Reid",
                "Stephen Gould",
                "Anton Van Den Hengel"
            ],
            "title": "Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environ",
            "year": 2018
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol."
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Tejas Gokhale",
                "Hamid Palangi",
                "Besmira Nushi",
                "Vibhav Vineet",
                "Eric Horvitz",
                "Ece Kamar",
                "Chitta Baral",
                "Yezhou Yang."
            ],
            "title": "Benchmarking spatial relationships in text-to-image generation",
            "venue": "arXiv preprint arXiv:2212.10015.",
            "year": 2022
        },
        {
            "authors": [
                "Danna Gurari",
                "Yinan Zhao",
                "Meng Zhang",
                "Nilavra Bhattacharya."
            ],
            "title": "Captioning images taken by people who are blind",
            "venue": "ECCV. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Cheng-Yu Hsieh",
                "Jieyu Zhang",
                "Zixian Ma",
                "Aniruddha Kembhavi",
                "Ranjay Krishna."
            ],
            "title": "Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality",
            "venue": "Thirty-Seventh Conference on Neural Information Processing Systems Datasets and",
            "year": 2023
        },
        {
            "authors": [
                "Drew A Hudson",
                "Christopher D Manning."
            ],
            "title": "GQA: A new dataset for real-world visual reasoning and compositional question answering",
            "venue": "CVPR, pages 6700\u20136709.",
            "year": 2019
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc V. Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig."
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "International Conference",
            "year": 2021
        },
        {
            "authors": [
                "Justin Johnson",
                "Bharath Hariharan",
                "Laurens Van Der Maaten",
                "Li Fei-Fei",
                "C Lawrence Zitnick",
                "Ross Girshick."
            ],
            "title": "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning",
            "venue": "CVPR.",
            "year": 2017
        },
        {
            "authors": [
                "Amita Kamath",
                "Christopher Clark",
                "Tanmay Gupta",
                "Eric Kolve",
                "Derek Hoiem",
                "Aniruddha Kembhavi."
            ],
            "title": "Webly supervised concept expansion for general purpose vision models",
            "venue": "ECCV.",
            "year": 2022
        },
        {
            "authors": [
                "Amita Kamath",
                "Jack Hessel",
                "Kai-Wei Chang."
            ],
            "title": "What\u2019s \u201cup\u201d\u2019 with vision-language models? investigating their struggle with spatial reasoning",
            "venue": "EMNLP.",
            "year": 2023
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick."
            ],
            "title": "Microsoft COCO: Common objects in context",
            "venue": "European conference on computer vision, pages 740\u2013755. Springer.",
            "year": 2014
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "ArXiv, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zixian Ma",
                "Jerry Hong",
                "Mustafa Omer Gul",
                "Mona Gandhi",
                "Irena Gao",
                "Ranjay Krishna"
            ],
            "title": "Crepe: Can vision-language foundation models reason compositionally",
            "year": 2022
        },
        {
            "authors": [
                "Cynthia Matuszek",
                "Nicholas FitzGerald",
                "Luke Zettlemoyer",
                "Liefeng Bo",
                "Dieter Fox."
            ],
            "title": "A joint model of language and perception for grounded attribute learning",
            "venue": "ICML.",
            "year": 2012
        },
        {
            "authors": [
                "der",
                "Paul Francis Christiano",
                "Jan Leike",
                "Ryan J. Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback. NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Letitia Parcalabescu",
                "Albert Gatt",
                "Anette Frank",
                "Iacer Calixto."
            ],
            "title": "Seeing past words: Testing the cross-modal capabilities of pretrained V&L models on counting tasks",
            "venue": "Proceedings of the 1st Workshop on Multimodal Semantic Representations",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen."
            ],
            "title": "Hierarchical textconditional image generation with CLIP latents",
            "venue": "arXiv preprint arXiv:2204.06125.",
            "year": 2022
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on",
            "year": 2019
        },
        {
            "authors": [
                "Mark Riedl."
            ],
            "title": "A penguin on mars wearing a spacesuit and walking a robot dog next to santa claus",
            "venue": "Tweet ID 1511745781870514176.",
            "year": 2022
        },
        {
            "authors": [
                "wig Schmidt",
                "Robert Kaczmarczyk",
                "Jenia Jitsev"
            ],
            "title": "LAION-5b: An open large-scale dataset for training next generation image-text models",
            "venue": "In Thirtysixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track",
            "year": 2022
        },
        {
            "authors": [
                "Santi Segu\u00ed",
                "Oriol Pujol",
                "Jordi Vitria."
            ],
            "title": "Learning to count with deep object features",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 90\u201396.",
            "year": 2015
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut."
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "ACL.",
            "year": 2018
        },
        {
            "authors": [
                "Noam Shazeer",
                "Mitchell Stern."
            ],
            "title": "Adafactor: Adaptive learning rates with sublinear memory cost",
            "venue": "International Conference on Machine Learning, pages 4596\u20134604. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "Alane Suhr",
                "Stephanie Zhou",
                "Ally Zhang",
                "Iris Zhang",
                "Huajun Bai",
                "Yoav Artzi."
            ],
            "title": "A corpus for reasoning about natural language grounded in photographs",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Tristan Thrush",
                "Ryan Jiang",
                "Max Bartolo",
                "Amanpreet Singh",
                "Adina Williams",
                "Douwe Kiela",
                "Candace Ross."
            ],
            "title": "Winoground: Probing vision and language models for visio-linguistic compositionality",
            "venue": "CVPR.",
            "year": 2022
        },
        {
            "authors": [
                "Michael Tschannen",
                "Manoj Kumar",
                "Andreas Steiner",
                "Xiaohua Zhai",
                "Neil Houlsby",
                "Lucas Beyer"
            ],
            "title": "Image captioners are scalable vision learners too",
            "year": 2023
        },
        {
            "authors": [
                "Terry Winograd."
            ],
            "title": "Procedures as a representation for data in a computer program for understanding natural language",
            "venue": "Technical report.",
            "year": 1971
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-ofthe-art natural language processing",
            "year": 2019
        },
        {
            "authors": [
                "Yutaro Yamada",
                "Yingtian Tang",
                "Ilker Yildirim."
            ],
            "title": "When are lemons purple? the concept association bias of clip",
            "venue": "arXiv preprint arXiv:2212.12043.",
            "year": 2022
        },
        {
            "authors": [
                "Mert Yuksekgonul",
                "Federico Bianchi",
                "Pratyusha Kalluri",
                "Dan Jurafsky",
                "James Zou"
            ],
            "title": "When and why vision-language models behave like bag-of-words models, and what to do about it? ICLR",
            "year": 2023
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "\u201cA penguin on Mars wearing a spacesuit and walking a robot dog next to Santa Claus.\u201d Riedl (2022)\u2019s text-to-image query is the type that modern multimodal models should be able to support. It is spatially precise (the dog is next to Santa, not in front), compositional (robot dog, but not robot Santa), and imaginative (it is unlikely such an image exists already). However, several recent works have shown that a variety of multimodal models (despite achieving strong benchmark performance) are frequently unable to reason about even simple spatial relations or attribute attachments (Gokhale et al., 2022; Thrush et al., 2022; Yuksekgonul et al., 2023).\nUnderlying several popular multimodal models like CLIP (Radford et al., 2021), DALL-E 2 (Ramesh et al., 2022) and ALIGN (Jia et al., 2021) is a pooled text encoder, i.e., a text representation model that outputs a single vector for a given input caption.1 In this work, we use this representational bottleneck as an interface to ask: how precise are textual representations of visually-descriptive language in these modern multimodal models?\n1Pooled text encoders (c.f., bidirectional multimodal encoders) are used for a variety of practical reasons: e.g., for guided diffusion (Dhariwal and Nichol, 2021), for fast k-NN queries over billions of images (Schuhmann et al., 2022), for contrastive objectives dependent on large batch sizes like Radford et al. (2021)\u2019s 32K example \u201cmini\u201d-batch, etc.\nOur strategy is as follows: for a fixed pooled text encoder T : x \u2192 y, which maps from captions x to vectors y \u2208 Rd, we test how accurately x can be recovered by an expressive generative decoder given y, i.e., P(x|T (x)). In an ideal case, T should result in no information loss, i.e., an exact reconstruction of the original caption should be possible, to account for specific visual factors. However, we hypothesize that if a specific visually descriptive property (e.g., a spatial relation) cannot be accurately decoded from y (using a decoder trained with significant supervision), then it is unlikely a multimodal model can effectively use that property of x using T . Different from existing probes, ours does not require images, enabling exploration of a broader range of captions, e.g., creative text-to-image queries for which there may be no associated image (like \u201cA penguin on Mars...\u201d).\nWe execute our probe using an increasinglycompositional hierarchy of image captions we curate, CompPrompts, which covers cases ranging from a single object with no attributes (e.g. \u201ca cat\u201d) to multiple objects with attributes and relations (e.g. \u201can orange cat to the left of a dog\u201d). We also test counting (e.g. \u201cthree cats\u201d) (Segu\u00ed et al.,\n2015; Parcalabescu et al., 2021) and negations (e.g. \u201ca cat that is not yawning\u201d). We compare five text encoders, and find that top contrastive VL models: (1) are broadly ineffective at textually encoding spatial relations, numbers, and negations; (2) frequently cannot match attributes to their corresponding objects; and (3) fail more as inputs grow more compositional. While some text encoders perform significantly better than others, all underperform a proof-of-concept model which demonstrates that our prompts can indeed be compressed into single vectors with little information loss.\nIn order to verify that our text-only probe predicts performance in multimodal settings, we curate an evaluation set of image-caption pairs, ControlledImCaps, which operationalizes the compositional factors of CompPrompts in a multimodal setting. Results on this corpus suggest our textonly probe gives a necessary condition: if the text-only recovery probe fails to recover a textonly property on CompPrompts, then the associated multimodal model also performs poorly for that property on ControlledImCaps. However, our results also suggest that text-only recoverability is not a sufficient condition: a model can achieve low text-only information loss on a particular prompt type but not fully solve it on ControlledImCaps. To facilitate future probing experiments, we release our code alongside the newly collected CompPrompts and ControlledImCaps corpora at https://github.com/amitakamath/vl_ text_encoders_are_bottlenecks.\n2 Evaluation Corpora\n2.1 CompPrompts\nWe create an evaluation dataset of 18,100 text prompts describing potential visual scenes with varying degrees of specificity and composition. Our starting point is animate nouns with corresponding verbs and adjectives from the Web10K dataset (Kamath et al., 2022). We remove some synonyms to prevent ambiguity in the prompt (e.g. \u201ca rhino to the left of a rhinoceros\u201d).\nThe prompts are increasingly compositional: They have 1 or 2 unique nouns, and 0, 1, or 2 attributes, of which there are 4 types: adjective, verb, spatial, and temporal. Nouns are randomly matched to generate prompts with two unique nouns \u2014 this results in unusual and imaginative text inputs that cannot be guessed based on priors learned during model pre-training (e.g., \u201ca crab lift-\ning a rhino\u201d). The verb and spatial attributes can have either one associated noun (i.e. intransitive, e.g. \u201ca koala yawning\u201d, \u201ca policeman on the left\u201d) or two (i.e. transitive, e.g. \u201ca poet chasing a rabbit\u201d, \u201ca dinosaur left of a tiger\u201d). We also test multiples and negations in the one-attribute setting.\nPrompt examples of each type are given in Tables 2 and 3. There are 300-500 examples of each prompt type in the dataset.\n2.2 ControlledImCaps\nWe create a second evaluation dataset to evaluate the overall vision-language model, rather than the text encoder specifically: where CompPrompts contains text prompts alone, ControlledImCaps contains 600 pairs of images, along with a corresponding caption for each image.\nThe images are sourced from the COCO validation set (Lin et al., 2014), and the captions are handwritten to study one of six specific fine-grained textual properties: spatial relations with one associated noun, spatial relations with two associated nouns, temporal relations, verbs with one associated noun, verbs with two associated nouns, or adjectives. For spatial relations, we evaluate only \u201cleft\u201d and \u201cright\u201d (unlike CompPrompts, which evaluates also \u201cabove\u201d, \u201cunder\u201d, \u201cin front of\u201d, and \u201cbehind\u201d), due to insufficient presence of other spatial relations clearly depicted in the COCO data.\nA key property of ControlledImCaps is that only one word changes between the two captions associated with a given image pair, such that the relation is changed or inverted: e.g., the caption pair \u201ca person before opening an umbrella\u201d, \u201ca person after opening an umbrella\u201d, along with the corresponding images for each (as in Figure 1) tests the overall model\u2019s understanding of temporal relations alone, without conflating any other types of reasoning."
        },
        {
            "heading": "3 Text-only Recovery",
            "text": "For a given text encoder T , our first step is to obtain a training corpus of representations to fit a decoding probe P(x|T (x)). We use (just the text of) Conceptual Captions 3M (Sharma et al., 2018) (CC3M) split into a 90/10 train/val set; this corpus consists of cleaned alt-texts from web images, and thus is similar to the pretraining corpora of many VL models. For P , we use T5-large: specifically, we condition the decoder on T (x), followed by a linear transformation and layer normalization. We train using Adafactor (Shazeer and Stern, 2018) with\na batch size of 512 for 4 epochs over CC3M; we select checkpoints with the lowest val loss. Models are trained using 4xA6000 GPUs with 48GB of memory using Transformers (Wolf et al., 2019) and accelerate2. At evaluation time, we generate captions for CompPrompts set using beam=5 search.\nText Models. We evaluate several T models: CLIP ViT-B/32 (12 layers, 512 dim) and ViTL/14 (12 layers, 768 dim) (Radford et al., 2021), CLIP with a RoBERTa-pretrained text encoder (Liu et al., 2019; Ilharco et al., 2021), and Yuksekgonul et al. (2023)\u2019s more-order-aware CLIP encoder finetuned with hard negatives, negCLIP. For comparison, we also consider the uni-modal SentenceBERT (Reimers and Gurevych, 2019) model all-mpnet-base-v2, which is trained on several sentence similarity datasets including COCO captions (Lin et al., 2014).\nProof-of-concepT5 We also consider a T5-large text encoder that produces a single vector output via mean pooling over the token embeddings. In contrast to the other fixed encoders, we fine-tune this model on CC3M, like an autoencoder3. Then, we use the resulting encoder as a feature extractor, and hand a dimension-shuffled version of the resulting embeddings to the probe. This \u201cproof of concept\u201d encoder is specifically optimized to generate a vector from which a T5 model can decode the full sentence, and serves to validate that our probe setup is even possible.\nEvaluation. We evaluate using exact match (EM). While we report BLEU scores in the Appendix, for our high-precision setting, partial credit metrics are too generous, e.g., generating \u201ca re-\n2https://github.com/huggingface/accelerate 3There is no overlap between CC3M and CompPrompts.\nporter on top of a penguin\u201d as \u201ca penguin on top of a hill\u201d scores 48 BLEU-4 points. Similarly for BERT-Score (Zhang et al., 2020), where generating \u201ctwo rabbits and three shrimps\u201d as \u201cfour of the shrimps and a rabbit\u201d scores 0.91 F1."
        },
        {
            "heading": "3.1 Text-only Recovery Results",
            "text": "Table 1 presents the average exact match of each model over the corpus of prompts in CompPrompts, excluding negations and multiples, which are reported in Table 3. The proof-of-concepT5 model\u2019s high performance illustrates that it is possible in theory to nearly exactly decode all captions in CompPrompts using T5-large, given the \u201cright\u201d encoding4. Beyond proof-of-concepT5, the best performing model is SBERT; and the best performing multimodal model is RoBERTa-CLIP."
        },
        {
            "heading": "3.2 Fine-Grained Results on Different Prompt Types",
            "text": "Table 2 contains EM results of all models on the various types of prompts in CompPrompts. A separate study on multiples and negations in Table 3 shows that text encoders struggle to encode those as well. These results show that it is fairly difficult to decode input sentences from text representations for most VL models, with increasingly compositional categories proving more difficult (e.g., \u201can orange cat\u201d to \u201can orange cat yawning\u201d to \u201can orange cat chasing a dog\u201d).\nSpatial relations. Text encoders of VL models struggle to represent spatial relations (average 23.7\n4Most errors made by proof-of-concepT5 are minor e.g., \u201ctwo physicians on the right\u201d \u2192 \u201ctwo physician on the right\u201d.\nEM), particularly those between two objects (average 13.8 EM). SBERT, in comparison, scores 36.9 and 22.3 EM, respectively.\nTemporal relations. VL models perform poorly on temporal relations, scoring on average 17.1 EM. In comparison, SBERT scores 29.6 EM \u2014 likely because temporal relations appear more frequently in language than in web alt-text.\nTransitive vs intransitive verbs and prepositions. On transitive verbs (e.g., \u201cchasing\u201d), CLIP ViTB/32 and ViT-L/14 do worse by an average of 21 EM than vs. intransitive verbs (e.g., \u201cyawning\u201d), whereas negCLIP and RoBERTa-CLIP do better by an averaged 18.7 points. On transitive prepositions (\u201cto the left of\u201d) instead of intransitive (\u201con the left\u201d), all models do worse by an averaged 35 EM.\nNegations and multiples. Models perform poorly on negations (average EM 13.0) and multiples (average EM 5.1). This agrees with previous observations that VL models struggle with counting (Segu\u00ed et al., 2015; Parcalabescu et al., 2021).\nPrompts where word order matters. VL text encoders struggle to capture word order: on prompts where word order matters less (e.g., \u201ca cat and a dog\u201d), they score an average of 34 EM, but where word order matters more, they score an average of 15.8 EM. The failure cases are often caused by assigning attributes to nouns incorrectly, as highlighted in the Appendix. This extends Thrush et al. (2022)\u2019s and Yuksekgonul et al. (2023)\u2019s finding that contrastive VL models can behave like bagsof-words \u2014 this issue manifests just in the text encoder as well.\nAdjectives and verbs. VL models perform relatively well in the basic one-object, one-attribute setting on both adjectives (average EM 44.8) and verbs (average EM 34.5): even higher than the zero-attribute setting, where error analysis reveals they tend to hallucinate information (\u201ca tarantula\u201d \u2192 \u201ca tarantula in a hand\u201d). While these numbers are well behind SBERT (EM 91.8 and 78.4 respectively), they agree with previous observations that VL models exhibit good visual recognition of basic adjectives and actions (Radford et al., 2021).\nCompositionality. Text encoders struggle with increasingly compositional information, e.g., the probe decodes SBERT(\u201ca dentist after examining an ape\u201d) \u2192 \u201can ape after examining a dentist\u201d. On average, performance on two unique objects drops by 49% from their performance on one unique object (for CLIP ViT-B/32, it drops 71%). VL model performance drops on average by 35% when the prompt contains two attributes compared to one."
        },
        {
            "heading": "3.3 Fine-Grained Results for Different Model Designs",
            "text": "Pre-training the text encoder helps, especially on negations. The average EM of RoBERTa-CLIP on prompts without multiples or negations is 15.7 points higher than CLIP ViT-B/32. However, on the prompts that do include negations, its average\nEM is 29 points higher. This provides evidence that text pre-training the text encoder helps negations, presumably because negations are less likely in alt-texts compared to other settings.\nIncreasing model size helps overall, but not on spatial relations. The average EM of CLIP ViTL/14 on prompts that do not include spatial relations is 20.7 points higher than CLIP ViT-B/32. However, on the prompts that do include spatial relations, its average EM is only 4 points higher. The modest increase of text encoder size in the CLIP training regime appear less reliable for encoding spatial relations than text pre-training or hard negatives (though, more significant scaling could be beneficial, as in Imagen (Saharia et al., 2022)).\nHard negatives from Yuksekgonul et al. (2023) help, especially where word order matters. On average, negCLIP does 15.4 points better than CLIP. On prompts where word order matters (e.g. \u201ca cat chasing a dog\u201d), it scores 16.3 points higher; on prompts where word order does not matter (e.g. \u201ca cat and a dog\u201d), it scores 12.8 points higher."
        },
        {
            "heading": "3.4 Incorrect Model Predictions",
            "text": "We manually inspect models\u2019 incorrect predictions. Decoded VL text encoder predictions often come close (e.g. \u201cthree shrimps\u201d \u2192 \u201cthree of shrimp\u201d is a pattern shown by CLIP ViT-B/32, CLIP ViT-L/14 and negCLIP), whereas SBERT\u2019s incorrect decodings fall further afield (e.g. \u201cthree gardeners\u201d \u2192 \u201cthree gardeners and a third man.\u201d). Thus, while the superior results of the unimodal SBERT compared to the VL text encoders when evaluated in the same frozen-encoder setting (including CLIP ViT-L/14, which has the same text embedding size) show that there is significant room for improvement for VL text encoders, the types of errors made by each model may not be fully captured by EM. Nonetheless, EM remains an appropriate metric for our high-precision setting, as discussed in Section 3."
        },
        {
            "heading": "4 Experiments and Results in the Multi-modal Setting",
            "text": "We investigate the hypothesis that if a textual property cannot be decoded from the VL text encoder\u2019s vector representation with a highly expressive decoder (like T5-Large), then it also cannot be readily modeled in the multimodal setting. ControlledImCaps studies the attributes from CompPrompts in the multimodal setting. We then compare the text\nencoder performance of a VL model on a particular prompt type in CompPrompts with the performance of the overall VL model on that prompt type in ControlledImCaps. As discussed in Section 2.2, the two captions in every example differ by only one word which changes or inverts the relation, allowing us to perform fine-grained analyses in controlled settings without conflating multiple types of compositional reasoning. Figure 3 depicts the six types of attributes studied in ControlledImCaps, their corresponding prompt type in CompPrompts, and an example of each.\nVL Models. We evaluate the same VL models as in Section 3: CLIP ViT-B/32, CLIP ViT-L/14, CLIP with a RoBERTa-pretrained text encoder (Liu et al., 2019; Ilharco et al., 2021), and negCLIP (Yuksekgonul et al., 2023). Each of these models can return a score when given an image and a caption, representing how well they match.\nEvaluation. We follow the evaluation scheme from Winoground (Thrush et al., 2022): for a given pair of images with corresponding captions, we measure both a text score, the fraction of instances where a model scores the correct caption higher\nthan the incorrect caption when given an image, and an image score, the fraction of instances where a model scores the correct image higher than the incorrect image when given a caption."
        },
        {
            "heading": "4.1 Multi-modal Results",
            "text": "Table 4 presents the results of CLIP ViT-L/14 on both CompPrompts and ControlledImCaps (all model results in Appendix). The CompPrompts results correspond to the prompt type(s) most closely matching the captions in ControlledImCaps (specified in Figure 3). For the spatial relations, for this table alone, we calculate the EM on the data points in CompPrompts containing \u201cleft\u201d and \u201cright\u201d spatial relations only due to lack of sufficient support in COCO for other spatial relations, as discussed in Section 2.2. On prompt types where the text encoder performance on CompPrompts is poor, the overall model performance on ControlledImCaps is also poor: showing that the text encoder does indeed bottleneck VL models\u2019 compositionality.\nWe see similar findings per prompt type and model design as those discussed in Section 3.3."
        },
        {
            "heading": "4.2 Fine-Grained Results on Different Prompt Types",
            "text": "We discuss findings on the prompt types in ControlledImCaps, with 95% confidence intervals.\nModels do poorly on spatial relations. On average, VL models perform poorly on spatial relations, achieving an average image | text score of 2.5 | 12.4 (\u00b1 2.2 | 3.7). Their text encoder performance on the corresponding prompts in CompPrompts was similarly poor, with an average EM of 27.8. This agrees with Kamath et al. (2023), which shows that VL models struggle with spatial relations.\nModels do poorly on temporal relations. VL performs poorly on temporal relations, with an average image | text score of 5.3 | 30.8 (\u00b1 2.7 | 4.8). Their text encoder performance on CompPrompts temporal reasoning was similarly low at 18.9 EM.\nModels do well on verbs and adjectives. VL models perform well on verbs (average image | text score 65.4 | 78.1, \u00b1 5.0 | 4.8) and even better on adjectives (average image | text score 78.5 | 89.0, \u00b1 7.0 | 3.5), mirroring their text encoder performance on CompPrompts, where the average EM for verbs and adjectives were 33.7 and 44.8 respectively.\nTwo-object verbs are more difficult than oneobject verbs. We find that for all models, twoobject verbs are harder than one-object verbs, with the former achieving an image | text score of 52.3 | 68.5 and the latter 78.5 | 87.8 (with p < 0.05 under the Wilcoxon signed-rank test). This follows performance on CompPrompts for ViT-B/32 and ViT-L/14, but not for negCLIP and RoBERTa-CLIP, hinting that ability to reconstruct is necessary but not sufficient, as discussed in Section 4.5."
        },
        {
            "heading": "4.3 Fine-grained results on different model design choices",
            "text": "We discuss findings on the model designs in ControlledImCaps. All findings are statistically significant at p < 0.05 using the Wilcoxon signed-rank test to compare models.\nPre-training the text encoder improves text score on verbs. RoBERTa-CLIP obtains a higher text score than CLIP ViT-B/32 (78.0 vs 68.0), as well as a higher EM on the prompts in CompPrompts corresponding to verbs (39.4 vs 11.0).\nIncreasing model size does not help on spatial or temporal reasoning. On both spatial and temporal reasoning inputs, ViT-L/14 performance on ControlledImCaps was not statistically significantly higher than that of ViT-B/32.\nHard negatives from Yuksekgonul et al. (2023) help where word order matters. On prompts where word order matters, negCLIP scores an image | text score of 36.5 | 50.0 and a CompPrompts EM of 27.2, and other models score an average image | text score of 24.8 | 37.5 and a CompPrompts EM of 21.0. negCLIP also outperforms ViT-B/32 on all prompts on average."
        },
        {
            "heading": "4.4 Text reconstruction appears to be necessary...",
            "text": "To study the relationship between text reconstruction and overall model performance beyond Table 4, we evaluate text reconstruction on ControlledImCaps. Specifically, we use the trained T5 decoders from Section 3 and try to reconstruct the input when ControlledImCaps text inputs are evaluated. On the cases where the reconstruction is incorrect according to human evaluation5 on either of the two text inputs, the overall model Image Score on ControlledImCaps for CLIP ViT-L/14 is zero 96% of the time, and the Text Score is zero 83% of the time. This text reconstruction vs. multimodal matching correlation is more direct compared to the similar correlation reported in Table 4 because we compare on the same instances."
        },
        {
            "heading": "4.5 ... but insufficient.",
            "text": "Conversely, just because a model performs well on the CompPrompts probe does not mean it will perform well on ControlledImCaps. For example, ViT-L/14 outperformed ViT-B/32 overall on CompPrompts, but not (statistically significantly) on ControlledImCaps. Also, RoBERTa-CLIP outperforms ViT-B/32 on temporal relations on CompPrompts, but achieves a worse text score on ControlledImCaps. When we evaluate text reconstruction on ControlledImCaps, on cases where the reconstruction is correct for both text inputs, the overall model Image Score on ControlledImCaps for ViT-L/14 is zero 59% of the time, and the Text Score is zero 47% of the time. This suggests that text recoverability is a necessary but insufficient condition for overall model performance. The insufficiency is intuitive, as multimodal errors could potentially stem from the image encoder."
        },
        {
            "heading": "4.6 A Note on Winoground",
            "text": "We evaluate our four VL models on the Winoground dataset (Thrush et al., 2022). They perform poorly, with an average image | text score of 10.3 | 30.8, where random chance is 25.0 | 25.0. However, on shorter inputs (5 words or less) which exhibit fewer compositional concepts on average, e.g., \u201ca bird eats a snake\u201d | \u201ca snake eats a bird\u201d, the four models achieve higher scores of 20.4 | 47.2 on average. On longer (over 10 words), more\n5For the simple inputs of ControlledImCaps, we found human evaluation by the authors tractable, with the added advantage of not penalizing minor errors as EM does.\ncompositional inputs, e.g. \u201cin the stadium, the person wearing gray outperformed the one wearing blue\u201d | \u201cin the stadium, the person wearing blue outperformed the one wearing gray\u201d, models achieve a much lower score of 3.4 | 18.5. This mirrors our finding on CompPrompts that VL text encoders struggle with increasingly compositional inputs."
        },
        {
            "heading": "5 Related work",
            "text": "Building models capable of reasoning jointly about visual and textual inputs is a long-standing goal of AI (Winograd, 1971), with potential applications in the fields of vision-language navigation (Anderson et al., 2018), human-robot interaction (Matuszek et al., 2012), accessible image captioning (Gurari et al., 2020), etc.\nRecent challenge datasets have been designed to probe the capacity of multimodal models to represent descriptions of precise visual compositions (Johnson et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019; Thrush et al., 2022). Yuksekgonul et al. (2023) and Yamada et al. (2022) study CLIP specifically, demonstrating its shortcomings (and some potential fixes) in terms of modeling syntax. Ma et al. (2022) study OpenCLIP models for various types of compositional reasoning, with programmatically sourced hard negatives. Different from these works, our textual probe does not require access to images.\nOur image-and-text evaluation most closely resembles Thrush et al. (2022). However, we stratify the examples based on type of input (e.g., temporal relations) to provide more detailed insights. We also keep our prompts relatively simple, never having more than two objects or two attributes in the input. We believe this is a more realistic goal for our current vision-language models. The word order shuffling aspect is also discussed in Yuksekgonul et al. (2023). However, as their proposed benchmark does not provide pairs of images with corresponding captions, it is possible to achieve state-of-the-art with a text-only model (specifically, 2-shot ChatGPT6 (Ouyang et al., 2022), details in Appendix and the recent Hsieh et al. (2023)). While this does not detract from their finding that visionlanguage models ignore word order, our benchmarks have an additional advantage of being insensitive to text-only priors.\n6https://platform.openai.com/docs/ api-reference/chat, using the gpt-3.5-turbo model"
        },
        {
            "heading": "6 Conclusion and Discussion",
            "text": "We present probing results that suggest significant information loss upon text encoding of compositional inputs in vision and language models. This information loss is quantified using CompPrompts, a test set of increasingly compositional image descriptions, and ControlledImCaps, a test set that we use to verify that this information loss affects the performance of multimodal models on compositional inputs. Harder negatives, more text pretraining, and larger models all improve encoder quality, but information is still lost even for the most performant models, compared to the unimodal SBERT as well as a T5-based auto-encoder.\nGoing forward, even more difficult test sets than CompPrompts and ControlledImCaps might be required to analyze and evaluate vision-language model capabilities. Returning to Riedl (2022)\u2019s tweet from the intro, \u201cA penguin on Mars wearing a spacesuit and walking a robot dog next to Santa Claus.\u201d, even our highly accurate proof-ofconcepT5 model struggles, predicting: \u201ccompulsory penguin onexposition wearing a spacesuit and walking a dog robot next tohoc\u201d. To support imaginative text-to-image generation queries (for images that may not exist yet), future work would be wellsuited to design text encoders that can generalize to captions that contain compositions never-beforeseen in web alt-text corpora.\nOur probing results suggest two future modeling directions: (1) Modifying contrastive VL models\u2019 training objectives to additionally encourage ability to reconstruct the text input, either through an additional reconstruction loss on the text encoder during finetuning, or through the addition of even harder negatives than Yuksekgonul et al. (2023) and Ma et al. (2022), would be an exciting avenue for future work. Alternatives to contrastive training, such as captioning, have also shown promise in recent work (Tschannen et al., 2023); and (2) explicitly encouraging linear recovery with a modified loss function: while the gap between SBERT and the VL Text encoders can be partially explained by the superior pooling method and training data, SBERT\u2019s training objective does not require linear recoverability (whereas CLIP\u2019s dot product interaction term might): explicitly encouraging linear text-text recoverability might improve multimodal performance. Finally, we hope that ControlledImCaps can facilitate research beyond single-vector bottleneck VL models.\nLimitations\nFirst, our probing method involves a pre-trained T5 decoder. It is possible that language biases from the pre-training emerge while decoding from the VL text embedding, e.g., predicting \u201ca dog chasing a cat\u201d instead of \u201ca cat chasing a dog\u201d because the former is more likely under the T5 decoder\u2019s priors from pre-training. However, as the methodology is the same across all models we evaluate, we believe that the evaluation is fair. Second, we evaluate with only one probe, whereas probing with complementary methods (e.g., especially deterministic ones, like a convex linear probe) could reveal more insights. Third, text encoders that do well on our evaluation may not perform well if directly plugged into a contrastive VL model like CLIP, if the text encoders were not trained to encode the information in a manner that is linearly recoverable."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors thank John Hewitt, Akhila Yerukola, and anonymous reviewers for useful discussion and feedback. This work was funded by the Allen Institute for AI. AK was additionally supported by the UCLA Computer Science Department FirstYear Fellowship. KC was supported in part by U.S. DARPA MCS Program under contract number N660011924032, U.S. DARPA ECOLE Program No. HR00112390060, and ONR N00014-231-2780, and a Sloan Fellowship. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, or the U.S. Government."
        },
        {
            "heading": "A Additional Results",
            "text": "Table 5 contains average BLEU-4 scores of the models. Table 6 contains a study of model performance on object-attribute association in CompPrompts. Table 7, Table 8 and Table 9 contain results of other models on ControlledImCaps in comparison to CompPrompts (ViT-L/14 is discussed in Table 4). Table 10 discusses text-only results on the ARO benchmark (Yuksekgonul et al., 2023)."
        }
    ],
    "title": "Text encoders bottleneck compositionality in contrastive vision-language models",
    "year": 2023
}