{
    "abstractText": "A proper evaluation of stories generated for a sequence of images\u2014the task commonly referred to as visual storytelling\u2014must consider multiple aspects, such as coherence, grammatical correctness, and visual grounding. In this work, we focus on evaluating the degree of grounding, that is, the extent to which a story is about the entities shown in the images. We analyze current metrics, both designed for this purpose and for general vision-text alignment. Given their observed shortcomings, we propose a novel evaluation tool, GROOViST, that accounts for cross-modal dependencies, temporal misalignments (the fact that the order in which entities appear in the story and the image sequence may not match), and human intuitions on visual grounding. An additional advantage of GROOViST is its modular design, where the contribution of each component can be assessed and interpreted individually.",
    "authors": [
        {
            "affiliations": [],
            "name": "Aditya K Surikuchi"
        },
        {
            "affiliations": [],
            "name": "Sandro Pezzelle"
        },
        {
            "affiliations": [],
            "name": "Raquel Fern\u00e1ndez"
        }
    ],
    "id": "SP:0c75e804e48cd18206f7f0d6403934e64a1cf846",
    "references": [
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
            "year": 2005
        },
        {
            "authors": [
                "Marc Brysbaert",
                "Amy Beth Warriner",
                "Victor Kuperman."
            ],
            "title": "Concreteness ratings for 40 thousand generally known english word lemmas",
            "venue": "Behavior Research Methods, 46:904\u2013911.",
            "year": 2014
        },
        {
            "authors": [
                "Fredrik Carlsson",
                "Philipp Eisen",
                "Faton Rekathati",
                "Magnus Sahlgren."
            ],
            "title": "Cross-lingual and multilingual CLIP",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 6848\u20136854, Marseille, France. European Language",
            "year": 2022
        },
        {
            "authors": [
                "Hong Chen",
                "Yifei Huang",
                "Hiroya Takamura",
                "Hideki Nakayama."
            ],
            "title": "Commonsense knowledge aware concept selection for diverse and informative visual storytelling",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 35(2):999\u20131008.",
            "year": 2021
        },
        {
            "authors": [
                "Jack Hessel",
                "Ari Holtzman",
                "Maxwell Forbes",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "CLIPScore: A reference-free evaluation metric for image captioning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Micah Hodosh",
                "Peter Young",
                "J. Hockenmaier."
            ],
            "title": "Framing image description as a ranking task: Data, models and evaluation metrics (extended abstract)",
            "venue": "J. Artif. Intell. Res., 47:853\u2013899.",
            "year": 2013
        },
        {
            "authors": [
                "Xudong Hong",
                "Asad Sayeed",
                "Khushboo Mehra",
                "Vera Demberg",
                "Bernt Schiele."
            ],
            "title": "Visual writing prompts: Character-grounded story generation with curated image sequences",
            "venue": "Transactions of the Association for Computational Linguistics, 11:565\u2013581.",
            "year": 2023
        },
        {
            "authors": [
                "Junjie Hu",
                "Yu Cheng",
                "Zhe Gan",
                "Jingjing Liu",
                "Jianfeng Gao",
                "Graham Neubig."
            ],
            "title": "What makes a good story? designing composite rewards for visual storytelling",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7969\u20137976.",
            "year": 2020
        },
        {
            "authors": [
                "garet Mitchell"
            ],
            "title": "Visual storytelling",
            "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2016
        },
        {
            "authors": [
                "Taehyeong Kim",
                "Min-Oh Heo",
                "Seonil Son",
                "KyoungWha Park",
                "Byoung-Tak Zhang."
            ],
            "title": "GLAC Net: GLocal Attention Cascading Networks for Multi-image Cued Story Generation",
            "venue": "CoRR, abs/1805.10973.",
            "year": 2018
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "GloVe: Global vectors for word representation",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar.",
            "year": 2014
        },
        {
            "authors": [
                "Bryan A. Plummer",
                "Liwei Wang",
                "Chris M. Cervantes",
                "Juan C. Caicedo",
                "Julia Hockenmaier",
                "Svetlana Lazebnik."
            ],
            "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models",
            "venue": "Proceedings of the IEEE In-",
            "year": 2015
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Hareesh Ravi",
                "Kushal Kafle",
                "Scott Cohen",
                "Jonathan Brandt",
                "Mubbasir Kapadia."
            ],
            "title": "Aesop: Abstract encoding of stories, objects and pictures",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2052\u20132063.",
            "year": 2021
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun."
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "Ramakrishna Vedantam",
                "C Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "CIDEr: Consensus-based image description evaluation",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\u20134575.",
            "year": 2015
        },
        {
            "authors": [
                "Eileen Wang",
                "Caren Han",
                "Josiah Poon"
            ],
            "title": "RoViST: Learning robust metrics for visual storytelling",
            "year": 2022
        },
        {
            "authors": [
                "Xin Wang",
                "Wenhu Chen",
                "Yuan-Fang Wang",
                "William Yang Wang."
            ],
            "title": "No metrics are perfect: Adversarial reward learning for visual storytelling",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2018
        },
        {
            "authors": [
                "C.L. Zitnick",
                "Devi Parikh."
            ],
            "title": "Bringing semantics into focus using visual abstraction",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2013
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Generating a textual story that is plausible given a sequence of images is a challenging task involving aspects such as cross-modal interactions, temporal dependencies between linguistic and visual content, and causal reasoning. In the language-and-vision community, Huang et al. (2016) operationalized the task and released the Visual Storytelling Dataset (VIST), a collection of English stories created by speakers on top of 5-image visual sequences. Several models have been proposed for the task of generating plausible stories for a given sequence, ranging from RNNs (Kim et al., 2018) to Transformers, trained either end-to-end or leveraging additional knowledge-graphs (Chen et al., 2021).\nEvaluating the quality of the automatically generated stories is extremely difficult: Given the creative nature of the task (many stories could be sensible for a given image sequence), referencebased metrics like METEOR (Banerjee and Lavie, 2005) or CIDEr (Vedantam et al., 2015) are not\nappropriate\u2014they indeed poorly correlate with human judgments (Wang et al., 2018). Moreover, a proper evaluation must consider multiple aspects, such as coherence, grammaticality and, importantly, visual grounding. Yet, most evaluation metrics proposed specifically for visual storytelling do not consider the images at all (Hu et al., 2020).\nIn this paper, we focus on evaluating a story\u2019s degree of grounding, that is, the extent to which a story is about the entities shown in the images. To the best of our knowledge, there is only one metric proposed to date for evaluating grounding in visual storytelling, the Visual Grounding scorer (RoViST-VG) by Wang et al. (2022). We carry out an extensive analysis of this metric and reveal that it has critical shortcomings. To overcome this, we propose a novel, modular evaluation tool, which we name GROOViST (grounding objects in visual storytelling). We show that GROOViST is robust to temporal misalignments, correlated with human intuitions about grounding, and easy to interpret. Our code is available at: https://github.com/akskuchi/groovist"
        },
        {
            "heading": "2 Analyses of Existing Metrics",
            "text": "To assess the level of visual grounding of a story in visual storytelling, Wang et al. (2022) proposed\nRoViST-VG. This metric is the output of a model pre-trained on the Flickr30K Entities dataset (Plummer et al., 2015) to learn the relationships between the nouns in a story and the regions of an image in a contrastive learning regime. For a given <imagesequence, story> pair, RoViST-VG extracts: from each image, the bounding boxes and corresponding visual features of its 10 most salient regions, using FasterRCNN (Ren et al., 2015); from the story, the GloVe (Pennington et al., 2014) representations of each noun in it. The pre-trained model receives these extracted embeddings (from GLoVe and FasterRCNN) and returns the final representations T and I , respectively. The grounding score is then calculated using Eq. (1) as the maximum cosine similarity between T and I , weighted by inverse document frequencies (idf ) of the nouns.1\nRoViST-VG = log |Te|\u2211 i=1 exp(idf(Ti) max Ie,j\u2208Ie (cos(Te,i, Ie,j))) (1)\nTo analyze the suitability of RoViST-VG, we compare it to CLIPScore (Hessel et al., 2021). CLIPScore has not been designed to evaluate visual storytelling. Here, we use it to score each imagesentence pair independently in a story sequence. This approach is not ideal as it cannot capture temporal misalignments between a text and a visual content (e.g., an early sentence may be \u2018they were getting ready to go to the circus\u2019 but the circus may only appear later). However, since CLIPScore has been designed for general vision-text alignment, we expect it to be reasonably effective at capturing visual grounding at the image-sentence level. It corresponds to the cosine similarity between CLIP\u2019s (Radford et al., 2021) representations of a sentence c and an image v (with 2.5 as re-scaling factor).\nNext we explore how good the above metrics are at capturing grounding in visual storytelling data."
        },
        {
            "heading": "2.1 Grounding in visual storytelling datasets",
            "text": "We analyze the scores assigned by these metrics to the stories in three visual storytelling datasets: (1) VIST (Huang et al., 2016), that comprises sequences of five natural images (from Flickr) and corresponding five-sentence stories; (2) AESOP (Ravi et al., 2021), that includes sequences of three synthetic images (created using entities from Abstract Scenes; Zitnick and Parikh, 2013) and corresponding three-paragraph long stories; (3) VWP (Hong et al., 2023), which comprises sequences\n1More details on RoViST-VG are provided in Appendix C.\nof movie shots, each including 5-10 images with corresponding sentences that make up their stories.\nWe compute RoViST-VG and CLIPScore on the original <image sequence, story> pairs in the test splits of these datasets,2 and compare these scores to the ones obtained on a random setting where each image sequence is paired with five random stories (from the corresponding dataset); among these, we consider the pair that receives the highest score. We expect a metric that properly captures visual grounding to assign higher scores to the original stories than to the randomly paired stories.\nFigure 2 shows the average scores of the metrics in both settings. Surprisingly, RoViST-VG scores are not higher in the original setting than in the random setting. In fact, on VIST, the random <image sequence, story> pairs receive higher RoViST-VG scores than the original ones. In contrast, CLIPScore follows the expected pattern."
        },
        {
            "heading": "2.2 Correlation with Flickr8k-Expert ratings",
            "text": "We assess the ability of the two metrics to capture general image-caption grounding using Flickr8kExpert (Hodosh et al., 2013), a publicly available dataset with human ratings for image-caption pairs. In particular, we consider the subset of 3391 samples where all three annotators agree.3 CLIPScore is designed for this purpose and is therefore wellsuited for the task. RoViST-VG is not meant for measuring image-caption grounding, although it should align with human ratings to some extent, given its purpose and pre-training. However, as we can see in Table 1, RoViST-VG shows no correlation with human ratings\u2014while CLIPScore does.\n25055 samples for VIST and 991 for AESOP. Due to the lack of a separate test split for VWP, we considered all 13843 samples in the dataset.\n3Human annotators rated captions on a scale of 1 to 4."
        },
        {
            "heading": "3 GROOViST",
            "text": "Our analyses showed that RoViST-VG has some important limitations as a metric for assessing the degree of visual grounding\u2014both in stories and image captions. To overcome these issues, we propose GROOViST, a modular metric consisting of various components informed by insights from both CLIPScore and RoViST-VG. These are:\nNoun phrase (NP) extraction We process the story and extract all the NPs;4 this is similar to RoViST-VG but better because RoViST-VG only considers nouns and fails to handle compounds such as \u2018parking lot\u2019. Additionally, focusing on NPs allows for the contribution of accompanying adjectives (e.g., \u2018silly faces\u2019).\nVision-language alignment We compute alignment scores between all the extracted bounding boxes and NPs and select the highest score for each NP. This step is similar to RoViST-VG but, instead of training a dedicated model, we use the off-theshelf CLIP (Radford et al., 2021) model.\nPenalizing poorly grounded NPs The previous steps result in a positive score for all the NPs in a story. Yet, some may in fact be poorly grounded (i.e., have low visual alignment score). Such NPs, therefore, should contribute negatively to the overall degree of grounding of a story. To operationalize this, we select the mean score over all NPs in the entire dataset as a threshold \u03b8 and calculate the distance of each NP\u2019s score from \u03b8, assigning negative values to NPs with scores below \u03b8 (NPneg) while retaining the scores of NPs with values above \u03b8 (NPpos).\nConcreteness weighting RoViST-VG uses inverse document frequencies (idf ) for weighting the similarity scores of nouns to handle abstract frequent words such as \u2018time\u2019. However, we observe that idf weights tend to increase the similarity scores of some less-frequent non-grounded\n4Using spaCy\u2019s English transformer pipeline for chunking: https://spacy.io/models/en#en_core_web_trf\nnouns and decrease the scores of some frequentand-grounded nouns, adversely affecting the overall score.5 Hence, after the penalization step, we use word concreteness ratings (Brysbaert et al., 2014) for weighting the resulting scores (instead of idf ) and capture the fact that concrete NPs are more likely to be visible.6\nNormalization Finally, to obtain the GROOViST score of a story, we aggregate the weighted scores of all its NPs and normalize the sum by the total number of NPs in the story, which results in a value unaffected by story length (or more precisely, by the number of NPs in it):\n( \u2211n i=1NPposi + \u2211m i=1NPnegi) / (n+m) (2)\nwhere n and m are the number of NPs with positive and negative scores, respectively. See Figure 1 for how this facilitates interpretability. The pseudocode and a working example for GROOViST are provided in Algorithm 2 and Figure 4, respectively. GROOViST scores are unbounded by default, but tanh can be used to map them to the [\u22121, 1] range."
        },
        {
            "heading": "4 Role of GROOViST Components",
            "text": "We test GROOViST on the same evaluation criteria used in Section 2. From Figure 2 and Table 1, we observe that GROOViST fares well on both evaluation criteria. First, it assigns higher grounding scores to original compared to random stories. Second, it moderately correlates with human imagecaption ratings. This indicates that GROOViST is a more robust metric than RoViST-VG.\nTo understand the impact of GROOViST\u2019s components on the final grounding score, we conduct several experiments by both ablating the components and replacing them with plausible alternatives.\nAblations Penalizing poorly grounded NPs and Concreteness weighting are the two components of GROOViST that can be ablated from the metric.\nReplacements The Concreteness weighting and Noun phrase (NP) extraction components of GROOViST can be replaced with idf weights and nouns, respectively.\nIn total, we consider six alternative versions of our metric, which we obtain by applying all possible combinations of ablations and replacements.\n5Examples are provided in Appendix A. 698.7% of NPs in the VIST test set contain words for which\nconcreteness ratings are available.\nWe test these versions on the same evaluation criteria used in Section 2. Table 2 reports how they fare with respect to the two criteria we consider.\nWe observe that ablating or replacing components from GROOViST results in scores that either do not meet at least one of the criteria or do so to a much lower extent.7 This is particularly apparent in the metric versions where the Penalizing poorly grounded NPs component is ablated, which further confirms its importance. The GROOViST (-C +idf ) version satisfies Criterion 1, indicating that frequency-based information can be helpful as a heuristic. However, it may result in discrepancies as shown in Appendix A, Figure 4. We consider concreteness to be a more theoretically motivated notion than frequency to capture visual grounding. Its value is apparent with respect to Criterion 2: replacing Concreteness weighting with idf weighting decreases the correlation of the metric scores with Flickr8k-Expert ratings."
        },
        {
            "heading": "5 Evaluation of GROOViST",
            "text": "To further evaluate the extent to which GROOViST captures intuitions on stories\u2019 degree of visual grounding, we compare our metric to human judgments. Since no previous work collected human data for this specific purpose, we run a small data collection by asking 5 participants to rate a sample of the VIST data. In particular, we ask participants to provide ratings for 100 randomly sampled VIST <image sequence, story> pairs, using a 4- point Likert-like scale (instructions: \u201ca score of 4\n7The resulting values are provided in Appendix E.\nindicates that most aspects mentioned in the story are depicted in the sequence of images\u201d).8 We formulate two hypotheses about the strengths and weaknesses of GROOViST and CLIPScore and experimentally test their validity using the human grounding judgments."
        },
        {
            "heading": "5.1 Temporal misalignment",
            "text": "Effective metrics for measuring grounding in visual storytelling should account for possible temporal misalignments between the visual and textual modality. That is, they should account for the fact that entities that are grounded in an image could be mentioned earlier or later in the story\u2014 not necessarily in the corresponding sentence. We hypothesize that GROOViST\u2014since it takes into account the entire story holistically\u2014correlates better with human judgments than CLIPScore on samples with high temporal misalignment. To test this hypothesis, we define temporal misalignment t of a sentencei in a sequence as the number of its NPs matching with visual entities in images (imgj \u0338=i) at other positions of the sequence, normalized by the total number of its NPs. The overall temporal misalignment T of a story is then the average of its sentence-level t values:\nt(sentencei) = #(NPs matching imgj \u0338=i)\n#(NPs in sentencei) (3a)\nT (story) = \u2211n\ni=1 t(sentencei) / n (3b)\nwhere n is the number of sentences in a story. We consider a story to have high temporal misalignment if T \u2265 1.0, i.e., at least as many as the average number of NPs per sentence are misaligned. In the annotated data, T \u2208 [0.16, 1.53] and 18% of the stories exhibit high temporal misalignment, indicating the prevalence of the phenomenon.\nAs can be seen in Figure 3, our hypothesis is confirmed: GROOViST exhibits a higher correlation with human ratings than CLIPScore on samples with a high T , i.e., its scores are overall more aligned with human intuitions when in the presence of temporally misaligned entities. This confirms the ability of GROOViST to handle non-trivial grounding dynamics in a story, different from CLIPScore. At the same time, we notice that CLIPScore achieves a higher correlation than our metric in samples with low T , which confirms once again that the former is an effective tool for capturing grounding in well-aligned multimodal data.\n8Appendix B provides further details."
        },
        {
            "heading": "5.2 Proportion of noun phrases",
            "text": "GROOViST builds on noun phrases. As explained above, this has some obvious advantages, e.g., it allows to measure the individual contribution of each NP toward the final score (see Figure 1), but also some possible limitations. For example, we hypothesize that GROOViST scores may be dependent on the number of NPs; for stories where grounding hinges mostly on NPs, we expect GROOViST to be well aligned with human intuitions; less so when it hinges on verbs, for example, in which case CLIPScore may be better. To test this hypothesis, we define proportion-of-NPs (P ) of a story as the fraction of NPs to all the words in the story:\nP (story) = #(NPs in story)\n#(all words in story) (4)\nWe focus on the subset of <image sequence, story> pairs with high human ratings,9 to ensure our analysis genuinely explores the role of NPs in well-grounded stories without being influenced by other factors. We then compute P values for these sequences and bin them into two sets\u2014low P and high P\u2014using the distribution\u2019s mode (0.2325).10 The high P bin comprises 32.7% of the total number of subset samples.\nIn Figure 3, we see that our hypothesis is confirmed. GROOViST scores turn out to be very well aligned with human intuitions\u2014and indeed significantly more correlated than CLIPScore\u2014in the high P bin. In contrast, our metric lags behind CLIPScore in the low P bin, though the distance between the metrics is rather small, and the two metrics generally achieve very low correlations. Although the dependency of GROOViST on the proportion of NPs in a story might be seen as a limitation of the metric, we argue that nouns and accompanying phrases tend to offer the most visual information (Wang et al., 2022). As for RoViSTVG, it achieves a very low correlation with human ratings in both analyses, which confirms its flaws."
        },
        {
            "heading": "6 Conclusion",
            "text": "We proposed GROOViST, a novel reference-free metric for evaluating grounding in visual storytelling, an aspect that surprisingly is often overlooked in this task. We showed that existing metrics have serious shortcomings, and analyzed the strengths and limitations of our proposed metric.\n9Human rating \u2265 3 on a scale of 1 to 4. 10The same results also hold when using mean and median.\nGROOViST is modular, highly interpretable, and aligned with human intuitions on visual grounding. Preliminary results indicate that GROOViST is a suitable tool for evaluating automatically generated stories. We plan to test this aspect extensively in future work.\nLimitations\nIn this section, we discuss the limitations specific to our metric and to the general reference-free evaluation paradigm. As discussed in Section 5.2, GROOViST is heavily dependent on noun phrases making it oblivious to other visually informative words, such as verbs. For identifying poorly grounded NPs, GROOViST relies on a threshold value, which is determined based on the dataset of interest. This makes GROOViST vulnerable to the skew of the dataset. Despite our preliminary analysis, GROOViST\u2019s evaluation of model-generated stories is yet to be fully tested. Also, in general, reference-free metrics rely on an underlying pre-trained model, which often is stagnant in learning and might require regular fine-tuning for prolonged future relevance. We would also like to underline that throughout this work, we only used and evaluated models trained in the English language text. However, given the modularity of GROOViST, it is possible to switch to models such as multilingual-CLIP (Carlsson et al., 2022).\nEthics Statement\nFor collecting human judgments, we recruited participants on a voluntary basis among colleagues of our institution. All data collected for this work is de-identified to ensure the privacy and security of everyone involved. The authors of the VIST dataset (Huang et al., 2016) mention that all images used are CC-licensed."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the participants of the human evaluation study and the members of the Dialogue Modelling Group for their vital feedback on the design and experiments. Furthermore, we are grateful to the anonymous EMNLP reviewers for their helpful suggestions and for engaging in fruitful discussions. AKS is supported by the TIMELY project under EU H2020 grant 101017424. RF is supported by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreement No. 819455)."
        },
        {
            "heading": "A Text concreteness example",
            "text": "Figure 4 shows that through idf weighting RoViSTVG penalizes the alignment score of the relatively frequent noun \u2018church\u2019 (0.266). Inversely, idf weighting increases the low alignment score of the abstract and relatively less-frequent noun \u2018pic\u2019 (0.232). This could have unintended effects on the overall score of the metric, resulting in several discrepancies discussed in Sections 2 and 5."
        },
        {
            "heading": "B Human evaluation",
            "text": "We recruited participants on a voluntary basis among colleagues of our institution. We asked the participants for their consent through an informed consent form (see Figure 5). Participants who expressed their consent, were provided access to a scoring web interface with instructions. Each of the participants provided ratings for 100 <image sequence, story> pairs of the VIST data test set on a 4-point Likert-like scale."
        },
        {
            "heading": "C RoViST-VG",
            "text": "Model The RoViST-VG model comprises an image-encoder and a text-encoder. The imageencoder encompasses a pre-trained ViT model, an additional linear layer (Wi), and a tanh activation function for obtaining the image representations Ie. The text-encoder has a linear layer (Wt) and a tanh activation function for encoding GLoVe vectors into text embeddings Te.\nPre-training The procedure used for pre-training the RoViST-VG model is provided in Algorithm 1.\nAlgorithm 1 RoViST-VG pre-training Input: 1)A mini-batch of image regions In with shape (m \u00d7 3 \u00d7 224 \u00d7 224) where m is the batch size. 2)A mini-batch of matching noun pairs Tn with shape (m, 300) where 300 represents the dimensions of the GLoVe vectors. Output: Symmetric loss for the mini-batch. Initialization: Pretrained ViT model with linear head for the image encoder, and a single linear layer for the text encoder.\n1: hn = VisionTransformer(In) 2: Ie = tanh(Wihn + bi) \u25b7 image embeddings;\nshape=[m,1024] 3: Te = tanh(WtTn + bt) \u25b7 text embeddings;\nshape=[m,1024] 4: logits = Te \u00d7 ITe \u25b7 shape=[m, m] 5: Isim = Ie \u00d7 ITe \u25b7 shape=[m, m] 6: Tsim = Te \u00d7 T Te \u25b7 shape=[m, m] 7: labels = (Isim + Tsim)/2 \u25b7 shape=[m, m] 8: Limage = cross_entropy_loss(labelsT , logitsT ) 9: Ltext = cross_entropy_loss(labels, logits)\n10: Lsymmetric = (Limage + Ltext)/2"
        },
        {
            "heading": "D GROOViST pseudocode",
            "text": "For a given <image sequence, story> pair, the pseudocode in Algorithm 2 outlines the steps involved in computing the GROOViST score.\nAlgorithm 2 GROOViST Input: Image sequence bounding boxes (Vi), corresponding story NPs (Ti), concreteness weights (Wi), pre-trained CLIP model, score threshold (\u03b8) Output: unbounded GROOViST score G Steps:\n1: NPpos, NPneg \u2190 { }, { } 2: for k \u2190 1 to #(Ti) do \u25b7 # = number of NPs 3: npe, w\u2190 CLIP(Ti[k]), Wi[k] 4: nps\u2190 0.0 \u25b7 score of noun phrase np 5: for each v \u2208 Vi do 6: ve\u2190 CLIP(v) 7: nps\u2190 max(nps, cos(npe, ve)) 8: end for 9: if nps \u2265 \u03b8 then\n10: NPpos \u2190 nps \u00d7 w 11: else 12: NPneg \u2190 -(\u03b8 - nps) \u00d7w 13: end if 14: end for 15: G = ( \u2211 NPpos + \u2211 NPneg) / #(Ti)"
        },
        {
            "heading": "E Ablation and replacement results",
            "text": ""
        }
    ],
    "title": "GROOViST: A Metric for Grounding Objects in Visual Storytelling",
    "year": 2023
}