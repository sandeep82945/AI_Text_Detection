{
    "abstractText": "ASTE (Aspect Sentiment Triplet Extraction) has gained increasing attention. Recent advancements in the ASTE task have been primarily driven by Natural Language Generationbased (NLG) approaches. However, most NLG methods overlook the supervision of the encoder-decoder hidden representations and fail to fully utilize the semantic information provided by the labels to enhance supervision. These limitations can hinder the extraction of implicit aspects and opinions. To address these challenges, we propose a tagging-assisted generation model with encoder and decoder supervision (TAGS), which enhances the supervision of the encoder and decoder through multipleperspective tagging assistance and label semantic representations. Specifically, TAGS enhances the generation task by integrating an additional sequence tagging task, which improves the encoder\u2019s capability to distinguish the words of triplets. Moreover, it utilizes sequence tagging probabilities to guide the decoder, improving the generated content\u2019s quality. Furthermore, TAGS employs a selfdecoding process for labels to acquire the semantic representations of the labels and aligns the decoder\u2019s hidden states with these semantic representations, thereby achieving enhanced semantic supervision for the decoder\u2019s hidden states. Extensive experiments on various public benchmarks demonstrate that TAGS achieves state-of-the-art performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xianlong Luo"
        },
        {
            "affiliations": [],
            "name": "Meng Yang"
        },
        {
            "affiliations": [],
            "name": "Yihao Wang"
        }
    ],
    "id": "SP:39e701430b26068f87959de21955ea08b4ac7f9d",
    "references": [
        {
            "authors": [
                "Hongjie Cai",
                "Rui Xia",
                "Jianfei Yu."
            ],
            "title": "Aspectcategory-opinion-sentiment quadruple extraction with implicit aspects and opinions",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Hao Chen",
                "Zepeng Zhai",
                "Fangxiang Feng",
                "Ruifan Li",
                "Xiaojie Wang."
            ],
            "title": "Enhanced multi-channel graph convolutional network for aspect sentiment triplet extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Shaowei Chen",
                "Jie Liu",
                "Yu Wang",
                "Wenzheng Zhang",
                "Ziming Chi."
            ],
            "title": "Synchronous double-channel recurrent network for aspect-opinion pair extraction",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6515\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Shaowei Chen",
                "Yu Wang",
                "Jie Liu",
                "Yuelin Wang."
            ],
            "title": "Bidirectional machine reading comprehension for aspect sentiment triplet extraction",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Ap-",
            "year": 2021
        },
        {
            "authors": [
                "Yuqi Chen",
                "Keming Chen",
                "Xian Sun",
                "Zequn Zhang."
            ],
            "title": "A span-level bidirectional network for aspect sentiment triplet extraction",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi,",
            "year": 2022
        },
        {
            "authors": [
                "Zhuang Chen",
                "Tieyun Qian."
            ],
            "title": "Relation-aware collaborative learning for unified aspect-based sentiment analysis",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3685\u20133694, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Zhifang Fan",
                "Zhen Wu",
                "Xin-Yu Dai",
                "Shujian Huang",
                "Jiajun Chen."
            ],
            "title": "Target-oriented opinion words extraction with target-fused neural sequence labeling",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Hao Fei",
                "Fei Li",
                "Chenliang Li",
                "Shengqiong Wu",
                "Jingye Li",
                "Donghong Ji."
            ],
            "title": "Inheriting the wisdom of predecessors: A multiplex cascade framework for unified aspect-based sentiment analysis",
            "venue": "Proceedings of the Thirty-First International Joint Confer-",
            "year": 2022
        },
        {
            "authors": [
                "Tianhao Gao",
                "Jun Fang",
                "Hanyu Liu",
                "Zhiyuan Liu",
                "Chao Liu",
                "Pengzhang Liu",
                "Yongjun Bao",
                "Weipeng Yan."
            ],
            "title": "LEGO-ABSA: A prompt-based task assemblable unified generative framework for multi-task aspect-based sentiment analysis",
            "venue": "Proceedings of",
            "year": 2022
        },
        {
            "authors": [
                "Zhibin Gou",
                "Qingyan Guo",
                "Yujiu Yang."
            ],
            "title": "Mvp: Multi-view prompting improves aspect sentiment tuple prediction",
            "venue": "CoRR, abs/2305.12627.",
            "year": 2023
        },
        {
            "authors": [
                "Ruidan He",
                "Wee Sun Lee",
                "Hwee Tou Ng",
                "Daniel Dahlmeier."
            ],
            "title": "An interactive multi-task learning network for end-to-end aspect-based sentiment analysis",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2019
        },
        {
            "authors": [
                "Mengting Hu",
                "Yike Wu",
                "Hang Gao",
                "Yinhao Bai",
                "Shiwan Zhao."
            ],
            "title": "Improving aspect sentiment quad prediction via template-order data augmentation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP",
            "year": 2022
        },
        {
            "authors": [
                "Mengting Hu",
                "Yike Wu",
                "Hang Gao",
                "Yinhao Bai",
                "Shiwan Zhao."
            ],
            "title": "Improving aspect sentiment quad prediction via template-order data augmentation",
            "venue": "EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Minghao Hu",
                "Yuxing Peng",
                "Zhen Huang",
                "Dongsheng Li",
                "Yiwei Lv."
            ],
            "title": "Open-domain targeted sentiment analysis via span-based extraction and classification",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2019
        },
        {
            "authors": [
                "Samson Yu Bai Jian",
                "Tapas Nayak",
                "Navonil Majumder",
                "Soujanya Poria."
            ],
            "title": "Aspect sentiment triplet extraction using reinforcement learning",
            "venue": "CIKM \u201921: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event,",
            "year": 2021
        },
        {
            "authors": [
                "Xin Li",
                "Lidong Bing",
                "Piji Li",
                "Wai Lam."
            ],
            "title": "A unified model for opinion target extraction and target sentiment prediction",
            "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial In-",
            "year": 2019
        },
        {
            "authors": [
                "Xin Li",
                "Lidong Bing",
                "Wenxuan Zhang",
                "Wai Lam"
            ],
            "title": "Exploiting BERT for end-to-end aspect-based",
            "year": 2019
        },
        {
            "authors": [
                "Shuo Liang",
                "Wei Wei",
                "Xian-Ling Mao",
                "Yuanyuan Fu",
                "Rui Fang",
                "Dangyang Chen."
            ],
            "title": "STAGE: span tagging and greedy inference scheme for aspect sentiment triplet extraction",
            "venue": "CoRR, abs/2211.15003.",
            "year": 2022
        },
        {
            "authors": [
                "Shuo Liang",
                "Wei Wei",
                "Xian-Ling Mao",
                "Yuanyuan Fu",
                "Rui Fang",
                "Dangyang Chen."
            ],
            "title": "STAGE: span tagging and greedy inference scheme for aspect sentiment triplet extraction",
            "venue": "Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-",
            "year": 2023
        },
        {
            "authors": [
                "Jian Liu",
                "Zhiyang Teng",
                "Leyang Cui",
                "Hanmeng Liu",
                "Yue Zhang."
            ],
            "title": "Solving aspect category sentiment analysis as a text generation task",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4406\u20134416, Online",
            "year": 2021
        },
        {
            "authors": [
                "Yaojie Lu",
                "Qing Liu",
                "Dai Dai",
                "Xinyan Xiao",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun",
                "Hua Wu."
            ],
            "title": "Unified structure generation for universal information extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Haoran Lv",
                "Junyi Liu",
                "Henan Wang",
                "Yaoming Wang",
                "Jixiang Luo",
                "Yaxiao Liu."
            ],
            "title": "Efficient hybrid generation framework for aspect-based sentiment analysis",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computa-",
            "year": 2023
        },
        {
            "authors": [
                "Yue Mao",
                "Yi Shen",
                "Jingchao Yang",
                "Xiaoying Zhu",
                "Longjun Cai."
            ],
            "title": "Seq2path: Generating sentiment tuples as paths of a tree",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 2215\u20132225.",
            "year": 2022
        },
        {
            "authors": [
                "Yue Mao",
                "Yi Shen",
                "Chao Yu",
                "Longjun Cai."
            ],
            "title": "A joint training dual-mrc framework for aspect based sentiment analysis",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial",
            "year": 2021
        },
        {
            "authors": [
                "Rajdeep Mukherjee",
                "Tapas Nayak",
                "Yash Butala",
                "Sourangshu Bhattacharya",
                "Pawan Goyal"
            ],
            "title": "PASTE: A tagging-free decoding framework",
            "year": 2021
        },
        {
            "authors": [
                "Giovanni Paolini",
                "Ben Athiwaratkun",
                "Jason Krone",
                "Jie Ma",
                "Alessandro Achille",
                "Rishita Anubhai",
                "C\u00edcero Nogueira dos Santos",
                "Bing Xiang",
                "Stefano Soatto."
            ],
            "title": "Structured prediction as translation between augmented natural languages",
            "venue": "9th",
            "year": 2021
        },
        {
            "authors": [
                "Haiyun Peng",
                "Lu Xu",
                "Lidong Bing",
                "Fei Huang",
                "Wei Lu",
                "Luo Si."
            ],
            "title": "Knowing what, how and why: A near complete solution for aspect-based sentiment analysis",
            "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The",
            "year": 2020
        },
        {
            "authors": [
                "Joseph Peper",
                "Lu Wang."
            ],
            "title": "Generative aspectbased sentiment analysis with contrastive learning and expressive structure",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11,",
            "year": 2022
        },
        {
            "authors": [
                "talia Loukachevitch",
                "Evgeniy Kotelnikov",
                "Nuria Bel",
                "Salud Mar\u00eda"
            ],
            "title": "Jim\u00e9nez-Zafra, and G\u00fcl\u015fen Eryi\u011fit",
            "venue": "In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016),",
            "year": 2016
        },
        {
            "authors": [
                "Maria Pontiki",
                "Dimitris Galanis",
                "Haris Papageorgiou",
                "Suresh Manandhar",
                "Ion Androutsopoulos."
            ],
            "title": "SemEval-2015 task 12: Aspect based sentiment analysis",
            "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages",
            "year": 2015
        },
        {
            "authors": [
                "Maria Pontiki",
                "Dimitris Galanis",
                "John Pavlopoulos",
                "Harris Papageorgiou",
                "Ion Androutsopoulos",
                "Suresh Manandhar."
            ],
            "title": "SemEval-2014 task 4: Aspect based sentiment analysis",
            "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (Se-",
            "year": 2014
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Yixuan Su",
                "Tian Lan",
                "Yan Wang",
                "Dani Yogatama",
                "Lingpeng Kong",
                "Nigel Collier."
            ],
            "title": "A contrastive framework for neural text generation",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Yi Chern Tan",
                "L. Elisa Celis."
            ],
            "title": "Assessing social and intersectional biases in contextualized word representations",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "Zengzhi Wang",
                "Rui Xia",
                "Jianfei Yu."
            ],
            "title": "Unifiedabsa: A unified ABSA framework based on multitask instruction tuning",
            "venue": "CoRR, abs/2211.10986.",
            "year": 2022
        },
        {
            "authors": [
                "Zhen Wu",
                "Chengcan Ying",
                "Fei Zhao",
                "Zhifang Fan",
                "Xinyu Dai",
                "Rui Xia."
            ],
            "title": "Grid tagging scheme for aspect-oriented fine-grained opinion extraction",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2576\u20132585, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Lu Xu",
                "Yew Ken Chia",
                "Lidong Bing."
            ],
            "title": "Learning span-level interactions for aspect sentiment triplet extraction",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Lu Xu",
                "Hao Li",
                "Wei Lu",
                "Lidong Bing."
            ],
            "title": "Position-aware tagging for aspect sentiment triplet extraction",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Lu Xu",
                "Hao Li",
                "Wei Lu",
                "Lidong Bing."
            ],
            "title": "Position-aware tagging for aspect sentiment triplet extraction",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2339\u20132349, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Hang Yan",
                "Junqi Dai",
                "Tuo Ji",
                "Xipeng Qiu",
                "Zheng Zhang."
            ],
            "title": "A unified generative framework for aspect-based sentiment analysis",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Chengze Yu",
                "Taiqiang Wu",
                "Jiayi Li",
                "Xingyu Bai",
                "Yujiu Yang."
            ],
            "title": "Syngen: A syntactic plug-andplay module for generative aspect-based sentiment analysis",
            "venue": "CoRR, abs/2302.13032.",
            "year": 2023
        },
        {
            "authors": [
                "dong Bing",
                "Wai Lam"
            ],
            "title": "2021a. Aspect sentiment",
            "year": 2021
        },
        {
            "authors": [
                "Wai Lam"
            ],
            "title": "Towards generative aspect-based",
            "year": 2021
        },
        {
            "authors": [
                "Wai Lam"
            ],
            "title": "Towards generative aspect-based",
            "year": 2021
        },
        {
            "authors": [
                "Hui Xue"
            ],
            "title": "SpanMlt: A span-based multi-task",
            "year": 2020
        },
        {
            "authors": [
                "SpanMlt (Zhao"
            ],
            "title": "2020), a synchronous double channel extraction model SDRN (Chen et al., 2020), HAST+TOWE and JERE-MHS model compared in (Zhang et al., 2021b), GAS (Zhang et al., 2021b)",
            "venue": "LEGO(Gao et al.,",
            "year": 2022
        },
        {
            "authors": [
                "He"
            ],
            "title": "2019), a Relation-Aware Collaborative Learning (RACL) model RACL (Chen and Qian, 2020), a machine reading comprehension models Dual-MRC (Mao et al., 2021",
            "venue": "GAS (Zhang et al., 2021b) and EHG(Lv et al.,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Aspect Sentiment Triplet Extraction (ASTE) aims to extract sentiment triplets from a sentence, i.e., Aspect: the aspect term represents an explicit mention of a discussed target, Opinion: the mentioned comment terms/phrases, Sentiment: sentiment polarity of the aspect, holding significant potential in downstream research and applications. Unlike sentence sentiment classification, ASTE emphasizes\n\u2217Corresponding author.\nthe explanation for sentiments, explicitly highlighting the causes of sentiments and the entities to which they are attached. This task involves addressing challenges such as the diversity of emotional expressions and the complexity of linguistic contexts. For instance, in the sentence \"Food wise, it\u2019s ok but a bit pricey for what you get considering the restaurant isn\u2019t a fancy place,\" three sentiment triplets can be extracted: (food, ok, neutral), (food, pricey, neutral), and (restaurant, isn\u2019t a fancy place, neutral).\nExisting Methods The current mainstream approaches for ASTE can be classified into two categories: sequence tagging-based approaches and sequence generation-based approaches. ASTE employed a sequence tagging method initially introduced by Peng et al. (2020). However, the sequence tagging-based approaches in ASTE fail to capture the semantic information conveyed by the labels, which can result in semantic mismatches in the predicted results (Zhang et al., 2021b). By leveraging the rich label semantic information and mitigating the potential error propagation in pipeline methods (Paolini et al., 2021; Yu et al., 2023), generation methods achieve better performance in ASTE.\nGeneration-based approaches still face two significant challenges. Firstly, the supervision of hidden representations within encoder-decoder architectures has been overlooked, leading to potential issues such as the degeneration of neural language models and difficulty in identifying distinctive information (Su et al., 2022). In the context of the ASTE task, this oversight can fail to extract implicit aspects and opinions (Cai et al., 2021; Peper and Wang, 2022). Secondly, during training, the semantic information of the labels has yet to be fully utilized. Traditional supervision utilizes labels in the form of one-hot probability vectors without fully leveraging the semantic information of the labels at the hidden state level.\nTAGS To address the challenges mentioned\nabove, we propose a novel tagging-assisted generation model called TAGS, which enhances the supervision of both the encoder and decoder through multiple-perspective tagging assistance and label semantic representations. TAGS consists of two modules: \"Empowering Generation through Sequence Tagging\" (EGST) and \"Label-Driven Semantic Alignment\" (LDSA).\nIn EGST, we utilize a sequence tagging task to enhance the generation task through three aspects: Multitask Learning, Guided Generation, and Result Optimization. Multitask learning: we enhance the supervision in the encoder of the generation model by introducing a sequence tagging task. This additional task empowers the encoder to distinguish between triplet and irrelevant words effectively, thereby benefiting the generation task. Guided Generation: We incorporate the sequence tagging outputs into the decoder\u2019s attention mechanism. This encourages the model to focus more on the keywords identified by the sequence tagging task. Result Optimization: Finally, during inference, we utilize the sequence tagging results to optimize the generation results, thereby improving the quality of the results.\nIn LDSA, we further enhance the supervision for the decoder\u2019s hidden states in the generation model by utilizing the semantic information conveyed by labels. Firstly, we convert label triplets into a natural context, referred to as a label sentence, and input the label sentence into the TAGS model to obtain a more accurate hidden state, which also serves as a semantic label representation. Subsequently, we dynamically align the hidden states of the decoder to the label\u2019s semantic representation according to the comparison results between the tokens corresponding to the semantic representation and the ground truth tokens. By this alignment, the model can better capture the semantic information conveyed by the labels, making the generation more in line with the intended label semantics.\nExtensive experimental results validate the effectiveness of the TAGS model. In summary, our contributions to this work are threefold:\n1. We propose a novel ASTE generation model, which utilizes sequence tagging to assist the generation via enhancing the supervision of the encoder\u2019s hidden state and incorporating sequence tagging probabilities and results to improve the generation process.\n2. We obtain the semantic representation of la-\nbels at the decoder level and achieve semantic alignment of the decoder\u2019s hidden state to the labels in the generation model.\n3. The experimental results show that our proposed framework significantly outperforms recent SOTA methods."
        },
        {
            "heading": "2 Problem statement",
            "text": "The input of the ASTE task is a sentence X = {x1, x2, ..., xn}, where each xi represents a word and n is the maximum length of the sentence. The goal of the ASTE task is to generate a set of sentiment triplets T = {(a,o, s)k} |T | k=1, where |T | means the number of triplets in T . Each triplet consists of an aspect term (a), an opinion term (o), and the corresponding sentiment polarity (s) (s \u2208 {POS,NEU,NEG} ).\nOur proposed TAGS is an encoder-decoder model designed for the generation task, in which the input is a natural sentence and the generation target, i.e., the label sentence, is constructed by concatenating triplets from the set T as follows: Y = \"a1, o1, s1; a2, o2, s2; . . . ; ak, ok, sk\" , where ai, oi, and si correspond to the i-th triplet (a, o, s)i."
        },
        {
            "heading": "3 Methodology",
            "text": "Fig. 1 shows our proposed TAGS method. TAGS comprises two modules, an Empowering Generation through Sequential Tagging module (EGST) and a Label-Driven Semantic Alignment (LDSA) module. EGST leverages sequence tagging task to enhance the generation model in three aspects: Multitask Learning, Guided Generation, and Result Optimization. LDSA utilizes a label self-decoding process to obtain the semantic representation of labels and aligns the decoder\u2019s hidden states to the semantic representation during training, thereby achieving enhanced semantic supervision for the decoder\u2019s hidden states."
        },
        {
            "heading": "3.1 Empowering Generation through Sequence Tagging",
            "text": "TAGS leverages sequence tagging to enhance the generation task from multiple perspectives, shown in the right part of Fig. 1. Firstly, TAGS employs a sequence tagging task as an additional task to enhance the supervision of the encoder, thereby improving its ability to differentiate between triplet and irrelevant words. By sharing parameters between the sequence tagging model and the generation model, the enhanced discriminative power\nobtained from the sequence tagging task can also benefit the triplets extraction process in the generation model. Next, the sequence tagging task probabilities are integrated into the generation model, compelling it to prioritize the words identified as crucial by the sequence tagging results. This integration ensures that the generation model produces content closely aligned with those words. Lastly, TAGS utilizes the sequence tagging task results during inference to optimize the generated results. By considering the results from both methods, TAGS achieves a more comprehensive information fusion, enhancing overall model performance."
        },
        {
            "heading": "3.1.1 Sequence Tagging Task",
            "text": "We perform multitask learning by simultaneously training a generation task and a sequence tagging task.\nTagging Scheme In our designed sequence tagging scheme, each word will be classified into one of 7 categories. The \"N\" category represents nonkeywords, while the remaining 6 categories represent aspects and opinions, each combined with three sentiment types (positive, negative, and neutral). Read Appendix A.1 for detailed descriptions.\nTagging Task A tagging sample is de-\nnoted as (X,Z), where Z is the tagging label {z1, z2, z3, . . . , zn}. The encoder encodes X to obtain hidden states HXEn:\nHXEn = En([x1, x2, . . . , xn]) = [h X 1 , h X 2 , . . . , h X n ] (1)\nwhere En is Encoder,HXEn \u2208 Rn\u00d7d, d denotes the hidden dimension. HXEn is also the encoder hidden state for the generation task. Pass HXEn through a fully connected layer to obtain the tag probabilities pi:\npi = softmax(W1hXi + b1) (2)\nwhere W1 \u2208 R7\u00d7d, b1 \u2208 R7, and pi \u2208 R7 represents the probability distribution of the i-th word across 7 tags. We calculate the sequence tagging loss using cross-entropy loss:\nLtagging = n\u2211\ni=1\nCE(pi, zi) (3)"
        },
        {
            "heading": "3.1.2 Generation guided by sequence tagging",
            "text": "Tag Attention To leverage the guidance information the sequence tagging task provides, we compute the probability p\u0303i of the i-th word being a keyword. pi[0] denotes the probability of the i-th\nword belonging to the non-keyword category (\"N\"). Consequently, p\u0303i = 1 \u2212 pi[0] indicates the probability of the i-th word belonging to the keyword category. We incorporate p\u0303i into the cross-attention mechanism of the decoder in the generation model as follows:\na\u0303ti = exp((1 + p\u0303i) \u00b7 ati)\u2211n j=1 exp((1 + p\u0303j) \u00b7 atj)\n(4)\nwhere ati represents the attention score at the tth row and i-th column in the cross-attention score matrix before applying softmax. a\u0303ti denotes the final adjusted attention score after applying softmax. (1 + p\u0303i) ensures a balanced contribution from both the sequence tagging task and inherent generation task to the attention distribution. Compared to the formulation without adding 1 to p\u0303i (Appendix A.3), this formulation effectively enhances the generation process while mitigating the potential impact of tagging errors on overall generation quality (Appendix B.2).\nGeneration Task A sample is denoted as (X,Y), where Y represents the label sentence {y1, y2, y3, . . . , ym} , with m being the maximum length of Y. The loss function for the generation task with model parameters \u03b8 is defined as follows:\nLgeneration = \u2212 m\u2211 t=1 log p\u03b8(yt|X, y<t) (5)"
        },
        {
            "heading": "3.1.3 Inference",
            "text": "During inference, we leverage the sequence tagging results to optimize the generated outputs. The main operation involves comparing the generated triplets from the generation task with the triplets from the sequence tagging task. If the generated aspect is a subset of the sequence tagging aspect set, or vice versa, and the generated opinion is a subset of the sequence tagging opinion set, or vice versa, the triplet is retained. Otherwise, the triplet is discarded. Read Appendix A.2 for details."
        },
        {
            "heading": "3.2 Label-Driven Semantic Alignment",
            "text": "The form of supervision, similar to Equation 5, lacks fine-grained supervision at the hidden state level and fails to fully utilize the semantic information embedded in the labels. In Label-Driven Semantic Alignment (shown in the left part of Fig. 1), we employ a label self-decoding process to obtain a more accurate decoder hidden state, which serves as a semantic representation of the label. During\ntraining, We align the decoder\u2019s hidden state to the semantic representation, thereby enhancing the supervision of the decoder\u2019s hidden state. This alignment ensures that the generated output closely matches the semantic content of the label.\nLabel Semantic Representation During training, we input the label sentence Y into the model to obtain the decoder\u2019s hidden state:\nHYDe = En-De([y1, y2, . . . , ym]) = [h\u0302 Y 1 , h\u0302 Y 2 , . . . , h\u0302 Y m] (6)\nwhere En-De means encoder-decoder architecture. Since the label sentence contains only the words of the correct triplets, the model can effortlessly extract the correct triplets from it. In this case, the model\u2019s input and output are both the label sentence, essentially forming a self-decoding process. Furthermore, due to the absence of irrelevant words in the input, HYDe is more accurate compared to HXDe, where H X De = De(H X En) = [h\u0302X1 , h\u0302 X 2 , . . . , h\u0302 X m], as demonstrated in Experiment 4.3.3. Therefore, we regard HYDe as an accurate semantic representation of the label that can provide substantial supervision at the decoder stage.\nAlignment Labels The main objective of semantic alignment is to establish alignment between HXDe and HYDe. One significant challenge arises from the fact that even though HYDe represents a more accurate hidden state, its corresponding output tokens Y \u2032, as shown in Equation 7, may not always match the ground truth token sequence Y during the early stages of training. Therefore, we compare y\u2032i with yi, and only when y\u2032i is equal to yi, it indicates that h\u0302iY is correct. We then allow h\u0302Xi to be close to h\u0302 Y i . Otherwise, we move h\u0302Xi away from h\u0302 Y i . Use Li to represent the comparison result between y\u2032i and yi:\nY \u2032 = (Lm_head (HYDe)).argmax() (7) Li = Equal(y\u2032i, yi) (8)\nwhere Lm_head represents a linear layer that takes the decoder\u2019s hidden states as input and outputs a probability distribution over the vocabulary. The predicted tokens Y \u2032 are obtained by selecting the words with the highest probability using the argmax operation. The function \"Equal\" outputs 1 when the inputs are equal and 0 otherwise.\nAlignment Task Alignment is achieved by adjusting the distance between h\u0302Xi and h\u0302 Y i accoding to Li. Employ cosine similarity to quantify the distance:\nsi = cos(h\u0302Xi , h\u0302 Y i ) (9) s\u2032i = ReLu(si) (10)\nwhere cos is the cosine similarity, and ReLu is used to limit the similarity values between 0 and 1 (Appendix B.3). We compute the alignment loss using binary cross-entropy to enforce the cosine similarity scores to align with the labels L:\nLalignment = m\u2211 i=1 BCEloss(s\u2032i, Li) (11)\nFinal Loss. Therefore, the final loss is defined as follows:\nL = \u03b11Lgeneration + \u03b12Ltagging + \u03b13Lalignment (12)\nwhere \u03b11, \u03b12 and \u03b13 are hyperparameters."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experiment Setup",
            "text": "ASTE Dataset We evaluate our TAGS on four popular ASTE datasets shown in Table 1: 14Res, 14Lap, 15Res, 16Res (Pontiki et al., 2014, 2015, 2016), which are modified for ASTE task by Fan et al. (2019); Peng et al. (2020); Xu et al. (2020a); Wu et al. (2020).\nBaseline Models We categorize the comparison models into the following three types:\n1.Sequence tagging-based models, such as OTEMTL (Zhang et al., 2020), GTS (Wu et al., 2020), JET (Xu et al., 2020b), EMC-GCN (Chen et al., 2022a), SyMux (Fei et al., 2022), SCEDD (Zhang et al., 2022b), BDTF (Zhang et al., 2022a), SATransformer (Yuan et al., 2023), STAGE (Liang et al., 2023).\n2.Generation-based models, such as GAS(Zhang et al., 2020), Paraphrase (Wu et al., 2020), BARTABSA (Yan et al., 2021), PASTE (Mukherjee et al., 2021), Seq2Path (Mao et al., 2022), DLO (Hu et al., 2022a), LEGO-ABSA (Gao et al., 2022), EHG (Lv et al., 2023) and Mvp (Gou et al., 2023).\n3.Models based on other methods: reinforcement learning based model ASTE-RL (Jian et al., 2021), reading comprehension based model BMRC (Chen et al., 2021), and span-level models Span-ASTE (Xu et al., 2021) and SBN (Chen et al., 2022b).\nExperiment Details We employ the T5-base model (Raffel et al., 2020) from the huggingface Transformer library as our pre-trained generative encoder-decoder model. During training, we set the learning rate to 3e-4 for T5 and 5e-3 for all the\nlinear layers. The model is trained for 40 epochs on Nvidia 3090 GPUs, and the hyperparameters of Equation 12 are set as follows: \u03b11 = 10, \u03b12 = 1, and \u03b13 = 1. The probability threshold in the inference stage is 0.999. All the reported results are the average of five runs with different random seeds.\nEvaluation Metrics Following previous works (Peng et al., 2020), we employ widely used evaluation metrics, namely F1 scores (F1), recall (R), and precision (P )."
        },
        {
            "heading": "4.2 Main Results",
            "text": "The main results are reported in Table 2. In this task, F1 is the most important metric (Peng et al., 2020; Chen et al., 2022b; Gao et al., 2022; Gou et al., 2023). TAGS significantly outperforms the previous state-of-the-art method Mvp (Gou et al., 2023), specifically achieving a lead of up to 3.13% on the 16res dataset and 2.01% on the 15res dataset according to the F1 metric.\nBased on the principles of sequence taggingbased methods, these approaches tend to be conservative, which means they only predict a triplet when they are highly confident. Consequently, the precision of these methods tends to be higher than the recall, as shown in both the OTE-MTL and JET methods in Table 2. In contrast, generation methods tend to over-predict the number of triplets due to their strong creativity. Consequently, the recall in the results of generation methods is generally higher than the precision.\nBy introducing a sequence tagging task, the\nTAGS method alleviates the excessive creativity of the generation model by directing its focus toward keywords. This not only enhances the quality of the generated output but also objectively limits the number of excessively generated triplets. Leveraging the semantic alignment with labels, TAGS further enhances the quality of the generated triplets. Consequently, compared to conventional generation methods, our method can extract more correct triplets with fewer predicted triplets. This leads to higher precision, recall, and consequently, a higher F1 score. Furthermore, when compared to conventional sequence tagging methods, TAGS surpasses them due to the generation model\u2019s ability to utilize semantic information from the labels and its inherent creativity. Thus, TAGS outperforms most previous methods in terms of F1 score, precision, and recall."
        },
        {
            "heading": "4.3 Ablation",
            "text": "The results of the ablation experiments are presented in Table 3.\nEffectiveness of the Sequence Tagging Task: The \"w/o Tagging training\" condition denotes the removal of the sequence tagging task, including multitask training, tag attention, and the specialized inference stage. It means that the model only relies on the Semantic Alignment component. Compared to the \"Full Model\", the performance under this condition decreased in all datasets: 16res\n(-3.78%), 15res (-3.56%), 14Lap (-2.38%), and 14Res (-2.09%), providing evidence for the effectiveness of the sequence tagging task. To further investigate the role of the sequence tagging task, we conducted Experiments 4.3.1.\nEffectiveness of Tag Attention: The \"w/o Tag attention\" condition refers to the absence of tag attention while still retaining the training of the sequence tagging task, special inference stage, and the Semantic Alignment component. When compared to the \"full model,\" there was an average performance decrease of 1.11% across all datasets, providing evidence for the effectiveness of Tag Attention. In Appendix B.2, we further analyze the impact of different utilization methods for sequence tagging probabilities on Tag Attention. This analysis enables us to gain a deeper understanding of how the utilization of sequence tagging probabilities influences the performance of Tag Attention.\nEffectiveness of Inference: The \"w/o Inference\" condition refers to the absence of a special inference stage. In comparison to the \"Full model,\" there was an average performance decrease of 0.85% across all datasets. This provides evidence for the effectiveness of the Inference stage. In Experiment 4.3.2, we further investigate the experimental results related to the threshold hyperparameter in the inference stage.\nEffectiveness of the Semantic Alignment: The \"w/o Alignment\" condition refers to the removal of the Semantic Alignment component. Compared to the \"Full Model\", the performance under this condition decreased in all datasets: 16res (-1.24%), 15res (-2.08%), 14Lap (-1.33%), and 14Res (-1.61%). This demonstrates the effectiveness of the Semantic Alignment component in improving overall performance. To further investigate the impact of the loss function on the Semantic Alignment component, we conducted Experiment B.3."
        },
        {
            "heading": "4.3.1 Loss Hyperparameters",
            "text": "In this section, we investigate the impact of loss hyperparameters. First, we fix \u03b12 and vary \u03b11 and \u03b13, as shown in Fig. 2(a). As \u03b11 gradually increases, the performance initially improves and then decreases, achieving the best result at 10. Comparing the three curves in the graph, the curve corresponding to \u03b13 = 1 achieves the best result. Next, we fix \u03b11 = 10 and vary \u03b12 and \u03b13 as shown in Fig. 2(b). As the \u03b12 increases, the performance initially improves and then decreases, achieving the best result at 1. Furthermore, the curve corresponding to\n\u03b13 = 1 achieves the best result. We select the loss ratios corresponding to the optimal performance as our final hyperparameter settings: \u03b11 = 10, \u03b12 = 1, \u03b13 = 1. This suggests that our method primarily focuses on the generative task, with the other two components serving as auxiliary factors."
        },
        {
            "heading": "4.3.2 Threshold Hyperparameter in Inference",
            "text": "We conducted experiments on the development set to determine the most suitable probability threshold hyperparameter. We experimented with four different values for the threshold hyperparameter. The results are shown in Table 4. As the threshold increases, the performance initially improves and then decreases, achieving the best result at 0.999. This threshold value is very close to 1. In the generated results, the probability of each word is also very close to 1, even for some incorrect words. Therefore, when we require a threshold to filter out potentially erroneous triplets, this threshold should also be very close to 1. Hence, 0.999 is a reasonable choice."
        },
        {
            "heading": "4.3.3 Correctness of Semantic Representation",
            "text": "To demonstrate that HYDe is more accurate, during self-decoding, we replace each label sentence with the original input sentence with a probability of r. This increases the influence of irrelevant words on semantic representation. We then train the TAGS model using the semantic representation obtained from this self-decoding process and the correspond-\ning performance reflects the correctness of the semantic representation. We conducted experiments on the 16res dataset and the results are presented in Fig. 3. The results indicate that as r increases, performance decreases. This demonstrates that an increasing number of irrelevant words in the input lead to a decrease in the correctness of the semantic representation, resulting in a gradual decline in performance."
        },
        {
            "heading": "4.4 Results on Other ABSA Tasks",
            "text": "The proposed model provides a unified framework to effectively address the Aspect-Based Sentiment Analysis (ABSA) problem. To demonstrate the effectiveness of TAGS and its generalizability across different tasks, we conducted experiments on two ABSA tasks: AOPE and UABSA. We compared TAGS with the models in Appendix B.4.\nAOPE focuses on extracting (aspect, opinion) pairs, similar to ASTE, but without sentiment analysis. This task requires accurate identification of keywords in the sequence tagging task, as well as the assistance of Tag Attention and Semantic Alignment components. The F1 results for the AOPE task are presented in Table 5. TAGS outperforms the previous model on all four datasets: 2.28% for 16Res, 1.50% for 14Lap, 0.83% for 15Res, and 0.51% for 14Res. The improvement in results demonstrates the effectiveness of the aforementioned components.\nUABSA focuses on extracting (aspect, sentiment) pairs, similar to ASTE, but without extracting opinions. This task presents challenges in accurately classifying sentiments in sequence tagging and aligning sentiments in Semantic Alignment. The F1 results for the UABSA tasks are presented in Table 6. TAGS has achieved an average improvement of 1.27% compared to the previous model. This improvement demonstrates the effectiveness of the aforementioned components in enhancing\nthe accuracy of sentiment analysis. These results demonstrate the effectiveness and generalization of TAGS across different tasks."
        },
        {
            "heading": "5 Related Work",
            "text": "ASTE employed sequence tagging methods, when it was first introduced by (Peng et al., 2020). Subsequent research efforts (Xu et al., 2020b; Wu et al., 2020; Chen et al., 2022a; Liang et al., 2022; Gou et al., 2023) have been focused on enhancing the sequence tagging schemes and model components to facilitate the integration and mutual interpretation of the triple elements. However, the sequence tagging technique in ASTE fails to capture the semantic information conveyed by the labels, which can lead to semantic mismatches in the predicted results(Zhang et al., 2021b). Generation methods were initially proposed by Zhang et al. (2021c). The generation-based approach in ASTE has achieved good performance by reducing potential error propagation present in pipeline methods and effectively utilizing the rich semantic information provided by labels(Paolini et al., 2021; Yu et al., 2023). They employed various targets for generation, such as sentiment element sequences (Zhang et al., 2021c,c; Hu et al., 2022b), natural language (Liu et al., 2021; Zhang et al., 2021a), and structured extraction patterns (Lu et al., 2022). Recently proposed models, LEGO-ABSA (Gao et al., 2022), UnifiedABSA (Wang et al., 2022) and Mvp (Gou et al., 2023), have focused on leveraging task prompts or guided design for multi-task processing."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we introduce a generation model called TAGS, which enhances the supervision of both the encoder and decoder through multipleperspective tagging assistance and label semantic representations. Specifically, TAGS utilizes sequence tagging to enhance the generation model in multiple aspects: Multitask Learning, Guided Generation, and Result Optimization. Additionally, TAGS employs a label self-decoding process to obtain semantic representations of labels and aligns the decoder\u2019s hidden states with these representations, thereby providing enhanced semantic supervision for the decoder\u2019s hidden states. These two components enhance the supervision of the encoder and decoder\u2019s hidden states, resulting in improved generation quality. Extensive experiments demonstrate that our method significantly advances the state-of-the-art on benchmark datasets."
        },
        {
            "heading": "7 Limitations",
            "text": "Despite achieving state-of-the-art performance, our proposed methods still have some limitations that point to potential future directions.\n1. Compared to conventional generation methods, our approach requires an additional generation step to obtain more accurate hidden states, namely semantic labels. As a result, there is an increase in training overhead.\n2. Although we apply a simple yet effective aggregation strategy to combine the results of the sequence tagging task and generation task, more advanced strategies can be explored to further enhance performance.\n3. We have indeed observed that the improvement of our model varies on different datasets, which may be due to the differences in the characteristics of these datasets.\n4. Our work utilizes a relatively simple sequence tagging approach, specifically characterized by the absence of explicit pairing between extracted aspects and opinions. There is room for designing a more robust and sophisticated sequence tagging scheme that can also seamlessly integrate with generation models, thereby enhancing performance."
        },
        {
            "heading": "8 Ethics Statement",
            "text": "In all our experiments, we used existing datasets that have been widely used in previous scientific publications. When analyzing the experimental results, we strive to maintain fairness and honesty, ensuring that our work does not cause harm to anyone.\nAs for broader implications, this work may contribute to further research in sentiment analysis and the use of generation methods to simplify and automate the extraction of user opinions in realworld applications. However, it is important to note that this work involves fine-tuning large-scale pre-trained language models to generate sentiment triplets. Due to the nature of the Internet-based large-scale pre-training corpora, the predicted sentiment polarities may be influenced by unintended biases related to gender, race, and intersectional identities (Tan and Celis, 2019). LPMLs often inherit biases present in their training data, potentially leading to biased sentiment analysis results, particularly when assessing text from underrepresented or marginalized groups, thereby perpetuating and amplifying societal prejudices. Another limitation is the opacity of these models. Their complex architectures make it challenging to fully understand the reasoning behind their predictions, raising concerns about transparency and accountability. This lack of interpretability may hinder the identification and mitigation of harmful biases and ethical violations in sentiment analysis applications. It is crucial for the natural language processing community to consider these biases more extensively. Fortunately, these issues are actively being addressed within the research community, including efforts to standardize datasets and methodologies."
        },
        {
            "heading": "9 Acknowledgements",
            "text": "We would like to thank the anonymous reviewers for their insightful comments. This work is partially supported by National Natural Science Foundation of China (Grants No. 62176271), and Science and Technology Program of Guangzhou (Grant No. 202201011681)."
        },
        {
            "heading": "A Additional details about the methodology",
            "text": "A.1 Tagging Scheme\nSpecific details of the sequence tagging scheme and their explanations are presented in Table 7.\nA.2 Inference\nHere\u2019s an overview of the inference process:\n1. Obtain the probabilities of each word in the generated sentence.\n2. Conduct experiments on the development dataset to find a suitable probability threshold.\n3. For each triplet in the generated result, check if all the words in the triplet have probabilities greater than the threshold. If so, retain the triplet. Otherwise, proceed to the next step.\n4. Compare the generated triplet with the triplet identified by the sequence tagging task. If the generated aspect is a subset of the sequence tagging aspect set, or vice versa, and the generated opinion is a subset of the sequence tagging opinion set, or vice versa, retain the triplet. Otherwise, discard the triplet.\nThe specific algorithm pseudocode is presented in algorithm 1.\nA.3 Other Attention\na\u0303ti = exp(ati)\u2211n j=1 exp(atj)\n(13)\na\u0303ti = exp(p\u0303i \u00b7 ati)\u2211n j=1 exp(p\u0303j \u00b7 atj)\n(14)"
        },
        {
            "heading": "B Additional Experiment",
            "text": "B.1 case study In this case study section, we compare our model with the Paraphrase model (Zhang et al., 2021a) to illustrate how our two components benefit the results. For the first example, the Paraphrase model fails to extract the triplet (barmenu,Disappointingly,NEG) because it is a relatively hard and implicit triplet. Additionally, thanks to the semantic alignment training of the decoder hidden state, our model can generate higher-quality results. Therefore, our model can extract this triplet successfully. In the second example, the Paraphrase model incorrectly extracts the triplet (staff, supportive,NEG). However, during the inference stage, our model optimizes the generation results based on the sequence tagging output, resulting in the discarding of this incorrect triplet.\nB.2 Arithmetic Operations in Tag Attention In the context of Tag Attention, we have explored several approaches to incorporating tagging probability into the cross-attention mechanism:\n1. Multiplication before softmax: Multiply the attention scores by the probability weights and then apply softmax.\n2. Multiplication is performed before softmax, but without adding 1 to p\u0303i. The attention formula is given by Equation (14).\n3. Softmax after multiplication: Apply softmax to the attention scores and then multiply them by the probability weights.\n4. Addition: We directly add the probability information to the attention scores.\nThrough the evaluation of these various operations, our objective is to gain insights into their impact on the Tag Attention mechanism and their effectiveness in incorporating probability information. We conducted this experiment on a model without the \"Inference\" process because including\nthe \"Inference\" process could potentially narrow down the performance gaps observed in these results. The results in Table 9 demonstrate that the first approach performs better. This is because it provides valuable information to the generation model while minimizing any disruptive effects on the original generation process. It can be regarded as a gentle process. The results of the second approach are worse compared to the first approach. One possible reason for this is that in the first approach, by adding 1 to the p\u0303, the attention is not solely determined by the sequence tagging results. This helps mitigate the potential impact of tagging errors on the overall generation quality. Furthermore, we found that the performance of the last two arithmetic operations is worse.\nB.3 Loss Function for Semantic Alignment\nWe discuss loss function for semantic alignment in our approach. Specifically, we compare two different approaches:\n1. Confining the similarity scores to the range of 0 to 1 and utilizing the Binary Cross Entropy (BCE) loss function.\n2. Preserving the cosine similarity scores in the range of -1 to 1 and employing the margin ranking loss function to constrain the similarity.\nThe results in Table 10 indicate that in our method, the BCE loss function outperforms the margin rank loss function. From this, we can conclude that it is not necessary to push the similarity of incorrect hidden states to -1, i.e., there is\nno need to excessively move away from the negative hidden states associated with incorrect words. Since the hidden states are generated from the label sentences, even if some negative hidden states are incorrect, they remain relatively close to the correct hidden states. Moving too far away from negative hidden states may lead to an increase in the distance from the correct hidden state.\nB.4 ABSA subtask Detail The subtasks are described as follows:\n1. Aspect Opinion Pair Extraction (AOPE) aims to extract aspect terms and their corresponding opinion terms as pairs (Zhang and Qian, 2020; Chen et al., 2020).\n2. Unified ABSA (UABSA) is the task of extracting aspect terms and predicting their sentiment polarities at the same time (Li et al., 2019a; Chen and Qian, 2020). We also formulate it as an (aspect, sentiment polarity) pair extraction problem\nFor these tasks, we adopt the dataset used in (Zhang et al., 2021b).\nFor AOPE task, we compare our model with the following models: a multi-task learning model SpanMlt (Zhao et al., 2020), a synchronous double channel extraction model SDRN (Chen et al., 2020), HAST+TOWE and JERE-MHS model compared in (Zhang et al., 2021b), GAS (Zhang et al., 2021b), LEGO(Gao et al., 2022) and EHG(Lv et al., 2023).\nFor the UABSA task, we compare our model with the following models: a BERT base model BERT+GRU (Li et al., 2019b), a span-base extraction model SPAN-BERT (Hu et al., 2019), an interactive multi-task learning network LMN-BERT\n(He et al., 2019), a Relation-Aware Collaborative Learning (RACL) model RACL (Chen and Qian, 2020), a machine reading comprehension models Dual-MRC (Mao et al., 2021) , GAS (Zhang et al., 2021b) and EHG(Lv et al., 2023).\nB.5 Analysis on Potential Practical Applications\nTime Complexity: The time complexity of the TAGS model is quadratic relative to the input data. The primary source of complexity in this quadratic time complexity is the attention operations within the transformer. It\u2019s important to note that the additional modules introduced in our model, such as the sequence tagging classification layer and the label-driven semantic alignment module, have a linear time complexity relative to the input data. Consequently, the time complexity introduced by our additional modules remains exceedingly modest compared to that of the transformer. As such, the primary temporal overhead in our model stems from the transformer\u2019s attention operations. Consequently, the complexity of the TAGS model closely aligns with the time complexity of baseline models that rely on transformers. Moreover, existing lightweight and acceleration-oriented designs based on the transformer can be readily assimilated into our model. Hence, although our model does introduce some additional time overhead, it does not impose a significant obstacle to the training process.\nSpace Complexity: Apart from the core model architecture and input data, the additional space utilization of the TAGS model primarily consists of a linear layer for sequence tagging classification and the semantic representation of label sentences. The additional space occupation amounts to 5.2 M, which is notably minor when compared to the parameter size of the T5 model, standing at 222 M. Additionally, the tag attention module does not introduce any additional parameters.\nBased on the aforementioned explanation, it\u2019s evident that the TAGS model demonstrates commendable scalability. As dataset volumes increase, the incremental rise in both time and space overheads within our model remains consistent.\nInput: Generated triplets T \u20321 = {(ai, oi, si)k} |T \u20321| k=1, Word probabilities G={gk = (gk1, gk2, gk3)k} |T \u20321| k=1, Tagging Aspect Set Saspect = {ak} |Saspect| k=1 , Tagging Opinion Set Sopinion = {ak} |Sopinion| k=1 , Threshold threshold. Output: Result triplets\nFunction Verify(gen_element, sequenceTag): foreach tag_element in sequenceTag do\nif gen_element is a part of tag_element OR tag_element is a part of gen_element then return true\nend end return false\nbegin Result\u2190 \u2205 foreach (triplet,wordProb) in (T \u20321, G) do\nif (wordProb \u2265 threshold).all() then Result\u2190 Result \u222a {triplet} end else\naspect\u2190triplet.aspect Averified\u2190Verify(aspect,Saspect) opinion\u2190triplet.opinion Overified\u2190Verify(opinion,Sopinion) if Averified AND Overified then\nResult\u2190 Result \u222a {triplet} end\nend end\nend return Result\nAlgorithm 1: Inference"
        }
    ],
    "title": "Tagging-Assisted Generation Model with Encoder and Decoder Supervision for Aspect Sentiment Triplet Extraction",
    "year": 2023
}