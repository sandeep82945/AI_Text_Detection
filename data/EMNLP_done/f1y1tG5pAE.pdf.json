{
    "abstractText": "Despite the impressive performance achieved by pre-trained language-and-vision models in downstream tasks, it remains an open question whether this reflects a proper understanding of image-text interaction. In this work, we explore to what extent they handle basic linguistic constructions\u2014active-passive voice, coordination, and relative clauses\u2014that even preschool children can typically master. We present BLA, a novel, automatically constructed benchmark to evaluate multimodal models on these Basic Language Abilities. We show that different types of Transformer-based systems, such as CLIP, ViLBERT, and BLIP2, generally struggle with BLA in a zero-shot setting, in line with previous findings. Our experiments, in particular, show that most of the tested models only marginally benefit when fine-tuned or prompted with construction-specific samples. Yet, the generative BLIP2 shows promising trends, especially in an in-context learning setting. This opens the door to using BLA not only as an evaluation benchmark but also to improve models\u2019 basic language abilities.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xinyi Chen"
        },
        {
            "affiliations": [],
            "name": "Raquel Fern\u00e1ndez"
        },
        {
            "affiliations": [],
            "name": "Sandro Pezzelle"
        }
    ],
    "id": "SP:c593527efff744e257550d0b904f044e17a0f2cd",
    "references": [
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katherine Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning",
            "venue": "Advances in Neural",
            "year": 2022
        },
        {
            "authors": [
                "Anas Awadalla",
                "Irena Gao",
                "Josh Gardner",
                "Jack Hessel",
                "Yusuf Hanafy",
                "Wanrong Zhu",
                "Kalyani Marathe",
                "Yonatan Bitton",
                "Samir Gadre",
                "Shiori Sagawa"
            ],
            "title": "Openflamingo: An open-source framework for training large autoregressive vision-language models",
            "year": 2023
        },
        {
            "authors": [
                "Emanuele Bugliarello",
                "Ryan Cotterell",
                "Naoaki Okazaki",
                "Desmond Elliott"
            ],
            "title": "Multimodal pretraining unmasked: A meta-analysis and a unified framework",
            "year": 2021
        },
        {
            "authors": [
                "Emanuele Bugliarello",
                "Laurent Sartrain",
                "Aishwarya Agrawal",
                "Lisa Anne Hendricks",
                "Aida Nematzadeh."
            ],
            "title": "Measuring progress in fine-grained vision-and-language understanding",
            "venue": "Proceedings of the 61th Annual Meeting of the Association for",
            "year": 2023
        },
        {
            "authors": [
                "Yen-Chun Chen",
                "Linjie Li",
                "Licheng Yu",
                "Ahmed El Kholy",
                "Faisal Ahmed",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu."
            ],
            "title": "Uniter: Universal image-text representation learning",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, Au-",
            "year": 2020
        },
        {
            "authors": [
                "Holger Diessel",
                "Michael Tomasello"
            ],
            "title": "The development of relative clauses in spontaneous child",
            "year": 2001
        },
        {
            "authors": [
                "Anuj Diwan",
                "Layne Berry",
                "Eunsol Choi",
                "David Harwath",
                "Kyle Mahowald."
            ],
            "title": "Why is Winoground hard? Investigating failures in visuolinguistic compositionality",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Radina Dobreva",
                "Frank Keller."
            ],
            "title": "Investigating negation in pre-trained vision-and-language models",
            "venue": "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 350\u2013362, Punta Cana, Dominican Repub-",
            "year": 2021
        },
        {
            "authors": [
                "Constantin Eichenberg",
                "Sidney Black",
                "Samuel Weinbach",
                "Letitia Parcalabescu",
                "Anette Frank."
            ],
            "title": "Magma\u2013multimodal augmentation of generative models through adapter-based finetuning",
            "venue": "Findings of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Evelina Fedorenko",
                "Rosemary A. Varley."
            ],
            "title": "Language and thought are not the same thing: evidence from neuroimaging and neurological patients",
            "venue": "Annals of the New York Academy of Sciences, 1369.",
            "year": 2016
        },
        {
            "authors": [
                "Naama Friedmann",
                "Jo\u00e3o Costa."
            ],
            "title": "The child heard a coordinated sentence and wondered: On children\u2019s difficulty in understanding coordination and relative clauses with crossing dependencies",
            "venue": "Lingua, 120(6):1502\u20131515. Contrast as an information-",
            "year": 2010
        },
        {
            "authors": [
                "Pauline Frizelle",
                "Clodagh O\u2019Neill",
                "Dorothy VM Bishop"
            ],
            "title": "Assessing understanding of relative clauses: A comparison of multiple-choice comprehension versus sentence repetition",
            "venue": "Journal of Child Language,",
            "year": 2017
        },
        {
            "authors": [
                "LouAnn Gerken",
                "Michele E Shady"
            ],
            "title": "The picture selection",
            "year": 1996
        },
        {
            "authors": [
                "Lisa Anne Hendricks",
                "Aida Nematzadeh."
            ],
            "title": "Probing image-language transformers for verb understanding",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3635\u20133644.",
            "year": 2021
        },
        {
            "authors": [
                "Jack Hessel",
                "Ari Holtzman",
                "Maxwell Forbes",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "Clipscore: A reference-free evaluation metric for image captioning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Dianne Horgan."
            ],
            "title": "The development of the full passive",
            "venue": "Journal of Child Language, 5(1):65\u201380.",
            "year": 1978
        },
        {
            "authors": [
                "Jing Yu Koh",
                "Ruslan Salakhutdinov",
                "Daniel Fried."
            ],
            "title": "Grounding language models to images for multimodal inputs and outputs",
            "venue": "ICML.",
            "year": 2023
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Yuke Zhu",
                "Oliver Groth",
                "Justin Johnson",
                "Kenji Hata",
                "Joshua Kravitz",
                "Stephanie Chen",
                "Yannis Kalantidis",
                "Li-Jia Li",
                "David A Shamma"
            ],
            "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "year": 2017
        },
        {
            "authors": [
                "Gen Li",
                "Nan Duan",
                "Yuejian Fang",
                "Ming Gong",
                "Daxin Jiang"
            ],
            "title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597.",
            "year": 2023
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Mark Yatskar",
                "Da Yin",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang."
            ],
            "title": "Visualbert: A simple and performant baseline for vision and language",
            "venue": "arXiv preprint arXiv:1908.03557.",
            "year": 2019
        },
        {
            "authors": [
                "Jiasen Lu",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee."
            ],
            "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Cecile McKee",
                "Dana McDaniel",
                "Jesse Snedeker."
            ],
            "title": "Relatives children say",
            "venue": "Journal of Psycholinguistic research, 27(5).",
            "year": 1998
        },
        {
            "authors": [
                "George A. Miller."
            ],
            "title": "Wordnet: A lexical database for english",
            "venue": "Commun. ACM, 38(11):39\u201341.",
            "year": 1995
        },
        {
            "authors": [
                "Letitia Parcalabescu",
                "Michele Cafagna",
                "Lilitta Muradjan",
                "Anette Frank",
                "Iacer Calixto",
                "Albert Gatt."
            ],
            "title": "Valse: A task-independent benchmark for vision and language models centered on linguistic phenomena",
            "venue": "Proceedings of the 60th Annual Meet-",
            "year": 2022
        },
        {
            "authors": [
                "Sandro Pezzelle."
            ],
            "title": "Dealing with semantic underspecification in multimodal NLP",
            "venue": "To appear in the Proceedings of ACL 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Sandro Pezzelle",
                "Ece Takmaz",
                "Raquel Fern\u00e1ndez."
            ],
            "title": "Word representation learning in multimodal pre-trained transformers: An intrinsic evaluation",
            "venue": "Transactions of the Association for Computational Linguistics, 9:1563\u20131579.",
            "year": 2021
        },
        {
            "authors": [
                "Manuela Pinto",
                "Shalom Zuckerman."
            ],
            "title": "Coloring book: A new method for testing language comprehension",
            "venue": "Behavior research methods, 51(6):2609\u20132628.",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Cristina Schmitt",
                "Karen Miller."
            ],
            "title": "Using comprehension methods in language acquisition research",
            "venue": "Experimental methods in language acquisition research, pages 35\u201356.",
            "year": 2010
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Romain Beaumont",
                "Richard Vencu",
                "Cade Gordon",
                "Ross Wightman",
                "Mehdi Cherti",
                "Theo Coombes",
                "Aarush Katta",
                "Clayton Mullis",
                "Mitchell Wortsman"
            ],
            "title": "Laion-5b: An open large-scale dataset for training next generation",
            "year": 2022
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut."
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Ravi Shekhar",
                "Sandro Pezzelle",
                "Yauhen Klimovich",
                "Aurelie Herbelot",
                "Moin Nabi",
                "Enver Sangineto",
                "Raffaella Bernardi."
            ],
            "title": "Foil it! find one mismatch between image and language caption",
            "venue": "ACL, pages 255\u2013265. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Ravi Shekhar",
                "Sandro Pezzelle",
                "Yauhen Klimovich",
                "Aur\u00e9lie Herbelot",
                "Moin Nabi",
                "Enver Sangineto",
                "Raffaella Bernardi."
            ],
            "title": "FOIL it! find one mismatch between image and language caption",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for",
            "year": 2017
        },
        {
            "authors": [
                "Weijie Su",
                "Xizhou Zhu",
                "Yue Cao",
                "Bin Li",
                "Lewei Lu",
                "Furu Wei",
                "Jifeng Dai."
            ],
            "title": "Vl-bert: Pre-training of generic visual-linguistic representations",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Hao Tan",
                "Mohit Bansal."
            ],
            "title": "Lxmert: Learning cross-modality encoder representations from transformers",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Tristan Thrush",
                "Ryan Jiang",
                "Max Bartolo",
                "Amanpreet Singh",
                "Adina Williams",
                "Douwe Kiela",
                "Candace Ross."
            ],
            "title": "Winoground: Probing vision and language models for visio-linguistic compositionality",
            "venue": "Proceedings of the IEEE/CVF Conference on Com-",
            "year": 2022
        },
        {
            "authors": [
                "Wanrong Zhu",
                "Jack Hessel",
                "Anas Awadalla",
                "Samir Yitzhak Gadre",
                "Jesse Dodge",
                "Alex Fang",
                "Youngjae Yu",
                "Ludwig Schmidt",
                "William Yang Wang",
                "Yejin Choi"
            ],
            "title": "Multimodal c4: An open, billion-scale corpus of images interleaved with text",
            "year": 2023
        },
        {
            "authors": [
                "Wanzheng Zhu",
                "Suma Bhat."
            ],
            "title": "Gruen for evaluating linguistic quality of generated text",
            "venue": "Findings of the Association for Computational Linguistics, ACL 2020: EMNLP 2020, pages 94\u2013108. Association for Computational Linguistics (ACL).",
            "year": 2020
        },
        {
            "authors": [
                "Awadalla"
            ],
            "title": "2023), which requires the token <image> for image input before each question, as follows: [<image>Question: Is",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Powered by the Transformer architecture, extensive pre-training, and task-specific fine-tuning, recent language and vision models (Lu et al., 2019; Tan and Bansal, 2019; Li et al., 2019; Chen et al., 2020; Li et al., 2020; Su et al., 2019; Radford et al., 2021) have achieved unprecedented performance in many downstream multimodal tasks. Despite the impressive results, it remains an open question whether, and to what extent, this improvement goes hand in hand with a genuine understanding of image, text, and their interaction. In particular, the ability of models to handle linguistic skills that are essential to understanding an event or situation has recently been questioned. Hendricks and Nematzadeh (2021), for example, showed that these models fail in scenarios that require understanding verbs and verb arguments; Parcalabescu et al.\n(2022) revealed a more generalized struggle of these models with phenomena that require grounding relations between objects; Pezzelle (2023) reported that their ability to ground language into vision is affected by the presence of semantically underspecified language, e.g., pronouns or locatives; Thrush et al. (2022) showed that no models appreciate the (substantial) difference between, e.g., a lightbulb surrounding some plants and some plants surrounding a lightbulb, pointing at flaws in\nthe way these models handle compositionality.\nArguably, all these previous studies require models to do more than plain language comprehension, including mathematical, spatial (Parcalabescu et al., 2022), pragmatic (Pezzelle, 2023), and compositional reasoning abilities (Thrush et al., 2022). While mastering these abilities is clearly desirable for any intelligent system, we notice that models may struggle with them due to their pre-training data and objectives. Indeed, these models are typically trained to verify whether a fragment of text is about the content of an image\u2014via the Image-Text Matching (ITM) objective\u2014which closely resembles the language comprehension tests administered to children to assess their lexical and grammatical abilities. To illustrate, in these tests, children are presented with a sentence, e.g., The red monkey is being scratched by the blue monkey, and asked to either pick the corresponding image from a set of alternatives (for an overview of this work, see Schmitt and Miller, 2010) or color the entities in a drawing accordingly (Pinto and Zuckerman, 2019). Consistent with their goal, these tests are aimed at excluding, or at least minimizing, the need for reasoning, which has been shown to be separate from language and to recruit different brain areas (Fedorenko and Varley, 2016).\nIn this work, we take inspiration from this line of research and investigate whether, and to what extent, language-and-vision models deal with language comprehension tasks that require virtually no reasoning abilities. We focus on three basic language constructions\u2014active-passive voice, coordination, and relative clauses\u2014that even preschool children typically master (Pinto and Zuckerman, 2019; Friedmann and Costa, 2010; Frizelle et al., 2017). We refer to these constructions as Basic Language Abilities (BLA) and propose an automatically constructed benchmark (see Figure 1) to assess pre-trained language-and-vision models, either in a zero-shot or in fine-tuning and in-context learning scenarios. We test several types of Transformerbased systems, i.e., CLIP, LXMERT, ViLBERT, BLIP2 and OpenFlamingo, and show that, while human (adult) speakers have no trouble at verifying these linguistic constructions, models generally struggle. Yet, the generative BLIP2 model shows promising trends, especially in an in-context learning setting. This reveals that, while BLA is a challenging benchmark, it can be used not only as an evaluation tool but also to improve SotA\nmodels\u2019 basic language abilities\u2014which are currently generally poor. We release the BLA benchmark and the code to reproduce our results at: https://github.com/shin-ee-chen/BLA."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Basic Language Comprehension Abilities",
            "text": "Language comprehension abilities\u2014in children, but also in adults, e.g., L2 learners\u2014are typically assessed in a multimodal setup: the subject is administered a sentence and some visual content and asked to verify whether the two match. Common paradigms are the Picture Selection Task (PST; Gerken and Shady, 1996), where the subject is given multiple candidate images to choose from, and the more recent Coloring Book (Pinto and Zuckerman, 2019), where the subject is presented with a single black-and-white drawing and asked to color objects in it according to the content of the sentence. Without any alternatives to choose from, in particular, the latter paradigm was introduced to minimize the recruitment of other executive functions connected to reasoning, such as selective attention and inhibition, that are not relevant to the assessment of genuine language comprehension.\nUsing these and similar paradigms, researchers demonstrated that linguistic constructions such as a sentence\u2019s active-passive voice, e.g., The red monkey scratches/is being scratched by the blue monkey (Pinto and Zuckerman, 2019), various types of coordination, e.g., Grandma smiles and the girl sang (Friedmann and Costa, 2010), and relative clauses, e.g., He saw the girl that picked the flowers (Frizelle et al., 2017), are generally mastered by preschool children across various languages, although with some differences due to the stimuli and, particularly, the experimental paradigm used. These results confirm that the comprehension of these linguistic constructions involves somewhat basic language abilities, as also indicated by previous evidence (Horgan, 1978; Diessel and Tomasello, 2001; McKee et al., 1998, inter alia).\nIn this work, we take inspiration from this line of research and aim at testing models for basic language comprehension abilities that require no or little reasoning skills. Our focused, controlled approach is novel compared to other work evaluating language-and-vision models, that we review below."
        },
        {
            "heading": "2.2 Language Abilities of Pre-Trained Multimodal Models",
            "text": "Motivated by the impressive performance of pretrained multimodal Transformer models, a recent line of research investigated whether, and to what extent, this corresponds to a genuine understanding of visually-grounded language. Using FOIL (Shekhar et al., 2017a), a benchmark of minimally wrong image descriptions where all previous-generation models have proven to fail, some work (Hessel et al., 2021; Parcalabescu et al., 2022) showed that Transformer-based models can almost perfectly distinguish between correct and foil sentences in a zero-shot setting. This indicated that models are good at grounding nouns in vision, likely due to their ITM pre-training objective. Consistently, an intrinsic evaluation of the embeddings learned by these models showed that they are better at representing highly concrete\u2014rather than abstract\u2014words (Pezzelle et al., 2021).\nLeveraging a FOIL-like paradigm, subsequent studies revealed that Transformer-based models struggle when dealing with verb arguments (SVOProbes; Hendricks and Nematzadeh, 2021), negation (Dobreva and Keller, 2021), numbers, spatial relations (VALSE; Parcalabescu et al., 2022), and expressions requiring compositional abilities (WinoGround; Thrush et al., 2022; Diwan et al., 2022) or embedding semantically underspecified language (Pezzelle, 2023). Crucially, all this previous work focused on phenomena and tasks that require more than a basic language understanding to be properly mastered. As recently pointed out by Bugliarello et al. (2023), indeed, performing well on each of these benchmarks requires models to handle different skills, ranging from mathematics to pragmatics and reasoning abilities.\nInspired by the work discussed above, we take a novel perspective and assess language-and-vision models on their genuine lexical and grammatical competence. We consider three linguistic constructions\u2014active-passive voice, coordination, and relative clauses\u2014that have been shown to be mastered even by preschool children. In this paper, we refer to them as Basic Language Abilities."
        },
        {
            "heading": "3 The BLA Benchmark",
            "text": "In this section, we describe our Basic Language Abilities (BLA) benchmark."
        },
        {
            "heading": "3.1 Linguistic Constructions",
            "text": "BLA includes three types of linguistic constructions: active-passive voice, coordination, and relative clauses, which we briefly describe below.\nActive-Passive voice (AP) In active voice sentences, the agent of the action is the subject of the verb, as in \u2018the monkey scratches the mouse\u2019, while in passive voice sentences the form of the verb indicates that the subject is the receiver of the action; e.g., \u2018the mouse is being scratched by the monkey\u2019. Understanding the contrast between active and passive voice implies being able to verify whether two sentences with different syntactic structure may have the same meaning.\nCoordination (CO) Coordination, and in particular conjunction, binds together two properties that must hold. We focus on the coordination of verb phrases joined together via the conjunction \u2018and\u2019, e.g., \u2018the monkey eats an apple and smiles\u2019. Mastering this type of coordination implies being able to verify whether both predicates (\u2019eats an apple\u2019 and \u2019smiles\u2019) apply to the subject of the sentence.\nRelative Clause (RC) Relative clauses are embedded clauses introduced by a relative pronoun that qualify an entity previously mentioned in the sentence. We focus on relative clauses attached to the subject of the sentence and introduced by the pronoun \u2018who\u2019, e.g., \u2018the monkey who scratches the mouse is tall\u2019. Understanding sentences with relative clauses implies identifying the entity qualified by the relative clause (e.g., \u2018the monkey who scratches the mouse\u2019) and verifying whether the predicate in the main clause applies (e.g., \u2019is tall\u2019)."
        },
        {
            "heading": "3.2 Benchmark Format",
            "text": "We construct a dataset of natural images and template-based sentences for each of the linguistic constructions: active-passive voice (AP), coordination (CO), and relative clause (RC). Building on FOIL (Shekhar et al., 2017b) and FOIL-like paradigms, each datapoint in our benchmark consists of an image paired with 4 automatically generated sentences, two correct ones, hence true, and two incorrect ones, hence false. False sentences are identical to true ones with respect to their format and syntactic structure but contain a mistake that makes them semantically incorrect for the image. Concretely, in the false AP sentences the agent and the recipient are reversed; in the false CO sentences one of the conjuncts does not apply to the subject,\nand in the false RC sentences the predicate of the main clause does not apply to the subject. Figure 1 shows one datapoint per linguistic construction."
        },
        {
            "heading": "3.3 Dataset Construction",
            "text": "To generate our datapoints, we use Visual Genome (Krishna et al., 2017), a dataset of natural images densely annotated with objects (bounding boxes around entities labeled with a WordNet synset; Miller, 1995), object attributes (such as color) and relationships between the objects in the image (predicates with their arguments).1 The construction procedure includes the following steps:\nI. Selection of entities and predicates Firstly, we select images in Visual Genome that include at least two objects that are persons, which we verify using the WordNet synsets. For AP, we select images where the two persons are the arguments of the same transitive verb (one as the agent and the other one as the recipient).2 We make sure that they belong to two different types (e.g., man and woman) or that they at least have two distinct attributes (e.g., running player and sitting player).\nFor CO and RC, we select images where the two persons are the subject arguments of two unique predicates, i.e., two different predicates per person that apply only to that person (e.g., \u2018wears a wetsuit\u2019 and \u2019carries a surfboard\u2019 for the man in Figure 1). Due to multiple annotations present in Visual Genome, checking whether two predicates are really different is not trivial. For example, a given person may be annotated as being the subject of two predicates, \u2019wears a t-shirt\u2019 and \u2018wears a shirt\u2019. To capture the fact that these refer to the same relationship and should not be considered as different, we encode the predicates using Sentence-BERT (Reimers and Gurevych, 2019) and consider them independent predicates only if their cosine similarity is lower than 0.8. Moreover, for all datasets, we filter out the samples that involve reasoning language by means of handcrafted rules.3\nII. Minimum object size Secondly, we filter out images where the persons identified in the previous step, or the objects that are in a relationship with such persons, are too small. We consider them too\n1More details on the annotation are in Appendix A. 2We check whether the predicates in the relationships field are part of the following list of transitive verbs: https://englishvocabs.com/transitive-verbs/ 184-transitive-verbs-list-in-english/.\n3See Appendix B for details.\nsmall if their size is below a certain ratio between the area of their bounding box and that of the entire image. We use a threshold of 0.1% for persons and of 0.05% for other objects (such as \u2018bikini\u2019 or\n\u2018stuffed bear\u2019 in the examples in Figure 1).\nIII. Sentence construction Thirdly, we construct true and false sentences using the templates in Table 5 and Table 6 in the Appendix by randomly filling them in with entities and predicates that meet the constraints above.4 Since there may be a multitude of suitable entities and predicates per image, at this construction stage each image may end up being paired with more than one set of 4 sentences.\nIV. Grammar acceptability Finally, we encode each sentence with the GRUEN pre-trained language model (Zhu and Bhat, 2020) and obtain a score that, following previous work (Parcalabescu et al., 2022), we use as a proxy for grammar acceptability. We discard datapoints where any of the 4 sentences has a GRUEN score equal to or lower than 0.7. If, after this filter, an image is still paired with more than one set of 4 sentences, we only keep the one with the highest average GRUEN score."
        },
        {
            "heading": "3.4 Descriptive Statistics",
            "text": "In Table 1, we report the dataset size (number of datapoints per linguistic construction), vocabulary size, average sentence length, and average GRUEN scores. The AP dataset is the smallest of the three, the main reason being the limited number of transitive verbs (43) with arguments that meet our construction constraints. The sentences in CO and RC are constructed from the same set of entities and predicates. The slightly lower number of datapoints and vocabulary size in RC is due to more sentences with relative clauses being discarded by the grammar acceptability filter.\n4We turn all verbs into 3rd person singular forms of the simple present tense using Python\u2019s Pattern library."
        },
        {
            "heading": "3.5 Human Performance",
            "text": "To assess the quality of our automatically constructed BLA benchmark, we run a human evaluation on 50 randomly selected datapoints for each linguistic construction dataset, using the Appen platform,5 We create 4 HITs per datapoint, each displaying the image and one of the sentences. This results in a total of 200 HITs per dataset. The task is to judge whether the sentence is correct or incorrect given the image (therefore, random accuracy on this task is 50%). We ask 3 annotators (co-authors of the paper) to judge all HITs and compute human accuracy by considering the majority vote. Since human accuracy exceeds 85% on all three datasets, we conclude that, for human speakers, verifying the sentences included in BLA is a trivial task. In contrast, we verified that BLA cannot be solved by (even powerful) text-only language models, which do not fare better than chance in any of its tasks.6"
        },
        {
            "heading": "4 Vision & Language Transformers",
            "text": "We experiment with 5 pre-trained language-andvision models: three discriminative models (CLIP, ViLBERT, and LXMERT), and two generative models (BLIP2 and OpenFlamingo).7 We briefly describe the models below."
        },
        {
            "heading": "4.1 Discriminative Models",
            "text": "CLIP CLIP (Radford et al., 2021) has two separate Transformer-based encoders, one for language, and one for vision (here, we use the ViT-B/32 version). These encoders are jointly trained on 400M <image, caption> pairs gathered from the web. It is trained through contrastive learning for predicting high similarity scores for paired <image, caption> and low scores for unpaired samples. We compute image-text alignment scores between an image and either its corresponding true or false sentences.\nViLBERT ViLBERT (Lu et al., 2019) is a dualstream Transformer that encodes language and visual inputs separately before combining them via co-attention layers. It is pre-trained on the Conceptual Captions (Sharma et al., 2018) dataset with two learning objectives: multimodal masked learning (both word and object prediction), as well as\n5https://appen.com/ 6Further details about our experiments with GPT-2 (Radford et al., 2019) are provided in Appendix H. 7We also experimented with FROMAGe (Koh et al., 2023) and MAGMA (Eichenberg et al., 2022) but decided not to include them in our study due to their limited ability to generate yes/no answers. More details are provided in Appendix F.\nimage-text matching (ITM). The pre-trained checkpoint we use is the one released by the VOLTA framework (Bugliarello et al., 2021).8 Here, we use the pre-trained ITM head to straightly perform the binary classification (true/false) for each <image, sentence> pair in our benchmark.\nLXMERT LXMERT (Tan and Bansal, 2019) is a dual-stream Transformer model that encodes vision and language via two separate streams and combines them via cross-model layers. The model checkpoint we use is pre-trained on the same exact data and with the same learning objectives as ViLBERT, again from the VOLTA framework.9 Therefore, the two models are directly comparable."
        },
        {
            "heading": "4.2 Generative Models",
            "text": "BLIP2 BLIP2 (Li et al., 2023) is a generative model that uses a Querying Transformer (QFormer) to combine the information from a frozen large language model (LLM) and a frozen image encoder. The Q-Former contains two submodules\u2014 one image Transformer and one text Transformer\u2014 that share the same self-attention layers. The Qformer is trained in two steps: first, it connects the image Transformer submodule to a frozen image encoder to learn multimodal representations via image-text contrastive learning, image-grounded text generation, and image-text matching. Second, it performs vision-to-language generation by learning query embeddings that force the underlying LLM to generate text based on the visual information by the Q-former. We use BLIP2-FlanT5XXL.\nOpenFlamingo OpenFlamingo (Awadalla et al., 2023) is an open-source reproduction of the Flamingo models (Alayrac et al., 2022). The model is pre-trained to generate text from a sequence of text tokens interleaved with images. It contains a frozen pre-trained CLIP-like image encoder and a frozen pre-trained large language model. The two components are connected via cross-attention layers that allow the language model to attend to features produced by the vision model. The models are pre-trained on the LAION-2B (Schuhmann et al., 2022) and Multimodal C4 (Zhu et al., 2023) datasets. In our experiments, we use CLIP ViTL/14 as the vision encoder and one of the 3B ver-\n8Available at https://sid.erda.dk/share_redirect/ aQCx8cLWK7.\n9Available at https://sid.erda.dk/share_redirect/ Dp1g16DIA5.\nsions of the underlying language model.10"
        },
        {
            "heading": "5 Exp 1: Zero-Shot Evaluation",
            "text": "To explore whether, and to what extent, the pretrained models described in Section 4 can deal with linguistic constructions in BLA without any taskspecific fine-tuning, we evaluate them on the three datasets in a zero-shot setting. For each dataset, we frame the problem as a binary task: given an <image, sentence> pair, the models are asked to evaluate whether the sentence is true or false.\nViLBERT and LXMERT can be straightforwardly evaluated on the binary task thanks to their pre-trained image-text matching (ITM) classification head. For CLIP, we compute similarity scores between the image and each sentence, and rank the four <image, sentence> pairs according to them. We consider the 2 top-ranked sentences as true and the 2 lower-ranked sentences as false.\nWe evaluate BLIP2 and OpenFlamingo by prompting. The prompt template used with BLIP2 is similar to the one proposed by Li et al. (2023) for Visual Question Answering: \u2018Question: Is the sentence [sentence] appropriate for this image? yes or no? Answer:\u2019. For OpenFlamingo, following Awadalla et al. (2023), we use the following prompt template: \u2018<image>Question: Is the sentence [sentence] appropriate for this image? yes or no? Short Answer:\u2019. We let the models generate a response and consider \u2018yes\u2019 answers as true and \u2018no\u2019 answers as false.11\nWe use three metrics to measure model performance: (1) accuracy, measuring how well the models perform on the binary task, (2) precision_true, measuring how well models identify the true sentences, and (3) precision_false, measuring how well the models identify the false sentences."
        },
        {
            "heading": "5.1 Results",
            "text": "All models lag far behind human performance Results by all models are reported in Table 2. As can be seen, none of the models performs anywhere close to human performance on the BLA benchmark; indeed, most of them obtain results around chance level. While BLIP2 achieves a remarkably higher accuracy on AP (64%) than on the other two datasets, this result is still very far from 92%,\n10Concretely, as language model we use the instructionfinetuned model RedPajama-INCITE-Instruct-3B-v1.\n11Both models always generated a \u2018yes\u2019/\u2018no\u2019 answer.\nMetric Model / Humans\nTask\nAP CO RC\nAcc\nViLBERT 50.57 49.81 49.96 LXMERT 49.31 49.77 50.00 CLIP 50.08 49.24 49.33 BLIP2 64.15 52.10 52.19 OpenFlamingo 50.73 50.15 49.52 Humans 92.00 90.00 85.00\nPt ViLBERT 50.43 49.80 49.97 LXMERT 49.49 49.81 50.00 CLIP 50.16 49.24 49.18 BLIP2 66.73 51.88 52.15 OpenFlamingo 50.40 50.11 49.50 Humans 90.00 79.00 79.00\nPf ViLBERT 50.84 49.82 49.96 LXMERT 48.94 49.71 50.00 CLIP 50.16 49.24 49.18 BLIP2 62.26 52.38 52.25 OpenFlamingo 54.13 50.26 49.53 Humans 94.00 95.00 95.00\nchance 50.00 50.00 50.00\nTable 2: Zero-shot model performance and human performance on BLA. Acc: Accuracy. Pt: Precision_true. Pf : Precision_false. Scores are reported in percentage. The highest results for each metric and task are in bold.\ni.e., human accuracy on this linguistic construction. Overall, this pattern of results reveals that, in a zero-shot setting, models struggle with the BLA benchmark, in line with previous work investigating other linguistic and reasoning phenomena (Thrush et al., 2022; Parcalabescu et al., 2022).\nBLIP2 is the best-performing model The highest-performing model on the BLA benchmark is the generative model BLIP2. It outperforms OpenFlamingo and the best discriminative models with respect to all evaluation metrics (see Table 2). BLIP2 is the only model that consistently surpasses chance level on all three tasks\u2014though by a small margin in both CO and RC\u2014while the other models perform around or below chance level."
        },
        {
            "heading": "5.2 Discussion",
            "text": "The overall poor performance obtained in the zeroshot setting indicates that pre-trained multimodal models struggle with the language comprehension abilities evaluated by the BLA benchmark. This could be due to the way in which these models are typically pre-trained, i.e., maximizing cross-\nmodal alignment, which might not be fine-grained enough to account for the complex dynamics that intertwine language and vision. Performing well on BLA, indeed, requires understanding how entities interact with each other, how their attributes combine, and what attributes refer to which entity. Neither discriminative nor generative pre-trained models seem to handle these abilities.\nAt the same time, we notice that BLIP2 performs better than the other models, particularly on the active-passive construction. This advantage could result from the more varied data and pretraining objectives\u2014image-text matching, imagetext contrastive learning, and image-grounded text generation\u2014of this model, which would help the model better understand verbs and verb arguments."
        },
        {
            "heading": "5.3 Error Analysis",
            "text": "We conduct an error analysis focused on the samples where the models consider all four sentences as either all true or all false. Considering that, in our dataset, each image is systematically paired with two true and two false sentences (see Figure 2), these cases are interesting since they indicate that models fail to make consistent predictions\u2014 intuitively, the sentences the man holds the baby and the baby holds the man cannot be true at the same time for a given image. This, in turn, would reveal that the models are unable to correctly identify the entities mentioned in the sentence.\nFor each model except CLIP,12 we consider all the cases where all four sentences are assigned the same predicted label, either true or false. For BLIP2, these cases constitute 54.65%, 32.75%, and 38.55% of the samples in the AP, CO, and RC constructions, respectively. While these numbers may already seem quite high, we find out they are even higher in other models. For ViLBERT, they increase particularly for AP (86.95%), with CO (49.43%) and RC (54.0%) experiencing a less dramatic increase. Similar percentages for AP, and a further increase for the other constructions, are observed for OpenFlamingo (88.5%, 68.5%, and 64.5% for AP, CO, and RC, respectively) and LXMERT (87.77%, 58.4, and 60.02%, resp.).\nThese patterns reveal that models are very often inconsistent with their predictions. This suggests they have a limited ability to identify the relevant entities, as well as their properties, in the image.\n12Recall that, for CLIP, we use a ranking-based approach."
        },
        {
            "heading": "6 Exp 2: BLA-Specific Learning",
            "text": "To explore whether the models\u2014which obtain poor performance in the zero-shot setting\u2014can learn to deal with the linguistic constructions in BLA via some degree of task-specific learning, we expose them to a relatively little amount of data from each dataset. We use these samples to fine-tune the discriminative models and prompt the generative models to allow them to perform in-context learning.\nIn particular, we experiment with two types of BLA-specific learning, i.e., (1) we train and test a model with data from the same dataset (SD), and (2) we train a model with data from one dataset and test it with data from a different dataset (DD) in a cross-task setting. With the former, we evaluate whether the models can learn some key properties of the linguistic construction at hand by having exposure to the same phenomenon in other visual contexts. With the latter, we test whether the models can develop general language comprehension abilities that are key to all linguistic constructions.\nWe downsize each dataset to include 400 samples. Then, we randomly split it into training (100 samples), validation (100), and test (200) sets. While each sample in the validation and test sets has the standard format\u2014one image and four corresponding sentences: two true sentences and two false ones\u2014in the training set that we use to finetune or prompt our models we only keep two sentences per image. In particular, we randomly sample one true and one false sentence to keep the supervision of the models relatively little.\nDiscriminative Models As CLIP does not have a classification head, we perform fine-tuning on ViLBERT and LXMERT. We use the training samples to fine-tune their ITM classification head. Further details are provided in Appendix D. In SD, models are fine-tuned, selected, and evaluated with data from the same dataset. In DD, they are evaluated with data from the test set of a different dataset.\nGenerative Models We perform in-context learning with BLIP2 only. For OpenFlamingo, our preliminary investigations revealed that, with our data and task, using the in-context learning setup proposed in the original model paper led to nonsense outputs in many cases. The same was observed when using the same in-context learning setup that we used for BLIP2, which we describe below.13\nIn BLIP2, we perform in-context learning using samples from the test set of a given dataset. In SD, for each <sentence, image> pair that we aim to evaluate, we pick one true and one false sentence belonging to the same datapoint. Compared to the sentence under evaluation [target], these sentences have either a different voice (AP) or describe attributes for a different person (CO and RC). In BLIP2, we fill these sentences into a template that we use as a prompt for the model. For example: \u2018Question: Is the sentence [true] appropriate for this image? yes or no? Answer: yes. Question: Is the sentence [false] appropriate for this image? yes or no? Answer: no. Question: Is the sentence [target] appropriate for this\n13Further details about these preliminary experiments and corresponding prompting setups are reported in Appendix G.\nimage? yes or no? Answer:\u2019. In DD, the setup is the same as above, except that the [true] and [false] sentences are from another BLA dataset (different linguistic construction), and yet about the same image (same entities and attributes). While images in CO and RC greatly overlap, images in AP do not overlap much with those in the other two datasets. Therefore, we only experiment with CO and RC in this setting."
        },
        {
            "heading": "6.1 Results",
            "text": "BLA-specific learning generally helps As reported in Figure 3, BLA-specific learning has a generally positive impact on model performance. The generative BLIP2 is shown to improve on all three datasets, which confirms the effectiveness of the in-learning setup in boosting this model\u2019s comprehension abilities. As for discriminative models, ViLBERT experiences the greatest accuracy improvement compared to the zero-shot setting on AP, and a smaller (though consistent) improvement on the other two tasks; LXMERT, in contrast, does not seem to equally benefit from fine-tuning, except perhaps for a little boost on the AP construction.\nBLIP2 is the best overall model As shown in Figure 3, BLIP2 is again the overall best model across the board. Indeed, it outperforms the best discriminative models by 10.1, 3.2, and 8.6 accuracy points on AP, CO, and RC, respectively. Moreover, it is the model achieving the higher relative improvement in accuracy on all BLA datasets over the zero-shot setting. It is worth mentioning that, looking at the relative improvement in precision obtained by various models over the zero-shot setting (Table 3), BLIP2 exhibits a fairly high improvement on all tasks, particularly CO and RC, with respect to precision_false. That is, task-specific learning particularly helps the model to better spot\nfalse sentences, which in turn allows it to achieve an overall higher performance on the tasks.\nActive-passive voice is the most easily learned Overall, the active-passive voice construction appears to be the one that models can learn to a greater extent. While BLIP2 achieves an accuracy of more than 70%, ViLBERT ranks second with a respectable 61%, which is an even more notable result considering the performance around chance level in the zero-shot setting. LXMERT also outperforms the results obtained in zero-shot learning, though by a much smaller margin.\nCross-task learning is only effective for BLIP2 As reported in Table 4, BLIP2 is the only model across the board that benefits from learning about a linguistic construction that is not the one under investigation (our DD setting). As can be seen, the improvement by BLIP2 on RC after having been exposed to CO exceeds 11% accuracy compared to the zero-shot learning, while the improvement in the other direction (RC to CO) is around 7%. This is not the case, instead, for the discriminative models, for which the role played by this setup is either insignificant or even detrimental."
        },
        {
            "heading": "6.2 Discussion",
            "text": "The results presented above generally show that BLA-specific learning has an overall positive role in helping models understand the linguistic constructions included in the benchmark. This suggests that having (even limited) experience with how these linguistic constructions work in visuallygrounded contexts is beneficial for these models,\nwhich are shown to improve their performance over the zero-shot setting. In particular, BLAspecific learning helps the generative BLIP2, which is shown to improve not only in the SD setting but also in DD, where examples of other linguistic constructions are provided. This pattern is encouraging and suggests that understanding these linguistic constructions may underlie some common basic language abilities dealing with the semantic properties of entities, attributes, and predicates, and their interaction with the image.\nYet, their performance on the benchmark is still far from human performance, with the best overall model (BLIP2) lagging 20 accuracy points behind human accuracy on AP, the highest-scoring dataset. At the same time, some linguistic constructions appear to be more challenging to learn than others, with coordination experiencing much lower improvement compared to AP and RC. On the other hand, AP stands out as the construction that can be best learned by the models, possibly due to the fact that it requires models to ground the entities\u2014but not necessarily their attributes."
        },
        {
            "heading": "7 Conclusions",
            "text": "We introduced a novel benchmark, BLA, aimed at investigating how well multimodal models understand basic linguistic constructions\u2014active-passive voice, coordination, and relative clauses. We showed that the linguistic constructions in BLA are challenging for current language-and-vision models, which lag well behind human performance. Yet, the recent generative model BLIP2 exhibits a better performance than discriminative models, both in the zero-shot and task-specific learning setting. We highlight that prompting generative models with examples embedding both the same or a different linguistic construction is a promising method to improve their understanding of specific linguistic constructions. This opens the door to using BLA not only to evaluate pre-trained models but also to improve their basic language abilities.\nLimitations\nThe BLA benchmark currently contains three tasks while more can be added for a more comprehensive understanding of basic language ability of multimodal models. But our pipeline can be easily adapted to construct more tasks. In the experiment, we only investigated a limited number of models, which include (i) generative models (BLIP2 and\nOpenFlamingo), (ii) discriminative models with an image-text classification head (ViLBERT and LXMERT) and (iii) discriminative models with cross-modality similarity scores (CLIP), which we believe our selections are representative of current mainstream Vision-and-Language (V&L) models. Due to the in-context learning constraints for BLIP2, we only investigate its BLA-specific cross-task learning setup on Coordination and Relative Clause tasks. The two tasks are constructed with the same pipeline and contain similar descriptions of human attributes, so more investigation can be done to explore whether the model can improve with learning examples that are more semantically different."
        },
        {
            "heading": "Acknowledgements",
            "text": "We are grateful to Iacer Calixto for the valuable feedback on a preliminary version of the dataset and experiments. We want to thank Emanuele Bugliarello for his help with the VOLTA framework and Hongyi Li for his support with the OpenFlamingo model evaluation. We also thank the members of the Dialogue Modelling Group at the ILLC and the members of the IRLab at IvI, University of Amsterdam, for their insightful feedback on a draft of the paper and the experiments. Xinyi Chen is funded by the project LESSEN with project number NWA.1389.20.183 of the research program NWA-ORC 2020/21, which is (partly) financed by the Dutch Research Council (NWO). RF is supported by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreement No. 819455)."
        },
        {
            "heading": "B Avoiding Reasoning in BLA Construction",
            "text": "By analyzing the VG annotations that contain at least two human entities, we found the most common annotations related to reasoning language is the positional description (e.g. \u2018on the right\u2019, \u2018behind\u2019). We came up with a list of commonly-used positional words like \u2018right\u2019, \u2018above\u2019 and adjusted the keyword search rules to better filter reasoning\nlanguage. We also added more phrases and words to the list by checking the annotations that contain prepositions. In addition, we try to avoid numerical descriptions in our extracted information by filtering annotations that contain numbers from one to ten. This was an effective rule based on our observation on the VG annotations."
        },
        {
            "heading": "C Sentence Generation Templates",
            "text": "We use sentence construction templates (Table 5 and Table 6) and the extracted information described in Section 3 to generate a set of four sentences (two true, two false) for a given image."
        },
        {
            "heading": "D Model Finetuning Details",
            "text": "We fine-tune ViLBERT and LXMERT pretrained model on their entire model layers with the ImageText Matching learning objective only. We modify the pretraining code from the VOLTA framework (available at https://github.com/e-bug/ volta/blob/main/train_concap.py) and adapt the hyperparameter settings for finetuning provided by the code owners. We set the learning rate to 1e5, the training batch size to 16 and the maximum training epoch to 10 after hyperparameter search-\ning. The model checkpoint that achieves the highest accuracy performance on the validation set will be used for evaluation. We also experiment with only fine-tuning specific layers of the models but find out that fine-tuning the whole model achieves the best performance."
        },
        {
            "heading": "E Human Annotation Details",
            "text": "We collect human annotations with Appen\u2019s internal channel option. The question interface and test question setups are shown in Figure 5.\nThe annotations are collected from three coauthors of this paper, including two linguistic experts and one dataset constructor. The annotators are requested to make their judgment only based on the visual and text information provided in the questions."
        },
        {
            "heading": "F MAGMA and FROMAGe",
            "text": "In the zero-shot evaluation for generative models, we employ prompting to guide the models to generate constrained outputs, specifically \u2018yes\u2019 or \u2018no\u2019 responses, to perform the binary classification tasks on BLA. We use the BLIP2 prompt template and apply minor changes to it, e.g., we replace \u2018Question\u2019 with \u2018Q\u2019 and \u2018Answer\u2019 with \u2018A\u2019 following the prompting examples reported in the papers of the models. Our investigation revealed that, in over 20% of the cases, MAGMA and FROMAGe fail to generate the desired outputs. Indeed, many answers instead provided explanations for why a sentence is correct or incorrect. Since evaluating their performance would require more careful (including manual) analysis, in our zeroshot experiments we focused on BLIP2 and OpenFlamingo, which exhibited higher ability to adhere to the task instructions. We share the code used to preliminary test MAGMA and FROMAGe at https://github.com/shin-ee-chen/BLA."
        },
        {
            "heading": "G OpenFlamingo In-context Learning",
            "text": "After conducting preliminary in-context learning experiments, we observed that OpenFlamingo struggled to generate constrained \u2018yes\u2019 or \u2018no\u2019 answers. In particular, we experimented with two templates. The first template, similar to the BLIP2 prompt template, uses only one image input for both the examples and the question, as follows: [<image>Question: Is the sentence [true] appropriate for this image? yes\nor no? Short Answer: yes. Question: Is the sentence [false] appropriate for this image? yes or no? Short Answer: no. Question: Is the sentence [target] appropriate for this image? yes or no? Short Answer:]. This prompt template led to nonsense outputs in most of the cases.\nThe second template we used closely follows the one provided in the original model paper Awadalla et al. (2023), which requires the token <image> for image input before each question, as follows: [<image>Question: Is the sentence [true] appropriate for this image? yes or no? Short Answer: yes.<|endofchunk|><image>Question: Is the sentence [false] appropriate for this image? yes or no? Short Answer: no.<|endofchunk|><image>Question: Is the sentence [target] appropriate for this image? yes or no? Short Answer:]. Note that the three <image> tokens always refer to the same image. With this template, the model generated more constrained answers, but only for about 40% of the cases, which is still unsatisfactory. We hypothesize this could be due, at least in part, to the properties of our questions, that are longer and more complex than the examples provided in the model paper. This could harm OpenFlamingo\u2019s ability to follow instructions."
        },
        {
            "heading": "H Language-Only Model on BLA Tasks",
            "text": "As a sanity check, we test whether the BLA tasks can be solved by a powerful text-only model, namely, GPT2 (Radford et al., 2019). We calculate the perplexity scores for the four sentences in each datapoint and rank them such that the lower the perplexity, the higher the ranking. Similarly to our experiments with CLIP, we consider two topranked sentences as true and the other two as false. As expected, GPT-2 performs around chance level in all tasks: 50.08% for Active-Passive, 50.47% for Coordination, and 49.96% for Relative Clause."
        }
    ],
    "title": "The BLA Benchmark: Investigating Basic Language Abilities of Pre-Trained Multimodal Models",
    "year": 2023
}