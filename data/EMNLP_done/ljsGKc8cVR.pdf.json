{
    "abstractText": "Pre-trained language models (PLMs) have achieved the preeminent position in dense retrieval due to their powerful capacity in modeling intrinsic semantics. However, most existing PLM-based retrieval models encounter substantial computational costs and are infeasible for processing long documents. In this paper, a novel retrieval model Longtriever is proposed to embrace three core challenges of long document retrieval: substantial computational cost, incomprehensive document understanding, and scarce annotations. Longtriever splits long documents into short blocks and then efficiently models the local semantics within a block and the global context semantics across blocks in a tightly-coupled manner. A pretraining phase is further proposed to empower Longtriever to achieve a better understanding of underlying semantic correlations. Experimental results on two popular benchmark datasets demonstrate the superiority of our proposal. The source code is released at https: //github.com/SamuelYang1/Longtriever .",
    "authors": [
        {
            "affiliations": [],
            "name": "Junhan Yang"
        },
        {
            "affiliations": [],
            "name": "Zheng Liu"
        },
        {
            "affiliations": [],
            "name": "Chaozhuo Li"
        },
        {
            "affiliations": [],
            "name": "Guangzhong Sun"
        },
        {
            "affiliations": [],
            "name": "Xing Xie"
        }
    ],
    "id": "SP:e8793b8fb4563bd8018e55a6cd038634bd366d05",
    "references": [
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv preprint arXiv:2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Cheng Chang",
                "Felix X Yu",
                "Yin-Wen Chang",
                "Yiming Yang",
                "Sanjiv Kumar."
            ],
            "title": "Pre-training tasks for embedding-based large-scale retrieval",
            "venue": "arXiv preprint arXiv:2002.03932.",
            "year": 2020
        },
        {
            "authors": [
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Ilya Sutskever."
            ],
            "title": "Generating long sequences with sparse transformers",
            "venue": "arXiv preprint arXiv:1904.10509.",
            "year": 2019
        },
        {
            "authors": [
                "Nick Craswell",
                "Bhaskar Mitra",
                "Emine Yilmaz",
                "Daniel Campos",
                "Ellen M Voorhees."
            ],
            "title": "Overview of the trec 2019 deep learning track",
            "venue": "arXiv preprint arXiv:2003.07820.",
            "year": 2020
        },
        {
            "authors": [
                "Zhuyun Dai",
                "Jamie Callan."
            ],
            "title": "Context-aware sentence/passage term importance estimation for first stage retrieval",
            "venue": "arXiv preprint arXiv:1910.10687.",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Siyu Ding",
                "Junyuan Shang",
                "Shuohuan Wang",
                "Yu Sun",
                "Hao Tian",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "Erniedoc: A retrospective long-document modeling transformer",
            "venue": "arXiv preprint arXiv:2012.15688.",
            "year": 2020
        },
        {
            "authors": [
                "Luyu Gao",
                "Jamie Callan."
            ],
            "title": "Condenser: a pre-training architecture for dense retrieval",
            "venue": "arXiv preprint arXiv:2104.08253.",
            "year": 2021
        },
        {
            "authors": [
                "Luyu Gao",
                "Jamie Callan."
            ],
            "title": "Unsupervised corpus aware language model pre-training for dense passage retrieval",
            "venue": "arXiv preprint arXiv:2108.05540.",
            "year": 2021
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "Simcse: Simple contrastive learning of sentence embeddings",
            "venue": "arXiv preprint arXiv:2104.08821.",
            "year": 2021
        },
        {
            "authors": [
                "Jiafeng Guo",
                "Yinqiong Cai",
                "Yixing Fan",
                "Fei Sun",
                "Ruqing Zhang",
                "Xueqi Cheng."
            ],
            "title": "Semantic models for the first-stage retrieval: A comprehensive review",
            "venue": "ACM Transactions on Information Systems (TOIS), 40(4):1\u201342.",
            "year": 2022
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Deberta: Decoding-enhanced bert with disentangled attention",
            "venue": "arXiv preprint arXiv:2006.03654.",
            "year": 2020
        },
        {
            "authors": [
                "Mandar Joshi",
                "Danqi Chen",
                "Yinhan Liu",
                "Daniel S Weld",
                "Luke Zettlemoyer",
                "Omer Levy."
            ],
            "title": "Spanbert: Improving pre-training by representing and predicting spans",
            "venue": "Transactions of the Association for Computational Linguistics, 8:64\u201377.",
            "year": 2020
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas O\u011fuz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "arXiv preprint arXiv:2004.04906.",
            "year": 2020
        },
        {
            "authors": [
                "Nikita Kitaev",
                "\u0141ukasz Kaiser",
                "Anselm Levskaya."
            ],
            "title": "Reformer: The efficient transformer",
            "venue": "arXiv preprint arXiv:2001.04451.",
            "year": 2020
        },
        {
            "authors": [
                "Kenton Lee",
                "Ming-Wei Chang",
                "Kristina Toutanova."
            ],
            "title": "Latent retrieval for weakly supervised open domain question answering",
            "venue": "arXiv preprint arXiv:1906.00300.",
            "year": 2019
        },
        {
            "authors": [
                "Dmitry Lepikhin",
                "HyoukJoong Lee",
                "Yuanzhong Xu",
                "Dehao Chen",
                "Orhan Firat",
                "Yanping Huang",
                "Maxim Krikun",
                "Noam Shazeer",
                "Zhifeng Chen."
            ],
            "title": "Gshard: Scaling giant models with conditional computation and automatic sharding",
            "venue": "arXiv preprint",
            "year": 2020
        },
        {
            "authors": [
                "Canjia Li",
                "Andrew Yates",
                "Sean MacAvaney",
                "Ben He",
                "Yingfei Sun."
            ],
            "title": "Parade: Passage representation aggregation for document reranking",
            "venue": "arXiv preprint arXiv:2008.09093.",
            "year": 2020
        },
        {
            "authors": [
                "Chaozhuo Li",
                "Bochen Pang",
                "Yuming Liu",
                "Hao Sun",
                "Zheng Liu",
                "Xing Xie",
                "Tianqi Yang",
                "Yanling Cui",
                "Liangjie Zhang",
                "Qi Zhang."
            ],
            "title": "Adsgnn: Behavior-graph augmented relevance modeling in sponsored search",
            "venue": "Proceedings of SIGIR.",
            "year": 2021
        },
        {
            "authors": [
                "Rui Li",
                "Jianan Zhao",
                "Chaozhuo Li",
                "Di He",
                "Yiqi Wang",
                "Yuming Liu",
                "Hao Sun",
                "Senzhang Wang",
                "Weiwei Deng",
                "Yanming Shen"
            ],
            "title": "2022. House: Knowledge graph embedding with householder parameterization",
            "venue": "Proceedings of ICML",
            "year": 2022
        },
        {
            "authors": [
                "Jimmy Lin",
                "Rodrigo Nogueira",
                "Andrew Yates."
            ],
            "title": "Pretrained transformers for text ranking: Bert and beyond",
            "venue": "Synthesis Lectures on Human Language Technologies, 14(4):1\u2013325.",
            "year": 2021
        },
        {
            "authors": [
                "Fangyu Liu",
                "Ivan Vuli\u0107",
                "Anna Korhonen",
                "Nigel Collier."
            ],
            "title": "Fast, effective, and self-supervised: Transforming masked language models into universal lexical and sentence encoders",
            "venue": "arXiv preprint arXiv:2104.08027.",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zheng Liu",
                "Yingxia Shao."
            ],
            "title": "Retromae: Pretraining retrieval-oriented transformers via masked auto-encoder",
            "venue": "arXiv preprint arXiv:2205.12035.",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Shuqi Lu",
                "Chenyan Xiong",
                "Di He",
                "Guolin Ke",
                "Waleed Malik",
                "Zhicheng Dou",
                "Paul Bennett",
                "Tie-Yan Liu",
                "Arnold Overwijk"
            ],
            "title": "Less is more: Pre-training a strong siamese encoder using a weak decoder",
            "year": 2021
        },
        {
            "authors": [
                "Wenhao Lu",
                "Jian Jiao",
                "Ruofei Zhang."
            ],
            "title": "Twinbert: Distilling knowledge to twin-structured compressed bert models for large-scale retrieval",
            "venue": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pages",
            "year": 2020
        },
        {
            "authors": [
                "Yi Luan",
                "Jacob Eisenstein",
                "Kristina Toutanova",
                "Michael Collins."
            ],
            "title": "Sparse, dense, and attentional representations for text retrieval",
            "venue": "Transactions of the Association for Computational Linguistics, 9:329\u2013 345.",
            "year": 2021
        },
        {
            "authors": [
                "Xinyu Ma",
                "Jiafeng Guo",
                "Ruqing Zhang",
                "Yixing Fan",
                "Xueqi Cheng."
            ],
            "title": "Pre-train a discriminative text encoder for dense retrieval via contrastive span prediction",
            "venue": "arXiv preprint arXiv:2204.10641.",
            "year": 2022
        },
        {
            "authors": [
                "Xinyu Ma",
                "Jiafeng Guo",
                "Ruqing Zhang",
                "Yixing Fan",
                "Xiang Ji",
                "Xueqi Cheng."
            ],
            "title": "Prop: pre-training with representative words prediction for ad-hoc retrieval",
            "venue": "Proceedings of the 14th ACM International Conference on Web Search and Data Mining, pages",
            "year": 2021
        },
        {
            "authors": [
                "Xinyu Ma",
                "Jiafeng Guo",
                "Ruqing Zhang",
                "Yixing Fan",
                "Yingyan Li",
                "Xueqi Cheng."
            ],
            "title": "B-prop: bootstrapped pre-training with representative words prediction for ad-hoc retrieval",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Re-",
            "year": 2021
        },
        {
            "authors": [
                "Tri Nguyen",
                "Mir Rosenberg",
                "Xia Song",
                "Jianfeng Gao",
                "Saurabh Tiwary",
                "Rangan Majumder",
                "Li Deng."
            ],
            "title": "Ms marco: A human generated machine reading comprehension dataset",
            "venue": "CoCo@ NIPs.",
            "year": 2016
        },
        {
            "authors": [
                "Bochen Pang",
                "Chaozhuo Li",
                "Yuming Liu",
                "Jianxun Lian",
                "Jianan Zhao",
                "Hao Sun",
                "Weiwei Deng",
                "Xing Xie",
                "Qi Zhang."
            ],
            "title": "Improving relevance modeling via heterogeneous behavior graph learning in bing ads",
            "venue": "Proceedings of KDD.",
            "year": 2022
        },
        {
            "authors": [
                "Yingqi Qu",
                "Yuchen Ding",
                "Jing Liu",
                "Kai Liu",
                "Ruiyang Ren",
                "Wayne Xin Zhao",
                "Daxiang Dong",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "Rocketqa: An optimized training approach to dense passage retrieval for open-domain question answering",
            "venue": "arXiv preprint",
            "year": 2020
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza"
            ],
            "title": "The probabilistic relevance framework: Bm25 and beyond",
            "venue": "Foundations and Trends\u00ae in Information Retrieval,",
            "year": 2009
        },
        {
            "authors": [
                "Yu Sun",
                "Shuohuan Wang",
                "Yukun Li",
                "Shikun Feng",
                "Xuyi Chen",
                "Han Zhang",
                "Xin Tian",
                "Danxiang Zhu",
                "Hao Tian",
                "Hua Wu."
            ],
            "title": "Ernie: Enhanced representation through knowledge integration",
            "venue": "arXiv preprint arXiv:1904.09223.",
            "year": 2019
        },
        {
            "authors": [
                "Zhoujin Tian",
                "Chaozhuo Li",
                "Zhiqiang Zuo",
                "Zengxuan Wen",
                "Lichao Sun",
                "Xinyue Hu",
                "Wen Zhang",
                "Haizhen Huang",
                "Senzhang Wang",
                "Weiwei Deng"
            ],
            "title": "Pass: Personalized advertiser-aware sponsored search",
            "venue": "In Proceedings of the 29th ACM SIGKDD",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Tao Qi",
                "Yongfeng Huang."
            ],
            "title": "Hi-transformer: hierarchical interactive transformer for efficient and effective long document modeling",
            "venue": "arXiv preprint arXiv:2106.01040.",
            "year": 2021
        },
        {
            "authors": [
                "Zhuofeng Wu",
                "Sinong Wang",
                "Jiatao Gu",
                "Madian Khabsa",
                "Fei Sun",
                "Hao Ma."
            ],
            "title": "Clear: Contrastive learning for sentence representation",
            "venue": "arXiv preprint arXiv:2012.15466.",
            "year": 2020
        },
        {
            "authors": [
                "Shitao Xiao",
                "Zheng Liu",
                "Weihao Han",
                "Jianjin Zhang",
                "Yingxia Shao",
                "Defu Lian",
                "Chaozhuo Li",
                "Hao Sun",
                "Denvy Deng",
                "Liangjie Zhang"
            ],
            "title": "Progressively optimized bi-granular document representation for scalable embedding based retrieval",
            "year": 2022
        },
        {
            "authors": [
                "Shitao Xiao",
                "Zheng Liu",
                "Yingxia Shao",
                "Tao Di",
                "Bhuvan Middha",
                "Fangzhao Wu",
                "Xing Xie."
            ],
            "title": "Training large-scale news recommenders with pretrained language models in the loop",
            "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge",
            "year": 2022
        },
        {
            "authors": [
                "Lee Xiong",
                "Chenyan Xiong",
                "Ye Li",
                "Kwok-Fung Tang",
                "Jialin Liu",
                "Paul Bennett",
                "Junaid Ahmed",
                "Arnold Overwijk."
            ],
            "title": "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
            "venue": "arXiv preprint arXiv:2007.00808.",
            "year": 2020
        },
        {
            "authors": [
                "Yuanmeng Yan",
                "Rumei Li",
                "Sirui Wang",
                "Fuzheng Zhang",
                "Wei Wu",
                "Weiran Xu."
            ],
            "title": "Consert: A contrastive framework for self-supervised sentence representation transfer",
            "venue": "arXiv preprint arXiv:2105.11741.",
            "year": 2021
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Jingtao Zhan",
                "Jiaxin Mao",
                "Yiqun Liu",
                "Jiafeng Guo",
                "Min Zhang",
                "Shaoping Ma."
            ],
            "title": "Jointly optimizing query encoder and product quantization to improve retrieval performance",
            "venue": "Proceedings of the 30th ACM International Conference on Information",
            "year": 2021
        },
        {
            "authors": [
                "Jingtao Zhan",
                "Jiaxin Mao",
                "Yiqun Liu",
                "Jiafeng Guo",
                "Min Zhang",
                "Shaoping Ma."
            ],
            "title": "Optimizing dense retrieval model training with hard negatives",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information",
            "year": 2021
        },
        {
            "authors": [
                "Jingtao Zhan",
                "Jiaxin Mao",
                "Yiqun Liu",
                "Jiafeng Guo",
                "Min Zhang",
                "Shaoping Ma."
            ],
            "title": "Learning discrete representations via constrained clustering for effective and efficient dense retrieval",
            "venue": "Proceedings of",
            "year": 2022
        },
        {
            "authors": [
                "Hang Zhang",
                "Yeyun Gong",
                "Yelong Shen",
                "Jiancheng Lv",
                "Nan Duan",
                "Weizhu Chen."
            ],
            "title": "Adversarial retriever-ranker for dense text retrieval",
            "venue": "arXiv preprint arXiv:2110.03611.",
            "year": 2021
        },
        {
            "authors": [
                "Xingxing Zhang",
                "Furu Wei",
                "Ming Zhou."
            ],
            "title": "Hibert: Document level pre-training of hierarchical bidirectional transformers for document summarization",
            "venue": "arXiv preprint arXiv:1905.06566.",
            "year": 2019
        },
        {
            "authors": [
                "Yan Zhang",
                "Ruidan He",
                "Zuozhu Liu",
                "Lidong Bing",
                "Haizhou Li."
            ],
            "title": "Bootstrapped unsupervised sentence representation learning",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint",
            "year": 2021
        },
        {
            "authors": [
                "Yukun Zhu",
                "Ryan Kiros",
                "Rich Zemel",
                "Ruslan Salakhutdinov",
                "Raquel Urtasun",
                "Antonio Torralba",
                "Sanja Fidler."
            ],
            "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
            "venue": "The IEEE International Con-",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Document retrieval aims at retrieving the most relevant documents from a vast corpus in response to an input query (Guo et al., 2022), facilitating a myriad of applications such as web search (Xiao et al., 2022a) and question answering (Karpukhin et al., 2020). Recent works (Karpukhin et al., 2020; Xiong et al., 2020) generally first embed queries and documents into low-dimensional dense vectors, and then subsequently calculate their relevance based on these vectors, dubbed as dense retrieval. Due to their powerful capacity in modeling\n\u2217Work was done during Junhan\u2019s internship in MSRA \u2020Corresponding author\nthe intrinsic semantics, pre-trained language models (e.g., BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) and DeBERTa (He et al., 2020)) have attained a preeminent position in dense retrieval. Due to the high computational complexity (Vaswani et al., 2017; Lepikhin et al., 2020), most existing PLM-based retrieval models (Gao and Callan, 2021a,b; Liu and Shao, 2022) are designed for passage retrieval (passages are short texts generally no longer than 100 words (Karpukhin et al., 2020)). Nevertheless, long documents are ubiquitous in real life. For example, the average number of words and tokens in the documents of the MS MARCO dataset (Nguyen et al., 2016) are 1,165.46 and 1,631.30 respectively, significantly larger than the maximum input length (e.g. 512) of passage retrieval. In this paper, we aim to investigate the crucial task of long document retrieval, which is challenging due to the following three reasons. (1) Substantial computational cost. One straightforward solution is to directly employ short passage retrievers on long documents. Such approaches suffer from the rapidly growing computational costs due to the quadratic time complexity of vanilla transformers (O(L2d), where L is the input sequence length, and d denotes the dimension of latent embeddings). Truncating the documents into short passages is a common workaround, which may lead to the potential information loss (Karpukhin et al., 2020; Zhan et al., 2021b). (2) Incomprehensive document understanding. Another typical method is to employ efficient transformers, such as sparse attention transformers (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020) and hierarchical transformers (Zhang et al., 2019; Lin et al., 2021; Wu et al., 2021; Tian et al., 2023). Such approaches contribute to reduc-\ning the computational cost while suffering from severe incomprehensive document understanding. For instance, sparse attention transformers sparsify the full attention via masking, in which several virtual tokens are proposed as the global tokens attended by all tokens (Beltagy et al., 2020; Zaheer et al., 2020). After aggregating the information from all tokens, these global tokens tend to be overloaded, leading to the information mess (Child et al., 2019; Li et al., 2022). In addition, the popular heuristic masking strategy (e.g., random masking) may further aggravate the risk of information loss. The hierarchical transformers usually first split the document into short blocks. The semantics of different blocks are modeled independently, which are further fed into a readout layer in a cascaded manner. Semantics from different blocks are loosely coupled as a token can only attend to the other tokens in the same block and rich longrange context information is largely ignored. Such a loosely-coupled paradigm might be insufficient in modeling the sophisticated cross-block relations and thus cannot learn the comprehensive document representations.\n(3) Scarce annotations. Existing dense retrieval models generally rely on annotations to fine-tune the PLMs. The scarcity of annotations is further exacerbated in long document retrieval. Compared to short passage retrieval, more training signals are indispensable to achieve an accurate understanding of long documents. However, manually labeled annotations are usually expensive and time-consuming. Hence, the elaborate unsupervised training signals are expected to facilitate the modeling of long documents and alleviate the challenge of annotation scarcity in the fine-tuning phase.\nIn this paper, we propose a novel dense retrieval model for long documents, dubbed as Longtriever. Longtriever follows the hierarchical paradigm, in which the long document is split into multiple short blocks to ensure the model\u2019s efficiency. Each layer of Longtriever consists of two core modules: the intra-block encoder to convey messages among tokens belonging to the same block, and the inter-block encoder to pass information across different blocks. After iteratively stacking multiple Longtriever layers, the tokens in a block are capable of attending to tokens in other blocks, resulting in a tightly-coupled paradigm. Longtriever models the local semantics within a single block and the global context correlations across multiple blocks\nsimultaneously under a desirable time complexity. Different from previous fine-tuning based methods (Wu et al., 2021; Li et al., 2020; Xiong et al., 2020), here we further design a pre-training phase to gain a better understanding of inherent semantics within the long documents. A novel pre-training task, local masked autoencoder (LMAE), is proposed to mine the intrinsic semantics by reconstructing the raw input of each block based on the global document representations, the block representations, and the original context tokens. The pre-training phase empowers Longtriever with the capability of capturing the unsupervised semantic correlations and contributes to alleviating the reliance on the annotations. Longtriever is extensively evaluated over two popular benchmark datasets, and the experimental results demonstrate the superiority of our proposal."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Dense Retrieval",
            "text": "In the field of dense retrieval models, a bi-encoder architecture is typically employed to separately encode queries and documents, thereby improving search efficiency. This architectural design has found its use in a multitude of applications, such as search engines (Karpukhin et al., 2020), advertising (Lu et al., 2020), and recommendation systems (Xiao et al., 2022b). Karpukhin et al. (2020) illustrated that incorporating in-batch negatives during training could substantially boost the performance of dense retrieval models compared to the traditional BM25 model. Consequently, Xiong et al. (2020) introduced ANCE, proposing the utilization of approximate nearest neighbors as negative samples. To refine the selection of negative samples, Qu et al. (2020) presented RocketQA, which used a more precise cross-encoder. Zhang et al. (2021a) demonstrated the AR2 model, which concurrently trained the bi-encoder and cross-encoder.\nAdvancements in the quality of dense retrieval are largely attributed to the recent progress in pretrained language models (Karpukhin et al., 2020; Luan et al., 2021). A common approach entails deploying a universally pre-trained model as the bi-encoder. These models are usually pre-trained using one or multiple masked language modeling (MLM) tasks. For instance, BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) predict masked tokens based on their context. In Ernie (Sun et al., 2019) and Spanbert (Joshi et al., 2020),\nmasked entities and spans are predicted, leading to enhanced performance in entity typing and question answering tasks. However, these generic models, usually pre-trained on token-level tasks, might not effectively cultivate the ability to represent sentences (Chang et al., 2020).\nTo address this limitation, recent studies proposed two primary types of pre-training tasks. The first one is self-contrastive learning (SCL) (Wu et al., 2020; Zhang et al., 2021b; Yan et al., 2021; Liu et al., 2021; Gao et al., 2021), which trains the language models using relevant sentence pairs from an unlabeled corpus. The second is auto-encoding (AE) (Gao and Callan, 2021a; Lu et al., 2021; Liu and Shao, 2022; Pang et al., 2022), which primarily trains language models to reconstruct the input sentence based on the sentence embedding."
        },
        {
            "heading": "2.2 Efficient Transformers",
            "text": "Efficient transformers have recently gained substantial attention for their ability to model long documents. Three primary types of efficient transformers have been proposed: sparse attention transformers, hierarchical transformers, and recurrence transformers. Sparse attention transformers, such as Sparse Transformer (Child et al., 2019) and Reformer (Kitaev et al., 2020), sparsify the selfattention matrix to reduce computational costs. Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020), meanwhile, replace dense attention with a combination of random attention, window attention, and global attention.\nHierarchical transformers, significant for tasks such as document summarization (Zhang et al., 2019) and document ranking (Lin et al., 2021), typically divide a long document into shorter blocks and aggregate these block representations to produce the overall document representation. HIBERT\n(Zhang et al., 2019) and Hi-Transformer (Wu et al., 2021) are representative examples of this category.\nRecurrence transformers, generally used for generation tasks, also partition the document into short blocks and process them using a recurrence mechanism (Dai et al., 2019). Examples of such models are Transformer-XL and XLNet (Yang et al., 2019), which keep the recurrence mechanism and introduce a permutation language modeling objective to capture bidirectional contextual information. In response to the problem of individual blocks lacking contextual information, ERNIE-Doc (Ding et al., 2020) implements a retrospective feed mechanism, simulating human reading behavior."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Longtriever",
            "text": "Figure 1 illustrates the framework of the proposed Longtriever model. The input long document is tokenized into a sequence of tokens X = {x1, x2, \u00b7 \u00b7 \u00b7 , xL} via WordPiece tokenizer (Wu et al., 2016; Li et al., 2021). This token sequence is further split into a set of blocks {T1, T2, \u00b7 \u00b7 \u00b7 , TN} where Ti = {x(i\u22121)\u00d7M+1, \u00b7 \u00b7 \u00b7 , xi\u00d7M}. M denotes the maximum number of tokens within a single block, which is a pre-defined hyperparameter (e.g., 512). N = \u2308L/M\u2309 denotes the number of blocks. Each block is appended with a special token [CLS] as the representation of this entire block. In addition, a special token [DOC] is further padded in the front of the input text as the document representation. Assume hi \u2208 Rd\u00d71 denotes the representation of i-th token, sd \u2208 Rd\u00d71 is the document representation, and ci \u2208 Rd\u00d71 represents the [CLS] token of i-th block. Each layer of Longtriever consists of two major modules: the inter-block encoder to exchange information across\nblocks, and the intra-block encoder to convey messages between tokens within a single block. Next, we will introduce the details of the l-th Longtriever layer."
        },
        {
            "heading": "3.1.1 Inter-block encoder.",
            "text": "The inter-block encoder aims to depict the global semantics via collecting and dispatching messages for different blocks. The document representation s (l) d , and the [CLS] tokens c (l) i from all blocks are combined into an embedding matrix:\nB\u0302(l) \u2208 R(N+1)\u00d7d \u2190 [s(l)d , c (l) 1 , \u00b7 \u00b7 \u00b7 , c (l) N ]. (1)\nMatrix B\u0302(l) essentially preserves the global semantics of the entire long document. Next, a multihead transformer is employed on the matrix B\u0302(l) to transmit information across different blocks. For an arbitrary attention head, inter-block passing is formalized as:\nB\u0303(l) = softmax\n( Q (l) c K\n(l)\u22a4 c\u221a d\n) V(l)c , (2)\nwhere  Q (l) c = B\u0302(l)W (l) Qc, K (l) c = B\u0302(l)W (l) Kc,\nV (l) c = B\u0302(l)W (l) V c\n(3)\nin which matrices W(l)Qc,W (l) Kc,W (l) V c \u2208 Rd\u00d7d are learnable parameters. The information from different blocks is fused and exchanged via the selfattention mechanism of the inter-block encoder, ensuring the [CLS] token of a single block is capable of attending to the global semantics."
        },
        {
            "heading": "3.1.2 Intra-block encoder.",
            "text": "The embeddings of [CLS] tokens learned by the inter-block encoder are dispatched back into the corresponding blocks. Given a single block, the embedding matrix fed into the intra-block encoder is formally defined as follows:\nH\u0302(l) \u2208 R(M+1)\u00d7d \u2190 [c\u0303(l)i ,h (l) 1 , \u00b7 \u00b7 \u00b7 ,h (l) M ]. (4)\nin which c\u0303(l)i is the [CLS] embedding dispatched from matrix B\u0303(l) learned by the inter-block encoder. h (l) i represents the embedding of the i-th token of this block. Similar to the inter-block encoder, a multi-head transformer is applied on the matrix H\u0302(l) to conduct token-wise information propagation:\nH\u0303(l) = softmax\n( Q (l) e K\n(l)\u22a4 e\u221a d\n) V(l)e , (5)\nwhere  Q (l) e = H\u0302(l)W (l) Qe, K (l) e = H\u0302(l)W (l) Ke,\nV (l) e = H\u0302(l)W (l) V e\n(6)\nin which W(l)Qe,W (l) Ke,W (l) V e \u2208 Rd\u00d7d are learnable matrices. The [CLS] embedding c\u0303(l)i preserves the global signals from other blocks, which are incorporated to facilitate the modeling of tokens in the belonging block. Different from previous looselycoupled hierarchical transformers, the modeling of different blocks is tightly coupled together. Each token is capable of attending to tokens in other blocks with the [CLS] token as the intermediary, leading to a comprehensive understanding of the long document semantics. L Longtriever layers are stacked as the model architecture, and the embedding of [DOC] is output as the final document representation.\nFor the document retrieval task, Longtriever is utilized as both query encoder and document encoder. Following (Karpukhin et al., 2020; Gao and Callan, 2021a; Liu and Shao, 2022), the dot product is selected as the similarity metric. The relevance score between a query and a candidate document is calculated as:\nrelq,d = LT (q)LT (d) \u22a4 (7)\nwhere q is the query, d represents the document and function LT denotes the feed-forward process of Longtriever.\nA training sample is defined as < q, d+, d\u22121 , \u00b7 \u00b7 \u00b7 , d\u2212n >, in which d+ is the relevant (positive) candidate and d\u2212i is the irrelevant (negative) candidate. To ensure the training efficiency, here we adopt the in-batch negative sampling strategy (Karpukhin et al., 2020). The loss function of fine-tuning phase is formalized as the negative log-likelihood of the positive candidates:\nLr = \u2211 \u2212 log e relq,d+\nerelq,d+ + \u2211n\ni=1 e rel q,d\u2212 i\n. (8)\nThe time complexity of the inter-block encoder is O((N + 1)2d). The complexity of a single intrablock encoder is O((M + 1)2d). Since the input document is split into N blocks, the time cost of intra-block encoders is O((M + 1)2 \u2217 d \u2217 N). Overall, the time complexity of Longtriever is O(M2Nd + N2d), which is more efficient than the vanilla transformers O(M2N2d)."
        },
        {
            "heading": "3.2 Pre-training for long document retrieval",
            "text": "Conventional dense retrieval models solely rely on the annotations to fine-tune PLMs, which is unsuitable for long document retrieval since longer texts generally require more training signals. Thus, here we add a pre-training phase to encode the unsupervised semantic correlations into the proposed Longtriever model."
        },
        {
            "heading": "3.2.1 Masked Language Modeling",
            "text": "We adopt the vanilla masked language modeling (MLM) task to pre-train the Longtriever model. The input textX is randomly masked and the objective is defined to predict the masked tokens based on the last hidden states h(L)."
        },
        {
            "heading": "3.2.2 Local Masked Autoencoder",
            "text": "To address the annotation scarcity, we propose a novel pre-training task for long document retrieval called local masked autoencoder (LMAE). Shown as figure 2, LMAE first learns two types of representations: the global representation capturing the general semantics of the whole document, and the local representation preserving the specific semantics of the local block. A shallow decoder is further integrated into the Longtriever. LMAE aims to reconstruct the original input tokens via the decoder based on the global and local representations.\nIn Longtriever, the last hidden state of [DOC], s (L+1) d (denoted by s L\u2032 d ), is the global representation, and the last hidden states of [CLS] in each block, c\u0303(L+1)i (denoted by c\u0303 (L\u2032) i ), are the local representations. We utilize a transformer layer as our decoder. For each block Ti, we generate distinct query\nHQi \u2208 RM\u00d7d and key inputs HKi \u2208 R(M+2)\u00d7d for the multi-head attention layer within the transformer:\nHQi \u2190 [s (L\u2032) d + p(i\u22121)\u00d7M+1, ..., s (L\u2032) d + pi\u00d7M ];\nHKi \u2190 [s (L\u2032) d , c\u0303 (L\u2032) i ,h 0 (i\u22121)\u00d7M+1, ...,h 0 i\u00d7M ].\n(9) where p and h0 denote the positional embeddings and the orignial token embeddings. After that, a vanilla transformer is employed on the constructed matrices:\nHDeci = softmax ( QdiK\nd\u22a4 i\u221a d +A\n) Vdi , (10)\nwhere A is a random mask matrix and Qdi = H Q i WQd, Kdi = H K i WKd,\nVdi = H K i WV d\n(11)\nin which WQd,WKd,WV d \u2208 Rd\u00d7d are learnable matrices.\nFinally, the output hidden states of the decoder, denoted by HDeci , are processed by the token prediction head \u03c8, and the following objective is optimized:\nLLMAE = \u2211 xk\u2208X CE(xk|\u03c8(hDeck )) (12)\nwhere CE is the cross-entropy loss."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Datasets. In order to evaluate the performance of our proposal, extensive experiments are conducted on two popular document retrieval datasets: 1) MARCO Dev Doc (MS MARCO Document Ranking) (Nguyen et al., 2016) is a large-scale benchmark dataset for web document retrieval, comprising about 3 million documents, 0.4 million training queries, and 5 thousand development queries. 2) TREC 2019 Doc (TREC 2019 Document Ranking) (Craswell et al., 2020) is a test set in MS Marco document ranking task produced by TREC, consisting of 43 queries with more comprehensive labeling. We use the official metrics of these two benchmarks (Nguyen et al., 2016; Craswell et al., 2020). For the MS MARCO document ranking task, we report the mean reciprocal rank at 100 (MRR@100) and recall at 100 (R@100). For the TREC 2019 document ranking task, we report normalized discounted cumulative gain at 10 (NDCG@10) and recall at 100 (R@100). Longtriever is pre-trained on the BookCorpus (Zhu et al., 2015) and English Wikipedia (Devlin et al., 2018). Baselines. Multiple SOTA passage retrieval and document retrieval methods are selected as the baselines. BM25 (Robertson et al., 2009), DeepCT (Dai and Callan, 2019) and TRECTrad (Craswell et al., 2020) are classic sparse document retrievers. JPQ (Zhan et al., 2021a) and RepCONC (Zhan\net al., 2022) are recent vector compression methods. ANCE (Xiong et al., 2020), STAR (Zhan et al., 2021b), and ADORE (Zhan et al., 2021b) are complicated fine-tuning methods to enhance dense retrievers.\nSeveral popular PLMs are also introduced as baselines: 1) BERT (Devlin et al., 2018) is the most popular pre-trained language model in NLP tasks. 2) RetroMAE (Liu and Shao, 2022) is a pre-training paradigm that focuses on dense passage retrieval tasks. It utilizes the masked autoencoder technique and exhibits remarkable performance in these types of tasks. 3) Parade (Li et al., 2020) is a method for aggregating passage representations into a document representation. It employs a transformer layer to achieve this aggregation. 4) Longformer (Beltagy et al., 2020) uses a technique called \"local attention\", which allows the model to process much longer sequences than traditional transformer models. 5) BigBird (Zaheer et al., 2020) is an efficient transformer model with several sparse attention mechanisms. 6) HiTransformer (Wu et al., 2021) is a hierarchical interactive transformer for efficient long document modeling. 7) XLNet (Yang et al., 2019) is an extension of Transformer-XL (Dai et al., 2019), pre-trained using an autoregressive method to learn bidirectional contexts. It utilizes the recurrent memory mechanism to handle long text.\nImplementation details. Longtriever employs 24 transformer layers, 12 layers as intra-block en-\ncoders, and 12 layers as inter-block encoders. The dimension of the hidden states is 768, and the vocabulary size is 30,522. The masking ratios for the masked language modeling (MLM) and local masked autoencoder (LMAE) are 30% and 50%. The maximum text length for each block is 512, and the maximum number of blocks is 8. The model is continuously pre-trained from the BERT checkpoint on 8\u00d7 NVIDIA A100 (40GB) GPUs for 8 epochs with a batch size of 3 (per device), which takes about 3 days. We use the AdamW (Loshchilov and Hutter, 2017) as the optimizer. The peak learning rate is set to 1e-4, with linear warmup over 0.1 ratio and linear decay. The weight decay is set to 0.01."
        },
        {
            "heading": "4.2 Main Results",
            "text": "Following previous work (Ma et al., 2022), Longtriever is first fine-tuned on the MS MARCO Passage Ranking dataset and subsequently on the MS MARCO Document Ranking dataset. Each fine-tuning stage includes three iterations: one iteration with BM25 negatives, and two iterations\nwith hard negatives. Longtriever is trained five times on each dataset, and the average performance is reported. Table 1 presents the retrieval performance of Longtriever and SOTA models. One can clearly see that Longtriever consistently achieves the best performance on all metrics over all datasets. Specifically, Longtriever surpasses the strongest baselines by +2.84% (MRR@100) and +2.29% (Recall@100) on MARCO Dev Doc, and by +2.71% (NDCG@10) and +11.25% (Recall@100) on TREC 2019 Doc. By enjoying the merits of nested inter-block and intra-block aggregations, Longtriever is capable of precisely modeling the semantics within each block, and comprehensively capturing the global semantic correlations between different blocks, leading to superior performance.\nIn order to further investigate the superiority of Longtriever on long document retrieval, we also present the results of SOTA pre-trained models in Table 2. All models are fine-tuned using in-batch negatives for a single iteration. The notation \"*- Passage\" indicates that the model takes the first 512 tokens of the text as the input (Ma et al., 2022). The results clearly demonstrate that Longtriever consistently achieves superior performance on both datasets.\nThe time and memory costs of various models are evaluated on a single NVIDIA V100 GPU with 32GB memory. To ensure a fair comparison, all models are given a batch of 16 documents as input, each document comprised of 2048 tokens or 4 blocks of 512 length. BERT-Passage is modified to BERT-Document for this comparison (The input length is expanded from 512 to 2048). RetroMAE is omitted, as it shares the same backbone with\nBERT. The table 3 shows: 1) The time cost analysis reveals that BERT-PARADE, Hi-Transformer, and Longtriever are the fastest models, taking less than 700ms to process a batch, due to their efficient hierarchical architectures. On the other hand, BERT-Document, Longformer, and BigBird take approximately 1000ms due to their similar attention strategies. XLNet is the slowest model with over 2000ms, due to the low parallelism of its recurrent architecture. 2) The memory cost analysis shows that BERT-PARADE, Hi-Transformer, and XLNet consume no more than 4 GiB. Longtriever is slightly higher, as it concatenates an extra token at the beginning of each block in every intrablock encoder layer. As the attention matrix becomes denser, Longformer, BigBird, and BERTDocument consume more memory in that order. Based on the above analysis, it can be concluded that Longtriever provides an appropriate trade-off between performances and costs."
        },
        {
            "heading": "4.3 Ablation Study",
            "text": "Table 4 presents the results of the Longtriever model on the MARCO Dev Doc after removing different components. Various components are gradually removed from Longtriever. In Experiment II, the model is pre-trained with only the MLM task. In Experiment III, the model is fine-tuned without continuous pre-training (just the BERT checkpoint is loaded). In Experiment IV, the [DOC] token is removed, and mean pooling is used to aggregate the [CLS] hidden states of each block to obtain the document representation.\nBased on the results of Experiments I and III, the proposed pre-training phase is quite useful, yielding 17.50% and 5.68% improvements on MRR@100 and Recall@100. The scarcity of annotations has been effectively alleviated. Moreover, comparing the results of Experiments I and II, one can see that the LMAE task brings a 7.17% improvement on MRR@100 and 4.81% on Recall@100. It means that Longtriver is able to cap-\nture the general semantics of the whole document through the LMAE task. After removing the [DOC] token, the model\u2019s performance presents a decline in Experiment III, which reveals that to generate high-quality document representation, an information collector among blocks is helpful. In addition, the inter-block encoders are also essential for facilitating interactions between different blocks as shown in the comparison between Experiments IV and V. Therefore, integrating inter-block interactions in the modeling of document representation is crucial."
        },
        {
            "heading": "4.4 Hyperparameter Study",
            "text": "The influence of block size and LMAE\u2019s masking ratio on the performance of Longtriever is depicted in Figure 3. By maintaining a maximum input length of 2,048, the block size is increased from 64 to 512. One can see that model performance consistently increases with larger block sizes. This is reasonable as a larger block size brings more rich intra-block correlations while suffering from inferior model efficiency. We also increase LMAE\u2019s masking ratio from 0.1 to 0.9, while MLM\u2019s masking ratio is kept at 0.3. The performance improves with LMAE\u2019s masking ratio and reaches a peak at\n0.5. After that, the performance declines, which demonstrates that a proper masking ratio of LMAE is suitable for Longtriever\u2019s pre-training, but a tooaggressive masking ratio harms Longtriever\u2019s ability to represent the entire document."
        },
        {
            "heading": "5 Conclusion",
            "text": "We propose Longtriever, a novel dense retrieval model for long documents. Longtriever effectively incorporates both local and global semantic modeling, while maintaining a desirable time complexity. Besides, Longtriever is pre-trained with a novel task LMAE to gain a better understanding of inherent semantics within the long documents. Experimental results demonstrate that Longtriever consistently outperforms existing retrieval methods on various document retrieval datasets.\nLimitations\nOne limitation of our current study is the unexamined performance of the BERT architecture when utilized as a document encoder (BERT-Document). This deficiency is attributed to the substantial GPU memory requirements integral to the fine-tuning process. Our future work endeavours aim to extend the input length of BERT to 2048, which is anticipated to function as a referential upper bound to inform and optimize our methodological approach."
        }
    ],
    "title": "Longtriever: a Pre-trained Long Text Encoder for Dense Document Retrieval",
    "year": 2023
}