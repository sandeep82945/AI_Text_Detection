{
    "abstractText": "Document-level Relation Extraction (DocRE) intends to extract relationships from documents. Some works introduce logic constraints into DocRE, addressing the issues of opacity and weak logic in original DocRE models. However, they only focus on forward logic constraints and the rules mined in these works often suffer from pseudo rules with high standardconfidence but low support. In this paper, we proposes Bidirectional Constraints of Beta Rules(BCBR), a novel logic constraint framework. BCBR first introduces a new rule miner which model rules by beta contribtion. Then forward and reverse logic constraints are constructed based on beta rules. Finally, BCBR reconstruct rule consistency loss by bidirectional constraints to regulate the output of the DocRE model. Experiments show that BCBR outperforms original DocRE models on relation extraction performance (\u223c2.7 F1) and logic consistency(\u223c3.1 Logic). Furthermore, BCBR consistently outperforms two other logic constraint frameworks. Our code is available at https://github.com/Louisliu1999/BCBR.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yichun Liu"
        },
        {
            "affiliations": [],
            "name": "Zizhong Zhu"
        },
        {
            "affiliations": [],
            "name": "Xiaowang Zhang"
        },
        {
            "affiliations": [],
            "name": "Zhiyong Feng"
        },
        {
            "affiliations": [],
            "name": "Daoqi Chen"
        },
        {
            "affiliations": [],
            "name": "Yaxin Li"
        }
    ],
    "id": "SP:1e9888f690b66b7ab928796f851572508f7e53f4",
    "references": [
        {
            "authors": [
                "Fenia Christopoulou",
                "Makoto Miwa",
                "Sophia Ananiadou."
            ],
            "title": "Connecting the dots: Document-level neural relation extraction with edge-oriented graphs",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
            "year": 2019
        },
        {
            "authors": [
                "William W. Cohen."
            ],
            "title": "Tensorlog: A differentiable deductive database",
            "venue": "CoRR, abs/1605.06523.",
            "year": 2016
        },
        {
            "authors": [
                "Shengda Fan",
                "Shasha Mo",
                "Jianwei Niu."
            ],
            "title": "Boosting document-level relation extraction by mining and injecting logical rules",
            "venue": "Proceedings of",
            "year": 2022
        },
        {
            "authors": [
                "Luis Gal\u00e1rraga",
                "Christina Teflioudi",
                "Katja Hose",
                "Fabian M. Suchanek."
            ],
            "title": "Fast rule mining in ontological knowledge bases with AMIE+",
            "venue": "VLDB J., 24(6):707\u2013730.",
            "year": 2015
        },
        {
            "authors": [
                "Xu Han",
                "Pengfei Yu",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Peng Li."
            ],
            "title": "Hierarchical relation extraction with coarse-to-fine grained attention",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, Octo-",
            "year": 2018
        },
        {
            "authors": [
                "Youmi Ma",
                "An Wang",
                "Naoaki Okazaki."
            ],
            "title": "DREEAM: guiding attention with evidence for improving document-level relation extraction",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Christian Meilicke",
                "Melisachew Wudage Chekol",
                "Daniel Ruffinelli",
                "Heiner Stuckenschmidt."
            ],
            "title": "Anytime bottom-up rule learning for knowledge graph completion",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelli-",
            "year": 2019
        },
        {
            "authors": [
                "Dat Quoc Nguyen",
                "Karin Verspoor."
            ],
            "title": "Convolutional neural networks for chemical-disease relation extraction are improved with character-based word embeddings",
            "venue": "Proceedings of the BioNLP 2018 workshop, Melbourne, Australia, July 19, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Pouya Ghiasnezhad Omran",
                "Kewen Wang",
                "Zhe Wang."
            ],
            "title": "An embedding-based approach to rule learning in knowledge graphs",
            "venue": "IEEE Trans. Knowl. Data Eng., 33(4):1348\u20131359.",
            "year": 2021
        },
        {
            "authors": [
                "Dongyu Ru",
                "Changzhi Sun",
                "Jiangtao Feng",
                "Lin Qiu",
                "Hao Zhou",
                "Weinan Zhang",
                "Yong Yu",
                "Lei Li."
            ],
            "title": "Learning logic rules for document-level relation extraction",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Ali Sadeghian",
                "Mohammadreza Armandpour",
                "Patrick Ding",
                "Daisy Zhe Wang."
            ],
            "title": "DRUM: end-toend differentiable rule mining on knowledge graphs",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information",
            "year": 2019
        },
        {
            "authors": [
                "Qingyu Tan",
                "Lu Xu",
                "Lidong Bing",
                "Hwee Tou Ng."
            ],
            "title": "Revisiting docred - addressing the overlooked false negative problem in relation extraction",
            "venue": "CoRR, abs/2205.12696.",
            "year": 2022
        },
        {
            "authors": [
                "Difeng Wang",
                "Wei Hu",
                "Ermei Cao",
                "Weijian Sun."
            ],
            "title": "Global-to-local neural networks for documentlevel relation extraction",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November",
            "year": 2020
        },
        {
            "authors": [
                "Hong Wang",
                "Christfried Focke",
                "Rob Sylvester",
                "Nilesh Mishra",
                "William Yang Wang."
            ],
            "title": "Finetune bert for docred with two-step process",
            "venue": "CoRR, abs/1909.11898.",
            "year": 2019
        },
        {
            "authors": [
                "Yijun Wang",
                "Changzhi Sun",
                "Yuanbin Wu",
                "Hao Zhou",
                "Lei Li",
                "Junchi Yan."
            ],
            "title": "ENPAR: enhancing entity and entity pair representations for joint entity relation extraction",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Benfeng Xu",
                "Quan Wang",
                "Yajuan Lyu",
                "Yong Zhu",
                "Zhendong Mao."
            ],
            "title": "Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction",
            "venue": "CoRR, abs/2102.10249.",
            "year": 2021
        },
        {
            "authors": [
                "Benfeng Xu",
                "Quan Wang",
                "Yajuan Lyu",
                "Yong Zhu",
                "Zhendong Mao."
            ],
            "title": "Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI",
            "year": 2021
        },
        {
            "authors": [
                "Zezhong Xu",
                "Peng Ye",
                "Hui Chen",
                "Meng Zhao",
                "Huajun Chen",
                "Wen Zhang."
            ],
            "title": "Ruleformer: Contextaware differentiable rule mining over knowledge graph",
            "venue": "CoRR, abs/2209.05815.",
            "year": 2022
        },
        {
            "authors": [
                "Yuan Yao",
                "Deming Ye",
                "Peng Li",
                "Xu Han",
                "Yankai Lin",
                "Zhenghao Liu",
                "Zhiyuan Liu",
                "Lixin Huang",
                "Jie Zhou",
                "Maosong Sun"
            ],
            "title": "Docred: A large-scale",
            "year": 2019
        },
        {
            "authors": [
                "Deming Ye",
                "Yankai Lin",
                "Jiaju Du",
                "Zhenghao Liu",
                "Peng Li",
                "Maosong Sun",
                "Zhiyuan Liu."
            ],
            "title": "Coreferential reasoning learning for language representation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Klim Zaporojets",
                "Johannes Deleu",
                "Chris Develder",
                "Thomas Demeester."
            ],
            "title": "DWIE: an entity-centric dataset for multi-task document-level information extraction",
            "venue": "Inf. Process. Manag., 58(4):102563.",
            "year": 2021
        },
        {
            "authors": [
                "Daojian Zeng",
                "Kang Liu",
                "Siwei Lai",
                "Guangyou Zhou",
                "Jun Zhao."
            ],
            "title": "Relation classification via convolutional deep neural network",
            "venue": "COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical",
            "year": 2014
        },
        {
            "authors": [
                "Shuang Zeng",
                "Runxin Xu",
                "Baobao Chang",
                "Lei Li."
            ],
            "title": "Double graph based reasoning for documentlevel relation extraction",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November",
            "year": 2020
        },
        {
            "authors": [
                "Wen Zhang",
                "Bibek Paudel",
                "Liang Wang",
                "Jiaoyan Chen",
                "Hai Zhu",
                "Wei Zhang",
                "Abraham Bernstein",
                "Huajun Chen."
            ],
            "title": "Iteratively learning embeddings and rules for knowledge graph reasoning",
            "venue": "The World Wide Web Conference, WWW 2019, San Francisco,",
            "year": 2019
        },
        {
            "authors": [
                "Yuhao Zhang",
                "Victor Zhong",
                "Danqi Chen",
                "Gabor Angeli",
                "Christopher D. Manning."
            ],
            "title": "Position-aware attention and supervised data improve slot filling",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP",
            "year": 2017
        },
        {
            "authors": [
                "Wei Zheng",
                "Hongfei Lin",
                "Zhiheng Li",
                "Xiaoxia Liu",
                "Zhengguang Li",
                "Bo Xu",
                "Yijia Zhang",
                "Zhihao Yang",
                "Jian Wang."
            ],
            "title": "An effective neural model extracting document level chemical-induced disease relations from biomedical literature",
            "venue": "J. Biomed. In-",
            "year": 2018
        },
        {
            "authors": [
                "Wenxuan Zhou",
                "Kevin Huang",
                "Tengyu Ma",
                "Jing Huang."
            ],
            "title": "Document-level relation extraction with adaptive thresholding and localized context pooling",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, DocRE attracts significant attention from researchers, with its intention to distinguish the relations between entity pairs in the documents. It\u2019s not limited to sentence-level relation extraction (Zeng et al., 2014; Zhang et al., 2017; Han et al., 2018; Wang et al., 2021). It aims to uncover the dependencies between entities in different sentences of one document (Zhou et al., 2021; Ma et al., 2023). The challenges in DocRE mainly include two aspects: first, it\u2019s difficult to capture complex long-range dependencies between entity pairs in documents; second, it\u2019s prone to errors in logical reasoning due to lack of logic. To address the aforementioned challenges, the academic community has made a lot of efforts.\n\u2217 These authors contributed equally. \u2020 Corresponding authors\nBased on the use of rules, we can classify previous DocRE works into two categories: plain DocRE models and logic constraint DocRE models. In plain DocRE models, attention is mainly given to learning more powerful implicit representations. These methods include models based on sequence encoders (Wang et al., 2019; Xu et al., 2021a; Zhou et al., 2021), and models based on graph encoders (Zeng et al., 2020; Christopoulou et al., 2019). Although these methods have achieved decent results, their inferences are non-transparent and lack logic, making them prone to errors in logical inference. Meanwhile, the combination of relation extraction frameworks and logical rules has alleviated the issues of low transparency and weak logic. As the Figure 1 shows, we can derive two relations based_in0 (Porsche,Argentina) and gpe0 (Argentina,Argentine) from the document. When the rule is applied,we can predict the relation based_in0-x between Porsche and Argentina. LogiRE(Ru et al., 2021) is the first work that introduces logical rules into\ndocument-level relation extraction. It employed the Expectation-Maximization (EM) algorithm to iteratively update the rule generator and relation extractor, optimizing the results of relation extraction. However, rule generator and relation extractor are in isolation. The EM algorithm enables joint optimization of the two models, but it can still lead to suboptimal results. MILR(Fan et al., 2022) addresses the issue of suboptimality by jointly training the relation classification and rule consistency losses. But MILR utilizes confidence-based methods to mine rules, which can lead to pseudo rules with high standard-confidence but low support, affecting the effectiveness of relation extraction.\nIn the paper, we propose BCBR, a novel framework which assists relation extraction with the help of logical rules. BCBR models the bidirectional constraints of beta rules and optimizes relation extraction through rule consistency loss. (1) The prior rule set derived from the documents is different from the one extracted from the knowledge graph. Textual data has a relatively small volume, and the inter-document correlations are low, leading to sparsity in the prior rule set. Thus, general rule miners in previous works cause the prevalence of pseudo rules with high standard-confidence but low support. To tackle this problem, we utilize the beta distribution to model the rules and consider both their successful predictions and failures to filter the rules. (2) In addition, we discovered that the constraints between rule head and rule body are bidirectional, while previous methods often only considered the forward constraints from rule body to rule head. Therefore, we introduce reverse constraints from rule head to rule body. (3) Finally, based on above constraints, we reconstruct the rule consistency loss to enhance the performance of the original DocRE models. We summarize our contributions as follows:\n\u2022 To our knowledge, we first propose a rule miner that utilizes the Beta distribution to model rules.\n\u2022 We introduce reverse logic constraint to ensure that the output of DocRE models satisfies the necessity of rules.\n\u2022 We model bidirectional logic constraints as reasonable probability patterns and turn them into rule consistency loss.\n\u2022 Our experiments demonstrate that BCBR sur-\npasses LogiRE and MILR in terms of both relation extraction performance and logic consistency."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Rule Learning",
            "text": "Rule learning is the foundation of logic constraint DocRE. Rule learning is primarily applied in the field of knowledge graphs, but DocRE can draw on its ideas. Currently, rule learning methods can be divided into three types: symbol-based rule learning, embedding-based rule learning, and differentiable rule learning based on TensorLog (Cohen, 2016). Symbol-based rule learning aims to mine rule paths of high frequency on knowledge graphs more efficiently. Gal\u00e1rraga et al. proposes the openworld assumption and utilizes pruning strategy to mine rules. Meilicke et al. adopts a bottom-up rule generation approach. Embedding-based rule learning focuses on learning more powerful embeddings for entities and relations. Omran et al. calculates the similarity between rule head and rule body to select better rules. Zhang et al. iteratively learns embeddings and rules to generate the rule set. TensorLog-based methods transform the rule learning process into a differentiable process, allowing neural network models to generate rules. For example, Sadeghian et al.; Sadeghian et al. trains a rule miner by using bidirectional RNN model, and Xu et al.utilizes transformer model."
        },
        {
            "heading": "2.2 Document-level Relation Extraction",
            "text": "Previous works on DocRE can be divided into two categories: plain DocRE and logic constraint DocRE. Plain document-level relation extraction focuses on learning more powerful representations (Zheng et al., 2018; Nguyen and Verspoor, 2018). There are methods based on sequence models that introduce pre-trained models to generate better representations (Wang et al., 2019; Ye et al., 2020; Xu et al., 2021b), Zhou et al. sets an adaptive threshold and uses attention to guide context pooling. Ma et al. uses evidence information as a supervisory signal to guide attention modules. Graph-based methods model entities and relations as nodes and edges in a graph and use graph algorithms to generate better representations (Zeng et al., 2020; Wang et al., 2020).\nHowever, previous works lack transparency and logic, making them prone to errors in logical inference. Currently, research on logic constraint\ndocument-level relation extraction is limited. There are two noteworthy works in this area: LogiRE(Ru et al., 2021)and MILR (Fan et al., 2022). LogiRE involves two modules, the rule generator and the relation extractor. It uses the EM algorithm to efficiently maximize the overall likelihood. But this method often leads to suboptimal results due to the isolation between rule generator and relation extractor. To address this limitation, MILR constructs a joint training framework that combines rule consistency loss and relation classification loss of the backbone model. Previous works only applied forward logic constraints, while our works introduce reverse logic constraints and enhance the result of backbone model."
        },
        {
            "heading": "3 Method",
            "text": "In this chapter, we introduce our framework \u2013 Bidirectional Constraints of Beta Rules(BCBR) (Fig.2). We define concepts related to DocRE and rules(Sec.3.1). Then We propose a novel rule extraction method(Sec.3.2) and model bidirectional logic constraints based on rules(Sec.3.3). Finally, we construct rule consistency loss and jointly train with relation classification loss to enhance relation extraction performance(Sec.3.4)."
        },
        {
            "heading": "3.1 Preliminaries",
            "text": "Document-level Relation Extraction Given a documentD and entities E = {ei}n1 . Entities constitute entity pairs (eh, et)1\u2264h,t\u2264n,h \u0338=t, which eh and et indicate the head entity and tail entity, respectively. The task of DocRE is to distinguish the relation r between each entity pair (eh, et), where r \u2208 R andR = R\u222a{NA}. R is a set of relations and NA indicates there is no relation on the entity pair.\nLogic Rules We define a binary variable r (eh, et) to indicate the existence of r \u2208 R between eh and et. When r is true, r (eh, et) = 1; otherwise, r (eh, et) = 0. A rule consists of rule head and rule body. The rule head is denoted as rhead (e0, el), and the rule body is defined as the conjunction of l binary variables, denoted as rbody (e0, el). We define the rule set as S and the pattern of rules is as follows:\nrhead (e0, el)\u2190 r1 (e0, e1)\u2227 ...\u2227rl (el\u22121, el) (1)\nwhere ei \u2208 E , l represents the rule length, and rhead (eh, et) and ri (ei\u22121, ei) are referred to as the head atom and the body atom, respectively.\nOn the basis of Closed World Assumption(CWA)(Gal\u00e1rraga et al., 2015), we introduce two concepts: standard confidence and head coverage. Standard confidence refers to the conditional probability of rule head being satisfied given that the rule body is satisfied. The standard confidence of rule s can be modeled as the following conditional probability distribution:\npsc (s) = C (rhead \u2227 rbody) C (rbody)\n(2)\nC (\u00b7) represents a counter. Head coverage refers to the conditional probability of rule body being satisfied given that the rule head is satisfied. The head coverage of rule s can be modeled as the following conditional probability distribution:\nphc (s) = C (rhead \u2227 rbody) C (rhead)\n(3)\nBackbone Model Paradigm Our approach involves using logic rules to assist the original\nDocRE model, which can be generalized to any backbone DocRE model. Therefore, we define the paradigm of the backbone model here. For all entity pairs (eh, et) in the document, the backbone model generates a score G (eh, et) for their relation r. The probability of this triple being true is defined as follows:\nP (r | eh, et) = \u03c3 (G (eh, et)) (4)\nwhere \u03c3 (\u00b7) is a sigmoid function. During training, the backbone model uses binary cross-entropy loss or adaptive threshold loss to compute the relation classification loss Lcls. During inference, the model sets a global threshold or uses a learned adaptive threshold to determine whether the triple r (eh, et) holds:\n\u03c1r (eh, et) = I (P (r | eh, et) > \u03b8) (5)\nwhere I (\u00b7) represents the indicator function, and \u03b8 represents a threshold. If the probability of the triple being true satisfies the threshold, then \u03c1r (eh, et) = 1, indicating that the triple r (eh, et) holds. Conversely, if the probability does not satisfy the threshold, then \u03c1r (eh, et) = 0, indicating that the triple r (eh, et) does not hold."
        },
        {
            "heading": "3.2 Beta Rule Miner",
            "text": "Rule mining methods on knowledge graphs are mainly based on the large-scale and data-intensive essence of knowledge graphs. However, when these methods are transferred to document data (Ru et al., 2021; Fan et al., 2022), they still rely on confidence to filter rules. It leads to a inadaptable phenomenon that there are massive pseudo rules with high standard-confidence but low support. Therefore, we abandon the approach of using confidence or support alone and instead use the Beta distribution to model rules. In this section, we propose a new rule mining method called beta rule miner.\nThe Beta distribution Beta(\u03b1s, \u03b2s) for rule s has two parameters, which we set as follows:\n\u03b1s = C (\u03c6 (s) = 1) + 1 (6a) \u03b2s = C (\u03c6 (s) = 0) + 1 (6b)\nwhere \u03c6(s) represents whether rule s holds. Taking the example of mining high-confidence rules, if both rbody and rhead exist for rule s, then \u03c6(s) = 1, indicating that the rule holds. Conversely, if rbody exists for rule s but rhead does not, then \u03c6(s) = 0,\nAlgorithm 1 Beta Rule Miner Input: training set\u2019s labels : T , rule template set\ngenerated by labels : Stemplate, lower bound of integration for Beta distribution : k, rule fitness threshold : \u03b7 Output: high quality rules : S 1: S \u2190 {} 2: for s in Stemplate do 3: \u03b1s = 1 4: \u03b2s = 1 5: for TD in T do 6: if rbody (e0, el) in TD and rhead (e0, el) in\nTD then 7: \u03b1s += 1 8: else if rbody (e0, el) in TD and rhead (e0, el) not in TD then 9: \u03b2s += 1\n10: end if 11: end for 12: \u03c1s = \u03a0(Ps (x > k) > \u03b7) 13: if \u03c1s == 1 then 14: S.add(s) 15: end if 16: end for 17: return S\nindicating that the rule does not hold. The probability density function of the Beta distribution for rule s is given by:\nfs (x;\u03b1s, \u03b2s) = x\u03b1s\u22121 (1\u2212 x)\u03b2s\u22121\nB (\u03b1s, \u03b2s) (7)\nwhere x \u2208 [0, 1] and B(\u00b7) represents the beta function. Next, we calculate the integration of the Beta distribution for rule s (rule fitness). It determines whether rule s is a high-quality rule or not.\nPs (x > k) = \u222b 1 k fs (x;\u03b1s, \u03b2s) dx (8a) \u03c1s = I (Ps (x > k) > \u03b7) (8b)\nwhere k is the lower bound of integration for Beta distribution and \u03b7 is the threshold for rule fitness. We select s as a high-quality rule when the integration of its Beta distribution satisfies the threshold.\nAs shown in Algorithm 1, we summarize how to extract high quality rules. For each rule s, we calculates \u03b1s and \u03b2s (lines 5-11). Then we computes Ps(x > k) using equations (7) and (8a) (line 12) and add high standard-confidence rules to the rule set using equation (8b) (lines 13-17)."
        },
        {
            "heading": "3.3 Bidirectional logic constraints",
            "text": "We utilize the above rules to impose constraints on the DocRE task. However, previous methods only employed forward logic constraints from rbody to rhead(Fan et al., 2022). They could not leverage the reverse logic constraints from rhead to rbody due to the uncertainty of rule body atoms. BCBR models the reverse logic constraints based on headcoverage rules, thereby compensating for the loss of constraint conditions. Below, we provide a detailed explanation of the modeling process for bidirectional logic constraints: Forward logic constraints Forward logic constraints exist in high standard-confidence rules. As shown in Equation (2), when rbody occurs and rhead simultaneously occurs, it is considered to satisfy the forward logic constraint. Conversely, if rhead does not occur, it is considered not to satisfy the forward logic constraint. It represents the sufficiency of rbody for rhead. We model the ideal form of forward constraints as follows:\nP (rhead (e0, el)) \u2265 bconf \u2217min (P (ri (ei, ei+1) )) , if min (P (ri (ei, ei+1))) > \u03b8 (9) where bconf represents the rule fitness of high standard-confidence rules, l denotes the length of the rule, and \u03b8 is a threshold. Forward constraints are only generated in high standard-confidence rules. When the score of the weakest body atom ri in rbody is greater than \u03b8, the forward constraint of the rule comes into play. It constrains that the probability of rhead being present is greater than the probability of ri being present. Reverse logic constraints Reverse constraints exist in high head-coverage rules. As shown in the probability model in Equation (3), when rhead is present, rbody is also expected to be present. It is referred to as satisfying the reverse constraint. Conversely, if rbody is not present, it is considered as not satisfying the reverse constraint. It represents the necessity of rbody for rhead. The reverse constraint is formulated as shown in Equation (10a). Reverse constraint differs from the rule form of forward constraint shown in Equation (1), as it derives rbody from rhead. rbody contains multiple uncertain body atoms because the entities connecting the triple may not exist. But conjunction rules require to consider each triple in constructing the constraint probability model. So We use De Morgan\u2019s laws for (10a) and obtain a disjunctive rule as shown in\nEquation (10b), which states that if any body atom does not exist, then rhead does not exist.\nrhead (e0, el)\u2192 r1 (e0, e1) \u2227 ... \u2227 rl (el\u22121, el) (10a)\n\u00acrhead (e0, el)\u2190 \u00acr1 (e0, e1) \u2228 ... \u2228 \u00acrl (el\u22121, el) (10b)\nWe model the ideal probability form of the reverse constraint as Equation (11):\nP (rhead (e0, el)) \u2264 bhead \u2217min (P (ri (ei, ei+1) )) , if min (P (ri (ei, ei+1))) < \u03b8 (11) where bhead represents the rule fitness of high headcoverage rules. The inverse constraint is only generated in high head-coverage rules. When the score of weakest body atom ri in rbody is less than \u03b8, the reverse constraint of the rule comes into effect. It constrains that the existence probability of rhead must be less than the existence probability of ri."
        },
        {
            "heading": "3.4 Rule consistency loss",
            "text": "In addition to the original relation classification loss Lcls of backbone models, we construct a rule consistency loss based on the bidirectional constraints of beta rules. This loss is jointly trained with Lcls to improve the logical consistency and performance of relation extraction.\nThe rule consistency loss is derived from the bidirectional constraints of beta rules and consists of two parts: forward loss generated by high standardconfidence rules and reverse loss generated by high head-coverage rules. The loss function is formulated as shown in equations (12a) and (12b).\nLsc = \u2211 s\u2208Ssc \u2211 d\u2208D max (0, (log (bconf ) + log (min\n(P (ri | ei\u22121, ei)))\u2212 log (P (rhead | e0, el)))) \u2217 \u03c1rmin (eh, et) (12a)\nLhc = \u2211 s\u2208Ssc \u2211 d\u2208D max (0,\u2212 (log (bhead)\u2212 log (min\n(P (ri | ei\u22121, ei))) + log (P (rhead | e0, el)))) \u2217 \u03c1rmin (eh, et) (12b)\nwhere Ssc and Shc represent the sets of high standard-confidence rules and high head-coverage rules, respectively. bconf and bhead are their rule fitness. \u03c1rmin (eh, et) is an indicator function mentioned in equation (5), which takes 1 if the weakest triple in the rule body holds true, and 0 otherwise.\nWe combine the bidirectional constraint losses\ninto a unified loss, which is jointly computed with Lcls. The formulation is as follows:\nLglobal = Lcls + \u03bb \u2217 (Lsc + Lhc) (13)\nwhere \u03bb is a relaxation factor that reflects the weight of the rule consistency loss."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "To demonstrate the ability of our method to generalize, we conducted evaluations on three datasets for document-level relation extraction. including DWIE(Zaporojets et al., 2021), DocRED(Yao et al., 2019), and Re-DocRED(Tan et al., 2022). The detailed information of datasets are shown in Table 1. DWIE This dataset is a human-annotated collection used for document-level information extraction, which includes DocRE. It contains gold rule labels, which can be used to evaluate the logical consistency of the output of DocRE models. DocRED It\u2019s a popular large-scale DocRE dataset, which is sourced from Wikipedia articles. It is the most widely used dataset for DocRE, and the majority of methods are experimented on this dataset. Re-DocRED It analyzes the causes and impacts of false negatives in the DocRED dataset and reannotates 4,053 documents. Compared to the DocRED dataset, most document-level relation extraction methods show significant improvement in performance on this dataset."
        },
        {
            "heading": "4.2 Experimental Setups",
            "text": "Metrics Following the experimental settings of (Ru et al., 2021) and (Fan et al., 2022), we evaluate our method using three metrics: F1, Ign F1, and Logic. The Ign F1 score excludes relation triplets that involved by either train set or dev set, preventing leakage of information from the test set. Logic\nis used to assess the adherence of our predictions to the golden rule. Baselines To verify the generalizability of our method as a plugin model for DocRE, we select the following four models as backbone models: BiLSTM(Yao et al., 2019), GAIN(Zeng et al., 2020), ATLOP(Zhou et al., 2021), and DREEAM(Ma et al., 2023). For fairness, we choose bert-basecased as the pretraining model for GAIN, ATLOP, and DREEAM. Meanwhile, we also compare our model BCBR with other logic constraint DocRE models \u2013 LogiRE(Ru et al., 2021) and MILR(Fan et al., 2022)1. Implementation Details For fairness, we conduct experiments based on the recommended parameters in the baselines. We average the results over five different random seeds. The specific hyperparameter settings for the new parameters introduced by BCBR are provided in Appendix A. All models were implemented using PyTorch 1.8.1 and trained on a Quadro RTX 6000 GPU."
        },
        {
            "heading": "4.3 Results & Discussions",
            "text": "Results on DWIE We can observe results on DWIE in Table 2. Among all baseline models, our BCBR model consistently outperforms LogiRE and MILR, indicating its strong generality, making it compatible with the majority of DocRE models. Building upon the state-of-the-art baseline model, DREEAM, BCBR achieves 3.33% Ign F1, 3.34% F1 and 4.02% Logic improvements on test set, reaching state-of-the-art performance. In comparison to LogiRE and MILR, BCBR achieves 1.94% Ign F1, 1.40% F1 and 2.83% Logic improvements. It demonstrates that BCBR has achieved significant improvements in both relation extraction performance and rule consistency. Meanwhile, We conducted a comparative experiment between our beta rule miner and a general rule miner, as detailed in the Appendix B. Results on DocRED The experimental results on DocRED are presented in Table 2. Apart from DWIE, we only include the performance of strong baselines on other datasets. LogiRE does not exhibit significant improvements on the DocRED dataset, primarily due to the presence of a large number of false negative labels. The EM algorithm used in LogiRE leads to overfitting issues. On the other hand, MILR and BCBR perform rela-\n1Our implementation.\ntively better as they jointly train with DocRE models. BCBR achieves the best results on this dataset, with 1.53% Ign F1 and 1.59% F1 improvements. It demonstrates that BCBR performs better than LogiRE and MILR on DocRED.\nResults on Re-DocRED The results on ReDocRED can be seen from Table 4. Due to the resolution of false-negative labels in DocRED, most relation extraction models exhibit significant improvements on this dataset. BCBR achieves 1.34% Ign F1 and 1.29% F1 improvement, which is slightly higher than the improvement on DocRED. By this, we can conclude that the BCBR can assist the backbone model more effectively when a majority of the false-negative label issues are resolved."
        },
        {
            "heading": "4.4 Ablation study",
            "text": "To demonstrate the effectiveness of each component of the BCBR framework, we conduct ablation experiments, and the experimental results are shown in Table 5. We use the DWIE dataset and perform the experiments on the strongest baseline model DREEAM. In the table, BR and BC refer to the Beta Rule and Bidirectional Constraint, respectively. We exclude the beta rules using the original rule miner and exclude the bidirectional constraint using the rule forward constraint. From the table, we can observe that when exclude one of the components, our method still outperforms the baseline approach. This indicates that both components are effective. The quality of rules and the comprehensiveness of logic constraints are both\ncrucial factors."
        },
        {
            "heading": "4.5 Case study & LLM",
            "text": "We list some rules mined by our Beta Rule Miner and their beta scores in Table 6. From the table, we can learn about various rule patterns that we can mine. Then we present several inference cases of BCBR framework on the DWIE dataset, as shown in Figure 3. We compare the results of BCBR with the strongest baseline - BREEAM and highlight the advantages of using logic constraints. We also compare ours to the outputs of large language models to demonstrate the significance of our task during the era of large language models. In the first case, BREEAM can only predict the relation vs from Bayern to Dortmund, while our BCBR can predict the relation vs from Dortmund to Bayern due to the assistance of rules. However, ChatGPT\nalso choose the correct answer, which indicates that general DocRE models cannot even perform simple logical inference, but there is no significant gap between our framework and ChatGPT in simple logical inference. In the second example, BREEAM also cannot predict the head_of_gov relation from Modi to India, while ChatGPT makes an incorrect prediction. This fully demonstrates the superiority of our method in complex logical inference. In the third example, the rule used is different from the front two. It is a rule that satisfies the reverse constraint. We can infer the non-existence of the rule head agency_of -x by the non-existence of the rule body atom baesd_in0. Both BREEAM and ChatGPT cannot satisfy reverse constraint and make an incorrect prediction, which reflects the unique advantage of our method in reverse constraints. However, BCBR is not in conflict with ChatGPT, as logic constraints can enhance the reasoning ability of large models. Thus, ChatGPT can work together with logic constraints to improve the performance of DocRE. The integration will be a interesting direction in future research."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose a novel logic constraint framework BCBR, which utilises bidirectional\nlogic constraints of beta rules to regulate the output of DocRE. We are the first to propose the use of beta distribution for modeling rules, which effectively solves the problem of pseudo-rules. Then we model the reverse logic constraints and utilize bidirectional constraints of beta rules to construct rule consistency loss. By jointly training with relation classification loss, we improve the performance of DocRE. Experimental results on multiple datasets demonstrate that BCBR outperforms baseline models and other logic constraint frameworks.\nLimitations\nOur BCBR brings additional rule consistency loss, resulting in a significant increase in training time. We need to traverse all rules when processing each document to generate rule consistency loss. It leads to a significant increase in time cost. We will optimize the code structure in future work to achieve convergence of the model in a relatively short period of time."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by National Natural Science Foundation of China (NSFC) (61972455) and the Project of Science and Technology Research and Development Plan of China Railway Corporation (N2023J044)."
        },
        {
            "heading": "A Hyper-Parameter Settings",
            "text": "We detail the hyperparameter settings of BCBR on different datasets in Table 7."
        },
        {
            "heading": "B Rule miner comparison",
            "text": "We analyzed the distribution of rules mined by the Beta rule miner and the general rule miner at different support intervals on DWIE dataset. Beta(SC) and Beta(HC) represent the highstandard-confidence rules and high-head-coverage rules extracted by the Beta rule extractor, respectively. The results are shown in Figure 4. We can observe that the rules mined by Beta rule miner are\nmostly scattered in the 101 to 500 support interval, and there are no low-support rules scattered in the 1 to 10 support interval. In contrast, the general rule miner has a large number of rules scattered in the 1 to 10 support interval. It reflects that the high quality of rules mined by the Beta rule miner is much higher than that of the general rule miner. In addition, we can observe the proportion between Beta(SC) and Beta(HC), which indicates that the rules satisfying the reverse constraint cannot be ignored. If only high-standard-confidence rules are used to constrain the relation extraction process, a large amount of consistency information among rules will be lost, leading to a decline in the performance of relation extraction."
        }
    ],
    "title": "Document-level Relationship Extraction by Bidirectional Constraints of Beta Rules",
    "year": 2023
}