{
    "abstractText": "Warning: This paper discusses and contains offensive or upsetting content. Nowadays, many hate speech detectors are built to automatically detect hateful content. However, their training sets are sometimes skewed towards certain stereotypes (e.g., race or religion-related). As a result, the detectors are prone to depend on some shortcuts for predictions. Previous works mainly focus on token-level analysis and heavily rely on human experts\u2019 annotations to identify spurious correlations, which is not only costly but also incapable of discovering higherlevel artifacts. In this work, we use grammar induction to find grammar patterns for hate speech and analyze this phenomenon from a causal perspective. Concretely, we categorize and verify different biases based on their spuriousness and influence on the model prediction. Then, we propose two mitigation approaches including Multi-Task Intervention and Data-Specific Intervention based on these confounders. Experiments conducted on 9 hate speech datasets demonstrate the effectiveness of our approaches. The code is available at https://github. com/SALT-NLP/Bias_Hate_Causal.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhehao Zhang"
        },
        {
            "affiliations": [],
            "name": "Jiaao Chen"
        },
        {
            "affiliations": [],
            "name": "Diyi Yang"
        }
    ],
    "id": "SP:f31fca0eb711b9e73e78d2798a957681f1e8698b",
    "references": [
        {
            "authors": [
                "Badr AlKhamissi",
                "Faisal Ladhak",
                "Srini Iyer",
                "Ves Stoyanov",
                "Zornitsa Kozareva",
                "Xian Li",
                "Pascale Fung",
                "Lambert Mathias",
                "Asli Celikyilmaz",
                "Mona Diab"
            ],
            "title": "Token: Task decomposition and knowledge infusion for few-shot hate speech detection",
            "year": 2022
        },
        {
            "authors": [
                "Esma Balkir",
                "Isar Nejadgholi",
                "Kathleen Fraser",
                "Svetlana Kiritchenko."
            ],
            "title": "Necessity and sufficiency for explaining text classifiers: A case study in hate speech detection",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Com-",
            "year": 2022
        },
        {
            "authors": [
                "Taylor Berg-Kirkpatrick",
                "David Burkett",
                "Dan Klein."
            ],
            "title": "An empirical investigation of statistical significance in NLP",
            "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learn-",
            "year": 2012
        },
        {
            "authors": [
                "Federico Bianchi",
                "Stefanie Anja Hills",
                "Patricia Rossini",
                "Dirk Hovy",
                "Rebekah Tromble",
                "Nava Tintarev"
            ],
            "title": "it\u2019s not just hate\u201d: A multi-dimensional perspective",
            "year": 2022
        },
        {
            "authors": [
                "Bonaldi",
                "Helena",
                "Dellantonio",
                "Sara",
                "Tekiro\u011flu",
                "Serra Sinem",
                "Guerini",
                "Marco"
            ],
            "title": "HumanMachine Collaboration Approaches to Build a Dialogue Dataset for Hate Speech Countering",
            "year": 2022
        },
        {
            "authors": [
                "Tulika Bose",
                "Nikolaos Aletras",
                "Irina Illina",
                "Dominique Fohr"
            ],
            "title": "Dynamically refined regularization for improving cross-corpora hate speech detection",
            "venue": "In Findings of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Sam McCandlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei."
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.",
            "year": 2020
        },
        {
            "authors": [
                "Ke-Li Chiu",
                "Annie Collins",
                "Rohan Alexander."
            ],
            "title": "Detecting hate speech with gpt-3",
            "venue": "Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. 2020. Learning to model and ignore dataset bias with mixed capacity ensembles. In Findings of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Dehghani."
            ],
            "title": "Improving counterfactual generation for fair hate speech detection",
            "venue": "Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. In Proceedings",
            "year": 2021
        },
        {
            "authors": [
                "Ona de Gibert",
                "Naiara Perez",
                "Aitor Garc\u00eda-Pablos",
                "Montse Cuadros"
            ],
            "title": "Hate Speech Dataset from a White Supremacy Forum",
            "venue": "AAAI Conference on Web and Social Media,",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep",
            "venue": "Workshop on Abusive Language Online (ALW2),",
            "year": 2019
        },
        {
            "authors": [
                "Sarah Dreier",
                "Emily Gade",
                "Dallas Card",
                "Noah Smith"
            ],
            "title": "Patterns of bias: How mainstream media operationalize links between mass shootings and terrorism",
            "venue": "lis, Minnesota",
            "year": 2022
        },
        {
            "authors": [
                "Mengnan Du",
                "Varun Manjunatha",
                "Rajiv Jain",
                "Ruchi Deshpande",
                "Franck Dernoncourt",
                "Jiuxiang Gu",
                "Tong Sun",
                "Xia Hu"
            ],
            "title": "Towards interpreting and mitigating shortcut learning behavior of NLU models",
            "venue": "Political Communication,",
            "year": 2021
        },
        {
            "authors": [
                "Mai ElSherief",
                "Caleb Ziems",
                "David Muchlinski",
                "Vaishnavi Anupindi",
                "Jordyn Seybolt",
                "Munmun De Choudhury",
                "Diyi Yang"
            ],
            "title": "Latent hatred: A benchmark for understanding implicit hate speech",
            "year": 2021
        },
        {
            "authors": [
                "Mai ElSherief",
                "Caleb Ziems",
                "David Muchlinski",
                "Vaishnavi Anupindi",
                "Jordyn Seybolt",
                "Munmun De Choudhury",
                "Diyi Yang"
            ],
            "title": "Latent hatred: A benchmark",
            "venue": "Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Amir Feder",
                "Katherine A. Keith",
                "Emaad Manzoor",
                "Reid Pryzant",
                "Dhanya Sridhar",
                "Zach Wood-Doughty",
                "Jacob Eisenstein",
                "Justin Grimmer",
                "Roi Reichart",
                "Margaret E. Roberts",
                "Brandon M. Stewart",
                "Victor Veitch",
                "Diyi Yang"
            ],
            "title": "Causal inference in natural language",
            "year": 2021
        },
        {
            "authors": [
                "Dan Friedman",
                "Alexander Wettig",
                "Danqi Chen."
            ],
            "title": "Finding dataset shortcuts with grammar induction",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4345\u20134363.",
            "year": 2022
        },
        {
            "authors": [
                "Lei Gao",
                "Ruihong Huang."
            ],
            "title": "Detecting online hate speech using context aware models",
            "venue": "Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 260\u2013 266, Varna, Bulgaria. INCOMA Ltd.",
            "year": 2017
        },
        {
            "authors": [
                "Matt Gardner",
                "William Merrill",
                "Jesse Dodge",
                "Matthew Peters",
                "Alexis Ross",
                "Sameer Singh",
                "Noah A. Smith."
            ],
            "title": "Competency problems: On finding and removing artifacts in language data",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Sahaj Garg",
                "Vincent Perot",
                "Nicole Limtiaco",
                "Ankur Taly",
                "Ed H. Chi",
                "Alex Beutel"
            ],
            "title": "Online and Punta Cana, Dominican Republic",
            "year": 2019
        },
        {
            "authors": [
                "Sam Gehman",
                "Suchin Gururangan",
                "Maarten Sap",
                "Yejin Choi",
                "Noah A Smith."
            ],
            "title": "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
            "venue": "Findings of EMNLP. Jonas Gehring, Michael Auli, David Grangier, Denis",
            "year": 2020
        },
        {
            "authors": [
                "Yarats",
                "Yann N. Dauphin."
            ],
            "title": "Convolutional sequence to sequence learning",
            "venue": "Proceedings of the 34th International Conference on Machine Learning Volume 70, ICML\u201917, page 1243\u20131252. JMLR.org. Suchin Gururangan, Swabha Swayamdipta, Omer Levy,",
            "year": 2017
        },
        {
            "authors": [
                "Roy Schwartz",
                "Samuel Bowman",
                "Noah A. Smith."
            ],
            "title": "Annotation artifacts in natural language inference data",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2018
        },
        {
            "authors": [
                "Moritz Hardt",
                "Eric Price",
                "Nati Srebro"
            ],
            "title": "Meeting of the Association for Computational Linguistics, pages 5553\u20135563",
            "year": 2016
        },
        {
            "authors": [
                "Curran Associates",
                "Inc. Camille Harris",
                "Matan Halevy",
                "Ayanna Howard",
                "Amy Bruckman",
                "Diyi Yang"
            ],
            "title": "Exploring the role of grammar and word choice in bias toward african",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Camille Harris",
                "Matan Halevy",
                "Ayanna Howard",
                "Amy"
            ],
            "title": "american english (aae) in hate speech classification",
            "year": 2022
        },
        {
            "authors": [
                "Bruckman",
                "Diyi Yang."
            ],
            "title": "Exploring the role of grammar and word choice in bias toward african american english (aae) in hate speech classification",
            "venue": "2022 ACM Conference on Fairness, Accountability, and Transparency, pages 789\u2013798.",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Hartvigsen",
                "Saadia Gabriel",
                "Hamid Palangi",
                "Maarten Sap",
                "Dipankar Ray",
                "Ece Kamar."
            ],
            "title": "ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection",
            "venue": "Proceedings of the 60th Annual Meeting of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Dublin",
                "Ireland"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Akbar Karimi",
                "Leonardo Rossi",
                "Andrea Prati."
            ],
            "title": "AEDA: An easier data augmentation technique for text classification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2748\u20132754, Punta Cana, Dominican Republic. Association for Com-",
            "year": 2021
        },
        {
            "authors": [
                "Linguistics. Brendan Kennedy",
                "Xisen Jin",
                "Aida Mostafazadeh Davani",
                "Morteza Dehghani",
                "Xiang Ren"
            ],
            "title": "Contextualizing hate speech classifiers with post-hoc explanation",
            "venue": "In Proceedings of the 58th Annual Meeting",
            "year": 2020
        },
        {
            "authors": [
                "Bernhard Sch\u00f6lkopf."
            ],
            "title": "Avoiding discrimination through causal reasoning",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Yoon Kim, Chris Dyer, and Alexander Rush. 2019.",
            "year": 2017
        },
        {
            "authors": [
                "Ana Kotarcic",
                "Dominik Hangartner",
                "Fabrizio Gilardi",
                "Selina Kurer",
                "Karsten Donnay."
            ],
            "title": "Human-in-theloop hate speech classification in a multilingual context",
            "venue": "Allison Lahnala, Charles Welch, B\u00e9la Neuendorf, and Lucie Flek. 2022. Mitigating toxic degeneration with",
            "year": 2022
        },
        {
            "authors": [
                "Association for Computational Linguistics. Xinyao Ma",
                "Maarten Sap",
                "Hannah Rashkin",
                "Yejin Choi."
            ],
            "title": "Powertransformer: Unsupervised controllable revision for biased language correction",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Thomas Mandl",
                "Sandip Modha",
                "Prasenjit Majumder",
                "Daksh Patel",
                "Mohana Dave",
                "Chintak Mandlia",
                "Aditya Patel."
            ],
            "title": "Overview of the hasoc track at fire 2019: Hate speech and offensive content identification in indo-european languages",
            "venue": "Proceedings of the",
            "year": 2019
        },
        {
            "authors": [
                "Tom McCoy",
                "Ellie Pavlick",
                "Tal Linzen"
            ],
            "title": "Hatexplain: A benchmark dataset for explainable hate speech detection",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Yejin Choi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Reframing instructional prompts to GPTk\u2019s language",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 589\u2013612, Dublin, Ireland. Association for",
            "year": 2022
        },
        {
            "authors": [
                "Ioannis Mollas",
                "Zoe Chrysopoulou",
                "Stamatis Karlos",
                "Grigorios Tsoumakas."
            ],
            "title": "ETHOS: a multi-label hate speech detection dataset",
            "venue": "Complex & Intelligent Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Xing Niu",
                "Prashant Mathur",
                "Georgiana Dinu",
                "Yaser Al-Onaizan."
            ],
            "title": "Evaluating robustness to input perturbations for neural machine translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8538\u20138544, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Judea Pearl"
            ],
            "title": "Models, reasoning and inference",
            "venue": "Cambridge, UK: CambridgeUniversityPress, 19(2).",
            "year": 2000
        },
        {
            "authors": [
                "Pouya Pezeshkpour",
                "Sarthak Jain",
                "Sameer Singh",
                "Byron Wallace."
            ],
            "title": "Combining feature and instance attribution to detect artifacts",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1934\u20131946, Dublin, Ireland. Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Alan Ramponi",
                "Sara Tonelli."
            ],
            "title": "Features or spurious artifacts? data-centric baselines for fair and robust hate speech detection",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
            "year": 2022
        },
        {
            "authors": [
                "Kunal Relia",
                "Zhengyi Li",
                "Stephanie H Cook",
                "Rumi Chunara."
            ],
            "title": "Race, ethnicity and national originbased discrimination in social media and hate crimes across 100 us cities",
            "venue": "Proceedings of the International AAAI Conference on Web and Social Media, volume 13,",
            "year": 2019
        },
        {
            "authors": [
                "Shachar Rosenman",
                "Alon Jacovi",
                "Yoav Goldberg."
            ],
            "title": "Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data",
            "venue": "Proceedings of the",
            "year": 2020
        },
        {
            "authors": [
                "Paul R\u00f6ttger",
                "Bertie Vidgen",
                "Dong Nguyen",
                "Zeerak Waseem",
                "Helen Margetts",
                "Janet Pierrehumbert"
            ],
            "title": "Empirical Methods in Natural Language Processing (EMNLP), pages 3702\u20133710",
            "year": 2020
        },
        {
            "authors": [
                "Paul R\u00f6ttger",
                "Debora Nozza",
                "Federico Bianchi",
                "Dirk Hovy"
            ],
            "title": "Data-efficient strategies for expanding hate speech detection into under-resourced languages",
            "year": 2022
        },
        {
            "authors": [
                "Koustuv Saha",
                "Eshwar Chandrasekharan",
                "Munmun De Choudhury."
            ],
            "title": "Prevalence and psychological effects of hateful speech in online college communities",
            "venue": "Proceedings of the 10th ACM Conference on Web Science, WebSci \u201919, page 255\u2013264, New York, NY,",
            "year": 2019
        },
        {
            "authors": [
                "Machinery. Joni Salminen",
                "Hind Almerekhi",
                "Milica Milenkovi\u0107",
                "Soon-gyo Jung",
                "Jisun An",
                "Haewoon Kwak",
                "Bernard Jansen"
            ],
            "title": "Anatomy of online hate: Developing a taxonomy and machine learning models for identifying",
            "year": 2018
        },
        {
            "authors": [
                "Maarten Sap",
                "Dallas Card",
                "Saadia Gabriel",
                "Yejin Choi",
                "Noah A Smith"
            ],
            "title": "The risk of racial bias in hate",
            "venue": "and classifying hate in online news media. Proceedings of the International AAAI Conference on Web and Social Media,",
            "year": 2019
        },
        {
            "authors": [
                "ACL. Maarten Sap",
                "Saadia Gabriel",
                "Lianhui Qin",
                "Dan Jurafsky",
                "Noah A Smith",
                "Yejin Choi"
            ],
            "title": "Social bias frames: Reasoning about social and power implications of language",
            "year": 2020
        },
        {
            "authors": [
                "Maarten Sap",
                "Swabha Swayamdipta",
                "Laura Vianna",
                "Xuhui Zhou",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "Annotators with attitudes: How annotator beliefs and identities bias toxic language detection",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter",
            "year": 2022
        },
        {
            "authors": [
                "ner",
                "Isabelle Augenstein"
            ],
            "title": "How does counterfactually augmented data impact models for social computing constructs",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Indira Sen",
                "Mattia Samory",
                "Claudia Wagner",
                "Isabelle Augenstein"
            ],
            "title": "Counterfactually augmented data and unintended bias: The case of sexism and hate",
            "venue": "Dominican Republic",
            "year": 2022
        },
        {
            "authors": [
                "Dinghan Shen",
                "Mingzhi Zheng",
                "Yelong Shen",
                "Yanru Qu",
                "Weizhu Chen."
            ],
            "title": "A simple but toughto-beat data augmentation approach for natural language understanding and generation",
            "venue": "arXiv preprint arXiv:2009.13818.",
            "year": 2020
        },
        {
            "authors": [
                "Avanti Shrikumar",
                "Peyton Greenside",
                "Anshul Kundaje."
            ],
            "title": "Learning important features through propagating activation differences",
            "venue": "Proceedings of the 34th International Conference on Machine Learning Volume 70, ICML\u201917, page 3145\u20133153. JMLR.org.",
            "year": 2017
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
            "year": 2013
        },
        {
            "authors": [
                "Daniel Smilkov",
                "Nikhil Thorat",
                "Been Kim",
                "Fernanda Vi\u00e9gas",
                "Martin Wattenberg"
            ],
            "title": "Smoothgrad: removing noise by adding noise",
            "year": 2017
        },
        {
            "authors": [
                "Rohit Sridhar",
                "Diyi Yang."
            ],
            "title": "Explaining toxic text via knowledge enhanced text generation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 811\u2013826,",
            "year": 2022
        },
        {
            "authors": [
                "Gabriel Stanovsky",
                "Noah A. Smith",
                "Luke Zettlemoyer."
            ],
            "title": "Evaluating gender bias in machine translation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1679\u2013 1684, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan."
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 3319\u20133328. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Zeerak Talat",
                "James Thorne",
                "Joachim Bingel"
            ],
            "title": "Bridging the gaps: Multi task learning for domain transfer of hate speech detection",
            "year": 2018
        },
        {
            "authors": [
                "Himanshu Thakur",
                "Atishay Jain",
                "Praneetha Vaddamanu",
                "Paul Pu Liang",
                "Louis-Philippe Morency"
            ],
            "title": "Language models get a gender makeover: Mitigating gender bias with few-shot data interventions",
            "year": 2023
        },
        {
            "authors": [
                "Thanh Tran",
                "Yifan Hu",
                "Changwei Hu",
                "Kevin Yen",
                "Fei Tan",
                "Kyumin Lee",
                "Se Rim Park."
            ],
            "title": "HABERTOR: An efficient and effective deep hatespeech detector",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Lifu Tu",
                "Garima Lalwani",
                "Spandana Gella",
                "He He"
            ],
            "title": "An empirical study on robustness to spurious correlations using pre-trained language models",
            "year": 2020
        },
        {
            "authors": [
                "Ameya Vaidya",
                "Feng Mai",
                "Yue Ning."
            ],
            "title": "Empirical analysis of multi-task learning for reducing model bias in toxic comment detection",
            "venue": "Bertie Vidgen, Alex Harris, Dong Nguyen, Rebekah Tromble, Scott Hale, and Helen Margetts. 2019. Chal-",
            "year": 2020
        },
        {
            "authors": [
                "Sharon Qian",
                "Daniel Nevo",
                "Yaron Singer",
                "Stuart Shieber."
            ],
            "title": "Investigating gender bias in language models using causal mediation analysis",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages 12388\u201312401. Curran Associates, Inc.",
            "year": 2020
        },
        {
            "authors": [
                "Tianlu Wang",
                "Rohit Sridhar",
                "Diyi Yang",
                "Xuezhi Wang."
            ],
            "title": "Identifying and mitigating spurious correlations for improving robustness in NLP models",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1719\u20131729, Seattle, United",
            "year": 2022
        },
        {
            "authors": [
                "States. Association for Computational Linguistics. Zhao Wang",
                "Aron Culotta."
            ],
            "title": "Identifying spurious correlations for robust text classification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3431\u20133440, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Michael Wiegand",
                "Josef Ruppenhofer",
                "Thomas Kleinbauer"
            ],
            "title": "Detection of Abusive Language: the Problem of Biased Datasets",
            "venue": "In Proceedings of the 2019 Conference of the North American Chapter",
            "year": 2019
        },
        {
            "authors": [
                "Tomer Wullach",
                "Amir Adler",
                "Einat Minkov"
            ],
            "title": "Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 602\u2013608, Minneapolis, Minnesota",
            "year": 2021
        },
        {
            "authors": [
                "Wenjie Yin",
                "Arkaitz Zubiaga."
            ],
            "title": "Towards generalisable hate speech detection: a review on obstacles and solutions",
            "venue": "Xinchen Yu, Eduardo Blanco, and Lingzi Hong. 2022. Hate speech and counter speech detection: Conversa-",
            "year": 2021
        },
        {
            "authors": [
                "Xiangji Zeng",
                "Yunliang Li",
                "Yuchen Zhai",
                "Yin Zhang."
            ],
            "title": "Counterfactual generator: A weakly-supervised method for named entity recognition",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7270\u20137280,",
            "year": 2020
        },
        {
            "authors": [
                "Ziqi Zhang",
                "D. Robinson",
                "Jonathan Tepper"
            ],
            "title": "Detecting hate speech on twitter using a convolution-gru based deep neural network",
            "year": 2018
        },
        {
            "authors": [
                "Xuhui Zhou",
                "Maarten Sap",
                "Swabha Swayamdipta",
                "Yejin Choi",
                "Noah Smith."
            ],
            "title": "Challenges in automated debiasing for toxic language detection",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Vol-",
            "year": 2021
        },
        {
            "authors": [
                "Caleb Ziems",
                "William Held",
                "Omar Shaikh",
                "Jiaao Chen",
                "Zhehao Zhang",
                "Diyi Yang"
            ],
            "title": "Can large language models transform computational social science",
            "year": 2023
        },
        {
            "authors": [
                "Following ElSherief et al",
                "2021b",
                "Ramponi",
                "Tonelli"
            ],
            "title": "2022, we use a grid search to find the most suitable hyperparameters including learning rate [2e-5, 3e-5, 5e-5, 8e-6, 5e-6], batch size [4, 8, 10] and the number of epochs",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Over the past few years, hate speech on social media has grown significantly in different forms. This causes serious consequences and societal impact on victims of all demographics (Mathew et al., 2021), largely affects their mental health (Saha et al., 2019) and even triggers real-world hate crimes (Relia et al., 2019). As a result, automated detection of hate speech becomes especially important and there have been numerous research studies conducted to characterize and detect these hateful or toxic contents (Zhou et al., 2021; Lahnala et al., 2022; Kotarcic et al., 2022).\nHowever, current hate speech detection approaches often fail to be robust and generalizable,\neven to slightly different datasets on the same task, which largely prevents them from being applied in real-world applications (Vidgen et al., 2019). This might be due to the fact that current models are suffering from the spurious correlations between training data and labels (e.g., hateful) (Ramponi and Tonelli, 2022), which might lead to the biased treatment of vulnerable and minority groups such as African American Vernacular English Speakers and may exacerbate racism (Harris et al., 2022a). One example is shown in Figure 1. The frequent co-occurrence of identity words and the hateful label in the training set can bias the detectors (e.g., fine-tuned BERT) to make false predictions during inference. As a result, it is of great need to comprehensively understand and identify the spurious correlations in hate speech detection and further mitigate the bias caused by spurious correlations.\nA growing amount of recent work has examined the spurious correlation in hate speech detection\n(Zhou et al., 2021; Kennedy et al., 2020; Sap et al., 2022), and found that a large number of tokens that target minority groups are highly correlated with the hateful label (Bender et al., 2021). As a result, methods like masking and removal of such tokens with the help of human annotations (Ramponi and Tonelli, 2022) have been proposed to mitigate these spurious correlations, together with finetuned large language models to generate artifacts to augment the training (Wullach et al., 2021; Hartvigsen et al., 2022). Despite these successes, they usually focus on identifying token-level spurious patterns in one specific dataset manually (Bender et al., 2021) while neglecting spurious correlations beyond tokens (i.e., in sentence levels, shown in the right part of Figure 1(b)) and lacking comprehensive studies across different datasets and domains. Moreover, these mitigations are often costly and timeconsuming as human annotation and fine-tuning large language models to generate numerous data are required. Thus, a systematic study is needed to first automatically identify spurious correlations in a given hate speech dataset and then effectively and efficiently mitigate them.\nTo fill in this gap, we first conduct a comprehensive analysis to discover spurious correlations from both token-level and sentence-level on 9 hate speech datasets in an automatic way. Specifically, we use mutual information with domain knowledge to identify token-level spurious correlations such as identity words and leverage context-free grammar to investigate sentence-level highly correlated grammar patterns. We further propose a novel metric called Relative Spuriousness (RS) to verify the spuriousness of discovered spurious correlations based on its influences on model prediction. Our analysis shows that token-level spurious patterns are usually more general that exist in almost all datasets while sentence-level spurious patterns are more dataset-specific. We further study how spurious correlations cause model bias from a casual perspective (as shown in Figure 2). one affecting the distribution between PLM\u2019s pretraining data and vulnerable identities, and another influencing the distribution between vulnerable identities and their context in hate speech datasets. To mitigate these two biases, we propose Multi-Task Intervention (MTI) and Data-Specific Intervention (DSI) for bias mitigation. MTI tries to mitigate biases from the pre-training corpus through training auxiliary tasks, while DSI focuses on eliminating biases\noriginating from limited data and contains a framework that automatically detects, validates, and mitigates biases through a counterfactual generator. Experiments conducted on 9 hate speech datasets and out-of-domain challenge sets demonstrate the effectiveness and robustness of our proposal.\nTo summarize, our contributions are: (1) We automatically identify spurious correlations and comprehensively analyze them in hate speech detection from both token-level and sentence-level across 9 datasets. (2) We introduce a novel metric, Relative Spuriousness, to evaluate the spuriousness of identified spurious correlations at the sentence level. (3) We study the bias caused by spurious correlations from a causal perspective. (4) We propose two strategies, MTI and DSI, to mitigate the biases and show consistent improvements in 9 datasets."
        },
        {
            "heading": "2 Related Work",
            "text": "Debias Hate Speech Detection Recent works (Yin and Zubiaga 2021; Wiegand et al. 2019; Kennedy et al., 2020; Ma et al., 2020; Gehman et al., 2020; Sap et al., 2020; Dreier et al., 2022; Stanovsky et al., 2019; Sridhar and Yang, 2022; Thakur et al., 2023; Ziems et al., 2023) have been studying the generalizability and biases for hate speech detection (Talat et al., 2018; AlKhamissi et al., 2022; R\u00f6ttger et al., 2022; Bianchi et al., 2022). For instance, prior work found that existing hate speech detection models are biased against African American Vernacular English Speakers (Harris et al., 2022b; Sap et al., 2019) and certain identity words are highly correlated with these hateful labels (Bender et al., 2021; ElSherief et al., 2021a). A group of data augmentation methods (Sen et al. 2021; Sen et al., 2022; Hartvigsen et al. 2022) are proposed to mitigate the biases. For instance, Wullach et al. propose a simple data augmentation method using a finetuned GPT-2 model to generate 100k hate and non-hate data. Ramponi and Tonelli find highly correlated tokens and manually categorize them into different groups, and further mask or remove these annotated spurious artifacts. However, human annotations are not practical in large-scale or multi-platform scenarios. Different from these prior works that mainly focus on data-specific biases and mitigation methods, our method automatically finds these shortcuts and proposes Multi-Task Interventions on model levels."
        },
        {
            "heading": "2.1 Spurious Correlation in NLP",
            "text": "Increasing attention has been focused on spurious correlations in NLP tasks (Tu et al., 2020; McCoy et al., 2019; Jia and Liang, 2017; Niu et al., 2020; Wang and Culotta, 2020; Vaidya et al., 2020; Clark et al., 2020; Du et al., 2021). A line of work tries to evaluate the models\u2019 robustness on pre-defined shortcuts by proposing challenge test datasets (McCoy et al., 2019; Rosenman et al., 2020; R\u00f6ttger et al., 2021) or finding salient words (Simonyan et al., 2013; Pezeshkpour et al., 2022; Shrikumar et al., 2017; Han et al., 2020). Another line of work also intends to verify the spuriousness of extracted features. Gardner et al., 2021 suggests that all correlations between labels and low-level features are spurious. Eisenstein, 2022 claims these correlations will naturally appear in the majority of classification tasks and that domain knowledge is required to identify any correlations that may be harmful. Joshi et al. define the spuriousness based on the sufficiency of the feature and counterfactual intervention. However, their definition is based on an unbiased classifier to model the probability of sufficiency, which could not be practical in real applications. Besides, they do not use this metric to find spurious artifacts. Instead, they use domain knowledge to pre-define the potential feature. Furthermore, previous analyses (Zhang et al., 2018) of biases\u2019 influences on model predictions are limited. To this end, we propose a novel metric named Relative Spuriousness (RS) which considers the biases\u2019 influence on the model\u2019s decision-making."
        },
        {
            "heading": "3 Methods",
            "text": "This section describes how we identify, understand, and mitigate the spurious correlations in Hate Speech Detection (HS)."
        },
        {
            "heading": "3.1 Identifying Spurious Correlations",
            "text": "Current HS models often suffer from spurious correlations (Ramponi and Tonelli, 2022; Bose et al., 2022; Wang et al., 2022): they tend to utilize prediction patterns that hold for the majority examples but do not hold in general. This might cause the model to be biased in applications (Ramponi and Tonelli, 2022; Gardner et al., 2021). In this section, we identify potential spurious correlations from two levels across multiple datasets."
        },
        {
            "heading": "3.1.1 Token-level Spurious Correlations",
            "text": "One specific label (e.g., hateful) might be highly correlated with certain tokens (e.g., \u201cwomen\u201d) in the datasets (Wang et al., 2022; Ramponi and Tonelli, 2022). As a result, models might learn to make biased predictions only based on those token-level correlations while neglecting the whole semantic meaning. In order to identify such token-level spurious correlations, we utilize tokenlevel PMI-based searching methods (Ramponi and Tonelli, 2022) to find biased words in HS. Specifically, following Ramponi and Tonelli, 2022, we compute the PMI using the equation described in Appendix A.1 between every word and the hateful label. After that, we select the highest correlated words for further investigation across 9 datasets. This led to a large set of identity-related words (over 80, shown in Figure 1 as an example) that are highly correlated with the hateful labels while the words themselves are neutral. We also observe that such identity words are often common across different datasets. Thus, we treat these identity words as general token-level spurious correlations1."
        },
        {
            "heading": "3.1.2 Sentence-level Spurious Correlations",
            "text": "Beyond token-level identity words, certain sentence-level patterns might also be highly correlated with specific labels that might be spurious correlations. For example, in a platform with discussion on politics, patterns such as The wall should and The wall is are usually connected with discontent with the US border wall. A large number of such speeches are hateful. However, these grammar patterns are completely neutral. To automatically discover these sentence-level spurious correlations, following Friedman et al., 2022, we induce a grammar for HS training data and obtain the maximum likelihood trees in an unsupervised manner. Specifically, we use a probabilistic context-free grammar (PCFG), which contains the distinguished start symbol S, terminal symbols V (words), non-terminal symbols N and the rules of the form A \u2192 B \u2208 R, where A \u2208 N \u222a S and B \u2208 N \u222a V . We use the same parameterization and training methods as Kim et al., 2019. We then compute the mutual information between grammar patterns (non-terminal root) and labels and find\n1Note that this identification process is automatic and we manually inspect it to ensure its quality. To analyze such spurious correlations more comprehensively and reduce noises, we adopt another module (a pre-trained NER model) to ensure these highly-correlated words are identity-related.\n!!\nC\n!\"\nP\n!#\nX\nM I\nE\nI C\nX\nP\nL\nY\n(. )/ )0\nI C\nX\nP\nL\nY\n)/ )0 I C\nX\nP\nL\nY\n(. )/ )0 *.\n(a) Original SCM\n!!\nC\n!\"\nP\n!#\nX\nM\nE\nI C\nX\nP\nL\nY\n(. )/ )0\nI C\nX\nP\nL\nY\n)/ )0 I C\nX\nP\nL\nY\n(. )/ )0 *.\n(b) Multi-Task Intervention (MTI)\n!!\nC\n!\"\nP\n!#\nX\nM I\nE\nI C\nX\nP\nL\nY\n(. )/ )0\nI C\nX\nP\nL\nY\n)/ )0 I C\nX\nP\nL\nY\n(. )/ )0 *.\n(c) Data-Specific Intervention (DSI)\nFigure 2: SCM (Structural Causal Mode) for HS detection with vulnerable identities. We have omitted the unmeasured variable U\u2217 for each variable for brevity. P \u2192 L: Using corpus P to pretrain the language model L. (I, C) \u2192 X: Vulnerable objectives I (e.g., identity words) and their contexts C constitute the input data X . (L,X) \u2192 Y : Input the language model L with data X to output the prediction Y . (a) Two confounders G1 and G2 bias the generation distribution of pretraining corpus and HS data. (b) MTI intervenes with the bias from G1, where P0 represents the training corpus of auxiliary tasks. (c) DSI further intervenes with the bias from G2, where C0 denotes the newly generated context. To analyze grammar patterns biases, we only need to change I and other parts of the SCM remain the same.\nhighly-correlated ones. We define the patterns in terms of grammar subtrees. After we apply the above method to 9 hate speech datasets, we find that different datasets have different highly correlated patterns (Section 4.5.1 as examples). As a result, such patterns can be considered potential data-specific spurious correlations.\nSpuriousness Validation After discovering these sentence-level spurious correlations candidates, we further validate their spuriousness to identify clean spurious correlations. We define Relative Spuriousness (RS) based on the necessity of a feature\u2019s influence on the model\u2019s prediction. Our definition depends on two properties of spurious features: a feature\u2019s existence significantly impacts a specific label on a biased model, and it is not important for an unbiased model ideally. Definition 1 (Relative Spuriousness (RS)) The RS of a feature xi for the label y is:\nPRS = Dl(xi, y)\u2212Dg(xi, y), Dl(xi, y) = Pl(Y = y|Xi = xi, Y = y) \u2212 Pl(Y (X \u0338= xi) = y|Xi = xi, Y = y), Dg(xi, y) = Pg(Y = y|Xi = xi, Y = y)\n\u2212 Pg(Y (X \u0338= xi) = y|Xi = xi, Y = y).\nHere Pl indicates the local probability, where we use the HS detector\u2019s softmax output trained on a single HS dataset to model it. Pg represents the global one, where we use the average response of models trained on every HS dataset. Note that we do not use the model trained on all datasets for the class-imbalance problems. Dl and Dg mean the local and global probability difference between HS examples with and without feature xi, respectively. Intuitively, for spurious features, it can have\na great impact on local probability and almost no impact on global probability. Therefore, it has large Dl(xi, y) and small Dg(xi, y), causing higher RS. Compared with the prior work (Joshi et al., 2022), we consider a feature\u2019s relative influence on the model\u2019s prediction locally and globally, filling in the gap of considering features other than xi, which is a reasonable and practical metric to find spurious features. We select the features that have RS higher than a certain threshold as data-specific biases. The following sections investigate the origin of the above biases and propose two intervention approaches to mitigate them."
        },
        {
            "heading": "3.2 Understanding Spurious Correlations",
            "text": "With the identified spurious correlations in HS, it is then of great need to study how they might be generated and how the models might suffer from them by making biased predictions. In this section, we visualize the relations between spurious correlations and the caused bias from a causal perspective (Feder et al., 2021; Hardt et al., 2016; Kilbertus et al., 2017; Vig et al., 2020). Specifically, we use Structural Causal Model (SCM) (Pearl et al., 2000) which is a conceptual model that describes the causal mechanisms of a system, to recognize the confounders which are variables that influence both the dependent variable and independent variable, causing spurious correlations. We use directed acyclic graphs (DAGs) G = {V, f} to describe the causal relationships between different variables. V refers to variable nodes and edges denote causal relation function f . We visualize the SCM for HS detection described in Figure 2a, where identities I and the corresponding context C constitute the input data X . The\nevaluation result Y (accuracy or F1 score) is generated by PLM L and input X .\nConfounders Two confounders G1 and G2 influence their correlated variable\u2019s independence of generation distribution, causing spurious correlations. The two confounders are as follows.\n\u2022 G1: is the confounder which affects the distribution of both P and vulnerable identities I . For example, the unbalance appearance frequencies of different races of people in the pretraining data for the pretrained language models may constitute this confounder.\n\u2022 G2: influences the distribution of vulnerable objectives I and their context C. The training data\u2019s source may compose this confounder. For instance, de Gibert et al. 2018 collect data from Stormfront, a white supremacist forum, where mentions of people of color (POC) usually occur in hateful contexts."
        },
        {
            "heading": "3.3 Mitigating the Bias in HS",
            "text": "Based on the above two confounders that cause spurious correlations, we then propose two intervention techniques to mitigate the bias in HS.\nMulti-Task Intervention (MTI) The motivation for MTI is twofold. Firstly, MTI tries to mitigate the spurious correlations between PLM\u2019s training\ncorpus P and vulnerable objectives I (caused by G1) in order to resolve two kinds of biases in Section 3.1. By training with MTI objectives on a wide range of HS, we expect that the original distribution connection of P and I can be altered. Secondly, most HS is composed of unregulated sentences. MTI can make the PLM get a more robust representation for HS. Specifically, we introduce two auxiliary tasks on PLM to alleviate the bias from the pre-training corpus: (1) Masked Language Modeling (MLM)(Devlin et al., 2019): we randomly mask 15 % tokens from hateful sentences from 9 HS datasets and let PLM predict the tokens on the masking position with language modeling objective. (2) Multi-Task Learning (MTL): We define different tasks as different hate speech datasets Di = {xji , y j i } ni j=1 and train on all the datasets {Di}Ni=1 (see Section 4.1 for details on datasets) simultaneously using the same PLM and different classification heads.\nData-Specific Intervention (DSI) To mitigate the influence of G2 and inspired by current progress in counterfactual learning (Zeng et al., 2020; Sen et al., 2021; Eisenstein, 2022; Garg et al., 2019; Davani et al., 2021), we propose a counterfactual generator for DSI. Recent works (Mishra et al., 2022; Ouyang et al., 2022) discover that the powerful Large Language Models (LLMs) (Brown et al.,\n2020; Touvron et al., 2023; OpenAI, 2023b) with instruction prompt can have outstanding performances in many NLP tasks. Therefore, we use off-the-shell GPT-3 with instructions as the backbone of the counterfactual generator. The overall framework of DSI is illustrated in Figure 3. After we find spurious features using the method described in Section 3.1.2, we employ different instructional prompts for general biases (identity) and data-specific biases. The details on prompt design can be found in Appendix A.4. In practice, we find that the counterfactual data may not always remain the same label as described in the prompt. Thus we further conduct a majority voting with the models trained on different HS datasets. We use a majority vote to make sure the generated counterfactual data are non-hateful, which is a way to further control the quality of the generated data. We then train the HS detector with the original training data plus the counterfactual ones."
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Datasets and Setup",
            "text": "We conduct experiments on 9 datasets 2 including Multimodal Meme Dataset (text) (Suryawanshi et al.), Hate speech dataset from a white supremacist forum (de Gibert et al., 2018), FoxNews-User-Comments (Gao and Huang, 2017), HASOC19 (Mandl et al., 2019), ETHOS (Mollas et al., 2022), Twitter Sentiment Analysis (TSA), Anatomy of Online Hate (AOH) (Salminen et al., 2018), Hate Speech on Twitter (Zhang et al., 2018) and Hate-Offensive (AHS) (Davidson et al., 2017). Following previous work (Ramponi and Tonelli, 2022; ElSherief et al., 2021b), we adopt BERTbased-cased as the detector backbone model. To better avoid the issue of class imbalances, we use macro F1 as the primary metric (other metrics such as RS are also explored in Section 4.5.2). More implementation details can be found in Appendix A.5.\n2Statistics along with generated counterfactual data can be found in Appendix A.2."
        },
        {
            "heading": "4.2 Baselines",
            "text": "To evaluate the effectiveness of MTI, we compare them with the following baselines3: Finetune (Devlin et al., 2019): finetunes the BERT for hate speech detection (Tran et al., 2020). text-davinci002/text-davinci-003/ChatGPT zero/few (Brown et al., 2020; OpenAI, 2023a): uses text-davinci002/text-davinci-003/ChatGPT along with a text prompt to model the classification problem into a conditional generation problem (with a few examples). MLM + Finetune: first conducts Masked Language Modeling (MLM) (Devlin et al., 2019) on hateful speeches to get a more robust representation before finetuning. MTL + Finetune first conducts Multi-Task-Learning (MTL) before finetuning. Specifically, we train 9 datasets simultaneously with separate label space in MTL. To evaluate the effectiveness of DSI, we use the following methods as baselines based on MTL + Finetune (best models): AEDA (Karimi et al., 2021): randomly inserts punctuation marks into the original text for data augmentation. Back translation (Edunov et al., 2018): first translates sentences into certain intermediate languages and then translates them back. Cutoff (Shen et al., 2020): masks tokens, spans, and features in the input space. Artifacts-removal/ masking (Ramponi and Tonelli, 2022): removes (or masks) any occurrence of spurious lexical artifacts (based on PMI) for training and validation data."
        },
        {
            "heading": "4.3 Main results",
            "text": "The main experiment results are shown in Table 1 and 2, respectively. As shown in Table 1, we observe that LLMs\u2019 performances on different datasets vary greatly. Besides, the performance of few-shot LLMs can not outperform zero-shot consistently, which is different from other tasks. As a result, these results indicate that the off-the-shell LLMs is not always a good HS detector, which is consistent with the results in (Ziems et al., 2023). One of the reasons behind this may be that HS\u2019s grammatical structure may not conform to the standard grammatical norms. For example, the authors of HS online hardly use complete sentences. As a result, HS datasets contain a large proportion of such unregulated sentences, which may confuse the off-the-shell GPT model. Among the LLMs we use, text-davinci-003 performs the best while ChatGPT has the worst performance. On the other hand,\n3Implementation details can be found in Appendix A.6.2"
        },
        {
            "heading": "Methods F1 score",
            "text": "the finetune-based model can learn these abnormal structures through training data. Furthermore, for the two MTI methods, MTL can further consistently improve the performance (8.61 F1 score), indicating the importance of getting a robust representation of HS. Besides, MTL is orthogonal to other data-specific approaches (DA, data-specific debias methods). Therefore, the following experiments in Table 2 are all based on MTL. From Table 2, we observe that our proposed DSI can outperform other baselines on average and have a noticeable improvement (2.46 F1 score) over MTL. Besides, we find that token-level DA methods can even impair the overall performance, indicating that these methods are not as effective as in other tasks. On the other hand, three hiddenlevel DA methods do have a positive impact on the performance. Besides, previous debias methods such as artifact removal or masking also boost the performance but the improvement is subtle."
        },
        {
            "heading": "4.4 Generalization Analysis",
            "text": "Joshi et al. claim that building a \"challenge set\" to see if the intervention of the input cause model predictions to vary expectedly is a standard method of testing a model\u2019s robustness. Hence, to verify the robustness of our method, we construct an OOD challenge set using non-hateful counterfactual data generated by GPT-3 and hateful data from CONAN (Bonaldi, Helena and Dellantonio, Sara and Tekirog\u0306lu, Serra Sinem and Guerini, Marco, 2022). Details on OOD datasets can be found in Appendix A.3. Given MTL\u2019s strong performances in Table 1, we evaluate other baselines based on MTL, as shown in Table 3. Our proposed DSI outperforms other methods by a larger margin than that in Ta-\nble 2, indicating the effectiveness of our method\u2019s robustness and great generalization ability."
        },
        {
            "heading": "4.5 Deep Dive of Sentence-level Biases",
            "text": "This part takes a deep dive into these sentence-level biases via qualitative analyses and visualization of their relative spuriousness distributions."
        },
        {
            "heading": "4.5.1 Potential Data-Specific Biases",
            "text": "After conducting grammar induction and investigating 180 grammar patterns, we observe the following categories with high PMI with the hateful label: (1) Absolute expression (8.8%): A large number of HS sentences contain absolute statements, where patterns like all the, all other, any other, etc. frequently occur. (2) Hashtag before \u2019bad\u2019 words: (7.2%) In online datasets, hashtags are common patterns. We observe that hashtags before some bad words (e.g., # dickhead, # Liar, # Thief etc.) are highly correlated with hateful labels. (3) Aggressive actions (6.7%): A large number of HS contain radical action words (e.g., fuck anything, kill yourself, etc.). (4) All capitalization (2.8%): All-caps patterns such as FUCK YOU and HAVE NEVER are more likely to be found in hateful sentences. In addition to the aforementioned types, there are other types of spurious patterns without cohesive themes in HS (see Table 6 for some examples). We also find that different datasets have different patterns. However, not all of the above patterns are spurious. The following section illustrates the RS distribution of these data-specific biases."
        },
        {
            "heading": "4.5.2 Bias Distribution Analysis",
            "text": "For potential data-specific biases, we use RS described in Section 3.1.2 to validate their spuriousness. We visualize the distribution of these biases based on our proposed RS in Figure 4 in the Appendix. We find that most data-specific patterns\u2019 RS is positive, indicating that these highlycorrelated artifacts do make the model more likely to predict a particular label. However, the experiment result shows that most of their RS is less than 0.2. As a result, most of the data-specific biases\u2019 impact on the HS detector\u2019s output is minimal. This is because most of these biased patterns do not contain hateful semantics. Although they are highly correlated with a specific label statistically, PLM can ignore them to some extent during inference."
        },
        {
            "heading": "4.6 Case Study of Token-Level Biases",
            "text": "To further analyze our proposal\u2019s effectiveness on identity biases, we analyze the errors made by the\nbaseline model that is solved by our approach. As a result, we conduct the following case study on one of our 9 datasets used in this work, i.e., the white supremacist forum dataset (de Gibert et al., 2018). We randomly sample four false positive sentences where the baseline model makes mistakes in the test set. As shown in Table 4, the baseline model can not correctly identify some complex non-hateful examples containing identity words. For example, the sentences containing ethnicity-related words (e.g., jews, Blacks, hebrews, and Arabs) confuse the baseline detector. Besides, gender-related words (e.g., women) may also make the model more likely to classify the sentence as hateful. However, these sentences are non-hateful based on their neutral contexts. Inspired by saliency methods (Simonyan et al., 2013; Sundararajan et al., 2017; Smilkov et al., 2017; Balkir et al., 2022), we compute the probability differences between original sentences and identity-masked sentences to better interpret the model prediction. The average differences in the above samples are 0.76 and 0.04 for the baseline detector and our proposal, indicating that the impact of these identity words has been considerably reduced by our approach."
        },
        {
            "heading": "5 Conclusion",
            "text": "This work investigates biases in hate speech detection from lexical and sentence levels. Apart from the statistical correlation between artifacts and a specific label, we analyze the relative spuriousness of the feature based on its impact on local and global models. We find that most highly-correlated pattern features do not have high RS. After that, we analyze the generation process of HS biases from a causal view. We identify two confounders that cause the biases and propose Multi-Task Intervention from the model level and Data-Specific Intervention from the data level to mitigate them. Noticeable performance improvements on nine HS datasets and a label-balance challenge set indicate the effectiveness and robustness of our approach."
        },
        {
            "heading": "Limitations",
            "text": "Our work is subject to a few limitations. First, our experiments are limited to English datasets. However, multilingual hate speeches are also frequently found on many social media platforms. There are still many challenges in languages other than English, especially some minor languages. A thorough examination of our methods\u2019 effec-\ntiveness in languages other than English is necessary, which we leave as future work. Second, we use BERT-base-cased as the PLM backbone for most HS detectors following Ramponi and Tonelli, 2022, and we add GPT-3, ChatGPT and text-davinci-003 as baselines. Other PLMs of different scales or architectures\u2019 robustness on various biases needs to be verified. Moreover, we only examine and mitigate biases in explicit HS datasets following previous works. However, biases in implicit HS (ElSherief et al., 2021b) are also important and we leave it as future work. In addition, our proposed RS and mitigation methods are applicable to any other text classification tasks, which we leave as future work. Besides, diverse prompt designs for counterfactual generators also need to be validated in future works. In this work, we only focus on single-turn HS detection tasks following previous works without considering user contexts (Yu et al., 2022), which is another practical setting. Finally, we find that some data is falsely annotated or based on different datasets. We do not modify them for fair comparisons. However, it is necessary to uniform the criteria for HS and correct problematic annotations. For token-level biases, although a large number of them are identity-related, there are still other highly-correlated tokens. We do not investigate them and leave it as future work."
        },
        {
            "heading": "Ethics Statement",
            "text": "Our proposals in the research aim to mitigate biases and accurate detection of HS. The main datasets utilized in the study are open-access and publicly available. The offensive terms and identity-related words included as examples are mainly used to help researchers analyze the models more effectively. We do not involve annotators in this process given the sensitive nature of this work, and to reduce exposure of hateful content to human participants. We also add a content warning in the beginning of this paper to warn readers."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Point-wise Mutual Information (PMI)",
            "text": "According to Gururangan et al., PMI is defined as following:\nPMI(t, c) = log p(t, c)\np(t, \u00b7)p(\u00b7, c) We use this equation to find general (identity) biases in hate speech datasets."
        },
        {
            "heading": "A.2 Dataset Statistics",
            "text": "Table 5 illustrates the statistics of datasets for MLM, MTL, and baseline models. The number of generated counterfactual examples after majority filtering are { MM: 631, WSF: 779, Fox: 558, HASOC: 872, ETHOS: 873, TSA: 250, AHS: 579, AOH: 1072, ZH: 520 }. Because many hate speech datasets do not have a specific split, we employ a 60-20-20 split for them."
        },
        {
            "heading": "A.3 Details on OOD Datasets",
            "text": "We find that the phenomenon of label imbalance exists for many current HS datasets. Specifically, a large proportion of HS datasets contain much more hateful data instances than non-hateful ones in both train and test sets. As a result, to further mitigate the influence of such label imbalance, we construct a label-balance OOD challenge set containing five frequently occurring vulnerable groups, including Muslims, Women, LGBT, POC, and Jews."
        },
        {
            "heading": "Dataset # train # dev # test",
            "text": "Each group contains 15 hateful and non-hateful sentences. The total size of the challenge set is 150."
        },
        {
            "heading": "A.4 Details for Counterfactual Generator",
            "text": "For token-level general biases (identity), we use the following example prompt for GPT-3.\nGenerate 20 non-hateful sentences containing \u2019Jews\u2019:\nFor data-specific biases, we use the following prompt.\nParaphrase the original sentence without using \u2019Trump is\u2019: Original sentence: Trump is the face of corporate greed. And the poor love him? Ironic. Paraphrased sentence:\nWe set the temperature to 1, max_tokens to 4000, top_p to 1.0, frequency_penalty to 1, and presence_penalty to 1 for GPT-3. The RS threshold we select is 0.2 for data-specific artifacts.\nA.5 Implementation Details Following ElSherief et al., 2021b and Ramponi and Tonelli, 2022, we use a grid search to find the most suitable hyperparameters including learning rate [2e-5, 3e-5, 5e-5, 8e-6, 5e-6], batch size [4, 8, 10] and the number of epochs [1, 2, 3, 4]. We repeat every experiment three times using different random seeds and select the best models based on average macro F1 on validation sets. Other hyperparameters are set to the default one in HuggingFace transformers\u2019 trainer. All experiments are\nconducted on NVIDIA RTX 2080 Ti with 11GB memory. Every run of experiments can be finished within an hour."
        },
        {
            "heading": "A.6 Set up for Baselines",
            "text": ""
        },
        {
            "heading": "A.6.1 Prompt Design for GPT-3 Baselines",
            "text": "Following Chiu et al., 2021, we use the following prompts for LLMsbaselines: Zero-shot:\nIs the following text hateful? Answer yes or no. \u2019Men can see beauty, so they don\u2019t want to sleep with black women or make goblin babies.\u2019 Answer:\nFew-shot:\nClassify the following texts into \u2019non-hateful\u2019 or \u2019hateful\u2019: \u2019My best friend is muslim. And he is the best man in the world\u2019: non-hateful \u2019He seems like the very talkative taxi driver\u2019: non-hateful \u2019Men can see beauty, so they don\u2019t want to sleep with black women or make goblin babies.\u2019: hateful. \u2019Islam is a religion of hate. These propaganda type videos are dishonest. Simply read the Quran and Hadith.\u2019: hateful. \u2019Kill all muslims\u2019:"
        },
        {
            "heading": "A.6.2 Other Detail for Baselines",
            "text": "For AEDA, we generate four additional examples for augmentation for one original sentence. For LLMs\u2019 baselines, we randomly select 60 examples from the test sets and use the powerful text-davinci002, text-davinci-003 and text-chat-davinci-002 through openai API and set the temperature to 0, max_tokens to 5, top_p to 1.0, frequency_penalty to 0, temperature to 0.7 and presence_penalty to 0. For MLM, we randomly mask 15% tokens and apply the default hyperparameters from the HuggingFace transformers trainer. Back translation is one of the commonly-used sentence-level DA methods. We use German as the intermediate language and Fairseq (Gehring et al., 2017) machine translation model pretrained on WMT19 (Foundation). We also try to combine two kinds of MTI including MTL and MLM by some weights. How-\never, this combination can not yield a noticeable performance improvement."
        },
        {
            "heading": "A.7 Examples of Data-Specific Biases",
            "text": "Apart from the categories in Section 4.5.1, a large number of data-specific biases can hardly be categorized.As a result, we do not list it on the tables. We listed Top-5 data-specific biases in Table 6."
        }
    ],
    "title": "Mitigating Biases in Hate Speech Detection from A Causal Perspective",
    "year": 2023
}