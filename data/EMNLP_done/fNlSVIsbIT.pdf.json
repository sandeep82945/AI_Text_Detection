{
    "abstractText": "We propose an instruction-based process for trustworthy data curation in materials science (MatSci-Instruct), which we then apply to finetune a LLaMa-based language model targeted for materials science (HoneyBee). MatSciInstruct helps alleviate the scarcity of relevant, high-quality materials science textual data available in the open literature, and HoneyBee is the first billion-parameter language model specialized to materials science. In MatSciInstruct we improve the trustworthiness of generated data by prompting multiple commercially available large language models for generation with an Instructor module (e.g. ChatGPT) and verification from an independent Verifier module (e.g. Claude). Using MatSciInstruct, we construct a dataset of multiple tasks and measure the quality of our dataset along multiple dimensions, including accuracy against known facts, relevance to materials science, as well as completeness and reasonableness of the data. Moreover, we iteratively generate more targeted instructions and instructiondata in a finetuning-evaluation-feedback loop leading to progressively better performance for our finetuned HoneyBee models. Our evaluation on the MatSci-NLP benchmark shows HoneyBee\u2019s outperformance of existing language models on materials science tasks and iterative improvement in successive stages of instruction-data refinement. We study the quality of HoneyBee\u2019s language modeling through automatic evaluation and analyze case studies to further understand the model\u2019s capabilities and limitations. Our code and relevant datasets are publicly available.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Yu Song"
        },
        {
            "affiliations": [],
            "name": "Santiago Miret"
        },
        {
            "affiliations": [],
            "name": "Huan Zhang"
        },
        {
            "affiliations": [],
            "name": "Bang Liu"
        }
    ],
    "id": "SP:d728abd94f82a578a2f7504b8ab212f2a76eaea8",
    "references": [
        {
            "authors": [
                "Nicholas Joseph",
                "Sam McCandlish",
                "Tom Brown",
                "Jared Kaplan"
            ],
            "title": "Constitutional ai: Harmlessness from ai feedback",
            "year": 2022
        },
        {
            "authors": [
                "Daniil A Boiko",
                "Robert MacKnight",
                "Gabe Gomes."
            ],
            "title": "Emergent autonomous scientific research capabilities of large language models",
            "venue": "arXiv preprint arXiv:2304.05332.",
            "year": 2023
        },
        {
            "authors": [
                "Andres M Bran",
                "Sam Cox",
                "Andrew D White",
                "Philippe Schwaller."
            ],
            "title": "Chemcrow: Augmenting large-language models with chemistry tools",
            "venue": "arXiv preprint arXiv:2304.05376.",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Zongwei Zhou",
                "Xuezhi Wang",
                "Brennan Saeta",
                "Mark Diaz",
                "Orhan Firat",
                "Michele Catasta",
                "Jason Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Gao",
                "Rong Tan",
                "Guanghui Li."
            ],
            "title": "Research on text mining of material science based on natural language processing",
            "venue": "IOP conference series: materials science and engineering, volume 768, page 072094. IOP Publishing.",
            "year": 2020
        },
        {
            "authors": [
                "Tanishq Gupta",
                "Mohd Zaki",
                "Devanshi Khatsuriya",
                "Kausik Hira",
                "N M Anoop Krishnan",
                "Mausam ."
            ],
            "title": "DiSCoMaT: Distantly supervised composition extraction from tables in materials science articles",
            "venue": "Proceedings of the 61st Annual Meeting of the",
            "year": 2023
        },
        {
            "authors": [
                "Tanishq Gupta",
                "Mohd Zaki",
                "NM Krishnan"
            ],
            "title": "Matscibert: A materials domain language model for text mining and information extraction",
            "venue": "npj Computational Materials,",
            "year": 2022
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark"
            ],
            "title": "Training compute-optimal large language models",
            "year": 2022
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "year": 2021
        },
        {
            "authors": [
                "Shu Huang",
                "Jacqueline M Cole."
            ],
            "title": "Batterybert: A pretrained language model for battery database enhancement",
            "venue": "Journal of Chemical Information and Modeling.",
            "year": 2022
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant"
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Yunxiang Li",
                "Zihan Li",
                "Kai Zhang",
                "Ruilong Dan",
                "You Zhang"
            ],
            "title": "Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge",
            "year": 2023
        },
        {
            "authors": [
                "Sourab Mangrulkar",
                "Sylvain Gugger",
                "Lysandre Debut",
                "Younes Belkada",
                "Sayak Paul."
            ],
            "title": "Peft: Stateof-the-art parameter-efficient fine-tuning methods",
            "venue": "https://github.com/huggingface/peft.",
            "year": 2022
        },
        {
            "authors": [
                "Elsa A Olivetti",
                "Jacqueline M Cole",
                "Edward Kim",
                "Olga Kononova",
                "Gerbrand Ceder",
                "Thomas Yong-Jin Han",
                "Anna M Hiszpanski."
            ],
            "title": "Data-driven materials research enabled by natural language processing and information extraction",
            "venue": "Applied Physics Reviews,",
            "year": 2020
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "openaiintroducingchatgpt",
            "venue": "https: //openai.com/blog/chatgpt. [Accessed 22-Jun2023].",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Jeff Stanway",
                "Lorrayne Bennett",
                "Demis Hassabis",
                "Koray Kavukcuoglu",
                "Geoffrey Irving"
            ],
            "title": "Scaling language models: Methods, analysis & insights from training gopher",
            "year": 2022
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Yu Song",
                "Santiago Miret",
                "Bang Liu."
            ],
            "title": "Matscinlp: Evaluating scientific language models on materials science language tasks using text-to-schema modeling",
            "venue": "arXiv preprint arXiv:2305.08264.",
            "year": 2023
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Nicholas Walker",
                "Amalie Trewartha",
                "Haoyan Huo",
                "Sanghoon Lee",
                "Kevin Cruse",
                "John Dagdelen",
                "Alexander Dunn",
                "Kristin Persson",
                "Gerbrand Ceder",
                "Anubhav Jain"
            ],
            "title": "The impact of domain-specific pre-training on named entity recognition tasks",
            "year": 2021
        },
        {
            "authors": [
                "Haochun Wang",
                "Chi Liu",
                "Nuwa Xi",
                "Zewen Qiang",
                "Sendong Zhao",
                "Bing Qin",
                "Ting Liu"
            ],
            "title": "Huatuo: Tuning llama model with chinese medical knowledge",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560.",
            "year": 2022
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Natural language processing (NLP) holds considerable promise in expediting the discovery and un-\n\u2217 Equal advising. \u2020 Corresponding author. Canada CIFAR AI Chair.\n1https://github.com/BangLab-UdeM-Mila/NLP4MatSciHoneyBee\nderstanding of novel material systems, which will be crucial for addressing contemporary societal challenges like climate change and drug discovery. The potential impact of NLP in materials science is chiefly underpinned by the vast reservoir of materials science knowledge contained in text-based resources, such as textbooks, scientific journals, and assorted reports. In spite of the prospective richness of materials science textual data available from diverse sources, a number of challenges continue to significantly hinder the effective digestion and comprehension of relevant materials science textual knowledge (Song et al., 2023; Kononova et al., 2021). Some of the challenges relate to the general availability of data, while other relate to the ability to effectively process domain-specific information, such as chemical notation and data contained in figures and tables (Gupta et al., 2023). This scarcity of readily accessible, high-quality text corpora suitable for efficient language model training has in turn slowed the development of comprehensive language models capable of spanning the extensive conceptual range within the highly interdisciplinary materials science field.\nWhile data availability remains an ongoing challenge in applying modern NLP tools for materials science, recent advancements have led to the emergence of large language models (LLMs) proficient in handling general language tasks that concurrently demonstrate substantial aptitude in areas like chemistry and materials science (Bran et al., 2023; Boiko et al., 2023). Such advancements provide the potential to harness the implicit knowledge encapsulated in these models, which have been trained on vast text corpora spanning a broad range of subjects, to generate accessible, instruction-based datasets for specialized domains like materials science.\nYet, while we can generate targeted instructionbased data to make applying NLP for materials science more accessible, the quality of these instructions requires rigorous evaluation before be-\ning utilized for language model training. This is particularly salient in the context of complex scientific applications like materials science, which encompasses a wide range of subfields that together describe the properties and behavior of matter that make up materials systems. This need for trustworthy and pertinent instructions necessitates the creation of a robust process to validate the quality of instructions for downstream applications.\nAside from data scarcity in scientific domains, another significant impediment to the application of NLP in materials science is the limited presence of specialized language models that incorporate both in-depth materials science knowledge and a robust understanding of general language. The bulk of today\u2019s available language models for materials science are built on the BERT architecture (Gupta et al., 2022; Walker et al., 2021; Huang and Cole, 2022), whose performance in general NLP tasks has been superseded by several more advanced language model architectures in recent years (Touvron et al., 2023; Scao et al., 2022; Brown et al., 2020; Chung et al., 2022). This highlights the need for the development of more capable language models in materials science that can accommodate a\nbroader knowledge base while effectively performing pertinent materials science language tasks.\nThis paper seeks to concurrently address the previously outlined challenges of trustworthy instruction generation and capable, open-source language models for materials science. We propose MatSciInstruct to generate reliable, instruction-based data from large language models. This data is then used to train HoneyBee, a billion-parameter specialized materials science language model based on the LLaMa architecture (Touvron et al., 2023). The key contributions of our research are as follows:\n\u2022 MatSci-Instruct \u2013 A Two-Step Framework for Trustworthy Instruction-Data Generation: We propose a universally applicable methodology suited for instruction-data generation in scientific domains. MatSci-Instruct generates specialized instruction-data using a two-step framework \u2013 Generation and Verification. In the Generation step, an instructor model (Chat-GPT 2) creates domain-specific instruction-data focused on materials science. During the Verification step, the instructiondata are cross-verified by a separate verifier model (Claude 3) for accuracy and relevance as shown by the example in Figure 1. Moreover, in Section 4.1 we conduct human evaluations that suggest good alignment of our generated MatSci-Instruct Dataset with human experts across several dimensions: accuracy against known facts, relevance to materials science, and the completeness and reasonableness of the language model output.\n\u2022 HoneyBee \u2013 A High-Performance LLaMaBased Model Progressively Trained via MatSci-Instruct: Utilizing the MatSciInstruct two-step framework, we apply a Progressive Refinement-Feedback strategy to finetune a LLaMa model, culminating in the HoneyBee model. In our progressive finetuning strategy, the HoneyBee model\u2019s performance on MatSci-Instruct data guides subsequent instruction-data generation. This iterative process results in further refined instructions to generate higher-quality instructiondata for finetuning, ensuring the progressive acquisition of specialized knowledge by the\n2https://platform.openai.com/docs/ api-reference/chat\n3https://docs.anthropic.com/claude/docs\nmodel. We evaluate the performance of HoneyBee using a materials science language benchmark (Song et al., 2023), and thoroughly analyze its strengths and limitations."
        },
        {
            "heading": "2 Related Work",
            "text": "Large Language Models Large Language Models (LLMs) have gained substantial attention from the NLP research and wider technology communities due to their remarkable proficiency in language understanding and generative tasks. Pioneers like GPT-3 (Brown et al., 2020), with its 175 billion parameters, demonstrated the capacity to capture complex linguistic patterns, and subsequent models like Gopher (Rae et al., 2022), GLM (Zeng et al., 2022), PaLM (Chowdhery et al., 2022), BloomZ (Scao et al., 2022), Chincilla (Hoffmann et al., 2022), and OPT (Zhang et al., 2022) continue to drive progress. Commercial models like ChatGPT (OpenAI, 2022) and Claude (Bai et al., 2022) further expand the landscape of performant LLMs. Compared to commercial LLMs, LLaMa (Touvron et al., 2023) stands out for its greater accessibility and good performance, offering an efficient and accessible platform for domain-specific finetuning in various domains, including materials science.\nNLP for Materials Science NLP applications within materials science are constrained by the dual shortage of openly accessible, high-quality data and high-performing language models. While strides have been made towards enhancing data availability (Song et al., 2023; Olivetti et al., 2020; Kononova et al., 2021; Gao et al., 2020), the primary focus has been on generating expertannotated data for finetuning BERT-based models, which lack the advanced capabilities of contemporary LLMs. For a detailed review of the performance of various BERT models on materials science language tasks, we refer the reader to Song et al. (2023). The prevailing scarcity of data and specialized LLMs in materials science motivates us to propose MatSci-Instruct, an instruction-based method for data creation, and HoneyBee, a specialized LLM tailored for materials science.\nInstruction Finetuning LLMs LLMs consistently demonstrate substantial improvements when finetuned for specialized tasks, as seen with biomedical models like ChatDoctor (Li et al., 2023) and HuaTuo (Wang et al., 2023). While the large model size of LLMs poses a challenge for effective finetuning, several efficient methods have been\nproposed (Mangrulkar et al., 2022), such as PTuning (Liu et al., 2021), Prefix Tuning (Li and Liang, 2021), Prompt Tuning (Lester et al., 2021), and LoRA (Hu et al., 2021). Among these, LoRA utilizes low-rank matrix decomposition to limit the additional parameters required for fine-tuning. For data curation in specialized fields, instructionsbased fine-tuning extracts detailed data directly from LLMs (Ouyang et al., 2022), reducing human annotation effort and providing scalable solutions. For example, Alpaca (Taori et al., 2023; Wang et al., 2022) exploits LLMs to generate synthetic instructions for model finetuning. However, LLMsynthesized data still suffer from data quality issues, which is especially critical for scientific domains. To address these concerns, we design a generationverification strategy for trustworthy data generation and a progressive refinement-feedback strategy for finetuning LLMs on specialized instructions."
        },
        {
            "heading": "3 Method",
            "text": "Our work consists of two interacting components: 1) MatSci-Instruct: a trustworthy instruction generation framework for obtaining scientific textual data from LLMs; 2) HoneyBee: a materials science LLM progressively finetuned from LLaMA (Touvron et al., 2023) using MatSci-Instruct generated data. We connect HoneyBee to MatSci-Instruct with a refinement-feedback loop to progressively generate new data and finetune HoneyBee based on its training status as shown in Figure 2."
        },
        {
            "heading": "3.1 MatSci-Instruct",
            "text": "The challenges of cost-effectively generating highquality instruction data are not unique to materials science, but rather, pervasive across various scientific domains. Our proposed solution, MatSciInstruct, is an innovative, domain-agnostic methodology that leverages the power of large language models (LLMs) to generate specialized instruction sets for subsequent model finetuning.\nDepicted in Figure 2, MatSci-Instruct employs a trifecta of distinct LLMs. The Instructor model crafts instructions using structured prompts encapsulating topic and task details. The Verifier then evaluates these instructions against accuracy, relevance, completeness, and reasonableness criteria, ensuring only dependable instructions advance to finetuning. Finally, the Evaluator assesses the output of the finetuned model along similar dimensions as the Verifier. Poorly executed instructions"
        },
        {
            "heading": "Model capability evaluation:",
            "text": "are flagged for further refinement, verification, and evaluation. Ultimately, we generate 52k instructions spanning content-based and open-ended tasks, some of which include empty inputs. Table 1 shows that the number of instructions gets reduced in later stages of the progressive-refinement-feedback loop mainly due to greater emphasis on quality."
        },
        {
            "heading": "3.1.1 MatSci-Instruct Example",
            "text": "Next, we show a full example of the progressive MatSci-Instruct instruction-data refinement procedure in the loop with HoneyBee model finetuning:\n1. Instruction-Data Generation and Finetuning\n\u2022 Data Generation: Instructor generates training data (data_train0[1\u2212 10]).\n\u2022 Data Verification: Verifier removes lowscoring data (data_train0[1, 2])\n\u2022 Finetuning: LLaMa-7b becomes HoneyBee-7b-Stage-1 after finetuning with data from (data_train0[3\u2212 10]).\n2. Evaluation\n\u2022 HoneyBee-7b-Stage-1 performs inference on new test data (data_test0), crafted by the Instructor, with outputs evaluated by the Evaluator.\n3. Feedback Response\n\u2022 Response Generation: HoneyBee-7bStage-1 generates responses for the test data (data_test0). \u2022 Scoring Responses: The Evaluator spots weak responses (data_test0[7, 8]).\n4. Instruction-Data Adaptation and Refinement\n\u2022 Focusing on Weaknesses: The Instructor crafts more training (data_train1) and test data (data_test1), focusing on issues identified by the Evaluator.\n\u2022 Finetuning Stage 2: HoneyBee-7b-Stage1 refines to HoneyBee-7b-Stage-2.\n\u2022 Re-Evaluation: HoneyBee-7b-Stage-2 is tested with new test data (data_test1).\nThis process is repeated in an iterative feedback loop for continued refinement as shown in Figure 2."
        },
        {
            "heading": "3.1.2 Instructor Module",
            "text": "The Instructor module of our framework, embodied by ChatGPT, performs the generation of material science instruction-data. This module employs a concise instruction prompt schema composed of three elements: <instruction>, <input>, and <output>. The <instruction> outlines the task using a standardized NLP task set, the <input>\ncontains the relevant data, and the <output> generates a pertinent response to the task.\nWe query ChatGPT with this schema, populating the <instruction> and <input> fields with a selection of 20 NLP tasks and 20 materials science subtopics shown in Figure 3, to ensure task and content diversity. These selections are manually verified before they are utilized in structured prompts for generating detailed finetuning instruction-data. Detailed lists of prompts and materials science topics are available in Appendix B and Appendix E.\nFollowing the schema, we engage in a random sampling process, selecting five candidate topics and five tasks, then applying them to the instruction prompts for data generation. For robustness, we direct ChatGPT to flag in the <output> field any instruction that cannot be processed based solely on the <input> and <instruction>. To control task difficulty and boost diversity, we occasionally limit the length of <instruction> or <output>.\nTo enhance the diversity and robustness of the instruction-data generation process, our design incorporates several additional strategies. One such strategy employs an open-ended task where the <input> field remains intentionally blank, allowing the model to generate responses without predefined constraints. This approach tests the generative abilities of the model under uncertainty and promotes more varied outcomes. Another key strategy is content-based instruction-data generation. Instead of relying on predefined topics and tasks, this approach utilizes real-world materials science literature. We select a random open-access paper from the materials science category on arXiv and extract a specific fragment to fill the <input> field.\nThis method not only diversifies the task prompts but also aligns the generated instruction-data more closely with practical, domain-specific contexts.\nTo conclude the instruction-data generation process, ChatGPT compiles ten representative instruction prompt samples from all possible options. These samples are formatted in a standardized JSON format, readily available for use in the subsequent steps of the MatSci-Instruct process. This approach ensures a comprehensive and diverse set of instructions, which in turn contributes to a robust and adaptable language model during finetuning."
        },
        {
            "heading": "3.1.3 Verifier Module",
            "text": "Generating high-quality instruction-data can be challenging, and the presence of low-quality data in finetuning a model can lead to misleading results. To address this issue, MatSci-Instruct employs a two-step framework by incorporating a Verfier model to improve the trustworthiness of generated data. Specifically, we use Claude as the Verifier to ensure the quality of the instructions generated by the Instructor (Chat-GPT).\nOur evaluation is based on four dimensions: accuracy, relevance, completeness, and reasonableness. Similar to the instruction-data generation, instruction verification is based on a standard set of prompts, shown in Appendix E, which include precise definitions of the evaluation criteria along with the complete instructions generated by the Instructor. Concretely, the evaluation criteria are:\n\u2022 Accuracy: The accuracy of the instructiondata is evaluated by comparing it with known facts or credible sources. This involves checking the accuracy of any claims or statements made in the text and verifying that they are supported by evidence.\n\u2022 Relevance: The relevance of the instructiondata is assessed by determining how directly it relates to materials science. This is achieved by analyzing the text\u2019s content and ascertaining its applicability to the field.\n\u2022 Completeness: Completeness is an essential dimension to ensure the instruction-data comprehensively address the given task, inclusive of all sub-questions. This involves considering both depth and conciseness to ensure that the output is complete and comprehensive.\n\u2022 Reasonableness: The reasonableness of the instruction-data is about logical consistency.\nThis dimension ensures no evident contradictions exist within the generated data.\nThe verifier module (i.e., Claude) evaluates the instruction-data based on the four dimensions mentioned above and identifies any low-quality data that falls below a predetermined threshold. This rigorous verification ensures the use of high-quality data in model fine-tuning, thereby improving the overall efficacy and accuracy of the system. Our verification protocol is designed for modularity and extensibility. This modular design facilitates the incorporation of additional agents into a multi-agent system, each assessing instruction-data based on pre-defined criteria. The final decision on data quality is then reached through a consensus mechanism, augmenting the robustness and comprehensiveness of the verification process, ensuring high-quality data for model finetuning."
        },
        {
            "heading": "3.1.4 Evaluator Module",
            "text": "The Evaluator model assesses the output of the HoneyBee language model along similar evaluation dimensions as the Verifier, namely: accuracy, completeness, and reasonableness. We no longer consider relevance at this stage since the verification step filtered out all instructions with little relevance to materials science. In this paper, we use GPT-4 4 (OpenAI, 2023) as the Evaluator model, which provides an additional independent LLM that is different, and potentially more advanced, than the Instructor and Verifier LLMs. The Evaluator also helps with the identification of poorly formulated instructions according to the performance of the HoneyBee model. These instructions are then passed back to the Instructor for additional iterative refinement."
        },
        {
            "heading": "3.2 HoneyBee",
            "text": "Upon obtaining a set of trustworthy instruction data from the Verifier, we can use the generated instruction dataset to finetune a LLaMa-based model for a specific domain. In this work, we finetune a model for materials science using a progressive finetuning technique to convert a standard LLaMa model to a specialized model in material science: HoneyBee."
        },
        {
            "heading": "3.2.1 Progressive Instruction Finetuning",
            "text": "In our approach, as depicted in Figure 2, we harness a progressive instruction finetuning methodology that relies on a feedback loop. This loop enables\n4https://openai.com/research/gpt-4\nthe progressive generation of new instruction data that takes into account the evaluated model\u2019s performance on different criteria, tasks, and topics.\nInstructions leading to suboptimal performance by HoneyBee are returned to the Instructor, triggering the creation of more detailed and targeted instructions for future iterations. This iterative process also includes instruction evaluation by the Instructor, enabling the generation of more precise instruction data for subsequent rounds. For instance, should HoneyBee score low on \u2019Completeness\u2019 for a particular instruction, we inform the Instructor of this deficiency, providing the criteria for \u2019Completeness\u2019. Consequently, the Instructor generates enhanced instruction-data to improve HoneyBee\u2019s completeness in responding to similar tasks.\nOur progressive finetuning process for the language model is based on LoRA (Hu et al., 2021), where we create and train a separate set of lowrank matrices \u03c8 that bypass the need for changing the actual parameters of the language model \u03d5. Since \u03c8 consists of low rank-matrices, it is significantly more parameter and compute efficient for model finetuning. In our finetuning process, we assume that the Instructor + Verifier models act as the teacher model and the HoneyBee model acts as the student model. In this setting, the student model will continually learn from the instruction-data and undergo testing during the learning process, allowing us to monitor its performance in real-time. The finetuning process continues for a set number of epochs with early stopping if the student model converges to a given loss value. Next, we evaluate the response quality of the student model for any given instruction with the Evaluator. In our progressive finetuning strategy, we monitor the evaluation scores after each stage, denoted as Svalbest , and terminate the process when the Svalbest stops yielding significant improvements. In our experiments in Section 4, we perform three stages of progressively finetuning both the instruction-data and the HoneyBee model parameters."
        },
        {
            "heading": "4 Experiments",
            "text": "Our experiments focus on assessing the ability of MatSci-Instruct to create high-quality, trustworthy instruction-data relevant to materials science, as described in Section 3.1, along with understanding the capabilities and limitations of HoneyBee."
        },
        {
            "heading": "4.1 MatSci-Instruct Evaluation",
            "text": "A critical piece of the MatSci-Instruct pipeline is the independent verification and evaluation of the instruction-data generated by the Instructor model. Given the importance of the Verifier and Evaluator in ensuring the quality of the instruction-data, and the fact that understanding materials science textual data requires deep domain understanding, we conducted an evaluation with human experts on the trustworthiness of the instructions generated by MatSci-Instruct. In our human expert evaluation, we asked two graduate students majoring in material science to evaluate 50 randomly selected instruction data along the same evaluation dimensions as the Verifier module (accuracy, relevance, completeness, reasonableness). Next, we conducted a verification and evaluation of the same 50 instructions using Claude and GPT-4 respectively. We measure agreement between the human experts and the LLMs by calculating Spearman and Pearson correlation coefficients between the scores along each of the dimensions.\nAs shown in Figure 4, both Claude and GPT-4 had correlation coefficients higher than 0.6 for each dimension and an overall coefficient as high as 0.8 when compared to manual evaluation. This indicates a decent level of consistency between manual and automatic evaluations for a random sample of instructions, which gives us confidence in the ability of MatSci-Instruct to generate trustworthy, high-quality instructions for HoneyBee finetuning."
        },
        {
            "heading": "Model Accuracy Completeness Reasonableness",
            "text": ""
        },
        {
            "heading": "Zero-Shot LLMs",
            "text": ""
        },
        {
            "heading": "HoneyBee without Verfication",
            "text": ""
        },
        {
            "heading": "HoneyBee with MatSci-Instruct",
            "text": "highlight scores that outperform Chat-GPT."
        },
        {
            "heading": "4.2 HoneyBee Task Evaluation",
            "text": "We generated a test set of instruction-data with the same generation process as the train set that is only introduced it during model evaluation. The results in Table 2 show that HoneyBee gets progressively better with each iteration of MatSci-Instruct for both HoneyBee-7b and HoneyBee-13b. HoneyBee without verification also outperforms LLaMA and Alpaca LLMs of equal size indicating the value of the progressive finetuning approach on specialized materials science instruction-data. HoneyBee-13b closely matches, and in some exceeds, the evaluation performance of Chat-GPT which served as the Instructor. Notably, HoneyBee-13b is \u223c 10x more parameter efficient than GPT-3."
        },
        {
            "heading": "4.3 HoneyBee Performance on MatSci-NLP",
            "text": "In addition to evaluating the performance of HoneyBee based on LLM assessment in Section 4.2, we investigate the performance of HoneyBee on MatSci-NLP, a broad benchmark of materials science NLP tasks (Song et al., 2023). We study HoneyBee\u2019s performance under two settings: 1. Lowdata training setting as applied in the original paper by Song et al. (2023); 2. Zero-shot performance\nTable 3: Low-resource finetuning and zero-shot evaluation results for various HoneyBee on MatSci-NLP tasks. For low-resource finetuning, we follow the method described in Song et al. (2023). HoneyBee outperforms all models across the vast majority of tasks for both low-resource finetuning and zero-shot settings. MatSci-Instruct\u2019s Progressive-Refinement-Feedback method improves HoneyBee\u2019s performance for each consecutive stage. We report\nmacro-F1 (top) and micro-F1 (bottom) scores highlighting the best, second-best and third-best performing LLM. Honey-7b and HoneyBee-13b outperform both ChatGPT and Claude and are generally competitive with GPT-4.\nModel Named Entity Recognition Relation Extraction Event Argument Extraction Paragraph Classification Synthesis Action Retrieval Sentence Classification Slot Filling Overall (All Tasks)\nLow-Resource Finetuning on MatSci-NLP\nMatSciBERT (Gupta et al., 2022)\n0.707 0.470\n0.791 0.507\n0.436 0.251\n0.719 0.623\n0.692 0.484\n0.914 0.660\n0.436 0.194\n0.671 0.456\nMatBERT (Walker et al., 2021)\n0.875 0.630\n0.804 0.513\n0.451 0.288\n0.756 0.691\n0.717 0.594\n0.909 0.614\n0.548 0.273\n0.722 0.517\nHoneyBee-7b 0.787 0.644\n0.852 0.518\n0.551 0.389\n0.741 0.641\n0.792 0.617\n0.991 0.711\n0.529 0.391\n0.749 0.559\nHoneyBee-13b 0.860 0.748\n0.921 0.578\n0.653 0.486\n0.761 0.658\n0.853 0.662\n0.998 0.743\n0.554 0.401\n0.80 0.611\nZero-Shot LLM Performance\nLLaMA-7b (Touvron et al., 2023)\n0.042 0.064\n0.094 0.013\n0.160 0.042\n0.279 0.218\n0.052 0.013\n0.096 0.087\n0.142 0.010\n0.208 0.064\nLLaMA-13b (Touvron et al., 2023)\n0.057 0.066\n0.109 0.016\n0.042 0.054\n0.233 0.189\n0.039 0.009\n0.079 0.074\n0.138 0.008\n0.1 0.059\nAlpaca-7b (Taori et al., 2023)\n0.031 0.018\n0.053 0.037\n0.029 0.009\n0.375 0.294\n0.179 0.129\n0.180 0.180\n0.139 0.039\n0.141 0.101\nAlpaca-13b (Taori et al., 2023)\n0.053 0.046\n0.016 0.035\n0.111 0.072\n0.310 0.237\n0.442 0.278\n0.375 0.334\n0.110 0.015\n0.202 0.145\nChat-GPT (OpenAI, 2022)\n0.063 0.052\n0.232 0.145\n0.204 0.203\n0.433 0.450\n0.300 0.183\n0.320 0.318\n0.368 0.280\n0.274 0.233\nClaude (Bai et al., 2022)\n0.063 0.048\n0.232 0.143\n0.195 0.169\n0.442 0.467\n0.280 0.177\n0.329 0.326\n0.393 0.305\n0.276 0.234\nGPT-4 (OpenAI, 2023)\n0.189 0.121\n0.445 0.432\n0.453 0.353\n0.679 0.522\n0.743 0.677\n0.788 0.689\n0.502 0.483\n0.543 0.468\nZero-Shot HoneyBee with MatSci-Instruct\nHoneyBee-7b-Stage1 0.173 0.148\n0.138 0.120\n0.196 0.096\n0.380 0.207\n0.592 0.208\n0.416 0.334\n0.292 0.105\n0.301 0.174\nHoneyBee-7b-Stage2 0.243 0.166\n0.199 0.145\n0.237 0.123\n0.440 0.301\n0.612 0.289\n0.467 0.345\n0.344 0.176\n0.363 0.221\nHoneyBee-7b-Stage3 0.267 0.190\n0.245 0.178\n0.290 0.189\n0.490 0.343\n0.688 0.342\n0.490 0.365\n0.393 0.289\n0.409 0.271\nHoneyBee-13b-Stage1 0.369 0.256\n0.301 0.224\n0.389 0.265\n0.500 0.379\n0.701 0.378\n0.512 0.402\n0.467 0.334\n0.463 0.320\nHoneyBee-13b-Stage2 0.391 0.299\n0.367 0.290\n0.437 0.303\n0.576 0.411\n0.765 0.401\n0.557 0.461\n0.508 0.379\n0.514 0.363\nHoneyBee-13b-Stage3 0.429 0.372\n0.412 0.346\n0.481 0.378\n0.611 0.467\n0.801 0.429\n0.589 0.503\n0.578 0.423\n0.557 0.417\non MatSci-NLP tasks shown in Table 3. MatSciNLP contains a wide range of text data related to material science that spans a wide range of NLP tasks and types of materials, including but not limited to fuel cells, inorganic materials, glasses, and superconductors. For evaluation on MatSci-NLP, we follow the same convention as in Song et al. (2023) where we report both macro-F1 and microF1 scores in Table 3. Low-Resource Finetuning: In our experiments, we follow the same low-resource procedure described in (Song et al., 2023) by splitting the data in MatSci-NLP into 1% training subset and a 99% test subset for evaluation. Subsequently, all models, including HoneyBee, are finetuned on the training subset and evaluated on the test subset of MatSciNLP data. The results on low-resource finetuning in Table 3 show that both HoneyBee-7b and HoneyBee-13b perform best overall, outperform-\ning MatBERT (Walker et al., 2021) and MatSciBERT (Gupta et al., 2022) among all tasks in MatSciNLP with the exception of named entity recognition. MatBERT and MatSci-BERT are both BERT models pretrained on different corpora of materials science textual data. While the domain-specific pretraining significantly boosts the score of both models for MatSci-NLP tasks, HoneyBee shows better performance without requiring pretraining on materials science textual data. This is a significant advantage of HoneyBee and MatSci-Instruct given that large, high-quality corpora of materials science text are generally difficult to obtain as described in Section 2.\nZero-Shot Performance: The zero-shot performance results, where we assess performance directly on the test subset, in the lower part of Table 3 show that HoneyBee outperforms both LLaMa and Alpaca models. Notably, HoneyBee-7b-Stage1, which corresponds to only one round of MatSciInstruct, outperforms both LLaMa and Alpaca models for equal (7b) and larger (13b) parameter sizes. The data in Table 3 further confirms the results from Table 2 that show progressive improvement with each stage of MatSci-Instruct where both HoneyBee-7 and Honey13b exhibit clear improvement in iterative stages. We also observe that model parameter size matters for zero-shot performance with 13b parameter models outperforming 7b for HoneyBee and Alpaca, both of which are instruction finetuned models. Interestingly, LLaMA-7b generally outperforms LLaMa13b across most MatSci-NLP tasks and in the overall score on MatSci-NLP."
        },
        {
            "heading": "4.4 HoneyBee \u2014 Case Study",
            "text": "We perform a case study to further understand the capabilities and limitations of the various LLMs we studied, including HoneyBee, Alpaca, and ChatGPT. Our case study results, with full data and text included in Appendix D, show that HoneyBee-13b generally produces outputs of the same quality as Chat-GPT while other models generally produce lower quality outputs. This provides additional weight to the results in Section 4.1 indicating that HoneyBee-13b can match the quality of Chat-GPT after multiple rounds of progressive refinementfeedback finetuning using MatSci-Instruct."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we introduce MatSci-Instruct, an iterative instruction generation method for materials science, and HoneyBee, a state-of-the-art large language model for materials science. To the best of our knowledge, HoneyBee is the first billionparameter scale language model that is specialized in materials science. HoneyBee outperforms current state-of-the-art general language models (LLaMa, Alpaca) and materials science BERTbased language models (MatBERT, MatSciBERT) in various materials science NLP tasks, and note HoneyBee\u2019s performance improvement with each successive MatSci-Instruct stage. MatSci-Instruct provides a valuable framework for generating instruction-data to progressively finetune LLMs where instruction-data from an Instructor are verified by a Verifier before being used for finetuning. Additionally, poor instruction-data is refined based on feedback from an Evaluator leading to higher quality instruction-data and model performance for the desired specialization as shown by the results in Section 4. Future work remains in augmenting materials science LLMs with external knowledge, such as known scientific facts, which can further improve an LLM\u2019s reliability and interpretability."
        },
        {
            "heading": "Limitations",
            "text": "While HoneyBee outperforms current state-of-theart methods in various materials science NLP tasks, it remains unclear how well HoneyBee would generalize the tasks outside of the MatSci-NLP benchmark and MatSci-Instruct instruction-data to solve complex materials science challenges. Such challenges may include creating a synthesis recipe for a new materials or explaining the behavior of a materials system based on fundamental scientific concepts. Materials science remains a wide-ranging and complex field with many open questions remaining on the true capability of HoneyBee and other LLMs to understand important materials science concepts. MatSci-Instruct also relies on the availability of highly performant LLMs to serve as the Instructor, Verifier and Evaluator which can be limited in their own capabilities. Furthermore, our work focuses primarily on the materials science domain and further studies are needed to understand how applicable our methods would be to additional scientific domains.\nBroader Impact\nBoth HoneyBee and MatSci-Instruct can help promote research on NLP for material science in applying, training and evaluating LLMs for practical applications in the field. The general frameworks described in this paper can also be transferred to other scientific domain, such biology, physics and chemistry, where trustworthy textual data is required.\nOur research does not raise major ethical concerns."
        },
        {
            "heading": "Acknowlegments",
            "text": "This work is supported by the Mila internal funding - Program P2-V1: Industry Sponsored Academic Labs (project number: 10379), the Canada CIFAR AI Chair Program, and the Canada NSERC Discovery Grant (RGPIN-2021-03115)."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "A Experimental Details",
            "text": "We finetune LLaMA models with 7B and 13B parameters using instructions from MatSci-Instruct. As shown in Table 2, we analyze the effect of various rounds of iterative instruction feedback. For finetuning, we use the AdamW optimizer with an initial learning rate of 1e-4 on 2 A100 GPUs for LLaMa-7b and 4 A100 GPUs for LLaMa-13b. We assign a batch size of 4 to each GPU with a gradient accumulation step of 32 and a maximum sequence length of 2048.\nB Instruction Generation Details"
        },
        {
            "heading": "MatSci-Instruct Topics",
            "text": "The full list of materials science topics and NLP tasks sampled for MatSci-Instruct instruction are included in Table 4 and Table 5. We sample a broad range of materials science topics and NLP that are generally balanced yielding a set of instructions that includes specialized materials science text as well as general language capabilities."
        },
        {
            "heading": "MatSci-Instruct NLP Tasks",
            "text": ""
        },
        {
            "heading": "C Quality of Instructions for each MatSci-Instruct Stage",
            "text": "The preset threshold for filtering low-quality data is an average score of 95 or above for all four dimensions and a score of no less than 90 for each individual dimension. As shown in Table 6, after filtering, the remaining instruction data exhibits significantly improved quality, approaching full marks making the data significantly more trustworthy for finetuning specialized LLMs."
        },
        {
            "heading": "D LLM Case Study",
            "text": "This section provides the details for the case study in Section 4.4."
        },
        {
            "heading": "D.1 Open-ended Instruction",
            "text": "We analyze the response of different LLMs to the following open-ended query: Analyze the properties and applications of electronic materials.\n\u2022 Chat-GPT: Electronic materials are a class of materials that possess unique properties that make them suitable for use in electronic devices and technologies. These materials are characterized by their electrical conductivity,\nlium nitride (GaN) possess unique optical properties, ideal for use in displays, touch screens, and flat panel displays. Magnetic materials like iron, cobalt, and nickel are used in magnetic sensors, hard drives, and magnetic memory due to their unique magnetic properties. Materials like diamond and silicon carbide possess unique thermal properties ideal for use in high-temperature electronic devices. The properties of electronic materials are critical for modern electronic devices, and research is ongoing to discover new materials with even more unique properties.\n\u2022 HoneyBee-7B: Electronic materials possess unique properties that make them suitable for various applications. They exhibit high electrical conductivity, low resistivity, and excellent thermal conductivity. These materials are commonly used in electronic devices such as transistors, integrated circuits, and sensors. They also find applications in optoelectronics, energy storage devices, and flexible electronics. The properties of electronic materials enable efficient charge transport and facilitate the flow of electrons, making them essential components in modern technology.\n\u2022 Alpaca-13b: Electronic materials are materials that have electrical properties and can be used in a variety of applications. These materials can be classified into two categories: conductors and insulators. Conductors are materials that allow the flow of electricity, while insulators are materials that do not allow the flow of electricity. Electronic materials can be used in a variety of applications, such as in semiconductors, transistors, and diodes. Semiconductors are materials that have properties that are intermediate between those of conductors and insulators and are used in the manufacture of integrated circuits. Electronic materials are also used in the manufacture of solar cells, which convert light into electricity.\n\u2022 Alpaca-7b: Electronic materials are materials that have the ability to control the flow of electrons. These materials are used in a variety of applications, such as electronics, optics, and sensors. Examples of electronic materials include semiconductors, which are materials that can be doped to control the flow of electrons, and metallic materials, which are con-\nductors of electricity. Other examples include superconductors, which are materials that can conduct electricity with zero resistance, and dielectrics, which are materials that can store electricity. Electronic materials are used in a variety of applications, such as electronics, optics, and sensors.\nChatGPT is the most comprehensive and relevant answer as it provides a detailed analysis of the properties and applications of electronic materials, covering electrical conductivity, optical properties, magnetic properties, and thermal properties. Chat-GPT also gives specific examples of materials and their applications in various electronic devices. HoneyBee-13b is the second-best answer as it also covers the key properties of electronic materials and their applications in various electronic devices. HoneyBee-13b provides specific examples of materials and their unique properties that make them ideal for use in different applications. However, HoneyBee-13b\u2019s output lacks specificity and does not provide enough detail to fully address the question. HoneyBee-7b gives the thirdbest answer as it gives a general overview of the unique properties of electronic materials and their applications in various electronic devices. However, HoneyBee-7b\u2019s output lacks specificity and does not provide enough detail. Alpaca-13b is the fourth-best answer as it provides a basic understanding of electronic materials and their use in various applications but lacks detail and specificity. Alpaca-7b is the least comprehensive and relevant answer as it provides a limited understanding of electronic materials and their use in various applications. Alpaca-7b\u2019s output does not provide enough detail or specific examples to support its claims."
        },
        {
            "heading": "D.2 Content-based Instruction",
            "text": "Next, we analyze the response of different LLMs to the following query: What are the exciton interactions responsible for? with the following <input>: \u201c Inspired by the recent experimental observation of strongly coupled polaritons in a Moire heterobilayer, we study a model of dipole-interacting excitons localized on sites of a lattice and coupled to planar cavity photons. We calculate the phase diagram of this system by computing fluctuations around the mean field and determining the stability of the excitation spectrum. We find that the transition from the normal state to a polariton condensate is intermediated by a series of ordered states at\npartial fillings of the exciton lattice, stabilized by the exciton interactions. In particular we predict a supersolid phase in which a polariton condensate coexists with spatial order. \u201d The replies from the different LLMs are below:\n\u2022 Chat-GPT: The exciton interactions are responsible for stabilizing the ordered states at partial fillings of the exciton lattice, and in particular for predicting a supersolid phase in which a polariton condensate coexists with spatial order.\n\u2022 HoneyBee-13b: The exciton interactions are responsible for stabilizing the ordered states at partial fillings of the exciton lattice, which are intermediates between the normal state and a polariton condensate. In particular, they are responsible for the predicted supersolid phase, where a polariton condensate coexists with spatial order.\n\u2022 HoneyBee-7b: The exciton interactions are responsible for stabilizing the ordered phases that mediate the transition to the polariton condensate, including the supersolid phase.\n\u2022 Alpaca-13b: The exciton interactions studied in the given text are responsible for the phase transition from the normal state to a polariton condensate, as well as the supersolid phase\n\u2022 Alpaca-7b: The exciton interactions are responsible for the transition from the normal state to a polariton condensate, as well as the stabilization of the supersolid phase\nThe answers from HoneyBee-13b and Chat-GPT are better compared to the outputs of the other models. HoneyBee-13b and Chat-GPT directly address the question by mentioning the ordered states at partial fillings of the exciton lattice, which are intermediates between the normal state and a polariton condensate, and the predicted supersolid phase. The answers also use language that closely matches the language used in the original text, indicating a good understanding of the material."
        },
        {
            "heading": "E LLM Prompts",
            "text": "In this section we provide some of the prompts used for the different modules in MatSci-Instruct. We plan to make the full list of prompts, data and code available upon publication.\n\u2022 \u201cEvaluate accuracy of the given text by comparing with known facts or credible sources. This involves checking the accuracy of any claims or statements made in the text, and verifying that they are supported by evidence. The next line directly provide the text. {output_text} Please return a score ranging from 0 to 100, with 0 being the worst and 100 being the best. Please use the strictest grading standard. The score should be in JSON format with a field name of \u2019score\u2019. You should not output any other information or text.\u201d\n\u2022 \u201cEvaluate relevance of the given text by considering how directly the text is related to materials science. The next line directly provide the text. {output_text} Please return a score ranging from 0 to 100, with 0 being the worst and 100 being the best. Please use the strictest grading standard. The score should be in JSON format with a field name of \u2019score\u2019. You should not output any other information or text.\u201d\n\u2022 \u201cEvaluate completeness of the given text (in-\ncluding input, instruction and output) by assessing how fully the output addresses the instruction, including all sub-questions. Consider both depth and conciseness. The next 3 lines directly provide the input, instruction and output respectively. {input_text} {instruction} {output_text} Please return a score ranging from 0 to 100, with 0 being the worst and 100 being the best. Please use the strictest grading standard. The score should be in JSON format with a field name of \u2019score\u2019. You should not output any other information or text.\u201d\n\u2022 \u201cEvaluate reasonableness of the given text by considering how logically consistent the content is, with no obvious contradictions. The next line directly provide text. {output_text} The score should range from 0 to 100, with 0 being the worst and 100 being the best. Please use the strictest grading standard. The score should be in JSON format with a field name of \u2019score\u2019. You should not output any other information or text.\u201d"
        }
    ],
    "title": "HoneyBee: Progressive Instruction Finetuning of Large Language Models for Materials Science",
    "year": 2023
}