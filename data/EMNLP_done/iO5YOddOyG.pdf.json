{
    "abstractText": "Large Language Models (LLMs) have emerged as influential instruments within the realm of natural language processing; nevertheless, their capacity to handle multi-party conversations (MPCs) \u2013 a scenario marked by the presence of multiple interlocutors involved in intricate information exchanges \u2013 remains uncharted. In this paper, we delve into the potential of generative LLMs such as ChatGPT and GPT-4 within the context of MPCs. An empirical analysis is conducted to assess the zero-shot learning capabilities of ChatGPT and GPT-4 by subjecting them to evaluation across three MPC datasets that encompass five representative tasks. The findings reveal that ChatGPT\u2019s performance on a number of evaluated MPC tasks leaves much to be desired, whilst GPT-4\u2019s results portend a promising future. Additionally, we endeavor to bolster performance through the incorporation of MPC structures, encompassing both speaker and addressee architecture. This study provides an exhaustive evaluation and analysis of applying generative LLMs to MPCs, casting a light upon the conception and creation of increasingly effective and robust MPC agents. Concurrently, this work underscores the challenges implicit in the utilization of LLMs for MPCs, such as deciphering graphical information flows and generating stylistically consistent responses.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chao-Hong Tan"
        },
        {
            "affiliations": [],
            "name": "Jia-Chen Gu"
        },
        {
            "affiliations": [],
            "name": "Zhen-Hua Ling"
        }
    ],
    "id": "SP:615aea519ed87175e187ed3034fd229fc94824fe",
    "references": [
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: an automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or",
            "year": 2005
        },
        {
            "authors": [
                "Tom B. Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell",
                "Sandhini Agarwal",
                "Ariel Herbert-Voss",
                "Gretchen Krueger"
            ],
            "title": "Language models are",
            "year": 2020
        },
        {
            "authors": [
                "Hutchinson",
                "Reiner Pope",
                "James Bradbury",
                "Jacob Austin"
            ],
            "title": "PaLM: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of",
            "year": 2019
        },
        {
            "authors": [
                "Jia-Chen Gu",
                "Zhen-Hua Ling",
                "Quan Liu",
                "Cong Liu",
                "Guoping Hu."
            ],
            "title": "GIFT: graph-induced finetuning for multi-party conversation understanding",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, ACL",
            "year": 2023
        },
        {
            "authors": [
                "Jia-Chen Gu",
                "Chao-Hong Tan",
                "Chongyang Tao",
                "ZhenHua Ling",
                "Huang Hu",
                "Xiubo Geng",
                "Daxin Jiang."
            ],
            "title": "HeterMPC: A heterogeneous graph neural network for response generation in multiparty conversations",
            "venue": "Proceedings of the 60th",
            "year": 2022
        },
        {
            "authors": [
                "Jia-Chen Gu",
                "Chongyang Tao",
                "Zhen-Hua Ling",
                "Can Xu",
                "Xiubo Geng",
                "Daxin Jiang."
            ],
            "title": "MPCBERT: A pre-trained language model for multiparty conversation understanding",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Chao-Chun Hsu",
                "Sheng-Yeh Chen",
                "Chuan-Chun Kuo",
                "Ting-Hao K. Huang",
                "Lun-Wei Ku."
            ],
            "title": "Emotionlines: An emotion corpus of multi-party conversations",
            "venue": "Proceedings of the Eleventh",
            "year": 2018
        },
        {
            "authors": [
                "Wenpeng Hu",
                "Zhangming Chan",
                "Bing Liu",
                "Dongyan Zhao",
                "Jinwen Ma",
                "Rui Yan."
            ],
            "title": "GSN: A graph-structured network for multi-party dialogues",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI",
            "year": 2019
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Yiyang Li",
                "Hai Zhao."
            ],
            "title": "EM pre-training for multi-party dialogue response generation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, ACL 2023, (Volume 1: Long Papers), Toronto, Canada,",
            "year": 2023
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out.",
            "year": 2004
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu."
            ],
            "title": "G-eval: NLG evaluation using GPT-4 with better human alignment",
            "venue": "CoRR, abs/2303.16634.",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
            "year": 2019
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "CoRR, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Hiroki Ouchi",
                "Yuta Tsuboi."
            ],
            "title": "Addressee and response selection for multi-party conversation",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages",
            "year": 2016
        },
        {
            "authors": [
                "Baolin Peng",
                "Chunyuan Li",
                "Pengcheng He",
                "Michel Galley",
                "Jianfeng Gao."
            ],
            "title": "Instruction tuning with GPT-4",
            "venue": "CoRR, abs/2304.03277.",
            "year": 2023
        },
        {
            "authors": [
                "Soujanya Poria",
                "Devamanyu Hazarika",
                "Navonil Majumder",
                "Gautam Naik",
                "Erik Cambria",
                "Rada Mihalcea"
            ],
            "title": "MELD: A multimodal multi-party",
            "year": 2019
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 - November 1, 2018, pages 186\u2013191. Association for Computational",
            "year": 2018
        },
        {
            "authors": [
                "Chengwei Qin",
                "Aston Zhang",
                "Zhuosheng Zhang",
                "Jiaao Chen",
                "Michihiro Yasunaga",
                "Diyi Yang"
            ],
            "title": "Is ChatGPT a general-purpose natural language processing task solver? CoRR, abs/2302.06476",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever."
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog, 1(8):9.",
            "year": 2019
        },
        {
            "authors": [
                "Xiaohui Song",
                "Longtao Huang",
                "Hui Xue",
                "Songlin Hu."
            ],
            "title": "Supervised prototypical contrastive learning for emotion recognition in conversation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP",
            "year": 2022
        },
        {
            "authors": [
                "Weiwei Sun",
                "Lingyong Yan",
                "Xinyu Ma",
                "Pengjie Ren",
                "Dawei Yin",
                "Zhaochun Ren."
            ],
            "title": "Is chatgpt good at search? investigating large language models as re-ranking agent",
            "venue": "CoRR, abs/2304.09542.",
            "year": 2023
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "ht tps://github.com/tatsu-lab/stanfor",
            "year": 2023
        },
        {
            "authors": [
                "Jiaan Wang",
                "Yunlong Liang",
                "Fandong Meng",
                "Haoxiang Shi",
                "Zhixu Li",
                "Jinan Xu",
                "Jianfeng Qu",
                "Jie Zhou."
            ],
            "title": "Is chatgpt a good NLG evaluator? A preliminary study",
            "venue": "CoRR, abs/2303.04048.",
            "year": 2023
        },
        {
            "authors": [
                "Weishi Wang",
                "Steven C.H. Hoi",
                "Shafiq R. Joty."
            ],
            "title": "Response selection for multi-party conversations with dynamic topic tracking",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "Proceedings of the 61st Annual Meeting of the",
            "year": 2023
        },
        {
            "authors": [
                "Gloria Willcox."
            ],
            "title": "The feeling wheel: A tool for expanding awareness of emotions and increasing spontaneity and intimacy",
            "venue": "Transactional Analysis Journal, 12(4):274\u2013276.",
            "year": 1982
        },
        {
            "authors": [
                "Sayyed M. Zahiri",
                "Jinho D. Choi."
            ],
            "title": "Emotion detection on TV show transcripts with sequence-based convolutional neural networks",
            "venue": "The Workshops of the The Thirty-Second AAAI Conference on Artificial Intelligence, New Orleans, Louisiana, USA, February",
            "year": 2018
        },
        {
            "authors": [
                "Rui Zhang",
                "Honglak Lee",
                "Lazaros Polymenakos",
                "Dragomir R. Radev."
            ],
            "title": "Addressee and response selection in multi-party conversations with speaker interaction rnns",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-",
            "year": 2018
        },
        {
            "authors": [
                "Shen Zheng",
                "Jie Huang",
                "Kevin Chen-Chuan Chang"
            ],
            "title": "Why does chatgpt fall short in answering questions faithfully? CoRR, abs/2304.10513",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large Language Models (LLMs), including notable instances such as ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023), are ushering in a new era in the field of natural language processing (NLP), showcasing remarkable zero-shot and fewshot generalization capabilities. The methods of pretraining language models on extensive text corpora (Brown et al., 2020; Touvron et al., 2023), followed by alignment fine-tuning to ensure adherence to human instructions (Wang et al., 2023b;\n\u2217Corresponding author.\nPeng et al., 2023), has significantly amplified their proficiency in language comprehension, generation, interaction, and reasoning. The impressively high performance exhibited by these models has been extensively documented in related works (Qin et al., 2023; Wang et al., 2023a; Bubeck et al., 2023).\nMulti-Party Conversations (MPCs) represent a prevalent and natural facet of human communication. In these exchanges, more than two participants engage in interactive discourse on a variety of topics. Such a dynamic introduces new challenges and opportunities for dialogue systems. Here, the key requirement isn\u2019t merely generating coherent and relevant utterances, but also making strategic determinations about when to intervene and whom to address in the conversation. We queried ChatGPT about its potential strategies for addressing the inherent challenges of MPCs with \u201cCan you solve multi-party conversation tasks?\u201d. ChatGPT\u2019s response was as follows: \u201cI do not have built-in mechanisms to keep track of individual participants in a conversation. Therefore, it\u2019s important to explicitly mention the name or identifier of the participant you are addressing when providing instructions or asking questions.\u201d, which was intriguing and touched upon some of the critical aspects we are investigating in this paper.\nConsiderable efforts have been made to explore the capabilities of LLMs across a variety of NLP tasks (Qin et al., 2023; Sun et al., 2023). Despite these advancements, the effectiveness of LLMs in handling MPCs remains largely underexplored. In this study, we scrutinize the potential of LLMs such as ChatGPT and GPT-4 in managing MPCs by implementing five distinct tasks including Emotion Detection, Addressee Recognition, Speaker Identification, Response Selection, and Response Generation, across three different MPC datasets. Our experiments reveal that both ChatGPT and GPT-4 can achieve performance on par with supervised methods when evaluated on the EmoryNLP\nand MELD datasets. However, the performance of these models on the more complex Ubuntu IRC dataset remains less than satisfactory. To address this, we introduce a strategy known as MPC Structure Incorporation, which weaves the speaker and addressee structure information of MPCs into the LLMs. This addition leads to a significant performance boost of ChatGPT and GPT-4 across all three datasets, underpinning the value of this approach for improving LLM effectiveness in MPC scenarios.\nIn summary, our contributions in this paper are three-fold: 1) An exploratory study to examine the performance of ChatGPT and GPT-4 in handling MPCs within a zero-shot context is carried out. This is the first study of its kind to investigate how these LLMs perform in MPC scenarios. 2) An MPC structure incorporation approach is proposed, which enhances the performance of ChatGPT and GPT-4 in managing MPCs. This strategy integrates the speaker and addressee structure information of MPCs into LLMs, leading to substantial performance improvements. 3) We delve into the potential of LLMs in handling MPCs and shed light on the challenges that need to be tackled in future research. This discussion forms the basis for continued investigations into improving LLM effectiveness in MPC contexts."
        },
        {
            "heading": "2 Related Work",
            "text": "Large Language Models Recently, the development of large language models (LLMs) has made tremendous progress, as evidenced by models such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023), ChatGPT (OpenAI, 2022), and GPT-4 (OpenAI, 2023). These LLMs exhibit emergent abilities, including in-context learning, mathematical reasoning, and commonsense reasoning (Wei et al., 2022). A recent line of work focuses on instruction learning, either via generating high-quality instructions data (Wang et al., 2023b; Peng et al., 2023) or by boosting LLMs with instructions (Chung et al., 2022; Taori et al., 2023).\nMulti-Party Conversations Existing methods on building MPC systems can be generally categorized into retrieval-based approaches (Ouchi and Tsuboi, 2016; Zhang et al., 2018; Wang et al., 2020; Gu et al., 2021, 2023) or generation-based (Hu et al., 2019; Gu et al., 2022; Li and Zhao, 2023). On the one hand, Ouchi and Tsuboi (2016)\nand Zhang et al. (2018) proposed to update speaker embeddings with conversation streams dynamically and role-sensitively. Wang et al. (2020) proposed to track the dynamic topic in a conversation. Gu et al. (2021) proposed jointly learning \u201cwho says what to whom\" in a unified framework by designing selfsupervised tasks. Gu et al. (2023) present graphinduced fine-tuning which adapts Transformerbased LMs by integrating four types of edges into attention mechanisms. On the other hand, Hu et al. (2019) explored generation-based approaches by proposing a graph-structured network (GSN) that encoded the conversation context using homogeneous GNNs. Gu et al. (2022) proposed HeterMPC to model the complicated interactions between utterances and interlocutors with a heterogeneous graph. Li and Zhao (2023) proposed to iteratively generate missing addressees and optimize the generative model via the EM algorithm.\nLLMs for NLP Tasks Several contemporaneous papers present empirical studies of leveraging LLMs for various NLP tasks to explore whether LLMs can achieve competitive performance. For example, Qin et al. (2023) study the zero-shot learning capability of ChatGPT by evaluating it on 7 representative task categories, such as reasoning, natural language inference, and summarization. Liu et al. (2023) and Wang et al. (2023a) explore whether ChatGPT is a good evaluator of natural language generation. Sun et al. (2023) investigate LLMs for relevance ranking in information retrieval. Zheng et al. (2023) seek to understand why ChatGPT falls short in providing truthful answers and provide a guideline towards truthfulness in question answering via LLMs.\nTo the best of our knowledge, this paper makes the first attempt to empirically analyze the zeroshot learning ability of LLMs on the MPC tasks, providing comprehensive evaluation and analysis to inspire the development of more effective and robust MPC agents."
        },
        {
            "heading": "3 Approach",
            "text": "To explore an out-of-box MPC solver, our emphasis lies in the zero-shot setting. Instruction for each task is shown in Figure 1. For each task, LLMs are first instructed with the prompt \u201cYou have been presented with a sequence of multi-party conversational turns, organized in chronological order.\u201d. Then, LLMs are instructed to complete the task with task-specific prompts. To stabilize\nthe output of LLMs, the effect of temperature is amplified with the prompt \u201cUse temperature=0, minimize unnecessary words to not get confused.\u201d.1 For more comprehensive explication, certain tasks require an Extended Instruction as shown in Table 5."
        },
        {
            "heading": "3.1 Task-Specific Prompts",
            "text": "Different instructions for each task are designed to guide LLMs to complete the task, namely taskspecific prompts.\nEmotion Detection (ED) LLMs are tasked to predict the emotion of each utterance with the task instruction \u201cPlease evaluate the emotions of each utterance in the dialogue using the following n labels: {...}.\u201d and the output template \u201cThe output format must be: #{num} \u2013 {utterance} // {emotion}\u201d Here, n is the number of emotion labels, and {...} is the list of emotion labels. Dialogue history is formalized as \u201c#{num} \u2013 {utterance}\u201d.\nAddressee Recognition (AR) LLMs are tasked to predict the addressee of each utterance with the task instruction \u201cYour task is to find the addressee of each utterance.\u201d and the output template \u201cThe output format must be: #{num} \u2013 {utterance} // Reply to #{reply_i}\u201d. And we can get the addressee information if we know the reply-to utterance of each utterance. Since the addressee of the first utterance is unknown, we add the extra description \u201cPlease start from #1 since #0 is the first utterance that has no reply-to utterance. You should not leave any utterance unattended.\u201d to inform LLMs of the addressee of the first utterance. Dialogue history is formalized as the same as emotion detection.\nSpeaker Identification (SI) LLMs are tasked to predict the speaker of the last utterance with the\n1We found that this prompt can improve the performance, even if the temperature is already set to 0 in API.\ntask instruction \u201cPlease identify the speaker of the last sentence.\u201d and output template \u201cThe output format should be only one speaker.\u201d. Dialogue history with speaker is formalized as \u201c#{num} \u2013 {speaker}: {utterance}\u201d.\nResponse Selection (RS) LLMs are tasked to select the most appropriate response from the candidates with the task instruction \u201cYour task is to select the most appropriate response from the candidate set.\u201d and output template \u201cThe output format must be: #{num} \u2013 {utterance}\u201d. The candidate set is formalized as \u201c#{num} \u2013 {utterance}\u201d. Thus the history with input template is formalized as \u201cDialogue History: {conversation turns} Candidates: {candidates}\u201d.\nResponse Generation (RG) LLMs are tasked to generate a response with the task instruction \u201cYour task is to generate the most appropriate response.\u201d. There is no need to provide the output template since the generation task is free-form."
        },
        {
            "heading": "3.2 MPC Structure Incorporation",
            "text": "Considering that the complicated interactions between interlocutors, between utterances and between an interlocutor and an utterance naturally increase the difficulty of fully understanding MPCs, it might be helpful to incorporate the MPC structure information.\nSpeaker Structure The speaker structure is incorporated into LLMs to help understand utterances. Specifically, the prompts with \u201c{utterance}\u201d is replaced with \u201c{speaker}: {utterance}\u201d to inform LLMs of the speaker of each utterance. An example is shown in Figure 2.\nAddressee Structure The addressee structure of MPCs is constructed with adding sentence \u201cReply to #{reply_uid} \u2013 {reply_utterance}\u201d into prompt.\nSpeaker-Addressee Structure The speakeraddressee structure of MPCs denotes the information flow from a speaker to an addressee, which is constructed with the prompt \u201cReply to #{reply_uid} \u2013 Speaker {reply_spk}: {reply_utterance}\u201d and \u201c{utterance}\u201d is replaced with \u201c{speaker}: {utterance}\u201d."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "EmoryNLP EmoryNLP (Zahiri and Choi, 2018) is a collection of the TV show Friends, where\neach utterance is annotated with one of the seven emotions borrowed from the six primary emotions in the Willcox (1982)\u2019s feeling wheel, including sad, mad, scared, powerful, peaceful, joyful, and a default emotion of neutral.\nMELD The Multimodal EmotionLines Dataset (MELD) (Poria et al., 2019) was developed by improving and expanding the original EmotionLines dataset (Hsu et al., 2018). MELD includes the same multi-party dialogue instances as EmotionLines but incorporates additional audio and visual modalities alongside text. Each utterance within a dialogue is tagged with one of the seven emotions, including: Anger, Disgust, Sadness, Joy, Neutral, Surprise, Fear. This comprehensive tagging facilitates deep emotion-focused analysis.\nUbuntu IRC benchmark Ubuntu IRC (Hu et al., 2019) represents a substantial, Ubuntu Internet Relay Chat (IRC) channel corpus, replete with annotations for multiple interlocutors. Furthermore, it is distinguished by the provision of addressee labels accompanying each individual utterance.\nThe EmoryNLP and MELD datasets proffer emotion labels, hence enabling the execution of ED tasks. Conversely, the Ubuntu IRC dataset offers Addressee tags, thereby serving as a resource for AR tasks. Table 1 presents the statistics of the three datasets evaluated in our experiments."
        },
        {
            "heading": "4.2 Baselines",
            "text": "(1) BERT (Devlin et al., 2019) is a bidirectional language representation model and can be finetuned for various NLP tasks. (2) GPT-2 (Radford et al., 2019) is a uni-directional pre-trained language model. (3) BART (Lewis et al., 2020) is a denoising autoencoder using a standard Tranformerbased architecture, trained by corrupting text with an arbitrary noising function and learning to reconstruct the original text. (4) SPCL-CL-ERC (Song et al., 2022) introduced a novel Supervised Prototypical Contrastive Learning loss function for the Emotion Recognition in Conversation task, addressing the issues stemming from imbalanced classification through the medium of contrastive learning, obviating the necessity for large batch sizes. It is the SOTA of the EmoryNLP and MELD dataset on ED task. (5) BART w/. EM (Li and Zhao, 2023) introduce an ExpectationMaximization (EM) method that alternately performs the expectation steps to infer addressee labels, and the maximization steps to fine-tune a response generation model. It is the SOTA of the Ubuntu IRC datasets on RG task. (6) MPC-BERT w/. GIFT (Gu et al., 2021, 2023) is a pre-trained model for MPCs that learns \u2018who says what to whom\u2019 with self-supervised tasks (MPC-BERT) and a graph-based fine-tuning method (GIFT). It is the SOTA of the Ubuntu IRC datasets on AR, SI and RS tasks. (7) ChatGPT (OpenAI, 2022), ingeniously enriched by the infusion of Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies, ensuring seamless synchronization between the model and human directives. (8) GPT-4 (OpenAI, 2023) is a large-scale, multimodal model which can accept image and text inputs and produce text outputs, exhibiting human-level performance on various professional and academic benchmarks. 2"
        },
        {
            "heading": "4.3 Implementation Details",
            "text": "All supervised models were trained with the AdamW method (Loshchilov and Hutter, 2019).\n2ChatGPT and GPT-4 are recognized representatives of LLMs, so we only consider them to evaluate MPCs.\nThe learning rate was initialized as 6.25e-5 and was decayed linearly down to 0. The batch size was set to 128 gradient accumulation steps. Models were trained in 10 epochs. For ChatGPT and GPT-4, we used the API endpoints gpt-3.5turbo-0301 and gpt-4-0314 provided by OpenAI respectively.3"
        },
        {
            "heading": "4.4 Metrics",
            "text": "To evaluate ED task, we employed the weight-F1 score, which is the harmonic mean of precision and recall. To evaluate SI and AR tasks, we employed accuracy. To evaluate RS task, we employed R10@1, which is the percentage of the first correct response selected from 10 candidates. To evaluate the quality of the generated text, we employed the standard string-similarity-based metrics SacreBLEU (Post, 2018), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). Higher is better for all metrics. All metrics were calculated by the evaluate toolkit 4."
        },
        {
            "heading": "4.5 Evaluation Results of MPC Understanding",
            "text": "As shown in Table 2, we evaluated the dialogue understanding performance of supervised language models and LLMs on three test sets. Here, the SOTA results of EmoryNLP and MELD on ED are copied from Song et al. (2022). The SOTA results of Ubuntu IRC of all three tasks are copied from Gu et al. (2023).\nSupervised Language Models Versus LLMs When one juxtaposes the outcomes associated with supervised language models and LLMs, it becomes apparent that the LLMs demonstrate parity in performance with their supervised counterparts on the EmoryNLP and MELD datasets. However, their performance on Ubuntu IRC falls short of the mark. It is unsurprising to discern that the capability of GPT-4 surpasses its predecessor, ChatGPT across all four understanding tasks. In the ED task, both ChatGPT and GPT-4 outperform BERT but fall short of the state-of-the-art (SOTA) on EmoryNLP. Additionally, ChatGPT lags behind BERT on the MELD dataset. For the AR task, both ChatGPT and GPT-4 trail behind BERT and SOTA on the Ubuntu IRC dataset, respectively. Regarding the SI\n3Code is available at https://github.com/lxcht an/ChatMPC.\n4https://github.com/huggingface/evalu ate\ntask, it is essential to note that speaker information detection is unattainable without the provision of explicit speaker information. Consequently, our evaluation of ChatGPT and GPT-4 in the SI task is limited to instances where speaker information is provided. Results demonstrate that both models outperform BERT on both EmoryNLP and MELD datasets. Specifically, ChatGPT and GPT-4 exhibit superior performance to BERT by 3.60% and 16.46% on EmoryNLP, and by 13.49% and 34.42% on MELD, respectively. However, it is worth noting that ChatGPT andGPT-4 significantly trail behind supervised models on the Ubuntu IRC dataset. Regarding the RS task, only in the context of the MELD dataset does GPT-4 outshine BERT. Specifically, on the SI task of the Ubuntu IRC dataset, ChatGPT and GPT-4 lag behind BERT by 25.58% and 12.31%, respectively. This can be attributed to the fact that Ubuntu IRC leans towards a more technical and specialized domain, which is also difficult for humans to understand.\nSpeaker Information Enhancement To delve into the significance of the interlocutor within the MPC understanding, we incorporate speaker information into three distinctive tasks across three datasets, disregarding the ED task. Comparing the line of ChatGPT (GPT-4)5 and ChatGPT (GPT-4) w/. Speaker, we can find that the incorporation of speaker information can improve the performance of ChatGPT (GPT-4) on all five tasks except for the RS task of Ubuntu IRC. This improvement is not surprising. In the context of MPCs, the speaker doesn\u2019t adhere to the rigid alternation characteristic of two-party dialogues, hence the incorporation of speaker information can enhance the lucidity of the conversation, rendering it more readily comprehensible. Note that the substantial advancement observed in the AR task partly stems from our modification of the task from identifying the response sentences to directly recognizing the addressee. Nonetheless, compared with ChatGPT, the subpar performance of ChatGPT w/. Speaker on the RS task of Ubuntu IRC suggests that the imparted speaker information could not be optimally assimilated and deployed by ChatGPT. On the contrary, it appeared to disrupt the process of response selection. However, GPT-4\u2019s performance on this task was substantially superior, enhancing the effectiveness of the RS task by a margin of\n5This syntax denotes that the conclusion is applicable to both ChatGPT and GPT-4.\n9.50%. This result suggests that GPT-4 is better at fusing speaker information than ChatGPT.\nAddressee Information Enhancement To probe the import of the addressee within the context of MPC comprehension, we integrate addressee information into the SI and RS tasks of Ubuntu IRC. This is primarily due to the absence of addressee information within the other two datasets. The AR task is excluded from this process, given that addressee information is unavailable within the corresponding Ubuntu IRC task. When drawing comparisons between the results of ChatGPT w/. Speaker and ChatGPT w/. Speaker & Addressee on\n6For SI task, the speakers in dialogue history are needed. It means that we cannot lack SI information, so these cells are empty. For EmoryNLP and MELD datasets lacking the addressee information, therefore corresponding rows are empty. For the Ubuntu IRC AR task, it asked not to add the addressee information, so these cells are empty.\nthe SI and RS tasks, as well as between the results of ChatGPT and ChatGPT w/. Addressee on the RS task, we find, to our surprise, that the integration of addressee information has led to a diminution in performance. When comparing the results of GPT4 w/. Speaker and GPT-4 w/. Speaker & Addressee on SI and RS tasks, we find that the incorporation of addressee information can slightly improve the performance. The only marginal improvement of approximately 1.00% observed on the SI and RS tasks of Ubuntu IRC can be predominantly attributed to the proficiency of GPT-4 w/. Speaker in correctly inferring addressee information, given its noteworthy accuracy of 82.50% on AR tasks.\nSpeaker and Addressee Information Enhancement When contrasting the results of ChatGPT (GPT-4) w/. Speaker & Addressee with the ChatGPT (GPT-4) on RS tasks, we discern that the\nintegration of speaker and addressee information precipitates a performance decrement of 25.18% for ChatGPT, whilst conversely, it elicits an augmentation of 11.50% in the performance of GPT4. Indeed, both categories of information serve to enhance the comprehension of the conversation, yet the infusion of additional data concurrently amplifies the complexity of understanding. Empirical outcomes indicate that ChatGPT grapples with the processing of this surplus information, whereas its more potent successor, GPT-4, exhibits the capacity to assimilate it effectively."
        },
        {
            "heading": "4.6 Evaluation Results of MPC Generation",
            "text": "As shown in Table 3, we evaluated the dialogue response generation performance of supervised language models and LLMs on three test sets. The SOTA results of Ubuntu IRC of all three tasks are copied from Li and Zhao (2023).\nSupervised Language Models Versus LLMs It becomes apparent that the SacreBLEU scores of supervised models consistently eclipse those of LLMs in a significant number of cases across all three evaluative subsets. This phenomenon is largely a byproduct of ChatGPT and GPT-4\u2019s inclination to generate more prolix responses, an approach inherently detrimental to the calculation of SacreBLEU. Regarding the ROUGEL metrics, LLMs yield superior results compared to supervised models on the EmoryNLP and MELD test sets. In contrast, within the ambit of the Ubuntu IRC dataset, supervised models command a greater presence\u2014an outcome likely driven by the amplified comprehension complexity associated with\nthis particular set. In terms of the METEOR metric, LLMs outdistance supervised models across all test sets, thereby affirming the formidable aptitude of LLMs in generating responses.\nMPC Structure Incorporation Similar to the results in MPC Understanding, the inclusion of interlocutors\u2019 information emerges as beneficial to the generation of dialogue responses. This is substantiated by the marked superiority of ChatGPT (GPT-4) w/. Speaker over ChatGPT (GPT4) across all three test sets, with the exception of ChatGPT\u2019s performance on the EmoryNLP dataset. Nonetheless, addressee information doesn\u2019t seem to confer a discernible advantage in the generation of dialogue responses when comparing the performance of ChatGPT w/. Addressee to the ChatGPT, as well as the performance of ChatGPT w/. Speaker & Addressee to ChatGPT w/. Speaker. When our focus shifts to GPT-4, we observe that the performance of GPT-4 w/. Addressee does not exhibit the anticipated enhancement. However, with the addition of interlocutor information, there is a noticeable improvement in the performance of GPT-4 w/. Speaker & Addressee."
        },
        {
            "heading": "4.7 Case Study",
            "text": "To analyze the performance of LLMs on the tasks of MPC understanding and generation specifically, case studies were conducted by presenting randomly selected examples for further illustration.\nMPC Understanding As shown in Table 4, in the absence of addressee information, the model exhibits inadequacies in comprehending MPC,\n### Instruction: You have been presented with a sequence of multi-party conversational turns, organized in chronological order. Your task is to generate the most appropriate response. The output format is \"[Reply to #{reply_uid} \u2013 Speaker {reply_spk}: {reply_utterance}] Speaker {rsp_spk}: {rsp}\". Use temperature=0, minimize unnecessary words to not get confused.\n### Input: \u2013 Dialogue History: #0 \u2013 Speaker 1: yep - only 4 primaries.. but ive never needed more then 4 [Reply to #0 ... ] #1 \u2013 Speaker 2: but you ca n\u2019t see your other partitions right ? [Reply to #3 ... ] #4 \u2013 Speaker 2: i see there \u2019s some confusion , that \u2019s my mistake i guess.. with active i meant that windows ca n\u2019t \u2019see \u2019 the other partitions.. but i can see that \u2019s only a historical issue.. \u2013 Please give a response on behalf of Speaker 1 for Uttenrance #4. The part of response is [Reply to #4 \u2013 Speaker 2: i see there \u2019s some confusion , that \u2019s my mistake i guess.. with active i meant that windows ca n\u2019t \u2019see \u2019 the other partitions.. but i can see that \u2019s only a historical issue..] Speaker 1. Please finish the response generation.\nresulting in inaccurate responses from both ChatGPT and GPT-4. However, when furnished with addressee information, GPT-4 shows an improved aptitude to comprehend the dialogue, leading to a correct response. Conversely, ChatGPT appears to be confounded by the addressee information, yielding an incorrect answer that lies beyond the purview of the candidates.\nMPC Generation As shown in Table 5, the response generated by BART is conspicuously devoid of substantive content. All the LLMs, especially GPT-4, exhibit the ability to produce lengthy and meaningful responses. Although ChatGPT and GPT-4 are capable of crafting responses with greater pertinence, their propensity to yield verbose responses hampers the SacreBLEU score."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we have explored the abilities of generative LLMs for MPCs, which have been largely underexplored. We empirically analyze the\nzero-shot learning ability of ChatGPT and GPT-4 by evaluating them on three popular MPC datasets covering five representative tasks. On EmoryNLP and MELD datasets, ChatGPT and GPT4 achieve comparable performance to the supervised training models. However, ChatGPT performs poorly on the evaluated Ubuntu IRC tasks, while GPT-4 shows promising results. However, there is still a large gap relative to the supervised training on SI task of Ubuntu IRC. Taking into account the structure of the MPC, it is evident that both ChatGPT and GPT-4 exhibit enhanced performance across nearly all tasks when equipped with speaker information. However, in the context of addressee information, ChatGPT\u2019s performance may decline if it is encumbered with extraneous data. Conversely, GPT-4 aptly leverages this information to accomplish the task with heightened proficiency. Devoting efforts towards efficaciously intertwining the graphical structure inherent in MPCs with LLMs through prompts or even supervised finetuning constitutes one of future work.\nLimitations\nThe design of prompts plays a crucial role in determining the final results, as it holds significant sway over the outcome. The way in which prompts are constructed and structured can greatly impact the performance and effectiveness of ChatGPT and GPT-4 when it comes to handling MPC tasks. However, our current prompt architecture might not fully encompass the ideal potential and capabilities of these advanced language models in tackling MPC tasks. There is a possibility that further improvements and refinements in the prompt design can unlock even greater performance and unleash the full potential of ChatGPT and GPT4 in handling MPC tasks. It is essential to explore and enhance the prompt architecture to ensure optimal results and leverage the capabilities of these powerful language models to their fullest extent in the realm of MPC tasks."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the Opening Foundation of State Key Laboratory of Cognitive Intelligence, iFLYTEK COGOS-2022005. We thank anonymous reviewers for their valuable comments."
        }
    ],
    "title": "Is ChatGPT a Good Multi-Party Conversation Solver?",
    "year": 2023
}