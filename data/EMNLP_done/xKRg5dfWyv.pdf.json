{
    "abstractText": "BabyBERTa, a language model trained on small-scale child-directed speech while none of the words are unmasked during training, has been shown to achieve a level of grammaticality comparable to that of RoBERTa-base, which is trained on 6,000 times more words and 15 times more parameters (Huebner et al., 2021). Relying on this promising result, we explore in this paper the performance of BabyBERTabased models in downstream tasks, focusing on Semantic Role Labeling (SRL) and two Extractive Question Answering tasks, with the aim of building more efficient systems that rely on less data and smaller models. We investigate the influence of these models both alone and as a starting point to larger pre-trained models, separately examining the contribution of the pre-training data, the vocabulary, and the masking policy on the downstream task performance. Our results show that BabyBERTa trained with unmasking-removal policy is a much stronger starting point for downstream tasks compared to the use of RoBERTa masking policy when 10M words are used for training and that this tendency persists, although to a lesser extent, when adding more training data. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Yahan Yang"
        },
        {
            "affiliations": [],
            "name": "Elior Sulem"
        },
        {
            "affiliations": [],
            "name": "Insup Lee"
        },
        {
            "affiliations": [],
            "name": "Dan Roth"
        }
    ],
    "id": "SP:7a75876a497a6f9808dbce473e223fd7dfa6789f",
    "references": [
        {
            "authors": [
                "Xavier Carreras",
                "Llu\u00eds M\u00e0rquez."
            ],
            "title": "Introduction to the conll-2005 shared task: Semantic role labeling",
            "venue": "Proceedings of the ninth conference on computational natural language learning (CoNLL-2005), pages 152\u2013164.",
            "year": 2005
        },
        {
            "authors": [
                "Vijeta Deshpande",
                "Dan Pechi",
                "Shree Thatte",
                "Vladislav Lialin",
                "Anna Rumshisky."
            ],
            "title": "Honey, I shrunk the language: Language model behavior at reduced scale",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 5298\u20135314,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Rotem Dror",
                "Gili Baumer",
                "Segev Shlomov",
                "Roi Reichart."
            ],
            "title": "The hitchhiker\u2019s guide to testing statistical significance in natural language processing",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2018
        },
        {
            "authors": [
                "Nicholas FitzGerald",
                "Julian Michael",
                "Luheng He",
                "Luke Zettlemoyer."
            ],
            "title": "Large-scale QA-SRL parsing",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2051\u20132060, Melbourne,",
            "year": 2018
        },
        {
            "authors": [
                "Noah A. Smith"
            ],
            "title": "Don\u2019t stop pretraining: Adapt language models to domains and tasks",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Luheng He",
                "Kenton Lee",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "Deep semantic role labeling: What works and what\u2019s next",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 473\u2013483,",
            "year": 2017
        },
        {
            "authors": [
                "Luheng He",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "Question-answer driven semantic role labeling: Using natural language to annotate natural language",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2015
        },
        {
            "authors": [
                "Philip A. Huebner",
                "Elior Sulem",
                "Fisher Cynthia",
                "Dan Roth."
            ],
            "title": "BabyBERTa: Learning more grammar with small-scale child-directed language",
            "venue": "Proceedings of the 25th Conference on Computational Natural Language Learning, pages 624\u2013646, Online.",
            "year": 2021
        },
        {
            "authors": [
                "Philip A. Huebner",
                "Jon A. Willits."
            ],
            "title": "Chapter eight - using lexical context to discover the noun category: Younger children have it easier",
            "venue": "Kara D. Federmeier and Lili Sahakyan, editors, The Context of Cognition: Emerging Perspectives, volume 75 of",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Julian Michael",
                "Gabriel Stanovsky",
                "Luheng He",
                "Ido Dagan",
                "Luke Zettlemoyer."
            ],
            "title": "Crowdsourcing question-answer meaning representations",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
            "year": 2018
        },
        {
            "authors": [
                "Martha Palmer",
                "Daniel Gildea",
                "Nianwen Xue."
            ],
            "title": "Semantic role labeling",
            "venue": "Synthesis Lectures on Human Language Technologies, 3(1):1\u2013103.",
            "year": 2010
        },
        {
            "authors": [
                "Sameer Pradhan",
                "Alessandro Moschitti",
                "Nianwen Xue",
                "Hwee Tou Ng",
                "Anders Bj\u00f6rkelund",
                "Olga Uryupina",
                "Yuchen Zhang",
                "Zhi Zhong."
            ],
            "title": "Towards robust linguistic analysis using OntoNotes",
            "venue": "Proceedings of the Seventeenth Conference on Computational",
            "year": 2013
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing",
            "year": 2018
        },
        {
            "authors": [
                "Alex Warstadt",
                "Yian Zhang",
                "Xiaocheng Li",
                "Haokun Liu",
                "Samuel R. Bowman."
            ],
            "title": "Learning which features matter: RoBERTa acquires a preference for linguistic generalizations (eventually)",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Wettig",
                "Tianyu Gao",
                "Zexuan Zhong",
                "Danqi Chen"
            ],
            "title": "Should you mask 15% in masked language modeling",
            "venue": "In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics,",
            "year": 2023
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in Neural Information Processing Systems, volume 32. Curran Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Yian Zhang",
                "Alex Warstadt",
                "Xiaocheng Li",
                "Samuel R. Bowman"
            ],
            "title": "When do you need billions of words of pretraining data",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Yu Zhang",
                "Qingrong Xia",
                "Shilin Zhou",
                "Yong Jiang",
                "Guohong Fu",
                "Min Zhang"
            ],
            "title": "Semantic role labeling as dependency parsing: Exploring latent",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large-scale pre-trained language models (LMs) (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have shown promising ability on handling various downstream tasks including textual classification (Wang et al., 2018) and question answering (QA, Rajpurkar et al., 2016). Previous research (Zhang et al., 2021; Warstadt et al., 2020) showed that the size of architecture and amount of pretraining data actually affect the linguistics features learned by state-of-the-art (SOTA) pre-trained LMs like RoBERTa (Liu et al., 2019). It also showed LMs require much more data to understand com-\n1Our code can be found in https://github.com/ yangy96/babyberta_continual\nmonsense knowledge in order to achieve high performance on natural language understanding tasks, compared to grammatical ability. On the other hand, LMs like RoBERTa are costly to train in terms of GPU computation power and time at both pre-training and fine-tuning stages. Huebner et al. (2021) proposed BabyBERTa, a smaller RoBERTa architecture that is trained on a 5M child-directed data corpora without using unmasked tokens during the masked language modeling training. BabyBERTa reaches the same level of grammaticality as RoBERTa but considerably saves training expenses. However, no further evaluation on tasks other than the grammaticality tests were performed for the model. Deshpande et al. (2023) also focus on the performance of smaller language models but emphasize the relationship between the architecture size and downstream task performance and train on larger data corpora. In this paper, we would like to answer the following questions: (1) What is the performance for LMs based on smaller models like BabyBERTa on downstream tasks that require fine-tuning? and (2) What is an efficient way to improve the behavior of small pre-trained LMs on downstream tasks?\nIn our work, we first evaluated both BabyBERTa and RoBERTa on three downstream tasks that target sentence structure and are closely associated with grammatical capabilities. Additionally, we propose to have various starting points by combining different ingredients in pre-training including the masking policy, the size of the vocabulary, and the type of the pre-training data corpus (childdirected language, online written language). Then, we continually pre-train BabyBERTa and its variants on more Wikipedia data to improve performance on target tasks. We observe that: (1) although BabyBERTa has a lower performance on the downstream tasks compared to RoBERTa, the use of the unmasking removal policy and of a small vocabulary is still effective after fine-tuning; (2)\nrunning thorough experiments to identify which factors are important for small language models when performing continual pre-training, we find that the influence of the unmasking removal policy persists, although to a lesser extent, when adding more training data."
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 Masked language model objective (MLM)",
            "text": "The large transformers for language models are pre-trained on billions of tokens and show their high-capability in various downstream tasks. The success of large-scale pre-trained language models is inseparable from the masked language model objective, which is a widely-used self-supervised learning approach to construct a text representation (Devlin et al., 2019). With the MLM objective, there are p% of the tokens that are masked, and the model learns to reconstruct the masked tokens at the pre-training stage. The loss function is defined as\nL = \u2212 n\u2211\ni=1 mi\u2211 j=1 logP (wi,j |x\u0303i) (1)\nwhere wi,j is the ground truth of the jth masked tokens of ith sequence and x\u0303i is the masked context, n is the total number of sentences and mi is the number of masked tokens for the sentence."
        },
        {
            "heading": "2.1.1 80-10-10 Masking Policy",
            "text": "80% of the masked tokens are replaced by the <mask> token, 10% are replaced by randomly selected tokens, and the rest are kept as the same tokens (Devlin et al., 2019). In the paper, we use the 80-10-10 and RoBERTa masking policy interchangeably."
        },
        {
            "heading": "2.1.2 Unmasking Removal Policy",
            "text": "Different from the default masking strategy, we instead remove the prediction of unchanged / unmasked tokens. In other words, we replace corrupted tokens with <mask> 90% of the time and use random tokens 10% of the time2. Previous work (Huebner et al., 2021; Wettig et al., 2023) shows masking policies are important to pre-training."
        },
        {
            "heading": "2.2 BabyBERTa",
            "text": "BabyBERTa (Huebner et al., 2021), a small-scale version of RoBERTa, differs from RoBERTa in architecture, corpora, masking policy, and other pre-\n2We use 90-10 masking policy and unmasking removal policy interchangeably\ntraining hyperparameters. The details are shown in Table 8. The default masking policy of BabyBERTa is the unmasking removal policy, and the pre-training data corpora is AO-CHILDES (Huebner and Willits, 2021), which consists of childdirected speech. We consider four versions of BabyBERTa, each of them being trained on a different corpus (Table 1)."
        },
        {
            "heading": "2.3 Fine-tune on downstream tasks",
            "text": "In this work, we are interested in following downstream tasks including semantic role labeling (SRL) and two extractive question-answering tasks: question-answer driven semantic role labeling (QASRL) and question-answer meaning representation (QAMR). 1) SRL Semantic role labeling aims to detect predicates (in most cases, verbs) in a sentence and assign its associated arguments with different semantic roles (Palmer et al., 2010; Carreras and M\u00e0rquez, 2005; He et al., 2017). In this paper, we evaluate models on CoNLL12, an SRL benchmark based on OntoNotes v5.0 dataset (Pradhan et al., 2013). 2) QASRL (He et al., 2015) also presents the predicate-argument structure in the sentence but in the format of question-answer pairs 3. In this paper, we evaluate models on the QA-SRL Bank 2.1 dataset (FitzGerald et al., 2018). 3) QAMR (Michael et al., 2018) provides predicate-argument structure for more diverse relationships compared to those presented in QASRL and SRL (including noun relationship)."
        },
        {
            "heading": "3 BabyBERTa on downstream tasks",
            "text": "In this section, we evaluate BabyBERTa models on various downstream tasks and experiment with different methods including continually pre-training. To perform question-answering tasks like QAMR and QASRL, we train two linear layers on top of\n3We here address the Extractive QA tasks (rather than the parsing tasks) related to the QAMR and QASRL formalisms.\nthe encoder of the language model (LM) to predict the start and end of the answer span within the context. We implement the classifier using Huggingface (Wolf et al., 2020). For fine-tuning LMs for SRL tasks, we utilize the implementation provided in (Zhang et al., 2022)."
        },
        {
            "heading": "3.1 How does the BabyBERTa perform on downstream tasks?",
            "text": "We are interested in the performance of BabyBERTa and its variations on downstream tasks including SRL, QASRL, and QAMR. We report the F1 score in Table 2, and compare the performance of BabyBERTa models and RoBERTa. Our experiments show that BabyBERTa has comparable performance on QASRL with RoBERTa-10M and only 3 points lower compared to RoBERTa. For tasks like SRL and QAMR, BabyBERTa\u2019s performance is also within a slight 3-point margin in comparison to RoBERTa-10M. We also observe that the content of the pre-training dataset impacts its performance on downstream tasks. The Wikipedia dataset is closer to the target domain compared to the other two datasets, so BabyBERTa pre-trained on Wikipedia dataset achieves higher performance on QAMR, which is a more challenging task.4"
        },
        {
            "heading": "3.1.1 Effect of vocabulary size",
            "text": "The vocabulary size of RoBERTa is approximately 6x that of BabyBERTa, so it is possible that the size of the vocabulary size limits the understanding of language at the MLM training stage. In this experiment, we compare the performance with different vocabulary sets for pre-training the BabyBERTa model. Table 3 summarizes our experiments for pre-training BabyBERTa with various factors. We observe that the larger vocabulary does not give any improvement in most of the cases. We hypothesize that the training efficiency is low for Baby-\n4More details about the training procedure are in Appendix.\nBERTa when we have a larger vocabulary but less pre-training data."
        },
        {
            "heading": "3.1.2 Effect of masking policy",
            "text": "One observation in previous work (Huebner et al., 2021) is that, compared to BabyBERTa trained with 80-10-10 masking policy, BabyBERTa trained with unmasking-removal policy achieves higher scores on grammar tests. This leads to an interesting questions: what is the impact of the masking policy of the starting point on downstream tasks? Here, we apply two masking policies at the pre-training stage. The results in Table 3 show that the unmasking policy works better for models with smaller architectures like BabyBERTa on these three downstream tasks. Thus, we conclude that BabyBERTa pre-training with unmasking removal policy and smaller vocabulary set achieves the best performance across three different tasks given the results in Table 3."
        },
        {
            "heading": "3.2 Does continually pre-training BabyBERTa improve downstream tasks performance?",
            "text": "Since there is a performance gap between BabyBERTa and RoBERTa as shown in previous experiments, we consider improving the performance by continually pre-train the BabyBERTa architecture on more data. To be specific, each time we pretrain the models on a new subset of the Wikipedia dataset contains about 100M words repeatedly. Given the results in section 3.1, we choose the starting points5 trained with the unmasking removal policy and BabyBERTa vocabulary set.\nFor all continually pre-train procedures, we keep using RoBERTa masking policy. The masking ratio used in our experiments is 15% as the default. Table 4 presents the downstream performance of models trained with continual pre-training, considering various starting points.6 For comparison, we include results from RoBERTa-100M (Zhang et al., 2021). To assess the impact of a more diverse dataset, we mix BookCorpus and Wikipedia as an additional dataset for continual pre-training (Gururangan et al., 2020) 7. We observe that the smaller architecture, after continually training on 100M data, can achieve better and comparable performance for the QASRL and QAMR tasks respectively, compared to a RoBERTa-base pre-trained\n5A starting point here is a BabyBERTa model with a specific masking policy, vocabulary set, and initial training corpus\n6The results correspond to the mean value of three runs. 7Mixed with a ratio 1:3 as in (Zhang et al., 2021)\non 10M data. Moreover, among the small models, the BabyBERTa-Wikipedia model trained on the mixed dataset overall demonstrates the best performance on QAMR and SRL, and achieves comparable performance on QASRL with the bestperforming model.\nAdditionally, we show how the masking policy of the starting points affects continually pretraining in Table 5. We report the mean value of three runs of the models in the table and use the methods recommended in (Dror et al., 2018) for F1 score evaluation: we apply bootstrap to perform the significance test of 3 runs and get the p-value of 0.04 when \u03b1 = 0.05 for BabyBERTa-CHILDES and 0.0 for BabyBERT-Wikipedia. It again shows that the BabyBERTa-CHILDES and BabyBERTaWikipedia gain from unmasking removal policy for QAMR. We summarize that for BabyBERTaCHILDES and BabyBERT-Wikipedia, the unmasking removal policy at the starting point improves the performance on downstream tasks, and, for QAMR, even after continuing pre-training. However, the BabyBERTa-Curriculum does not show the same trend on QAMR (p = 0.25)."
        },
        {
            "heading": "3.3 Scale to more data",
            "text": "After combining the optimal training policies as discussed in the previous section, we continually pre-train the smaller models on more data. The learning curve of the model is presented in Figure 1 on downstream tasks as more data become available (500M tokens). The performance continually improves as we keep pre-training the model on new data sequentially. In Table 6, we report the final performance after continually pre-training the model on 1B tokens. However, the performance is still lower than that of RoBERTa-base (Liu et al., 2019)."
        },
        {
            "heading": "4 Conclusion",
            "text": "In this work, we investigate three important factors for improving smaller language models on downstream tasks: vocabulary set, masking policy, and dataset at the starting point. Our findings reveal that continuously pre-training a smaller model like BabyBERTa leads to continued improvement in downstream performance. Additionally, employing the unremoval masking policy and utilizing a smaller vocabulary prove advantageous for downstream tasks. We provide a comprehensive investigation into the relationship between pre-training procedures and downstream tasks for small models. In future research, we aim to delve deeper into the abilities acquired during the pre-training stage and their impact on downstream task performance.\nLimitations\nOur study specifically concentrated on masked language model objectives and downstream tasks that are closely associated with grammaticality. However, it would be interesting to evaluate our findings on diverse downstream tasks, such as the GLUE benchmark (Wang et al., 2018). Furthermore, our investigation primarily focused on the BabyBERTa architecture configuration and small data corpus (\u2264 1B). It would be valuable to explore the correlation between different pre-training factors and various architecture configurations."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the members of the Cognitive Computation Group and the anonymous reviewers for their insightful suggestions. Research was sponsored by the Army Research Office and was accomplished under Grant Number W911NF-20-1-0080.\nIt was also supported by Contracts FA8750-19-20201 and FA8750-19-2-1004 with the US Defense Advanced Research Projects Agency (DARPA) as well as by grants from the Israeli Ministry of Innovation, Science & Technology (#000519) and the BGU/Philadelphia Academic Bridge (The Sutnick/Zipkin Endowment Fund). Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Army Research Office, the Department of Defense or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. This research was also supported by a gift from AWS AI for research in Trustworthy AI."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Dataset details Here we provide more details about the dataset of the downstream tasks.\nA.2 Architecture and datasets of BabyBERTa Here we provide the model configurations and the size of the datasets.\nA.3 Compare masking policy with more pre-training data\nIn this section, we investigate the impact of masking policy of starting point when continually pretraining the model with more than 100M tokens. Specifically, we plot the performance on QAMR versus the number of tokens for BabyBERTaCHILDES trained with unremoval masking policy and 80-10-10 masking policy. We observe that the performance of CHILDES trained with unremoval masking policy keeps getting better performance compared to CHILDES with 80-10-10 masking policy after continue pre-training on more and more data.\nA.4 Continually pre-train on the task-specific data\nPrior work (Gururangan et al., 2020) suggests that continually pre-training on a task-specific dataset is an effective domain adaptation for downstream tasks. Following this work, we continually pretraining the model on the dataset such as QASRL,\nQAMR, and OntoNotes. To ensure consistency, we convert the context from QASRL and QAMR into the same format as the pre-training data. The results are listed in Table 10.\nA.5 Continually pre-train with RoBERTa vocabulary\nHere we present additional results of continually pre-train the model with RoBERTa vocabulary.\nA.6 Implementation details All of our models are trained and evaluated on two Nvidia Quadro RTX 6000. At the initial pre-\ntraining stage, the number of steps we use is 260K and the batch size is 16. The learning rate is 1e-4 and the weight decay is 0.01. At the continually pre-training stage, the number of steps we use is 300K and the batch size is 256. The learning rate is 1e-4, the warmup steps are set to be 6000 and the weight decay is 0.01. At the fine-tuning stage for QAMR, QASRL and SRL, the model is fine-tuned on the target dataset for 10 epochs, 3 epochs, and 10 epochs respectively. The batch size is set to 16 and the learning rate is 2e-4."
        }
    ],
    "title": "Bootstrapping Small & High Performance Language Models with Unmasking-Removal Training Policy",
    "year": 2023
}