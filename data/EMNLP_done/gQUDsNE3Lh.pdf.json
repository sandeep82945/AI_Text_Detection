{
    "abstractText": "With the proliferation of social media, accurate detection of hate speech has become critical to ensure safety online. To combat nuanced forms of hate speech, it is important to identify and thoroughly explain hate speech to help users understand its harmful effects. Recent benchmarks have attempted to tackle this issue by training generative models on free-text annotations of implications in hateful text. However, we find significant reasoning gaps in the existing annotations schemes, which may hinder the supervision of detection models. In this paper, we introduce a hate speech detection framework, HARE, which harnesses the reasoning capabilities of large language models (LLMs) to fill these gaps in explanations of hate speech, thus enabling effective supervision of detection models. Experiments on SBIC and Implicit Hate benchmarks show that our method, using model-generated data, consistently outperforms baselines, using existing free-text human annotations. Analysis demonstrates that our method enhances the explanation quality of trained models and improves generalization to unseen datasets. Our code is available at https://github.com/ joonkeekim/hare-hate-speech.git.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yongjin Yang"
        },
        {
            "affiliations": [],
            "name": "Joonkee Kim"
        },
        {
            "affiliations": [],
            "name": "Yujin Kim"
        },
        {
            "affiliations": [],
            "name": "Namgyu Ho"
        },
        {
            "affiliations": [],
            "name": "James Thorne"
        },
        {
            "affiliations": [],
            "name": "Se-young Yun"
        }
    ],
    "id": "SP:82a4288292861057999d744fff25743a4bee69fe",
    "references": [
        {
            "authors": [
                "Shourya Aggarwal",
                "Divyanshu Mandowara",
                "Vishwajeet Agrawal",
                "Dinesh Khandelwal",
                "Parag Singla",
                "Dinesh Garg."
            ],
            "title": "Explanations for commonsenseqa: New dataset and models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Pete Burnap",
                "Matthew L Williams."
            ],
            "title": "Us and them: identifying cyber hate on twitter across multiple protected characteristics",
            "venue": "EPJ Data science, 5:1\u201315.",
            "year": 2016
        },
        {
            "authors": [
                "Tommaso Caselli",
                "Valerio Basile",
                "Jelena Mitrovi\u0107",
                "Michael Granitzer."
            ],
            "title": "Hatebert: Retraining bert for abusive language detection in english",
            "venue": "arXiv preprint arXiv:2010.12472.",
            "year": 2020
        },
        {
            "authors": [
                "Aaron Chan",
                "Zhiyuan Zeng",
                "Wyatt Lake",
                "Brihi Joshi",
                "Hanjie Chen",
                "Xiang Ren."
            ],
            "title": "Knife: Distilling meta-reasoning knowledge with free-text rationales",
            "venue": "ICLR 2023 Workshop on Pitfalls of limited data and computation for Trustworthy ML.",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Lin Chen",
                "An-Zi Yen",
                "Hen-Hsen Huang",
                "ChengKuang Wu",
                "Hsin-Hsi Chen."
            ],
            "title": "Zara: Improving few-shot self-rationalization for small language models",
            "venue": "arXiv preprint arXiv:2305.07355.",
            "year": 2023
        },
        {
            "authors": [
                "Minje Choi",
                "Jiaxin Pei",
                "Sagar Kumar",
                "Chang Shu",
                "David Jurgens."
            ],
            "title": "Do llms understand social knowledge? evaluating the sociability of large language models with socket benchmark",
            "venue": "arXiv preprint arXiv:2305.14938.",
            "year": 2023
        },
        {
            "authors": [
                "Thomas Davidson",
                "Debasmita Bhattacharya",
                "Ingmar Weber."
            ],
            "title": "Racial bias in hate speech and abusive language detection datasets",
            "venue": "arXiv preprint arXiv:1905.12516.",
            "year": 2019
        },
        {
            "authors": [
                "Mai ElSherief",
                "Caleb Ziems",
                "David Muchlinski",
                "Vaishnavi Anupindi",
                "Jordyn Seybolt",
                "Munmun De Choudhury",
                "Diyi Yang."
            ],
            "title": "Latent hatred: A benchmark for understanding implicit hate speech",
            "venue": "arXiv preprint arXiv:2109.05322.",
            "year": 2021
        },
        {
            "authors": [
                "Paula Fortuna",
                "S\u00e9rgio Nunes."
            ],
            "title": "A survey on automatic detection of hate speech in text",
            "venue": "ACM Computing Surveys (CSUR), 51(4):1\u201330.",
            "year": 2018
        },
        {
            "authors": [
                "Tarleton Gillespie."
            ],
            "title": "Custodians of the Internet: Platforms, content moderation, and the hidden decisions that shape social media",
            "venue": "Yale University Press.",
            "year": 2018
        },
        {
            "authors": [
                "Namgyu Ho",
                "Laura Schmid",
                "Se-Young Yun."
            ],
            "title": "Large language models are reasoning teachers",
            "venue": "arXiv preprint arXiv:2212.10071.",
            "year": 2022
        },
        {
            "authors": [
                "Fan Huang",
                "Haewoon Kwak",
                "Jisun An."
            ],
            "title": "Chain of explanation: New prompting method to generate higher quality natural language explanation for implicit hate speech",
            "venue": "arXiv preprint arXiv:2209.04889.",
            "year": 2022
        },
        {
            "authors": [
                "David Jurgens",
                "Libby Hemphill",
                "Eshwar Chandrasekharan."
            ],
            "title": "A just and comprehensive strategy for using nlp to address online abuse",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3658\u20133666.",
            "year": 2019
        },
        {
            "authors": [
                "Jiyun Kim",
                "Byounghan Lee",
                "Kyung-Ah Sohn."
            ],
            "title": "Why is it hate speech? masked rationale prediction for explainable hate speech detection",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 6644\u20136655, Gyeongju,",
            "year": 2022
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Andrew K Lampinen",
                "Ishita Dasgupta",
                "Stephanie CY Chan",
                "Kory Matthewson",
                "Michael Henry Tessler",
                "Antonia Creswell",
                "James L McClelland",
                "Jane X Wang",
                "Felix Hill"
            ],
            "title": "Can language models learn from explanations in context",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Lorraine Li",
                "Adhiguna Kuncoro",
                "Jordan Hoffmann",
                "Cyprien de Masson d\u2019Autume",
                "Phil Blunsom",
                "Aida Nematzadeh"
            ],
            "title": "A systematic investigation of commonsense knowledge in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Jessica Lin."
            ],
            "title": "Leveraging world knowledge in implicit hate speech detection",
            "venue": "arXiv preprint arXiv:2212.14100.",
            "year": 2022
        },
        {
            "authors": [
                "Hui Liu",
                "Qingyu Yin",
                "William Yang Wang."
            ],
            "title": "Towards explainable nlp: A generative explanation framework for text classification",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5570\u20135581.",
            "year": 2019
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu."
            ],
            "title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
            "venue": "arXiv preprint arXiv:2303.16634.",
            "year": 2023
        },
        {
            "authors": [
                "Josh Magnus Ludan",
                "Yixuan Meng",
                "Tai Nguyen",
                "Saurabh Shah",
                "Qing Lyu",
                "Marianna Apidianaki",
                "Chris Callison-Burch."
            ],
            "title": "Explanation-based finetuning makes models more robust to spurious cues",
            "venue": "arXiv preprint arXiv:2305.04990.",
            "year": 2023
        },
        {
            "authors": [
                "Ana Marasovi\u0107",
                "Iz Beltagy",
                "Doug Downey",
                "Matthew E Peters."
            ],
            "title": "Few-shot selfrationalization with natural language prompts",
            "venue": "arXiv preprint arXiv:2111.08284.",
            "year": 2021
        },
        {
            "authors": [
                "Binny Mathew",
                "Punyajoy Saha",
                "Seid Muhie Yimam",
                "Chris Biemann",
                "Pawan Goyal",
                "Animesh Mukherjee."
            ],
            "title": "Hatexplain: A benchmark dataset for explainable hate speech detection",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.",
            "year": 2021
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21(1).",
            "year": 2020
        },
        {
            "authors": [
                "Manoel Ribeiro",
                "Pedro Calais",
                "Yuri Santos",
                "Virg\u00edlio Almeida",
                "Wagner Meira Jr."
            ],
            "title": "Characterizing and detecting hateful users on twitter",
            "venue": "Proceedings of the International AAAI Conference on Web and Social Media.",
            "year": 2018
        },
        {
            "authors": [
                "Maarten Sap",
                "Dallas Card",
                "Saadia Gabriel",
                "Yejin Choi",
                "Noah A Smith."
            ],
            "title": "The risk of racial bias in hate speech detection",
            "venue": "Proceedings of the 57th annual meeting of the association for computational linguistics, pages 1668\u20131678.",
            "year": 2019
        },
        {
            "authors": [
                "Maarten Sap",
                "Saadia Gabriel",
                "Lianhui Qin",
                "Dan Jurafsky",
                "Noah A Smith",
                "Yejin Choi."
            ],
            "title": "Social bias frames: Reasoning about social and power implications of language",
            "venue": "arXiv preprint arXiv:1911.03891.",
            "year": 2019
        },
        {
            "authors": [
                "Anna Schmidt",
                "Michael Wiegand."
            ],
            "title": "A survey on hate speech detection using natural language processing",
            "venue": "Proceedings of the fifth international workshop on natural language processing for social media, pages 1\u201310.",
            "year": 2017
        },
        {
            "authors": [
                "Noam Shazeer",
                "Mitchell Stern."
            ],
            "title": "Adafactor: Adaptive learning rates with sublinear memory cost",
            "venue": "International Conference on Machine Learning, pages 4596\u20134604. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "Rohit Sridhar",
                "Diyi Yang."
            ],
            "title": "Explaining toxic text via knowledge enhanced text generation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2022
        },
        {
            "authors": [
                "Jiao Sun",
                "Swabha Swayamdipta",
                "Jonathan May",
                "Xuezhe Ma."
            ],
            "title": "Investigating the benefits of freeform rationales",
            "venue": "arXiv preprint arXiv:2206.11083.",
            "year": 2022
        },
        {
            "authors": [
                "Alon Talmor",
                "Jonathan Herzig",
                "Nicholas Lourie",
                "Jonathan Berant."
            ],
            "title": "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Bertie Vidgen",
                "Tristan Thrush",
                "Zeerak Waseem",
                "Douwe Kiela."
            ],
            "title": "Learning from the worst: Dynamically generated datasets to improve online hate detection",
            "venue": "arXiv preprint arXiv:2012.15761.",
            "year": 2020
        },
        {
            "authors": [
                "PeiFeng Wang",
                "Aaron Chan",
                "Filip Ilievski",
                "Muhao Chen",
                "Xiang Ren."
            ],
            "title": "PINTO: Faithful language reasoning using prompt-generated rationales",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Peifeng Wang",
                "Zhengyang Wang",
                "Zheng Li",
                "Yifan Gao",
                "Bing Yin",
                "Xiang Ren."
            ],
            "title": "Scott: Self-consistent chain-of-thought distillation",
            "venue": "arXiv preprint arXiv:2305.01879.",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2203.11171.",
            "year": 2022
        },
        {
            "authors": [
                "Zeerak Waseem",
                "Thomas Davidson",
                "Dana Warmsley",
                "Ingmar Weber."
            ],
            "title": "Understanding abuse: A typology of abusive language detection subtasks",
            "venue": "arXiv preprint arXiv:1705.09899.",
            "year": 2017
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "arXiv preprint arXiv:2109.01652.",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Jack Hessel",
                "Swabha Swayamdipta",
                "Mark Riedl",
                "Yejin Choi."
            ],
            "title": "Reframing human-ai collaboration for generating free-text explanations",
            "venue": "arXiv preprint arXiv:2112.08674.",
            "year": 2021
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Ana Marasovi\u0107",
                "Noah A Smith."
            ],
            "title": "Measuring association between labels and free-text rationales",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10266\u201310284.",
            "year": 2021
        },
        {
            "authors": [
                "Seonghyeon Ye",
                "Doyoung Kim",
                "Sungdong Kim",
                "Hyeonbin Hwang",
                "Seungone Kim",
                "Yongrae Jo",
                "James Thorne",
                "Juho Kim",
                "Minjoon Seo."
            ],
            "title": "Flask: Fine-grained language model evaluation based on alignment skill sets",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric Xing"
            ],
            "title": "Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685",
            "year": 2023
        },
        {
            "authors": [
                "Wang et al",
                "Lampinen"
            ],
            "title": "2022), while others fine-tune smaller models using the rationales generated by LLMs (Ho et al., 2022",
            "venue": "Wang et al., 2023a; Chan et al.,",
            "year": 2023
        },
        {
            "authors": [
                "Adafactor (Shazeer",
                "Stern"
            ],
            "title": "2018) optimizer with batch size of 32 and learning rate \u2208 {5e-3, 5e-4",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The increase in the use of online media has intensified the exposure to hate speech, prompting the need for effective detection systems (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018). While early works have been limited to the classification of explicit hate speech (Caselli et al., 2020; Mathew et al., 2021), recent works have drawn our attention to implicit forms of hate speech which are more prevalent, yet subtle. (Jurgens et al., 2019).\nTo tackle these nuanced forms of hate speech, it is important for systems to not only identify hate speech but also provide interpretable explanations\n\u2217equal contribution \u2020corresponding authors\n(Liu et al., 2019). This can help mitigate distributional biases inherent in simple classification, allowing people to understand and reason about the potential harms of hateful text (Sap et al., 2019b). Explanations can also improve the transparency of content moderation on social media (Gillespie, 2018).\nRecent works on hate speech understanding (Sap et al., 2019b; ElSherief et al., 2021; Huang et al., 2022) have considered training autoregressive language models to generate underlying explanations on hate speech. The models are trained on humanwritten free-text rationales such as implied statements and targeted groups. However, despite the use of novel benchmark datasets, i.e., SBIC (Sap et al., 2019b) and Implicit Hate (ElSherief et al., 2021), the trained models struggle to generate detailed and comprehensive explanations. Moreover, we observe that the provided rationales give marginal improvement to detection performance under joint training.\nA potential cause of the limited supervision provided by existing annotations on understanding and explaining hate speech may be the existence of critical gaps in reasoning. For example, as shown in Figure 1, the implied statement of the post \u201cHow dark is my humour? It picks cotton\u201d is annotated as \u201cblack folks are slaves\u201d, in SBIC. To understand this implication, one must understand that \u201cdark\u201d implies \u201cblack folks\u201d, and the phrase \u201cpicks cotton\u201d relates to the historical background of African Americans. While this may be obvious to human annotators, language models are known to lack societal knowledge and commonsense reasoning skills to understand these nuances (Talmor et al., 2019; Li et al., 2022; Choi et al., 2023). This leaves a significant gap between the training objectives of classification and generating annotated implications, which may harm supervision (Wiegreffe et al., 2021b; Wang et al., 2023a).\nDrawing inspiration from the reasoning capabilities of large language models (LLMs) improved with chain-of-thought (CoT) reasoning (Wei et al., 2022), we present our novel approach \u201cExplainable HAte Speech Detection with Step-by- Step REasoning (HARE)\u201d. We leverage LLM-generated free-text rationales using CoT prompts to fill in the gaps of reasoning in existing hate speech annotations and enhance supervision of generative detection models. To create these rationales, we propose two approaches: (1) adopt CoT prompts to create comprehensive rationales that align with the given texts and (2) incorporate existing human annotations from benchmarks in the CoT prompts to bridge the logical gap between the input text and human annotations. When tested on the challenging SBIC and Implicit Hate datasets, our approach outperforms standard fine-tuning with given human annotations and provides enhanced explanations behind the detection results."
        },
        {
            "heading": "2 Method",
            "text": ""
        },
        {
            "heading": "2.1 Preliminaries",
            "text": "The task of hate speech detection can be framed as a generative task that inputs the text P and outputs a prediction class C, formulated as p(C|P ), indicating whether the speech is classified as \u201chate\u201d or \u201cnot hate\u201d.1 Furthermore, by incorporating humanwritten rationales of the target groups T and implied statements I of hate speech from the SBIC\n1We refer to the \u201coffensive\u201d category in SBIC as \u201chate\u201d class. In the experiment, we use the wording \u201coffensive\u201d as is.\nand Implicit Hate datasets, the task can also be formulated to sequentially output C, T , and then I , as p([C;T ; I]|P ). Hence, the model trained with human-written annotations is designed to generate C and then provide explanations using annotations T and I . However, due to a logical gap between the speech P and the annotations T and I , training a model with these annotated rationales does not significantly enhance the model\u2019s ability to comprehend hate speech.\nChain-of-thought prompting (Wei et al., 2022) refers to a prompting method that generates a chain of reasoning to derive answers. Kojima et al. (2022) introduce a method of including the phrase \u201cLet\u2019s think step by step\u201d at the end of the prompt to generate reasoning in a zero-shot setting."
        },
        {
            "heading": "2.2 Fr-HARE & Co-HARE",
            "text": "In this paper, we propose HARE, a framework that fine-tunes using rationales generated by LLMs through CoT prompts. By leveraging these rationales, the model is trained to discern whether a text is classified as hate speech or not. This approach of utilizing the LLM-generated rationales not only improve the model performance as in previous works (Ho et al., 2022; Wang et al., 2023a) but also enables the model to generate more plausible explanations. Subsequently, we introduce two variants that employ different methods for extracting rationales from LLMs.\nFr-HARE Our first strategy, Free-HARE (Fr-HARE), extracts rationales using LLM from the LLM without referencing the annotated rationales. In line with Kojima et al. (2022), we employ zero-shot prompts to prompt the LLM to extract rationales, as follows:\nDetermine whether the following post is offensive, and explain why. Post: P Answer: Let\u2019s explain step by step.\nFrom the given prompt above, both the rationale R without any logical leap and the classification result C are generated. Then, we utilize the generated rationale R by training the model to sequentially generate C and R, represented as p([C;R]|P ). Utilizing these rationales enhances the model\u2019s ability to comprehend the context of hate speech, thus leading to explanations that are more intelligible to humans.\nCo-HARE We also propose a second strategy, Conditioned-HARE (Co-HARE), which utilizes human-written rationales to formulate CoT prompts. Fr-HARE generates complete rationales entirely from scratch, disregarding human annotations included in the datasets. Although Fr-HARE offers the advantage of being universally applicable to any hate speech dataset, the LLM-generated rationales might lack consistency due to wide variation in reasoning sequences. To address this inconsistency, Co-HARE integrates human-written rationales about target groups T and implied statements I into the CoT prompt as follows:\nDetermine whether the following post is offensive, and explain why. Provide a brief, step-by-step explanation of how the post targets the specified group and how it leads to the implied statement provided. Post: P Target: T Implied statement: I Answer: Let\u2019s explain step by step.\nThen, we train the model using extracted rationales, as in Fr-HARE. For samples labeled as \u201cnot hate\u201d that do not include human-written rationales, we apply the prompt used in Fr-HARE. While Co-HARE requires human-written rationales, it generates rationales that are more tailored to the specific requirements and features of the dataset, due to its guided nature. Therefore, the model trained with Co-HARE can provide explanations that align more closely with the forms of rationales that humans construct.\nDetails of HARE Once we have extracted the rationales from the LLMs, we follow the approach of Kojima et al. (2022) to have the LLMs predict the class. Specifically, we employ a two-stage extraction process. In the first stage, we extract both the class C and the rationale R from the LLMs using our HARE method, represented as p([C;R]|P ), as previously outlined. In the second stage, we prompt the LLMs again, this time to predict the class C given the extracted rationales R and the post P , denoted as p(C|R,P ). During fine-tuning on hate speech datasets, if the predicted class C coincides with the true answer C, we concatenate C with the extracted rationale R. If the predicted labels are incorrect, the models are solely trained to predict the class C. Furthermore, following the findings of Ho et al. (2022), we generate multiple distinct rationales to facilitate the learning process."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Experimental Setup",
            "text": "We utilize SBIC and Implicit Hate datasets for our fine-tuning experiments. Our models are trained to classify the offensiveness and hatefulness of posts, using SBIC and Implicit Hate, respectively. It is noteworthy that in our Implicit Hate experiments, we combine both the explicit and implicit hate classes into a single \u201chate\u201d category. We set up baselines with two families of models: C, a model trained exclusively for classification, and C+T+I , a model trained using human-written rationales. For Fr-HARE and Co-HARE, by using gpt-3.5-turbo-0613 that is known for its reasoning capabilities (Ouyang et al., 2022), we extract four and eight different rationales per each sample in SBIC and Implicit Hate, respectively, following the hyperparameter setting of Ho et al. (2022). Subsequently, we fine-tune the model, setting LLM-generated rationales R and class C as target sequence. For performance evaluation, we measure detection accuracy and compute the F1 score of classification, regarding \u201chate\u201d as the positive class. We make use of Flan-T5 (Wei et al., 2021) with different model configurations: small, base and large. We also conduct experiments using the large models of T5 (Raffel et al., 2020) and GPT-2 (Radford et al., 2019). A more detailed explanation of our experimental setup can be found in Appendix B."
        },
        {
            "heading": "3.2 Results and Discussions",
            "text": "Do LLM-generated rationales improve detection performance? Table 1 presents the performance of hate speech detection according to different methods on the SBIC and Implicit Hate datasets. Our strategies Fr-HARE and Co-HARE consistently exhibit superior performance over other baseline methods, regardless of the model size. This suggests that even though the baseline method is trained using human-written rationales, the more detailed and logically-sequenced LLM-generated rationales of HARE can further aid the model in understanding the input text and accurately classifying it as hate speech. Therefore, the results demonstrate that the quality of rationales has a strong impact on classification. Furthermore, the performance of our method consistently improves as the model size increases, in contrast to baselines. This suggests that diverse reasoning becomes increasingly beneficial as scale grows. This notable im-\nprovement with HARE is achieved by using only 40$ for each method in our approach, demonstrating that the ability to reason can be effectively trained with rationales from LLMs.\nAdditionally, while Fr-HARE and Co-HARE exhibit similar performance, Co-HARE has a slight edge in most cases. This is because Co-HARE is guided by human-written annotations, which results in better alignment with the setting of the datasets, as we mentioned in Section 2.2. It is also noteworthy that all the fine-tuned models surpass both Zero-Shot (ZS) and Zero-Shot CoT (ZSCoT, Kojima et al. (2022)) classification performance of GPT-3.5-turbo, indicating that merely employing LLM with CoT prompts is not sufficient to tackle this task.\nAre HARE models more generalizable? To assess the ability of our methods to generalize across different datasets, we evaluate the models finetuned on the SBIC datasets using each method on two distinct datasets, HateXplain (Mathew et al., 2021) and DynaHate (Vidgen et al., 2020). Both datasets encompass forms of explicit and implicit hate. On both datasets, our methods Fr-HARE and Co-HARE both outperform baseline methods, indicating that our methods enhance the generalizability of the models by improving their reasoning\nAs Co-HARE is designed to create rationales that align better with human-written rationales, we also conduct pairwise comparison between Co-HARE and C+T+I to determine which method aligns better with human-written labels. Figure 2 (b) clearly shows that the rationales generated by the model trained using Co-HARE are aligned more to actual human-written rationales than the ones generated by the model trained directly with humanwritten rationales. This also highlights the value of Co-HARE, as it aligns better with natural intuition, indicating that both Fr-HARE and Co-HARE can be utilized for different purposes.\nHow does HARE detect and contextualize hate speech? Figure 3 presents model outputs using HARE and baseline method. A more detailed qualitative study of randomly selected samples is available in Appendix D. In the first sample, the post makes light of harming a young girl using phrases like \u201clike I like cigar\u201d and \u201cburlap sack\u201d. While our model, Fr-HARE, does not explicitly connect the \u201cburlap sack\u201d to the idea of a kidnapped girl, it does recognize the harmful implications towards the girl. Furthermore, Co-HARE understands the historical context behind the term \u201cburlap sack\u201d. The presence of the positive term \u201clike\u201d and the ab-\nsence of overtly derogatory words might lead some models to classify the statement as non-offensive. However, our approach appropriately identifies the underlying hateful context.\nIn the second sample, the post contains hate speech targeting Jewish victims of the Holocaust by referencing Anne Frank. Our model accurately recognizes the historical background of Anne Frank as a Holocaust victim. While the baseline seems to overlook the historical significance associated with Anne Frank, our method correctly identifies her and assumes that the reference constitutes harassment against a Jewish victim, even though there is a slight misunderstanding about the context of \u201clack of speaking\u201d in Fr-HARE.\nIs GPT-3.5 a qualified teacher? Since our framework is based on distillation of generated rationales from GPT-3.5 to smaller models, it is crucial to verify whether the teacher is qualified. Figure 4 displays rationales produced by GPT-3.5-turbo, which is employed to train the student model. This example illustrates that the LLM not only discerns the hateful nuances towards both white and black individuals, but also offers more detailed explanations compared to rationales written by humans. Notably, it accurately correlates the historical context, associating the word \u201cslaves\u201d with \u201cpets\u201d. More analysis of rationales from GPT-3.5-turbo can be found in Appendix D.2."
        },
        {
            "heading": "4 Conclusion",
            "text": "In this paper, we present HARE framework to improve the ability of the language model to understand hate speech and provide clearer explanations for its decisions. We propose utilizing CoT reasonings extracted from LLMs in two variants to overcome the logical gaps in human-annotated rationales. When fine-tuned on the SBIC and Implicit Hate datasets, our methods achieve superior detection performance and better qualified explanations.\nLimitations\nWhile we assess the quality of explanations generated by HARE using GPT-4, we do not conduct human evaluations, which are crucial for tasks requiring human-readable explanations. The primary reason for this omission is that the hate speech content and its respective explanations could be excessively offensive for annotators and GPT-4 already aligns with the level of inter-human agreement. In addition, the \"verbosity bias\", characterized by a preference for the longer text of GPT-4 as indicated by (Liu et al., 2023), may also serve as a limitation in our evaluation process.\nEthics Statement\nPredicting whether an online post contains hatespeech is both technically and socially challenging. While methods for automating hatespeech detection have utility in an online platform, it is critical that these are tuned and used appropriately. False-positive errors have potential to censor online speech, further marginalizing specific user groups, for example: use of n***** in AAVE English may be flagged. It is critical to understand specific reasoning behind a classification including deeply social reasons. While language models act as a mechanism to generate reasonable explanations, it is critical that they are used appropriately to prevent them from inadvertently educating users on how to craft more subtle and toxic language. We used automated evaluation metrics in this paper to prevent exposure of toxic language to human annotators. However, real-world usage would require validation that deeply rooted social issues are expressed correctly by these models.\nIt is also important to note that there might be concerns about the inherent bias in the GPT-3.5 model. While not flawless, GPT-3.5 has demonstrated its impartiality regarding gender, race, ethnicity, and religion by achieving the highest grade on the Harmfulness metric within the FLASK evaluation framework (Ye et al., 2023). Crucially, we only select rationales that align with the ground truth label for training, thereby mitigating biases not in sync with human annotators. Analysis of GPT-3.5-turbo can be found in Section 3 and Appendix D.2."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was supported by Institute of Information & communications Technology Planning &\nEvaluation (IITP) grant funded by Korea government (MSIT) [No. 2021-0-00907, Development of Adaptive and Lightweight Edge-Collaborative Analysis Technology for Enabling Proactively Immediate Response and Rapid Learning, 90%] and [No. 2019-0-00075, Artificial Intelligence Graduate School Program (KAIST), 10%]."
        },
        {
            "heading": "A Related Work",
            "text": "Hate Speech Detection Hate speech (Waseem et al., 2017) is a form of language designed to offend a particular individual or groups. In this study, we expand this definition by incorporating the broader concept of offensive language as in (Burnap and Williams, 2016; Ribeiro et al., 2018). Numerous recent works on hate speech detection have delved into providing underlying explanations of prediction on hate speech (Sap et al., 2019a,b; Mathew et al., 2021; ElSherief et al., 2021; Lin, 2022). One line on research focuses on keywordbased explanations (Sap et al., 2019a; Davidson et al., 2019; Mathew et al., 2021; Kim et al., 2022), but this approach often fails to capture implicit hatefulness that is not explicitly present in the text. Another approach involves explanations utilizing external knowledge sources (Sridhar and Yang, 2022; Lin, 2022), but these methods aim to solely improve classification performance. Yet another studies involve training generative models with humanwritten free-text rationales (Sap et al., 2019b; ElSherief et al., 2021; Huang et al., 2022) present in multiple benchmarks (Sap et al., 2019b; ElSherief et al., 2021). Nevertheless, due to the existence of logical gaps in these human-annotated rationales (Aggarwal et al., 2021; Sun et al., 2022), relying solely on these rationales results in sub-optimal detection and explanation quality. Our proposed HARE shows its effectiveness by incorporating LLM-generated rationales, which include logical completeness and abundant explanatory power extracted with our CoT prompting.\nSelf-Rationalization Self-rationalization, a technique where models provide explanations for their predictions, has been extensively studied to make models more understandable and transparent (Marasovic\u0301 et al., 2021; Wiegreffe et al., 2021a,b). Recent studies leverage rationale-augmented exemplars to few-shot prompt LLMs (Wei et al., 2022; Wang et al., 2022; Lampinen et al., 2022), while others fine-tune smaller models using the rationales generated by LLMs (Ho et al., 2022; Wang et al., 2023a; Chan et al., 2023; Wang et al., 2023b). Several works apply their self-rationalization methods to detect hate speech and generate underlying explanations (Marasovic\u0301 et al., 2021; Ludan et al., 2023; Chen et al., 2023). However, they directly use human-written rationales in their framework, which may lack logical completeness. Instead, our\nmethod adopts the approach of utilizing the rationales generated by LLMs.\nB Implementation Details\nTraining Details All the experiments utilize an Adafactor (Shazeer and Stern, 2018) optimizer with batch size of 32 and learning rate \u2208 {5e-3, 5e-4, 5e-5} without learning rate scheduling. We train all models for 10 epochs and utilize the checkpoint with the highest validation F1 score to evaluate on the test set. Top-k sampling with k=20 is adopted as a decoding method. For dataset split, we use official dataset split for SBIC and random split with 6:2:2 ratio for train, validation, and test sets for Implicit Hate. All our experiments are conducted using a single NVIDIA A6000 GPU."
        },
        {
            "heading": "C Details on GPT-4 Evaluation",
            "text": "To conduct GPT-4 evaluation, we utilize the prompts proposed by Zheng et al. (2023). For single answer grading, the prompt format for GPT-4 is written as follows:\n[Instruction] Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, please rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\" [Question] Determine whether the following post is offensive or not. You must choose one of the options. Post: P Options: (A) Offensive (B) Not offensive [The Start of Assistant\u2019s Answer] C,R [The End of Assistant\u2019s Answer]\nTo compare the alignment of generated rationales with human-written rationales, we conduct a pairwise comparison of explanations against ground-truth rationales. These ground-truth rationales encompass the annotated target groups and implied statements from the instruction, and we compare the results. Additionally, to minimize bias from the order of candidate answers, we adopt the approach of Zheng et al. (2023), considering both original and swapped orders of predicted explanations. If C + T + I and Co-HARE are chosen alternately, it is deemed a tie. Should one method\nbe selected following a tie, that method is considered as the chosen one. The prompt format for the pairwise comparison is provided below:\n[Instruction] Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user\u2019s instructions and answers the user\u2019s question more accurate. When choosing the assistant, please consider the true answers below: Target: T Implied Statement: I Your evaluation should consider which response is more similar to the true answers. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any positional biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is more accurate, \"[[B]]\" if assistant B is more accurate, and \"[[C]]\" for a tie. [Question] Determine whether the following post is offensive or not. You must choose one of the options. Post: P Options: (A) Offensive (B) Not offensive [The Start of Assistant A\u2019s Answer] Answer from one method [The End of Assistant A\u2019s Answer] [The Start of Assistant B\u2019s Answer] Answer from another method [The End of Assistant B\u2019s Answer]"
        },
        {
            "heading": "D Qualitative Study",
            "text": "D.1 Qualitative Study of HARE\nFigures 5, 6, 7, and 8 showcase results generated by the fine-tuned Flan-T5-large model using HARE and C+T+I , based on test samples from SBIC. Although a brief explanation is provided in Section 3.2, we delve deeper with an extended analysis of the 20 examples from our qualitative study. These 20 samples were randomly chosen in proportion to their correct and incorrect predictions across the different methods.\nWhen comparing human-written annotations with HARE, it becomes evident that the annotated rationales in SBIC often take the form of implied statements, following a simple Hearst-like pattern (Sap et al., 2019b). Learning from such rationales, which are closely tied to the conclusion, creates a logical gap for the model and makes interpretation challenging for humans. For instance, understand-\ning hate speech without background knowledge references, such as \u2019burlap sack\u2019, can make it difficult to see the connection between the statement \"girls are not worthy of equal life\" and the provided sentence. Figures 5 and 6 showcase successful cases where models have attempted to bridge this reasoning gap through HARE, offering more detailed rationales that encompass the context. Furthermore, these models exhibit capabilities not seen in previous research, such as detecting terms with historical significance (e.g., \u2019burlap sack\u2019 or \u2019Anne Frank\u2019) or common words that may carry hateful connotations (e.g., \u2019reds\u2019), thus enhancing the intermediate reasoning process.\nHowever, when examining the failure cases in Figures 7 and 8, the results show that HARE sometimes fails due to increased sensitivity to potentially harmful terms, thereby classifying them as offensive. While this increased sensitivity can be viewed as a drawback, there are instances, such as with the Alzheimer example, where an expression might be interpreted as hateful depending on the individual. This suggests that HARE aims to classify a post as hateful if it could be considered offensive to certain groups. Moreover, considering the David Bread Katz example, it is also challenging for HARE to decide if the post is offensive if it post with background that it hasn\u2019t encountered, possibly due to a lack of background knowledge regarding the implied shooting incident, illustrating the limitation of LLM distillation.\nD.2 Qualitative Study on GPT-3.5 rationales\nWhen comparing annotations with rationales generated by GPT-3.5, we observe that human-written rationales from SBIC use implied statements that follow simple Hearst-like patterns (Sap et al., 2019b). In contrast, models like LLM (e.g., GPT-3.5) tend to provide detailed, step-by-step explanations, often complemented by relevant social background information, which is immensely beneficial. For example, while earlier rationales might omit mentioning Bill Cosby\u2019s conviction of sexual assault, GPT-3.5 explicitly informs us of this fact, greatly enhancing comprehension. A particularly striking example is the \"pet\" case. While a human annotator perceived it as hate speech targeted at black individuals, GPT-3.5 points out that it could also be used derogatorily against white individuals, thereby emphasizing the potential biases in hate speech detection.\nWhen rationales are categorized and structured, as seen in SBIC, instead of being tailored to individual posts, they may not be sufficient for learning implications. This could explain why, as suggested by Table 2, there\u2019s a decrease in generalization for C+T+I . Our approach offers aligned rationales for each post at minimal cost, enabling the learning of diverse reasons for potential hate, which in turn leads to enhanced generalization."
        },
        {
            "heading": "3. The use of the word \"cunt\" is a derogatory term for female genitalia. 4. The post implies that the grandad is a woman, as it uses the term \"cunt\" to insult him. 5. The post also implies that women are weak and need to be controlled by men. Overall, the post targets women by using derogatory language and perpetuating harmful stereotypes about their strength and need for control. Therefore, it can be considered offensive.",
            "text": ""
        },
        {
            "heading": "3. The post ends with a smiling emoji, indicating that the person is expressing their positive sentiment. Based on this analysis, the post does not appear to be offensive. It is a personal statement about personal beliefs and a suggestion to live a life pleasing to God.",
            "text": ""
        }
    ],
    "title": "HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning",
    "year": 2023
}