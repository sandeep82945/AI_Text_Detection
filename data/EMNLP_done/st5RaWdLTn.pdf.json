{
    "abstractText": "To alleviate the data scarcity problem in Endto-end speech translation (ST), pre-training on data for speech recognition and machine translation is considered as an important technique. However, the modality gap between speech and text prevents the ST model from efficiently inheriting knowledge from the pre-trained models. In this work, we propose AdaTranS for end-to-end ST. It adapts the speech features with a new shrinking mechanism to mitigate the length mismatch between speech and text features by predicting word boundaries. Experiments on the MUST-C dataset demonstrate that AdaTranS achieves better performance than the other shrinking-based methods, with higher inference speed and lower memory usage. Further experiments also show that AdaTranS can be equipped with additional alignment losses to further improve performance.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xingshan Zeng"
        },
        {
            "affiliations": [],
            "name": "Liangyou Li"
        },
        {
            "affiliations": [],
            "name": "Qun Liu"
        },
        {
            "affiliations": [],
            "name": "Huawei Noah\u2019s"
        }
    ],
    "id": "SP:d573e7166e1ce89bf46e3ecffae5cee823877670",
    "references": [
        {
            "authors": [
                "Sameer Bansal",
                "Herman Kamper",
                "Karen Livescu",
                "Adam Lopez",
                "Sharon Goldwater."
            ],
            "title": "Pre-training on high-resource speech recognition improves lowresource speech-to-text translation",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Alexandre Berard",
                "Laurent Besacier",
                "Ali Can Kocabiyikoglu",
                "Olivier Pietquin."
            ],
            "title": "End-to-end automatic speech translation of audiobooks",
            "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2018, Calgary, AB,",
            "year": 2018
        },
        {
            "authors": [
                "Alexandre Berard",
                "Olivier Pietquin",
                "Christophe Servan",
                "Laurent Besacier."
            ],
            "title": "Listen and translate: A proof of concept for end-to-end speech-to-text translation",
            "venue": "CoRR, abs/1612.01744.",
            "year": 2016
        },
        {
            "authors": [
                "Chih-Chiang Chang",
                "Hung-yi Lee."
            ],
            "title": "Exploring continuous integrate-and-fire for adaptive simultaneous speech translation",
            "venue": "Interspeech 2022, 23rd Annual Conference of the International Speech Commu-",
            "year": 2022
        },
        {
            "authors": [
                "Jan Chorowski",
                "Dzmitry Bahdanau",
                "Dmitriy Serdyuk",
                "Kyunghyun Cho",
                "Yoshua Bengio."
            ],
            "title": "Attention-based models for speech recognition",
            "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Pro-",
            "year": 2015
        },
        {
            "authors": [
                "Mattia A. Di Gangi",
                "Roldano Cattoni",
                "Luisa Bentivogli",
                "Matteo Negri",
                "Marco Turchi."
            ],
            "title": "MuST-C: a Multilingual Speech Translation Corpus",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2019
        },
        {
            "authors": [
                "Linhao Dong",
                "Bo Xu."
            ],
            "title": "CIF: continuous integrate-and-fire for end-to-end speech recognition",
            "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020, pages 6079\u20136083.",
            "year": 2020
        },
        {
            "authors": [
                "Qian Dong",
                "Yaoming Zhu",
                "Mingxuan Wang",
                "Lei Li."
            ],
            "title": "Learning when to translate for streaming speech",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 680\u2013694. Association",
            "year": 2022
        },
        {
            "authors": [
                "Qianqian Dong",
                "Mingxuan Wang",
                "Hao Zhou",
                "Shuang Xu",
                "Bo Xu",
                "Lei Li."
            ],
            "title": "Consecutive decoding for speech-to-text translation",
            "venue": "CoRR, abs/2009.09737.",
            "year": 2020
        },
        {
            "authors": [
                "Qianqian Dong",
                "Yaoming Zhu",
                "Mingxuan Wang",
                "Lei Li."
            ],
            "title": "Unist: Unified end-to-end model for streaming and non-streaming speech translation",
            "venue": "CoRR, abs/2109.07368.",
            "year": 2021
        },
        {
            "authors": [
                "Long Duong",
                "Antonios Anastasopoulos",
                "David Chiang",
                "Steven Bird",
                "Trevor Cohn."
            ],
            "title": "An attentional model for speech translation without transcription",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Qingkai Fang",
                "Rong Ye",
                "Lei Li",
                "Yang Feng",
                "Mingxuan Wang."
            ],
            "title": "STEMM: Self-learning with speech-text manifold mixup for speech translation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Marco Gaido",
                "Mauro Cettolo",
                "Matteo Negri",
                "Marco Turchi."
            ],
            "title": "CTC-based compression for direct speech translation",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Alex Graves",
                "Santiago Fern\u00e1ndez",
                "Faustino J. Gomez",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
            "venue": "Machine Learning, Proceedings of the Twenty-Third Interna-",
            "year": 2006
        },
        {
            "authors": [
                "Anmol Gulati",
                "James Qin",
                "Chung-Cheng Chiu",
                "Niki Parmar",
                "Yu Zhang",
                "Jiahui Yu",
                "Wei Han",
                "Shibo Wang",
                "Zhengdong Zhang",
                "Yonghui Wu",
                "Ruoming Pang."
            ],
            "title": "Conformer: Convolution-augmented transformer for speech recognition",
            "venue": "Interspeech 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Chi Han",
                "Mingxuan Wang",
                "Heng Ji",
                "Lei Li."
            ],
            "title": "Learning shared semantic space for speech-to-text translation",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2214\u20132225, Online. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Wenyong Huang",
                "Wenchao Hu",
                "Yu Ting Yeung",
                "Xiao Chen."
            ],
            "title": "Conv-transformer transducer: Low latency, low frame rate, streamable end-to-end speech recognition",
            "venue": "Interspeech 2020, 21st Annual Conference of the International Speech Communication",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson."
            ],
            "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2018
        },
        {
            "authors": [
                "Phuong-Hang Le",
                "Hongyu Gong",
                "Changhan Wang",
                "Juan Pino",
                "Benjamin Lecouteux",
                "Didier Schwab."
            ],
            "title": "Pre-training for speech translation: CTC meets optimal transport",
            "venue": "International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Hon-",
            "year": 2023
        },
        {
            "authors": [
                "Yuchen Liu",
                "Junnan Zhu",
                "Jiajun Zhang",
                "Chengqing Zong."
            ],
            "title": "Bridging the modality gap for speechto-text translation",
            "venue": "CoRR, abs/2010.14920.",
            "year": 2020
        },
        {
            "authors": [
                "Lambert Mathias",
                "William Byrne."
            ],
            "title": "Statistical phrase-based speech translation",
            "venue": "2006 IEEE International Conference on Acoustics Speech and Signal Processing, ICASSP 2006, Toulouse, France, May 14-19, 2006, pages 561\u2013564. IEEE.",
            "year": 2006
        },
        {
            "authors": [
                "Hermann Ney."
            ],
            "title": "Speech translation: coupling of recognition and translation",
            "venue": "Proceedings of the 1999 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP \u201999, Phoenix, Arizona, USA, March 15-19, 1999, pages 517\u2013520.",
            "year": 1999
        },
        {
            "authors": [
                "Vassil Panayotov",
                "Guoguo Chen",
                "Daniel Povey",
                "Sanjeev Khudanpur."
            ],
            "title": "Librispeech: An ASR corpus based on public domain audio books",
            "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2015, South",
            "year": 2015
        },
        {
            "authors": [
                "Elizabeth Salesky",
                "Alan W Black."
            ],
            "title": "Phone features improve speech translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2388\u20132397, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Elizabeth Salesky",
                "Matthias Sperber",
                "Alan W Black."
            ],
            "title": "Exploring phoneme-level speech representations for end-to-end speech translation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1835\u20131841, Flo-",
            "year": 2019
        },
        {
            "authors": [
                "Yun Tang",
                "Hongyu Gong",
                "Ning Dong",
                "Changhan Wang",
                "Wei-Ning Hsu",
                "Jiatao Gu",
                "Alexei Baevski",
                "Xian Li",
                "Abdelrahman Mohamed",
                "Michael Auli",
                "Juan Pino."
            ],
            "title": "Unified speech-text pre-training for speech translation and recognition",
            "venue": "Proceedings",
            "year": 2022
        },
        {
            "authors": [
                "Yun Tang",
                "Juan Pino",
                "Xian Li",
                "Changhan Wang",
                "Dmitriy Genzel."
            ],
            "title": "Improving speech translation by understanding and learning from the auxiliary text translation task",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Yun Tang",
                "Juan Miguel Pino",
                "Changhan Wang",
                "Xutai Ma",
                "Dmitriy Genzel."
            ],
            "title": "A general multitask learning framework to leverage text data for speech to text tasks",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Process-",
            "year": 2021
        },
        {
            "authors": [
                "Chengyi Wang",
                "Yu Wu",
                "Shujie Liu",
                "Zhenglu Yang",
                "Ming Zhou"
            ],
            "title": "Bridging the gap between pretraining and fine-tuning for end-to-end speech",
            "year": 2020
        },
        {
            "authors": [
                "Ron J. Weiss",
                "Jan Chorowski",
                "Navdeep Jaitly",
                "Yonghui Wu",
                "Zhifeng Chen."
            ],
            "title": "Sequence-to-sequence models can directly transcribe foreign speech",
            "venue": "CoRR, abs/1703.08581.",
            "year": 2017
        },
        {
            "authors": [
                "Chen Xu",
                "Bojie Hu",
                "Yanyang Li",
                "Yuhao Zhang",
                "Shen Huang",
                "Qi Ju",
                "Tong Xiao",
                "Jingbo Zhu."
            ],
            "title": "Stacked acoustic-and-textual encoding: Integrating the pre-trained models into speech translation encoders",
            "venue": "Proceedings of the 59th Annual Meet-",
            "year": 2021
        },
        {
            "authors": [
                "Rong Ye",
                "Mingxuan Wang",
                "Lei Li."
            ],
            "title": "End-toend speech translation via cross-modal progressive training",
            "venue": "Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September",
            "year": 2021
        },
        {
            "authors": [
                "Rong Ye",
                "Mingxuan Wang",
                "Lei Li."
            ],
            "title": "Crossmodal contrastive learning for speech translation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Xingshan Zeng",
                "Liangyou Li",
                "Qun Liu."
            ],
            "title": "RealTranS: End-to-end simultaneous speech translation with convolutional weighted-shrinking transformer",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2461\u20132474,",
            "year": 2021
        },
        {
            "authors": [
                "Biao Zhang",
                "Barry Haddow",
                "Rico Sennrich."
            ],
            "title": "Efficient CTC regularization via coarse labels for end-to-end speech translation",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL",
            "year": 2023
        },
        {
            "authors": [
                "Ziqiang Zhang",
                "Long Zhou",
                "Junyi Ao",
                "Shujie Liu",
                "Lirong Dai",
                "Jinyu Li",
                "Furu Wei."
            ],
            "title": "Speechut: Bridging speech and text with hidden-unit for encoder-decoder based speech-text pre-training",
            "venue": "EMNLP2022. Association for Computational Lin-",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "End-to-end speech translation (ST), which directly translates source speech into text in another language, has achieved remarkable progress in recent years (Berard et al., 2016; Duong et al., 2016; Weiss et al., 2017; Berard et al., 2018; Xu et al., 2021; Ye et al., 2022). Compared to the conventional cascaded systems (Ney, 1999; Mathias and Byrne, 2006), the end-to-end models are believed to have the advantages of low latency and less error propagation. A well-trained end-to-end model typically needs a large amount of training data. However, the available direct speech-translation corpora are very limited (Di Gangi et al., 2019). Given the fact that data used for automatic speech recognition (ASR) and machine translation (MT) are much richer, the paradigm of \u201cpre-training on ASR and MT data and then fine-tuning on ST\u201d becomes one of the approaches to alleviate the data scarcity problem (Bansal et al., 2019; Xu et al., 2021).\nIt has been shown that decoupling the ST encoder into acoustic and semantic encoders is ben-\neficial to learn desired features (Liu et al., 2020; Zeng et al., 2021). Initializing the two encoders by pre-trained ASR and MT encoders, respectively, can significantly boost the performance (Xu et al., 2021). However, the modality gap between speech and text might prevent the ST models from effectively inheriting the pre-trained knowledge (Xu et al., 2021).\nThe modality gap between speech and text can be summarized as two dimensions. First, the length gap \u2013 the speech features are usually much longer than their corresponding texts (Chorowski et al., 2015; Liu et al., 2020). Second is the representation space gap. Directly fine-tuning MT parameters (semantic encoder and decoder) with speech features as inputs, which learned independently, would result in sub-optimal performance. Previous work has explored and proposed several alignment objectives to address the second gap, e.g., Cross-modal Adaption (Liu et al., 2020), Cross-Attentive Regularization (Tang et al., 2021a) and Cross-modal Contrastive Learning (Ye et al., 2022).\nA shrinking mechanism is usually used to address the length gap. Some leverage Continuous Integrate-and-Fire (CIF) (Dong and Xu, 2020) to shrink the long speech features (Dong et al., 2022; Chang and Lee, 2022), but they mostly work on simultaneous ST and need extra efforts to perform better shrinking. Others mainly depend on the CTC greedy path (Liu et al., 2020; Gaido et al., 2021), which might introduce extra inference cost and lead to sub-optimal shrinking results. AdaTranS uses a new shrinking mechanism called boundary-based shrinking, which achieves higher performance.\nThrough extensive experiments on the MUSTC (Di Gangi et al., 2019) dataset, we show that AdaTranS is superior to other shrinking-based methods with a faster inference speed or lower memory usage. Further equipped with alignment objectives, AdaTranS shows competitive performance compared to the state-of-the-art models."
        },
        {
            "heading": "2 Proposed Model: AdaTranS",
            "text": ""
        },
        {
            "heading": "2.1 Problem Formulation",
            "text": "An ST corpus is denoted as DST = {(x, z,y)}, containing triples of speech, transcription and translation. Here x = (x1, x2, ..., xTx) is a sequence of speech features or waves as speech input, while z = (z1, z2, ..., zTz) and y = (y1, y2, ..., yTy) are the corresponding transcription in source language and translation in target language, respectively. Tx, Tz , and Ty are the lengths of speech, transcription and translation, respectively, where usually Tx \u226b Tz and Tx \u226b Ty."
        },
        {
            "heading": "2.2 Architecture",
            "text": "AdaTranS decouples the ST encoder into an acoustic encoder and a semantic encoder. To bridge the modality gap between speech and text, an adaptor is usually needed before the semantic encoder. We choose the shrinking operation (Liu et al., 2020; Zeng et al., 2021) as our adaptor, where the long speech sequences are shrunk to the similar lengths as the transcription based on designed mechanisms (details will be introduced in the next subsection). The shrunk representations are sent to the semantic encoder and ST decoder for output:\nLST = \u2212 \u2211 |DST | Ty\u2211 t=1 log p(yt|y<t,x) (1)\nTo incorporate extra ASR and MT data, we use the pre-trained ASR encoder to initialize the ST acoustic encoder, and the pre-trained MT encoder and decoder to initialize the ST semantic encoder and decoder, respectively. Both pre-trained models are first trained with extra ASR (or MT) data and then fine-tuned with the in-domain data (the ASR part or MT part in the ST corpus). Figure 1 displays our architecture as well as the training process."
        },
        {
            "heading": "2.3 Boundary-based Shrinking Mechanism",
            "text": "Previous shrinking mechanisms (Liu et al., 2020; Zeng et al., 2021) mostly depend on a CTC module (Graves et al., 2006) to produce token-label probabilities for each frame in the speech representations. Then, a word boundary is recognized if the labeled tokens of two consecutive frames are different. There are two main drawbacks to such CTC-based methods. First, the word boundaries are indirectly estimated and potentially affected by error propagation from the token label predictions which are usually greedily estimated by the argmax operation on the CTC output probabilities. Second, the token labels are from a large source vocabulary resulting in extra parameters and computation cost in the CTC module during inference.\nWe introduce a boundary-based shrinking mechanism to address the two drawbacks. A boundary predictor is used to directly predict the probability of each speech representation being a boundary, which is then used for weighted shrinking. Since the boundary labels on the speech representations are unknown during training, we introduce signals from the CTC module to guide the training of the boundary predictor. The CTC module will be discarded during inference. Below shows the details.\nCTC module. We first briefly introduce the CTC module. It predicts a path \u03c0 = (\u03c01, \u03c02, ..., \u03c0Tx), where Tx is the length of hidden states after the acoustic encoder. And \u03c0t \u2208 V \u222a {\u03d5} can be either a token in the source vocabulary V or the blank symbol \u03d5. By removing blank symbols and consecutively repeated labels, denoted as an operation B, we can map the CTC path to the corresponding transcription. A CTC loss is defined as the probability of all possible paths that can be mapped to the ground-truth transcription z:\nLCTC = \u2212 \u2211 |DST | \u2211 \u03c0\u2208B\u22121(z) log p(\u03c0|x) (2)\nCTC-guided Boundary Predictor. We propose to use a boundary predictor to replace the CTC module, which has a similar architecture but with only three labels. The three labels are <BK> (blank label), <BD> (boundary label) and <OT> (others), respectively. However, the ground-truth labels for training the predictor are unknown. Therefore, we introduce soft training signals based on the output probabilities of the CTC module. Specifically, the ground-truth probabilities of each frame t to be\nlabeled as the three labels are defined as:\np\u2032t(<BK>) = p(\u03c0t = \u03d5) p\u2032t(<BD>) = \u2211\ni \u0338=\u03d5 p(\u03c0t = i)p(\u03c0t+1 \u0338= i)\np\u2032t(<OT>) = 1\u2212 p\u2032t(<BK>)\u2212 p\u2032t(<BD>)\n(3)\nThen, the objective for the boundary predictor1 is:\nLPred = \u2212 \u2211 |DST | T \u2032x\u2211 t=1 \u2211 i\u2208\u2206 p\u2032t(i) log pt(i) (4) where \u2206 = {<BK>,<BD>,<OT>}. The CTC module is only used in the training process and can be discarded during inference. Since the number of labels in the predictor is significantly smaller than the size of the source vocabulary, the time and computation costs introduced by the predictor are negligible. Figure 2 shows an example to elaborate the advantage of such a predictor.\nWeighted Shrinking. For shrinking, we define boundary frames as those with the probabilities of the <BD> label higher than a pre-defined threshold \u03b8. The frames between two boundary frames are defined as one segment, which can be aligned to one source token. Inspired by Zeng et al. (2021), we sum over the frames in one segment weighted by their probabilities of being blank labels to distinguish informative and non-informative frames: hAt\u2032 = \u2211\nt\u2208seg t\u2032 hAt exp(\u00b5(1\u2212 pt(<BK>)))\u2211 s\u2208seg t\u2032 exp(\u00b5(1\u2212 ps(<BK>)))\n(5) where \u00b5 \u2265 0 denotes the temperature for the Softmax Function.\n1Since the training of the predictor highly depends on the quality of the CTC output, the CTC module is also pre-trained.\nForced Training. We introduce a forced training trick to explicitly solve the length mismatch between speech and text representations. During training, we set the threshold \u03b8 dynamically based on the length of z to make sure the shrunk representations have exactly the same lengths as their corresponding transcriptions. Specifically, we first sort the probabilities to be <BD> of all frames in descending order, and then select the Tz-th one as the threshold \u03b8."
        },
        {
            "heading": "2.4 Training Objectives",
            "text": "The total loss of our AdaTranS will be:\nL = LST + \u03b1 \u00b7 LCTC + \u03b2 \u00b7 LPred (6)\nwhere \u03b1, \u03b2 are hyper parameters that control the effects of different losses."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Experimental Setup",
            "text": "Datasets. We conduct experiments on three language pairs of MUST-C dataset (Di Gangi et al., 2019): English-German (En\u2013De), English-French (En\u2013Fr) and English-Russian (En\u2013Ru). We use the official data splits for train and development and tst-COMMON for test. We use LibriSpeech (Panayotov et al., 2015) to pre-train the acoustic model. OpenSubtitles20182 or WMT143 are used to pretrain the MT model. The data statistics are listed in Table 3 of Appendix A.\nPreprocessing. We use 80D log-mel filterbanks as speech input features and SentencePiece4 (Kudo and Richardson, 2018) to generate subword vocabulary with a size of 16000 for each language pair. More details please refer to the Appendix B.\nModel Setting. Conv-Transformer (Huang et al., 2020) or Conformer (Gulati et al., 2020) (results in Table 2 are achieved by AdaTranS with Conformer, while the remaining results utilized ConvTransformer) is used as our acoustic encoder, both containing 12 layers. For the semantic encoder and ST decoder, we follow the general NMT Transformer settings (i.e., both contain 6 layers). Each Transformer layer has an input embedding dimension of 512 and a feed-forward layer dimension of 2048. The whole model contains 107M (140M if\n2http://opus.nlpl.eu/OpenSubtitles-v2018.php 3https://www.statmt.org/wmt14/translation-task.html 4https://github.com/google/sentencepiece\nuse Conformer) parameters. The hyper-parameters in Eq. 6 are set as: \u03b1 = 1.0 and \u03b2 = 1.0, respectively. The temperature of the softmax function in Eq. 5 (\u00b5) is 1.0, while the threshold \u03b8 in the boundary predictor is set to 0.4 during inference5. Training details please refer to Appendix B.\nWe apply SacreBLEU6 for evaluation, where case-sensitive detokenized BLEU is reported."
        },
        {
            "heading": "3.2 Experiment results",
            "text": "Table 1 compares different shrinking-based methods in terms of quality and efficiency (for fair comparison, we use the same architecture pre-trained with the same data and add a CTC loss to all the compared models). Besides translation quality, we use length differences between the shrunk representations and the corresponding transcriptions to evaluate shrinking quality following Zeng et al. (2021). We use inference speedup and memory usage to evaluate efficiency.\nFor comparisons, the Fix-Shrink method shrinks the speech features with a fixed rate (e.g. every 3 frames). The CIF-Based method (Dong et al., 2022) is based on a continuous integrate-and-fire mechanism. The CTC-Based method (Liu et al., 2020) shrinks features based on CTC greedy paths. As can be seen, poor shrinking (Fix-Shrink and CIFbased) hurts the performance, although with better efficiency. The boundary-based shrinking used in AdaTranS and the CTC-based method achieve better shrinking quality, with performance improved. However, CTC-Based method hurts the inference\n5All the hyper-parameters are set through grid search based on the performance of the development set.\n6https://github.com/mjpost/sacreBLEU\nefficiency (lower inference speed and higher memory usage) as they introduce extra computation cost producing greedy CTC path in a large source vocabulary. Our method performs the best in both shrinking and translation quality with nice inference efficiency. On the other hand, we also notice that removing forced training trick (\u201c-Forced Train\u201d) or weighted-shrinking (i.e., \u201c-Blank Label\u201d, simply average the frame representations rather than use Eq. 5) will affects the translation quality, showing the effectiveness of these two components.\nAdopting Alignment Objectives. AdaTranS can be further improved with objectives that align speech and text representations (i.e. bridging the representation space gap introduced in Section 1). Table 2 shows the results of AdaTranS equipped with Cross-modal Contrastive (Ye et al., 2022) and knowledge distillation (KD) guided by MT, compared to the models that also work on modality alignment objectives (more complete comparisons please refer to Appendix C). The results show that AdaTranS achieves competitive results in all three datasets. We also examine and show the effects of boundary-based shrinking in such setting (\u201c- Boundary-base Shrink\u201d).\nInfluence of the Threshold. We also examine the threshold \u03b8 for the boundary predictor. Figure 3 shows the distribution of the predicted boundary probability (i.e. pt(<BD>)) for each frame. We find that the boundary predictor is confident (< 0.1 and > 0.9) in most cases. However, even though only a small portion of predictions are in the range of [0.1, 0.9], they significantly affect the BLEU scores when the threshold changes (the red line in Figure 3). The model achieves the best perfor-\nmance when the threshold is around 0.4."
        },
        {
            "heading": "4 Related Work",
            "text": "Numerous techniques have been proposed to adapt speech and text representations in order to mitigate the modality gap in end-to-end ST. Wang et al. (2020) introduce noise into the text input, while Salesky and Black (2020); Tang et al. (2021a,b) employ phoneme sequences for text input. These approaches reduce differences between speech and text input by extending text representations. Conversely, some research aims at shrinking lengthy speech input by presenting various shrinking strategies (Salesky et al., 2019; Dong et al., 2020, 2021; Liu et al., 2020; Gaido et al., 2021; Zeng et al., 2021). Our work falls within this category, proposing a new approach that offers enhanced effectiveness and efficiency. Others prioritize aligning the speech-text representation space (Liu et al., 2020; Tang et al., 2021a; Xu et al., 2021; Ye et al., 2022; Zhang et al., 2022; Le et al., 2023).\nCTC alignment\u2019s role in enhancing ST is particularly pertinent to our research. Earlier studies have demonstrated that integrating an additional CTC loss for multi-task learning or pre-training assists in ST model training (Wang et al., 2020; Xu et al., 2021). Some also leverage CTC alignment to shrink speech inputs (Salesky et al., 2019; Liu et al., 2020; Gaido et al., 2021; Zeng et al., 2021). Moreover, Le et al. (2023) and Zhang et al. (2023) delve deeper, optimizing CTC objectives for ST."
        },
        {
            "heading": "5 Conclusion",
            "text": "This work proposes a new end-to-end ST model called AdaTranS, which uses a boundary predictor trained by signals from CTC output probabilities, to adapt and bridge the length gap between speech and text. Experiments show that AdaTranS per-\nforms better than other shrinking-based methods, in terms of both quality and efficiency. It can also be further enhanced by modality alignment objectives to achieve state-of-the-art results.\nLimitations\nLearning of the proposed boundary predictor. The learning objective of our boundary predictor is constructed by the soft labels from CTC objective. Since the labels are not accurate labels, it is inevitable to introduce errors during training. However, the groundtruth labels for boundary predictor is difficult to obtain. One alternative is to use forced alignment tools. That also introduces other problems. First, off-the-shelf forced alignment tools only support speech in popular languages, which limits the use of the method to other languages. Second, forced alignment also doesn\u2019t guarantee the correctness of labeling, and we still need to further approximate the labeling results when applying them to speech features after acoustic encoder (with 4x or 8x downsampling).\nSensitivity of the selective threshold. From Figure 3, we can find that the BLEU score is sensitive to the threshold selection, although the boundary predictor is confident in most cases. We leave it to our future work to alleviate this phenomenon."
        },
        {
            "heading": "A Data Statistics.",
            "text": "Table 3 shows the data statistics of the used datasets. ST datasets are all from MUST-C, and LibriSpeech serves as extra ASR data. MT data either comes from OpenSubtitles2018 or WMT14 following settings of previous work.\nB Implementation Details.\nData Preprocessing. We use 80D log-mel filterbanks as speech input features, which are calculated with 25ms window size and 10ms step size and normalized by utterance-level Cepstral Mean and Variance Normalization (CMVN). All the texts in ST and MT data are preprocessed in the same way, which are case-sensitive with punctuation preserved. For training data, we filter out samples with more than 3000 frames, over 256 tokens, or whose ratios of source and target text lengths are outside the range [2/3, 3/2]. We use SentencePiece (Kudo and Richardson, 2018) to generate subword vocabulary for each language pair. Each vocabulary is learned on all the texts from ST and MT data and shared across source and target languages, with a total size of 16000.\nTraining Details. We train all the models using Adam optimizer (Kingma and Ba, 2015) with a 0.002 learning rate and 10000 warm-up steps followed by the inverse square root scheduler. Label smoothing and dropout strategies are used, both set to 0.1. The models are fine-tuned on 8 NVIDIA Tesla V100 GPUs with 40000 steps, which takes about 10 hours in average. The batch size is set to 40000 frames per GPU. We save checkpoints every epoch and average the last 10 checkpoints for evaluation with a beam size of 10."
        },
        {
            "heading": "C More Analysis.",
            "text": "Complete Comparisons with the SOTA models. Table 4 extends the results in Table 2 to include the detailed settings of the compared models, including external data used and training objectives.\nWe also include two SOTA works based on speechtext joint pre-training (ST Joint PT), which shows great improvements by applying complex joint pretraining objectives. It should be noted that it is not in line with the focus of this work and our AdaTranS might also benefit from them by initializing the modules after such joint pre-training.\nBetter Source-Target Alignment. We evaluate the entropy of the cross attention from the ST baseline (i.e. no any shrinking) and AdaTranS7. Let \u03b1ij be the attention weight for a target token yi and a source speech feature (after shrinking) xj , the entropy for each target token is defined as Ei = \u2212 \u2211|x| j=1 \u03b1ij log\u03b1ij . We then average the attention entropy of all target tokens in the test set. Lower entropy means the attention mechanism is more confident and concentrates on the sourcetarget alignment. Figure 4 shows the entropy of different decoder layers. AdaTranS exhibits consistently lower entropy than the ST baseline. This means that our shrinking mechanism improves the learning of attention distributions.\nInfluence of Text Input Representations. Representing text input with phonemes helps reduce the differences between speech and text (Tang et al., 2021a,b). However, word representations and punctuation are important for learning semantic information, which are usually ignored when phonemes are used in prior works. Table 5 shows the MT results when using different text input representations, together with the ST performance that is initialized from the corresponding MT model. We can observe that the performance of downstream ST model is affected by the pre-trained MT model. Therefore, instead of following prior phoneme-level work for\n7To fairly compare, we also shrink the speech features of the ST baseline with the same boundaries detected by our boundary predictor.\npre-training the MT, in this work we use subword units with punctuation and incorporate the shrinking mechanism to mitigate the length gap.\nInfluence of Multi-task Learning (MTL). Previous work usually applies multi-task learning together with MT task (adding external MT data) to improve performance. However, our experiments (Table 6) show that when using KD, MTL might not be always helpful, especially with MT corpus in different domains (OpenSutitles is in spoken language domain like ST, but data from WMT14 is mostly in news domain)."
        }
    ],
    "title": "AdaTranS: Adapting with Boundary-based Shrinking for End-to-End Speech Translation",
    "year": 2023
}