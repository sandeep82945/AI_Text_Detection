{
    "abstractText": "Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning. Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes. Large language models (LLMs) encode a wealth of world knowledge and can be prompted to generate descriptive features for each class. Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs\u2019 hallucinations, as is observed by our analysis. In this paper, we propose to apply world knowledge to enhance OOD detection performance through selective generation from LLMs. Specifically, we introduce a consistency-based uncertainty calibration method to estimate the confidence score of each generation. We further extract visual objects from each image to fully capitalize on the aforementioned world knowledge. Extensive experiments demonstrate that our method consistently outperforms the state-of-the-art.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yi Dai"
        },
        {
            "affiliations": [],
            "name": "Hao Lang"
        },
        {
            "affiliations": [],
            "name": "Kaisheng Zeng"
        },
        {
            "affiliations": [],
            "name": "Fei Huang"
        },
        {
            "affiliations": [],
            "name": "Yongbin Li"
        }
    ],
    "id": "SP:17275c1090b4ec510fd73357e6b7320a938b1f5a",
    "references": [
        {
            "authors": [
                "Dario Amodei",
                "Chris Olah",
                "Jacob Steinhardt",
                "Paul Christiano",
                "John Schulman",
                "Dan Man\u00e9."
            ],
            "title": "Concrete problems in ai safety",
            "venue": "arXiv preprint arXiv:1606.06565.",
            "year": 2016
        },
        {
            "authors": [
                "Abhijit Bendale",
                "Terrance Boult."
            ],
            "title": "Towards open world recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1893\u20131902.",
            "year": 2015
        },
        {
            "authors": [
                "Lukas Bossard",
                "Matthieu Guillaumin",
                "Luc Van Gool."
            ],
            "title": "Food-101\u2013mining discriminative components with random forests",
            "venue": "Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13,",
            "year": 2014
        },
        {
            "authors": [
                "Terrance E Boult",
                "Steve Cruz",
                "Akshay Raj Dhamija",
                "Manuel Gunther",
                "James Henrydoss",
                "Walter J Scheirer."
            ],
            "title": "Learning and the unknown: Surveying steps toward open world recognition",
            "venue": "Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Likun Cai",
                "Zhi Zhang",
                "Yi Zhu",
                "Li Zhang",
                "Mu Li",
                "Xiangyang Xue."
            ],
            "title": "Bigdetection: A large-scale benchmark for improved object detector pre-training",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4777\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Xinyun Chen",
                "Maxwell Lin",
                "Nathanael Sch\u00e4rli",
                "Denny Zhou."
            ],
            "title": "Teaching large language models to self-debug",
            "venue": "arXiv preprint arXiv:2304.05128.",
            "year": 2023
        },
        {
            "authors": [
                "Zhenfang Chen",
                "Qinhong Zhou",
                "Yikang Shen",
                "Yining Hong",
                "Hao Zhang",
                "Chuang Gan."
            ],
            "title": "See, think, confirm: Interactive prompting between vision and language models for knowledge-based visual reasoning",
            "venue": "arXiv preprint arXiv:2301.05226.",
            "year": 2023
        },
        {
            "authors": [
                "Mircea Cimpoi",
                "Subhransu Maji",
                "Iasonas Kokkinos",
                "Sammy Mohamed",
                "Andrea Vedaldi."
            ],
            "title": "Describing textures in the wild",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3606\u20133613.",
            "year": 2014
        },
        {
            "authors": [
                "Yi Dai",
                "Hao Lang",
                "Yinhe Zheng",
                "Fei Huang",
                "Yongbin Li."
            ],
            "title": "Long-tailed question answering in an open world",
            "venue": "arXiv preprint arXiv:2305.06557.",
            "year": 2023
        },
        {
            "authors": [
                "Yi Dai",
                "Hao Lang",
                "Yinhe Zheng",
                "Bowen Yu",
                "Fei Huang",
                "Yongbin Li."
            ],
            "title": "Domain incremental lifelong learning in an open world",
            "venue": "arXiv preprint arXiv:2305.06555.",
            "year": 2023
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei."
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee.",
            "year": 2009
        },
        {
            "authors": [
                "Sepideh Esmaeilpour",
                "Bing Liu",
                "Eric Robertson",
                "Lei Shu."
            ],
            "title": "Zero-shot out-of-distribution detection based on the pre-trained model clip",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 36, pages 6568\u20136576.",
            "year": 2022
        },
        {
            "authors": [
                "Geli Fei",
                "Bing Liu."
            ],
            "title": "Breaking the closed world assumption in text classification",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 506\u2013514.",
            "year": 2016
        },
        {
            "authors": [
                "Stanislav Fort",
                "Jie Ren",
                "Balaji Lakshminarayanan."
            ],
            "title": "Exploring the limits of out-of-distribution detection",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 7068\u20137081. Curran Associates, Inc.",
            "year": 2021
        },
        {
            "authors": [
                "Stanislav Fort",
                "Jie Ren",
                "Balaji Lakshminarayanan."
            ],
            "title": "Exploring the limits of out-of-distribution detection",
            "venue": "Conference on Neural Information Processing Systems (NeurIPS).",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel."
            ],
            "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
            "venue": "Proceedings of the International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel."
            ],
            "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2017
        },
        {
            "authors": [
                "Yen-Chang Hsu",
                "Yilin Shen",
                "Hongxia Jin",
                "Zsolt Kira."
            ],
            "title": "Generalized odin: Detecting out-ofdistribution image without learning from out-ofdistribution data",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-",
            "year": 2020
        },
        {
            "authors": [
                "Rui Huang",
                "Yixuan Li."
            ],
            "title": "Mos: Towards scaling out-of-distribution detection for large semantic space",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8710\u20138719.",
            "year": 2021
        },
        {
            "authors": [
                "Wanqing Cui",
                "Dan Yang Hou",
                "Yingyan Li",
                "Junyi Li",
                "Peiyu Liu",
                "Zheng Gong",
                "Chuhao Jin",
                "Yuchong Sun",
                "Shizhe Chen",
                "Zhiwu Lu",
                "Zhicheng Dou",
                "Qin Jin",
                "Yanyan Lan",
                "Wayne Xin Zhao",
                "Ruihua Song",
                "Ji-Rong Wen"
            ],
            "title": "Wenlan: Bridging vision",
            "year": 2021
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Computing Surveys, 55(12):1\u201338.",
            "year": 2023
        },
        {
            "authors": [
                "Saurav Kadavath",
                "Tom Conerly",
                "Amanda Askell",
                "Tom Henighan",
                "Dawn Drain",
                "Ethan Perez",
                "Nicholas Schiefer",
                "Zac Hatfield Dodds",
                "Nova DasSarma",
                "Eli Tran-Johnson"
            ],
            "title": "Language models (mostly) know what they know",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Krause",
                "Michael Stark",
                "Jia Deng",
                "Li FeiFei."
            ],
            "title": "3d object representations for fine-grained categorization",
            "venue": "Proceedings of the IEEE international conference on computer vision workshops, pages 554\u2013561.",
            "year": 2013
        },
        {
            "authors": [
                "Lorenz Kuhn",
                "Yarin Gal",
                "Sebastian Farquhar."
            ],
            "title": "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation",
            "venue": "arXiv preprint arXiv:2302.09664.",
            "year": 2023
        },
        {
            "authors": [
                "Hao Lang",
                "Yinhe Zheng",
                "Yixuan Li",
                "Jian Sun",
                "Fei Huang",
                "Yongbin Li."
            ],
            "title": "A survey on outof-distribution detection in nlp",
            "venue": "arXiv preprint arXiv:2305.03236.",
            "year": 2023
        },
        {
            "authors": [
                "Hao Lang",
                "Yinhe Zheng",
                "Jian Sun",
                "Fei Huang",
                "Luo Si",
                "Yongbin Li."
            ],
            "title": "Estimating soft labels for out-of-domain intent detection",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 261\u2013276, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Kimin Lee",
                "Kibok Lee",
                "Honglak Lee",
                "Jinwoo Shin."
            ],
            "title": "A simple unified framework for detecting outof-distribution samples and adversarial attacks",
            "venue": "Advances in neural information processing systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Nayeon Lee",
                "Wei Ping",
                "Peng Xu",
                "Mostofa Patwary",
                "Pascale N Fung",
                "Mohammad Shoeybi",
                "Bryan Catanzaro."
            ],
            "title": "Factuality enhanced language models for open-ended text generation",
            "venue": "Advances in Neural Information Processing Systems, 35:34586\u201334599.",
            "year": 2022
        },
        {
            "authors": [
                "Chuyi Li",
                "Lulu Li",
                "Yifei Geng",
                "Hongliang Jiang",
                "Meng Cheng",
                "Bo Zhang",
                "Zaidan Ke",
                "Xiaoming Xu",
                "Xiangxiang Chu."
            ],
            "title": "Yolov6 v3",
            "venue": "0: A full-scale reloading. arXiv preprint arXiv:2301.05586.",
            "year": 2023
        },
        {
            "authors": [
                "Yujia Li",
                "David Choi",
                "Junyoung Chung",
                "Nate Kushman",
                "Julian Schrittwieser",
                "R\u00e9mi Leblond",
                "Tom Eccles",
                "James Keeling",
                "Felix Gimeno",
                "Agustin Dal Lago"
            ],
            "title": "Competition-level code generation with alphacode",
            "year": 2022
        },
        {
            "authors": [
                "Weitang Liu",
                "Xiaoyun Wang",
                "John Owens",
                "Yixuan Li."
            ],
            "title": "Energy-based out-of-distribution detection",
            "venue": "Proceedings of the Advances in Neural Information Processing Systems.",
            "year": 2020
        },
        {
            "authors": [
                "Sachit Menon",
                "Carl Vondrick."
            ],
            "title": "Visual classification via description from large language models",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Yifei Ming",
                "Ziyang Cai",
                "Jiuxiang Gu",
                "Yiyou Sun",
                "Wei Li",
                "Yixuan Li."
            ],
            "title": "Delving into out-ofdistribution detection with vision-language representations",
            "venue": "arXiv preprint arXiv:2211.13445.",
            "year": 2022
        },
        {
            "authors": [
                "Yifei Ming",
                "Hang Yin",
                "Yixuan Li."
            ],
            "title": "On the impact of spurious correlation for out-of-distribution detection",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10051\u2013 10059.",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Omkar M Parkhi",
                "Andrea Vedaldi",
                "Andrew Zisserman",
                "CV Jawahar."
            ],
            "title": "Cats and dogs",
            "venue": "2012 IEEE conference on computer vision and pattern recognition, pages 3498\u20133505. IEEE.",
            "year": 2012
        },
        {
            "authors": [
                "Baolin Peng",
                "Michel Galley",
                "Pengcheng He",
                "Hao Cheng",
                "Yujia Xie",
                "Yu Hu",
                "Qiuyuan Huang",
                "Lars Liden",
                "Zhou Yu",
                "Weizhu Chen"
            ],
            "title": "Check your facts and try again: Improving large language models with external knowledge and automated feedback",
            "year": 2023
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander Miller"
            ],
            "title": "2019a. Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference",
            "year": 2019
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander Miller"
            ],
            "title": "2019b. Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Jie Ren",
                "Jiaming Luo",
                "Yao Zhao",
                "Kundan Krishna",
                "Mohammad Saleh",
                "Balaji Lakshminarayanan",
                "Peter J Liu."
            ],
            "title": "Out-of-distribution detection and selective generation for conditional language models",
            "venue": "arXiv preprint arXiv:2209.15558.",
            "year": 2022
        },
        {
            "authors": [
                "Yilin Shen",
                "Yen-Chang Hsu",
                "Avik Ray",
                "Hongxia Jin."
            ],
            "title": "Enhancing the generalization for intent classification and out-of-domain detection in slu",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th",
            "year": 2021
        },
        {
            "authors": [
                "Lei Shu",
                "Yassine Benajiba",
                "Saab Mansour",
                "Yi Zhang."
            ],
            "title": "Odist: Open world classification via distributionally shifted instances",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3751\u20133756.",
            "year": 2021
        },
        {
            "authors": [
                "Chenglei Si",
                "Zhe Gan",
                "Zhengyuan Yang",
                "Shuohang Wang",
                "Jianfeng Wang",
                "Jordan Boyd-Graber",
                "Lijuan Wang."
            ],
            "title": "Prompting gpt-3 to be reliable",
            "venue": "arXiv preprint arXiv:2210.09150.",
            "year": 2022
        },
        {
            "authors": [
                "Chenglei Si",
                "Chen Zhao",
                "Sewon Min",
                "Jordan BoydGraber."
            ],
            "title": "Re-examining calibration: The case of question answering",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2814\u20132829.",
            "year": 2022
        },
        {
            "authors": [
                "Yiyou Sun",
                "Yifei Ming",
                "Xiaojin Zhu",
                "Yixuan Li."
            ],
            "title": "Out-of-distribution detection with deep nearest neighbors",
            "venue": "International Conference on Machine Learning, pages 20827\u201320840. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Vladimir Vapnik."
            ],
            "title": "Principles of risk minimization for learning theory",
            "venue": "Advances in neural information processing systems, 4.",
            "year": 1991
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "C. Wah",
                "S. Branson",
                "P. Welinder",
                "P. Perona",
                "S. Belongie."
            ],
            "title": "The caltech-ucsd birds-200-2011 dataset",
            "venue": "Technical Report CNS-TR-2011-001, California Institute of Technology.",
            "year": 2011
        },
        {
            "authors": [
                "Wenhai Wang",
                "Jifeng Dai",
                "Zhe Chen",
                "Zhenhang Huang",
                "Zhiqi Li",
                "Xizhou Zhu",
                "Xiaowei Hu",
                "Tong Lu",
                "Lewei Lu",
                "Hongsheng Li",
                "Xiaogang Wang",
                "Yu Qiao."
            ],
            "title": "Internimage: Exploring large-scale vision foundation models with deformable convolutions",
            "venue": "In",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2203.11171.",
            "year": 2022
        },
        {
            "authors": [
                "Jim Winkens",
                "Rudy Bunel",
                "Abhijit Guha Roy",
                "Robert Stanforth",
                "Vivek Natarajan",
                "Joseph R Ledsam",
                "Patricia MacWilliams",
                "Pushmeet Kohli",
                "Alan Karthikesalingam",
                "Simon Kohl"
            ],
            "title": "Contrastive training for improved out-of-distribution detection",
            "year": 2020
        },
        {
            "authors": [
                "Jianxiong Xiao",
                "James Hays",
                "Krista A Ehinger",
                "Aude Oliva",
                "Antonio Torralba."
            ],
            "title": "Sun database: Large-scale scene recognition from abbey to zoo",
            "venue": "2010 IEEE computer society conference on computer vision and pattern recognition, pages 3485\u20133492.",
            "year": 2010
        },
        {
            "authors": [
                "Yang Xu",
                "Yiheng Xu",
                "Tengchao Lv",
                "Lei Cui",
                "Furu Wei",
                "Guoxin Wang",
                "Yijuan Lu",
                "Dinei Florencio",
                "Cha Zhang",
                "Wanxiang Che",
                "Min Zhang",
                "Lidong Zhou."
            ],
            "title": "LayoutLMv2: Multi-modal pre-training for visually-rich document understanding",
            "venue": "Proceed-",
            "year": 2021
        },
        {
            "authors": [
                "Jingkang Yang",
                "Kaiyang Zhou",
                "Yixuan Li",
                "Ziwei Liu."
            ],
            "title": "Generalized out-of-distribution detection: A survey",
            "venue": "arXiv preprint arXiv:2110.11334.",
            "year": 2021
        },
        {
            "authors": [
                "Zhengyuan Yang",
                "Zhe Gan",
                "Jianfeng Wang",
                "Xiaowei Hu",
                "Yumao Lu",
                "Zicheng Liu",
                "Lijuan Wang."
            ],
            "title": "An empirical study of gpt-3 for few-shot knowledgebased vqa",
            "venue": "Proceedings of the AAAI Conference",
            "year": 2022
        },
        {
            "authors": [
                "Li-Ming Zhan",
                "Haowen Liang",
                "Bo Liu",
                "Lu Fan",
                "XiaoMing Wu",
                "Albert YS Lam."
            ],
            "title": "Out-of-scope intent detection with self-supervision and discriminative training",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Bolei Zhou",
                "Agata Lapedriza",
                "Aditya Khosla",
                "Aude Oliva",
                "Antonio Torralba."
            ],
            "title": "Places: A 10 million image database for scene recognition",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, 40(6):1452\u20131464.",
            "year": 2017
        },
        {
            "authors": [
                "Wenxuan Zhou",
                "Fangyu Liu",
                "Muhao Chen."
            ],
            "title": "Contrastive out-of-distribution detection for pretrained transformers",
            "venue": "arXiv preprint arXiv:2104.08812.",
            "year": 2021
        },
        {
            "authors": [
                "Zhuofan Zong",
                "Guanglu Song",
                "Yu Liu."
            ],
            "title": "Detrs with collaborative hybrid assignments training",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 6748\u20136758.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Machine learning models deployed in the wild often encounter out-of-distribution (OOD) samples that are not seen in the training phase (Bendale and Boult, 2015; Fei and Liu, 2016). A reliable model should not only obtain high performance on samples from seen distributions, i.e., in-distribution (ID) samples, but also accurately detect OOD samples for caution (Amodei et al., 2016; Boult et al., 2019; Dai et al., 2023b). Most existing OOD detection methods are built upon single-modal inputs, e.g., visual inputs (Hsu et al., 2020; Liu et al., 2020) or textual inputs (Zhou et al., 2021; Zhan et al., 2021). Recently, Esmaeilpour et al. (2022); Ming et al. (2022a) attempt to tackle multi-modal\n\u2217 Work done while the author was interning at Alibaba. \u2020 Equal contribution. \u2021 Corresponding author.\nOOD detection problem that explores the semantic information conveyed in class labels for visual OOD detection, relying on large-scale pre-trained vision-language models such as CLIP (Radford et al., 2021).\nIn this paper, we apply world knowledge from large language models (LLMs) (Petroni et al., 2019a) to multi-modal OOD detection by generating descriptive features for class names (Menon and Vondrick, 2023). As illustrated in Figure 1, to find a black swan, look for its long neck, webbed feet, and black feathers. These descriptors provide rich additional semantic information for ID classes, which can lead to a more robust estimation of OOD uncertainty (Ming et al., 2022a), i.e., measuring the distance from the visual features of an input to the closest textual features of ID classes.\nHowever, the knowledge encoding of LLMs such as GPT-3 (Brown et al., 2020) is lossy (Peng et al., 2023) and tends to hallucinate (Ji et al., 2023), which can cause damage when applied for OOD detection tasks. As shown in Figure 2, LLMs generate unfaithful descriptors for class \u201chen\u201d, assuming a featherless head appearing in a hen. Indiscriminately employing generated descriptive features to model ID classes brings noise to the inference process due to LLMs\u2019 hallucinations. Moreover, this issue becomes more severe as OOD detection deals with samples in an unbounded feature space (Shen\net al., 2021). Collisions between OOD samples and ID classes with augmented descriptors would be common.\nTo address the challenge mentioned above, we propose an approach for selective generation of high-quality descriptive features from LLMs, while abstaining from low-quality unfaithful ones (Ren et al., 2022). Recent studies show LLMs can predict the quality of their outputs, i.e., providing calibrated confidence scores for each prediction that accurately reflects the likelihood of the predicted answer being correct (Kadavath et al., 2022; Si et al., 2022a). Unfortunately, descriptors of a class name generated by LLMs are long-form and structured intermediate results for the ultimate OOD detection task, and calibration of LLMs for generating such long open-ended text (Lee et al., 2022) is still in its infancy.\nWe perform uncertainty calibration in LLMs by exploring a consistency-based approach (Wang et al., 2022). We assume if the same correct prediction is consistent throughout multiple generations, then it could serve as a strong sign that LLMs are confident about the prediction (Si et al., 2022b). Instead of computing literal similarity, we define consistency between multiple outputs from LLMs for a given input based on whether they can retrieve similar items from a fixed set of unlabeled images. Specifically, for each descriptor, we first retrieve a subset of images, leveraging the joint vision-language representations. Then, we measure generation consistency by calculating the overlap between these image subsets.\nTo further capitalize on the world knowledge expressed in descriptors from LLMs, we employ a general object detector to detect all the candidate objects (concepts) in an image (Cai et al., 2022) and represent them with their predicted class names (Chen et al., 2023b) such as \u201cmirror\u201d, \u201cchair\u201d, and \u201csink\u201d (see Figure 5). These visual concepts provide valuable contextual information about an image in the textual space and can potentially match descriptive features of an ID class if the image belongs to that class. Accordingly, we improve our distance metric of input samples from ID classes by considering the similarity between image visual concepts and ID class descriptive features in language representations. Our key contributions are summarized as follows:\n\u2022 We apply world knowledge from large language models (LLMs) to multi-modal OOD\ndetection for the first time by generating descriptive features for ID class names.\n\u2022 We analyse LLMs\u2019 hallucinations which can cause damage to OOD detection. A selective generation framework is introduced and an uncertainty calibration method in LLMs is developed to tackle the hallucination issue.\n\u2022 We detect objects in an image and represent them with their predicted class names to further explore world knowledge from LLMs. Our extensive experimentation on various datasets shows that our method consistently outperforms the state-of-the-art."
        },
        {
            "heading": "2 Related Work",
            "text": "OOD Detection is widely investigated in vision classification problems (Yang et al., 2021), and also in text classification problems (Lang et al., 2023). Existing approaches try to improve the OOD detection performance by logits-based scores (Hendrycks and Gimpel, 2017a; Liu et al., 2020), distance-based OOD detectors (Lee et al., 2018; Sun et al., 2022), robust representation learning (Winkens et al., 2020; Zhou et al., 2021), and generated pseudo OOD samples (Shu et al., 2021; Lang et al., 2022).\nMulti-modal OOD detection is recently studied by Fort et al. (2021a); Esmaeilpour et al. (2022); Ming et al. (2022a), which leverages textual information for visual OOD detection. These works do not explore world knowledge from LLMs.\nLarge language models like GPT3 (Brown et al., 2020) can serve as a knowledge base and help various tasks (Petroni et al., 2019b; Dai et al., 2023a). While some works demonstrate that world knowledge from LLMs can provide substantial aid to vision tasks (Yang et al., 2022) such as vision classification (Menon and Vondrick, 2023), its efficacy in multi-modal OOD detection is currently underexplored. Moreover, as LLMs tend to hallucinate and generate unfaithful facts (Ji et al., 2023), additional effects are needed to explore LLMs effectively.\nUncertainty Calibration provides confidence scores for predictions to safely explore LLMs, helping users decide when to trust LLMs outputs. Recent studies examine calibration of LLMs in multiple-choice and generation QA tasks (Kadavath et al., 2022; Si et al., 2022a; Kuhn et al., 2023). In multi-modal OOD detection task, open-ended\ntext (Lee et al., 2022) are generated to provide descriptive features for ID classes (Menon and Vondrick, 2023), and calibration in this task is yet underexplored."
        },
        {
            "heading": "3 Background",
            "text": ""
        },
        {
            "heading": "3.1 Problem Setup",
            "text": "We start by formulating the multi-modal OOD detection problem, following Ming et al. (2022a). We denote the input and label space by X and Y , respectively. Y is a set of class labels/names referring to the known ID classes. The goal of OOD detection is to detect samples that do not belong to any of the known classes or assign a test sample to one of the known classes. We formulate the OOD detection as a binary classification problem: G(x;Y, I, T ) : X \u2192 {0, 1}, where x \u2208 X denotes an input image, I and T are image encoder and text encoder from pre-trained vision-language models (VLMs), respectively. The joint visionlanguage embeddings of VLMs associate objects in visual and textual modalities well. Note that there is no training data of ID samples provided to train the OOD detector."
        },
        {
            "heading": "3.2 Analyzing Class Name Descriptors from LLMs",
            "text": "Recent work has demonstrated that class name descriptors, i.e., descriptive features for distinguishing a known object category in a photograph generated by prompting LLMs (see Section 4.2 for more details), can improve zero-shot visual classification performance (Menon and Vondrick, 2023) in a close-world setting (Vapnik, 1991). A natural extension of this work is to leverage the descriptors for OOD detection in an open world (Fei and Liu, 2016), which is largely unexplored.\nUnfortunately, we find that the descriptors used\nin previous approach fail to improve the OOD detection performance in a few datasets. As shown in Table 1, although descriptors can improve the classification performance in all five datasets, they degenerate the OOD detection performance in four ID datasets. We hypothesize this is because LLMs generate unfaithful descriptors due to hallucinations (see cases in Figure 2), which bring noise to the OOD detection process.\nTo verify our hypothesis, we visualize ID samples from ImageNet-1k dataset and OOD samples from iNaturalist dataset, together with their original class names, based on aligned vision-language features (Radford et al., 2021). As illustrated in Figure 3(b), class names of ID samples may coincide with these of OOD samples, when augmented with descriptors from LLMs. Thus, it is improper to indiscriminately adopt these descriptors.\nThe above assumptions are also evidenced in Figure 4. ID samples obtain higher similarity scores to their classes when augmented with descriptors (Figure 4(a)), which show the effect of descriptors for the classification task. Meanwhile, OOD samples gain larger maximum similarity scores with ID classes with descriptors (Figure 4(b)), which makes OOD detection more difficult."
        },
        {
            "heading": "4 Method",
            "text": ""
        },
        {
            "heading": "4.1 Overview",
            "text": "In this study, we build the multi-modal OOD detector following four steps: 1. Generate a set of descriptors d(c) for each class name c \u2208 Y by prompting LLMs; 2. Estimate a confidence score for descriptors d(c) with uncertainty calibration; 3. Detect visual objects for each test image x; 4. Build an OOD detector with selective generation of descriptors and image visual objects. Figure 5 shows an overview of our approach."
        },
        {
            "heading": "4.2 Descriptor Generation",
            "text": "To apply world knowledge from LLMs for OOD detection, we generate a set of descriptors d(c) for each known class name c by prompting LLMs (see Figure 6), following (Menon and Vondrick, 2023). We randomly select 1 visual category and manually compose descriptors to use as 1-shot incontext example. We prompt LLMs to describe the visual features for distinguishing a category in a photograph. The generated list composes the set d(c). Figure 2 shows cases of generated descriptors, which include shape, size, color, and object parts in natural language."
        },
        {
            "heading": "4.3 Uncertainty Calibration",
            "text": "As Figure 2 illustrates, LLMs may generate unfaithful descriptors due to hallucinations, which would hurt the performance of OOD detection if applied indiscriminately. To address this issue, we design a\nconsistency-based (Wang et al., 2022) uncertainty calibration method to estimate a confidence score for each generation, which helps decide when to trust the LLMs outputs. We assume if the same correct prediction is consistent throughout multiple generations, then it shows that LLMs are confident about the prediction (Si et al., 2022b), thus the generation results are more trustworthy.\nIt is non-trivial to directly extend previous consistency-based methods to our settings. Specifically, we leverage LLMs to generate long-form and structured descriptor lists without a fixed candidate answer set. Any permutation of the descriptor list can convey the same meaning, despite the inconsistency in surface form. Meanwhile, the outputs of LLMs are intermediate results for our OOD detection task. It is challenging to measure quality of these intermediate results while maintaining a tangible connection to the ultimate detection task.\nWe take inspiration from prior code generation works (Li et al., 2022; Chen et al., 2023a), which prompt LLMs to generate structured codes aiming at solving programming tasks. Multiple codes are sampled from LLMs for one input and then execution results are obtained by executing these codes. The code with the most frequent execution result is selected as the final prediction. In a similar manner, we quantify the characteristics of descriptor sets through their retrieval feedbacks from a fixed set of unlabeled images, and define their consistency according to consensus among retrieval results. Specifically, we propose a three-stage consistencybased uncertainty calibration method.\nStage I. We sample n sets of descriptors D(c) = {d1(c), \u00b7 \u00b7 \u00b7 ,dn(c)} from LLMs for each ID class name c \u2208 Y .\nStage II. We cluster descriptor sets D(c) into groups S(c), where each group s \u2208 S(c) is comprised of descriptor sets that consist with each other. We define descriptor consistency, C(\u00b7, \u00b7), which retains any two descriptor sets that share the same characteristics through retrieval feedback. Concretely, we retrieve top k images from an unlabeled image set M via a retriever R(\u00b7) for each set d \u2208 D(c). The resulting image subset for descriptor set d is denoted as R(d) \u2208 {0, 1}m, where m is the size of M and entry j of the vector is 1 if the j-th image of M is in the retrieved subset. Finally, we assume descriptor consistency C(d,d\u2032) holds if cosine similarity between R(d) and R(d\u2032) is above\n\u03b7. Note that text similarity between descriptor sets can also be used in consistency computation.\nStage III. We compute the confidence score p(c) for descriptor set d(c) as |s\n\u2217| n , where s \u2217 is the largest group in S(c)."
        },
        {
            "heading": "4.4 Visual Object Detection",
            "text": "To further capitalize on the world knowledge conveyed in generated descriptors, we introduce a general object detector with a vocabulary of 600 object categories to detect visual objects v(x) for each testing image x (Cai et al., 2022). Specifically, v(x) consists of detected objects\u2019 class names, such as \u201cmirror\u201d, \u201cchair\u201d, and \u201csink\u201d in a photograph of a barber shop (see Figure 5)."
        },
        {
            "heading": "4.5 OOD Detection",
            "text": "For each ID class name c, descriptor set d(c) is used to augment the representation of c if its confidence score p(c) is above threshold \u03b3, otherwise c is used to represent that class only. Thus, the textual features for class name c are:\nt(c) = {\n{g(d)|d \u2208 d(c)}, if p(c) \u2265 \u03b3, {c}, otherwise,\nwhere d is one descriptor in the set d(c) and g(\u00b7) transforms d into the form {c} which has {d}.\nFor an input image x, we calculate the class-wise matching score for each ID class name c \u2208 Y:\nsc(x) = E t\u2208t(c) \u03c3(I(x), T (t)) + E v\u2208v(x) t\u2208t(c) \u03c3(T (v), T (t)),\n(1)\nwhere \u03c3(\u00b7, \u00b7) denotes the cosine similarity function, the left term computes the similarity between image visual representations and class name textual representations, and the right term measures\nthe similarity between detected image objects and class names in the text space.\nLastly, we define the maximum class matching score as: smax(x;Y, I, T ) = maxc\nexp(sc(x))\u2211 c\u2032\u2208Y exp(sc\u2032 (x))\n, similar to Ming et al. (2022a). Our OOD detection function can be defined as:\nG(x;Y, I, T ) = { 1 smax(x;Y, I, T ) \u2265 \u03bb 0 smax(x;Y, I, T ) < \u03bb , (2)\nwhere 1 represents ID class and 0 indicates OOD conventionally. \u03bb is a chosen threshold."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Datasets and Metrics",
            "text": "Datasets Following recent works (Ming et al., 2022a), we use large-scale datasets that are more realistic and complex. We consider the following ID datasets: variants of ImageNet (Deng et al., 2009), CUB-200 (Wah et al., 2011), StanfordCars (Krause et al., 2013), Food-101 (Bossard et al., 2014), Oxford-Pet (Parkhi et al., 2012). For OOD datasets, we use iNaturalist (Van Horn et al., 2018), SUN (Xiao et al., 2010), Places (Zhou et al., 2017), and Texture (Cimpoi et al., 2014).\nMetrics For evaluation, we use these metrics (1) the false positive rate (FPR95) of OOD samples when the true positive rate of ID samples is at 95%, (2) the area under the receiver operating characteristic curve (AUROC)."
        },
        {
            "heading": "5.2 Implementation Details",
            "text": "In our experiments, we adopt CLIP (Radford et al., 2021) as the pre-trained vision-language model. Specifically, we mainly use CLIP-B/16 (CLIP-B), which consists of a ViT-B/16 Transformer as the image encoder and a masked self-attention Transformer (Vaswani et al., 2017) as the text encoder. We also use CLIP-L/14 (CLIP-L) as a representative of large models. To generate descriptors, we query text-davinci-003 (Ouyang et al., 2022) with sampling temperature T = 0.7 and maximum token length of 100. We construct the unlabeled image set M through the random selection of m = 50000 images from the training set of ImageNet. The retriever R(\u00b7) retrieves k = 50 images from M. We set the threshold \u03b7 = 0.9 and \u03b3 = 0.5. In visual object detection, we employ the object detection model CBNetV2-Swin-Base (Cai et al., 2022) as a general object detector with a\nvocabulary of 600 objects. See more details in Appendix B."
        },
        {
            "heading": "5.3 Baselines",
            "text": "We compared our method with competitive baselines: 1. MOS (Huang and Li, 2021) divides ID classes into small groups with similar concepts to improve OOD detection; 2. Fort et al. (Fort et al., 2021b) finetunes a full ViT model pre-trained on the ID dataset; 3. Energy (Liu et al., 2020) proposes a logit-based score to detect OOD samples; 4. MSP (Hendrycks and Gimpel, 2017b) employs the maximum classification probability of samples to estimate OOD uncertainty; 5. MCM (Ming et al., 2022a) estimates OOD uncertainty with the maximum similarity between the embeddings of a sample and ID class names; 6. Menon et al. (Menon and Vondrick, 2023) prompts LLMs to generate descriptors of each class as cues for image classification. We extend it to OOD detection and use the maximum classification probability as a measure of OOD uncertainty (Hendrycks and Gimpel, 2017b)."
        },
        {
            "heading": "5.4 Main Results",
            "text": "To evaluate the scalability of our method in realworld scenarios, we compare it with recent OOD detection baselines on the ImageNet-1k dataset (ID) in Table 2. It can be seen that our method outperforms all competitive zero-shot methods. Compared with the best-performing zero-shot baseline MCM, it reduces FPR95 by 5.03%. We can also observe that: 1. Indiscriminately employing knowledge from LLMs (i.e., Menon et al.) degenerates the OOD detection performance. This indicates the adverse impact of LLMs\u2019 hallucinations and underlines the importance of selective generation from LLMs. 2. Despite being training-free, our method favorably matches or even outperforms some strong task-specific baselines that require training (e.g., MOS). It shows the advantage of incorporating world knowledge from LLMs for OOD detection.\nWe further evaluate the effectiveness of our method on hard OOD inputs. Specifically, two kinds of hard OOD are considered, i.e., semantically hard OOD (Winkens et al., 2020) and spurious OOD (Ming et al., 2022b). As shown in Table 3, our method exhibits robust OOD detection capability and outperforms all competitive baselines, e.g., improvement of 1.93% in FPR95 compared to the best-performing baseline MCM. We can also observe that zero-shot methods generally obtain higher performance than task-specific baselines.\nThis indicates that exposing a model to a training set may suffer from bias and spurious correlations. We also make comparisons on a larger number of ID and OOD datasets in Appendix A."
        },
        {
            "heading": "5.5 Ablation Studies",
            "text": "Model Components Ablation studies are carried out to validate the effectiveness of each main component in our model. Specifically, the following variants are investigated: 1. w/o Obj. removes the visual object detection step, i.e., only the left term in Eq. 1 is adopted. 2. w/o Calib. removes the uncertainty calibration step and indiscriminately uses descriptors from LLMs. 3. w/o Know. only uses class names to represent each class without\ndescriptors from LLMs. Results in Table 2 and Table 3 show that our method outperforms all the above variants. Specifically, we can observe that: 1. Incorporating knowledge from LLMs (see w/o Know.) improves the OOD detection performance by 4.42%. This verifies that world knowledge is important in multi-modal OOD detection. 2. Both uncertainty calibration (see w/o Calib.) and visual object detection (see w/o Obj.) help to improve the OOD detection performance.\nUncertainty Calibration To evaluate our proposed uncertainty calibration method, we perform ablation on alternatives: 1. Confidence (Si et al., 2022a) leverages language model probabilities of\ngenerated descriptors as the confidence score. 2. Self-consistency (Wang et al., 2022) makes multiple predictions for one input and makes use of the frequency of the majority prediction as the confidence score. 3. Self-evaluation (Kadavath et al., 2022) asks LLMs to first propose answers and then estimate the probability that these answers are correct. Results in Table 4 show that our uncertainty calibration method performs better than other variants. This further indicates that dedicated uncertainty calibration approaches should be explored to safely explore generations from LLMs.\nVisual Object Detection We evaluate the visual object detection module by implementing the following variants: 1. Class Sim. uses class name c instead of the descriptive features t(c) in the right term of Eq. 1. 2. Simple Det. adopts a simple object detection model with a smaller vocabulary of 80 objects (Li et al., 2023). As shown in Table 4, our method outperforms the above variants. Specifically, we can observe that: 1. Calculating the similarity between detected image concept names and ID class names without descriptors degenerates the OOD detection performance. 2. Using a general object detection model with a large vocabulary of objects helps to improve the performance."
        },
        {
            "heading": "5.6 Further Analysis",
            "text": "Cases of Retrieval Feedback We provide a case study where descriptor sets for the same class are similar/dissimilar in textual form. Figure 7 illustrates that even with low textual similarity and variations in textual form, two descriptor sets can have consistent retrieval feedback if they accurately capture the descriptive features of the same object.\nAnalysis of Unlabeled Image Set Figure 8 shows the effect of unlabeled image set with varying sizes on the OOD detection performance. We compose image subsets either through random\ndown-sampling from the original unlabeled image set (\u201cRandom\u201d), or removing images from certain categories (\u201cDiversity\u201d). We can observe that: 1. Our method achieves superior OOD detection performance along with the increase of unlabeled image data. 2. Unlabeled image sets that lack diversity achieve limited detection performance, especially in small sizes."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we introduce a novel framework for multi-modal out-of-distribution detection. It employs world knowledge from large language models (LLMs) to characterize ID classes. An uncertainty calibration method is introduced to tackle the issue of LLMs\u2019 hallucinations, and visual object detection is proposed to fully capitalize on the generated world knowledge. Experiments on a variety of OOD detection tasks show the effectiveness of our method, demonstrating its exciting property where world knowledge can be reliably exploited\nvia LLMs by evaluating their uncertainty.\nLimitations\nWe identify one major limitation of this work is its input modality. Specifically, our method is limited to detecting visual out-of-distribution (OOD) inputs and ignores inputs in other modalities such as textual, audio, electroencephalogram (EEG) and robotic features. These modalities provide valuable information that can be used to construct better OOD detectors. Fortunately, through multi-modal pre-training models (Xu et al., 2021; Huo et al., 2021), we can obtain robust representations in various modalities.\nEthics Statement\nThis work does not raise any direct ethical issues. In the proposed work, we seek to develop a zeroshot multi-modal OOD detection model equipped with world knowledge from LLMs, and we believe this work can benefit the field of OOD detection, with the potential to benefit other fields requiring trustworthy models. All experiments are conducted on open datasets."
        },
        {
            "heading": "A More Results",
            "text": "We use an extra collection of ID datasets to showcase the versatility of our method: CUB-200 (Wah et al., 2011), STANFORDCARS (Krause et al., 2013), FOOD-101 (Bossard et al., 2014), OXFORD-PET (Parkhi et al., 2012), and three variants of ImageNet constructed by Ming et al. (2022a), i.e., ImageNet10, ImageNet-20, ImageNet-100. The results are shown in Table 5, demonstrating that our method offers superior performance on various multi-modal OOD detection tasks without training."
        },
        {
            "heading": "B More Implementation Details",
            "text": "In our experiments, we adopt CLIP (Radford et al., 2021) as the pre-trained vision-language model. Specifically, we mainly use CLIP-B/16 (CLIP-B), which consists of a ViT-B/16 Transformer as the image encoder and a masked self-attention Transformer (Vaswani et al., 2017) as the text encoder. We also use CLIP-L/14 (CLIP-L) as a representative of large models. To obtain world knowledge corresponding to each class, we query text-davinci003 (Ouyang et al., 2022) with a sampling temperature of 0.7 and a maximum token length of 100.\nTo obtain retrieval feedback for each descriptor set, We construct a fixed set of unlabeled images, denoted as M, through the random selection of m = 50000 images spanning 1000 categories. These images are extracted from the training set of ImageNet-1k without corresponding labels. For descriptor set d, the retriever R(\u00b7) retrieves top k similar images from M:\nR\u2032(d) = argmax M\u2282M,|M |=k E x\u2208M d\u2208d \u03c3(I(x), T (d)).\nFrom the retrieved image subset R\u2032(d) we derive a binary vector R(d), with entry j equal to 1 if the j-th image of M is in R\u2032(d). In order to determine whether two descriptor sets, d and d\u2032, are consistent with each other, denoted as C(d,d\u2032), we incorporate the following two constraints:\nC(d,d\u2032) =1 [ \u03c3(R(d), R(d\u2032)) \u2265 \u03b7 ] \u2227\n1 [ \u03c3( \u2211 d\u2208d T (d) |d| , \u2211 d\u2032\u2208d\u2032 T (d \u2032) |d\u2032| ) \u2265 \u03b7 \u2032 ] , (3)\nwhere the first constraint measures the cosine similarity between R(d) and R(d\u2032), the second constraint computes the cosine similarity between the averaged textual embeddings of descriptors in d and d\u2032. Note that the second constraint that computes textual similarity is optional in our method. We evaluate its impact by constructing an ablation variant relying solely on the first constraint. Its average performance on four OOD datasets with ImageNet-1k as ID dataset is 38.59 in FPR95 and 91.37 in AUROC, which outperforms the other zero-shot baselines as well. We set k = 50 for image retrieval and \u03b7 = 0.9, \u03b7\u2032 = 0.99 for consistency computation. We set \u03b3 = 0.5 as the confidence threshold for p(c).\nIn visual object detection, we use CBNetV2Swin-Base from Cai et al. (2022) with a vocabulary of 600 objects as our general object detector. To construct the ablation variant \u201cSimple Det.\u201d, we employ YOLOv6-L6 from Li et al. (2023) with a smaller vocabulary of 80 categories."
        },
        {
            "heading": "C Robustness to Sampling Temperature",
            "text": "T .\nWe vary sampling temperature T for LLM generation among {0.3, 0.5, 0.7, 0.9, 1.1}. It can be seen in Figure 9 that regardless of the temperature, our method consistently outperforms the ablation variant \u201cw/o Know.\u201d which does not incorporate additional world knowledge from LLMs. We can\nalso observe that an intermediate temperature of 0.7 can lead to the best performance."
        },
        {
            "heading": "D Reliability under Different LLMs, Image Detectors and OOD Detectors",
            "text": "To further verify the reliability of our method, we perform OOD detection using our method under different LLMs (GPT-4, ChatGPT, Claude-1, Claude-2, Bard and text-davinci-003), image detectors (YOLOv6 (Li et al., 2023), InternImage (Wang et al., 2023), Bigdetection (Cai et al., 2022), CoDETR (Zong et al., 2023)), and OOD detectors (CLIP-based OOD detector without softmax scaling (Ming et al., 2022a)). We use ImageNet-1K as ID dataset, and iNaturalist/SUN/Places/Texture as OOD datasets. As shown in Table 6, our method is reliable when using different LLMs, image detectors and OOD detectors."
        }
    ],
    "title": "Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection",
    "year": 2023
}