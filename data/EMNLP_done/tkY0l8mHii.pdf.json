{
    "abstractText": "Named entity recognition (NER) is a fundamental task in natural language processing. Recently, NER has been formulated as a machine reading comprehension (MRC) task, in which manually-crafted queries are used to extract entities of different types. However, current MRC-based NER techniques are limited to extracting a single type of entities at a time and are largely geared towards resource-rich settings. This renders them inefficient during the inference phase, while also leaving their potential untapped for utilization in low-resource settings. We suggest a query-parallel MRCbased approach to address these issues, which is capable of extracting multiple entity types concurrently and is applicable to both resourcerich and resource-limited settings. Specifically, we propose a query-parallel encoder which uses a query-segmented attention mechanism to isolate the semantics of queries and model the query-context interaction with a unidirectional flow. This allows for easier generalization to new entity types or transfer to new domains. After obtaining the query and context representations through the encoder, they are fed into a query-conditioned biaffine predictor to extract multiple entities at once. The model is trained with parameter-efficient tuning technique, making it more data-efficient. We conduct extensive experiments and demonstrate that our model performs competitively against strong baseline methods in resource-rich settings, and achieves state-of-the-art results in low-resource settings, including training-fromscratch, in-domain transfer and cross-domain transfer tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuhao Zhang"
        },
        {
            "affiliations": [],
            "name": "Yongliang Wang"
        }
    ],
    "id": "SP:e485cbfbcd006ab03105ec18c2f7c2ead15b5cfa",
    "references": [
        {
            "authors": [
                "Xiang Chen",
                "Lei Li",
                "Shumin Deng",
                "Chuanqi Tan",
                "Changliang Xu",
                "Fei Huang",
                "Luo Si",
                "Huajun Chen",
                "Ningyu Zhang."
            ],
            "title": "LightNER: A lightweight tuning paradigm for low-resource NER via pluggable prompting",
            "venue": "Proceedings of the 29th Inter-",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Cheng",
                "Mitchell Bowden",
                "Bhushan Ramesh Bhange",
                "Priyanka Goyal",
                "Thomas Packer",
                "Faizan Javed."
            ],
            "title": "An end-to-end solution for named entity recognition in ecommerce search",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Billy Chiu",
                "Gamal Crichton",
                "Anna Korhonen",
                "Sampo Pyysalo."
            ],
            "title": "How to train good word embeddings for biomedical NLP",
            "venue": "Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 166\u2013174, Berlin, Germany. Asso-",
            "year": 2016
        },
        {
            "authors": [
                "Jason P.C. Chiu",
                "Eric Nichols."
            ],
            "title": "Named entity recognition with bidirectional LSTM-CNNs",
            "venue": "Transactions of the Association for Computational Linguistics, 4:357\u2013370.",
            "year": 2016
        },
        {
            "authors": [
                "Leyang Cui",
                "Yu Wu",
                "Jian Liu",
                "Sen Yang",
                "Yue Zhang."
            ],
            "title": "Template-based named entity recognition using BART",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1835\u20131845, Online. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Shijin Wang",
                "Guoping Hu."
            ],
            "title": "Revisiting pre-trained models for Chinese natural language processing",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 657\u2013668, Online. As-",
            "year": 2020
        },
        {
            "authors": [
                "Sarkar Snigdha Sarathi Das",
                "Arzoo Katiyar",
                "Rebecca Passonneau",
                "Rui Zhang."
            ],
            "title": "CONTaiNER: Few-shot named entity recognition via contrastive learning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Alexander Fritzler",
                "Varvara Logacheva",
                "Maksim Kretov."
            ],
            "title": "Few-shot classification in named entity recognition task",
            "venue": "Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing, SAC \u201919, page 993\u20131000, New York, NY, USA. As-",
            "year": 2019
        },
        {
            "authors": [
                "Jinlan Fu",
                "Xuanjing Huang",
                "Pengfei Liu."
            ],
            "title": "SpanNER: Named entity re-/recognition as span prediction",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Yao Fu",
                "Chuanqi Tan",
                "Mosha Chen",
                "Songfang Huang",
                "Fei Huang."
            ],
            "title": "Nested named entity recognition with partially-observed treecrfs",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 35(14):12839\u201312847.",
            "year": 2021
        },
        {
            "authors": [
                "Demi Guo",
                "Alexander Rush",
                "Yoon Kim."
            ],
            "title": "Parameter-efficient transfer learning with diff pruning",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Dilek Hakkani-T\u00fcr",
                "Gokhan Tur",
                "Asli Celikyilmaz",
                "YunNung Chen",
                "Jianfeng Gao",
                "Li Deng",
                "Ye-Yi Wang."
            ],
            "title": "Multi-Domain Joint Semantic Frame Parsing Using Bi-Directional RNN-LSTM",
            "venue": "Proc. Interspeech 2016, pages 715\u2013719.",
            "year": 2016
        },
        {
            "authors": [
                "Matthew Henderson",
                "Ivan Vuli\u0107."
            ],
            "title": "ConVEx: Data-efficient and few-shot slot labeling",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel."
            ],
            "title": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units",
            "venue": "CoRR, abs/1606.08415.",
            "year": 2016
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for NLP",
            "venue": "Proceedings of the 36th International Conference",
            "year": 2019
        },
        {
            "authors": [
                "Edward Hu",
                "Yelong Shen",
                "Phil Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "year": 2021
        },
        {
            "authors": [
                "Jiaxin Huang",
                "Chunyuan Li",
                "Krishan Subudhi",
                "Damien Jose",
                "Shobana Balakrishnan",
                "Weizhu Chen",
                "Baolin Peng",
                "Jianfeng Gao",
                "Jiawei Han."
            ],
            "title": "Fewshot named entity recognition: An empirical baseline study",
            "venue": "Proceedings of the 2021 Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Guillaume Lample",
                "Miguel Ballesteros",
                "Sandeep Subramanian",
                "Kazuya Kawakami",
                "Chris Dyer."
            ],
            "title": "Neural architectures for named entity recognition",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Gina-Anne Levow."
            ],
            "title": "The third international Chinese language processing bakeoff: Word segmentation and named entity recognition",
            "venue": "Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 108\u2013117, Sydney, Australia. Asso-",
            "year": 2006
        },
        {
            "authors": [
                "Fei Li",
                "ZhiChao Lin",
                "Meishan Zhang",
                "Donghong Ji."
            ],
            "title": "A span-based model for joint overlapped and discontinuous named entity recognition",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Xiaoya Li",
                "Jingrong Feng",
                "Yuxian Meng",
                "Qinghong Han",
                "Fei Wu",
                "Jiwei Li."
            ],
            "title": "A unified MRC framework for named entity recognition",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5849\u20135859, On-",
            "year": 2020
        },
        {
            "authors": [
                "Bill Yuchen Lin",
                "Dong-Ho Lee",
                "Ming Shen",
                "Ryan Moreno",
                "Xiao Huang",
                "Prashant Shiralkar",
                "Xiang Ren."
            ],
            "title": "TriggerNER: Learning with entity triggers as explanations for named entity recognition",
            "venue": "Proceedings of the 58th Annual Meeting of",
            "year": 2020
        },
        {
            "authors": [
                "Pierre Lison",
                "Jeremy Barnes",
                "Aliaksandr Hubin",
                "Samia Touileb."
            ],
            "title": "Named entity recognition without labelled data: A weak supervision approach",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1518\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Jingjing Liu",
                "Panupong Pasupat",
                "Scott Cyphers",
                "Jim Glass."
            ],
            "title": "Asgard: A portable architecture for multilingual dialogue systems",
            "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 8386\u20138390.",
            "year": 2013
        },
        {
            "authors": [
                "Zihan Liu",
                "Yan Xu",
                "Tiezheng Yu",
                "Wenliang Dai",
                "Ziwei Ji",
                "Samuel Cahyawijaya",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Crossner: Evaluating cross-domain named entity recognition",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 35(15):13452\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "SGDR: stochastic gradient descent with warm restarts",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.",
            "year": 2017
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
            "year": 2019
        },
        {
            "authors": [
                "Chao Lou",
                "Songlin Yang",
                "Kewei Tu."
            ],
            "title": "Nested named entity recognition as latent lexicalized constituency parsing",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6183\u20136198,",
            "year": 2022
        },
        {
            "authors": [
                "Yaojie Lu",
                "Qing Liu",
                "Dai Dai",
                "Xinyan Xiao",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun",
                "Hua Wu."
            ],
            "title": "Unified structure generation for universal information extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Yi Luan",
                "Dave Wadden",
                "Luheng He",
                "Amy Shah",
                "Mari Ostendorf",
                "Hannaneh Hajishirzi."
            ],
            "title": "A general framework for information extraction using dynamic span graphs",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Jie Ma",
                "Miguel Ballesteros",
                "Srikanth Doss",
                "Rishita Anubhai",
                "Sunil Mallya",
                "Yaser Al-Onaizan",
                "Dan Roth."
            ],
            "title": "Label semantics for few shot named entity recognition",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1956\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Dublin",
                "Ireland"
            ],
            "title": "Association for Computational Linguistics",
            "year": 1971
        },
        {
            "authors": [
                "Xuezhe Ma",
                "Eduard Hovy."
            ],
            "title": "End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064\u20131074, Berlin, Germany.",
            "year": 2016
        },
        {
            "authors": [
                "Yuning Mao",
                "Lambert Mathias",
                "Rui Hou",
                "Amjad Almahairi",
                "Hao Ma",
                "Jiawei Han",
                "Scott Yih",
                "Madian Khabsa."
            ],
            "title": "UniPELT: A unified framework for parameter-efficient language model tuning",
            "venue": "Proceedings of the 60th Annual Meeting of the Associa-",
            "year": 2022
        },
        {
            "authors": [
                "Pedro Henrique Martins",
                "Zita Marinho",
                "Andr\u00e9 F.T. Martins."
            ],
            "title": "Joint learning of named entity recognition and entity linking",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 190\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Xue Mengge",
                "Bowen Yu",
                "Zhenyu Zhang",
                "Tingwen Liu",
                "Yue Zhang",
                "Bin Wang."
            ],
            "title": "Coarse-to-Fine Pretraining for Named Entity Recognition",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Makoto Miwa",
                "Mohit Bansal."
            ],
            "title": "End-to-end relation extraction using LSTMs on sequences and tree structures",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1105\u20131116, Berlin,",
            "year": 2016
        },
        {
            "authors": [
                "Tomoko Ohta",
                "Yuka Tateisi",
                "Jin-Dong Kim."
            ],
            "title": "The genia corpus: An annotated research abstract corpus in molecular biology domain",
            "venue": "Proceedings of the Second International Conference on Human Language Technology Research, HLT \u201902, page 82\u201386,",
            "year": 2002
        },
        {
            "authors": [
                "Junting Pan",
                "Ziyi Lin",
                "Xiatian Zhu",
                "Jing Shao",
                "Hongsheng Li."
            ],
            "title": "St-adapter: Parameter-efficient image-to-video transfer learning",
            "venue": "Advances in Neural Information Processing Systems, volume 35, pages 26462\u201326477. Curran Associates, Inc.",
            "year": 2022
        },
        {
            "authors": [
                "Giovanni Paolini",
                "Ben Athiwaratkun",
                "Jason Krone",
                "Jie Ma",
                "Alessandro Achille",
                "Rishita Anubhai",
                "Cicero Nogueira dos Santos",
                "Bing Xiang",
                "Stefano Soatto."
            ],
            "title": "Structured prediction as translation between augmented natural languages",
            "venue": "9th Inter-",
            "year": 2021
        },
        {
            "authors": [
                "Yongliang Shen",
                "Xinyin Ma",
                "Zeqi Tan",
                "Shuai Zhang",
                "Wen Wang",
                "Weiming Lu."
            ],
            "title": "Locate and label: A two-stage identifier for nested named entity recognition",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Yongliang Shen",
                "Xiaobin Wang",
                "Zeqi Tan",
                "Guangwei Xu",
                "Pengjun Xie",
                "Fei Huang",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "Parallel instance query network for named entity recognition",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Jana Strakov\u00e1",
                "Milan Straka",
                "Jan Hajic."
            ],
            "title": "Neural architectures for nested NER through linearization",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5326\u20135331, Florence, Italy. Association for Compu-",
            "year": 2019
        },
        {
            "authors": [
                "Zeqi Tan",
                "Yongliang Shen",
                "Shuai Zhang",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "A sequence-to-set network for nested named entity recognition",
            "venue": "Proceedings of the 30th International Joint Conference on Artificial Intelligence, IJCAI-21.",
            "year": 2021
        },
        {
            "authors": [
                "Minghao Tang",
                "Peng Zhang",
                "Yongquan He",
                "Yongxiu Xu",
                "Chengpeng Chao",
                "Hongbo Xu."
            ],
            "title": "DoSEA: A domain-specific entity-aware framework for crossdomain named entity recogition",
            "venue": "Proceedings of",
            "year": 2022
        },
        {
            "authors": [
                "Simone Tedeschi",
                "Simone Conia",
                "Francesco Cecconi",
                "Roberto Navigli."
            ],
            "title": "Named Entity Recognition for Entity Linking: What works and what\u2019s next",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2584\u20132596,",
            "year": 2021
        },
        {
            "authors": [
                "Erik F. Tjong Kim Sang",
                "Fien De Meulder."
            ],
            "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
            "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142\u2013",
            "year": 2003
        },
        {
            "authors": [
                "Jue Wang",
                "Lidan Shou",
                "Ke Chen",
                "Gang Chen."
            ],
            "title": "Pyramid: A layered model for nested named entity recognition",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5918\u20135928, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Congying Xia",
                "Chenwei Zhang",
                "Tao Yang",
                "Yaliang Li",
                "Nan Du",
                "Xian Wu",
                "Wei Fan",
                "Fenglong Ma",
                "Philip Yu."
            ],
            "title": "Multi-grained named entity recognition",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1430\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Hang Yan",
                "Tao Gui",
                "Junqi Dai",
                "Qipeng Guo",
                "Zheng Zhang",
                "Xipeng Qiu."
            ],
            "title": "A unified generative framework for various NER subtasks",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Yi Yang",
                "Arzoo Katiyar."
            ],
            "title": "Simple and effective few-shot named entity recognition with structured nearest neighbor learning",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6365\u20136375,",
            "year": 2020
        },
        {
            "authors": [
                "Juntao Yu",
                "Bernd Bohnet",
                "Massimo Poesio."
            ],
            "title": "Named entity recognition as dependency parsing",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6470\u2013 6476, Online. Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Zexuan Zhong",
                "Danqi Chen."
            ],
            "title": "A frustratingly easy approach for entity and relation extraction",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Enwei Zhu",
                "Jinpeng Li."
            ],
            "title": "Boundary smoothing for named entity recognition",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7096\u20137108, Dublin, Ireland. Association for",
            "year": 2022
        },
        {
            "authors": [
                "LightNER (Chen"
            ],
            "title": "2022) formulates NER as an entity span sequence generation task. It adopts a sequence-to-sequence model with pointer and pluggable prompting mechanism to tackle lowresource NER",
            "year": 2022
        },
        {
            "authors": [
                "Li"
            ],
            "title": "2022) treats NER as a span prediction problem and proposes the boundary smoothing regularization technique to boosting the model performance. C Parameter-Efficient Tuning with",
            "year": 2022
        },
        {
            "authors": [
                "Prefix-tuning (Li",
                "Liang"
            ],
            "title": "2021) enhances the multi-head self-attention in each Transformer layer by adding of a number of trainable vectors to the input, allowing the original tokens to attend",
            "year": 2021
        },
        {
            "authors": [
                "Hu"
            ],
            "title": "2021) introduces trainable low-rank matrices and utilizes them to update the Query Q and Value V in multi-head self-attention",
            "venue": "LoRA (Guo et al.,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Named entity recognition (NER) is a fundamental task in natural language processing, aiming at detecting the text spans that refer to entities. It has been widely used in various downstream tasks, such as entity linking (Martins et al., 2019; Tedeschi et al., 2021), relation extraction (Miwa\nand Bansal, 2016; Zhong and Chen, 2021) and information retrieval (Cheng et al., 2021).\nTraditionally, NER has been formalized as a sequence labeling task, assigning a single tag class to each token in a sentence (Chiu and Nichols, 2016; Ma and Hovy, 2016; Xia et al., 2019; Devlin et al., 2019; Lin et al., 2020). Recently, several new NER paradigms have been proposed, which conceptualize NER as span classification (Luan et al., 2019; Yu et al., 2020; Li et al., 2021; Shen et al., 2021; Fu et al., 2021a), sequence generation (Strakov\u00e1 et al., 2019; Yan et al., 2021; Tan et al., 2021; Paolini et al., 2021; Lu et al., 2022) and constituency parsing (Fu et al., 2021b; Lou et al., 2022) tasks. In spite of achieving promising performance, these approaches require large amounts of well-annotated in-domain data (Lison et al., 2020; Ma et al., 2022). The data annotation usually involves carefully defined guideline and annotators with domain expertise, which could be quite time-consuming and costprohibitive. As a result, the development of NER systems is a costly endeavor in real-world scenarios where usually only a small amount of labeled data is available for new domains (Huang et al., 2021).\nRecently, some works reformulate NER as a ma-\nchine reading comprehension (MRC) task (Li et al., 2020; Mengge et al., 2020). As illustrated in Figure 1(a), the input sentence is regarded as the context, a collection of queries that have been manually crafted to represent various entity types can be viewed as questions, then extracting entities from the context could be solved by employing question answer techniques. Prior MRC-based NER approaches are limited to extracting one type of entities at a time and have largely been applied to the standard supervised setting. In this context, they have exhibited superior performance in comparison to conventional sequence labeling techniques and have demonstrated comparable performance with reduced training data. Nonetheless, the extraction of a singular entity type at a time is not particularly efficient, and their potential for deployment in low-resource settings remains largely unexplored.\nTo alleviate these issues, this paper presents a query-parallel machine reading comprehension framework for named entity recognition (QPMRCNER). It is suitable for both resource-rich and resource-limited settings, and is characterized by enhanced efficiency, as illustrated in Figure 1(b). It consists of two key components: the query-parallel encoder and the query-conditioned biaffine predictor. The query-parallel encoder takes the combination of the context sentence and the queries with shared continuous prompt as input, and utilizes a query-segmented attention mechanism to separate the queries from one another and model the query-context interaction with a unidirectional flow, thus facilitating easier generalization to new entity types and domain transfer. After obtaining the contextualized representations from the query-parallel encoder, we feed them to the query-conditioned biaffine predictor to extract entities of multiple types simultaneously. The model is trained with parameter-efficient techniques for data-efficiency (Li and Liang, 2021; Pan et al., 2022). We conduct extensive experiments, and in most cases, our model outperforms the present SOTA methods.\nThe main contributions of this paper are summarized as follows: (1) We introduce a novel query-parallel machine reading comprehension framework QPMRC-NER that is capable of handling low-resource NER tasks effectively and efficiently. (2) Our MRC-based architecture facilitates simultaneous extraction of entities belonging to multiple categories, resulting in faster inference speed. (3) We conduct extensive evaluations of\nQPMRC-NER across a diverse range of NER tasks. Our model exhibits competitive performance in resource-rich settings and achieves state-of-the-art results in low-resource settings, including trainingfrom-scratch, in-domain transfer, and cross-domain transfer tasks."
        },
        {
            "heading": "2 Method",
            "text": "This section presents the task formulation in Section 2.1, then introduces the proposed method QPMRC-NER, illustrated in Figure 2. QPMRCNER consists of two components: the queryparallel encoder and the query-conditioned biaffine predictor, explained in Section 2.2 and Section 2.3 respectively."
        },
        {
            "heading": "2.1 Task Formulation",
            "text": "Given an input sentence S = {w1, w2, ..., wn} with sequence length n, the goal is to extract all entities L = {< Istarti , Iendi , T tag i >}mi=0 from it. Here, Istarti and I end i indicate the start and end positions of the i-th entity span, T tagi denotes the type of the i-th entity which belongs to a finite set of entity types E . Our method utilizes a query Q for each entity type to extract entities from the input sequence. Thus, the task can be formulated as extracting all entities L from S based on queries {Q1, Q2, ..., Q|E|}."
        },
        {
            "heading": "2.2 Query-Parallel Encoder",
            "text": "In order to extract different types of entities simultaneously with the machine reading comprehension framework, we concatenate the input sentence and queries and feed them into a transformer-based encoder, from which we obtain the context word representations and entity type representations that are used for entity prediction.\nQuery Generation and Model Inputs Conventional MRC-based NER methods rely on manuallycrafted queries to represent each entity type, which often requires domain expertise and laboriously tuning of query words, rendering it non-reusable for new entity types.\nRather than relying on manually tuning, we construct queries using a shared prompt prefix that is composed of a set of learnable vectors. This approach renders the prompt more suitable for NER task and facilitates generalization when dealing with new entity types. So the query for the i-th entity type is Qi = {p0, p1, ..., pm, ei,1, ..., ei,n}, where p0, p1, ..., pm represent the shared learnable\nvectors and ei,1, ..., ei,n represent tokens of the i-th entity type name.\nThe model inputs are illustrated in Figure 2. We use the concatenation of context sentence and queries as the token inputs and assign the two parts with different segment ids, namely ua and ub. We use the absolute position of tokens within each query or context sentence as the position ids, which guarantees the continuous prompt share the same position ids across queries. This precludes the occurrence of position confliction, as the queries are isolated from each other by virtue of the querysegmented attention mechanism proposed below.\nQuery-Context Interaction via QuerySegmented Attention The attention mechanism plays a critical role in modeling interactions between tokens in the transformer-based backbone. With self-attention, tokens in different queries could interact with each other. While modeling the dependencies between multiple entity types may be beneficial in some occasions, it adds difficulty for generalizing to new entity types in low resource setting.\nIn order to address this issue, we propose the query-segmented attention, which is employed as a substitute for the self-attention component in the transformer architecture. The query-segmented attention segregates queries from one another and models the query-context interaction with a unidirectional flow as depicted in Figure 3. This mechanism enables context tokens to only attend to other context tokens, while query tokens can attend to tokens within the same query and to tokens in the context, allowing them to recognize the most pertinent parts of the context according to the entity type.\nThus, the query-segmented attention can be formulated as:\nQ = H l\u22121WQl , K = H l\u22121WKl (1)\nMij = { 0, allow to attend \u2212\u221e, prevent from attending (2)\nAl = softmax\n( QK\u22a4\u221a\ndk +M\n) (H l\u22121W Vl ) (3)\nwhere parameters WQl ,W K l ,W V l \u2208 Rdh\u00d7da project the previous layer\u2019s output H l\u22121 \u2208 Rdx\u00d7dh to queries, keys and values respectively. The mask matrix M \u2208 {0,\u2212inf}dx\u00d7dx controls whether two tokens can attend to each other by setting its values to 0 or \u2212inf . Al \u2208 Rdx\u00d7da represents the output of the query-segmented attention.\nAfter the transformer encoding, the outputs of the last-layer are used to acquire the query and\ncontext representations. For each query Qi, the representation of its first token in the prompt is employed as the query representation hqi . For each context word wj , its representation hcj is obtained by aggregating the corresponding word-piece token representations through mean pooling.\nParameter-Efficient Tuning The query-parallel encoder is initialized with the weights of a pretrained BERT model (Devlin et al., 2019). Rather than fine-tuning the BERT encoder directly, we adopt a parameter-efficient tuning strategy that enables effective tuning of a pretrained language model by updating only a small number of extra parameters while keeping most of the pretrained parameters frozen, thereby making the tuning process more data-efficient (Li and Liang, 2021; Pan et al., 2022). Specifically, our approach employs UNIPELT (Mao et al., 2022) as the parameterefficient tuning method, which integrates several existing parameter-efficient tuning techniques as sub-modules and controls them via gating mechanism. Further details are in Appendix C."
        },
        {
            "heading": "2.3 Query-Conditioned Biaffine Predictor",
            "text": "After obtaining the context and query representations from the query-parallel encoder, we feed them to an entity predictor to generate the entity predictions. The biaffine classifier (Yu et al., 2020) has been demonstrated to be an effective approach for entity prediction in prior studies, which jointly predicts the start and end positions of an entity span as well as the entity types. However, the prior biaffine approaches are employed in the span prediction paradigm without leveraging the the semantics of entity type names, which limits its ability to generalize to new entity types.\nIn this work, we propose a query-conditioned biaffine predictor for entity prediction, which integrates the biaffine predictor into the query-parallel machine reading comprehension framework and could utilizing the semantic meaning of entity type names. For each query Qi, we first obtain two query-conditioned representations hsi and hei corresponding to the start and end positions of entity spans through feed-forward neural networks:\nhsij = gelu((h qi \u2295 hcj)Ws + bs) (4)\nheij = gelu((h qi \u2295 hcj)We + be) (5)\nwhere gelu is an activation function (Hendrycks and Gimpel, 2016), Ws,We \u2208 R2dh\u00d7dp , bs, be \u2208 Rdp are learnable parameters.\nThen we predict whether a span starting from position j and ending at position k belongs to certain entity type i by:\nrij,k =h si j \u22a4Umh ei j\n+ (hsij \u2295 h ei j \u2295 lk\u2212j)Wm + bm (6)\nyij,k = sigmoid(r i j,k) (7)\nwhere lk\u2212j \u2208 Rdp represents the (k \u2212 j)-th width embedding from a learnable look-up table, Um \u2208 Rdp\u00d71\u00d7dp , Wm \u2208 R(2dp+dl)\u00d71 and bm \u2208 R1 are learnable parameters. Different from traditional biaffine predictor, the probabilities of an entity span belong to each entity type are predicted independently without inter-type competition, thus allowing for the sharing of predictor parameters across different entity types, which is beneficial for entity type expanding. To avoid conflict predictions in flat NER tasks during inference, we only keep the span with the highest prediction probability for any overlapping entity spans.\nBesides the span-level prediction, we suggest a token-level auxiliary task to enhance the quality of the context and query representations. This task predicts whether a token is associated with a particular entity type through a feed-forward neural network as follows:\nhtij = gelu((h qi \u2295 hcj)Wt + bt) (8)\nyij \u2032 = sigmoid(htij W \u2032 t + b \u2032 t) (9)\nFor both the span and token level tasks, we use the binary cross entropy function as the loss metric:\nLs =\u2212 \u2211\n0\u2264j\u2264k\u2264n \u2211 i\u2208E 1[y\u0302ij,k = 1] log y i j,k\n+ 1[y\u0302ij,k = 0] log(1\u2212 yij,k) (10)\nLt =\u2212 \u2211\n0\u2264j\u2264n \u2211 i\u2208E 1[y\u0302ij = 1] log y i j\n+ 1[y\u0302ij = 0] log(1\u2212 yij) (11)\nwhere Ls and Lt are span level and token level losses, y\u0302ij,k and y\u0302 i j are the corresponding labels. Then the final loss L on training set D is:\nL = \u2211 D Ls + \u03bbLt (12)\nwhere \u03bb is a coefficient."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Setups",
            "text": "Evaluation Tasks To verify the effectiveness of the proposed method, we conduct extensive experiments in both resource-rich and resource-limited NER settings:\n\u2022 Standard supervised NER setting: This task evaluates the efficacy of a model when adequate labeled data is available for training.\n\u2022 Training-from-scratch low-resource NER setting: This task assess models\u2019 effectiveness under conditions of limited training data.\n\u2022 In-domain low-resource NER setting: In this task, models are trained on a domain where some entity types have a sufficient number of labeled data while others have only a limited amount of labeled data, reflecting a common scenario where new entity types emerge in existing text domain. This could demonstrate the model\u2019s capacity for in-domain transfer.\n\u2022 Cross-domain low-resource NER setting: This task involves training models on a resource-rich source domain, followed by finetuning on a target domain with different entity types and a limited amount of labeled data, demonstrating the model\u2019s capacity for crossdomain transfer.\nDatasets For standard supervised NER setting, we conduct experiments on three datasets: CoNLL03 (Tjong Kim Sang and De Meulder, 2003), MSRA (Levow, 2006) and GENIA (Ohta et al., 2002), which are English flat NER dataset, Chinese flat NER dataset and English nested NER dataset respectively. For training-from-scratch lowresource setting, we use the MIT Restaurant Review (Liu et al., 2013), MIT Movie Review (Liu et al., 2013), and Airline Travel Information Systems (ATIS) (Hakkani-T\u00fcr et al., 2016) datasets. For the in-domain low-resource setting, we use the CoNLL03 (Tjong Kim Sang and De Meulder, 2003). For cross-domain low-resource setting, we use the CrossNER dataset (Liu et al., 2021) which contains five diverse domains, including politics, natural science, music, literature and artificial intelligence.\nBaselines We conduct a comprehensive comparison of QPMRC-NER with several representative\nmethods as well as state-of-the-art approaches, including the classic BiLSTM-CRF (Ma and Hovy, 2016) and BERT-Tagger (Devlin et al., 2019), single query machine reading comprehension method BERT-MRC (Li et al., 2020), parameter-efficient tuning method LightNER (Chen et al., 2022), template-based low-resource NER method TemplateNER (Cui et al., 2021), cross-domain specialized method DoSEA (Tang et al., 2022), and several recently proposed supervised methods including Pyramid (Wang et al., 2020), BARTNER (Yan et al., 2021), PIQN (Shen et al., 2022), and Boundary Smoothing NER (BS-NER) (Zhu and Li, 2022). See Appendix B for a detailed introduction of those models.\nImplementation Details We use pretrained BERT (Devlin et al., 2019) to initialize our encoder. For a fair comparison, we use BERTlarge on CoNLL03, MIT Restaurant Review, MIT Movie Review and ATIS, BERT-base on CrossNER, BioBERT-large (Chiu et al., 2016) on GENIA and Chinese-BERT-WWM (Cui et al., 2020) on Chinese MSRA. We utilize the AdamW Optimizer (Loshchilov and Hutter, 2019) with a cosine annealed warm restart schedule (Loshchilov and Hutter, 2017) to train our model. In order to as-\nsess the model performance, we utilize span-based evaluation metrics, where an entity is only considered accurate if both the entity boundary and entity type are correct. The F1-score is used as the metric for evaluation. See Appendix A for more detailed hyper-parameter settings."
        },
        {
            "heading": "3.2 Standard Supervised NER",
            "text": "We first evaluate our method under the standard supervised NER settings on the CoNLL03, MSRA and GENIA datasets. For the CoNLL03, we follow (Lample et al., 2016; Yu et al., 2020; Yan et al., 2021) to train the model with the combined data from the training and development sets. A comparison with the state-of-the-art methods are listed in Table 1. It is evident that QPMRC-NER, which has been designed for low-resource NER, surpasses several recently proposed strong baseline models in rich-resource settings and performs comparable with the state-of-the-art methods, suggesting that QPMRC-NER is also suitable for NER tasks with abundant training data."
        },
        {
            "heading": "3.3 Training-from-scratch Low-resource NER",
            "text": "We analyze the efficacy of our method under the training-from-scratch low-resource NER settings where only K samples of each entity type are available for training. Following (Cui et al., 2021), we employ the MIT Restaurant Review, MIT Movie Review and ATIS datasets for model evaluation by randomly sampling a fixed number of training instances per entity type (K=10, 20, 50, 100, 200, 500 for MIT Movie and MIT restaurant, and K=10, 20, 50 for ATIS).\nThe experimental results shown in Table 2 demonstrates that our method achieves significant performance boosts over baseline approaches and can more effectively utilize low-resource data. Sepcifically, when compared to the traditional machine reading comprehension based method BERT-MRC,\nAvg represents micro-average F1-score of all entity\ntypes and * represents entity types with a limited amount\nof labeled data.\nour method surpasses it significantly especially in cases where K is small. For instance, the F1 score of our model in a 10-shot setting is either higher or comparable to the F1 score of BERT-MRC in a 50-shot setting across all three datasets, suggesting that our method is more suitable for low-resource NER setting, particularly in the machine reading comprehension NER paradigm."
        },
        {
            "heading": "3.4 In-domain Low-resource NER",
            "text": "We conduct experiments for in-domain lowresource settings on the CoNLL03 dataset, where only a limited amount of labeled data is available for some entity types. Following (Cui et al., 2021), we downsample 4,001 training instances, including 3,763 \"ORG\", 2,496 \"PER\", 50 \"LOC\", and 50 \"MISC\".\nThe evaluation results are presented in Table 3. We observe significant performance boosts on resource-rich entity type \"PER\" and \"ORG\", as well as resource-limited entity type \"LOC\". The performance gain on the entity type \"MISC\" is relatively smaller, which may due to the fact that the entity type name \"miscellaneous entity\" is more semantic ambiguous and difficult to be represented by the query compared to other entity types. Our method achieves +10.91% F1-score improvement on average compared to baseline methods, which\ntraining-from-scratch and cross-domain low-resource settings. The \"qs-attention1\" and \"qs-attention2\" denote two\nvariation models in query-segmented attention ablation study described in 3.7.\ndemonstrates it is more suitable for in-domain transfer and handling new emerging entities."
        },
        {
            "heading": "3.5 Cross-domain Low-resource NER",
            "text": "We explore the cross-domain low-resource NER settings in which models are initially trained in a resource-rich source domain and then fine-tuned and evaluated in a target domain with limited labeled data. In order to evaluate the models on different target domains with domain specific entity types, the CrossNER dataset is used as it is specifically designed for this task. It uses the CoNLL03 as the source domain and covers five distinct target domains, namely music, literature, artificial intelligence, politics, and natural science. There are 100 training instances for the first three domains and 200 instances for the last two domains. The empirical results in Table 4 indicate our approach surpasses existing state-of-the-art methods in all target domains, suggesting that our model is more suitable for cross-domain knowledge transfer."
        },
        {
            "heading": "3.6 Inference Speed",
            "text": "Table 6 presents the results of our method\u2019s inference speedup in comparison to BERT-MRC on the MIT Movie and ATIS datasets, demonstrating a\nconducted on NVIDIA Tesla V100 Graphics Card with\n32GB graphical memory.\nrespective speedup of 1.32\u00d7 and 2.16\u00d7. The observed acceleration in inference speed is attributed to the query parallel setting employed by QPMRCNER, which enables a single forward pass for all entities, as opposed to BERT-MRC\u2019s requirement of a separate forward pass for each entity. The degree of runtime improvement is contingent upon the number of entity types present in the dataset, with datasets featuring a greater number of entity types exhibiting a greater potential for leveraging the query parallel setting."
        },
        {
            "heading": "3.7 Ablation Studies",
            "text": "We conduct ablation studies to analyze the effects of different components of our model and validate the design decisions. Specifically, four settings are evaluated in the ablation studies: (1) Shared\nPrompt: In our model, a query representing an entity type is composed by combining the shared continuous prompt with the entity type name. We ablate the shared continuous prompt to testify whether it is helpful for entity type representation and knowledge transfer across domains. (2) QuerySegmented Attention: The query-segmented attention used in the query-parallel encoder isolates the queries from each other and allows queries to attend to the context but not vice versa. We verify the effectiveness of query-segmented attention through two variations by relaxing the constraints. The first variation permits the context tokens to attend to the queries, thereby enabling the semantics of queries to interact indirectly through the context. The second one applies vanilla self-attention directly, and the first token of each entity type name is utilized to represent the queries. (3) Query-Conditioned Biaffine Predictor: To evaluate the effectiveness of the query-conditioned biaffine predictor utilized in our model, we conduct an experimental analysis by removing it and only employing word-level classification for named entity recognition. (4) Auxiliary Task: We access the effectiveness of the word level classification auxiliary task by removing it and only use Ls as training object.\nThis experiment is conducted in both trainingfrom-scratch and cross-domain low-resource settings to assess the models\u2019 capacity to utilize small amounts of data and transfer knowledge across domains. The CrossNER music domain data is employed for evaluation. The experimental results, as presented in Table 5, demonstrate that the proposed model exhibits superior performance in the cross-domain settings as compared to the trainingfrom-scratch setting, suggesting its ability to transfer knowledge from the source domain to the target domain, thereby enhancing its overall performance. Notably, QPMRC-NER outperforms its ablation variations in both settings. The experimental results indicate that shared continuous prompting is beneficial in improving query representation quality. Furthermore, the query-segmented attention mechanism is found to be more efficient than other attention mechanisms in our query-parallel MRCbased framework. This may be attributed to its ability to leverage the shared query prompt while avoiding modeling entity type interaction with lowresource data, which can have negative effects, particularly in the cross-domain setting. As evidenced by the experimental results, the query-segmented\nattention mechanism outperforms the vanilla selfattention mechanism by a significant margin of 8.65% in the cross-domain setting. Additionally, the query-conditioned biaffine predictor, designed for the query parallel NER framework, achieves superior performance compared to using word-level classification directly. Also, incorporating wordlevel classification as an auxiliary task can further enhance the model\u2019s performance."
        },
        {
            "heading": "4 Related Work",
            "text": "Although NER is usually formalized as a sequence labeling task (Chiu and Nichols, 2016; Ma and Hovy, 2016; Xia et al., 2019; Devlin et al., 2019), several new NER paradigms have been proposed recently, conceptualizing NER as span classification (Luan et al., 2019; Yu et al., 2020; Li et al., 2021; Shen et al., 2021; Fu et al., 2021a), sequence generation (Strakov\u00e1 et al., 2019; Yan et al., 2021; Tan et al., 2021; Lu et al., 2022), constituency parsing (Fu et al., 2021b; Lou et al., 2022) and machine reading comprehension (Li et al., 2020; Mengge et al., 2020) tasks and achieving impressive results. However, these approaches are mainly focus on standard supervised setting, which is not suitable for low-resource scenarios. Another line of research sought to address the low-resource NER task using techniques such as prototype-based learning (Fritzler et al., 2019; Yang and Katiyar, 2020; Henderson and Vulic\u0301, 2021), template-based learning (Cui et al., 2021) and contrastive learning (Das et al., 2022). But they often fail to fully exploit the potential of pretrained language models (PLMs) and perform inferior to standard sequence labeling NER methods in resource-rich settings. To bridge the gap, we propose a machine reading comprehension based method, which is effective in both resource-rich and resource-limited NER settings."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose a query-parallel machine reading comprehension framework, which predicts all entities simultaneously and is applicable to both resource-rich and resource-limited settings. Specifically, we introduce the query-parallel encoder, which leverages the query-segmented attention mechanism to facilitate more straightforward generalization to new entity types. Additionally, we propose the query-conditioned biaffine predictor, which enables parallel prediction of entities. The\nmodel is trained with parameter-efficient technique for data-efficiency. Extensive experimentation has shown that our approach attains a faster inference speed, exhibits competitive performance against strong baselines in resource-rich setting, and yields state-of-the-art outcomes in low-resource contexts, including training-from-scratch, in-domain transfer, and cross-domain transfer tasks.\nLimitations\nIn this work, we propose a query-parallel machine reading comprehension framework for NER task, which extracts multiple entity types simultaneously and achieve promising results for both resourcerich and resource-limited settings. In this approach, each query is semantically isolated, responsible for giving semantic signals to the pretrained language model and extracting entities of its type. The method is effective in low-resource settings when the entity type name is semantically unambiguous, but encounters difficulties otherwise, such as facing the miscellaneous other-class words. Thus, further research is needed to determine how to address these entity types and improve the performance of the model. Moreover, in QPMRC-NER, the input length restriction imposed by pretrained language models limits the number of parallel processed queries. To handle fine-grained entity extraction with a large number of entity types, segmentation of queries into multiple groups and separate encoding becomes necessary. Another potential approach is using a lightweight model to filter out irrelevant entity types for each sentence, thereby retaining only a small subset of entity type candidates. This avenue of investigation is left for future research."
        },
        {
            "heading": "A Hyper-parameters",
            "text": "The detailed hyper-parameters used in our model are listed in Table 7. For the parameter-efficient tuning module UNIPELT (Mao et al., 2022), we adopt the original hyper-parameter settings."
        },
        {
            "heading": "B Baselines",
            "text": "We use the following methods as baselines: BiLSTM-CRF (Ma and Hovy, 2016) treats NER as a sequence labeling task utilizing BiLSTM and CRF.\nBERT-Tagger (Devlin et al., 2019) treats NER as a sequence labeling task utilizing BERT.\nBERT-MRC (Li et al., 2020) formulates NER as a machine reading comprehension task. It extracts entities from the context based on manually-crafted queries representing different entity types.\nPyramid (Wang et al., 2020) a layered neural model for nested entity recognition, which permits each decoding layer to consider the global information from both the upper and lower layers.\nBARTNER (Yan et al., 2021) formulates NER as an entity span generation task utilizing a sequence-to-sequence model with pointer mechanism.\nTemplateNER (Cui et al., 2021) is a templatebased method which treats NER as a language model ranking problem within a sequence-tosequence framework and is designed for lowresource NER.\nLightNER (Chen et al., 2022) formulates NER as an entity span sequence generation task. It adopts a sequence-to-sequence model with pointer and pluggable prompting mechanism to tackle lowresource NER task.\nPIQN (Shen et al., 2022) treats NER as a span prediction problem by setting up learnable instance queries to extract entities from a sentence simultaneously.\nDoSEA (Tang et al., 2022) treats NER as a machine reading comprehension based framework and is designed for cross-domain NER task. It is able to recognize distinctions that are domain-specific and mitigate the subtype conflicts between domains.\nBoundary Smoothing NER (BS-NER) (Zhu and Li, 2022) treats NER as a span prediction problem and proposes the boundary smoothing regularization technique to boosting the model performance."
        },
        {
            "heading": "C Parameter-Efficient Tuning with",
            "text": "UNIPELT\nThe UNIPELT (Mao et al., 2022) is utilized in our model as the parameter-efficient transfer learning component as illustrated in Figure 4, as it is able to adapt to the data or task setup dynamically. It incorporates three existing parameter-efficient tuning methods including Adapter (Houlsby et al., 2019) , Prefix-tuning (Li and Liang, 2021) and LoRA (Guo et al., 2021; Hu et al., 2021) as sub-modules and controls them via gating mechanism.\nformer layer used in the query-parallel encoder\nAdapter (Houlsby et al., 2019) augments the Transformer layer of the pretrained language model with a trainable bottleneck layer. This layer consists of a pair of down+up projections which reduce and then recover the size of the token hidden states. Mathematically, it could be denoted as:\nHA = W \u22a4 up\u03d5(W \u22a4 downHF ) (13)\nPrefix-tuning (Li and Liang, 2021) enhances the multi-head self-attention in each Transformer layer by adding of a number of trainable vectors to the input, allowing the original tokens to attend to virtual tokens as if they are real.\nLoRA (Guo et al., 2021; Hu et al., 2021) introduces trainable low-rank matrices and utilizes them to update the Query Q and Value V in multi-head self-attention in each Transformer layer, which can be formulated as:\nQ = (W\u22a4Q + \u03b1qW q\u22a4 up W q\u22a4 down)Hin (14)\nV = (W\u22a4V + \u03b1vW v\u22a4 up W v\u22a4 down)Hin (15)\nGating Mechanism The UNIPELT adds a trainable gate Gmi for each sub-module mi \u2208 {Adapter, PrefixTuning, LoRA} in the Transformer layer to achieve fine-grained control over these sub-modules. The gate output would be higher if its corresponding sub-module is more useful for the task.\nSpecifically, for adapter, we feed its direct input HF to a feedforward network to get the gating estimation GA \u2208 (0, 1). Then the output of adapter with gating mechanism HAG would be:\nHAG = GAHA +HF (16)\nSimilarly, for prefix-tuning, gating function GP is applied to the prefix vectors PK and PV . As for LoRA, the hyper-parameter \u03b1 is substituted by the gating function GL."
        }
    ],
    "title": "A Query-Parallel Machine Reading Comprehension Framework for Low-resource NER",
    "year": 2023
}