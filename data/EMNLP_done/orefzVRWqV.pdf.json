{
    "abstractText": "Work on personality detection has tended to incorporate psychological features from different personality models, such as BigFive and MBTI. There are more than 900 psychological features, each of which is helpful for personality detection. However, when used in combination, the application of different calculation standards among these features may result in interference between features calculated using distinct systems, thereby introducing noise and reducing performance. This paper adapts different psychological models in the proposed PsyAttention for personality detection, which can effectively encode psychological features, reducing their number by 85%. In experiments on the BigFive and MBTI models, PysAttention achieved average accuracy of 65.66% and 86.30%, respectively, outperforming state-ofthe-art methods, indicating that it is effective at encoding psychological features.",
    "authors": [
        {
            "affiliations": [],
            "name": "Baohua Zhang"
        },
        {
            "affiliations": [],
            "name": "Yongyi Huang"
        },
        {
            "affiliations": [],
            "name": "Wenyao Cui"
        },
        {
            "affiliations": [],
            "name": "Huaping Zhang"
        },
        {
            "affiliations": [],
            "name": "Jianyun Shang"
        }
    ],
    "id": "SP:05f3296d709b818f59d55285973d8f34c5887bdb",
    "references": [
        {
            "authors": [
                "Nur Haziqah Zainal Abidin",
                "Muhammad Akmal Remli",
                "Noorlin Mohd Ali",
                "Danakorn Nincarean Eh Phon",
                "Nooraini Yusoff",
                "Hasyiya Karimah Adli",
                "Abdelsalam H"
            ],
            "title": "Improving intelligent personality prediction using myers-briggs type",
            "venue": "Busalim",
            "year": 2020
        },
        {
            "authors": [
                "Kazemian-Hassan Amirhosseini",
                "Mohammad Hossein."
            ],
            "title": "Machine learning approach to personality type prediction based on the myers\u2013briggs type indicator\u00ae",
            "venue": "Multimodal Technologies and Interaction, 4(1):9.",
            "year": 2020
        },
        {
            "authors": [
                "Ryan L Boyd",
                "James W Pennebaker"
            ],
            "title": "Language-based personality: a new approach",
            "year": 2017
        },
        {
            "authors": [
                "Gregory J Boyle."
            ],
            "title": "Myers-briggs type indicator (mbti): some psychometric limitations",
            "venue": "Australian Psychologist, 30(1):71\u201374.",
            "year": 1995
        },
        {
            "authors": [
                "Fabio Celli",
                "Bruno Lepri."
            ],
            "title": "Is big five better than mbti? a personality computing challenge using twitter data",
            "venue": "Computational Linguistics CLiC-it 2018, page 93.",
            "year": 2018
        },
        {
            "authors": [
                "Hans Christian",
                "Derwin Suhartono",
                "Andry Chowanda",
                "Kamal Z Zamli."
            ],
            "title": "Text based personality prediction from multiple social media data sources using pre-trained language model and model averaging",
            "venue": "Journal of Big Data, 8(1):1\u201320.",
            "year": 2021
        },
        {
            "authors": [
                "Scott A Crossley",
                "Kristopher Kyle",
                "Danielle S McNamara."
            ],
            "title": "The tool for the automatic analysis of text cohesion (taaco): Automatic assessment of local, global, and text cohesion",
            "venue": "Behavior research methods, 48(4):1227\u20131237.",
            "year": 2016
        },
        {
            "authors": [
                "Scott A Crossley",
                "Kristopher Kyle",
                "Danielle S McNamara."
            ],
            "title": "Sentiment analysis and social cognition engine (seance): An automatic tool for sentiment, social cognition, and social-order analysis",
            "venue": "Behavior research methods, 49(3):803\u2013821.",
            "year": 2017
        },
        {
            "authors": [
                "Scott A Crossley",
                "Stephen Skalicky",
                "Mihai Dascalu."
            ],
            "title": "Moving beyond classic readability formulas: New methods and new models",
            "venue": "Journal of Research in Reading, 42(3-4):541\u2013561.",
            "year": 2019
        },
        {
            "authors": [
                "John M Digman."
            ],
            "title": "Personality structure: Emergence of the five-factor model",
            "venue": "Annual review of psychology, 41(1):417\u2013440.",
            "year": 1990
        },
        {
            "authors": [
                "Jennifer Golbeck",
                "Cristina Robles",
                "Karen Turner."
            ],
            "title": "Predicting personality with social media",
            "venue": "CHI\u201911 extended abstracts on human factors in computing systems, pages 253\u2013262.",
            "year": 2011
        },
        {
            "authors": [
                "Eun-Young Hong."
            ],
            "title": "A review of the korean nursing research literature with mbti personality and nursing students",
            "venue": "Journal of the Korea Convergence Society, 13(1):425\u2013436.",
            "year": 2022
        },
        {
            "authors": [
                "Molly E Ireland",
                "James W Pennebaker."
            ],
            "title": "Language style matching in writing: synchrony in essays, correspondence, and poetry",
            "venue": "Journal of personality and social psychology, 99(3):549.",
            "year": 2010
        },
        {
            "authors": [
                "Amirmohammad Kazameini",
                "Samin Fatehi",
                "Yash Mehta",
                "Sauleh Eetemadi",
                "Erik Cambria."
            ],
            "title": "Personality trait detection using bagged svm over bert word embedding ensembles",
            "venue": "arXiv preprint arXiv:2010.01309.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin Ming-Wei Chang Kenton",
                "Lee Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of NAACL-HLT, pages 4171\u20134186.",
            "year": 2019
        },
        {
            "authors": [
                "Elma Kerz",
                "Yu Qiao",
                "Sourabh Zanwar",
                "Daniel Wiechmann."
            ],
            "title": "Pushing on personality detection from verbal behavior: A transformer meets text contours of psycholinguistic features",
            "venue": "Proceedings of the 12th Workshop on Computational Approaches",
            "year": 2022
        },
        {
            "authors": [
                "Ali Nawaz Khan",
                "Xiongfei Cao",
                "Abdul Hameed Pitafi."
            ],
            "title": "Personality traits as predictor of mpayment systems: a sem-neural networks approach",
            "venue": "Journal of Organizational and End User Computing (JOEUC), 31(4):89\u2013110.",
            "year": 2019
        },
        {
            "authors": [
                "Kristopher Kyle",
                "Scott A Crossley."
            ],
            "title": "Automatically assessing lexical sophistication: Indices, tools, findings, and application",
            "venue": "Tesol Quarterly, 49(4):757\u2013 786.",
            "year": 2015
        },
        {
            "authors": [
                "Charles Li",
                "Monte Hancock",
                "Ben Bowles",
                "Olivia Hancock",
                "Lesley Perg",
                "Payton Brown",
                "Asher Burrell",
                "Gianella Frank",
                "Frankie Stiers",
                "Shana Marshall"
            ],
            "title": "Feature extraction from social media posts for psychometric typing of participants",
            "year": 2018
        },
        {
            "authors": [
                "Navonil Majumder",
                "Soujanya Poria",
                "Alexander Gelbukh",
                "Erik Cambria."
            ],
            "title": "Deep learning-based document modeling for personality detection from text",
            "venue": "IEEE Intelligent Systems, 32(2):74\u201379.",
            "year": 2017
        },
        {
            "authors": [
                "Str\u00f6bel Marcus",
                "Elma Kerz",
                "Daniel Wiechmann",
                "Stella Neumann."
            ],
            "title": "Cocogen-complexity contour generator: Automatic assessment of linguistic complexity using a sliding-window technique",
            "venue": "Proceedings of the Workshop on Computational Lin-",
            "year": 2016
        },
        {
            "authors": [
                "Gerald Matthews",
                "Ian J Deary",
                "Martha C Whiteman."
            ],
            "title": "Personality traits",
            "venue": "Cambridge University Press.",
            "year": 2003
        },
        {
            "authors": [
                "Robert R McCrae",
                "Paul T Costa Jr."
            ],
            "title": "More reasons to adopt the five-factor model",
            "year": 1989
        },
        {
            "authors": [
                "Robert R McCrae",
                "Oliver P John."
            ],
            "title": "An introduction to the five-factor model and its applications",
            "venue": "Journal of personality, 60(2):175\u2013215.",
            "year": 1992
        },
        {
            "authors": [
                "Yash Mehta",
                "Samin Fatehi",
                "Amirmohammad Kazameini",
                "Clemens Stachl",
                "Erik Cambria",
                "Sauleh Eetemadi."
            ],
            "title": "Bottom-up and top-down: Predicting personality with psycholinguistic and language model features",
            "venue": "2020 IEEE International Conference on",
            "year": 2020
        },
        {
            "authors": [
                "Yash Mehta",
                "Navonil Majumder",
                "Alexander Gelbukh",
                "Erik Cambria."
            ],
            "title": "Recent trends in deep learning based personality detection",
            "venue": "Artificial Intelligence Review, 53(4):2313\u20132339.",
            "year": 2020
        },
        {
            "authors": [
                "Saif M Mohammad",
                "Peter D Turney."
            ],
            "title": "Nrc emotion lexicon",
            "venue": "National Research Council, Canada, 2:234.",
            "year": 2013
        },
        {
            "authors": [
                "Yair Neuman",
                "Vladyslav Kozhukhov",
                "Dan Vilenchik"
            ],
            "title": "Data augmentation for modeling human personality: The dexter machine",
            "venue": "arXiv preprint arXiv:2301.08606",
            "year": 2023
        },
        {
            "authors": [
                "Kulsum Akter Nisha",
                "Umme Kulsum",
                "Saifur Rahman",
                "Md Hossain",
                "Partha Chakraborty",
                "Tanupriya Choudhury"
            ],
            "title": "A comparative analysis of machine learning approaches in personality prediction using mbti",
            "year": 2022
        },
        {
            "authors": [
                "Sakdipat Ontoum",
                "Jonathan H Chan."
            ],
            "title": "Personality type based on myers-briggs type indicator with text posting style by using traditional and deep learning",
            "venue": "arXiv preprint arXiv:2201.08717.",
            "year": 2022
        },
        {
            "authors": [
                "Gregory Park",
                "H Andrew Schwartz",
                "Johannes C Eichstaedt",
                "Margaret L Kern",
                "Michal Kosinski",
                "David J Stillwell",
                "Lyle H Ungar",
                "Martin EP Seligman."
            ],
            "title": "Automatic personality assessment through social media language",
            "venue": "Journal of personality and",
            "year": 2015
        },
        {
            "authors": [
                "James W Pennebaker",
                "Martha E Francis",
                "Roger J Booth."
            ],
            "title": "Linguistic inquiry and word count: Liwc 2001",
            "venue": "Mahway: Lawrence Erlbaum Associates, 71(2001):2001.",
            "year": 2001
        },
        {
            "authors": [
                "James W Pennebaker",
                "Laura A King."
            ],
            "title": "Linguistic styles: language use as an individual difference",
            "venue": "Journal of personality and social psychology, 77(6):1296.",
            "year": 1999
        },
        {
            "authors": [
                "Lawrence A Pervin",
                "Oliver P John."
            ],
            "title": "Handbook of personality",
            "venue": "Theory and research, 2.",
            "year": 1999
        },
        {
            "authors": [
                "Soujanya Poria",
                "Alexandar Gelbukh",
                "Basant Agarwal",
                "Erik Cambria",
                "Newton Howard."
            ],
            "title": "Common sense knowledge based personality recognition from text",
            "venue": "Mexican International Conference on Artificial Intelligence, pages 484\u2013496. Springer.",
            "year": 2013
        },
        {
            "authors": [
                "Moattar",
                "Taymaz Akan."
            ],
            "title": "Automatic personality prediction: an enhanced method using ensemble modeling",
            "venue": "Neural Computing and Applications, pages 1\u201321.",
            "year": 2022
        },
        {
            "authors": [
                "David P Schmitt",
                "Anu Realo",
                "Martin Voracek",
                "J\u00fcri Allik."
            ],
            "title": "Why can\u2019t a man be more like a woman? sex differences in big five personality traits across 55 cultures",
            "venue": "Journal of personality and social psychology, 94(1):168.",
            "year": 2008
        },
        {
            "authors": [
                "Xiangguo Sun",
                "Bo Liu",
                "Jiuxin Cao",
                "Junzhou Luo",
                "Xiaojun Shen."
            ],
            "title": "Who am i? personality detection based on deep learning for texts",
            "venue": "2018 IEEE international conference on communications (ICC), pages 1\u20136. IEEE.",
            "year": 2018
        },
        {
            "authors": [
                "Yla R Tausczik",
                "James W Pennebaker."
            ],
            "title": "The psychological meaning of words: Liwc and computerized text analysis methods",
            "venue": "Journal of language and social psychology, 29(1):24\u201354.",
            "year": 2010
        },
        {
            "authors": [
                "Daniel Wiechmann",
                "Yu Qiao",
                "Elma Kerz",
                "Justus Mattern."
            ],
            "title": "Measuring the impact of (psycho-) linguistic and readability features and their spill over effects on the prediction of eye movement patterns",
            "venue": "arXiv preprint arXiv:2203.08085.",
            "year": 2022
        },
        {
            "authors": [
                "Di Xue",
                "Lifa Wu",
                "Zheng Hong",
                "Shize Guo",
                "Liang Gao",
                "Zhiyong Wu",
                "Xiaofeng Zhong",
                "Jianshan Sun."
            ],
            "title": "Deep learning-based personality recognition from text posts of online social networks",
            "venue": "Applied Intelligence, 48(11):4232\u20134246.",
            "year": 2018
        },
        {
            "authors": [
                "Jianguo Yu",
                "Konstantin Markov."
            ],
            "title": "Deep learning based personality recognition from facebook status updates",
            "venue": "2017 IEEE 8th international conference on awareness science and technology (iCAST), pages 383\u2013387. IEEE.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Personality detection helps people to manage themselves and understand themselves, and has been involved in job screening, personal assistants, recommendation systems, specialized health care, counseling psychotherapy, and political forecasting (Mehta et al., 2020b). Personality measures originated from psychology (Pervin and John, 1999; Matthews et al., 2003), and psychologists judge their patients\u2019 personality features through a series of tests.\nThe content that users post on social media can give valuable insights into their personalities (Christian et al., 2021). Personality detection can identify personality features from this, and often requires a combination of psychology, linguistics, and computer science.\nThere are many psychological models of individual personality, the most famous being the BigFive (McCrae and Costa Jr, 1989; McCrae and\n\u2217*Corresponding author\nJohn, 1992; Schmitt et al., 2008) and MBTI (Boyle, 1995; Hong, 2022; Celli and Lepri, 2018). The BigFive model divides personality features into five factors(Digman, 1990): Openness (OPN), Conscientiousness (CON), Extraversion (EXT), Agreeableness (AGR), and Neuroticism (NEU). These are usually assessed through questionnaires in which people reflect on their typical patterns of thinking and behavior. The MBTI model describes personality by 16 types that result from the combination of binary categories in four dimensions1: (1) Extraversion (E) vs Introversion (I); (2) Thinking (T) vs Feeling (F); (3) Sensing (S) vs Intuition (N); and (4) Judging (J) vs Perceiving (P).\nPrevious work saw a person\u2019s use of language as a distillation of underlying drives, emotions, and thought patterns (Tausczik and Pennebaker, 2010; Boyd and Pennebaker, 2017). There are many theories and tools for extracting psychological features from texts, such as ARTE (Automatic Readability Tool For English) (Crossley et al., 2019), which has 55 psychological features); SEANCE (Sentiment Analysis And Cognition Engine) (Crossley et al., 2017), with 271 features; TAACO (Tool For The Automatic Analysis Of Cohesion) (Crossley et al., 2016), with 168 features; and TAALES (Tool For The Automatic Analysis Of Lexical Sophistication (Kyle and Crossley, 2015), with 491 features. These tools use multiple theories to obtain psychological features from text. For example, SEANCE uses eight theories, TAACO use fifteen theories. However, the application of different calculation standards among these features may result in interference between features calculated using distinct systems, thereby introducing noise and reducing performance. For example, when analyzing sentiment or emotion features, SEANCE uses more than 10 lexicons. A word\u2019s meaning may vary according to the lexicon, which leads to inconsistencies\n1https://www.simplypsychology.org/the-myers-briggstype-indicator.html\nbetween features, and thus increases noise. In the field of Natural Language Processing (NLP), personality detection aims to identify the personality features of individuals from their use of language. Work can be divided into two categories. One uses psychological features obtained by some statistical methods and uses machine learning tools such as Support Vector Machine (SVM) and KNearest Neighbor (KNN) for classification. Others take it as a text classification task, and directly input text to a neural network for classification. Work seldom combines psychological features with text embedding by treating them as sequences. However, because psychology and computer science are very different fields, psychologists often design features without considering how they will be introduced into a neural network model. NLP researchers ignore relations between those features, and focus on how to extract and encode more features, ignoring that this will introduce noise to the model, resulting in poor performance.\nIn this paper, we propose PsyAttention for personality detection, which can effectively encode psychological features and reduce their number by 85%. The main contributions of this paper are as follows.\n\u2022 We propose PsyAttention to adapt different psychological models. Experimental results on two personality detection models demonstrate that the method can improve accuracy;\n\u2022 We demonstrate that incorporating features designed by psychologists into neural networks is necessary, as pre-trained models fail to extract all of the psychological features present in the text;\n\u2022 We select some important features in personality detection task. To the best of our knowledge, this is the first method to integrate, analyze, and reduce the number of psychological features for personality detection."
        },
        {
            "heading": "2 Related work",
            "text": "Research on individual personality detection focuses on how to extract more appropriate psychological features from texts and devise classification models. We introduce some methods below.\nMethods to obtain personality features from text data generally use text analysis tools such as LIWC(Pennebaker et al., 2001) and\nNRC(Mohammad and Turney, 2013) to extract appropriate features, which are fed into standard machine learning classifiers such as SVM and Na\u00efve Bayes (NB) (Mehta et al., 2020a) for classification. Abidin et al. (2020) used logistic regression (LR), KNN, and random forest, and found that random forest has the best performance. With the deeper study of psychology, language use has been linked to a wide range of psychological correlates (Park et al., 2015; Ireland and Pennebaker, 2010). Pennebaker and King (1999) proposed that writing style (e.g., frequency of word use) has correlations with personality. Khan et al. (2019) thought the social behavior of the user and grammatical information have a relation to the user\u2019s personality. It was found that combining common-sense knowledge with psycho-linguistic features resulted in a remarkable improvement in accuracy (Poria et al., 2013). Golbeck et al. (2011) proposed 74 psychological features; Nisha et al. (2022) extracted features like age, education level, nationality, first language, country, and religion, and found that XGBoost performed better than SVM and Na\u00efve Bayes.\nMethods based on deep-learning mainly regard personality detection as a classification task. Majumder et al. (2017) proposed to use CNN and Word2Vec embeddings for personality detection from text; since a document is too long, they proposed dividing the data into two parts and extracting the text features separately using two identical CNN models separately. Yu and Markov (2017) experimented with n-grams (extracted with a CNN) and bidirectional RNNs (forward and backward GRUs). Instead of using the pretrained Word2Vec model, they trained the word embedding matrix using the skip-gram method to incorporate internet slang, emoticons, and acronyms. Sun et al. (2018) used bidirectional LSTMs (BiLSTMs), concatenated with a CNN, and combined abstract features with posts to detect a user\u2019s personality. Ontoum and Chan (2022) proposed an RCNN model with three CNN layers and one BiLSTM layer. Xue et al. (2018) proposed an RCNN text encoder with an attention mechanism, inputting the text to a GRU and performing an attention calculation to make full use of the text features, then using GRU encodings to input the text in normal and reverse order. Similar to Xue et al. (2018), Sun et al. (2018) used an RNN and CNN, divided the text into segments, input them separately to LSTM, and input the merged re-\nsults to the CNN to extract features. Some work has combined psychological features with deep learning models. Wiechmann et al. (2022) combined human reading behavior features with pretrained models. Kerz et al. (2022) used BiLSTM to encode 437 proposed features. Neuman et al. (2023) proposed an data augmentation methods for modeling human personality.\nHowever, as the number of psychological features designed by psychologists increases, the models used to encode them are not improving, and remain a dense layer and BiLSTM. No research has focused on the effective use of psychological features. All models take them as a sequence, ignoring that they are obtained by a statistical approach. To better use psychological features, we propose Pysattention, along with a feature optimization method that can reduce the noise introduced by too many features."
        },
        {
            "heading": "3 Model",
            "text": "We introduce PsyAttention, whose model is shown in Figure 1, to effectively integrate text and psychological features. An attention-based encoder encodes numerical psychological features, and BERT is fine-tuned with psychological features obtained by tools to extract more personality information."
        },
        {
            "heading": "3.1 Psychological Feature Optimization",
            "text": "Kerz et al. (2022) proposed the most psychological features in the task of personality detection, and got them by automated text analysis systems such as CoCoGen(Marcus et al., 2016). To detect sentiment, emotion, and/or effect, they designed: (1) features of morpho-syntactic complexity; (2) features of lexical richness, diversity, and sophistication; (3) readability features; and (4) lexicon features. These groups of features can be divided into those of expressiveness and emotions contained in the text. To obtain more features, we choose three tools to analyze input texts: (1) SEANCE (Crossley et al., 2017) can obtain the features of sentiment, emotion, and/or effect; (2) TAACO (Crossley et al., 2016) can obtain features of cohesion; and (3) TAALES (Kyle and Crossley, 2015) can obtain features of expressive ability. Table1 shows the numbers of features used for each tool.\nAs mentioned above, too many features can introduce noise. Hence they should be used effectively. We calculate the Pearson coefficient between each feature and other features as a corre-\nThird and fourth columns: number of features used in MBTI\nand BigFive Essays datasets, respectively.\nlation indicator. We set a threshold, and features with Pearson coefficients above this value are considered more relevant, and should be removed. To reduce the number of psychological features used, the feature with the largest correlation coefficients with other features was chosen to represent each group of correlated features. We leave fewer than 15% of those features, as shown in Table 1, and discussed in the Appendix."
        },
        {
            "heading": "3.2 Psychological Features Encoder",
            "text": "After we obtain and optimize numerical features, they must be encoded and merged with text embedding. How to encode these is a research challenge. Some researchers encode numerical sequences directly into the network by fully connected neural networks, but this is not effective because numeric sequences are too sparse for neural networks. Kerz et al. (2022) used BiLSTM as the encoder, but the numerical sequences are not temporally correlated. All of the psychological features are designed manually by psychologists. The 437 features used by Kerz et al. (2022) are discrete values and the order between them does not affect the result. In other words, feature extraction tools for psychological features, such as TAACO and TAALES, tend to prioritize the extraction of certain categories of features, such as emotional words followed by punctuation usage or others, when extracting features. These tools, which are designed artificially, do not take into account the order of the features, resulting in features that are disordered. Thus, we propose to encode them with a transformer based encoder. Since numerical sequences are not temporally correlated, we remove the position encoding. Taking W = (w0, w1, w2, . . . , wm) as the numerical features of psychology, and Fencoder as the final output of the encoder, the process can be defined as follows:\nHmutli_att = Mutil_att(W ) (1)\nHLNorm1 = LayerNorm(Hmutli_att +W ) (2)\nHffn = FeedForward(HLNorm1) (3)\nFencoder = LayerNorm(Hffn+HLNorm1) (4)\nwhere Mutil_att, LayerNorm, and FeedForward denote multi-headed attention, the layer of Add & Norm, and the Feed-Forward Network, respectively."
        },
        {
            "heading": "3.3 Fine-tuning BERT",
            "text": "We use BERT (Kenton and Toutanova, 2019) for text feature extraction. The pretrained model has proven its superior performance in most natural language processing tasks. Taking T = (x0, \u00b7, xm) as the input text, we only use [CLS] as the text embedding. As [CLS] is the first token of the BERT output, the process can be defined as H = h1 \u00b7 \u00b7 \u00b7hm = BERT (x1 \u00b7 \u00b7 \u00b7xn)[0].\nThen, we employ a dense layer to obtain a vector that has the same dimensional as the psychological feature p1 \u00b7 \u00b7 \u00b7 pl .\nf1 \u00b7 \u00b7 \u00b7 fl = Dense(H) (5)\nThe loss function is shown in formula 6. We fine-tune BERT to make the vector of CLS more similar to the psychological feature vector.\nloss = 1\u2212cos_sim({f1 \u00b7 \u00b7 \u00b7 fl} , {p1 \u00b7 \u00b7 \u00b7 pl}) (6)\nIt is noteworthy that during the training process, we first conduct training for this step. After finetuning BERT, we then fix the current weights and use them to extract textual features."
        },
        {
            "heading": "3.4 Personality detection Using PsyAttention",
            "text": "model\nWe design an embedding fusion layer to control the information that can be used in the final representation. We use a dense layer to calculate the weight, taking FPSY and FBERT to indicate the embedding of the psychological encoder and BERT, respectively. We can obtain the weights as follow formulas.\nWPSY = Dense(FPSY ) WBERT = Dense(FBERT ) (7)\nDue to the fact that the extractable psychological and textual features are determined by the original text, we employ a dynamic weighting scheme to integrate the psychological and textual features. We can obtain the final embedding by following formula.\nFinalemb = FPSY \u00b7WPSY |FBERT \u00b7WBERT (8)\nwhere (\u00b7 | \u00b7) is the concatenation operator. After we get the final representation, we input them to a fully connected neural network for classification. Taking P as the final result, the process can be defined as\nP = Classifier(Finalemb) (9)\nIt is worth noting that the final loss function of our model is the cross-entropy, where formula (6) is the loss function for the Fine-tune Bert component only."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "We conducted experiments on two widely used personality benchmark datasets: BigFive Essay dataset(Pennebaker and King, 1999) and MBTI Kaggle dataset (Li et al., 2018). The BigFive Essay dataset consists of 2468 essays written by students and annotated with binary labels of the BigFive personality features, which were obtained through a standardized self-reporting questionnaire. The average text length is 672 words, and the dataset contains approximately 1.6 million words. The MBTI dataset consists of samples of social media interactions from 8675 users, all of whom have indicated their MBTI type. The average text length is 1,288 words. The dataset consists of approximately 11.2 million words. The detailed statistics are shown in Table 2 and Table 3.\nTo retain the psychological features contained in the text, we perform no pre-processing on the text when extracting psychological features using text analysis tools, and we remove noise such as concatenation, multilingual punctuation, and multiple spaces before input to the BERT model."
        },
        {
            "heading": "4.2 Parameter Settings",
            "text": "We utilized Python 3.8.1, PyTorch 1.13.0, Transformers 4.24.0, and scikit-learn 1.1.3 for implementing our model. Our training process involved 4 NVIDIA RTX 3090 GPUs. We trained our model with 50 epochs. We have employed a split ratio of 7:2:1 for training, validation, and testing respectively.\nWe researched the number of psychological encoders that can obtain the best result, used even numbers from 2 to 12 to test the results, and found that the model obtained the best result with 8 en-\ncoders. The results are shown in Figures 2 and 3.\nTaking BERT-base as the text encoder model, we used four attention head layers, eight psychological feature encoder layers, a learning rate of 2e-5, and the Adam optimizer. The maximum text length was 510, the length of psychological features was 108, and other parameters were the same as for BERT-base."
        },
        {
            "heading": "4.3 Baselines and Results",
            "text": "We took the BERT+PSYLING Ensemble model of Kerz et al. (2022) as our baseline , who used 437 text contours of psycholinguistic features, with BERT as the text encoder. While we selected 108 psychological features and designed a psychological feature encoder based on an attention module, which we think is more effective. We also com-\npared our model with other models. Ramezani et al. (2022) proposed a Hierarchical Attention Network (HAN) combined with base method predictions to carry out stacking in document-level classification. They combined five basic model predictions with the text features, and input them to a dense layer to get the final prediction. Mehta et al. (2020a) also used psychological features with BERT, and experimented with logistic regression, SVM, a multilayer perceptron (MLP) with 50 hidden units, and machine learning models such as those of Amirhosseini (2020), Majumder et al. (2017), and Kazameini et al. (2020).\nWe conducted experiments on the BigFive Essays and Kaggle MBTI Kaggle datasets. The results are shown in Table 4. The PsyAttention listed in Table 4 is our model. Majumder et al. (2017),Kazameini et al. (2020) and Amirhosseini (2020) represent the results of the machine learning models. The others represent the results of the deep learning models. As we can see, the average accuracy of machine learning is lower than that of the deep learning model, which indicated that machine learing models are not as effective as deep learning models. Kerz et al. (2022)(single) is the model that named BERT+ATTN-PSYLING (FT) in Kerz et al. (2022)\u2019s work. Since we did not employ the stacking approach in our model, we only compared our model with Kerz et al. (2022)(single). Both PsyAttention and Kerz et al. (2022)(single) use deep learning models and psychological features. As we can see from Table 4, the accuracy of PsyAttention is 2% higher than Kerz et al. (2022)(single) on the BigFive datasets and 0.3% higher on the MBTI dataset, which proves that our model is better than Kerz et al. (2022)(single). The\narchitecture of BERT+BiLSTM is same as that of the of Kerz et al. (2022) (single), the difference is that we use all 930 psychological features while Kerz et al. (2022) (single) only use 437. The accuracy of BERT+BiLSTM is lower than that of Kerz et al. (2022)(single), which proves that more features will cause more noise."
        },
        {
            "heading": "4.4 Ablation Experiments",
            "text": "We performed ablation experiments, and the results of which are shown in Table 5. \u2019-psyencoder\u2019 indicates that we do not use the Psychological Encoder, \u2019-BERT\u2019 indicates that we do not use the Fine-tune BERT, \u2019-weight\u2019 indicates that we do not use the embedding fusion weight, \u2019\u2212BERTpsy\u2019 indicates that we do not use psychological features in Fine-tune BERT, and \u2019-Features_Selection \u2019 indicates that we use all 930 psychological features. As we can see, the results for \u2019-BERT\u2019 are reduced the most, indicating that the BERT coding component is the most important, and the results for \u2019-psyencoder\u2019 are also reduced, indicating that psychological features also have a significant effect on the results. The results of \u2019-Features_Selection \u2019 indicate that there is some noise in all features, and it is right to do the feature selection."
        },
        {
            "heading": "4.5 Psychological Features In Pretrained Models",
            "text": "We designed a simple experiment to verify whether text embedding includes psychological information. Consider that almost every model in NLP tasks takes word embedding as input and uses some neural networks to extract features, which are input to a discriminant model or generative model to get the final representation. If there is psychological information in the text embedding, we can extract\nit by some networks. Hence we designed a model to extract psychological features from text embedding, used BERT to obtain the text embedding, and a dense layer to extract psychological features. The objective was to make the vector of CLS more similar to the psychological feature vector. We calculated the cosine similarity between the features extracted from the text embedding and those calculated by the psychology tool, and took it as the loss function and evaluation metric of the results. The experimental results are shown in Table 6.\nWe used a BERT-base model. Table 6 shows the results of two encoders, where \"BERT\" indicates the direct use of BERT-base to obtain the text embedding, without fine-tuning on psychological datasets; BERT&Fine-tune is obtained by finetuning BERT-base on those two datasets. During training, we fixed the weight of BERT&Fine-tune but did not fix the weight of BERT. As we can see, even if we fine-tune BERT-base, we can only get a cosine similarity of less than 0.7, which indicates that there is little psychological information in text embedding, and we need to introduce it in the model."
        },
        {
            "heading": "4.6 The Weights of Psychological Features",
            "text": "After removing a substantial number of features, we sought to investigate the remaining features\u2019 impact on the psychological discrimination task. To accomplish this, we introduced an attention mechanism to calculate the attention scores of the psychological features and explore the importance of different features. Because psychological features represented by MBTI and BigFive differ, we separately calculated the attention scores for their\ncorresponding psychological features. After normalization, we plotted the feature distribution of the psychological features contained in both MBTI and BigFive, showed in Figure 4.\nTable 7 displays the features that scored above 0.6 after separate normalization on the two datasets. \"AWL_Sublist_9_Normed\" indicates the number of words in the text that belong to the Academic Word List Sublist 9. \"Powends_Lasswell\" and \"Powdoct_Lasswell\" both represent words related to dominance, respect, money, and power. The former includes words such as \"ambition,\" \"ambitious,\" \"arrangement,\" \"bring,\" and \"challenge,\" while the latter includes words like \"bourgeois,\" \"bourgeoisie,\" \"capitalism,\" \"civil,\" and \"collectivism.\"\nIt can be observed that the aforementioned features can be categorized into three groups:\npersonal emotion-related, personal realizationrelated, and word usage habit-related. The primary personal emotion-related features are Wlbgain_Lasswell, Shame_GALC, Sadness_GALC, Pride_GALC, Guilt_GALC, and Humility_GALC, which encompass both positive and negative emotions experienced by individuals. The personal realization-related features mainly consist of Powends_Lasswell and Powdoct_Lasswell, which are indicative of self-actualization and personal development. Lastly, the word usage habit feature, AWL_Sublist_9_Normed, relates to the frequency of academic words used by individuals and is believed to reflect their level of rigor. The proportion of words in the personal emotion category is over 66%, indicating that emotion is a critical characteristic in personal character discrimination. The proportion of words in the self-realization category is over 20%, reflecting the importance of self-actualization in personal character. The word usage habit feature is believed to be mainly related to whether an individual is rigorous or not, as casual individuals tend to use fewer academic words.\nIt should be noted that not all emotions are included in the personal emotion category, such as Loneliness, Envy, and Anxiety. This may be due to the fact that these emotions are often generated during interpersonal interactions, and therefore may not be as critical for personality detection. Furthermore, it should be noted that emotions represent only a portion of the content, and other self-actualization-related characteristics may be even more significant. Personal realizationrelated features, such as Powends_Lasswell and Powdoct_Lasswell, are indicative of an individual\u2019s self-actualization and personal development, which are critical components of personal character."
        },
        {
            "heading": "5 Conclusion",
            "text": "We proposed PysAttention for personality detection, which can effectively encode psychological features, and reduce their number by 85%. We categorized the psychological features contained in the text as either expressiveness or emotions, and extracted the relative features using three text analysis tools. We demonstrated that the pre-trained model contained only a small amount of psychological information, and proposed a method of psychological feature selection. Unlike models that simply encode a long series of numerical psychological features through BiLSTM, we efficiently encode psychological features with a transformer-like encoder. We also investigate the impact of different psychological features on the results and employe an attention mechanism to identify the more significant features. In experiments on two classical psychological assessment datasets, the proposed model outperformed state-of-the-art algorithms in terms of accuracy, demonstrating the effectiveness of PysAttention."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by Fundamental Strengthening Program Technology Field Fund, China (2022-JCJQ-JJ-0930)."
        },
        {
            "heading": "6 Appendix",
            "text": ""
        },
        {
            "heading": "6.1 The correlation coefficients for all psychological features",
            "text": "We introduce the psychological feature optimization method. We calculated correlation coefficients for all psychological features on both datasets, and plotted the heatmaps shown in Figure 7. The many light-colored blocks in the diagram indicate a large number of highly relevant features. We grouped features with correlation coefficients greater than 0.2, selected those with the highest number of correlated features from each group as representative,\nand did not use the rest. Table 8 shows some features with correlation coefficients; after processing feature selection, we only use Joy and Disgust.\nWe show the features that we used in Table 9 and Table 10. Figure 6 shows the heatmaps of correlation coefficients for the features we used in two datasets. There are no light-colored blocks, which indicates that these features have low relevance."
        },
        {
            "heading": "6.2 The Attention Score of Features",
            "text": "Figure 7 is the importance score on the BigFive and MBTI Kaggle datasets after summing and normalizing the attention scores obtained by inferring all the test data. Figure 8 is the bigger version of Figure 4.\nThe objective of this study is to investigate the relative importance of different features using attention scores. By analyzing the attention scores assigned to each feature, we aim to identify the most influential characteristics in personality detection."
        }
    ],
    "title": "PsyAttention: Psychological Attention Model for Personality Detection",
    "year": 2023
}