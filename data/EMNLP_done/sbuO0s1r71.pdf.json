{
    "abstractText": "Text-to-SQL benchmarks play a crucial role in evaluating the progress made in the field and the ranking of different models. However, accurately matching a model-generated SQL query to a reference SQL query in a benchmark fails for various reasons, such as underspecified natural language queries, inherent assumptions in both model-generated and reference queries, and the non-deterministic nature of SQL output under certain conditions. In this paper, we conduct an extensive study of several prominent cross-domain text-to-SQL benchmarks and reevaluate some of the top-performing models within these benchmarks, by both manually evaluating the SQL queries and rewriting them in equivalent expressions. Our evaluation reveals that attaining a perfect performance on these benchmarks is unfeasible due to the multiple interpretations that can be derived from the provided samples. Furthermore, we find that the true performance of the models is underestimated and their relative performance changes after a re-evaluation. Most notably, our evaluation reveals a surprising discovery: a recent GPT4-based model surpasses the gold standard reference queries in the Spider benchmark in our human evaluation. This finding highlights the importance of interpreting benchmark evaluations cautiously, while also acknowledging the critical role of additional independent evaluations in driving advancements in the field.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mohammadreza Pourreza"
        },
        {
            "affiliations": [],
            "name": "Davood Rafiei"
        }
    ],
    "id": "SP:045d13cdfa3238a3af289712cb85516787c9f61e",
    "references": [
        {
            "authors": [
                "Su Zhu",
                "Kai Yu"
            ],
            "title": "Lgesql: line graph",
            "year": 2021
        },
        {
            "authors": [
                "Martin Davis."
            ],
            "title": "The undecidable: Basic papers on undecidable propositions, unsolvable problems and computable functions",
            "venue": "Courier Corporation.",
            "year": 2004
        },
        {
            "authors": [
                "Yujian Gan",
                "Xinyun Chen",
                "Qiuping Huang",
                "Matthew Purver",
                "John R Woodward",
                "Jinxia Xie",
                "Pengsheng Huang."
            ],
            "title": "Towards robustness of textto-sql models against synonym substitution",
            "venue": "arXiv preprint arXiv:2106.01065.",
            "year": 2021
        },
        {
            "authors": [
                "Yujian Gan",
                "Xinyun Chen",
                "Matthew Purver."
            ],
            "title": "Exploring underexplored limitations of crossdomain text-to-sql generalization",
            "venue": "arXiv preprint arXiv:2109.05157.",
            "year": 2021
        },
        {
            "authors": [
                "Wenqiang Lei",
                "Weixin Wang",
                "Zhixin Ma",
                "Tian Gan",
                "Wei Lu",
                "Min-Yen Kan",
                "Tat-Seng Chua."
            ],
            "title": "Reexamining the role of schema linking in text-to-sql",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Jinyang Li",
                "Binyuan Hui",
                "Reynold Cheng",
                "Bowen Qin",
                "Chenhao Ma",
                "Nan Huo",
                "Fei Huang",
                "Wenyu Du",
                "Luo Si",
                "Yongbin Li."
            ],
            "title": "Graphix-t5: Mixing pretrained transformers with graph-aware layers for textto-sql parsing",
            "venue": "arXiv preprint arXiv:2301.07507.",
            "year": 2023
        },
        {
            "authors": [
                "Jinyang Li",
                "Binyuan Hui",
                "Ge Qu",
                "Binhua Li",
                "Jiaxi Yang",
                "Bowen Li",
                "Bailin Wang",
                "Bowen Qin",
                "Rongyu Cao",
                "Ruiying Geng"
            ],
            "title": "2023b. Can llm already serve as a database interface? a big bench for largescale database grounded text-to-sqls",
            "year": 2023
        },
        {
            "authors": [
                "Xi Victoria Lin",
                "Richard Socher",
                "Caiming Xiong."
            ],
            "title": "Bridging textual and tabular data for crossdomain text-to-sql semantic parsing",
            "venue": "arXiv preprint arXiv:2012.12627.",
            "year": 2020
        },
        {
            "authors": [
                "Aiwei Liu",
                "Xuming Hu",
                "Lijie Wen",
                "Philip S Yu."
            ],
            "title": "A comprehensive evaluation of chatgpt\u2019s zero-shot text-to-sql capability",
            "venue": "arXiv preprint arXiv:2303.13547.",
            "year": 2023
        },
        {
            "authors": [
                "Mohammadreza Pourreza",
                "Davood Rafiei."
            ],
            "title": "Din-sql: Decomposed in-context learning of text-to-sql with self-correction",
            "venue": "arXiv preprint arXiv:2304.11015.",
            "year": 2023
        },
        {
            "authors": [
                "Nitarshan Rajkumar",
                "Raymond Li",
                "Dzmitry Bahdanau."
            ],
            "title": "Evaluating the text-to-sql capabilities of large language models",
            "venue": "arXiv preprint arXiv:2204.00498.",
            "year": 2022
        },
        {
            "authors": [
                "Torsten Scholak",
                "Nathan Schucher",
                "Dzmitry Bahdanau."
            ],
            "title": "Picard: Parsing incrementally for constrained auto-regressive decoding from language models",
            "venue": "arXiv preprint arXiv:2109.05093.",
            "year": 2021
        },
        {
            "authors": [
                "Sankaranarayanan."
            ],
            "title": "Athena++ natural language querying for complex nested sql queries",
            "venue": "Proceedings of the VLDB Endowment, 13(12):2747\u20132759.",
            "year": 2020
        },
        {
            "authors": [
                "Ruoxi Sun",
                "Sercan O Arik",
                "Hootan Nakhost",
                "Hanjun Dai",
                "Rajarishi Sinha",
                "Pengcheng Yin",
                "Tomas Pfister."
            ],
            "title": "Sql-palm: Improved large language modeladaptation for text-to-sql",
            "venue": "arXiv preprint arXiv:2306.00739.",
            "year": 2023
        },
        {
            "authors": [
                "Bailin Wang",
                "Richard Shin",
                "Xiaodong Liu",
                "Oleksandr Polozov",
                "Matthew Richardson."
            ],
            "title": "Rat-sql: Relation-aware schema encoding and linking for textto-sql parsers",
            "venue": "arXiv preprint arXiv:1911.04942.",
            "year": 2019
        },
        {
            "authors": [
                "Tao Yu",
                "Michihiro Yasunaga",
                "Kai Yang",
                "Rui Zhang",
                "Dongxu Wang",
                "Zifan Li",
                "Dragomir Radev."
            ],
            "title": "Syntaxsqlnet: Syntax tree networks for complex and cross-domaintext-to-sql task",
            "venue": "arXiv preprint arXiv:1810.05237.",
            "year": 2018
        },
        {
            "authors": [
                "Tao Yu",
                "Rui Zhang",
                "Kai Yang",
                "Michihiro Yasunaga",
                "Dongxu Wang",
                "Zifan Li",
                "James Ma",
                "Irene Li",
                "Qingning Yao",
                "Shanelle Roman"
            ],
            "title": "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql",
            "year": 2018
        },
        {
            "authors": [
                "Ruiqi Zhong",
                "Charlie Snell",
                "Dan Klein",
                "Jason Eisner."
            ],
            "title": "Active programming by example with a natural language prior",
            "venue": "arXiv preprint arXiv:2205.12422.",
            "year": 2022
        },
        {
            "authors": [
                "Ruiqi Zhong",
                "Tao Yu",
                "Dan Klein."
            ],
            "title": "Semantic evaluation for text-to-sql with distilled test suites",
            "venue": "arXiv preprint arXiv:2010.02840.",
            "year": 2020
        },
        {
            "authors": [
                "Victor Zhong",
                "Mike Lewis",
                "Sida I Wang",
                "Luke Zettlemoyer."
            ],
            "title": "Grounded adaptation for zeroshot executable semantic parsing",
            "venue": "arXiv preprint arXiv:2009.07396.",
            "year": 2020
        },
        {
            "authors": [
                "Victor Zhong",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Seq2sql: Generating structured queries from natural language using reinforcement learning",
            "venue": "CoRR, abs/1709.00103.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Significant progress has been made in translating natural language text to SQL statements over the past few years. The execution accuracy on the hold-out test of Spider (Yu et al., 2018b)\u2013a large-scale cross-domain text-to-SQL benchmark\u2013 has improved from 53.5 in May, 2020 (Zhong et al., 2020b) to 85.3 in March, 2023 (Pourreza and Rafiei, 2023). The exact set match accuracy, without considering database cell values, on the same benchmark and over the same period has im-\nproved from 65.6 (Wang et al., 2019) to 74.0 (Li et al., 2023a). Measuring such progress is hinged on reliable benchmarks and evaluation metrics.\nTwo standard metrics for evaluating the performance in this domain have been exact set match accuracy and execution accuracy. The former measures if a model-generated SQL query lexically matches a reference SQL query, whereas the latter measures if a model-generated SQL query produces the same output as a reference query (\u00a7 4).\nConsider the example in Figure 1, which consists of a model-generated query (shown on the left) and a reference query (shown on the right). Both SQL queries return the id and name of makers that have more than 3 models. However, the model-generated query returns the column FullName, which gives the full name of a maker (e.g., \u201cFord Motor Company\u201d), whereas the reference query given in the benchmark returns the column Maker, which gives the short common name of a maker (e.g., \u201cFord\u201d). The model-generated query\nfails an exact set match since the column names in the select clause are different. The query outputs are also different and the model-generated query fails the execution accuracy as well. The natural language utterance is not specific about the type of name to be returned, and a human evaluator tags both queries correct.\nAs the models improve, these types of failures make up most of the errors, and the performance metrics become less relevant, as shown in our evaluation. In particular, we re-evaluated all development set queries of Spider on which two top-performing models, one using a fine-tuned model (Scholak et al., 2021) and another using a large language model (Pourreza and Rafiei, 2023), failed. We found out that 25% of the queries generated by one model and 87% of the queries generated by the other model were indeed correct but were wrongly evaluated by the benchmark. For the same set of queries, our re-evaluation of the ground truth queries found 33% of the SQL queries incorrect, which was more than the number of incorrect queries generated by one of the models. This evaluation places one of the models above the ground truth queries in this re-evaluation.\nWe further re-evaluated two well-known benchmarks, Spider (Yu et al., 2018b) and SpiderDK (Gan et al., 2021b), and a newly released benchmark, BIRD (Li et al., 2023b), and found similar problems in all three benchmarks that affect the evaluation. Our evaluation reveals that 18% of the queries in the train sets and 20%-23% of the queries in the dev sets of these benchmarks are subject to ties in the dataset and which one of the tied rows are returned. This means a model-generated query will be deemed incorrect if it does not return the same row, among tied rows, as the ground truth query. This can severely impact the evaluation, especially when there is a tight race among models. Considering these observations, it is crucial to emphasize the significance of additional independent evaluations when utilizing these benchmarks. To enhance the evaluation process further, a potential solution is to incorporate multiple SQL queries as the ground truth, each representing a different interpretation that may be valid.\nOur objective in this paper is to provide a comprehensive evaluation of existing Text-to-SQL benchmarks, underscoring the inherent issues they possess. We refrain from introducing a new dataset due to several considerations. First, addressing\nthe identified issues by updating these benchmarks requires considerable human effort. Additionally, benchmarks in the Text-to-SQL domain, like Spider and BIRD, have holdout test sets used for official leaderboards and comparisons of text-to-SQL methodologies. We only have access to the development and training sets of these benchmarks, which limits our capability to alter the test sets. As a result, making changes only to the development and training sets would not completely address the benchmark\u2019s inherent problems, given that final performance is gauged using the problematic test sets."
        },
        {
            "heading": "2 Related Work",
            "text": "Limited research has been dedicated to assessing the reliability and effectiveness of Text-to-SQL benchmarks. The authors of SQL-PaLM (Sun et al., 2023) note in their qualitative analysis of their model that some queries, labelled as incorrect by execution accuracy, were considered correct by human annotators. Similarly, Lei et al. (2020) conduct an analysis highlighting the discrepancy between automatic evaluations and human annotations. They emphasize that certain queries produced by the models were labeled as incorrect SQL queries but human annotators labelled them as correct queries. Generally, a query that is equivalent (but not identical) to ground truth may be mistakenly classified as incorrect by automated evaluation metrics. Another study by Zhong et al. (2022) identifies limitations within the Spider benchmark, such as issues with ties and certain syntactic problems. Their analysis is primarily focused on a subset of Spider, without quantifying the extent or impact of these limitations or conducting an assessment of other benchmarks."
        },
        {
            "heading": "3 Text-to-SQL Benchmarks",
            "text": "Benchmarks have played a crucial role in advancing the field and providing a platform for evaluation. WikiSQL (Zhong et al., 2017) consists of over 24,000 tables from Wikipedia with SQL queries generated based on some predefined rules and templates. The queries in this dataset are considered easy since they are all single-table queries. Spider, introduced by Yu et al. (2018b), consists of 200 database schemas of which 160 schemas are published as train and dev sets and 40 schemas are kept hidden for testing. The queries are written on those schemas by Computer Science students\nwithout using templates. This is considered a challenging dataset. Some other benchmarks are developed based on Spider, including Spider-Syn (Gan et al., 2021a), which replaces schema-related words with synonyms and eliminates explicit mentions between NLQ and schema, and Spider-DK (Gan et al., 2021b), which introduces rarely observed domain knowledge into the Spider development set. Other benchmarks include FIBEN (Sen et al., 2020), created for the financial domain and BIRD (Li et al., 2023b), which comprises 12,751 queries over 95 databases spanning 37 professional domains.\nOur study in this paper focuses on cross-domain large-scale benchmark Spider, its variants SpiderDK and Spider-SYN, and a more recent crossdomain large-scale benchmark BIRD. The selection of these benchmarks stems from their resemblance to real-world datasets, which is a crucial factor in conducting comprehensive research and analysis. One notable advantage of these benchmarks is the availability of a large training set, which plays a pivotal role in training and fine-tuning large-scale models. The inclusion of a substantial amount of training data enables the development of more robust and powerful models that can better handle the complexities and nuances present in real-world databases."
        },
        {
            "heading": "4 Evaluation Metrics",
            "text": "The performance evaluation of text-to-SQL systems involves comparing them to a reference system, typically a gold standard set of known correct SQL queries. Generating a reference can be challenging due to multiple interpretations of natural language questions, while SQL queries are based on logic and tend to cover only one interpretation. Even if an interpretation is fixed, detecting if a model-generated query is equivalent to a reference query is challenging, due to the halting problem which is undecidable (Davis, 2004). Nonetheless, to assess progress, proxy measures of performance have been developed in the literature. As two such metrics, we review exact set match accuracy and execution accuracy in this paper.\nUnder exact set match accuracy, SQL queries are evaluated by matching the query clauses and components independently, such as the select, where, having, group by, and order by clauses. The matching is based on comparing columns and predicates, disregarding the ordering of columns and predicates. An exact\nmatching of literals can be challenging since predicates such as nationality=\u201cCanada\u201d and nationality=\u201cCanadian\u201d will not match. However, accurately generating those literals without accessing database content may not be possible. Under exact set matching without values, which is used in Spider (Yu et al., 2018b), a matching of literals is not required.\nTwo equivalent SQL queries can have different expressions and may not match under an exact set match. An alternative metric that can reduce the number of false negatives is the execution accuracy. Under execution accuracy, the equivalence between a model-generated query and a reference query is established if they both produce the same results on all possible databases instances (Yu et al., 2018a). While testing all instances is impractical, running queries on a subset of instances can help identify candidates that are not equivalent to the reference query. Although execution accuracy can detect queries that are equivalent but not identical, it may mistakenly identify queries as equivalent if they produce the same result on tested instances. Therefore, an effective execution-based evaluation requires finding instances that cover various edge cases and can detect queries that are not equivalent to the reference. Test suite accuracy (Zhong et al., 2020a), which is simply referred to as execution accuracy in Spider benchmark and in our work, aims to minimize false positives by evaluating queries on a carefully selected collection of database instances, known as a test suite. Nevertheless, an execution-based accuracy cannot capture all correct SQL queries, highlighting the limitations and the continued importance of human evaluation for reliable assessment."
        },
        {
            "heading": "5 Execution Accuracy Failures",
            "text": "A model-generated query can be correct but still fail the execution accuracy. We classify these failures into three categories: (1) failures due to ties in output, (2) ambiguity in schema matching, (3) wrong assumptions made about database content."
        },
        {
            "heading": "5.1 Failures Due to Ties in Output",
            "text": "SQL queries can lead to ties and a subset of the tied rows may be returned. The selection of tied rows can vary between queries and this can affect the execution accuracy. We identify a few sources for such ties, as discussed next, and study their impact on benchmark evaluations in Section 6. Table 1\nprovides a detailed breakdown of the number of queries that can potentially yield tied rows in both train and development set of Spider, Spider-DK, and BIRD benchmarks."
        },
        {
            "heading": "5.1.1 Top with Ties",
            "text": "Sometimes the query asks for top rows that satisfy some conditions (e.g., the student with the highest GPA, or the youngest student). When there is a tie for the top position, and the query in natural language is not specific on how the ties should be handled, the corresponding SQL query may return all ties or only one. This becomes a problem in evaluation if a model-generated query and the reference query treat the ties differently. Figure 2 provides a concrete example from the Spider dataset, illustrating this issue, where the reference SQL query in the benchmark fails to account for ties and returns only one of them using the LIMIT keyword."
        },
        {
            "heading": "5.1.2 LIMIT N",
            "text": "The problems associated with using the LIMIT n clause in SQL queries is not limited to the top position, as discussed above. The use of this clause is problematic for evaluation in general. Firstly, without an explicit ordering, the result of a SQL query is expected to be a set. Two equivalent (but not identical) queries can return the same set of results, each listed in different orders, but selecting the first n rows from one ordering will not necessarily match the same selection from a different ordering. Secondly, with query results sorted, there can be a tie on row n with multiple rows having the same values. The ordering among tied rows can vary between two queries, and so is the first n rows that are returned. All benchmarks studied in this paper (Spider, Spider-DK, Spider-SYN, BIRD) use the limit keyword and suffer from the aforementioned problems associated with ties."
        },
        {
            "heading": "5.1.3 GROUP BY",
            "text": "Many text-to-SQL benchmarks encounter a different type of issue associated with ties, particularly arising due to incorrect usage of non-aggregated columns in both the SELECT clause and the GROUP BY clause. Within the benchmarks, these ties manifest in two scenarios: 1) a column appears in the SELECT clause without being inside an aggregation function and without being included in the GROUP BY clause; 2) the SELECT clause contains a mix of aggregated and non-aggregated columns without utilizing a GROUP BY clause. In\nboth cases, multiple records can be associated with the same grouping column or aggregation value, whereas each group can only return one record. Some database systems including Oracle and DB2 prevent these cases by treating them as syntax errors. However, other database systems such as SQLite and MySQL take a more lazy approach (sometimes for efficiency reasons) and allow these cases to happen. Many text-to-SQL benchmarks follow SQLite syntax and suffer from this issue. The affected queries in our benchmarks were identified after migrating from SQLite to PostgreSQL, as detailed in Section 6.4, and checking for queries that failed during PostgreSQL execution. Figure 3, illustrates one example of such a problem from the Spider dataset."
        },
        {
            "heading": "5.1.4 ORDER BY",
            "text": "Another subtle ambiguity with tied values arises in queries where the SELECT clause incorporates the \"distinct\" keyword, paired with an ORDER BY clause referencing a column absent in the SELECT clause. Consider the exemplary query from Spider train set: SELECT DISTINCT district_name FROM district ORDER BY city_area DESC. The ordering of the output, as well as the result of a comparison with a reference query, becomes uncertain if a single \u2019district_name\u2019 value maps to multiple \u2019city_area\u2019 values. Similar to GROUP BY, the affected queries in the benchmarks were identified through a SQLite to PostgreSQL migration(\u00a7 6.4)."
        },
        {
            "heading": "5.2 Ambiguity in Schema Matching",
            "text": "Schema matching refers to the task of establishing the correspondence between a natural language question and the tables, columns, and cell values in the database ((Cao et al., 2021; Pourreza and Rafiei, 2023; Wang et al., 2019; Li et al., 2023b). Ambiguities arise when there are multiple columns in the database that can represent the same semantic meaning, and the information needs of a query may be satisfied using any of those columns. As a result, there exist multiple SQL queries that can produce the correct answer, yet most benchmarks only provide one query among the many possible correct answers. Figure 1 illustrates an example question that can be satisfied by two different SQL queries, both of which are valid responses to the question at hand."
        },
        {
            "heading": "5.3 Wrong Assumptions on DB Content",
            "text": "Lastly, one type of limitation in text-to-SQL benchmarks stems from incorrect assumptions regarding cell values. It is common to make assumptions about database content and constraints when writing SQL queries, but those assumptions may not be supported by the database schema or content. This issue arises when the database content is created under assumptions that do not align with those in queries, leading to potential failures in the evaluation process. Text-to-SQL models often lack access to full database content due to limitations such as the context window problem and the inability to pass all cell values to the models for reasons such as privacy and cost. These models typically rely on the provided database schema and a selected sample of database rows to represent potential values (Pourreza and Rafiei, 2023; Liu et al., 2023; Rajkumar et al., 2022; Sun et al., 2023; Li et al., 2023a; Lin et al., 2020). Consequently, the assumptions made by these models may not align with the actual ground truth, resulting in SQL queries that are correct under the assumption made but do not match the reference query in the benchmark.\nOne observed case is when certain conditions (e.g., PetType=\u2018dog\u2019) are omitted from SQL queries due to the erroneous assumption that the condition holds for all rows in the database. Figure 4 exemplifies this issue using an example from the Spider dataset, where both queries yield the same answer on a specific database instance. However, changing the database values could result in failure, especially when evaluating performance using testsuite accuracy, which involves querying different database instances. Another case observed in the benchmarks is when the ground truth SQL queries assume a specific column has unique values, but in reality, that column does not possess that unique constraint. Figure 5 depicts an example of this\nproblem from the Spider dataset."
        },
        {
            "heading": "6 Experiments",
            "text": "To understand the extent at which the aforementioned problems affect the benchmarks, our evaluation and the ranking of the models, we conducted three types of evaluations on three benchmarks: Spider, Spider-DK, BIRD. Our findings here apply to the Spider-SYN dataset as well, which employs the same SQL queries as in the Spider dataset. For the same reason, we did not conduct a separate analysis of that benchmark."
        },
        {
            "heading": "6.1 Evaluation Through Query Rewriting",
            "text": "In this experiment, our focus is on ties and how a tie breaking strategy affects the benchmarks and our evaluation. This is done through query rewriting. Automating query rewriting faces inherent challenges, particularly when dealing with failures stemming from schema ambiguity, erroneous assumptions about the database content, and the ambiguity of natural language utterances. These challenges arise because there is no specific structure to address the failures systematically. Successful query rewriting in these cases necessitates a deeper understanding of table and column semantics to identify ambiguities and erroneous assumptions. In cases of ambiguity, human expertise is essential to disambiguate the context, as these situations often lack clear guidelines. Detecting erroneous assumptions often involves introducing new data to the database and meticulously reviewing and correcting failed queries on a case-by-case basis. Therefore, our efforts have been channeled towards rewriting queries concerning tied values, which adhere to a specific syntax structure, and the problems associated with the ambiguity in schema matching and wrong assumptions on database content are studied in the next section.\nMany benchmark queries use \u201cLIMIT 1\u201d to find top rows that satisfy some conditions. If there are ties on top, one arbitrary row among ties is returned. An alternative is to return all ties. We rewrote all queries that used \u201cLIMIT 1\u201d to return all ties. This was done by introducing min() and max() aggregation functions within nested queries to accurately identify extreme values. An example of such rewriting is shown in Figure 2. Breaking ties for queries that used \u201cLIMIT n\u201d for n > 1 was not straightforward, and those queries were left unchanged.\nFor resolving ties introduced by an incorrect usage of GROUP BY in benchmark queries, we included all non-aggregated columns from the SELECT clause in the GROUP BY clause. For example, if the SELECT clauses included id and name, but the GROUP BY clause only included name, we added id to the GROUP BY clause. This change will not affect queries where there is a one-to-one mapping between id and name, but it will resolve the ambiguity when such mapping does not hold.\nWith these two changes, 16% to 20% of the reference queries in our benchmarks were affected. Under a perfect evaluation scheme, the accuracy should not be affected with these changes that sim-\nply resolve the uncertainty. Table 2 displays both the execution accuracy and the exact set match accuracy for the reference queries from the BIRD, Spider, and Spider-DK benchmarks after our modifications. It\u2019s important to highlight that the performance metrics provided in this table encompass the entire development set of these benchmarks, combining both modified and unaltered queries. For clarity, in the Spider dataset, out of 1034 queries, 206 were modified. The performance assessment took into account a mixed set of predicted queries: 206 that were adjusted and 828 that remained as originally presented. This culminated in an execution accuracy of 92.3 percent.\nIt can be noted that the execution accuracy is not as adversely affected as the exact set match accuracy. We hypothesize that this could be attributed to the absence of ties in the test data used for these benchmarks. An evidence of this is the following two queries, (Q1) SELECT name, capacity FROM stadium WHERE average = (SELECT max(average) FROM stadium), and (Q2) SELECT name, capacity FROM stadium ORDER BY average DESC LIMIT 1, labelled as a correct match by the test scripts of Spider."
        },
        {
            "heading": "6.2 Human Evaluation",
            "text": "To gain a deeper understanding of the limitations within the benchmarks, we conducted an experiment focused on the widely-used text-toSQL benchmark, the Spider dataset. Specifically, we evaluated two top-performing methods from the Spider leaderboard: DIN-SQL (Pourreza and Rafiei, 2023) and T5-large + PICARD (Scholak et al., 2021). This experiment involved running these methods on the development set of Spider, which comprised 1034 question-query pairs. From the results obtained, we extracted the questions for which both methods failed to produce a correct answer, based on the execution accuracy, resulting in 102 pairs. We then presented these questions, along with the SQL queries generated by the methods as well as the ground truth SQL queries (treating them the same as model-generated queries), to\ntwo annotators 1 for labelling. The annotators had access to the database schemas and were tasked with identifying the queries they deemed correct for each question, without knowing which model generated which query or if the query was from the ground truth queries. Annotators could also create databases and validate queries, ensuring a thorough evaluation.\nFollowing our initial labelling process, we wanted to minimize the potential impact of human errors in our evaluation. For this, we identified queries with inconsistent labels among the annotators and presented them to the annotators. Each annotator was asked to provide an explanation for their assigned labels. In the final stage of evaluation, each annotator was presented the inconsistent queries and the explanations provided by the other annotator. They were then asked if they would revise their labels based on this additional information. The results of this experiment are presented in Table 3. This table presents the outcome of human evaluation on a sample of 102 queries that both DIN-SQL and T5+PICARD methods were deemed incorrect in terms of execution accuracy. SQL experts conducted this evaluation, with 81.6% of these queries judged as correct for DIN-SQL, and only 25.5% for T5+PICARD. Notably, among the reference queries, only 67.3% were deemed correct. Even after the second round of annotation, a few queries (more specifically, four question-query pairs) still exhibit inconsistent labeling by the annotators. The main challenge with these particular pairs is the inherent ambiguity in the questions or the subjectivity of interpretations, which leads to a lack of a definitive answer. Figure 6 demonstrates one example of such a question with two possible SQL query as answers.\nAn intriguing observation emerged from this experiment: the DIN-SQL method, powered by GPT4, produced the highest number of correct answers, surpassing even the ground truth SQL queries. This finding sheds light on the limitations of the current benchmarks and raises doubts about the reliability of current leaderboards and performance metrics."
        },
        {
            "heading": "6.3 Error Analysis of Human Evaluation",
            "text": "We performed an error analysis of the SQL queries that were labelled as incorrect in our human evaluation to better understand the error types and causes and to provide insights into areas for improving the\n1The human annotators are the authors of this paper.\nground truth SQL queries. Additionally, we compared the errors in ground truth queries with those of fine-tuning and prompting approaches. The identified errors, categorized into five groups, are briefly discussed next. The distribution of SQL queries across these groups is depicted in Figure 7.\nSchema The primary issue responsible for the majority of errors, affecting both the reference SQL queries and the two methods, is the incorrect usage of schemas, which arises when the SQL query utilizes incorrect tables or columns to answer the given question. These errors indicate ambiguities in the database schema and/or questions, as discussed in Section 5. Notably, the reference set shows the least number of errors, which is closely followed by DIN-SQL.\nCondition The second-largest group of errors observed pertains to the usage of incorrect conditions within the SQL queries. Unlike the schema\ngroup, where the tables and columns were incorrect, in this group, the correct tables and columns are used, but the conditions in the WHERE clause are erroneous. This error primarily manifested in the queries generated by the T5-PICARD method, but was also present in the reference set. The T5 model\u2019s tendency to introduce additional columns or omit necessary conditions could be attributed to its smaller size relative to larger models like GPT-4, limiting its grasp of intricate SQL syntax.\nNested The source of this problem is using a nonunique column for the nested SQL query, as also discussed in Section 5. Figure 5 shows an example of such an error in a SQL query. This error was more common in the SQL queries provided in the reference set as well as those of T5-PICARD.\nGROUP BY This category includes queries that incorrectly used GROUP BY, resulting in ambiguity or uncertainty in the result as discussed in Section 5. Notably, the reference set showed the largest number of errors, closely followed by the fine-tuned T5-PICARD. DIN-SQL exhibited the least number of errors.\nLIMIT As highlighted in Section 5, one of the error scenarios involves not properly handling ties when using the LIMIT keyword. The DIN-SQL method demonstrates a lower incidence of this type of error, attributed to its prompting nature. Conversely, T5-PICARD exhibits identical performance to the ground truth in this particular case."
        },
        {
            "heading": "6.4 Standard SQL validation",
            "text": "We undertook an extensive review of the development set of Spider, BIRD, and Spider-DK benchmarks through the lens of standard SQL validation. The objective was to identify some of the problematic queries discussed in Section 5 and assess the portability of the benchmarks. As part of this analysis, we migrated the databases and queries of these three benchmarks from Sqlite to PostgreSQL. Our decision to use PostgreSQL, a widely recognized RDBMS, stemmed from its rigorous adherence to SQL standards. Following the migration, we executed every query from the development set on these PostgreSQL databases, with a keen focus on identifying queries that failed during PostgreSQL execution. Table 4 provides a breakdown of queries by error type across all three benchmarks. Notably, errors such as UndefinedColumn, SyntaxError, and UndefinedFunction emerge due to the different SQL formats supported by Sqlite and PostgreSQL. These variances necessitate adjustments to make the queries compatible with PostgreSQL standards. For instance, the Spider dataset frequently showcases errors stemming from PostgreSQL\u2019s strict typing conventions. While SQLite allows for comparisons of int with text, PostgreSQL does not. Also, some queries run into problems because of SQLite-exclusive functions, such as strftime and iff, or because PostgreSQL interprets literals in double quotations as column names.\nThe two other types of failures, group by and Order by, included queries that introduced ambiguities to the benchmarks, as discussed in Section 5. It should be noted that these benchmarks present a range of issues that are not solely confined to syntax. Challenges related to wrong assumptions on DB content and ambiguities in schema matching are notably pervasive."
        },
        {
            "heading": "7 Discussion",
            "text": "Our analysis (\u00a7 6.1) reveals the limitations of major text-to-SQL benchmarks, highlighting the fact that even with a perfect model, achieving a perfect accuracy on these benchmarks is not possible. The\naccuracies presented in Table 2 serve as a lose upper bound for the achievable accuracy by models. It is lose because our rewritings were unable to address cases that required manual intervention to reconstruct a correct query. Thus, the upper bound is expected to be lower considering other issues such as wrong assumptions on the database content and ambiguity in schema matching.\nOur human evaluation (\u00a7 6.2) further supports our claim and provides more insight into the limitations within one of the benchmarks studied. The results in Table 3 demonstrate that prompting methods, such as DIN-SQL, are less affected by the inherent limitations of the training set in the benchmarks. However, they are not fully immune because of the few-shot input-output demonstrations that are taken from the train set. On the other hand, fine-tuned approaches, such as T5+PICARD, perfectly mirror the distribution of errors seen in the ground truth queries for types nested, LIMIT, and GROUP BY. The largest number of wrong queries in schema and condition classes belong to our finetuned model, due to inability of the model to generate correct SQL queries."
        },
        {
            "heading": "8 Conclusions",
            "text": "The reliance on standard text-to-SQL evaluation metrics, namely exact set match accuracy and execution accuracy, has become less reliable as the model performance approaches human-level performance. Our work is the first to systematically study the limitations of these metrics and benchmarks through both human evaluation and query rewriting. Our re-evaluation of well-known benchmarks (Spider, Spider-DK, and BIRD) uncovers common systematic issues that affect the evaluation process and performance estimates, revealing that a significant portion of queries in the train and dev sets are impacted by these issues. Incorporating multiple SQL queries as the ground truth and representing different interpretations of queries offer a promising solution to enhance the evaluation process and achieve a more comprehensive and accurate assessment of Text-to-SQL models.\nLimitations\nIn this study, our focus was primarily on crossdomain text-to-SQL benchmarks and models. The failure cases identified in this domain are likely to be present in other domain-specific text-to-SQL benchmarks and models as well. It is essential to conduct further analysis to identify specific failure cases within domain-specific benchmarks and models.\nFurthermore, it is worth mentioning that our work has a limitation regarding the analysis of failure cases that lack a specific structure and require manual effort for detection. Identifying and addressing such problems necessitates extensive work. The purpose of our study was to highlight these failure cases; a more in-depth analysis of their prevalence can provide a clearer understanding of their impact on the overall performance of text-to-SQL systems.\nEthics Statement\nIn this paper, we acknowledge the importance of ethical considerations in conducting and presenting our research. We affirm our commitment to comply with the ACL Ethics Policy and adhere to ethical guidelines and principles throughout the entire research process.\nWe have taken necessary measures to ensure the privacy, confidentiality, and consent of individuals or entities involved in our data collection, experimentation, and analysis. Any personal or sensitive information used in this study has been appropriately anonymized and safeguarded.\nFurthermore, we have made efforts to minimize any potential biases and discrimination in our research design, data selection, and interpretation of results. We have strived for transparency, accuracy, and fairness in reporting our findings, and we have provided appropriate citations and acknowledgments to give credit to the work of others.\nBy including this ethics statement, we aim to demonstrate our dedication to conducting research with integrity, respecting ethical principles, and contributing to the responsible advancement of knowledge in our field."
        }
    ],
    "title": "Evaluating Cross-Domain Text-to-SQL Models and Benchmarks",
    "year": 2023
}