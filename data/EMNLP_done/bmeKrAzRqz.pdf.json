{
    "abstractText": "Human gaze data offer cognitive information that reflects natural language comprehension. Indeed, augmenting language models with human scanpaths has proven beneficial for a range of NLP tasks, including language understanding. However, the applicability of this approach is hampered because the abundance of text corpora is contrasted by a scarcity of gaze data. Although models for the generation of humanlike scanpaths during reading have been developed, the potential of synthetic gaze data across NLP tasks remains largely unexplored. We develop a model that integrates synthetic scanpath generation with a scanpath-augmented language model, eliminating the need for human gaze data. Since the model\u2019s error gradient can be propagated throughout all parts of the model, the scanpath generator can be fine-tuned to downstream tasks. We find that the proposed model not only outperforms the underlying language model, but achieves a performance that is comparable to a language model augmented with real human gaze data. Our code is publicly available.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Shuwen Deng"
        },
        {
            "affiliations": [],
            "name": "Paul Prasse"
        },
        {
            "affiliations": [],
            "name": "David R. Reich"
        },
        {
            "affiliations": [],
            "name": "Tobias Scheffer"
        },
        {
            "affiliations": [],
            "name": "Lena A. J\u00e4ger"
        }
    ],
    "id": "SP:a46b5d9f706d62f0b4238abac8396fae75b55bd5",
    "references": [
        {
            "authors": [
                "Maria Barrett",
                "Joachim Bingel",
                "Nora Hollenstein",
                "Marek Rei",
                "Anders S\u00f8gaard."
            ],
            "title": "Sequence classification with human attention",
            "venue": "Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL), pages 302\u2013312, Brussels,",
            "year": 2018
        },
        {
            "authors": [
                "Maria Barrett",
                "Frank Keller",
                "Anders S\u00f8gaard."
            ],
            "title": "Cross-lingual transfer of correlations between parts of speech and gaze features",
            "venue": "Proceedings of the 26th International Conference on Computational Linguistics (COLING): Technical Papers, pages 1330\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Yevgeni Berzak",
                "Chie Nakamura",
                "Amelia Smith",
                "Emily Weng",
                "Boris Katz",
                "Suzanne Flynn",
                "Roger Levy."
            ],
            "title": "CELER: A 365-participant corpus of eye movements in L1 and L2 English reading",
            "venue": "Open Mind, pages 1\u201310.",
            "year": 2022
        },
        {
            "authors": [
                "Pieter Blignaut",
                "Dani\u00ebl Wium."
            ],
            "title": "Eye-tracking data quality as affected by ethnicity and experimental design",
            "venue": "Behavior Research Methods, 46:67\u201380.",
            "year": 2014
        },
        {
            "authors": [
                "Kyunghyun Cho",
                "Bart van Merri\u00ebnboer",
                "Dzmitry Bahdanau",
                "Yoshua Bengio."
            ],
            "title": "On the properties of neural machine translation: Encoder\u2013decoder approaches",
            "venue": "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical",
            "year": 2014
        },
        {
            "authors": [
                "Shuwen Deng",
                "David R Reich",
                "Paul Prasse",
                "Patrick Haller",
                "Tobias Scheffer",
                "Lena A J\u00e4ger."
            ],
            "title": "Eyettention: An attention-based dual-sequence model for predicting human scanpaths during reading",
            "venue": "Proceedings of the ACM on Human-Computer Interaction,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2019
        },
        {
            "authors": [
                "Ana Valeria Gonz\u00e1lez-Gardu\u00f1o",
                "Anders S\u00f8gaard."
            ],
            "title": "Using gaze to predict text readability",
            "venue": "Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, EMNLP, pages 438\u2013443, Copenhagen, Denmark.",
            "year": 2017
        },
        {
            "authors": [
                "Nora Hollenstein",
                "Ce Zhang."
            ],
            "title": "Entity recognition at first sight: Improving NER with eye movement information",
            "venue": "Proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-",
            "year": 2019
        },
        {
            "authors": [
                "Lena A. J\u00e4ger",
                "Silvia Makowski",
                "Paul Prasse",
                "Liehr Sascha",
                "Maximilian Seidler",
                "Tobias Scheffer."
            ],
            "title": "Deep Eyedentification: Biometric identification using micro-movements of the eye",
            "venue": "Machine Learning and Knowledge Discovery in Databases.",
            "year": 2020
        },
        {
            "authors": [
                "Eric Jang",
                "Shixiang Gu",
                "Ben Poole."
            ],
            "title": "Categorical reparameterization with gumbel-softmax",
            "venue": "Proceedings of the 5th International Conference on Learning Representations (ICLR), Toulon, France.",
            "year": 2017
        },
        {
            "authors": [
                "Marcel A Just",
                "Patricia A Carpenter."
            ],
            "title": "A theory of reading: From eye fixations to comprehension",
            "venue": "Psychological Review, 87(4):329.",
            "year": 1980
        },
        {
            "authors": [
                "Varun Khurana",
                "Yaman Kumar",
                "Nora Hollenstein",
                "Rajesh Kumar",
                "Balaji Krishnamurthy."
            ],
            "title": "Synthesizing human gaze feedback for improved NLP performance",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association",
            "year": 2023
        },
        {
            "authors": [
                "Sigrid Klerke",
                "Yoav Goldberg",
                "Anders S\u00f8gaard."
            ],
            "title": "Improving sentence compression by learning to predict gaze",
            "venue": "Proceedings of North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-",
            "year": 2016
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "Proceedings of International Conference on Learning Representations (ICLR), New Orleans, Louisiana, United States.",
            "year": 2019
        },
        {
            "authors": [
                "Silvia Makowski",
                "Paul Prasse",
                "David R Reich",
                "Daniel Krakowczyk",
                "Lena A J\u00e4ger",
                "Tobias Scheffer."
            ],
            "title": "Deepeyedentificationlive: Oculomotoric biometric identification and presentation-attack detection using deep neural networks",
            "venue": "IEEE Transac-",
            "year": 2021
        },
        {
            "authors": [
                "Yuning Mao",
                "Lambert Mathias",
                "Rui Hou",
                "Amjad Almahairi",
                "Hao Ma",
                "Jiawei Han",
                "Scott Yih",
                "Madian Khabsa."
            ],
            "title": "UniPELT: A unified framework for parameter-efficient language model tuning",
            "venue": "Proceedings of the 60th Annual Meeting of the Associa-",
            "year": 2022
        },
        {
            "authors": [
                "Abhijit Mishra",
                "Pushpak Bhattacharyya",
                "Abhijit Mishra",
                "Pushpak Bhattacharyya."
            ],
            "title": "Scanpath complexity: modeling reading/annotation effort using gaze information",
            "venue": "Cognitively Inspired Natural Language Processing: An Investigation Based on Eye-",
            "year": 2018
        },
        {
            "authors": [
                "Abhijit Mishra",
                "Kuntal Dey",
                "Pushpak Bhattacharyya."
            ],
            "title": "Learning cognitive features from gaze data for sentiment and sarcasm classification using convolutional neural network",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computa-",
            "year": 2017
        },
        {
            "authors": [
                "Abhijit Mishra",
                "Diptesh Kanojia",
                "Pushpak Bhattacharyya."
            ],
            "title": "Predicting readers\u2019 sarcasm understandability by modeling gaze behavior",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, Phoenix, Arizona, USA.",
            "year": 2016
        },
        {
            "authors": [
                "Abhijit Mishra",
                "Srikanth Tamilselvam",
                "Riddhiman Dasgupta",
                "Seema Nagar",
                "Kuntal Dey."
            ],
            "title": "Cognition-cognizant sentiment analysis with multitask subjectivity summarization based on annotators\u2019 gaze behavior",
            "venue": "Proceedings of the AAAI Con-",
            "year": 2018
        },
        {
            "authors": [
                "Mattias Nilsson",
                "Joakim Nivre."
            ],
            "title": "Entropydriven evaluation of models of eye movement control in reading",
            "venue": "Proceedings of the 8th International NLPCS Workshop, pages 201\u2013212, Copenhagen, Denmark.",
            "year": 2011
        },
        {
            "authors": [
                "Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "In Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Negar Sammaknejad",
                "Hamidreza Pouretemad",
                "Changiz Eslahchi",
                "Alireza Salahirad",
                "Ashkan Alinejad."
            ],
            "title": "Gender classification based on eye movements: A processing effect during passive face viewing",
            "venue": "Advances in Cognitive Psychology, 13(3):232.",
            "year": 2017
        },
        {
            "authors": [
                "Ekta Sood",
                "Fabian K\u00f6gel",
                "Philipp M\u00fcller",
                "Dominike Thomas",
                "Mihai Bace",
                "Andreas Bulling."
            ],
            "title": "Multimodal integration of human-like attention in visual question answering",
            "venue": "Computing Research Repository.",
            "year": 2021
        },
        {
            "authors": [
                "Ekta Sood",
                "Simon Tannert",
                "Philipp M\u00fcller",
                "Andreas Bulling."
            ],
            "title": "Improving natural language processing tasks with human gaze-guided neural attention",
            "venue": "Proceedings of the 34th Conference on Neural Information Processing Systems (NeurIPS), pages",
            "year": 2020
        },
        {
            "authors": [
                "Ece Takmaz",
                "Sandro Pezzelle",
                "Lisa Beinborn",
                "Raquel Fern\u00e1ndez."
            ],
            "title": "Generating image descriptions via sequential cross-modal alignment guided by human gaze",
            "venue": "Proceedings of Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proceedings of EMNLP Workshop BlackboxNLP: Analyzing and In-",
            "year": 2018
        },
        {
            "authors": [
                "Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of Empirical Methods in Natural Language Processing (EMNLP): System Demonstrations, pages",
            "year": 2020
        },
        {
            "authors": [
                "Duo Yang",
                "Nora Hollenstein."
            ],
            "title": "PLM-AS: Pretrained language models augmented with scanpaths for sentiment classification",
            "venue": "Proceedings of the Northern Lights Deep Learning Workshop, Troms\u00f8, Norway.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction and Related Work",
            "text": "When humans read, they naturally engage in the cognitive process of comprehending language, which, in turn, is reflected in their gaze behavior (Just and Carpenter, 1980). In a nutshell, a scanpath (i.e., sequence of consecutive fixations) on a stimulus text approximates the reader\u2019s attention, which can be exploited to inform Natural Language Processing (NLP) tasks.\nGaze data has been shown to be beneficial in various NLP tasks, such as part-of-speech-tagging (Barrett et al., 2016), named entity recognition (Hollenstein and Zhang, 2019), generating image captions (Takmaz et al., 2020) and question answering (Sood et al., 2021). Researchers have explored\n1https://github.com/aeye-lab/ EMNLP-SyntheticScanpaths-NLU-PretrainedLM.\nthe use of aggregated word-level gaze features to regularize neural attention mechanisms (Barrett et al., 2018; Sood et al., 2020). Moreover, non-aggregated scanpaths, which capture the complete sequential ordering of the reader\u2019s gaze behavior, have also demonstrated promise in NLP tasks (Mishra et al., 2017, 2018a; Yang and Hollenstein, 2023).\nHowever, collecting gaze data is a resourceintensive endeavor, even for very small text corpora. Hence, human gaze data is scarce, and NLP taskspecific gaze recordings are even scarcer. Moreover, applying a language model that additionally consumes gaze data requires gaze data to be available for the input text at deployment time\u2014which is unrealistic for most use cases. To overcome these limitations, researchers have proposed a multi-task learning approach for NLP tasks such as sentence compression (Klerke et al., 2016), sentiment analysis (Mishra et al., 2018b), and predicting text readability (Gonz\u00e1lez-Gardu\u00f1o and S\u00f8gaard, 2017). In this approach, labeled data for the specific NLP task is used as the primary task, while a separate eye-tracking corpus is utilized as an auxiliary task. While this approach helps mitigate the need for task-specific gaze data during training and testing, the problem of general scarcity of gaze samples\nremains and hinders effective supervision for dataintensive architectures.\nIn this paper, we propose an alternative approach by using synthetic gaze data, which can be generated easily for any given text, to provide cognitive signals across NLP tasks. The seminal work of Sood et al. (2020), which integrates eye movement data generated by a computational cognitive model of eye-movement-control-during-reading for tasks such as sentence compression and paraphrase generation, demonstrated the potential of synthetic eye-gaze data. Khurana et al. (2023) explored a proof-of-concept model that integrated synthetic gaze data across multiple NLP tasks, but their results did not reach the performance of a fine-tuned BERT model (Devlin et al., 2019) without eye gaze on the General Language Understanding Evaluation (GLUE) benchmark. In our work, we build on recent advances in the development of machinelearning models for generating human-like scanpaths during reading (Deng et al., 2023; Bolliger et al., 2023; Khurana et al., 2023; Nilsson and Nivre, 2011).\nWe develop a model that combines synthetic scanpath generation with a scanpath-augmented language model, eliminating the need for human gaze data. The model allows for fine-tuning the scanpath generator to downstream tasks by propagating the error gradient through the entire model. Our approach not only outperforms the underlying language model in multiple tasks on the GLUE, especially in low-resource settings, but even reaches a performance comparable to an eye-gaze augmented model that uses real, rather than synthetic, eye movement data in sentiment classification."
        },
        {
            "heading": "2 Model",
            "text": "We develop a model that combines a scanpath generation model with a scanpath-augmented language model to perform NLP downstream tasks. Figure 1 depicts the proposed model architecture.\nScanpath Generation Model We adopt Eyettention (Deng et al., 2023), an open-source state-ofthe-art model for scanpath generation over text. Eyettention predicts consecutive fixation locations, represented as word indices, based on a stimulus sentence and the preceding fixations. It consists of two encoders, one for embedding the stimulus sentence, and the other for embedding the scanpath history. A cross-attention layer aligns the outputs of the two encoders, and a decoder produces a\nprobability distribution over saccade ranges at each timestep. The next fixated word index is determined by sampling from this distribution.\nScanpath-Augmented Language Model We adopt the PLM-AS framework (Yang and Hollenstein, 2023), which augments pre-trained language models with human scanpaths for sentiment classification. This framework uses a language model to extract token embeddings for a sentence, associating each embedding with its position index. By utilizing a human scanpath (fixation index sequence) as input, the model rearranges the token embedding sequence based on the order in which the words are fixated by the reader. The transformed sequence is then fed into a scanpath encoder, implemented as a layer of gated recurrent units (GRU), where the output of the last step is used as the final feature for sentiment classification. This framework allows for the use of different language models and achieves high performance through fine-tuning. In this work, we employ BERTBASE2 (Devlin et al., 2019) as the language model, following Yang and Hollenstein (2023).\nJoint Modeling for NLP Tasks To eliminate the need for human gaze data, we integrate the synthetic scanpath generated by the Eyettention model consisting of a fixation index sequence into the PLM-AS framework. Before integration, the word index sequence generated by Eyettention is converted into a token index sequence. During training, the error gradient of the scanpath-augmented language model can be back-propagated through the Eyettention model, allowing its parameters to be adapted for a specific NLP task. To handle the non-differentiable sampling from a categorical distribution involved in scanpath generation, we employ the Gumbel-softmax distribution (Jang et al., 2017) as a fully differentiable approximation. The training process consists of two phases. First, we pre-train the Eyettention model on a natural reading task. Second, we train the entire model, which includes fine-tuning the language model and the Eyettention model, as well as training the scanpath encoder from scratch. For the Eyettention model, we add residual connections in both encoders to enhance its performance.\n2Note that BERT can be substituted with other advanced pre-trained language models, potentially leading to further enhancements in task performance."
        },
        {
            "heading": "3 Experiments",
            "text": "In this section, we describe the data and present the evaluation results of our model for a wide range of NLP tasks. Further details about training and hyperparameter tuning can be found in Appendix B."
        },
        {
            "heading": "3.1 Data Sets",
            "text": "CELER (Berzak et al., 2022): We pre-train the scanpath generation model Eyettention on the L1 subset of CELER, which contains eye-tracking recordings collected from 69 native speakers of English during natural reading of 5,456 sentences.\nETSA (Mishra et al., 2016) contains taskspecific gaze recordings for sentiment classification of 7 subjects who each read 383 positive and 611 negative sentences, including sarcastic quotes, short movie reviews, and tweets.\nGLUE (Wang et al., 2018) includes sentiment analysis (SST-2), linguistic acceptability (CoLA), similarity and paraphrase tasks (MRPC, STS-B, QQP), and natural language inference tasks (MNLI, QNLI, RTE). No gaze data are available."
        },
        {
            "heading": "3.2 Sentiment Classification",
            "text": "Table 1 presents the results of our model on the sentiment classification task ETSA (Mishra et al., 2016), in comparison to BERT and previous stateof-the-art eye-gaze augmented models. We follow a 10-fold cross-validation regime. In each iteration, BERT is fine-tuned on the training portion of the ETSA text corpus, and PLM-AS is fine-tuned on the training portion of the ETSA text corpus and gaze data. Our model is fine-tuned on the training portion of the ETSA text corpus and, instead of the ETSA gaze data, synthetic gaze data generated by Eyettention. Since each sentence is associated with multiple scanpaths, we compute the final prediction by averaging the pre-softmax logits obtained from the models across all scanpaths for the PLMAS baseline. Our model averages equally many synthetic scanpaths. We make multiple notable observations in Table 1: (a) Our model outperforms both BERT and the state-of-the-art ScanTextGAN (Khurana et al., 2023) augmented with gaze data. (b) Our model, augmented with synthetic scanpaths, achieves comparable performance to the PLM-AS model augmented with human scanpaths, eliminating the need for human scanpaths. (c) Ablation experiments (bottom two rows) show that when the Eyettention model is frozen or\nnot pre-trained, the performance decreases. This demonstrates the importance of both pre-training and task-specific fine-tuning of the scanpath generator.\nVarying the number of scanpaths We analyze the impact of the number of scanpaths sampled both at training and at application time on model performance. Figure 2 shows the F1 score as a function of the number of scanpaths used by BERT without eye gaze, PLM-AS with human scanpaths, and our model with synthetic scanpaths. We observe that the performance of scanpath-augmented models improves as the number of scanpaths increases, reaching its peak at seven scanpaths.3 Importantly, our model outperforms BERT and, when being augmented with five or more synthetic scanpaths, approaches the performance of PLM-AS augmented with human scanpaths.\nLow-Resource Performance We hypothesize that eye gaze might be most beneficial in lowresource settings. To test this hypothesis, we sample a small subset of the training sentences K = {200, 400, 600} from the total number of around 800 training instances, and evaluate the performance of our model augmented with seven syn-\n3The optimal number of scanpaths to be used by the model is considered a hyperparameter for the subsequent experiments.\nthetic scanpaths (the best-performing configuration from the previous experiments). The performance comparison between our model and the baseline model BERT is shown in Figure 3. Our model consistently outperforms BERT, with larger improvements observed when using less training data."
        },
        {
            "heading": "3.3 GLUE Benchmark",
            "text": "In contrast to the small and single task-specific ETSA data set, we extended our evaluation to assess whether gaze data could enhance language models across different tasks, including scenarios with substantial text data. To achieve this, we evaluate our model on the GLUE benchmark, a comprehensive collection of 8 diverse NLP tasks with a large number of text samples. As no eye gaze data is available for GLUE, we focus on the comparison with the BERT baseline, and investigate both, highand low-resource settings.\nHigh-Resource Performance The results of our model on the GLUE test set using all training samples (K = all) are reported in the bottom two rows of Table 2. The results are obtained from the GLUE leaderboard. Our model outperforms BERT in 4\nout of 8 tasks, and achieves comparable performance in 3 tasks. However, our model\u2019s performance is notably poor in the CoLA task, possibly due to the model\u2019s emphasis on gaze sequence ordering, potentially overshadowing the importance of the original word order, which is critical to determine linguistic acceptability of sentences.\nLow-Resource Performance We present the results on the GLUE benchmark with K = {200, 500, 1000} training samples in Table 2. We take additional 1,000 samples from the original training set as the development set used for early stopping. The original development set is utilized for testing. We perform 5 runs with different random seeds to shuffle the data and report the average results.\nOverall, our model consistently outperforms BERT across tasks, except for the STS-B task. In terms of average score, our model shows performance gains of 2-4% compared to BERT."
        },
        {
            "heading": "4 Discussion and Conclusion",
            "text": "We developed a model that integrates synthetic scanpath generation into a scanpath-augmented language model. We observe that the model achieves results that are comparable to a language model augmented with human scanpaths, which eliminates the need for human scanpaths during both training and testing. Human gaze data are only available for a very limited number of NLP tasks and data sets. At application time, under any standard use case scenario of NLP tasks, no gaze recordings are available. Synthetic gaze data not only open the possibility to train high-capacity gaze-augmented models across tasks, which would otherwise require the collection of an impractical large volume of gaze data, but also allow for the\nexploitation of eye gaze signals as model input at application time.\nUsing the GLUE benchmark, we observe that gaze signals show benefits not only for sentiment classification tasks (SST-2), as reported in previous research, but also for entailment classification tasks (MNLI, RTE) and a sentence similarity task (STS-B). This highlights the potential of integrating cognitive signals from eye gaze into a wider range of NLP tasks in the future. Nevertheless, it is evident that not all tasks derive equal benefits from gaze data. It remains up to future research to explore which types of tasks benefit most from gaze signals.\nOur results further show that the potential benefit of augmenting language models with gaze data is higher for low-resource settings. Hence, we believe that the augmentation with gaze data might be particularly interesting for low-resource languages. Two ongoing multi-lab efforts to collect large multilingual eye-tracking-while-reading corpora (MECO4 and MultiplEYE5) include a range of low-resource languages, which will allow for training scanpath generators and augmenting language models with synthetic eye gaze for these languages in the near future.\nLimitations\nOne limitation of our work is that the scanpath generation model Eyettention was pre-trained on eye-tracking data recorded on isolated sentences (single sentence reading paradigm). Since the majority of tasks in the GLUE benchmark involve two-sentence classification, future work could involve pre-training the model on an eye-tracking data set specifically designed for two-sentence reading tasks to enhance its performance. Additionally, scanpath augmentation turned out to be detrimental to the language model\u2019s performance for the task of identifying linguistically acceptable sentences (CoLA). This finding was to be expected as the actual word order is more relevant for linguistic acceptability of a sentence than the order in which the words are fixated. Pre-training the scanpath generator on an eye-tracking corpus that includes both acceptable and unacceptable sentences may be beneficial for improving the model\u2019s performance.\nFurthermore, in our proposed framework, the sampling process involved in scanpath generation\n4https://meco-read.com 5https://multipleye.eu\nduring training and at inference time is not conducive to a high model efficiency. Future work could explore alternative scanpath generation models that do not rely on auto-regressive architectures to improve efficiency.\nEthics Statement\nIt is crucial to acknowledge potential privacy risks in collecting, sharing, and processing human gaze data. Since eye movements are highly individual, it can be possible to extract a participant\u2019s identity from gaze data (J\u00e4ger et al., 2020; Makowski et al., 2021). Other personal information such as gender (Sammaknejad et al., 2017) and ethnicity (Blignaut and Wium, 2014) that can be detected to some degree today may turn out to be extractable accurately in the future, which incurs a risk of leakage of personal information from gaze data. Synthetic gaze data can reduce the need for large-scale experiments with human subjects, even though some amount of human gaze data is still necessary to train generative models."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was partially funded by the German Federal Ministry of Education and Research under grant 01| S20043."
        },
        {
            "heading": "A Model Details",
            "text": "PLA-AS Framework For the PLM-AS framework, we adhere to the design of the original paper (Yang and Hollenstein, 2023). The scanpath encoder consists of a single-direction GRU layer (Cho et al., 2014) with a hidden size of 768 and a dropout rate of 0.1. We initialize the hidden state of the scanpath encoder using the [CLS] token outputs from the final layer of BERT."
        },
        {
            "heading": "B Training Details",
            "text": "We train all neural networks using the PyTorch (Paszke et al., 2019) library on an NVIDIA A100-SXM4-40GB GPU using the NVIDIA CUDA platform. For training, we use the AdamW optimizer (Loshchilov and Hutter, 2019), and a batch size of 32. We train 20 epochs and select the model with the best validation performance for evaluation. The training is early stopped if the validation performance does not increase for 3 consecutive epochs. During the training of our model, we employ the Gumbel-softmax distribution with a temperature hyperparameter set to 0.5. We use the pre-trained checkpoints from the HuggingFace repository (Wolf et al., 2020) for the language model BERTBASE.\nSentiment Classification During training, each scanpath associated with one sentence is treated as a separate instance. However, during evaluation, the pre-softmax logits obtained from multiple scanpaths associated with the same sentence are averaged to generate a single prediction for this sentence. We use a learning rate of 1e-5 for training all investigated models.\nGLUE Benchmark We evaluate each GLUE data set using the metric specified in the benchmark. We use the code provided in the HuggingFace repository 6 to train the BERT model and compute the metrics.\nIn the high-resource setting, we fine-tune the BERT model using the hyperparameter tuning procedure outlined in the original paper (Devlin et al., 2019). We select the best learning rate from {5e-5, 4e-5, 3e-5, 2e-5} for each task based on the performance on the development set. The same learning rate is used for training our model.\nAdditionally, for our model, we perform a hyperparameter search on the development set to determine the optimal number of scanpaths to be used by the model for each task. We explore different numbers of scanpaths from {2, 3, 4} and select the configuration that achieves the best performance on the development set. The optimal configuration for each task can be found in Table 3.\nIn the low-resource setting, we use the same learning rate that was found optimal in the highresource setting for each task. Besides, we perform a hyperparameter search on the development set, investigating different numbers of scanpaths from {3, 5, 7} to be used by our model. The optimal configurations for each task can be found in Table 3.\nTo reduce variance, we apply shuffling to the training data using 5 different random seeds. We use the first K samples as the new training set, and the subsequent 1,000 samples as the development set. The data seeds used for shuffling are {111,222,333,444,555}, while the seed s=42 is consistently used for model training across all models. The procedure was adapted from Mao et al. (2022).\n6https://github.com/huggingface/transformers/ tree/main/examples/pytorch/text-classification"
        }
    ],
    "title": "Pre-Trained Language Models Augmented with Synthetic Scanpaths for Natural Language Understanding",
    "year": 2023
}