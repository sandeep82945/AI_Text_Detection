{
    "abstractText": "How to identify semantic relations among entities in a document when only a few labeled documents are available? Few-shot documentlevel relation extraction (FSDLRE) is crucial for addressing the pervasive data scarcity problem in real-world scenarios. Metric-based meta-learning is an effective framework widely adopted for FSDLRE, which constructs class prototypes for classification. However, existing works often struggle to obtain class prototypes with accurate relational semantics: 1) To build prototype for a target relation type, they aggregate the representations of all entity pairs holding that relation, while these entity pairs may also hold other relations, thus disturbing the prototype. 2) They use a set of generic NOTA (none-of-the-above) prototypes across all tasks, neglecting that the NOTA semantics differs in tasks with different target relation types. In this paper, we propose a relation-aware prototype learning method for FSDLRE to strengthen the relational semantics of prototype representations. By judiciously leveraging the relation descriptions and realistic NOTA instances as guidance, our method effectively refines the relation prototypes and generates task-specific NOTA prototypes. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches by average 2.61% F1 across various settings of two FSDLRE benchmarks.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Shiao Meng"
        },
        {
            "affiliations": [],
            "name": "Xuming Hu"
        },
        {
            "affiliations": [],
            "name": "Aiwei Liu"
        },
        {
            "affiliations": [],
            "name": "Shu\u2019ang Li"
        },
        {
            "affiliations": [],
            "name": "Fukun Ma"
        },
        {
            "affiliations": [],
            "name": "Yawen Yang"
        },
        {
            "affiliations": [],
            "name": "Lijie Wen"
        }
    ],
    "id": "SP:d87c9532b7ba47c36915cc226ae2625c93dba3fa",
    "references": [
        {
            "authors": [
                "Livio Baldini Soares",
                "Nicholas FitzGerald",
                "Jeffrey Ling",
                "Tom Kwiatkowski."
            ],
            "title": "Matching the blanks: Distributional similarity for relation learning",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2895\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "2019. BERT: Pre-training",
            "year": 2019
        },
        {
            "authors": [
                "Zhichao Duan",
                "Xiuxing Li",
                "Zhenyu Li",
                "Zhuo Wang",
                "Jianyong Wang."
            ],
            "title": "Not just plain text! fuel document-level relation extraction with explicit syntax refinement and subsentence modeling",
            "venue": "Findings of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Markus Eberts",
                "Adrian Ulges."
            ],
            "title": "An end-toend model for entity-level relation extraction using multi-instance learning",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Beliz Gunel",
                "Jingfei Du",
                "Alexis Conneau",
                "Veselin Stoyanov."
            ],
            "title": "Supervised contrastive learning for pre-trained language model fine-tuning",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Xuming Hu",
                "Junzhe Chen",
                "Shiao Meng",
                "Lijie Wen",
                "Philip S. Yu."
            ],
            "title": "Selflre: Self-refining representation learning for low-resource relation extraction",
            "venue": "Proceedings of the 46th International ACM SIGIR Conference on Research and Development in",
            "year": 2023
        },
        {
            "authors": [
                "Xuming Hu",
                "Zhaochen Hong",
                "Chenwei Zhang",
                "Aiwei Liu",
                "Shiao Meng",
                "Lijie Wen",
                "Irwin King",
                "Philip S. Yu."
            ],
            "title": "Reading broadly to open your mind improving open relation extraction with search documents under self-supervisions",
            "venue": "IEEE Transactions",
            "year": 2023
        },
        {
            "authors": [
                "Xuming Hu",
                "Aiwei Liu",
                "Zeqi Tan",
                "Xin Zhang",
                "Chenwei Zhang",
                "Irwin King",
                "Philip S. Yu."
            ],
            "title": "GDA: Generative data augmentation techniques for relation extraction tasks",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 10221\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Xuming Hu",
                "Lijie Wen",
                "Yusong Xu",
                "Chenwei Zhang",
                "Philip Yu."
            ],
            "title": "SelfORE: Self-supervised relational feature learning for open relation extraction",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Xuming Hu",
                "Chenwei Zhang",
                "Yawen Yang",
                "Xiaohe Li",
                "Li Lin",
                "Lijie Wen",
                "Philip S. Yu."
            ],
            "title": "Gradient imitation reinforcement learning for low resource relation extraction",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Quzhe Huang",
                "Shibo Hao",
                "Yuan Ye",
                "Shengqi Zhu",
                "Yansong Feng",
                "Dongyan Zhao"
            ],
            "title": "Does recommend-revise produce reliable annotations",
            "year": 2022
        },
        {
            "authors": [
                "Robin Jia",
                "Cliff Wong",
                "Hoifung Poon."
            ],
            "title": "Document-level n-ary relation extraction with multiscale representation learning",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2019
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "Chen Wang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan."
            ],
            "title": "Supervised contrastive learning",
            "venue": "Advances in Neural Information Processing Systems, pages 18661\u201318673.",
            "year": 2020
        },
        {
            "authors": [
                "Shu\u2019ang Li",
                "Xuming Hu",
                "Li Lin",
                "Aiwei Liu",
                "Lijie Wen",
                "Philip S. Yu"
            ],
            "title": "A multi-level supervised contrastive learning framework for low-resource natural language inference",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2023
        },
        {
            "authors": [
                "Aiwei Liu",
                "Xuming Hu",
                "Li Lin",
                "Lijie Wen."
            ],
            "title": "Semantic enhanced text-to-sql parsing via iteratively learning schema linking graph",
            "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, page 1021\u20131030.",
            "year": 2022
        },
        {
            "authors": [
                "Aiwei Liu",
                "Honghai Yu",
                "Xuming Hu",
                "Shu\u2019ang Li",
                "Li Lin",
                "Fukun Ma",
                "Yawen Yang",
                "Lijie Wen"
            ],
            "title": "2022b. Character-level white-box adversarial attacks against transformers via attachable subwords substitution",
            "venue": "In Proceedings of the 2022 Conference on Empiri-",
            "year": 2022
        },
        {
            "authors": [
                "Shuliang Liu",
                "Xuming Hu",
                "Chenwei Zhang",
                "Shu\u2019ang Li",
                "Lijie Wen",
                "Philip Yu"
            ],
            "title": "2022c. HiURE: Hierarchical exemplar contrastive learning for unsupervised relation extraction",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Yi Luan",
                "Luheng He",
                "Mari Ostendorf",
                "Hannaneh Hajishirzi."
            ],
            "title": "Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language",
            "year": 2018
        },
        {
            "authors": [
                "Fukun Ma",
                "Xuming Hu",
                "Aiwei Liu",
                "Yawen Yang",
                "Shuang Li",
                "Philip S. Yu",
                "Lijie Wen."
            ],
            "title": "AMRbased network for aspect-based sentiment analysis",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2023
        },
        {
            "authors": [
                "Youmi Ma",
                "An Wang",
                "Naoaki Okazaki."
            ],
            "title": "DREEAM: Guiding attention with evidence for improving document-level relation extraction",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Yubo Ma",
                "Yixin Cao",
                "YongChing Hong",
                "Aixin Sun"
            ],
            "title": "2023c. Large language model is not a good few-shot information extractor, but a good reranker for hard samples! arXiv preprint arXiv:2303.08559",
            "year": 2023
        },
        {
            "authors": [
                "Mike Mintz",
                "Steven Bills",
                "Rion Snow",
                "Daniel Jurafsky."
            ],
            "title": "Distant supervision for relation extraction without labeled data",
            "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on",
            "year": 2009
        },
        {
            "authors": [
                "Patrick Pantel",
                "Marco Pennacchiotti."
            ],
            "title": "Espresso: Leveraging generic patterns for automatically harvesting semantic relations",
            "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association",
            "year": 2006
        },
        {
            "authors": [
                "jani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Nicholas Popovic",
                "Michael F\u00e4rber."
            ],
            "title": "Few-shot document-level relation extraction",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5733\u20135746.",
            "year": 2022
        },
        {
            "authors": [
                "Meng Qu",
                "Xiang Ren",
                "Yu Zhang",
                "Jiawei Han."
            ],
            "title": "Weakly-supervised relation extraction by patternenhanced embedding learning",
            "venue": "Proceedings of the 2018 World Wide Web Conference, page 1257\u20131266.",
            "year": 2018
        },
        {
            "authors": [
                "Chris Quirk",
                "Hoifung Poon."
            ],
            "title": "Distant supervision for relation extraction beyond the sentence boundary",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages",
            "year": 2017
        },
        {
            "authors": [
                "Ohad Rubin",
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Learning to retrieve prompts for in-context learning",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2022
        },
        {
            "authors": [
                "Ofer Sabo",
                "Yanai Elazar",
                "Yoav Goldberg",
                "Ido Dagan"
            ],
            "title": "Revisiting Few-shot Relation Classification: Evaluation Data and Classification Schemes",
            "year": 2021
        },
        {
            "authors": [
                "Jake Snell",
                "Kevin Swersky",
                "Richard Zemel."
            ],
            "title": "Prototypical networks for few-shot learning",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2017
        },
        {
            "authors": [
                "Qi Sun",
                "Kun Huang",
                "Xiaocui Yang",
                "Pengfei Hong",
                "Kun Zhang",
                "Soujanya Poria."
            ],
            "title": "Uncertainty guided label denoising for document-level distant relation extraction",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational",
            "year": 2023
        },
        {
            "authors": [
                "Qingyu Tan",
                "Ruidan He",
                "Lidong Bing",
                "Hwee Tou Ng."
            ],
            "title": "Document-level relation extraction with adaptive focal loss and knowledge distillation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1672\u20131681.",
            "year": 2022
        },
        {
            "authors": [
                "Qingyu Tan",
                "Lu Xu",
                "Lidong Bing",
                "Hwee Tou Ng",
                "Sharifah Mahani Aljunied."
            ],
            "title": "Revisiting DocRED - addressing the false negative problem in relation extraction",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton."
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research, 9(11).",
            "year": 2008
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Charles Blundell",
                "Timothy Lillicrap",
                "koray kavukcuoglu",
                "Daan Wierstra"
            ],
            "title": "Matching networks for one shot learning",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2016
        },
        {
            "authors": [
                "Somin Wadhwa",
                "Silvio Amir",
                "Byron Wallace."
            ],
            "title": "Revisiting relation extraction in the era of large language models",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15566\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ying Wei",
                "Qi Li."
            ],
            "title": "Sagdre: Sequence-aware graph-based document-level relation extraction with adaptive margin loss",
            "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, page 2000\u20132008.",
            "year": 2022
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Yuxin Xiao",
                "Zecheng Zhang",
                "Yuning Mao",
                "Carl Yang",
                "Jiawei Han."
            ],
            "title": "SAIS: Supervising and augmenting intermediate steps for document-level relation extraction",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Associ-",
            "year": 2022
        },
        {
            "authors": [
                "Yiqing Xie",
                "Jiaming Shen",
                "Sha Li",
                "Yuning Mao",
                "Jiawei Han."
            ],
            "title": "Eider: Empowering document-level relation extraction with efficient evidence extraction and inference-stage fusion",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Benfeng Xu",
                "Quan Wang",
                "Yajuan Lyu",
                "Yong Zhu",
                "Zhendong Mao."
            ],
            "title": "Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Liyan Xu",
                "Jinho Choi."
            ],
            "title": "Modeling task interactions in document-level joint entity and relation extraction",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2022
        },
        {
            "authors": [
                "Tianyu Xu",
                "Wen Hua",
                "Jianfeng Qu",
                "Zhixu Li",
                "Jiajie Xu",
                "An Liu",
                "Lei Zhao."
            ],
            "title": "Evidence-aware document-level relation extraction",
            "venue": "Proceedings of the 31st ACM International Conference on Information & Knowledge Management, page 2311\u20132320.",
            "year": 2022
        },
        {
            "authors": [
                "Wang Xu",
                "Kehai Chen",
                "Tiejun Zhao."
            ],
            "title": "Discriminative reasoning for document-level relation extraction",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1653\u20131663.",
            "year": 2021
        },
        {
            "authors": [
                "Yawen Yang",
                "Xuming Hu",
                "Fukun Ma",
                "Shu\u2019Ang Li",
                "Aiwei Liu",
                "Lijie Wen",
                "Philip S. Yu"
            ],
            "title": "Gaussian prior reinforcement learning for nested named entity recognition",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal",
            "year": 2023
        },
        {
            "authors": [
                "Yuan Yao",
                "Deming Ye",
                "Peng Li",
                "Xu Han",
                "Yankai Lin",
                "Zhenghao Liu",
                "Zhiyuan Liu",
                "Lixin Huang",
                "Jie Zhou",
                "Maosong Sun."
            ],
            "title": "DocRED: A large-scale document-level relation extraction dataset",
            "venue": "Proceedings of the 57th Annual Meeting of the Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Junjie Ye",
                "Xuanting Chen",
                "Nuo Xu",
                "Can Zu",
                "Zekai Shao",
                "Shichun Liu",
                "Yuhan Cui",
                "Zeyang Zhou",
                "Chao Gong",
                "Yang Shen"
            ],
            "title": "A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. arXiv preprint arXiv:2303.10420",
            "year": 2023
        },
        {
            "authors": [
                "Jiaxin Yu",
                "Deqing Yang",
                "Shuyu Tian."
            ],
            "title": "Relation-specific attentions over entity mentions for enhanced document-level relation extraction",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Shuang Zeng",
                "Runxin Xu",
                "Baobao Chang",
                "Lei Li."
            ],
            "title": "Double graph based reasoning for documentlevel relation extraction",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1630\u20131640.",
            "year": 2020
        },
        {
            "authors": [
                "Ningyu Zhang",
                "Xiang Chen",
                "Xin Xie",
                "Shumin Deng",
                "Chuanqi Tan",
                "Mosha Chen",
                "Fei Huang",
                "Luo Si",
                "Huajun Chen."
            ],
            "title": "Document-level relation extraction as semantic segmentation",
            "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial",
            "year": 2021
        },
        {
            "authors": [
                "Ruoyu Zhang",
                "Yanzeng Li",
                "Lei Zou."
            ],
            "title": "A novel table-to-graph generation approach for documentlevel joint entity and relation extraction",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Yuhao Zhang",
                "Peng Qi",
                "Christopher D. Manning."
            ],
            "title": "Graph convolution over pruned dependency trees improves relation extraction",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2205\u20132215.",
            "year": 2018
        },
        {
            "authors": [
                "Wenxuan Zhou",
                "Kevin Huang",
                "Tengyu Ma",
                "Jing Huang."
            ],
            "title": "Document-level relation extraction with adaptive thresholding and localized context pooling",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, pages 14612\u201314620.",
            "year": 2021
        },
        {
            "authors": [
                "Peter Hans"
            ],
            "title": "Kolvenbach, it was not the norm for a Jesuit Superior General to resign; they, like the great majority of the Popes up until Benedict XVI, generally served until death. However, the Jesuit constitutions include provision for a resignation",
            "venue": "In October 2016 the thirty-sixth General Congregation of the Society of Jesus appointed his successor,",
            "year": 2016
        },
        {
            "authors": [
                "Reverend Joseph Brewster",
                "Sarah Jane Bunce Brewster"
            ],
            "title": "Roman Catholic Church> Query Document: Chauncey Bunce Brewster (September 5, 1848 \u2013 April 9, 1941 ) was the fifth Bishop of the Episcopal Diocese of Connecticut",
            "venue": "Brewster was born in Windham, Connecticut,",
            "year": 1941
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Document-level relation extraction (DocRE) aims to identify the relations between each pair of entities within a document, which is crucial for extracting complex cross-sentence relations and implementing large-scale information extraction (Zhou et al., 2021; Xie et al., 2022; Wei and Li, 2022; Sun et al., 2023). However, the annotation of DocRE data is both time-consuming and labor-intensive,\n\u2217Corresponding author. 1The data and code are available at https://github.com/\nTHU-BPM/RAPL.\nand many specific domains often lack annotated documents, making data scarcity a common issue in real-world scenarios. This motivates us to explore few-shot document-level relation extraction (FSDLRE) (Popovic and F\u00e4rber, 2022). We illustrate an example of the FSDLRE task under the 1- Doc setting in Figure 1, where only one annotated support document is given along with three target relation types: Place of Birth, Work Location and Place of Death. The task is to predict all instances of the target relation types for pre-given entities in the query document, such as (Grace Taylor, Place of Birth, Brisbane).\nCurrent efforts on FSDLRE (Popovic and F\u00e4rber, 2022) mainly adopt the popular metric-based metalearning framework (Vinyals et al., 2016; Snell et al., 2017), which aims to learn a metric space in which classification can be performed by computing distances to prototype representations of each class. By training on a collection of sampled FSDLRE tasks, the model learns general knowledge for FSDLRE, enabling rapid generalization to new tasks with novel relation types.\nIdeally, within the metric-based paradigm, prototype representations should accurately capture\nthe relational semantics of each category. However, this can be challenging for existing FSDLRE methods: (1) Considering that an entity pair may express multiple relations in a document, if a relation prototype is obtained by aggregating the representations of entity pairs in the support set holding that relation, the relational semantics of the prototype is inevitably disturbed by irrelevant relations, thus affecting the discriminability of the metric space, as depicted in Figure 2(a). (2) Since most query entity pairs do not express any target relation, NOTA (none-of-the-above) is also considered as a category. Given that the target relation types vary for different tasks, if we merely introduce a set of learnable vectors as NOTA prototypes and apply them to all tasks, this \u201cone-size-fits-all\u201d strategy could result in the NOTA prototypes deviating from ideal NOTA semantics in certain tasks, thereby confusing the classification. As shown in Figure 2(a), the set of generic NOTA prototypes seems reasonable for task 1, while does not work well for task 2.\nTo address the two aforementioned issues in FSDLRE, we propose a novel Relation-Aware Prototype Learning method (RAPL). First, for each entity pair that holds relations in the support document, we leverage the inherent relational semantics in relation descriptions as guidance, deriving an instance-level representation for each expressed relation, as illustrated in Figure 2(b). The relation prototype is constructed by aggregating the representations of all its support relation instances, thus better focusing on relation-relevant information. Based on the instance-level support embeddings, we propose a relation-weighted contrastive learning method to further refine the prototypes. By incorporating inter-relation similarities into a contrastive objective, we can better distinguish the prototypes of semantically-close relations. Moreover, we design a task-specific NOTA prototype generation strategy. For each task, we adaptively select support NOTA instances and fuse them into a set of learnable base NOTA prototypes to generate taskspecific NOTA prototypes, which more effectively capture the NOTA semantics in each task.\nIn summary, our main contributions are as follows: (1) We propose a novel relation-aware prototype learning method (RAPL) for FSDLRE, which effectively enhances the relational semantics of prototype representations. (2) In RAPL, we reframe the construction of relation prototypes into instance level and further propose a relation-weighted con-\nTask 1\n(a) Previous Methods (b) Our Method\nTask 1\nTask 2Task 2\nSupport Relation Instance EmbeddingSupport Entity Pair Embedding Relation Prototype Generic NOTA Prototype Relation Prototype Task-specific NOTA Prototype\nFigure 2: Embedding space illustration of previous methods (left) and our method (right). Task 1&2 are two FSDLRE tasks with different target relation types.\ntrastive learning method to jointly refine the relation prototypes. We also design a task-specific NOTA prototype generation strategy to better capture the NOTA semantics in each task. (3) Experiments demonstrate that our method outperforms state-of-the-art baselines by average 2.61% in F1 across various settings of two FSDLRE benchmarks."
        },
        {
            "heading": "2 Problem Formulation",
            "text": "Few-shot document-level relation extraction is defined with an N -Doc setting (Popovic and F\u00e4rber, 2022). In each individual FSDLRE task (also called an episode2), there are a set of N support documents {DS,1, ..., DS,N} and a query document DQ, and the entity mentions in each document are pre-annotated. For each support document DS,i, there is also a triple set TS,i containing all valid (eh, r, et) triples in the document. Here eh and et are the head and tail entity of a relation instance, and r \u2208 Repisode is a relation type, with Repisode being the relation type set for which instances are to be extracted. The annotations of support documents are complete, which means any entity pair for which no relation type has been assigned can be considered as NOTA. Given these as inputs, the FSDLRE task aims to predict the triple set TQ for the query document DQ, which contains all valid triples in DQ of relation types in Repisode.\nOur approach follows the typical meta-learning 2In this paper, we use the terms \u201ctask\u201d and \u201cepisode\u201d inter-\nchangeably, which refer to the same concept.\nparadigm. In training phase, we construct a group of training episodes by sampling support and query documents from a training document corpus Ctrain. The set Repisode of each training episode is a subset of Rtrain, a relation type set for meta-training. The model aims to learn general knowledge from these training tasks to better generalize to novel tasks. In test phase, the model is evaluated on a group of test episodes sampled from a test document corpus Ctest, which is disjoint with Ctrain. The set Repisode of each test episode is a subset of Rtest, a relation type set for meta-testing, which is also disjoint with Rtrain."
        },
        {
            "heading": "3 Methodology",
            "text": "The overall architecture of RAPL is illustrated in Figure 3. We first introduce the encoding procedure for documents and entities in Section 3.1. In Section 3.2 and Section 3.3, we elaborate on the learning of relation-aware relation prototypes and NOTA prototypes respectively. The training and inference processes are finally given in Section 3.4."
        },
        {
            "heading": "3.1 Document and Entity Encoding",
            "text": "We employ the pre-trained language model (Devlin et al., 2019) as the document encoder to encode each support or query document in a given episode. For each document D, we first insert a special token \u201c\u2217\u201d at the start and end of each entity mention to mark the position of entity mentions. Then we feed the document into the encoder to obtain the contextualized token embeddings H and cross token attention A:\nH,A = DocEncoder(D), (1)\nwhere H = [h1, . . . ,hNt ] \u2208 RNt\u00d7d, Nt is the number of tokens in D, d is the output dimension of encoder, and A = [a1, . . . ,aNt ] \u2208 RNt\u00d7Nt is the average of attention heads in the last encoder layer. We take the embedding of \u201c\u2217\u201d before each entity mention as the corresponding mention embedding. For an entity ei mentioned Nei times in the document via Mei = {mij} Nei j=1, we apply logsumexp pooling (Jia et al., 2019) over mention embeddings to obtain the entity embedding hei \u2208 Rd: hei = log \u2211Nei j=1 exp(hmij ), where hmij \u2208 R d is the embedding of ei\u2019s j-th mention."
        },
        {
            "heading": "3.2 Relation-Aware Relation Prototype Learning",
            "text": "For each target relation type in a given episode, we aim to obtain a prototype representation that can better capture the corresponding relational semantics. To this end, we first propose to construct the relation prototypes based on instance-level support embeddings, enabling each prototype to focus more on relation-relevant information in support documents. Then we propose an instance-level relationweighted contrastive learning method, which further refines the relation prototypes."
        },
        {
            "heading": "3.2.1 Instance-Based Prototype Construction",
            "text": "Given a relation instance (eh, r, et) in a support document, we first compute a pair-level importance distribution a(h,t) \u2208 RNt over all tokens in the document to capture the context relevant to the entity pair (eh, et) (Zhou et al., 2021):\na(h,t) = aeh \u2299 aet aTehaet , (2)\nwhere aeh \u2208 RNt is an entity-level attention obtained by averaging the mention-level attention amhi \u2208 RNt at the token \u201c\u2217\u201d before eh\u2019s each mention mhi : aeh = 1\nNeh \u2211Neh i=1 amhi\n, likewise for aet , and \u2299 is the Hadamard product. Meanwhile, we compute a relation-level attention distribution ar \u2208 RNt over all tokens to capture the context relevant to relation r. We employ another pre-trained language model as the relation encoder, and concatenate the name and description of relation r into a sequence, then feed the sequence into the encoder. We take the output embedding of \u201c[CLS]\u201d token as the relation embedding hr \u2208 Rd:\nhr = RelEncoder(r), (3)\nand compute the relation-level attention ar as:\nar = softmax( HWhr\u221a\nd ), (4)\nwhere W \u2208 Rd\u00d7d is a learnable parameter. Based on a(h,t) and ar, we further compute an instance-level attention distribution a(h,r,t) \u2208 RNt over all tokens to capture the context relevant to the instance. Specifically, the value a(h,r,t)i at i-th dimension of a(h,r,t) is obtained by:\na (h,r,t) i = a (h,t) i + I\n( i \u2208 top-k%(a(h,t)\u2299ar) ) \u00b7ari ,\n(5) where top-k%(x) returns the indices of the largest k% elements of x, k is a hyperparameter, and I is the indicator function. We also normalize a(h,r,t) to regain the attention distribution. Here we do not use a(h,t) \u2299 ar as the instance-level attention because, for an instance, the relation is expressed based on the entity pair. Multiplying them directly may erroneously increase the weight of tokens unrelated to the entity pair. Instead, we leverage relation-level attention to amplify the pair-level weight of the most relevant context with the instance.\nThen, we compute an instance context embedding c(h,r,t) \u2208 Rd by:\nc(h,r,t) = HTa(h,r,t), (6)\nand fuse it into the embeddings of head entity and tail entity to obtain the instance-aware entity representations z(h,r,t)h , z (h,r,t) t \u2208 Rd:\nz (h,r,t) h = tanh(W h[heh ; c (h,r,t)] + bh), (7)\nz (h,r,t) t = tanh(W t[het ; c (h,r,t)] + bt), (8)\nwhere W h,W t \u2208 Rd\u00d72d, bh, bt \u2208 Rd are learnable parameters. The instance representation of (eh, r, et) is then obtained by concatenating the head and tail entity representations, which we denote as s(h,r,t) = [z(h,r,t)h ; z (h,r,t) t ] \u2208 R2d.\nFinally, denoting the set of all instances of relation r in support documents as Sr, we compute the relation prototype pr \u2208 R2d by averaging the representations of relation instances in Sr:\npr = 1 |Sr| \u2211\n(eh,r,et)\u2208Sr\ns(h,r,t). (9)"
        },
        {
            "heading": "3.2.2 Contrastive-Based Prototype Refining",
            "text": "By reframing the construction of relation prototypes into instance level, each prototype can better focus on relation-relevant support information. However, due to the complexity of document context, different instances of the same relation may exhibit varying patterns in expressing the relation, making it difficult for prototypes to capture the common relational semantics. Additionally, limited support instances make it challenging for prototypes of semantically-close relations to capture their deeper semantic differences. Therefore, we propose a relation-weighted contrastive learning method to further refine the relation prototypes.\nSpecifically, given an episode, we denote the set of all relation instances in support documents as S, i.e., S = \u22c3 r\u2208Repisode Sr. Also, for a relation instance (eh, r, et), we define the set Ph,r,t = Sr \\{(eh, r, et)} which contains all other instances in the support set that also express the relation r, and the set Ah,r,t = S \\ {(eh, r, et)} which simply contains all other instances in the support set. Then we incorporate inter-relation similarities into a contrastive objective and define the relation-weighted contrastive loss LRCL as:\nLRCL = 1 |S| \u2211\n(eh,r,et)\u2208S\n\u22121 |Ph,r,t| \u2211 (eh\u0304,r,et\u0304)\u2208Ph,r,t\nexp(s(h,r,t) \u00b7 s(h\u0304,r,t\u0304)/\u03c4)\u2211 (eh\u0302,r\u0302,et\u0302)\u2208Ah,r,t \u03c9r,r\u0302 \u00b7 exp(s(h,r,t) \u00b7 s(h\u0302,r\u0302,t\u0302)/\u03c4) ,\n(10)\n\u03c9r,r\u0302 = 1 + I(r \u0338= r\u0302) \u00b7 cossim(hr,hr\u0302) + 1\n2 , (11)\nwhere \u03c4 is a hyperparameter and cossim denotes the cosine similarity. We argue that the proposal of this contrastive loss is non-trivial, considering\ntwo aspects. First, it is difficult for previous methods to integrate with contrastive objective as they only obtain the pair-level support embeddings. The multi-label nature of entity pairs makes it difficult to define positive and negative pairs reasonably. Moreover, by incorporating inter-relation similarities, the proposed contrastive loss focuses more on pushing apart the instance embeddings of semantically-close relations, thus helping to better distinguish the corresponding relation prototypes."
        },
        {
            "heading": "3.3 Relation-Aware NOTA Prototype Learning",
            "text": "Since most query entity pairs do not hold any target relation, NOTA is also treated as a class. Existing methods typically learn a set of generic NOTA prototypes that are applied to all tasks, which may not be optimal in certain tasks since the NOTA semantics differs in tasks with different target relation types. To this end, we propose a task-specific NOTA prototype generation strategy to better capture the NOTA semantics in each individual task.\nConcretely, we first introduce a set of learnable vectors Nbase = {pbasei \u2208 R2d} Nnota i=1 , where Nnota is a hyperparameter. Unlike previous works that directly treat this set of vectors as NOTA prototypes, we regard them as base NOTA prototypes that need further rectification in each task. Since the annotation of support documents are complete, we have access to a support NOTA distribution which implicitly expresses the NOTA semantics. Therefore we resort to support NOTA instances to capture the NOTA semantics in each specific task. For a support NOTA instance (eh, nota, et), we use Equation 2 as the instance-level attention and obtain the instance representation s(h,nota,t) = [z\n(h,nota,t) h ; z (h,nota,t) t ] \u2208 R2d based on Equation 6~8. Denoting the set of all support NOTA instances as Snota, we adaptively select a NOTA instance for each base NOTA prototype pbasei :\n(eh, nota, et) = argmax (eh,nota,et)\u2208Snota\n(s(h,nota,t) \u00b7 pbasei\n\u2212 max r\u2208Repisode s(h,nota,t) \u00b7 pr),\n(12) which locates the NOTA instance that is close to the base NOTA prototype while being far away from relation prototypes. Then we fuse it into pbasei to obtain the final NOTA prototype pnotai \u2208 R2d:\npnotai = \u03b1p base i + (1\u2212 \u03b1)s(h,nota,t), (13)\nwhere \u03b1 is a hyperparameter. In this way, we obtain a set of task-specific NOTA prototypes which not only contain the general knowledge from metalearning but also implicitly capture the NOTA semantics in each specific task."
        },
        {
            "heading": "3.4 Training Objective",
            "text": "Given an entity pair (eh, et) in the query document, we use Equation 2 as the pair-level attention and adopt a similar approach as Equation 6~8 to obtain the pair representation q(h,t) = [z(h,t)h ; z (h,t) t ] \u2208 R2d. For each target relation type r in the episode, we compute the probability of r as:\nP (h,t)r = exp(q(h,t) \u00b7 pr)\nexp(q(h,t)\u00b7pr)+max i\u2208I\nexp(q(h,t)\u00b7pnotai ) ,\n(14) where I = {1, ..., Nnota}. Then, denoting the set of all entity pairs in the query document as Q, we compute the classification loss as:\nLBCE = 1 |Q| \u2211\n(eh,et)\u2208Q\n\u2212 \u2211\nr\u2208Repisode( y(h,t)r log(P (h,t) r ) + (1\u2212 y(h,t)r )log(1\u2212 P (h,t)r ) ) , (15) where y(h,t)r = 1 if r exists between (eh, et), otherwise y(h,t)r = 0. The overall loss is defined as:\nL = LBCE + \u03bbLRCL, (16)\nwhere \u03bb is a hyperparameter. During inference, we extract the relation instance (eh, r, et) in the query document if q(h,t) \u00b7 pr > max\ni\u2208I q(h,t) \u00b7 pnotai ."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Benchmarks and Evaluation Metric",
            "text": "We conduct experiments on the public FSDLRE benchmark FREDo (Popovic and F\u00e4rber, 2022), and also construct ReFREDo, a revised version\nof FREDo which resolves the annotation errors, enabling more reliable evaluation.\nFREDo consists of two main tasks, an in-domain and a cross-domain task. For in-domain tasks, the training and test document corpus are from the same domain. For cross-domain tasks, the test documents are taken from a different domain, leading to wider disparities of text style, document topic and relation types between training and test. Each task has a 1-Doc and a 3-Doc subtask to measure the scalability of models. FREDo uses the training set of DocRED (Yao et al., 2019) as the training and development document corpus, the development set of DocRED as the in-domain test document corpus, and the whole set of SciERC (Luan et al., 2018) as the cross-domain test document corpus. The relation type set of DocRED is split into 3 disjoint sets for training (62), development (16) and in-domain test (18) in FREDo. FREDo samples 15k episodes for in-domain evaluation and 3k episodes for cross-domain evaluation.\nConsidering that FREDo uses DocRED, which suffers from the problem of incomplete annotation (Huang et al., 2022; Tan et al., 2022b), as the underlying document corpus, the episodes constructed in FREDo may also inherit these annotation errors. Therefore, we construct ReFREDo as a revised version of FREDo. In ReFREDo, we replace the training, development and in-domain test document corpus as Re-DocRED (Tan et al., 2022b), a revised version of DocRED with more complete annotations. Then we follow the same split of relation types as FREDo and sample 15k episodes for indomain evaluation. The cross-domain test episodes remain the same with FREDo. We also follow Popovic and F\u00e4rber (2022) to calculate the average class number N and average support instance number per class K across test episodes in ReFREDo, as shown in Table 1. An overview of the relation types and total instance number per relation of two\nbenchmarks is listed in Appendix A. Following Popovic and F\u00e4rber (2022), we use macro F1 to evaluate the overall performance."
        },
        {
            "heading": "4.2 Baselines",
            "text": "We compare our method with four baselines of FREDo (Popovic and F\u00e4rber, 2022): DL-Base is an initial baseline which uses the pre-trained language model without fine-tuning. DL-MNAV is a metricbased approach built upon the state-of-the-art supervised DocRE method (Zhou et al., 2021) and few-shot sentence-level relation extraction method (Sabo et al., 2021). DL-MNAVSIE uses all individual support entity pairs during inference instead of averaging their embeddings into a single prototype to improve DL-MNAV for cross-domain tasks. DL-MNAVSIE+SBN uses NOTA instances as additional NOTA prototypes during training and only uses NOTA instances during inference to further improve DL-MNAVSIE for cross-domain tasks. Besides, we also evaluate the supervised DocRE model by learning on the whole training corpus and fine-tuning on the support set. Here we choose KDDocRE (Tan et al., 2022a) which is the state-ofthe-art public-available supervised DocRE method. For a fair comparison, we follow Popovic and F\u00e4rber (2022) to use BERT-base (Devlin et al., 2019) as the encoder in our approach. We present the implementation details in Appendix B."
        },
        {
            "heading": "4.3 Main Results",
            "text": "The main results on FREDo and ReFREDo are shown in Table 2. We have following observations from the experimental results: (1) RAPL achieves significantly better average results on two benchmarks compared to baseline approaches (2.50% F1 on FREDo and 2.72% F1 on ReFREDo), demonstrating the superiority of our method. (2) RAPL consistently outperforms the best baseline method (which varies in different task settings) in each task\nModel / F1 In-Domain Cross-Domain\nsetting, making it more versatile than previous approaches. (3) RAPL shows more improvements on in-domain tasks compared to cross-domain tasks. This further reflects the greater challenge posed by cross-domain settings. (4) The performance of RAPL on 3-Doc tasks is consistently higher than that on 1-Doc tasks, which is not always guaranteed for the best baseline method, demonstrating the better scalability of RAPL. (5) The in-domain performance of all methods on ReFREDo is significantly higher than that on FREDo, while this performance gap is not reflected between two benchmarks under the cross-domain setting. This suggests that a higher-quality training set may not effectively resolve the domain adaption problem. (6) The performance of KDDocRE is not satisfactory, indicating that the supervised DocRE method may not adapt well to few-shot scenarios."
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "We conduct an ablation study on ReFREDo to investigate the influence of each module in our method. Specifically, for \u201c\u2212RCL\u201d, we remove the relation-weighted contrastive learning method; for \u201c\u2212IBPC\u2212RCL\u201d, we further remove the instancebased relation prototype construction method, and only obtain the pair-level embedding for each support entity pair in the same way as query entity pairs; for \u201c\u2212IBPC\u2212RCL+SCL\u201d, we add a supervised contrastive learning objective (Khosla et al., 2020; Gunel et al., 2021) into the \u201c\u2212IBPC\u2212RCL\u201d model, where we treat those entity pairs sharing common relations as positive pairs, else as negative pairs; for \u201c\u2212TNPG\u201d, we remove the task-specific NOTA prototype generation strategy, and directly treat the base NOTA prototypes as final NOTA prototypes. The average results are shown in Table 3. We can observe that the performance of model \u201c\u2212RCL\u201d and \u201c\u2212TNPG\u201d drops to varying degrees compared to RAPL, and the model \u201c\u2212IBPC\u2212RCL\u201d performs even worse than \u201c\u2212RCL\u201d, demonstrating the effectiveness of each module in our method. Be-\nsides, integrating the contrastive objective at pairlevel do not bring significant improvements, which indicates the importance of learning instance-level support embeddings."
        },
        {
            "heading": "4.5 Analyses and Discussions",
            "text": "Effect of Hyperparameters. We investigate the impact of different hyperparameters on the performance of our approach. We conduct the experiments on 3-Doc tasks in ReFREDo. As shown in Figure 4, we can observe that: (1) For the hyperparameter k which controls the derivation of instancelevel attention, the best value for in-domain tasks are larger than cross-domain tasks, which may be related to the longer document in in-domain corpus. (2) An appropriate temperature hyperparameter \u03c4 (around 0.4) in the contrastive objective is crucial for the synergy with classification objective and the overall model performance. (3) Blindly reducing the hyperparameter \u03b1 to increase the weight of support NOTA instances in NOTA prototypes may have a negative impact on the learning of NOTA prototypes. (4) Compared to other hyperparameters, the model is not very sensitive to the number of NOTA prototypes Nnota within a certain range.\nSupport Embeddings Visualization. To intuitively illustrate the advantage of our proposed method, we select three semantically-close relation types from the in-domain 3-Doc test corpus of ReFREDo and sample ten support instances for each relation type, then use t-SNE for visualization (Van der Maaten and Hinton, 2008), as shown in Figure 5. Apart from two model variants in ablation study, we also experiment with RAPL\u2212RCL+SCL, which replaces the relationweighted contrastive loss with the supervised con-\ntrastive loss (Khosla et al., 2020; Gunel et al., 2021) at instance level. Since some entity pairs express both the \u201cpart of\u201d and \u201cmember of\u201d relation, or both the \u201cpart of\u201d and \u201csubclass of\u201d relation, we only visualize \u201cpart of\u201d relation for RAPL\u2212IBPC\u2212RCL in Figure 5(a). We can observe that the support instance embeddings learned by RAPL\u2212RCL improve the support pair embeddings learned by RAPL\u2212IBPC\u2212RCL, demonstrating the effectiveness of instance-level embeddings for relation prototype construction. Besides, although incorporating instance-level supervised contrastive objective forms more compact clusters, the distinction among three relation types is still insufficient. As shown in Figure 5(d), our proposed relation-weighted contrastive learning method better distinguishes the three relation types.\nPerformance vs. NOTA Rate of Episodes. We further explore the impact of task-specific NOTA prototype generation strategy on the performance improvements. We divide the in-domain 3-Doc test episodes of ReFREDo into disjoint subsets according to the NOTA rate of each episode, i.e., the proportion of NOTA entity pairs to the total number of entity pairs in the query document of an episode. We establish four subsets, corresponding to the\nscenarios where the NOTA rate falls within [0%, 95%), [95%, 97%), [97%, 99%) and [99%, 100%], respectively. Then we evaluate models trained with or without task-specific NOTA prototype generation strategy on each subset. The experiment results are shown in Table 4. It is observed that the task-specific NOTA prototype generation strategy has brought improvement on each subset. More importantly, the performance gain gets larger as the NOTA rate increases. It demonstrates that the task-specific NOTA prototype generation strategy conduces to the capture of NOTA semantics for derived NOTA representations, especially in those episodes with more NOTA query pairs involved.\nPerformance vs. Number of Support Relation Instances. We also analyze the effect of the number of support relation instances on the overall performance. We conduct the experiments on indomain 3-Doc tasks in ReFREDo benchmark. For each relation type in each test episode, we calculate the number of support instances of that relation type in the episode. Here we divide the number of support instances into 10 categories, where the first 9 categories correspond to 1 to 9, and the last category corresponds to cases where the number of support instances is greater than or equal to 10. Then we evaluate the performance of RAPL method on each of these categories, as shown in Figure 6. We can observe that the performance of RAPL generally exhibits an upward trend as the number of support relation instances increases, while fluctuations also appear at certain points. This indicates that the proposed method demonstrates a certain level of scalability, but the performance may not be perfectly positively correlated with the number of support relation instances.\nPreliminary Exploration of LLM for FSDLRE. Recently large language models (LLM) (Brown et al., 2020; Touvron et al., 2023) have achieved promising results in many few-shot tasks through in-context learning (Wei et al., 2022; Rubin et al., 2022). Also some works focus on leveraging LLM to solve few-shot information extraction problems (Ma et al., 2023c; Ye et al., 2023; Wadhwa et al., 2023). However, most studies mainly target sentence-level tasks. Therefore, we conduct a preliminary experiment using gpt-3.5-turbo3 to explore the performance of LLM on FSDLRE tasks. Due to the input length limit, we only experiment on 1-Doc setting. We randomly select 1000 episodes from the in-domain test episodes of ReFREDo and design an in-context learning prompt template that includes task description, demonstration and query (detailed in Appendix C). The experimental results show that gpt-3.5-turbo achieves only 12.98% macro F1, even lower than some baseline methods. Although the test may not fully reflect the capabilities of LLM, we argue that FSDLRE remains a challenging problem even in the era of LLM."
        },
        {
            "heading": "5 Related Work",
            "text": "Sentence-Level Relation Extraction. Relation extraction is a pivot task of information extraction (Hu et al., 2021, 2023b; Yang et al., 2023). Early studies mainly focus on predicting the relation between two entities within a single sentence. A variety of pattern based (Pantel and Pennacchiotti, 2006; Mintz et al., 2009; Qu et al., 2018) and neural based (Zhang et al., 2018; Baldini Soares et al., 2019; Hu et al., 2020; Liu et al., 2022c) models have achieved satisfactory results. Nevertheless, sentence-level relation extraction has significant limitations in terms of extraction scope and scale. The demand for cross-sentence and large-scale relation extraction has led to a surge of research interest in document-level relation extraction (Quirk and Poon, 2017; Yao et al., 2019).\nDocument-Level Relation Extraction. Most of existing DocRE studies are grounded on a datadriven supervised scenario, and can be generally categorized into graph-based and sequence-based approaches. Graph-based methods (Zeng et al., 2020; Xu et al., 2021b; Zhang et al., 2021; Xu et al., 2022; Duan et al., 2022) typically abstract\n3https://platform.openai.com/docs/models/ gpt-3-5\nthe document by graph structures and perform inference with graph neural networks. Sequence-based methods (Xu et al., 2021a; Tan et al., 2022a; Yu et al., 2022; Xiao et al., 2022; Ma et al., 2023b) encode the long-distance contextual dependencies with transformer-only architectures. Both categories of methods have achieved impressive results in DocRE. However, the reliance on large-scale annotated documents makes these methods difficult to adapt to low-resource scenarios (Li et al., 2023; Hu et al., 2023a).\nFew-Shot Document-Level Relation Extraction. To tackle the data scarcity problem prevalent in real-world DocRE scenarios, Popovic and F\u00e4rber (2022) formulate DocRE into a few-shot learning task. To accomplish the task, they propose multiple metric-based models built upon the stateof-the-art supervised DocRE method (Zhou et al., 2021) and few-shot sentence-level relation extraction method (Sabo et al., 2021), aiming to address different task settings. We note that for an effective metric-based FSDLRE method, the prototype of each class should accurately capture the corresponding relational semantics. However, this can be challenging for existing methods due to their coarse-grained relation prototype learning strategy and \u201cone-for-all\u201d NOTA prototype learning strategy. In this work, we propose a relation-aware prototype learning method to better capture the relational semantics for prototype representations."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose RAPL, a novel relationaware prototype learning method for FSDLRE. We reframe the construction of relation prototypes into instance level and further propose a relationweighted contrastive learning method to jointly refine the relation prototypes. Moreover, we design a task-specific NOTA prototype generation strategy to better capture the NOTA semantics in each task. Experiment results and further analyses demonstrate the superiority of our method and effectiveness of each component. For future work, we would like to transfer our method to other few-shot document-level IE tasks such as few-shot document-level event argument extraction, which shares similar task structure with FSDLRE.\nLimitations\nFirstly, the incorporation of relation encoder and the search process for support NOTA instances add\nto both memory and time expenses. This motivates us to further refine the overall efficiency of our proposed method. Secondly, the assumption that the entity information should be specified may affect the robustness of the method (Liu et al., 2022b). We have noticed that in supervised scenarios, some recent DocRE studies explore the joint entity and relation extraction to circumvent this assumption (Eberts and Ulges, 2021; Xu and Choi, 2022; Zhang et al., 2023). We believe it is beneficial to investigate end-to-end DocRE in few-shot scenarios, where the RAPL method may shed some lights on future work. Lastly, the performance gain of RAPL on cross-domain tasks is lower than that on in-domain tasks. An intriguing avenue for future research is to explore techniques for better performance on cross-domain tasks, e.g., data augmentation (Hu et al., 2023c) and structured knowledge guidance (Liu et al., 2022a; Ma et al., 2023a)."
        },
        {
            "heading": "Acknowledgements",
            "text": "We sincerely thank the anonymous reviewers for their valuable comments. The work was supported by the National Key Research and Development Program of China (No. 2019YFB1704003), the National Nature Science Foundation of China (No. 62021002), Tsinghua BNRist and Beijing Key Laboratory of Industrial Bigdata System and Application."
        },
        {
            "heading": "A Relation Types in Benchmarks",
            "text": "In Table 5~9, we list the relation types of training, development, in-domain test and cross-domain test document corpus in FREDo and ReFREDo. We present the name, description and total instance number of each relation type.\nB Implementation Details\nWe implement our method with Pytorch (Paszke et al., 2019) and Huggingface\u2019s Transformers (Wolf et al., 2020). We use AdamW (Loshchilov and Hutter, 2019) for optimization with a linear warmup for the first 4% steps followed by a linear decay to 0. We train the model for 50k episodes and perform early stopping based on the macro F1 on the\ndevelopment set. We take the learning rate as 1e-5. The episode number per batch during training is set to 4. We clip the gradients to a max norm of 1.0. The hyperparameters k, \u03c4 , Nnota, \u03b1 and \u03bb are set to 15, 0.4, 15, 0.9 and 0.1 for in-domain tasks, 10, 0.4, 20, 0.95 and 0.1 for cross-domain tasks. All hyperparameters are tuned on the development set. We report the mean and standard deviation of macro F1 by five training trials with different random seeds. All experiments are conducted with one Tesla V100-32G GPU. For the baseline results on ReFREDo, we reimplement all baseline models with official public codes for comparison.\nC In-Context Learning Prompt Template for 1-Doc FSDLRE Tasks\nIn-context learning prompt template for 1-Doc FSDLRE tasks:\nGiven a target relation type list, a document, and all entity mentions of each entity in the document, please identify all valid given relation types between any two given entities in the document.\nTarget relation type names and descriptions: <Relation Name 1>: <Relation Description 1> <Relation Name 2>: <Relation Description 2> ...... Document (each entity mention is enclosed by the ID of the entity): <Document> ID and mentions of each entity in the document: [1]: <Mention 1 of Entity 1>; <Mention 2 of Entity 1>; ...... [2]: <Mention 1 of Entity 2>; <Mention 2 of Entity 2>; ...... ...... All non-duplicate valid \u201csubject entity\u201d-\u201crelation type\u201d-\u201cobject entity\u201d triples in the document (output format: \u201centity ID\u201d-\u201crelation type name\u201d\u201centity ID\u201d, e.g., [1]-country-[2]; one triple per line):\n[<Entity ID>]-<Relation Name>-[<Entity ID>] [<Entity ID>]-<Relation Name>-[<Entity ID>] ...... Document (each entity mention is enclosed by the ID of the entity): <Document> ID and mentions of each entity in the document: [1]: <Mention 1 of Entity 1>; <Mention 2 of Entity 1>; ...... [2]: <Mention 1 of Entity 2>; <Mention 2 of Entity 2>; ......\n...... All non-duplicate valid \u201csubject entity\u201d-\u201crelation type\u201d-\u201cobject entity\u201d triples in the document (output format: \u201centity ID\u201d-\u201crelation type name\u201d\u201centity ID\u201d, e.g., [1]-country-[2]; one triple per line):"
        },
        {
            "heading": "D Case Study",
            "text": "We select a representative in-domain 1-Doc episode from ReFREDo benchmark for a case study, as shown in Table 10, which intuitively illustrates both the superiority and the bottleneck of the RAPL method. We can observe that: (1) RAPL corrected a false negative prediction of relation P361 for the baseline method. Note that the entity pair of the only instance for P361 in the support document also expresses the relation P140 and P279, and the relation P279 and P463 are semantically close to P361. This suggests the effectiveness of instance-level prototype construction and relationweighted contrastive refinement in RAPL method. (2) RAPL corrected a false positive prediction of relation P140 between entity Mayflower and Episcopal Diocese of Connecticut. This pair of entities actually does not convey any target relationship in the query document. Such case may benefit from the task-specific NOTA prototype generation strategy, which better characters the NOTA semantics. (3) When the patterns or reasoning processes of the relation instances in query document differ significantly from the support instances with same relation type, RAPL often struggles with extraction. Also, RAPL tends to exhibit cases of overprediction, resulting in relatively lower precision. Although the proposed RAPL method achieves certain improvements, the overall performance of fewshot DocRE still lags far behind the supervised setting, and how to overcome the two aforementioned challenges is worth further exploration in future research."
        }
    ],
    "title": "RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation Extraction",
    "year": 2023
}