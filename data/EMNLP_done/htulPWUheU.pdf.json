{
    "abstractText": "Cross-document event coreference resolution (CD-ECR) is a task of clustering event mentions across multiple documents that refer to the same real-world events. Previous studies usually model the CD-ECR task as a pairwise similarity comparison problem by using different event mention features, and consider the highly similar event mention pairs in the same cluster as coreferent. In general, most of them only consider the local context of event mentions and ignore their implicit global information, thus failing to capture the interactions of long-distance event mentions. To address the above issue, we regard discourse structure as global information to further improve CD-ECR. First, we use a discourse rhetorical structure constructor to construct tree structures to represent documents. Then, we obtain shortest dependency paths from the tree structures to represent interactions between event mention pairs. Finally, we feed the above information to a multi-layer perceptron to capture the similarities of event mention pairs for resolving coreferent events. Experimental results on the ECB+ dataset show that our proposed model outperforms several baselines and achieves the competitive performance with the start-of-theart baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xinyu Chen"
        },
        {
            "affiliations": [],
            "name": "Sheng Xu"
        },
        {
            "affiliations": [],
            "name": "Peifeng Li"
        },
        {
            "affiliations": [],
            "name": "Qiaoming Zhu"
        }
    ],
    "id": "SP:ae9cc0e4588f5c68253c1073c82bcce309439a8e",
    "references": [
        {
            "authors": [
                "Jun Araki",
                "Teruko Mitamura."
            ],
            "title": "Joint event trigger identification and event coreference resolution with structured perceptron",
            "venue": "EMNLP, pages 2074\u2013 2080.",
            "year": 2015
        },
        {
            "authors": [
                "Amit Bagga."
            ],
            "title": "Evaluation of coreferences and coreference resolution systems",
            "venue": "LREC, pages 563\u2013 572.",
            "year": 1998
        },
        {
            "authors": [
                "Shany Barhom",
                "Vered Shwartz",
                "Alon Eirew",
                "Michael Bugert",
                "Nils Reimers",
                "Ido Dagan."
            ],
            "title": "Revisiting joint modeling of cross-document entity and event coreference resolution",
            "venue": "ACL, pages 4179\u2013 4189.",
            "year": 2019
        },
        {
            "authors": [
                "Cosmin Adrian Bejan",
                "Sanda M. Harabagiu."
            ],
            "title": "Unsupervised event coreference resolution with rich linguistic features",
            "venue": "ACL, pages 1412\u20131422.",
            "year": 2010
        },
        {
            "authors": [
                "Avi Caciularu",
                "Arman Cohan",
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arie Cattan",
                "Ido Dagan."
            ],
            "title": "CDLM: cross-document language modeling",
            "venue": "EMNLP (Findings), pages 2648\u20132662.",
            "year": 2021
        },
        {
            "authors": [
                "Arie Cattan",
                "Alon Eirew",
                "Gabriel Stanovsky",
                "Mandar Joshi",
                "Ido Dagan."
            ],
            "title": "Cross-document coreference resolution over predicted mentions",
            "venue": "ACL/IJCNLP (Findings), pages 5100\u20135107.",
            "year": 2021
        },
        {
            "authors": [
                "Chen Chen",
                "Vincent Ng."
            ],
            "title": "Joint inference over a lightly supervised information extraction pipeline: Towards event coreference resolution for resourcescarce languages",
            "venue": "AAAI, pages 2913\u20132920.",
            "year": 2016
        },
        {
            "authors": [
                "Zheng Chen",
                "Heng Ji."
            ],
            "title": "Graph-based event coreference resolution",
            "venue": "ACL, pages 54\u201357.",
            "year": 2009
        },
        {
            "authors": [
                "Fei Cheng",
                "Yusuke Miyao."
            ],
            "title": "Classifying temporal relations by bidirectional LSTM over dependency paths",
            "venue": "ACL, pages 1\u20136.",
            "year": 2017
        },
        {
            "authors": [
                "Haoyi Cheng",
                "Peifeng Li",
                "Qiaoming Zhu."
            ],
            "title": "Employing gated attention and multi-similarities to resolve document-level chinese event coreference",
            "venue": "IALP, pages 296\u2013301.",
            "year": 2019
        },
        {
            "authors": [
                "Prafulla Kumar Choubey",
                "Ruihong Huang."
            ],
            "title": "Improving event coreference resolution by modeling correlations between event coreference chains and document topic structures",
            "venue": "ACL, pages 485\u2013495.",
            "year": 2018
        },
        {
            "authors": [
                "Prafulla Kumar Choubey",
                "Aaron Lee",
                "Ruihong Huang",
                "Lu Wang."
            ],
            "title": "Discourse as a function of event: Profiling discourse structure in news articles around the main event",
            "venue": "ACL, pages 5374\u20135386.",
            "year": 2020
        },
        {
            "authors": [
                "Agata Cybulska",
                "Piek Vossen."
            ],
            "title": "Using a sledgehammer to crack a nut? lexical diversity and event coreference resolution",
            "venue": "LREC, pages 4545\u20134552.",
            "year": 2014
        },
        {
            "authors": [
                "Agata Cybulska",
                "Piek Vossen."
            ],
            "title": "Translating granularity of event slots into features for event coreference resolution",
            "venue": "EVENTS@HLP-NAACL, pages 1\u201310.",
            "year": 2015
        },
        {
            "authors": [
                "Greg Durrett",
                "Dan Klein."
            ],
            "title": "Easy victories and uphill battles in coreference resolution",
            "venue": "EMNLP, pages 1971\u20131982.",
            "year": 2013
        },
        {
            "authors": [
                "Jie Fang",
                "Peifeng Li."
            ],
            "title": "Data augmentation with reinforcement learning for document-level event coreference resolution",
            "venue": "NLPCC, pages 751\u2013763.",
            "year": 2020
        },
        {
            "authors": [
                "Jie Fang",
                "Peifeng Li",
                "Guodong Zhou."
            ],
            "title": "Employing multiple decomposable attention networks to resolve event coreference",
            "venue": "NLPCC, pages 246\u2013 256.",
            "year": 2018
        },
        {
            "authors": [
                "William Held",
                "Dan Iter",
                "Dan Jurafsky."
            ],
            "title": "Focus on what matters: Applying discourse coherence theory to cross document coreference",
            "venue": "EMNLP, pages 1406\u20131417.",
            "year": 2021
        },
        {
            "authors": [
                "Yin Jou Huang",
                "Jing Lu",
                "Sadao Kurohashi",
                "Vincent Ng."
            ],
            "title": "Improving event coreference resolution by learning argument compatibility from unlabeled data",
            "venue": "NAACL-HLT, pages 785\u2013795.",
            "year": 2019
        },
        {
            "authors": [
                "Mandar Joshi",
                "Omer Levy",
                "Luke Zettlemoyer",
                "Daniel S. Weld."
            ],
            "title": "BERT for coreference resolution: Baselines and analysis",
            "venue": "EMNLP/IJCNLP, pages 5802\u20135807.",
            "year": 2019
        },
        {
            "authors": [
                "Kian Kenyon-Dean",
                "Jackie Chi Kit Cheung",
                "Doina Precup."
            ],
            "title": "Resolving event coreference with supervised representation learning and clusteringoriented regularization",
            "venue": "*SEM@NAACL-HLT, pages 1\u201310.",
            "year": 2018
        },
        {
            "authors": [
                "Sebastian Krause",
                "Feiyu Xu",
                "Hans Uszkoreit",
                "Dirk Weissenborn."
            ],
            "title": "Event linking with sentential features from convolutional neural networks",
            "venue": "CoNLL, pages 239\u2013249.",
            "year": 2016
        },
        {
            "authors": [
                "Kenton Lee",
                "Luheng He",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "End-to-end neural coreference resolution",
            "venue": "EMNLP, pages 188\u2013197.",
            "year": 2017
        },
        {
            "authors": [
                "Shulin Liu",
                "Yubo Chen",
                "Kang Liu",
                "Jun Zhao."
            ],
            "title": "Exploiting argument information to improve event detection via supervised attention mechanisms",
            "venue": "ACL, pages 1789\u20131798.",
            "year": 2017
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zhengzhong Liu",
                "Jun Araki",
                "Eduard H. Hovy",
                "Teruko Mitamura."
            ],
            "title": "Supervised withindocument event coreference using information propagation",
            "venue": "LREC, pages 4539\u20134544.",
            "year": 2014
        },
        {
            "authors": [
                "Zhengzhong Liu",
                "Teruko Mitamura",
                "Eduard H. Hovy."
            ],
            "title": "Graph based decoding for event sequencing and coreference resolution",
            "venue": "COLING, pages 3645\u20133657.",
            "year": 2018
        },
        {
            "authors": [
                "Jing Lu",
                "Vincent Ng."
            ],
            "title": "Joint learning for event coreference resolution",
            "venue": "ACL, pages 90\u2013101.",
            "year": 2017
        },
        {
            "authors": [
                "Jing Lu",
                "Vincent Ng."
            ],
            "title": "Span-based event coreference resolution",
            "venue": "AAAI, pages 13489\u201313497.",
            "year": 2021
        },
        {
            "authors": [
                "Jing Lu",
                "Deepak Venugopal",
                "Vibhav Gogate",
                "Vincent Ng."
            ],
            "title": "Joint inference for event coreference resolution",
            "venue": "COLING, pages 3264\u20133275.",
            "year": 2016
        },
        {
            "authors": [
                "Yaojie Lu",
                "Hongyu Lin",
                "Jialong Tang",
                "Xianpei Han",
                "Le Sun."
            ],
            "title": "End-to-end neural event coreference resolution",
            "venue": "CoRR, abs/2009.08153.",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoqiang Luo."
            ],
            "title": "On coreference resolution performance metrics",
            "venue": "HLT/EMNLP, pages 25\u201332.",
            "year": 2005
        },
        {
            "authors": [
                "William C Mann",
                "Sandra A Thompson."
            ],
            "title": "Rhetorical structure theory: Toward a functional theory of text organization",
            "venue": "Text - Interdisciplinary Journal for the Study of Discourse, 8(3):243\u2013281.",
            "year": 1998
        },
        {
            "authors": [
                "Vincent Ng."
            ],
            "title": "Supervised noun phrase coreference research: The first fifteen years",
            "venue": "ACL, pages 1396\u2013 1411.",
            "year": 2010
        },
        {
            "authors": [
                "Thien Huu Nguyen",
                "Adam Meyers",
                "Ralph Grishman."
            ],
            "title": "New york university 2016 system for KBP event nugget: A deep learning approach",
            "venue": "TAC. NIST.",
            "year": 2016
        },
        {
            "authors": [
                "Cristina Nicolae",
                "Gabriel Nicolae."
            ],
            "title": "BESTCUT: A graph algorithm for coreference resolution",
            "venue": "EMNLP, pages 275\u2013283.",
            "year": 2006
        },
        {
            "authors": [
                "Haoruo Peng",
                "Yangqiu Song",
                "Dan Roth."
            ],
            "title": "Event detection and co-reference with minimal supervision",
            "venue": "EMNLP, pages 392\u2013402.",
            "year": 2016
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Karthik Raghunathan",
                "Heeyoung Lee",
                "Sudarshan Rangarajan",
                "Nate Chambers",
                "Mihai Surdeanu",
                "Dan Jurafsky",
                "Christopher D. Manning."
            ],
            "title": "A multipass sieve for coreference resolution",
            "venue": "EMNLP, pages 492\u2013501.",
            "year": 2010
        },
        {
            "authors": [
                "Satyan Sangeetha",
                "Michael Arock."
            ],
            "title": "Event coreference resolution using mincut based graph clustering",
            "venue": "International Journal of Computing and Information Sciences, pages 253\u2013260.",
            "year": 2012
        },
        {
            "authors": [
                "Marc B. Vilain",
                "John D. Burger",
                "John S. Aberdeen",
                "Dennis Connolly",
                "Lynette Hirschman."
            ],
            "title": "A model-theoretic coreference scoring scheme",
            "venue": "MUC, pages 45\u201352.",
            "year": 1995
        },
        {
            "authors": [
                "Charles L. Wayne."
            ],
            "title": "Topic detection & tracking: a case study in corpues creation & evaluation methodologies",
            "venue": "LREC, pages 111\u2013116.",
            "year": 1998
        },
        {
            "authors": [
                "Dirk Weissenborn",
                "Georg Wiese",
                "Laura Seiffe."
            ],
            "title": "Making neural QA as simple as possible but not simpler",
            "venue": "CoNLL, pages 271\u2013280.",
            "year": 2017
        },
        {
            "authors": [
                "Hu Xu",
                "Bing Liu",
                "Lei Shu",
                "Philip S. Yu."
            ],
            "title": "BERT post-training for review reading comprehension and aspect-based sentiment analysis",
            "venue": "ACL, pages 2324\u20132335.",
            "year": 2019
        },
        {
            "authors": [
                "Yan Xu",
                "Lili Mou",
                "Ge Li",
                "Yunchuan Chen",
                "Hao Peng",
                "Zhi Jin."
            ],
            "title": "Classifying relations via long short term memory networks along shortest dependency paths",
            "venue": "EMNLP, pages 1785\u20131794.",
            "year": 2015
        },
        {
            "authors": [
                "Bishan Yang",
                "Claire Cardie",
                "Peter I. Frazier."
            ],
            "title": "A hierarchical distance-dependent bayesian model for event coreference resolution",
            "venue": "Trans. Assoc. Comput. Linguistics, 3:517\u2013528.",
            "year": 2015
        },
        {
            "authors": [
                "Xiaodong Yu",
                "Wenpeng Yin",
                "Dan Roth."
            ],
            "title": "Pairwise representation learning for event coreference",
            "venue": "*SEM@NAACL-HLT, pages 69\u201378.",
            "year": 2022
        },
        {
            "authors": [
                "Yutao Zeng",
                "Xiaolong Jin",
                "Saiping Guan",
                "Jiafeng Guo",
                "Xueqi Cheng."
            ],
            "title": "Event coreference resolution with their paraphrases and argument-aware embeddings",
            "venue": "COLING, pages 3084\u20133094.",
            "year": 2020
        },
        {
            "authors": [
                "Longyin Zhang",
                "Fang Kong",
                "Guodong Zhou."
            ],
            "title": "Adversarial learning for discourse rhetorical structure parsing",
            "venue": "ACL/IJCNLP, pages 3946\u20133957.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In real-world texts, there are usually a large number of sentences that describe the same event in reality, and an event will be mentioned repeatedly in multiple documents. When multiple event mentions (an instance of a specific event in texts) point to the same event ontology, these event mentions are coreferent. Event coreference resolution is useful for many natural language processing (NLP) applications, such as information extraction (Liu et al., 2017), topic detection (Wayne, 1998), and question answering (Weissenborn et al., 2017). Depending on whether the event mentions are in the same document, this task can be further divided into\nwithin-document (WD-ECR) and cross-document (CD-ECR) event coreference resolution. This paper focuses on the cross-document task CD-ECR.\nEvents mainly consist of triggers and arguments. Since triggers are the main words that can most clearly express the occurrence of events, each event can be represented as its corresponding trigger. Consider the following two event mentions as examples:\nS1: The court would hand down a ruling on whether the former president will remain detained for three more months before the current extension expires.\nS2: The former president detainment was previously extended for three months in July.\nThe event triggers in the event sentences S1 and S2 are \u201cdetained\u201d and \u201cdetainment\u201d, respectively. Although these two triggers have different forms, they both refer to the same judicial type Arrest. Therefore, \u201cdetained\u201d in S1 and \u201cdetainment\u201d in S2 have a coreference relationship and can be aggregated to form a coreferent chain.\nPrevious studies on WD-ECR or CD-ECR typically took a pair of event mentions with their event sentences as input and then predicted whether they are coreferent using a binary classifier (Chen and Ji, 2009; Lu et al., 2016; Lu and Ng, 2017). To this end, an important step in their models was to extract discriminative features of event mentions and then used encoding method to represent event mentions as vectors. Simultaneously, almost all previous studies considered the task of event coreference resolution as a similarity model and focused on how to calculate the feature similarities between event mention pairs (Liu et al., 2014; Lu and Ng, 2017). Besides, some studies also focused on data augmentation (Nguyen et al., 2016; Choubey and Huang, 2018; Huang et al., 2019; Barhom et al., 2019; Fang and Li, 2020), which boost event coreference resolution on additional raw data.\nIn the CD-ECR task, there is a limitation that\ntwo event mentions in a pair may be scattered across one or more documents and have a long distance. Previous studies on both CD-ECR and WD-ECR have attempted to extract the context features, which only represented event mentions from the local perspective (Kenyon-Dean et al., 2018; Barhom et al., 2019) and the information from the global perspective was neglected.\nTo address the above issue, we introduce discourse structure to extract global information of event mentions and then apply them to CD-ECR. Discourse structure is a tree structure (called \u201cdiscourse tree\u201d below). Generally, discourse tree is constructed by discourse rhetorical structure (Mann and Thompson, 1998) (DRS) constructor, which aims to represent input text as a tree structure like the example in Figure 1, where the leaf nodes are elementary discourse units (EDU) and it has smaller fine-grained units than event sentences. The discourse tree can clearly express the rhetorical relation and nuclearity relation (e.g., NS-Summary) of each EDU, which can provide useful information for associating those long-distance event mention pairs.\nOn the other hand, the DRS constructor generally performs tree construction on single document. In the CD-ECR task, if two event mentions in the same document, we can directly input the document to DRS constructor to obtain discourse tree, and then extract the global information of the two event mentions. However, if the query is a crossdocument event mention pair, and the models send their documents to the DRS constructor separately, the global information of this event mention pair cannot be obtained directly. Therefore, we also propose a construction strategy of cross-document discourse tree, which is helpful to obtain the global information of cross-document event mention pairs. We summarize the contributions of our work as follows.\n\u2022 We extract global information from discourse\ntree which can provide useful information for associating those long-distance event mention pairs.\n\u2022 We propose a strategy for constructing the cross-document discourse tree. To the best of our knowledge, we are the first to apply the cross-document discourse tree to the CD-ECR task.\n\u2022 Our model outperforms several baselines and achieves the competitive performance with the start-of-the-art baseline."
        },
        {
            "heading": "2 Related Work",
            "text": "Research on event coreference resolution mainly draws on the method of entity coreference resolution, which aims to resolve noun phrases/mentions for entities (Raghunathan et al., 2010; Ng, 2010; Durrett and Klein, 2013; Lee et al., 2017; Joshi et al., 2019). Since event mentions have more complex structures than entity mentions, event coreference resolution is a more challenging task than entity coreference resolution (Yang et al., 2015).\nEarly research on event coreference resolution mainly applied machine learning methods (Chen and Ji, 2009; Bejan and Harabagiu, 2010; Liu et al., 2014). Some researchers incorporated various kinds of regular methods into machine learning methods and improved the performance of event coreference resolution (Nicolae and Nicolae, 2006; Sangeetha and Arock, 2012; Liu et al., 2018). Since these methods relied heavily on manual annotation features, some studies paid more attention to raw text event coreference resolution (Araki and Mitamura, 2015; Peng et al., 2016; Lu et al., 2016; Chen and Ng, 2016; Lu and Ng, 2017).\nRecently, deep learning work has been applied to both WD-ECR (Nguyen et al., 2016; Choubey and Huang, 2018; Fang et al., 2018; Huang et al., 2019; Cheng et al., 2019; Lu et al., 2020; Choubey et al., 2020; Lu and Ng, 2021) and CD-ECR (KenyonDean et al., 2018; Barhom et al., 2019; Zeng et al., 2020; Cattan et al., 2021; Yu et al., 2022). In WDECR, Krause et al. (2016) used convolutional neural networks to mine event features for event coreference resolution. Huang et al. (2019) incorporated argument compatibility knowledge from a large number of the unlabeled corpus. Lu and Ng (2021) investigated span-based models for event coreference resolution. In CD-ECR, argument information was introduced into event representations\n(Barhom et al., 2019; Zeng et al., 2020; Yu et al., 2022), Barhom et al. (2019) jointly learned entity and event coreference resolution and leveraged predicate-argument structures. Zeng et al. (2020) integrated event-specific paraphrases and argumentaware semantic embeddings for CD-ECR. Yu et al. (2022) augmented pairwise representation with structured argument features to improve CD-ECR performance. Caciularu et al. (2021) pretrained a cross-document language model via sets of related documents for CD-ECR. Held et al. (2021) extracted event mentions features from the local perspective and trained a fine-grained classifier to improve CD-ECR performance.\nCompared with previous deep learning work for CD-ECR, we present a novel global information representation for event mentions to enhance the interaction between long-distance event mentions."
        },
        {
            "heading": "3 Model",
            "text": "Formally, given a set of documents D = {d1, d2, ..., d|D|}, whose element di consists of a series of sentences {si1, si2, ..., si|di|}}, and a set of event mentions M = {m1,m2, ...,m|M |}, these event mentions in M can be distributed in different documents in D. CD-ECR aims to discover the event mentions in M from different documents in D that refer to the same event ontology in the real world and gather them into the same cluster. ECB+ is the most popular corpus for CD-ECR and our work is also performed on this corpus.\nFigure 2 shows an overview of our CD-ECR model, which includes three main components: 1) Local Information Representation (LIR) to obtain\nlocal perspective information of event mentions from EDU embeddings, 2) Global Information Representation (GIR) to extract global information representation from the discourse tree constructed by the DRS Constructor, 3) Event Coreference Prediction (ECP) to receive the global and local representations of event mention pairs and predict the probability that two event mentions are coreferent."
        },
        {
            "heading": "3.1 Local Information Representation",
            "text": "Generally, previous pairwise model on the CD-ECR task took the concatenated vector R(mi,mj) = [v(mi), v(mj), v(mi) \u25e6 v(mj)] as the base representation of event pairs, where \u25e6 is element-wise multiplication, the v(mi) and v(mj) are feature vector of the event mention mi and mj , respectively. They obtained the feature vector v(\u00b7) through encoding the event sentence by a pre-trained language model (e.g., BERT (Xu et al., 2019), RoBERTa (Liu et al., 2019)) and then consider the word embedding of trigger or trigger context tokens as the feature vector v(\u00b7).\nDifferent from previous work, we incorporate EDU representation in R(mi,mj) where event mention located. Specifically, we first encode each document using RoBERTaLARGE inspired by Cattan et al. (2021), which splits long documents into non-overlapping segments of up to 512 word-piece tokens and encodes them independently. Differently, when the document exceeds 512 tokens, in order to preserve the integrity of the EDU information where the trigger is located, we just split it into multiple segments of entire EDUs and not just 512 tokens.\nAfter encoded by RoBERTa, we extract the trigger pair feature t(mi,mj) and the EDU pair feature e(mi,mj) from word embeddings for the event mention pair (mi,mj), the local information representation Rlocal(i, j) is obtained as follows.\nt(mi,mj) = [ti, tj , ti \u25e6 tj ], e(mi,mj) = [ei, ej , ei \u25e6 ej ], Rlocal(i, j) = [t(mi,mj), e(mi,mj)], (1)\nwhere ti, tj denotes the trigger tokens embeddings of mi, mj , and ei, ej denotes the EDU tokens embeddings of mi, mj ."
        },
        {
            "heading": "3.2 Global Information Representation",
            "text": "GIR aims to obtain the global information representation of event mention pair from discourse tree. Figure 3(a) shows the discourse tree construction step for both within-document (WD) and crossdocument (CD) event mention pairs.\nGenerally, a DRS constructor consists of two main components: Segmentor and DRS parser. The segmentor receives an article with several sentences and splits it into a set of EDU sequences [EDU1, EDU2,...,EDUn], where all EDUs are leaf nodes of the discourse tree. Then, the DRS parser predicts a specific rhetoric and nuclearity relation between two adjacent EDUs, and then forms a superior DU, which links to others EDUs or DUs to obtain a discourse tree.\nSpecifically, we first prepare the input data for the DRS constructor. Since the two event mentions in a given input event mention pair (mi,mj) may come from different documents, we process it in\ntwo ways as shown in Figure 3(a). If (mi,mj) is a within-document event mention pair, we feed whole document directly to the DRS constructor to obtain a discourse tree. If (mi,mj) is a crossdocument event mention pair, we extract the two event sentences that mi and mj are located respectively, then the other texts are sent to T5BASE (Raffel et al., 2020) for compression. The compressed texts are concatenated with two event sentences by their origin order to form the input document for the DRS constructor. Finally, the cross-document discourse tree is obtained. In our work, we utilize the state-of-the-art discourse rhetoric structure parser (DRS) (Zhang et al., 2021) to construct discourse trees for the CD-ECR task-specific corpus.\nShortest Dependency Path (SDP) is widely used in various NLP tasks (Xu et al., 2015; Cheng and Miyao, 2017) to capture crucial interaction information between sentences, we combine it with discourse tree for our CD-ECR task, called DTSDP. As shown in Figure 3(b), we assume that the event mentions mi and mj are located in EDU1 and EDU3, respectively, and the shortest path between them is marked in red. We express nodes on the red line as the sequence [NS-R1,2, NS-R12,34, NS-R3,4] to represent DT-SDP. All non-leaf nodes contain the probability distributions information of the rhetoric and nuclearity relation obtained by the DRS parser. Since different event mention pairs can obtain different length of DT-SDP, we apply BiLSTM network to encode variable length sequence DT-SDP, and take the output of last hidden layer as DT-SDP representation, denoted as RDT\u2212SDP .\nAdditionally, we also extract the lowest common parent node LCP of two EDUs that the event mentions mi and mj are located from the discourse tree (as shown by dark green node in the Figure 3(b)). The probability distributions information of LCP is denoted as RLCP . We finally obtain the global information representation Rglobal(i, j) by concatenating RLCP and RDT\u2212SDP as follows.\nRglobal(i, j) = [RLCP , RDT\u2212SDP ]. (2)"
        },
        {
            "heading": "3.3 Event Coreference Prediction",
            "text": "In this stage, we have obtained the global and local information representations Rglobal(i, j) and Rlocal(i, j) of the event mention pair (mi, mj). We fuse the two features and send them to the multi-layer perceptron (MLP) and sigmoid activation function to obtain the coreference score S as follows.\n\u03b8 = MLP (Rglobal(i, j), Rlocal(i, j)), (3)\nS = Sigmoid(\u03b8). (4)"
        },
        {
            "heading": "3.4 Training and Inference",
            "text": "During training, we apply dropout in Bi-LSTM and MLP networks, the training objective is to minimize the binary cross-entropy loss L as follows.\nL = \u2212 1 N\n\u2211N i=1[yilogy\u0302i+(1\u2212yi)log(1\u2212y\u0302i)], (5)\nwhere N is the size of event mention pair samples and y \u2208 {0, 1} is a pair label.\nDuring inference, we first reproduce the topic predictor of Barhom et al. (2019) to perform document clustering, and take event mention pairs in the same document cluster as candidate coreferent pairs. We then send these pairs to our CD-ECR model to obtain the coreference score, and consider the pairs whose score > 0.5 as coreferent pairs, otherwise as non-coreferent. To handle the pairwise event coreference predictions, we perform bestfirst clustering (Huang et al., 2019) on the pairwise scores to build the coreferent event clusters."
        },
        {
            "heading": "4 Experimentation",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Dataset Following previous work (Cybulska and Vossen, 2014), we use the ECB+ dataset to train and test our model, which is the largest and most popular dataset for CD-ECR. ECB+ is extended from ECB (Bejan and Harabagiu, 2010), which\nannotated different but similar events as subtopics for each ECB topic. We use gold event mentions for both training and evaluation. The detailed statistics are shown in Table 1. Metrics Following the previous work (Barhom et al., 2019; Cattan et al., 2021; Yu et al., 2022), we use MUC (Vilain et al., 1995), B3 (Bagga, 1998), and CEAFe (Luo, 2005) to evaluate the performance of our model and also report the CoNLL scores, which is the average of the above three metrics. Among them, MUC is based on event links to evaluate the performance of the model, B3 compensates for MUC\u2019s neglected evaluation of non-coreferent events by using event nodes as the computational target. CEAFe is similar to B3 , adding entities to evaluate the performance of event coreference resolution. The comprehensive use of the above three metrics and CoNLL can more objectively measure model performance. Hyper Parameters We use the pre-trained language model RoBERTaLARGE to embed event mentions with 1024 dimensions, the training epoch of our model is set to 10, the learning rate is set to 10\u22125, Adam optimizer is used to update the parameters. The minimal and maximum output length of T5BASE generator are set to 20% and 50% of the input text, respectively. Additionally, in order to make a fair comparison with the baseline of Caciularu et al. (2021), which uses stronger encoder LongformerBASE, we also use the LongformerBASE to embed event mentions with 768 dimensions for resolving coreferent events."
        },
        {
            "heading": "4.2 Experimental Results",
            "text": "To verify the effectiveness of our model, we conduct the following strong baselines.\n1) Barhom et al. (2019), which jointly learns entity and event coreference resolution and leverages predicate-argument structures;\n2) Zeng et al. (2020), which integrates eventspecific paraphrases and argument-aware semantic embeddings for CD-ECR;\n3) Cattan et al. (2021), which develops an endto-end baseline for CD-ECR;\n4) Caciularu et al. (2021), which pretrains a language model via a sets of related documents for CD-ECR. Longformer was used for their encoder.\n5) Held et al. (2021), which extracts event mentions features from the local perspective and trained a fine-grained classifier for CD-ECR.\n6) Yu et al. (2022), which augments pairwise representation with structured argument features to improve CD-ECR performance.\nTable 2 reports the performance of the above six baselines and our model on ECB+ with encoder RoBERTa and Longformer, and the results show that our model (Longformer) significantly (P<0.01) outperforms the best Held et al. (2021), with the improvement of 0.7 in the average score CoNLL, and our model (RoBERTa) achieve the competitive result with them. This result indicates the effectiveness of our proposed model in resolving coreferent event.\nFormally, the common part of Barhom et al. (2019), Zeng et al. (2020) and Yu et al. (2022) is that their input feature of event mention pairs can be represented as R(mi,mj) = [v(mi), v(mj), v(mi) \u25e6 v(mj), f(mi,mj)], where f(mi,mj) is additional pairwise feature. Barhom et al. (2019) trains entity and event coreference together and takes argument features as f(mi,mj), which outperforms several early CD-ECR models (Cybulska and Vossen, 2015; Kenyon-Dean et al., 2018). Zeng et al. (2020) not only uses the argument features but also integrates event-specific paraphrases, significantly improving CoNLL by\n4.8 over Barhom et al. (2019). Yu et al. (2022) augments pairwise representation with structured argument features, improving CoNLL by 4.9 over Barhom et al. (2019) and achieving competitive performance with Zeng et al. (2020). This suggests that argument features are crucial for resolving coreferent events. However, using argument features to calculate event similarity is from local perspective, ignoring the global features of events, which leads to the poor performance on long-distance event mentions. Our model uses discourse trees to capture interactions between event mentions, thus enhancing the representation of event information and improving CD-ECR performance.\nCompared with Cattan et al. (2021), who develop an end-to-end baseline for CD-ECR and only use RoBERTa to encode event mentions without using other features, our model improves the CoNLL score by 4.7. It also shows the effectiveness of global information in the discourse tree, since our baseline is also RoBERTa.\nCompared with Caciularu et al. (2021), who employed Longformer as the encoder. Since Longformer is capable of encoding entire documents, it outperforms other encoders in the event coreference resolution task. For fair comparison, we replace our text encoder RoBERTa with Longformer and the experimental results shows that our model outperforms Caciularu et al. (2021) with average CoNLL score improvements of 0.8.\nCompared with Held et al. (2021), both our model (RoBERTa) and Held et al. (2021) achieve the same CoNLL score 85.7. Held et al. (2021) focuses on extracting event mentions features from the local perspective and trains a fine-grained classifier. Compared with them, our model pays more\nattention to extracting event mention features from the global perspective, which uses discourse tree to capture the features of long-distance event mention pairs. In a word, the performance improvement of our model and Held et al. (2021) can be owned to the intensive study of global and local features, respectively.\nComapred with our model Ours(RoBERTa) using RoBERTa as encoder, Ours(Longformer) using Longformer improves CoNLL by 0.7. This indocates that LongFomer is a stronger encoder than RoBERTa in the CD-ECR task.\nIn the above baselines, Cattan et al. (2021) evaluate their model not only at the corpus level with singletons, but also at the topic level without singletons. We also report the result on these experiment settings. The performance comparison with Cattan et al. (2021) at the corpus/topic level with (singletons+)/without (singletons-) singletons are shown in Table 3. The results show that our model significantly outperforms Cattan et al. (2021) on both corpus level and topic level with/without singletons on all metrics."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Impact of Global Information",
            "text": "We conduct ablation experiments to further evaluate the contribution of global information from discourse tree and design the following simplified models.\n1) Baseline(RoBERTa), which resolves coreferent events using local feature Rlocal(mi,mj) only;\n2) +WD-LCP, which adds the global feature Rglobal(mi,mj) of within-document event mention pairs to baseline 1). However, Rglobal(mi,mj) only contains RLCP ;\n3) +CD-LCP, which adds the global feature Rglobal(mi,mj) of cross-document event mention\npairs to baseline 2). However, Rglobal(mi,mj) only contains RLCP ;\n4) +SDP, which adds the feature RDT\u2212SDP to baseline 3).\nThe results are shown in Table 4. Since the process of constructing cross-document discourse trees is more complicated than that of within-document discourse tree (mainly in the data preparation stage), we compare the performance of adding the cross-document LCP information separately due to the other influencing factors such as data noise.\nCompared with baseline(RoBERTa), +WD-LCP improves CoNLL by 1.2 on the ECB+ dataset using the within-document LCP. This preliminarily shows the effectiveness of the LCP information in discourse tree. Compare with the baseline +WDLCP and +CD-LCP, the introduction of the crossdocument LCP leads to an improvement of 1.6 on CoNLL. This indicates that LCP can alleviate the long-distance problem which is poorly handled by local information representation. The comparison on +CD-LCP and +DT-SDP shows the significant improvement of 1.8 on CoNLL and indicates that DT-SDP is crucial to capture the interaction between long-distance event mentions."
        },
        {
            "heading": "5.2 Analysis on Construction Strategy for Cross Document Discourse Structure",
            "text": "In order to deeply analyse the impact of different construction strategies of the cross-document DRS tree, we design two other cross-document DRS construction strategies for comparison with the strategy in our model as follows.\n1) ES-comp, which takes event sentence and other sentences compressed by T5 model as constructor input;\n2) ES-only, which takes only event sentence as constructor input without any other text;\n3) Full-doc, which concatenates two documents that contain the event mention pairs and then sends them to the constructor.\nThe results are shown in Table 5. Regardless of whether DT-SDP information is introduced, the construction strategy ES-comp outperforms the two other strategies. If we exclude DT-SDP, the Fulldoc strategy achieves the worst performance due to the data noise introduced by irrelevant texts. The ES-only strategy achieves the second worst performance, which is slightly better than Full-doc. However, it is fails to outperform ES-comp because it has no other contextual information to guide it.\nComparing each strategy with (w/) or without (w/o) DT-SDP, we can see that the ES-only strategy has the least performance improvement when DTSDP is included because the input text contains only two event sentences, which is too short, and the depth and breadth of the constructed discourse tree are too small, leading to the short length of the shortest path between two EDUs. Using the node sequence to represent the path may only include common parent nodes, which tends to exclude DTSDP. In contrast, the performance of the Full-doc strategy achieves the most improvement. This is because the long input text increases the depth and breadth of the discourse tree, which also increases the length of the shortest path and enriches the representation of DT-SDP. However, the existence of a large amount of noise also limits the increase in performance so that it does not outperform ES-\ncomp. It shows that ES-comp is a compromise strategy, because taking event sentences and other compressed irrelevant texts as constructor input can increase the positive impact so as to achieve the best performance."
        },
        {
            "heading": "5.3 Case Study",
            "text": "In this subsection, we give the examples to analyse the effectiveness of global information and the strategy ES-comp in our model. Considering the following two documents A and B as examples, where the bolded texts represent event sentences (S1 and S4, respectively), and the underlined words indicate event mentions, the goal of our model is to predict whether the event mentions \u201cwinning\u201d and \u201cchosen\u201d are coreferent.\nA: (S1) Smith, 26, who played a young political researcher in the show, will become the biggest star of all after winning the role of the 11th Doctor. (S2) Speaking to The Guardian, Buchan said his old co-star would make an excellent Doctor Who. \u201cIt s a sublime bit of casting. He\u2019s got that huge hair, a twinkle in his eye - Matt\u2019s the king of geek chic. He is possibly going to be one of the best Doctors we\u2019ve ever had.\u201d\nB: (S3) 26-year-old Matt Smith has been cast as the next incarnation of the Doctor. Users on the Facebook Doctor Who forum that I frequent mostly had the same reaction: \u201c Who \u2019s Matt Smith?\u201d (S4) The guy is relatively unknown and the skeptics wondered if the right person was chosen. (S5) After all, everyone speculated that Paterson Joseph,\nwho had appeared on a couple episodes of the show, was going to be the next Doctor and here we get some no-name.\nWe utilize the ES-comp approach to generate a discourse tree. Initially, we compress the three non-bolded sections S2, S3, and S5, resulting in the compressed texts C1, C2, and C3, respectively, as follows.\nC1: It\u2019s a sublime bit of casting."
        },
        {
            "heading": "C2: Matt Smith has been cast as the next incarnation of the Doctor. \u201dWho \u2019s Matt Smith?\u201d",
            "text": "C3: we get Paterson Joseph. Then, the compressed documents A and B are concatenated as [S1, C1, C2, S4, C3] and then are fed to the DRS Constructor. The resulting EDU sequence is displayed as follows.\nEDU1: Smith, 26, EDU2: who played a young political researcher in the show, EDU3: will become the biggest star of all EDU4: after winning the role of the 11th Doctor EDU5: It\u2019s a sublime bit of casting. EDU6: \"Matt Smith has been cast as the next incarnation of the Doctor. EDU7: \u201d Who\u2019s Matt Smith ? \u201d EDU8: The guy is relatively unknown EDU9: and the skeptics wondered EDU10: if the right person was chosen EDU11: we get Paterson Joseph . Figure 4 illustrates the simplified discourse tree produced by the ES-comp strategy. Notably, since we solely examine the coreferent relation between \u201cwinning\u201d and \u201cchosen\u201d, Figure 4 excludes irrelevant nodes like EDU11. Furthermore, EDU1, EDU2, and EDU3 are merged into EDU1-3 since these three nodes are not part of DT-SDP. The DTSDP in this case is [NS-Elaborate, NS-Elaborate, NS-Cause, SN-Span, SN-Cause].\nDirectly measuring the similarity between EDU4 and EDU10 to predict coreference relation would result in misidentification due to their differing contexts. Since the rhetorical relation nodes\n\u201cNS-Elaborate\u201d in DT-SDP associate EDU4 with EDU1-3, EDU1-3 can introduce the agent \u201cSmith\u201d of the event mention \u201cwinning\u201d to EDU4. \u201cSmith\u201d can also be passed to the event mention \u201cchosen\u201d in EDU10 using DT-SDP. This makes \u201cSmith\u201d apparent in the tokens of \u201cwinning\u201d and \u201cchosen\u201d and facilitates accurate prediction.\nIn this case, if we use the strategy ES-only, the node \u201cNS-Elaborate\u201d will be excluded in the obtained DT-SDT and the common argument \u201cSmith\u201d cannot be perceived. Finally, if we use the Full-doc strategy, the pair still gets low coreference score, because the tree structure becomes complicated and the redundant information interferes with the model\u2019s perception of argument information."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose a cross-document event coreference resolution model. The novelty of our model is twofold. First, we introduce the discourse structure that represents global information for pairs of event mentions to provide support for event coreference resolution. Second, we propose a strategy for constructing cross-document discourse trees, which allows cross-document coreferent event mentions to be easily identified by the model. Experimental results on the ECB+ dataset show that our proposed model outperforms several baselines. In the future, we plan to extend our task to cross-modal event coreference resolution.\nLimitations\nOur method still suffers from several shortcomings. First, we focus only on the event coreference resolution step using annotated event mentions. The upstream task event detection is also critical for event coreference resolution. Second, although the introduction of the discourse tree and global information has a great performance boost, the inference time of the DRS constructor is long. Last but not least, there is still room for optimization in the strategy of cross-document DRS construction."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors would like to thank the three anonymous reviewers for their comments on this paper. This research was supported by the National Natural Science Foundation of China (Nos. 62276177, 61836007, and 62376181), and Project Funded by the Priority Academic Program Development of Jiangsu Higher Education Institutions (PAPD)."
        }
    ],
    "title": "Cross-Document Event Coreference Resolution on Discourse Structure",
    "year": 2023
}