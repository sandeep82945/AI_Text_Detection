{
    "abstractText": "Retrieval-Augmented Machine Translation (RAMT) is attracting growing attention. This is because RAMT not only improves translation metrics, but is also assumed to implement some form of domain adaptation. In this contribution, we study another salient trait of RAMT, its ability to make translation decisions more transparent by allowing users to go back to examples that contributed to these decisions. For this, we propose a novel architecture aiming to increase this transparency. This model adapts a retrieval-augmented version of the Levenshtein Transformer and makes it amenable to simultaneously edit multiple fuzzy matches found in memory. We discuss how to perform training and inference in this model, based on multiway alignment algorithms and imitation learning. Our experiments show that editing several examples positively impacts translation scores, notably increasing the number of target spans that are copied from existing instances.",
    "authors": [
        {
            "affiliations": [],
            "name": "Maxime Bouthors"
        },
        {
            "affiliations": [],
            "name": "Josep Crego"
        },
        {
            "affiliations": [],
            "name": "Fran\u00e7ois Yvon"
        }
    ],
    "id": "SP:503b374a5ffe9c0daad2d05c4eef2f16783f6576",
    "references": [
        {
            "authors": [
                "Lynne Bowker."
            ],
            "title": "Computer-aided translation technology: A practical introduction",
            "venue": "University of Ottawa Press.",
            "year": 2002
        },
        {
            "authors": [
                "Bram Bulte",
                "Arda Tezcan."
            ],
            "title": "Neural fuzzy repair: Integrating fuzzy matches into neural machine translation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1800\u20131809, Florence, Italy. Association for",
            "year": 2019
        },
        {
            "authors": [
                "Deng Cai",
                "Yan Wang",
                "Huayang Li",
                "Wai Lam",
                "Lemao Liu."
            ],
            "title": "Neural machine translation with monolingual translation memory",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Michael Carl",
                "Andy Way",
                "Walter Daelemans."
            ],
            "title": "Recent advances in example-based machine translation",
            "venue": "Computational Linguistics, 30:516\u2013520.",
            "year": 2004
        },
        {
            "authors": [
                "Humberto Carrillo",
                "David Lipman."
            ],
            "title": "The multiple sequence alignment problem in biology",
            "venue": "SIAM Journal on Applied Mathematics, 48(5):1073\u20131082.",
            "year": 1988
        },
        {
            "authors": [
                "Xin Cheng",
                "Shen Gao",
                "Lemao Liu",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Neural machine translation with contrastive translation memories",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association",
            "year": 2022
        },
        {
            "authors": [
                "Hal Daum\u00e9",
                "John Langford",
                "Daniel Marcu."
            ],
            "title": "Search-based structured prediction",
            "venue": "Machine Learning, 75(3):297\u2013325.",
            "year": 2009
        },
        {
            "authors": [
                "Michael R. Garey",
                "David S. Johnson."
            ],
            "title": "Computers and intractability: a guide to the theory of NPcompleteness",
            "venue": "W.H. Freeman and Company, New York.",
            "year": 1979
        },
        {
            "authors": [
                "Jiatao Gu",
                "Changhan Wang",
                "Junbo Zhao."
            ],
            "title": "Levenshtein transformer",
            "venue": "Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
            "year": 2019
        },
        {
            "authors": [
                "Jiatao Gu",
                "Yong Wang",
                "Kyunghyun Cho",
                "Victor O.K. Li."
            ],
            "title": "Search Engine Guided Neural Machine Translation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 32(1).",
            "year": 2018
        },
        {
            "authors": [
                "Dan Gusfield."
            ],
            "title": "Algorithms on Strings, Trees, and Sequences: Computer Science and Computational Biology",
            "venue": "Cambridge University Press.",
            "year": 1997
        },
        {
            "authors": [
                "Kelvin Guu",
                "Tatsunori B. Hashimoto",
                "Yonatan Oren",
                "Percy Liang."
            ],
            "title": "Generating sentences by editing prototypes",
            "venue": "Transactions of the Association for Computational Linguistics, 6:437\u2013450.",
            "year": 2018
        },
        {
            "authors": [
                "Junxian He",
                "Graham Neubig",
                "Taylor BergKirkpatrick."
            ],
            "title": "Efficient nearest neighbor language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5703\u20135714, Online and Punta Cana,",
            "year": 2021
        },
        {
            "authors": [
                "Qiuxiang He",
                "Guoping Huang",
                "Qu Cui",
                "Li Li",
                "Lemao Liu."
            ],
            "title": "Fast and accurate neural machine translation with translation memory",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Cuong Hoang",
                "Devendra Sachan",
                "Prashant Mathur",
                "Brian Thompson",
                "Marcello Federico."
            ],
            "title": "Improving Retrieval Augmented Neural Machine Translation by Controlling Source and Fuzzy-Match Interactions",
            "venue": "ArXiv:2210.05047 [cs].",
            "year": 2022
        },
        {
            "authors": [
                "Urvashi Khandelwal",
                "Angela Fan",
                "Dan Jurafsky",
                "Luke Zettlemoyer",
                "Mike Lewis."
            ],
            "title": "Nearest Neighbor Machine Translation",
            "venue": "Proceedings of the International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Yoon Kim",
                "Alexander M. Rush."
            ],
            "title": "Sequencelevel knowledge distillation",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317\u20131327, Austin, Texas. Association for Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Guillaume Klein",
                "Yoon Kim",
                "Yuntian Deng",
                "Jean Senellart",
                "Alexander Rush."
            ],
            "title": "OpenNMT: Opensource toolkit for neural machine translation",
            "venue": "Proceedings of ACL 2017, System Demonstrations, pages 67\u201372, Vancouver, Canada. Association for Compu-",
            "year": 2017
        },
        {
            "authors": [
                "Taku Kudo."
            ],
            "title": "Subword regularization: Improving neural network translation models with multiple subword candidates",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66\u201375,",
            "year": 2018
        },
        {
            "authors": [
                "Louis Martin",
                "Benjamin Muller",
                "Pedro Javier Ortiz Su\u00e1rez",
                "Yoann Dupont",
                "Laurent Romary",
                "\u00c9ric de la Clergerie",
                "Djam\u00e9 Seddah",
                "Beno\u00eet Sagot."
            ],
            "title": "CamemBERT: a tasty French language model",
            "venue": "Proceedings of the 58th Annual Meeting of the As-",
            "year": 2020
        },
        {
            "authors": [
                "Pedro Henrique Martins",
                "Zita Marinho",
                "Andr\u00e9 F.T. Martins."
            ],
            "title": "Chunk-based nearest neighbor machine translation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4228\u20134245, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Yuxian Meng",
                "Xiaoya Li",
                "Xiayu Zheng",
                "Fei Wu",
                "Xiaofei Sun",
                "Tianwei Zhang",
                "Jiwei Li."
            ],
            "title": "Fast nearest neighbor machine translation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 555\u2013565, Dublin, Ireland. Association",
            "year": 2022
        },
        {
            "authors": [
                "Makoto Nagao."
            ],
            "title": "A framework of a mechanical translation between Japanese and English by analogy principle",
            "venue": "Artificial and human intelligence. Elsevier Science Publishers. B.V.",
            "year": 1984
        },
        {
            "authors": [
                "Ayana Niwa",
                "Sho Takase",
                "Naoaki Okazaki."
            ],
            "title": "Nearest neighbor non-autoregressive text generation",
            "venue": "CoRR, abs/2208.12496.",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Minh Quang Pham",
                "Jitao Xu",
                "Josep Crego",
                "Fran\u00e7ois Yvon",
                "Jean Senellart."
            ],
            "title": "Priming neural machine translation",
            "venue": "Proceedings of the Fifth Conference on Machine Translation, pages 516\u2013527, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Maja Popovi\u0107."
            ],
            "title": "chrF: character n-gram F-score for automatic MT evaluation",
            "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392\u2013395, Lisbon, Portugal. Association for Computational Linguistics.",
            "year": 2015
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013 191, Brussels, Belgium. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Ricardo Rei",
                "Craig Stewart",
                "Ana C Farinha",
                "Alon Lavie."
            ],
            "title": "COMET: A neural framework for MT evaluation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685\u20132702, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Stephane Ross",
                "Geoffrey Gordon",
                "Drew Bagnell."
            ],
            "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of",
            "year": 2011
        },
        {
            "authors": [
                "Cynthia Rudin."
            ],
            "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
            "venue": "Nature Machine Intelligence, 1(5):206\u2013215.",
            "year": 2019
        },
        {
            "authors": [
                "Harold Somers."
            ],
            "title": "Review article: Examplebased machine translation",
            "venue": "Machine Translation, 14(2):113\u2013157.",
            "year": 1999
        },
        {
            "authors": [
                "Mitchell Stern",
                "William Chan",
                "Jamie Kiros",
                "Jakob Uszkoreit."
            ],
            "title": "Insertion transformer: Flexible sequence generation via insertion operations",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Ma-",
            "year": 2019
        },
        {
            "authors": [
                "Raymond Hendy Susanto",
                "Shamil Chollampatt",
                "Liling Tan."
            ],
            "title": "Lexically constrained neural machine translation with Levenshtein transformer",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3536\u20133543, On-",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Sys-",
            "year": 2017
        },
        {
            "authors": [
                "Mengzhou Xia",
                "Guoping Huang",
                "Lemao Liu",
                "Shuming Shi."
            ],
            "title": "Graph based translation memory for neural machine translation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):7297\u2013 7304.",
            "year": 2019
        },
        {
            "authors": [
                "Yisheng Xiao",
                "Lijun Wu",
                "Junliang Guo",
                "Juntao Li",
                "Min Zhang",
                "Tao Qin",
                "Tie-Yan Liu."
            ],
            "title": "A survey on non-autoregressive generation for neural machine translation and beyond",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1\u201320.",
            "year": 2023
        },
        {
            "authors": [
                "Jitao Xu",
                "Josep Crego",
                "Jean Senellart."
            ],
            "title": "Boosting neural machine translation with similar translations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1580\u20131590, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Jitao Xu",
                "Josep Crego",
                "Fran\u00e7ois Yvon."
            ],
            "title": "Integrating translation memories into non-autoregressive machine translation",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1326\u20131338,",
            "year": 2023
        },
        {
            "authors": [
                "Weijia Xu",
                "Marine Carpuat."
            ],
            "title": "EDITOR: An edit-based transformer with repositioning for neural machine translation with soft lexical constraints",
            "venue": "Transactions of the Association for Computational Linguistics, 9:311\u2013328.",
            "year": 2021
        },
        {
            "authors": [
                "Jingyi Zhang",
                "Masao Utiyama",
                "Eiichro Sumita",
                "Graham Neubig",
                "Satoshi Nakamura."
            ],
            "title": "Guiding neural machine translation with retrieved translation pieces",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Kangjie Zheng",
                "Longyue Wang",
                "Zhihao Wang",
                "Binqi Chen",
                "Ming Zhang",
                "Zhaopeng Tu."
            ],
            "title": "Towards a unified training for Levenshtein transformer",
            "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing",
            "year": 2023
        },
        {
            "authors": [
                "Xin Zheng",
                "Zhirui Zhang",
                "Junliang Guo",
                "Shujian Huang",
                "Boxing Chen",
                "Weihua Luo",
                "Jiajun Chen."
            ],
            "title": "Adaptive nearest neighbor machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Neural Machine Translation (NMT) has become increasingly efficient and effective thanks to the development of ever larger encoder-decoder architectures relying on Transformer models (Vaswani et al., 2017). Furthermore, these architectures can readily integrate instances retrieved from a Translation Memory (TM) (Bulte and Tezcan, 2019; Xu et al., 2020; Hoang et al., 2022), thereby improving the overall consistency of new translations compared to past ones. In such context, the autoregressive and generative nature of the decoder can make the process (a) computationally inefficient when the new translation has very close matches in the TM; (b) practically ineffective, as there is no guarantee that the output translation, regenerated from scratch, will resemble that of similar texts.\nAn alternative that is attracting growing attention is to rely on computational models tailored to edit existing examples and adapt them to new source\nsentences, such as the Levenshtein Transformer (LevT) model of Gu et al. (2019). This model can effectively handle fuzzy matches retrieved from memories, performing minimal edits wherever necessary. As decoding in this model occurs is nonautoregressive, it is likely to be computationally more efficient. More important for this work, the reuse of large portions of existing translation examples is expected to yield translations that (a) are more correct; (b) can be transparently traced back to the original instance(s), enabling the user to inspect the edit operations that were performed. To evaluate claim (a) we translate our test data (details in Section 5.1) using a basic implementation of a retrieval-augmented LevT with TM (TM-LevT). We separately compute the modified unigram and bigram precisions (Papineni et al., 2002) for tokens that are copied from the fuzzy match and tokens that are generated by the model.1 We observe that copies account for the largest share of output units and have better precision (see Table 1).\nBased on this observation, our primary goal is to optimize further the number of tokens copied from the TM. To do so, we propose simultaneously editing multiple fuzzy matches retrieved from memory, using a computational architecture \u2013 Multi-LevT,\n1Copies and generations are directly infered from the sequence of edit operations used to compute the output sentence.\nor TMN-LevT for short \u2013 which extends TM-LevT to handle several initial translations. The benefit is twofold: (a) an increase in translation accuracy; (b) more transparency in the translation process. Extending TM-LevT to TMN-LevT however requires solving multiple algorithmic and computational challenges related to the need to compute Multiple String Alignments (MSAs) between the matches and the reference translation, which is a notoriously difficult problem; and designing appropriate training procedures for this alignment module.\nOur main contributions are the following:\n1. a new variant of the LevT model that explicitly maximizes target coverage (\u00a74.2);\n2. a new training regime to handle an extended set of editing operations (\u00a73.3);\n3. two novel multiway alignment (\u00a74.2) and realignment (\u00a76.2) algorithms;\n4. experiments in 11 domains where we observe an increase of BLEU scores, COMET scores, and the proportion of copied tokens (\u00a76).\nOur code and experimental configurations are available on github.2"
        },
        {
            "heading": "2 Preliminaries / Background",
            "text": ""
        },
        {
            "heading": "2.1 TM-based machine translation",
            "text": "Translation Memories, storing examples of past translations, is a primary component of professional Computer Assisted Translation (CAT) environments (Bowker, 2002). Given a translation request for source sentence x, TM-based translation is a two-step process: (a) retrieval of one or several instances (x\u0303, y\u0303) whose source side resembles x, (b) adaptation of retrieved example(s) to produce a translation. In this work, we mainly focus on step (b), and assume that the retrieval part is based on a fixed similarity measure \u2206 between x and stored examples. In our experiments, we use:\n\u2206(x, x\u0303) = 1\u2212 ED(x, x\u0303) max(|x|, |x\u0303|) , (1)\nwith ED(x, x\u0303) the edit distance between x and x\u0303 and |x| the length of x. We only consider TM matches for which \u2206 exceeds a predefined threshold \u03c4 and filter out the remaining ones. The next step, adaptation, is performed by humans with CAT\n2https://github.com/Maxwell1447/fairseq/\ntools. Here, we instead explore ways to perform this step automatically, as in Example-Based MT (Nagao, 1984; Somers, 1999; Carl et al., 2004).\n2.2 Adapting fuzzy matches with LevT\nThe Levenshtein transformer of Gu et al. (2019) is an encoder-decoder model which, given a source sentence, predicts edits that are applied to an initial translation in order to generate a revised output (Figure 1). The initial translation can either be empty or correspond to a match from a TM. Two editing operations \u2013 insertion and deletion \u2013 are considered. The former is composed of two steps: first, placeholder insertion, which predicts the position and number of new tokens; second, the predictions of tokens to fill these positions. Editing operations are applied iteratively in rounds of refinement steps until a final translation is obtained.\nIn LevT, these predictions rely on a joint encoding of the source and the current target and apply in parallel for all positions, which makes LevT a representative of non-autoregressive translation (NAT) models. As editing operations are not observed in the training data, LevT resorts to Imitation Learning, based on the generation of decoding configurations for which the optimal prediction is easy to compute. Details are in (Gu et al., 2019), see also (Xu and Carpuat, 2021), which extends it with a repositioning operation and uses it to decode with terminology constraints, as well as the studies of Niwa et al. (2022) and Xu et al. (2023) who also explore the use of LevT in conjunction with TMs."
        },
        {
            "heading": "2.3 Processing multiple fuzzy matches",
            "text": "One of the core differences between TMN-LevT and LevT is its ability to handle multiple matches. This\nimplies adapting the edit steps (in inference) and the roll-in policy (in imitation learning).\nInference in TMN-LevT Decoding follows the same key ideas as for LevT (see Figure 1) but enables co-editing an arbitrary number N of sentences. Our implementation (1) applies deletion, then placeholder insertion simultaneously on each retrieved example; (2) combines position-wise all examples into one single candidate sentence; (3) performs additional steps as in LevT: this includes first completing token prediction, then performing Iterative Refinement operations that edit the sentence to correct mistakes and improve it (\u00a73.2).\nTraining in TMN-LevT TMN-LevT is trained with imitation learning and needs to learn the edit steps described above for both the first pass (1\u20132) and the iterative refinement steps (3). This means that we teach the model to perform the sequence of correct edit operations needed to iteratively generate the reference output, based on the step-by-step reproduction of what an expert \u2019teacher\u2019 would do. For this, we need to compute the optimal operation associated with each configuration (or state) (\u00a74). The roll-in and roll-out policies specify how the model is trained (\u00a73.3)."
        },
        {
            "heading": "3 Multi-Levenshtein Transformer",
            "text": ""
        },
        {
            "heading": "3.1 Global architecture",
            "text": "TMN-LevT has two modes of operations: (a) the combination of multiple TM matches into one single sequence through alignment, (b) the iterative refinement of the resulting sequence. In step (a), we use the Transformer encoder-decoder architec-\nture, extended with additional embedding and linear layers (see Figure 2) to accommodate multiple matches. In each of the N retrieved instances y = (y1, \u00b7 \u00b7 \u00b7 ,yN ), yn,i (the ith token in the ith instance) is encoded as Eyn,i + Pi + Sn, where E \u2208 R|V|\u00d7dmodel , P \u2208 RLmax\u00d7dmodel and S \u2208 R(N+1)\u00d7dmodel are respectively the token, position and sequence embeddings. The sequence embedding identifies TM matches, and the positional encodings are reset for each yn. The extra row in S is used to identify the results of the combination and will yield a different representation for these single sequences.3 Once embedded, TM matches are concatenated and passed through multiple Transformer blocks, until reaching the last layer, which outputs (h1, \u00b7 \u00b7 \u00b7 , h|y|) for a single input match or (h1,1, \u00b7 \u00b7 \u00b7 , h1,|y1|, \u00b7 \u00b7 \u00b7 , hN,1, \u00b7 \u00b7 \u00b7 , hN,|yN |) in the case of multiple ones. The learned policy \u03c0\u03b8 computes its decisions from these hidden states. We use four classifiers, one for each sub-policy:\n1. deletion: predicts keep or delete for each token ydeln,i with a projection matrix A \u2208 R2\u00d7dmodel :\n\u03c0del\u03b8 (d|n, i,ydel1 , \u00b7 \u00b7 \u00b7 ,ydelN ;x) = softmax \u00c4 hn,iA T \u00e4\n2. insertion: predicts the number of placeholder insertions between yplhn,i and y plh n,i+1 with a pro-\njection matrix B \u2208 R(Kmax+1)\u00d72dmodel :\n\u03c0plh\u03b8 (p|n,i,y plh 1 , \u00b7 \u00b7 \u00b7 ,y plh N ;x)\n= softmax \u00c4 [hn,i, hn,i+1]B T \u00e4 ,\nwith Kmax the max number of insertions.\n3. combination: predicts if token ycmbn,i in sequence n must be kept in the combination, with a projection matrix C \u2208 R2\u00d7dmodel :\n\u03c0cmb\u03b8 (c|n, i,ycmb1 , \u00b7 \u00b7 \u00b7 ,ycmbN ;x) = softmax \u00c4 hn,iC T \u00e4 .\n4. prediction: predicts a token in vocabulary V at each placeholder position, with a projection matrix D \u2208 R|V|\u00d7dmodel :\n\u03c0tok\u03b8 (t|n, i,ytok;x) = softmax \u00c4 hjD T \u00e4\nExcept for step 3, these classifiers are similar to those used in the original LevT."
        },
        {
            "heading": "3.2 Decoding",
            "text": "Decoding is an iterative process: in a first pass, the N fuzzy matches are combined to compute a candidate translation; then, as in LevT, an additional series of iterative refinement rounds (Gu et al., 2019) is applied until convergence or timeout. Figure 3 illustrates the first pass, where N = 2 matches are first edited in parallel, then combined into one output.\nTo predict deletions (resp. insertions and token predictions), we apply the argmax operator to \u03c0del\u03b8 (resp. \u03c0plh\u03b8 , \u03c0 tok \u03b8 ). For combinations, we need to aggregate separate decisions \u03c0cmb\u03b8 (one per token and match) into one sequence. For this, at each position, we pick the most likely token.\nDuring iterative refinement, we bias the model towards generating longer sentences since LevT outputs tend to be too short (Gu et al., 2019). As in LevT, we add a penalty to the probability of inserting 0 placeholder in \u03c0plh\u03b8 (Stern et al., 2019). This only applies in the refinement steps to avoid creating more misalignments (see \u00a76.2)."
        },
        {
            "heading": "3.3 Imitation learning",
            "text": "We train TMN-LevT with Imitation Learning (Daum\u00e9 et al., 2009; Ross et al., 2011), teaching the system to perform the right edit operation for each decoding state. As these operations are unobserved in the training data, the standard approach is to simulate decoding states via a roll-in policy; for each of these, the optimal decision is computed via an expert policy \u03c0\u2217, composed of intermediate experts \u03c0del\u2217 , \u03c0 plh \u2217 , \u03c0cmb\u2217 , \u03c0 tok \u2217 . The notion of optimality is discussed in \u00a74. Samples of pairs (state, decision) are then used to train the system policy \u03c0\u03b8.\n3yop denotes an intermediary sequence before applying edit operation op. yopn \u2208 NL is encoded with Sn; yop \u2208 NL with SN .\nFirst, from the initial set of sentences yinit, the unrolling of \u03c0\u2217 produces intermediate states (ydel, del\u2217), (yplh, plh\u2217), (ycmb, cmb\u2217), (ytok, tok\u2217) (see top left in Figure 4). Moreover, in this framework, it is critical to mitigate the exposure bias and generate states that result from non-optimal past decisions (Zheng et al., 2023). For each training sample (x,y1, \u00b7 \u00b7 \u00b7 ,yN ,y\u2217), we simulate multiple additional states as follows (see Figure 4 for the full picture). We begin with the operations involved in the first decoding pass:4\n1. Additional triplets\u266f: \u03c0rnd\u00b7del\u00b7N turns y\u2217 into N random substrings, which simulates the edition of N artificial examples.\n2. Token selection\u266f (uses \u03c0sel): our expert policy never aligns two distinct tokens at a given position (\u00a74.3). We simulate such cases that may occur at inference, as follows: with probability \u03b3, each <PLH> is replaced with a random token from fuzzy matches (Figure 5).\nThe expert always completes its translation in one decoding pass. Policies used in iterative refinement are thus trained with the following simulated states, based on roll-in and roll-out policies used in LevT and its variants (Gu et al., 2019; Xu et al., 2023; Zheng et al., 2023):\n3. Add missing words (uses \u03c0rnd\u00b7del\u00b71): with probability \u03b1, ypost\u00b7plh=y\u2217. With probability 1\u2212 \u03b1, generate a subsequence ypost\u00b7plh with length sampled uniformly in [0, |y\u2217|].\n4. Correct mistakes (uses \u03c0tok\u03b8 ): using the output of token prediction ypost\u00b7del, teach the model to erase the wrongly predicted tokens.\n5. Remove extra tokens\u266f (uses \u03c0ins\u03b8 , \u03c0 tok \u03b8 ): in-\nsert placeholders in ypost\u00b7tok and predict tokens, yielding ypost\u00b7del\u00b7extra, which trains the model to delete wrong tokens. These sequences differ from case (4) in the way <PLH> are inserted.\n6. Predict token\u266f (uses \u03c0rnd\u00b7msk): each token in y\u2217 is replaced by <PLH> with probability \u03b5. As token prediction applies for both decoding steps, these states also improve the first pass.\nThe expert decisions (e.g. inserting deleted tokens like in state (3) ; or deleting wrongly predicted\n4Families of (state, decision) pairs that are novel with respect to TM-LevT are marked with \u266f.\ntokens in state (4)) associated with most states are obvious, except for the initial state and state (5), which require an optimal alignment computation."
        },
        {
            "heading": "4 Optimal Alignment",
            "text": "Training the combination operation introduced above requires specifying the expert decision for each state. While LevT derives its expert policy \u03c0\u2217 from the computation of edit distances, we introduce another formulation based on the computation of maximal covers. For N=1, these formulations can be made equivalent5 (Gusfield, 1997)."
        },
        {
            "heading": "4.1 N-way alignments",
            "text": "We formulate the problem of optimal editing as an N-way alignment problem (see figure 6) that we define as follows. Given N examples (y1, \u00b7 \u00b7 \u00b7 ,yN ) and the target sentence y\u2217, a N-way alignment of (y1, \u00b7 \u00b7 \u00b7 ,yN ) w.r.t. y\u2217 is represented as a bipartite graph (V, V\u2217, E), where V is further partitioned into N mutually disjoint subsets V1 . . . VN . Vertices in each Vn (resp. V\u2217) correspond to tokens in yn (resp. y\u2217). Edges (n, i, j) \u2208 E connect node i\n5When the cost of replace is higher than insertion + deletion. This is the case in the original LevT code.\nin Vn to node j in V\u2217. An N-way alignment satisfies properties (i)-(ii):\n(i) Edges connect identical (matching) tokens: (n, i, j) \u2208 E \u21d2 yn,i = y\u2217,j .\n(ii) Edges that are incident to the same subset Vn do not cross: (n, i, j), (n, i\u2032, j\u2032) \u2208 E \u21d2 (i\u2032\u2212i)(j\u2032\u2212j) > 0.\nAn optimal N-way alignment E\u2217 maximizes the coverage of tokens in y\u2217, then the total number of edges, where y\u2217,j is covered if there exists at least one edge (n, i, j) \u2208 E. Denoting E the set of alignments maximizing target coverage:\nE = argmax E |{y\u2217,j : \u2203(n, i), (n, i, j) \u2208 E}|.\nE\u2217 = argmax E\u2208E\n|E|."
        },
        {
            "heading": "4.2 Solving optimal alignment",
            "text": "Computing the optimal N-way alignment is NPhard (see Appendix D). This problem can be solved using Dynamic Programming (DP) techniques similar to Multiple Sequence Alignment (MSA) (Carrillo and Lipman, 1988) with a complexity O(N |y\u2217| \u220f n |yn|). We instead implemented the following two-step heuristic approach:\n1. separately compute alignment graphs between each yn and y\u2217, then extract k-best 1-way alignments {En,1 . . . En,k}. This requires time O(k|yn||y\u2217|) using DP (Gusfield, 1997);\n2. search for the optimal recombination of these graphs, selecting 1-way alignments (E1,k1 . . . EN,kN ) to form E\u2217 = \u22c3 nEn,kn .\nAssuming N and k are small, we perform an exhaustive search in O(kN )."
        },
        {
            "heading": "4.3 From alignments to edits",
            "text": "From an alignment (V, V\u2217, E), we derive the optimal edits needed to compute y\u2217, and the associated intermediary sequences. Edges in E indicate the tokens that are preserved throughout this process:\n1. deletion: \u2200n,\u2200i,yn,i is kept only if (n, i, j) \u2208 E for some j; otherwise it is deleted. The resulting sequences are {yplhn }n=1...N .\n2. insertion: Placeholders are inserted between successive tokens in all yplhn , resulting in the set {ycmbn }n=1...N , under the constraints that (a) all ycmbn have the same length as y\u2217 and (b) non-placeholder tokens ycmbn,i are equal to the reference token y\u2217,i.\n3. combination: Sequences {ycmbn }n=1...N are combined into ytok such that for each position i, ycmbn,i \u0338= <PLH> \u21d2 ytoki = ycmbn,i . If \u2200n,ycmbn,i = <PLH>, then ytoki = <PLH>.\n4. prediction: The remaining <PLH> symbols in ytok are replaced by the corresponding target token in y\u2217 at the same position.\nThe expert policy \u03c0\u2217 edits examples y1, \u00b7 \u00b7 \u00b7 ,yN into y\u2217 based on the optimal alignment (V, V\u2217, E\u2217). It comprises \u03c0del\u2217 , \u03c0 plh \u2217 , \u03c0cmb\u2217 , and \u03c0 tok \u2217 , corresponding to the four steps listed above."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Data and metrics",
            "text": "We focus on translation from English to French and consider multiple domains. This allows us to consider a wide range of scenarios, with a varying density of matching examples: our datasets include\nECB, EMEA, Europarl, GNOME, JRC-Acquis, KDE4, PHP, Ubuntu, where high-quality matches are often available, but also News-Commentary, TED2013, and Wikipedia, where matches are more scarce (see Table 6, \u00a7B).\nFor each training sample (x,y), we retrieve up to 3 in-domain matches. We filter matches xn to keep only those with \u2206(x,xn) > 0.4. We then manually split each of the 11 datasets into train, valid, test-0.4, test-0.6, where the valid and test sets contain 1,000 lines each. test-0.4 (resp. test-0.6) contains samples whose best match is in the range [0.4, 0.6[ (resp. [0.6, 1[). As these two test sets are only defined based on the best match score, it may happen that some test instances will only retrieve 1 or 2 close matches (statistics are in Table 6).\nFor the pre-training experiments (\u00a76.2), we use a subsample of 2M random sentences from WMT\u201914. For all data, we use Moses tokenizer and 32k BPEs trained on WMT\u201914 with SentencePiece (Kudo, 2018). We report BLEU scores (Papineni et al., 2002) and ChrF scores (Popovic\u0301, 2015) as computed by SacreBLEU (Post, 2018) and COMET scores (Rei et al., 2020)."
        },
        {
            "heading": "5.2 Architecture and settings",
            "text": "Our code6 extends Fairseq7 implementation of LevT in many ways. It uses Transformer models (Vaswani et al., 2017) (parameters in Appendix A). Roll-in policy parameters (\u00a73.3) are empirically set as: \u03b1=0.3, \u03b2=0.2, \u03b3=0.2, \u03b4=0.2, \u03b5=0.4. The AR baseline uses OpenNMT (Klein et al., 2017) and uses the same data as TM-LevT (Appendix A)."
        },
        {
            "heading": "6 Results",
            "text": ""
        },
        {
            "heading": "6.1 The benefits of multiple matches",
            "text": "We compare two models in Table 2: one trained with one TM match, the other with three. Each\n6https://github.com/Maxwell1447/fairseq 7https://github.com/facebookresearch/fairseq\nmodel is evaluated with, at most, the same number of matches seen in training. This means that TM1-LevT only uses the 1-best match, even when more examples are found. In this table, test sets test-0.4 and test-0.6 are concatenated, then partitioned between samples for which exactly 1, 2, and 3 matches are retrieved. We observe that TM3-LevT, trained with 3 examples, consistently achieves better BLEU and ChrF scores than TM1-LevT, even in the case N=1, where we only edit the closest match.8 These better BLEU scores are associated with a larger number of copies from the retrieved instances, which was our main goal (Table 3). Similar results for the other direction are reported in the appendix \u00a7 E (Table 7).\nWe report the performance of systems trained using N=1, 2, 3 for each domain and test set in Table 4 (BLEU) and 12 (COMET). We see comparable average BLEU scores for N=1 and N=3, with large variations across domains, from which we conclude that: (a) using 3 examples has a smaller return when the best match is poor, meaning that bad matches are less likely to help (test-0.4 vs. test0.6); (b) using 3 examples seems advantageous for narrow domains, where training actually exploits several close matches (see also Appendix F). We finally note that COMET scores9 for TM3-LevT are\n8This is because the former model has been fed with more examples during training, which may help regularization.\n9Those numbers are harder to interpret, given the wide range of COMET scores across domains (from \u2248 -40 to +86).\nalways slightly lower than for TM1-LevT, which prompted us to develop several extensions.\n6.2 Improving TMN-LevT\nRealignment In preliminary experiments, we observed that small placeholder prediction errors in the first decoding pass could turn into catastrophic misalignments (Figure 7). To mitigate such cases, we introduce an additional realignment step during inference, where some predicted placeholders are added/removed if this improves the global alignment. Realignment is formulated as an optimization problem aimed to perform a tradeoff between the score \u2212 log \u03c0plh\u03b8 of placeholder insertion and an alignment cost (see Appendix C).\nWe assess realignment for N=3 (Tables 4 and 12) and observe small, yet consistent average gains (+0.2 BLEU, +1.5 COMET) for both test sets.\nPre-training Another improvement uses pretraining with synthetic data. For each source/target pair (x,y) in the pre-training corpus, we simulate N fuzzy matches by extracting from y N substrings yn of length \u2248 |y| \u00b7 r, with r \u2208 [0, 1]. Each yn is then augmented as follows:\n1. We randomly insert placeholders to increase the length by a random factor between 1 and 1 + f , f = 0.5 in our experiments.\n2. We use the CamemBERT language model (Martin et al., 2020) to fill the masked tokens.\nThese artificial instances simulate diverse fuzzy matches and are used to pre-train a model, using the same architecture and setup as in \u00a75.2. Pretraining yields markedly higher scores than the baseline (+1.3 BLEU, +6.4 COMET for test-0.4 and +0.9 BLEU, +4.6 COMET for test-0.6). Training curves also suggest that pre-trained models are faster to converge. Combining with realignment yields additional gains for TM3-LevT, which outperforms TM1-LevT in all domains and both metrics.\nKnowledge distillation Knowledge Distillation (KD) (Kim and Rush, 2016) is used to mitigate the effect of multimodality of NAT models (Zhou et al., 2020) and to ease the learning process. We trained a TMN-LevT model with distilled samples (x, y\u03031, \u00b7 \u00b7 \u00b7 , y\u0303N , y\u0303), where automatic translations y\u0303i and y\u0303 are derived from their respective source xi and x with an auto-regressive teacher trained with a concatenation of all the training data.\nWe observe that KD is beneficial (+0.3 BLEU) for low-scoring matches (test-0.4) but hurts performance (-1.7 BLEU) for the better ones in test-0.6. This may be because the teacher model, with a BLEU score of 56.7 on the test-0.6, fails to provide the excellent starting translations the model can access when using non-distilled data."
        },
        {
            "heading": "6.3 Ablation study",
            "text": "We evaluate the impact of the various elements in the mixture roll-in policy via an ablation study (Table 13). Except for \u03c0sel, every new element in the roll-in policy increases performance. As for \u03c0sel, our system seems to be slightly better with than without. An explanation is that, in case of misalignment, the model is biased towards selecting the first, most similar example sentence. As an ablation, instead of aligning by globally maximizing coverage (\u00a7 4.2), we also compute alignments that maximize coverage independently as in figure 6a.\nA complete run of TMN-LevT is in Appendix F."
        },
        {
            "heading": "7 Related Work",
            "text": "As for other Machine Learning applications, such as text generation (Guu et al., 2018), efforts to integrate a retrieval component in neural-based MT have intensified in recent years. One motivation is to increase the transparency of ML models by providing users with tangible traces of their internal computations in the form of retrieved examples (Rudin, 2019). For MT, this is achieved by integrating fuzzy matches retrieved from memory as an additional conditioning context. This can be performed simply by concatenating the retrieved target instance to the source text (Bulte and Tezcan, 2019), an approach that straightforwardly accommodates several TM matches (Xu et al., 2020), or the simultaneous exploitation of their source and target sides (Pham et al., 2020). More complex schemes to combine retrieved examples with the source sentence are in (Gu et al., 2018; Xia et al.,\n2019; He et al., 2021b). The recent work of Cheng et al. (2022) handles multiple complementary TM examples retrieved in a contrastive manner that aims to enhance source coverage. Cai et al. (2021) also handle multiple matches and introduce two novelties: (a) retrieval is performed in the target language and (b) similarity scores are trainable, which allows to evaluate retrieved instances based on their usefulness in translation. Most of these attempts rely on an auto-regressive (AR) decoder, meaning that the impact of TM match(es) on the final output is only indirect.\nThe use of TM memory match with a NAT decoder is studied in (Niwa et al., 2022; Xu et al., 2023; Zheng et al., 2023), which adapt LevT for this specific setting, using one single retrieved instance to initialize the edit-based decoder. Other evolutions of LevT, notably in the context of constraint decoding, are in (Susanto et al., 2020; Xu and Carpuat, 2021), while a more general account of NAT systems is in (Xiao et al., 2023).\nZhang et al. (2018) explore a different set of techniques to improve translation using retrieved segments instead of full sentences. Extending KNNbased language models (He et al., 2021a) to the conditional case, Khandelwal et al. (2021) proposes knearest neighbor MT by searching for target tokens that have similar contextualized representations at each decoding step, an approach further elaborated by Zheng et al. (2021); Meng et al. (2022) and extended to chunks by Martins et al. (2022)."
        },
        {
            "heading": "8 Conclusion and Outlook",
            "text": "In this work, we have extended the Levenshtein Transformer with a new combination operation, making it able to simultaneously edit multiple fuzzy matches and merge them into an initial translation that is then refined. Owing to multiple algorithmic contributions and improved training schemes, we have been able to (a) increase the number of output tokens that are copied from retrieved examples; (b) obtain performance improvements compared to using one single match. We have also argued that retrieval-based NMT was a simple way to make the process more transparent for end users.\nNext, we would like to work on the retrieval side of the model: first, to increase the diversity of fuzzy matches e.g. thanks to contrastive retrieval, but also to study ways to train the retrieval mechanism and extend this approach to search monolingual (target side) corpora. Another line of work will\ncombine our techniques with other approaches to TM-based NMT, such as keeping track of the initial translation(s) on the encoder side."
        },
        {
            "heading": "9 Limitations",
            "text": "As this work was primarily designed a feasibility study, we have left aside several issues related to performance, which may explain the remaining gap with published results on similar datasets. First, we have restricted the encoder to only encode the source sentence, even though enriching the input side with the initial target(s) has often been found to increase performance (Bulte and Tezcan, 2019), also for NAT systems (Xu et al., 2023). It is also likely that increasing the number of training epochs would yield higher absolute scores (see Appendix F).\nThese choices were made for the sake of efficiency, as our training already had to fare with the extra computing costs incurred by the alignment procedure required to learn the expert policy. Note that in comparison, the extra cost of the realignment procedure is much smaller, as it is only paid during inference and can be parallelized on GPUs.\nWe would also like to outline that our systems do not match the performance of an equivalent AR decoder, a gap that remains for many NAT systems (Xiao et al., 2023). Finally, we have only reported here results for one language pair \u2013 favoring here domain diversity over language diversity \u2013 and would need to confirm the observed improvements on other language pairs and conditions."
        },
        {
            "heading": "10 Acknowledgements",
            "text": "This work was performed using HPC resources from GENCI-IDRIS (Grant 2022-AD011013583) and Lab-IA from Saclay-IA.\nThe authors wish to thank Dr. Jitao Xu and Dr. Caio Corro for their guidance and help."
        },
        {
            "heading": "A Model Configuration",
            "text": "We use a Transformer architecture with embeddings of dimension 512; feed-forward layers of size 2048; number of heads 8; number of encoder and decoder layers: 6; batch size: 3000 tokens; sharedembeddings; dropout: 0.3; number of GPUs: 6. The maximal number of additional placeholders is Kmax = 64.\nDuring training, we use Adam optimizer with (\u03b21, \u03b22)=(0.9, 0.98); inverse sqrt scheduler; learning rate: 5e\u22124; label smoothing: 0.1; warmup updates: 10,000; float precision: 16. We fixed the number of iterations at 60k. For decoding, we use iterative refinement with an empty placeholder penalty of 3, and a max number of iterations of 10 (Gu et al., 2019).\nFor the n-way alignment (\u00a74.1), we use k=10. The hyper-parameters of the realigner (\u00a7C) were tuned on a subset of 1k samples extracted from the ECB training set.\nMetrics are used with default settings: SacreBLEU signature is nrefs:1|case:mixed|eff:no |tok:13a|smooth:exp|version:2.1.0; the ChrF signature is nrefs:1|case:mixed|eff:yes |nc:6|nw:0|space:no|version:2.1.0; as for COMET we use the default model of version 1.1.3: Unbabel/wmt22-comet-da."
        },
        {
            "heading": "B Data Analysis",
            "text": "Table 6 contains statistics about all 11 domains. They notably highlight the relationship between the average number of retrieved sentences during training and the ability of TM3-LevT to perform better than TM1-LevT in Table 4. The domains with retrieval rates lesser than 1 (Epp, News, TED, Ubu) have quite a broad content, meaning that training instances have fewer close matches, which also means that for these domains, TM3-LevT hardly sees two or three examples that it needs to use in inference."
        },
        {
            "heading": "C Realignment",
            "text": "The realignment process is an extra inference step aiming to improve the result of the placeholder insertion stage. To motivate our approach, let us con-\nsider the following sentences before placeholder insertion:\nyplh0 : < A B C > \u00d7 yplh1 : < B C > \u00d7 \u00d7 yplh2 : < A D C D >,\nwhere letters represent tokens, \u00d7 denotes padding, < and > respectively stand for <BOS> and <EOS>.\nThe output of this stage is a prediction for all pairs of consecutive tokens. This prediction takes the form of a tensor log \u03c0plh\u03b8 of dimensions N \u00d7 (L\u22121)\u00d7(Kmax+1), corresponding respectively to the number of retrieved sentences N , the maximum sentence length L, and the maximum number of additional placeholders Kmax.\nLet P (a N\u00d7(L\u22121) tensor) denote the argmax,\ne.g. P = 0 0 0 2 0 0 0 1 0 0 0 1 0 0 0\nInserting the prescribed number of placeholders (figured by _) then yields the following ycmb: ycmb0 : < A B C _ _ > ycmb1 : < B C _ > \u00d7 \u00d7 ycmb2 : < A _ D C D > This result is far from perfect, as it fails to align the repeated occurrences of C. For instance, a preferable alignment requiring 3 changes (1 change consists in a modification of \u00b11 in P ) could be:\nycmb0 \u2032: < A B C _ > \u00d7 ycmb1 \u2032: < _ B C _ > \u00d7 ycmb2 \u2032: < A D C D > \u00d7\nThe general goal of realignment is to improve such alignments by performing a small number of changes in P . We formalize this problem as a search for a good tradeoff between (a) the individual placeholder prediction scores, aggregated in LL (likelihood loss) and (b) LA an alignment loss. Under its simplest form, this problem is again an optimal multisequence alignment problem, for which exact dynamic programming solutions are computationally intractable in our setting.\nWe instead develop a continuous relaxation that can be solved effectively with SGD and is also easy to parallelize on GPUs. We, therefore, relax the integer condition for P and assume that Pi,j can take continuous real values in [0,Kmax], then solve the continuous optimization problem before turning Pi,j values back into integers.\nThe likelihood loss aims to keep the Pi,j values close to the model predictions. Denoting (\u00b5, \u03c3) respectively the mean and variance of the model\npredictions, our initial version of this loss is\nLL(P ) = \u2211 i,j (Pi,j \u2212 \u00b5i,j)2 2\u03c32 .\nIn practice, we found that using a weighted average \u00b5\u0302 and clamping the variance \u03c3\u03022 both yield better realignments, yielding:\nLL(P ) = \u2211 i,j (Pi,j \u2212 \u00b5\u0302i,j)2 2\u03c3\u03022\nTo define the alignment loss, we introduce a position matrix X of dimension N \u00d7L in R+, where Xn,i corresponds to the (continuous)position of token yn,i after inserting a real number of placeholders. X is defined as:\nXn,i(P ) = i+ \u2211 j<i Pn,j\nwith i the number of tokens occuring before Xn,i and \u2211 j<i Pn,j the cumulated sum of placeholders. Using X , we derive the distance tensor D of dimension N \u00d7 L\u00d7N \u00d7 L in R+ as:\nDn,i,m,j(P ) = |Xn,i \u2212Xm,j |\nFinally, let G be an N \u00d7 L\u00d7N \u00d7 L alignment graph tensor, where Gn,i,m,j = 1 if and only if yn,i = ym,j and n \u0338= m and Dn,i,m,j < Dmax. G connects identical tokens in different sentences when their distance after placeholder insertion is at most Dmax. This last condition avoids perturbations from remote tokens that coincidentally appear to be identical.\nEach token yn,i is associated with an individual loss:\ndn,i(P ) =  min m,j {Dn,i,m,j(P ) : Gn,i,m,j = 1}\nif \u2203(m, j) s.t. Gn,i,m,j = 1 0 otherwise.\nThe alignment loss aggregates these values over sentences and positions as:\nLA(P ) = N\u22121\u2211 n=0 L\u22121\u2211 i=0 dn,i(P )\nA final ingredient in our realignment model is related to the final discretization step. To avoid rounding errors, we further constrain the optimization process to deliver near-integer solutions. For this, we also include a integer constraint loss defined as :\nLint(P ) = \u00b5t \u2211 i,j sin2(\u03c0Pi,j)\nwhere \u00b5t controls the scale of Lint(P ). As x \u2192 sin2(\u03c0x) reaches its minimum 0 for integer values, minimizing Lint(P ) has the effect of enforcing a near-integer constraint to our solutions. Overall, we minimize in P :\nL = LL(P ) + LA(P ) + Lint(P ),\nslowly increasing the scale of \u00b5t according to the following schedule\n\u00b5t =  0 if t < t0 \u00b5T if t > T \u00b5T\n(t\u2212t0)2 (T\u2212t0)2 otherwise\n,\nwith t0, T the timestamps for respectively the activation of the integer constraint loss, and the activation of the clamping. This optimization is performed with gradient descent directly on GPUs, with a small additional cost to the inference procedure."
        },
        {
            "heading": "D NP-hardness of Coverage Maximization in N-way Alignment",
            "text": "Given the set of possible N-way alignments, the problem of finding the one that maximizes the target coverage is NP-hard. To prove it, we can reduce the NP-hard set cover problem (Garey and Johnson, 1979) to the N-way alignment coverage maximization problem.\n\u2022 Cover set decision problem (A):\nLet X = {x1, \u00b7 \u00b7 \u00b7 , xN} and C0 \u2282 2X . Is there c\u2217 = (c1, \u00b7 \u00b7 \u00b7 , cK) \u2208 CK0 s.t. | \u222ak c\u2217k| = |X|?\n\u2022 N-way alignment coverage maximization decision problem (B):\nLet X = {x1, \u00b7 \u00b7 \u00b7 , xN} and C = (C1, \u00b7 \u00b7 \u00b7 , CK) \u2282 (2X)K . For p \u2208 N, is there c \u2208 \u220fK k=1Ck s.t. | \u222ak ck| \u2265 p?\nA solution of (B) can be certified in polynomial time: we simply compute the cardinal of a union. Any instance of (A) can be transformed in polynomial time and space into a special instance of (B) where all Ck = C0 and p = |X|."
        },
        {
            "heading": "E Results for fr-en",
            "text": "Table ?? reports the BLEU scores for the reverse direction (fr\u2192en), using exactly the same configuration as in Table 2. Note that since we used the same data split (retrieving examples based on the similarity in English), and since the retrieval procedure is asymmetrical, 4,749 test samples happen to have no match. That would correspond to an extra column labeled \"0\", which is not represented here.\nThe reverse direction follows a similar pattern, providing further evidence of the method\u2019s effectiveness."
        },
        {
            "heading": "F Complementary Analyses",
            "text": "Diversity and difficulty Results in Table 4 show that some datasets do not seem to benefit from multiple examples. This is notably the case for Europarl, News-Commentary, TED2013, and Ubuntu. We claim that this was due to the lack of retrieved examples at training (as stated in \u00a7B), of diversity, and the noise in fuzzy matches. To further investigate this issue, we report two scores in Table 8. The first is the increase of bag-of-word coverage\nof the target gained by using N=3 instead of N=1; the second is the increase of noise in the examples, computed as the proportion of tokens in the examples that do not occur in the target. We observe that, in fact, low diversity is often associated with poor scores for TM3-LevT, and higher diversity with better performance.\nLong run All results in the main text were obtained with models trained for 60k iterations, which was enough to compare the various models while saving computation resources. For completeness, we also performed one longer training for 300k iterations for TM3-LevT (see Table 9), which resulted in an improvement of around +2 BLEU for each test set. This is without realignment nor pretraining.\nThe Benefits of realignment Table 10 shows that realignment also decreases the average number of refinement steps to converge. These results suggest that the edition is made easier with realignment.\nIn Table 11, we present detailed results of the unigram modified precision of LevT, TM3-LevT and\nTM3-LevT+realign. Using more examples indeed increases copy (+4.4), even though it diminishes copy precision (-1.7). Again we observe the positive effect of realignment, which amplifies the tendency of our model to copy input tokens.\nCOMET scores We compute COMET scores (Rei et al., 2020) separately for each domain with default wmt20-comet-da similarly to Table 4 (see Table 12). We observe that the basic version of TM3-LevTunderperforms TM1-LevT; we also see a great variation in the scores. A possible explanation can be a fluency decline when using multiple examples, which is not represented by the precision scores computed by BLEU. The improved version, using realignment and pre-training, confirms that adding more matches is overall beneficial for MT quality.\nPer-domain ablation study Table 13 details the results of our ablation study separately for each domain.\nIllustration A full inference run is in Table 14, illustrating the benefits of considering multiple examples and realignment. Even though the realignment does not change here the final output, it reduces the number of atomic edits needed to generate it, making the inference more robust."
        }
    ],
    "title": "Towards Example-Based NMT with Multi-Levenshtein Transformers",
    "year": 2023
}