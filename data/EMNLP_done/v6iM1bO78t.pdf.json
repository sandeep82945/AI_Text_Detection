{
    "abstractText": "Current practices regarding data collection for natural language processing on Amazon Mechanical Turk (MTurk) often rely on a combination of studies on data quality and heuristics shared among NLP researchers. However, without considering the perspectives of MTurk workers, these approaches are susceptible to issues regarding workers\u2019 rights and poor response quality. We conducted a critical literature review and a survey of MTurk workers aimed at addressing open questions regarding best practices for fair payment, worker privacy, data quality, and considering worker incentives. We found that worker preferences are often at odds with received wisdom among NLP researchers. Surveyed workers preferred reliable, reasonable payments over uncertain, very high payments; reported frequently lying on demographic questions; and expressed frustration at having work rejected with no explanation. We also found that workers view some quality control methods, such as requiring minimum response times or Master\u2019s qualifications, as biased and largely ineffective. Based on the survey results, we provide recommendations on how future NLP studies may better account for MTurk workers\u2019 experiences in order to respect workers\u2019 rights and improve data quality.",
    "authors": [
        {
            "affiliations": [],
            "name": "Olivia Huang"
        },
        {
            "affiliations": [],
            "name": "Eve Fleisig"
        },
        {
            "affiliations": [],
            "name": "Dan Klein"
        }
    ],
    "id": "SP:27a5a3fae0047a68a694006e1b62ae318a38e490",
    "references": [
        {
            "authors": [
                "Kimberly A Arditte",
                "Demet \u00c7ek",
                "Ashley M Shaw",
                "Kiara R Timpano"
            ],
            "title": "The importance of assessing clinical phenomena in Mechanical Turk research",
            "year": 2016
        },
        {
            "authors": [
                "Mana Ashida",
                "Mamoru Komachi."
            ],
            "title": "Towards automatic generation of messages countering online hate speech and microaggressions",
            "venue": "Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH), pages 11\u201323, Seattle, Washington (Hybrid).",
            "year": 2022
        },
        {
            "authors": [
                "Chris Callison-Burch",
                "Mark Dredze."
            ],
            "title": "Creating speech and language data with Amazon\u2019s Mechanical Turk",
            "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk, CSLDAMT \u201910, page",
            "year": 2010
        },
        {
            "authors": [
                "Lior Gideon."
            ],
            "title": "The art of question phrasing",
            "venue": "Handbook of Survey Methodology for the Social Sciences, pages 91\u2013107. Springer.",
            "year": 2012
        },
        {
            "authors": [
                "Fabrizio Gilardi",
                "Meysam Alizadeh",
                "Ma\u00ebl Kubli."
            ],
            "title": "ChatGPT outperforms crowd-workers for textannotation tasks",
            "venue": "ArXiv, abs/2303.15056.",
            "year": 2023
        },
        {
            "authors": [
                "Mary L. Gray",
                "Siddharth Suri."
            ],
            "title": "Ghost work how to stop Silicon Valley from building a new global underclass",
            "venue": "Houghton Mifflin Harcourt.",
            "year": 2019
        },
        {
            "authors": [
                "Kotaro Hara",
                "Abigail Adams",
                "Kristy Milland",
                "Saiph Savage",
                "Chris Callison-Burch",
                "Jeffrey P. Bigham."
            ],
            "title": "A data-driven analysis of workers\u2019 earnings on Amazon Mechanical Turk",
            "venue": "Proceedings of the 2018 CHI Conference on Human Factors in Com-",
            "year": 2018
        },
        {
            "authors": [
                "Jennifer Hughes",
                "Abigail Camden",
                "Tenzin Yangchen."
            ],
            "title": "Rethinking and updating demographic questions: Guidance to improve descriptions of research samples",
            "venue": "Psi Chi Journal of Psychological Research, 21:138\u2013151.",
            "year": 2016
        },
        {
            "authors": [
                "Abigail Z. Jacobs",
                "Hanna Wallach."
            ],
            "title": "Measurement and fairness",
            "venue": "Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 375\u2013385, New York, NY, USA. Association for Computing Machinery.",
            "year": 2021
        },
        {
            "authors": [
                "Jason T. Jacques",
                "Per Ola Kristensson."
            ],
            "title": "Crowdworker economics in the gig economy",
            "venue": "Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI \u201919, page 1\u201310, New York, NY, USA. Association for Computing Machin-",
            "year": 2019
        },
        {
            "authors": [
                "Raquel Justo",
                "M. Torres",
                "Jos\u00e9 Alcaide."
            ],
            "title": "Measuring the quality of annotations for a subjective crowdsourcing task",
            "venue": "pages 58\u201368.",
            "year": 2017
        },
        {
            "authors": [
                "Marzena Karpinska",
                "Nader Akoury",
                "Mohit Iyyer."
            ],
            "title": "The perils of using Mechanical Turk to evaluate open-ended text generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1265\u20131285, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Jane Paik Kim",
                "Katie Ryan",
                "Tenzin Tsungmey",
                "Max Kasun",
                "Willa A. Roberts",
                "Laura B. Dunn",
                "Laura Weiss Roberts"
            ],
            "title": "Perceived protectiveness of research safeguards and influences on willingness to participate in research: A novel mturk pilot study",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan K. Kummerfeld."
            ],
            "title": "Quantifying and avoiding unfair qualification labour in crowdsourcing",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Matthew Lease",
                "Jessica Hullman",
                "Jeffrey Bigham",
                "Michael Bernstein",
                "Juho Kim",
                "Walter Lasecki",
                "Saeideh Bakhshi",
                "Tanushree Mitra",
                "Robert Miller."
            ],
            "title": "Mechanical Turk is not anonymous",
            "venue": "SSRN Electronic Journal.",
            "year": 2013
        },
        {
            "authors": [
                "Ellie Pavlick",
                "Tom Kwiatkowski."
            ],
            "title": "Inherent disagreements in human textual inferences",
            "venue": "Transactions of the Association for Computational Linguistics, 7:677\u2013694.",
            "year": 2019
        },
        {
            "authors": [
                "Eyal Peer",
                "Joachim Vosgerau",
                "Alessandro Acquisti."
            ],
            "title": "Reputation as a sufficient condition for data quality on Amazon Mechanical Turk",
            "venue": "Behavior research methods, 46:1023\u20131031.",
            "year": 2014
        },
        {
            "authors": [
                "Billy Perrigo."
            ],
            "title": "Inside Facebook\u2019s African sweatshop",
            "venue": "TIME.",
            "year": 2022
        },
        {
            "authors": [
                "Maarten Sap",
                "Dallas Card",
                "Saadia Gabriel",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "The risk of racial bias in hate speech detection",
            "venue": "ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Maarten Sap",
                "Saadia Gabriel",
                "Lianhui Qin",
                "Dan Jurafsky",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Social bias frames: Reasoning about social and power implications of language",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Boaz Shmueli",
                "Jan Fell",
                "Soumya Ray",
                "Lun-Wei Ku."
            ],
            "title": "Beyond fair pay: Ethical implications of NLP crowdsourcing",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2021
        },
        {
            "authors": [
                "Rion Snow",
                "Brendan O\u2019Connor",
                "Daniel Jurafsky",
                "Andrew Ng"
            ],
            "title": "Cheap and fast \u2013 but is it good? evaluating non-expert annotations for natural language tasks",
            "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Process-",
            "year": 2008
        },
        {
            "authors": [
                "Veniamin Veselovsky",
                "Manoel Horta Ribeiro",
                "Robert West."
            ],
            "title": "Artificial artificial artificial intelligence: Crowd workers widely use large language models for text production tasks",
            "venue": "ArXiv, abs/2306.07899.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Amazon Mechanical Turk (MTurk) is an online survey platform that has become increasingly popular for NLP annotation tasks (Callison-Burch and Dredze, 2010). However, data collection on MTurk also runs the risk of gathering noisy, low-quality responses (Snow et al., 2008) or violating workers\u2019 rights to adequate pay, privacy, and overall treatment (Xia et al., 2017; Hara et al., 2018; Kim et al., 2021; Lease et al., 2013; Shmueli et al., 2021). We conducted a critical literature review that identifies key concerns in five areas related to NLP data collection on MTurk and observed MTurk worker\nopinions on public forums such as Reddit, then developed a survey that asked MTurk workers about open questions in the following areas:\nTask clarity. How can requesters ensure that workers fully understand a task and complete it accurately? Surveyed workers expressed a desire for more examples and more detailed instructions, and nearly half desired more information about the downstream context in which an annotation will be used. In addition, they indicated that task clarity is a major factor in their judgment of how responsible and reliable a requester is (Section 3).\nPayment. What is an appropriate level of pay on MTurk? Are there tradeoffs to different amounts or methods of pay? Respondents indicated that there is often a threshold pay below which they will refuse to complete a task, suggesting that pay rates below minimum wage are not only unethical, but also counterproductive to worker recruitment. However, very high pay attracts spammers, which moreover forces attentive workers to complete tasks more quickly before spammers take them all, meaning that response quality likely decreases across all workers. Workers expressed mixed opinions over bonus payments, suggesting that they are most effective if they are added to a reasonable base pay and workers trust the requester to deliver on the bonus. (Section 4).\nPrivacy. What methods best ensure worker privacy on MTurk? How do workers react to perceived privacy violations? Workers report that they often respond to questions for personal information untruthfully if they are concerned about privacy violations. Nearly half of workers reported lying on questions about demographic information, which means that tasks relying on demographic data collection should be very careful about collecting such information on MTurk (if at all). Some of these issues can be partially mitigated if requesters\nhave good reputations among workers and are clear about the purpose of personal data collection, since spam requesters are common on MTurk. (Section 4).\nResponse quality. What methods best ensure high-quality responses and minimize spam? Requesters must consider how traditional response filters have the side effect of influencing workers\u2019 perception of the task, and in turn, their response quality. The rising importance of incorporating worker perspectives is underscored by the establishment of organizations such as Turkopticon, which consolidates workers\u2019 reviews of requesters and advocates for MTurk workers\u2019 rights, including petitioning to \u201cend the harm that mass rejections cause\u201d (Turkopticon, 2022). We find that workers consider some common quality control methods ineffective and unfair. For example, workers reported that minimum response times often make them simply wait and do other things until they hit the minimum time, while allowing only workers with Masters qualifications to complete a task excludes all workers who joined after 2019, since the qualification is no longer offered. In addition, workers are very concerned about their approval rates because requesters often filter for workers with very high approval rates. Respondents expressed frustration over having work rejected, especially automatically or without justification, and receiving no pay for their labor. Workers thus avoid requesters with low approval rates or poor communication (Section 6).\nSensitive content. Tasks in which workers annotate sensitive content can pose a psychological risk, particularly since MTurk workers often face mental health issues (Arditte et al., 2016). Though it is common practice to put general content warnings before such tasks (e.g. \u201coffensive content\u201d), nearly a third of surveyed workers expressed a desire for specific content warnings (e.g., \u201chomophobia\u201d) despite this being uncommon in current research practices (Section 7).\nCommon themes emerge across these topics regarding the importance of maintaining a good reputation as a requester, understanding worker incentives, and communicating clearly with workers. In Section 8, we discuss the implications of worker preferences for survey design and provide recommendations for future data collection on MTurk."
        },
        {
            "heading": "2 Survey Design",
            "text": "We conducted a literature review and examined posts on r/mturk, a Reddit forum that MTurk workers use to discuss the platform, to identify the areas of uncertainty discussed in sections 3-7. Then, to collect data on MTurk workers\u2019 experiences, we conducted a Qualtrics survey posted as a task on MTurk. Our survey was broadly subdivided into sections on payment, sensitive questions, response quality, and miscellaneous issues (mainly task clarity and context). As a preliminary filter for response quality, we required that all respondents have a 97% HIT approval rate, at least 100 HITs completed, minimum of 18 years of age, and English fluency. This is a lower HIT approval rate than is typically used, which can increase the rate of spam; however, we aimed to collect opinions of workers who might be excluded by high approval rate filters, and therefore manually reviewed the data afterwards to remove spam responses by examining responses to the required free text fields. We collected 207 responses from our survey over one week, of which 59 responses were dropped for spam or incompleteness (we discuss further implications of high spam rates in Section 8). Each annotator was paid $2.50 to complete the survey, based on the estimated completion time and a minimum wage of $15/hour. Appendices A and B contain details on informed consent, the full text of the survey, and data cleaning details.1\nThe following sections address each area of concern raised in Section 1, discussing previous research on MTurk annotation practices, the open questions raised by the literature, the survey questions we included on those topics, and the results and implications of the survey responses."
        },
        {
            "heading": "3 Task Clarity",
            "text": "On MTurk, requesters publish \u201chuman intelligence tasks\u201d (or HITs) for MTurk workers to complete in exchange for a monetary incentive. Best practices for question phrasing and ordering are consistent with overall survey design guidelines, which encourage including clear instructions for how to use the platform, clearly outlining requirements for acceptance, including label definitions and examples, and avoiding ambiguous language (Gideon, 2012). To look for strategies for minimizing confusion in survey questions, our survey asked what additional\n1This study underwent IRB review and annotators provided informed consent prior to participation.\nprovided information, such as context and purpose of the task, would be most helpful for reducing ambiguity or confusion.\nTo ensure that results are accurate and useful, it is important to carefully conceptualize the task (define what quality is being measured by the study) and operationalize it (decide how the study will measure the construct defined by the conceptualization) along with clear compensation details (discussed further in Section 4 and Jacobs and Wallach, 2021). In a Reddit thread on r/mturk, worker u/dgrochester55 (2022) stated that \u201cwe are not likely to take your survey seriously if you as a requester aren\u2019t taking your own survey seriously\u201d due to issues such as \u201cpoor pay, irrelevant subject\u201d or being \u201cbadly put together.\u201d Thus, task clarity not only ensures that the worker can provide accurate answers, but also serves to increase their effort levels and willingness to complete the task itself.\nOur survey sought to further clarify the importance of task clarity by asking whether it influences effort level and willingness to start a task. In addition, we asked a free response question about common areas of confusion to observe common pitfalls in designing MTurk tasks."
        },
        {
            "heading": "3.1 Context",
            "text": "One way to improve task clarity is to provide additional context for the task. Context is commonly provided in two ways. The first way is to provide additional words, sentences, or paragraphs in the annotated text itself. Providing context can increase disagreement among responses, and can be useful if the goal is to mimic the distribution of differing human judgments (Pavlick and Kwiatkowski, 2019). The second way is to introduce background information about the speaker, such as age, gender, race, socioeconomic status, etc. In tasks such as annotating for the presence of harmful language, such information can affect an annotator\u2019s perception of text as influenced by individual biases.\nSometimes, context is inherent in the language itself, instead of explicitly provided. For example, some annotators may incorrectly interpret African American English (AAE) as offensive (Sap et al., 2019). Other forms of context include grammaticality, location, and relative positions of power between the speaker and the audience.\nThus, it is important to note that even without the presence of additional context, text is inherently contextual. It is important to identify and control\nfor such qualities in annotation tasks. To better understand how context can influence annotation tasks, we asked MTurk workers what additional information is most useful to them when given a question with an unclear answer."
        },
        {
            "heading": "3.2 Survey Results on Task Clarity",
            "text": "We found that task clarity and fair payment are key factors that determine both MTurk workers\u2019 willingness to start a task and the amount of effort they put into it. In our survey, 26.3% of respondents stated that difficulty of understanding the task influences whether they start the task, and 31.6% indicated that it influences the amount of effort they put into the task. One respondent suggested providing additional, more comprehensive examples, since requesters \u201coften skimp on these or only provide super basic, obvious examples.\u201d When respondents were asked to rank the importance of different qualities in determining how \u201creasonable\u201d and \u201creliable\u201d the requester is (from one to six), clarity of instructions was ranked second highest with an average rank of 2.71 (payment was ranked highest; see Section 4). These results reinforce the importance of task clarity in improving workers\u2019 perceptions of the task and its requester.\nWorkers often reported being confused by annotation tasks that involve sentiment analysis and subjective ratings. For such questions, the most commonly preferred additional information was the purpose of the question (69.1%), context (50.0%), and the platform the annotations would be used for (34.9%). This suggests that providing an explanation for the downstream use case of the task significantly aids workers in understanding tasks and providing quality annotations."
        },
        {
            "heading": "4 Payment",
            "text": "MTurk\u2019s platform, which enables researchers to collect large amount of data at low cost, does not currently regulate compensation beyond a $0.01 minimum per task that disregards task completion time. The mean and median hourly wages for MTurk workers are $3.13 and $1.77 per hour, respectively. 95% of MTurk workers earn below minimum wage requirements in their geographic location (Hara et al., 2018), and improvements in household income for U.S. MTurk workers lag behind the general U.S. population (Jacques and Kristensson, 2019). Low payment is not only unfair to workers, but can also serve as an additional sign to workers of an unreliable requester to avoid due to high potential of having a HIT rejected. u/Bermin299 (2022) claims that \u201cRequestors that pays cheaply are some of the most trigger happy people when it comes to handing out rejections.\u201d\nMost researchers agree on using minimum wage as a baseline hourly rate for MTurk work, though exact recommendations vary due to differences in the minimum wage worldwide (Hara et al., 2018). A common sentiment among MTurk workers is to pay at a rate of $15 per hour (u/Sharpsilverz, 2022). Whiting et al. (2019) found that workers are likely to overestimate their work time by 38% of the observed task time. Thus, the amount of time used to determine minimum wage can roughly account for this overestimation if using workers\u2019 self-reported work times as a metric. Previous research is divided on whether extra pay beyond minimum wage improves response quality. Callison-Burch and Dredze (2010) gave anecdotal evidence that unusually high payments, such as $1 for a very short task, may encourage cheating.\nHowever, Snow et al. (2008) found that nonguaranteed payments, which are paid only after submitted work is approved for work quality, are sometimes effective at improving response quality. MTurk\u2019s bonus payments can also be nonguaranteed payments, and they are given to workers after they have completed the task in addition to the advertised payment rate.\nGiven the wide range of payment options and formats, along with varying guidelines on the effectiveness of different payment options, we include multiple questions in our survey regarding MTurk workers\u2019 perception and response to payments. These included workers\u2019 perception of normal and bonus payments, how bonus versus normal\npayments affect effort, how payment influences likelihood of starting or completing the task, and how payment affects the degree to which requesters are perceived as \u201creasonable\u201d and \u201creliable.\u201d We also asked MTurk workers about their MTurk income relative to the opportunity cost of their work, as well as their own thresholds for the minimum pay rate at which they would do a task."
        },
        {
            "heading": "4.1 Survey Results on Payment",
            "text": "Survey results indicate reasonable payment to be around minimum wage; significantly higher or lower payment appears to be detrimental to the quality of responses. Respondents\u2019 minimum pay rate at which they would be willing to complete a task was $13.72 per hour on average (median of $12 per hour). 64.5% of respondents stated that the rationale behind their threshold is that they want to earn a wage on MTurk comparable to what they would make elsewhere. These numbers are significantly above the current mean and median, though still below minimum wage. Despite this, the survey surprisingly indicates that 70.4% of respondents make more on MTurk than they would make elsewhere, and 22.4% make the same amount. However, payment should not be set too high, as one worker stated that \u201c[e]xtremely high pay rates cause rushing in order to complete as many as possible prior to the batch getting wiped out by other workers.\u201d\nWorkers responded that both higher normal payments and bonus payments increase the amount of effort they put into a task. 73.0% of respondents indicated that bonus payments will increase their effort levels while 49.3% selected high(er) payments. However, some respondents noted that bonus payments are only a significant incentive if they are from a reputable requester. Uncertainty about whether the bonus will be paid is the main concern, with one worker noting that \u201cregular payments are guaranteed within 30 days, whereas bonuses can take however long the req decides to take to pay you (assuming they even do).\u201d Thus, the effectiveness of bonus payments varies with workers\u2019 perception of the requester\u2019s reliability and reputation."
        },
        {
            "heading": "5 Privacy",
            "text": "Demographic questions are commonly used in surveys to collect respondents\u2019 information. However, some MTurk workers may view these questions as\nthreats to their privacy and decide not to complete the survey. To clarify which questions are more commonly viewed as \u201csensitive,\u201d our survey asked MTurk workers to indicate types of questions, including demographic questions and questions about offensive topics or mental health, that cause them to not complete a task or answer untruthfully. We also asked for reasons would cause them to answer a question untruthfully, including whether they believe that the question is a privacy violation or that it will be used against them.\nIt is especially important to build trust and the perception of fairness between the requester and the annotator for surveys that ask more personal questions. Xia et al. (2017) found that crowdworkers\u2019 major areas of concern regarding data privacy involved collection of sensitive data, information processing, information dissemination, invasion into private lives, and deceptive practices. To minimize these concerns, they proposed providing information about the requester, being specific about why any private information is collected and how it will be used, not sharing any personal information with third parties, and avoiding using provided emails for spam content outside the context of the survey. Turkopticon\u2019s Guidelines for Academic Requesters recommend that requesters provide information that clearly identifies who the requester are and how to communicate with them, which indicates that the requester is \u201caccountable and responsible\u201d, as well as avoiding collection of workers\u2019 personally identifying information (Turkopticon, 2014). Building on this, our survey asked what additional information, such as content warnings and mental health resources, would be most helpful in surveys with sensitive questions.\nBest practices on demographic questions include ensuring inclusivity for questions on gender identity by allowing for open ended responses and considering that terminology changes over time (e.g. \u201cHow do you currently describe your gender identity?\u201d), distinguishing between ethnicity and race, and permitting annotators to indicate multiple responses for race, ethnicity, and gender in case they identify with multiple categories (Hughes et al., 2016). In our survey, we asked MTurk workers if they encountered questions where the responses provided did not adequately capture their identity.\nAnother MTurk worker vulnerability is the use of Worker IDs. Each MTurk worker has an identifying Worker ID attached to their account, associated\nwith all of their activities on Amazon\u2019s platform, including responses to MTurk tasks and activity on Amazon.com (Lease et al., 2013). As a result, any requester with access to a Worker ID (which is automatically included with survey responses) can identify the associated MTurk worker\u2019s personal information through a simple online search. This can return private information such as photographs, full names, and product reviews (Lease et al., 2013). As this is a vulnerability in MTurk\u2019s system that cannot be removed by the requester, it is common for requesters to simply drop the Worker IDs from the dataset before continuing with any data analysis. In our survey, we ask MTurk workers whether they are aware of the risk of their Worker IDs being attached to MTurk responses and if that knowledge is a concern that influences their behavior."
        },
        {
            "heading": "5.1 Survey Results on Privacy",
            "text": "Our survey results indicate that concerns about privacy often lead to untruthful answers. Overall, 79.6% of respondents stated they will answer a question untruthfully if they feel that it is a privacy violation, and 17.1% stated they will do so if they have concerns that the questions will be used against them. In regards to Worker IDs, 51.3% of workers stated that they are aware that they exist, but it has not caused them to change their behavior.\nIn addition, workers often feel that demographic questions do not capture their identity. This is most often an issue for questions about gender (38.2%), age (38.2%), and sexual orientation (37.5%). Workers also frequently answer demographic questions untruthfully (Figure 2), especially those regarding age (52.0%), gender\n(41.5%), and education (32.2%). 9.87% of respondents indicated that other questions caused them to answer untruthfully, and most frequently mentioned questions requesting a phone number, name, or zipcode. Other respondents were concerned that their education level could be held against them."
        },
        {
            "heading": "6 Response Quality",
            "text": "Response quality can be maximized by ensuring that the task is clear and well-designed through a preliminary pilot task (annotation tasks released before the actual task to fine-tune the survey) and filtering respondents to include those who are more likely to honestly complete the task (Gideon, 2012). Requesters can also work towards maximizing response quality by using various functionalities provided by MTurk\u2019s filtering software."
        },
        {
            "heading": "6.1 MTurk Qualifications",
            "text": "MTurk provides several mechanisms for improving response quality. These include Masters Qualifications (which filter for \u201cMasters Workers\u201d, selected on the basis of successful completion of a wide range of MTurk tasks over time), System Qualifications (e.g. HIT approval rate, location), and Premium Qualifications (e.g., income, job function). MTurk also has \u201cQualifications Types you have created,\u201d which allow requesters to assign specific workers with a custom score between 0-100 to determine their eligibility for tasks. One challenge to filtering MTurk workers is balancing the use of filters to minimize spam responses while still allowing enough real workers to respond in order to collect a large enough dataset. Filtering on location is common because it helps to filter out MTurk workers who may be unqualified to complete tasks that require proficiency in certain languages or experience with a certain culture or environment (Karpinska et al., 2021).\nTo prevent overfiltering respondents at the beginning, some papers suggest keeping numerical thresholds relatively low. Peer et al. (2014) stated that high-reputation and high-productivity workers can be filtered using 500 approved HITs and 95% HIT approval rate, while a looser lower bound to avoid low-reputation and low-productivity workers is 90% approval rate and 100 approved HITs. However, more recent research has found that around 2500 approved HITs and 99% HIT approval rate is a more effective filter for MTurk surveys, since approval rates below that result in a significant drop\nin response quality (Kummerfeld, 2021). Meanwhile, researchers such as Ashida and Komachi (2022) used a 98% HIT approval rate to allow more workers to attempt HITs. This aligns with views expressed by MTurk workers that a 99% approval rate is unreasonably difficult to obtain, as it requires a near perfect record when HITs are often rejected without a logical reason or explanation (u/Lushangdewww, 2019). Furthermore, a 99% HIT approval rate largely serves to unfairly discriminate against newer workers (rather than low quality workers), due to the relative margin of error based on the total number of HITs completed (u/ptethesen, 2022). Turkopticon encourages providing clear conditions for rejection due to the permanent mark that rejection leaves on a worker\u2019s record (Turkopticon, 2014).\nMany MTurk requesters also use time as a means of filtering responses both as workers are completing tasks (e.g. through timers) and after receiving the complete dataset, by removing the top and bottom percentile of response times (Justo et al., 2017). In response to one such task warning that \u201cspeeders\u201d would be rejected, however, an r/mturk user stated that they \u201cdid that survey, made some lunch, checked my text messages then submitted\u201d (u/gturker, 2022). Such forum posts suggest that these filters encourage workers to work on other tasks to pad time rather than spend more time thinking about their answers.\nWhile several papers outline strategies to ensure high response quality, there is little work on how such measures are received by MTurk workers themselves. Thus, our survey asked MTurk workers about how strategies such as limiting response time ranges and adding time limits affect their response quality. In addition, we asked if any of these measures are perceived as unreasonable. Lastly, we asked MTurk workers to provide their own input on reasonable checks for response quality and whether any existing qualifications are perceived as unfair or unreasonable."
        },
        {
            "heading": "6.2 Recent LLM Developments and Concerns",
            "text": "The rise of large language models (LLMs) poses a significant risk to response integrity. Veselovsky et al. (2023) found that 33 to 46% of workers used LLMs to complete a text summarization task on MTurk. Furthermore, these responses may be difficult to detect, as LLMs such as ChatGPT outperform humans on some annotation tasks (Gilardi\net al., 2023). In our survey, we found that required, open-ended text responses made it easiest to spot spam respondents who might pass unnoticed in rating or classification tasks, but such manual methods may become less effective in the face of better text generation tools. However, multi-stage filtering procedures, such as Zhang et al. (2022)\u2019s pipeline that uses a qualification task and a longer endurance task to identify a high-quality worker list before the main task begins, can help to identify a trustworthy pool of workers to minimize spam responses. AI-generated text detectors may also help to flag potential spam responses in these filtering pipelines (Veselovsky et al., 2023)."
        },
        {
            "heading": "6.3 Survey Results on Response Quality",
            "text": "One common metric to filter for response quality is response time. Overall, respondents indicated that task timers and time estimates are well-calibrated. However, we also found that time-based filters are largely unhelpful and counterproductive. When given minimum required response times, 41.5% of workers reported that they spend the additional time working on other things to extend their completion time. 22.4% of survey respondents reported having had a HIT unfairly rejected due to responding too fast. Meanwhile, time limits can reduce response quantity and quality. One respondent explained that \u201cit is frustrating when you have put in a great deal of conscientious effort only to find the time has expired and you cannot submit a HIT.\u201d These results align closely with the comments made in the Reddit thread described in the previous section (u/LaughingAllTheWay83, 2022a).\nThe filter most commonly seen as unreasonable is the Masters qualification (55.9%), because MTurk has reportedly stopped giving it out. One respondent explained that \u201cMasters is a dead qual. Hasnt been issued to anyone since 2019. It is the single biggest gatekeeper to success for newer turkers like myself.\u201d Thus, MTurk requesters who require a Masters qualification unknowingly filter out all workers that joined after this time.\nOn average, respondents indicated that a reasonable HIT approval rate is 93.6%, while the median response was 98%. Overall, MTurk workers reported a strong incentive to maintain a high HIT approval rate, as 40.1% of survey respondents stated that the approval rate of the requester influences the amount of effort they put into their responses and 36.2% state that it influences whether they even\nstart a task. Thus, a good practice for rejecting HITs is is to provide a clear rationale for why a HIT is being rejected as feedback for future work."
        },
        {
            "heading": "7 Sensitive Content",
            "text": "MTurk surveys involving more sensitive NLP tasks can pose psychological risks. MTurk workers are more likely to have mental health issues than the general population, making them a vulnerable population (Arditte et al., 2016). NLP tasks that pose a potential risk to workers include labeling hate speech applicable to the worker\u2019s own background, identifying adult or graphic content, or questions about workers\u2019 mental health. There is usually higher risk with tasks that involve language that provides strong emotional stimuli (e.g. offensive tweets), which can potentially cause trauma (Shmueli et al., 2021). In addition, work may be more personally damaging when the offensive content directly applies to the worker (Sap et al., 2020). It may be beneficial to modify studies being completed by targeted demographic groups with additional safeguards and warnings in the survey regarding potential harm or offense.\nIn addition to exposing workers to potential psychological harm, the presence and framing of sensitive questions can cause workers to leave tasks incomplete or answer untruthfully. u/LaughingAllTheWay83 (2022b) described providing \u201caccurate and thoughtful answers 99% of the time, the exception being in regards to my own mental health. . . Requesters have been known to overstep their lane and contact the police for a welfare check and I\u2019m not opening myself up to that possibility for an extra $2, ya know?\u201d Thus, it is vital for researchers to identify potentially sensitive content and take steps to protect workers from possible harm.\nGenerally, researchers are aware of the presence of offensive content, as reflected in content warnings included in the beginning of published papers (e.g., Sap et al., 2020). However, there is a clear need for standardized best practices due to the highly vulnerable nature of such MTurk work.\nA relevant example is Facebook\u2019s partnership with Sama, a company that pays workers to identify illegal or banned content on social media (Perrigo, 2022). An interview with the workers revealed high levels of psychological distress due to insufficient breaks and subpar in-person resources. In an online setting such as MTurk, such personal-\nized care is even less accessible, making it more difficult to provide support for vulnerable workers. These workers\u2019 experiences exemplify the difficulties faced by the large, invisible workforce behind MTurk and powerful NLP applications, also described by Gray and Suri (2019) in their discussion of the typically unregulated \u201cghost work\u201d that powers machine learning.\nCommon safeguards include listing clear descriptions of the benefits and risks of the task, providing mental health resources for workers, using data safety monitoring boards, adhering to institutional review board policies, and following confidential data code and informed consent procedures (Kim et al., 2021). We look to further add to this list of best practices by asking survey respondents whether there are questions that will immediately deter them from completing a task or to answer untruthfully and preferred actions or resources provided by requesters to minimize possible harm."
        },
        {
            "heading": "7.1 Survey Results on Sensitive Content",
            "text": "Sensitive survey questions, such as questions that ask workers to annotate harmful or offensive content (e.g. hate speech), not only expose workers to potential harm, but also often lead to inaccurate or incomplete responses. When we asked for specific types of questions that cause workers to decide to not complete a task, 23.7% of respondents indicated offensive content, and 19.1% indicated questions concerning mental health. As seen in Figure 2, 20.4% of workers reported answering questions on offensive questions untruthfully and 18.4% did so for questions regarding mental health.\nIn a survey that contains potentially sensitive questions, 43.4% of respondents most preferred general content warnings (whether the survey contains sensitive content), 30.9% preferred specific content warnings (the type of sensitive content the survey contains), and 23.7% preferred mental health resources (Figure 3). These numbers indicate that a far higher number of workers prefer specific content warnings than there are currently in MTurk studies."
        },
        {
            "heading": "8 Recommendations",
            "text": "We consolidate our findings on worker perspectives to arrive at several recommendations.\nPay at least minimum wage, but not exceptionally higher. Payment can influence MTurk workers\u2019 perception of the task as well as their effort\nlevels. We suggest paying around the minimum wage of $15 per hour (slightly above the median minimum threshold of $12, and in accordance with the views expressed by r/MTurk users), but not too much higher, which decreases response quality.2 Additional budget may be used as bonus payments, which are effective incentives when provided by reputable accounts.\nMinimize the collection of private information, and be transparent. Workers are highly sensitive to questions that they see as violations of privacy, such as demographic questions. It is thus prudent to minimize their use, provide a brief explanation for their importance, clarify that the answers are confidential and will not be held against the respondent, and include an option for respondents not to answer. To protect worker privacy, always drop Worker IDs before working with the dataset. Also follow worker-centered policies such as Turkopticon\u2019s Guidelines for Academic Requesters (Turkopticon, 2014) to maintain transparent requester policies and communication.\nAvoid time-based filters and the Masters qualification. Our results indicate that both lower and upper bounds on completion time are flawed methods of quality filtering. Thus, we discourage rejecting HITs based on time, barring humanly impossible extremes. In addition, to avoid unnecessarily filtering out workers who joined MTurk later, do not use the outdated Masters\u2019 Qualification. For high-quality responses, a 98% HIT approval rate filter adheres to both worker preferences and re-\n2We note that living wages vary by region and inflation, so the ideal pay is likely to change over time and may depend on the region from which MTurk workers are recruited.\nsearch findings, as it exactly equals the median value proposed in our survey results. Clearly outline the requirements for task approval to increase trust between the worker and the requester.\nConsider effects on worker approval rate when controlling for quality. Since MTurk spam rates are high (as we saw firsthand), quality filtering is key to obtaining good results. However, workers are sensitive to the fact that drops in approval rate bar them from future tasks, and so are both less willing to accept tasks from requesters with lower approval rates, and frustrated when work is rejected without explanation. As a result, it is best to provide explanations when rejecting workers. Paid qualification tasks that filter and/or train workers before they begin a larger main task, such as the pipeline proposed by Zhang et al. (2022), can help to minimize spam responses without needing to reject many responses on the main task. As text generation tools become more powerful, AI-generated text detectors may also help to flag potential spam responses (Veselovsky et al., 2023).\nProvide context for subjective tasks. When possible, preface questions with an explanation of the purpose of the annotation task, such as the downstream use case. Additional context is also helpful, such as whether the text is from a longer passage or was sourced from a specific platform.\nProvide general and specific content warnings. If questions in sensitive areas such as offensive content or mental health are necessary, provide general and specific warnings for sensitive content, an explanation of the questions\u2019 purpose, and an overview of how the data will be used.\nLimitations\nWe surveyed MTurk workers from the United States; the views of these workers may differ from those of workers in other parts of the world. In addition, because the survey was completed by the first 207 workers to see the posted task, there may be sampling bias in the workers who answered (i.e., ones who go on MTurk more often or are quicker to answer tasks are more likely to have filled out the survey). Future studies with a larger sample pool of workers from different parts of the world, and on different crowdworking platforms, could help to examine the extent to which these results generalize.\nEthical Considerations\nUnderstanding the factors that make MTurk workers more likely to trust a requester and thus more likely to provide personal information or accept tasks with unusual payment schemes (e.g., low normal pay and high bonus pay) could be misused by spam requesters or phishers to maliciously acquire more information from MTurk users or extract their labor without fair pay. We understand that these are potential consequences of our research, but hope that they are outweighted by the potential benefits of good-faith requesters understanding how to improve the effects of their tasks on workers."
        },
        {
            "heading": "Acknowledgments",
            "text": "Many thanks to Nicholas Tomlin and Eric Wallace for their feedback on earlier drafts of this paper."
        },
        {
            "heading": "B Survey and Data Cleaning Details",
            "text": "Figures 4 through 10 contain the full text of the survey.\nAfter data was collected, the mandatory free text responses were manually reviewed by two authors for spam, including free text responses that were identical for multiple survey responses, or direct copies of text from the question with nothing added; numbers in response to questions that do not ask for numbers; or responses that were incoherent. We also removed incomplete responses (where the survey was only partially answered)."
        }
    ],
    "title": "Incorporating Worker Perspectives into MTurk Annotation Practices for NLP",
    "year": 2023
}