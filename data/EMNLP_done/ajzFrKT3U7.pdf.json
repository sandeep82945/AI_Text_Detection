{
    "abstractText": "Neural models, including large language models (LLMs), achieve superior performance on multi-hop question-answering. To elicit reasoning capabilities from LLMs, recent works propose using the chain-of-thought (CoT) mechanism to generate both the reasoning chain and the answer, which enhances the model\u2019s capabilities in conducting multi-hop reasoning. However, several challenges still remain: such as struggling with inaccurate reasoning, hallucinations, and lack of interpretability. On the other hand, information extraction (IE) identifies entities, relations, and events grounded to the text. The extracted structured information can be easily interpreted by humans and machines (Grishman, 2019). In this work, we investigate constructing and leveraging extracted semantic structures (graphs) for multihop question answering, especially the reasoning process. Empirical results and human evaluations show that our framework: generates more faithful reasoning chains and substantially improves the QA performance on two benchmark datasets. Moreover, the extracted structures themselves naturally provide grounded explanations that are preferred by humans, as compared to the generated reasoning chains and saliency-based explanations.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Ruosen Li"
        },
        {
            "affiliations": [],
            "name": "Xinya Du"
        }
    ],
    "id": "SP:05dbcc423cb892c51e1039989c54f84a6d24931a",
    "references": [
        {
            "authors": [
                "Akari Asai",
                "Kazuma Hashimoto",
                "Hannaneh Hajishirzi",
                "Richard Socher",
                "Caiming Xiong"
            ],
            "title": "Learning to retrieve reasoning paths over wikipedia graph for question answering",
            "year": 2020
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hanwen Zha",
                "Zhiyu Chen",
                "Wenhan Xiong",
                "Hong Wang",
                "William Yang Wang."
            ],
            "title": "HybridQA: A dataset of multi-hop question answering over tabular and textual datayavuz-etal-2022modeling",
            "venue": "Findings of the Association for Com-",
            "year": 2020
        },
        {
            "authors": [
                "Miruna-Adriana Clinciu",
                "Arash Eshghi",
                "Helen Hastie"
            ],
            "title": "A study of automatic metrics",
            "year": 2021
        },
        {
            "authors": [
                "Nicola De Cao",
                "Wilker Aziz",
                "Ivan Titov."
            ],
            "title": "Question answering by reasoning across documents with graph convolutional networks",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2019
        },
        {
            "authors": [
                "Zhenyun Deng",
                "Yonghua Zhu",
                "Yang Chen",
                "Michael Witbrock",
                "Patricia Riddle"
            ],
            "title": "Interpretable amrbased question decomposition for multi-hop question answering",
            "year": 2022
        },
        {
            "authors": [
                "Nouha Dziri",
                "Ximing Lu",
                "Melanie Sclar",
                "Xiang Lorraine Li",
                "Liwei Jian",
                "Bill Yuchen Lin",
                "Peter West",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Jena D Hwang"
            ],
            "title": "Faith and fate: Limits of transformers on compositionality",
            "venue": "arXiv preprint arXiv:2305.18654",
            "year": 2023
        },
        {
            "authors": [
                "Anthony Fader",
                "Stephen Soderland",
                "Oren Etzioni."
            ],
            "title": "Identifying relations for open information extraction",
            "venue": "Proceedings of the 2011 conference on empirical methods in natural language processing, pages 1535\u20131545.",
            "year": 2011
        },
        {
            "authors": [
                "Angela Fan",
                "Yacine Jernite",
                "Ethan Perez",
                "David Grangier",
                "Jason Weston",
                "Michael Auli."
            ],
            "title": "Eli5: Long form question answering",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558\u20133567.",
            "year": 2019
        },
        {
            "authors": [
                "Yuwei Fang",
                "Siqi Sun",
                "Zhe Gan",
                "Rohit Pillai",
                "Shuohang Wang",
                "Jingjing Liu."
            ],
            "title": "Hierarchical graph network for multi-hop question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Ruiliu Fu",
                "Han Wang",
                "Xuejun Zhang",
                "Jun Zhou",
                "Yonghong Yan."
            ],
            "title": "Decomposing complex questions makes multi-hop QA easier and more interpretable",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 169\u2013180,",
            "year": 2021
        },
        {
            "authors": [
                "Olga Golovneva",
                "Moya Chen",
                "Spencer Poff",
                "Martin Corredor",
                "Luke Zettlemoyer",
                "Maryam Fazel-Zarandi",
                "Asli Celikyilmaz."
            ],
            "title": "Roscoe: A suite of metrics for scoring step-by-step reasoning",
            "venue": "arXiv preprint arXiv:2212.07919.",
            "year": 2022
        },
        {
            "authors": [
                "Ralph Grishman."
            ],
            "title": "Twenty-five years of information extraction",
            "venue": "Natural Language Engineering, 25(6):677\u2013692.",
            "year": 2019
        },
        {
            "authors": [
                "Xanh Ho",
                "Anh-Khoa Duong Nguyen",
                "Saku Sugawara",
                "Akiko Aizawa."
            ],
            "title": "Analyzing the effectiveness of the underlying reasoning tasks in multi-hop question answering",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Xanh Ho",
                "Anh-Khoa Duong Nguyen",
                "Saku Sugawara",
                "Akiko Aizawa."
            ],
            "title": "Constructing A multi-hop QA dataset for comprehensive evaluation of reasoning steps",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, COLING",
            "year": 2020
        },
        {
            "authors": [
                "Tushar Khot",
                "Harsh Trivedi",
                "Matthew Finlayson",
                "Yao Fu",
                "Kyle Richardson",
                "Peter Clark",
                "Ashish Sabharwal"
            ],
            "title": "Decomposed prompting: A modular approach for solving complex tasks",
            "year": 2023
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "year": 2023
        },
        {
            "authors": [
                "Tao Lei",
                "Regina Barzilay",
                "Tommi Jaakkola."
            ],
            "title": "Rationalizing neural predictions",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 107\u2013117.",
            "year": 2016
        },
        {
            "authors": [
                "Qi Li",
                "Heng Ji",
                "Liang Huang."
            ],
            "title": "Joint event extraction via structured prediction with global features",
            "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 73\u201382, Sofia, Bulgaria.",
            "year": 2013
        },
        {
            "authors": [
                "Qing Lyu",
                "Shreya Havaldar",
                "Adam Stein",
                "Li Zhang",
                "Delip Rao",
                "Eric Wong",
                "Marianna Apidianaki",
                "Chris Callison-Burch."
            ],
            "title": "Faithful chain-ofthought reasoning",
            "venue": "arXiv preprint arXiv:2301.13379.",
            "year": 2023
        },
        {
            "authors": [
                "Makoto Miwa",
                "Mohit Bansal."
            ],
            "title": "End-to-end relation extraction using lstms on sequences and tree structures",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1105\u20131116.",
            "year": 2016
        },
        {
            "authors": [
                "Sharan Narang",
                "Colin Raffel",
                "Katherine Lee",
                "Adam Roberts",
                "Noah Fiedel",
                "Karishma Malkan."
            ],
            "title": "Wt5?! training text-to-text models to explain their predictions",
            "venue": "arXiv preprint arXiv:2004.14546.",
            "year": 2020
        },
        {
            "authors": [
                "Ethan Perez",
                "Patrick Lewis",
                "Wen-tau Yih",
                "Kyunghyun Cho",
                "Douwe Kiela."
            ],
            "title": "Unsupervised question decomposition for question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Peng Qi",
                "Haejun Lee",
                "Tg Sido",
                "Christopher Manning."
            ],
            "title": "Answering open-domain questions of varying reasoning steps from text",
            "venue": "Proceedings of",
            "year": 2021
        },
        {
            "authors": [
                "Lin Qiu",
                "Yunxuan Xiao",
                "Yanru Qu",
                "Hao Zhou",
                "Lei Li",
                "Weinan Zhang",
                "Yong Yu."
            ],
            "title": "Dynamically fused graph network for multi-hop reasoning",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6140\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Haitian Sun",
                "William W. Cohen",
                "Ruslan Salakhutdinov"
            ],
            "title": "Iterative hierarchical attention for answering complex questions over long documents",
            "year": 2021
        },
        {
            "authors": [
                "Harsh Trivedi",
                "Niranjan Balasubramanian",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions",
            "venue": "arXiv preprint arXiv:2212.10509.",
            "year": 2022
        },
        {
            "authors": [
                "Ming Tu",
                "Guangtao Wang",
                "Jing Huang",
                "Yun Tang",
                "Xiaodong He",
                "Bowen Zhou."
            ],
            "title": "Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Com-",
            "year": 2019
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou"
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed H Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William W. Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D. Manning."
            ],
            "title": "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
            "venue": "Proceedings of the 2018 Conference on Em-",
            "year": 2018
        },
        {
            "authors": [
                "Deming Ye",
                "Yankai Lin",
                "Zhenghao Liu",
                "Zhiyuan Liu",
                "Maosong Sun"
            ],
            "title": "Multi-paragraph reasoning with knowledge-enhanced graph neural network",
            "year": 2019
        },
        {
            "authors": [
                "Ori Yoran",
                "Tomer Wolfson",
                "Ben Bogin",
                "Uri Katz",
                "Daniel Deutch",
                "Jonathan Berant"
            ],
            "title": "Answering questions by meta-reasoning over multiple chains of thought",
            "year": 2023
        },
        {
            "authors": [
                "Dmitry Zelenko",
                "Chinatsu Aone",
                "Anthony Richardella."
            ],
            "title": "Kernel methods for relation extraction",
            "venue": "Journal of machine learning research, 3(Feb):1083\u20131106.",
            "year": 2003
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Multi-hop question answering (QA) involves answering questions that require reasoning over multiple pieces of information, often spread across different parts of a document or multiple documents. It looks like \u201chopping\u201d over various facts to arrive at a conclusion or answer. In multi-hop question answering, the system needs to understand the context, maintain the sequence of information, and utilize this combined information to generate a correct and comprehensive answer. Traditionally, researchers (Qiu et al., 2019; Tu et al., 2019; Fang\n1Code is available at https://github.com/ bcdnlp/Structure-QA.\net al., 2020) have applied graph neural networks (GNN) to this task. In recent years, with the growing capabilities of large language models (LLMs), several works propose using prompting to address this task in a few- or zero-shot way (Wei et al., 2022; Ho et al., 2023).\nLLMs have achieved superior performance in reasoning-based question-answering tasks. These models have the potential to \u201creason\u201d and connect multiple pieces of information to generate the answers. However, they still struggle with complex multi-hop questions; and the end-to-end generation nature makes their answering process not explainable. To elicit the reasoning capabilities of LLMs, recent research introduced the chain-of-thought (CoT) mechanism and its variants (Wei et al., 2022; Wang et al., 2023). The CoT mechanism enables models to not only generates better answers but the reasoning chain (process) in natural language, improving the LLM\u2019s performance in conducting multi-hop reasoning.\nDespite the progress made, several challenges persist under this paradigm, One is the neural models\u2019 limitations in tackling compositional problems that require strict multi-hop reasoning to derive correct answers (Dziri et al., 2023). CoT-based methods still conduct generations in an end-to-end way, which is not a strict \u201csymbolic derivation\u201d and often causes inaccurate reasoning (e.g., generating answers that don\u2019t exist or two-hops away). Instead, we propose a multi-step approach (Figure 1) to tackle this problem: firstly, constructing the semantic graph structures (SG) with information extraction (IE) and then leveraging this symbolic information (including entities and semantic relations) for strictly guiding the model\u2019s reasoning process.\nThe second challenge is that, although the current model-generated reasoning chain provides explanations, they are only surface-level interpretations, and there is no guarantee that they are fully\ngrounded to context (Narang et al., 2020). For example, the model could make up a reasonablesounding explanation that is not faithfulness for its decision-making (hallucinations). While our extracted graphs from the IE step naturally provide explanations grounded to the given context (Figure 4), which are overwhelmingly preferred by human evaluators, compared to saliency- and NLGbased explanations. Plus, we also find that by leveraging our extracted SGs, the large language model also generates more faithful reasoning chains (Section 4.3),\nTo sum up, we make the following contributions in this work: (1) We propose creating and utilizing semantic graphs to enhance models\u2019 capability in multi-hop reasoning and achieve better performance on the task; (2) We demonstrate that leveraging semantic graphs in the prompting process help models generate higher-quality and more faithful reasoning chains; (3) We show that semantic graphs-based explanations are grounded to context and help improve the interpretability of the answer-predicting process, which human evaluators prefer."
        },
        {
            "heading": "2 Related Work",
            "text": "Multi-hop Question Answering Traditional approaches to solving the multi-hop QA problem can be mainly categorized as question decomposition (Perez et al. (2020); Fu et al. (2021); Sun et al. (2021); Deng et al. (2022)), graph-based method (Qiu et al. (2019); Fang et al. (2020)), iterative\nmethod (Asai et al. (2020); Qi et al. (2021)), and miscellaneous techniques (Chen et al. (2020); Ho et al. (2023)).\nIn these various directions, the graph-based method is a branch that solves the problem by building graphs and inferring on graph neural networks (GNN) to predict answers. Graphs can be categorized as entity-only graphs (Qiu et al. (2019); De Cao et al. (2019)) and multi-level node graphs (Tu et al. (2019); Ye et al. (2019); Fang et al. (2020)). Nodes in those graphs represent entities, sentences, paragraphs, and documents. Edges connect entities and include no semantic information. In our method, we build semantic graphs based on the definition in Section 3.2. Compared to previous work graph building, our constructed graphs include key semantic relations on edges, which provides important fine-grained information.\nIn addition, under our prompting-based QA setting, we sequentialize the graphs by concatenating the triples, which simplifies the process of encoding the semantic graphs. Specifically, our approach does not require the fine-tuning process since we apply the prompt-based method over LLMs, which simplifies the reasoning process.\nThis generative format is closely related and concurrent to the prompting-based method that is recently applied to the multi-hop QA task (Trivedi et al. (2022); Khot et al. (2023); Yoran et al. (2023)). We introduce the details and their variance in the paragraph below.\nPrompting and Reasoning The Chain-ofThought (CoT) prompting strategy (Wei et al., 2022), as a variation of LLM few-shot prompting, significantly improves the reasoning ability of LLMs. Compared to traditional few-shot prompting, it turns complex implicit reasoning chains into explicit long natural language text and adds it to the demonstrations, which helps LLMs conduct better reasoning. Kojima et al. (2023) introduce zero-shot-CoT by using a simpler prompt (\u201cLet\u2019s think step-by-step\u201d) to instruct LLMs. Wang et al. (2023) introduce the self-consistency strategy to improve the performance of the CoT strategy.\nWhen applying prompting strategy to the multihop QA task, Trivedi et al. (2022) introduces an iterative information retrieval method to provide more relevant context. Khot et al. (2023) decomposes prompts to simple solvable tasks. In our approach, we leverage semantic graphs extracted from documents, which guide reasoning processes and help accurately find the answer node. Those graphs are extracted from context, sequentialized into text form, and added back into context.\nGenerating faithful reasoning chains is difficult with the vanilla CoT strategy, as generated natural language contents may contain irrelevant information and cause hallucinations. Lyu et al. (2023) try to solve it by translating natural languages to symbolic languages and deducing results by deterministic external solvers. But, the process of translation itself remains opaque and cannot guarantee the prevention of hallucinations.\nInstead of directly generating symbolic presentations from contexts, we introduce semantic graphs to prevent hallucinations and improve interpretability. Since graphs are extracted from contexts, they are strictly grounded to context. Edges in the graphs can be seen as potential reasoning steps in the whole reasoning process."
        },
        {
            "heading": "3 Methodology",
            "text": "In this section, we first give a formal introduction to the multi-hop QA task and two different settings we tackle the problem. We then describe our technique for extracting semantic graphs, which will be used in our framework. Third, we introduce our overall and specific prompting for generating the reasoning chains and final answers."
        },
        {
            "heading": "3.1 Task and Settings",
            "text": "Given a question and candidate paragraphs, we aim to obtain an answer that involves multi-hop reasoning. We tackle the task with in-context learning from two different settings: (1) generate the answer as well as the explanations. \u2013 one representative method is chain-of-thought reasoning (Wei et al., 2022). We name it \u201cCoT setting\u201d here. \u2013 it is our default setting and of more importance since we focus on studying the faithfulness and interpretability of explanations; (2) directly generate the answer from the context and question (\u201cfewshot setting\u201d)."
        },
        {
            "heading": "3.2 Entity and Relation Extraction with Prompting",
            "text": "Drawing insights from the literature on information extraction, we propose two paradigms on entity and relation extractions: (1) treating them as separate tasks and conducting multi-step prompting for extracting entities first, and then relations in between them (Zelenko et al., 2003); (2) joint extraction (Li et al., 2013; Miwa and Bansal, 2016) of the entity and relations, thus building the structured semantic graph in an end-to-end way.\nFor entity (node), we define it as a short text span/sequence that clearly describes an object/event/truth in a sentence. \u2013 This formulation has broader coverage than traditional named entity recognition (NER) based on a fixed schema. For example, \u201c<Windermere, is popular for, its proximity to the lake>\u201d is an important triple in the semantic graph extracted from \u201cTourism is popular in Windermere mainly for its proximity to the lake.\u201d Traditional NER tools can only extract \u201cTourism\u201d, \u201cWindermere\u201d, and \u201clake\u201d, real-world objects, but not \u201cits proximity to the lake\u201d, a description of truth. We find in the prelim study that using traditional NER doesn\u2019t provide improvements, mainly because they are uninformative. For relations (edges), we borrow ideas from Open IE (Fader et al., 2011). Traditional relation extraction classifies relations into sets of categories, for example, OWNERSHIP, LOCATED, FAMILY)2. Thus, the number of closed relations is limited. They can hardly be used to present complex relations between entities that are defined above. OpenIE extracts any strings in the sentence that can describe relations between two entities. For our case, we extract the document-level relations described in\n2In total, 17 relations are used in ACE relation extraction evaluations; and Wikidata has 11,008 relations.\nnatural language in the text. Finally, to illustrate in the semantic graph, we represent the entities as nodes and semantic relations as edges (e.g., Figure 2). For example, the node \u201cPrincess Anne\u201d is connected to the node \u201cPrince Jean\u201d via the edge \u201cdaughter of\u201d.\nEntity Extraction The input we use as the prompt for the entity extraction step basically consists of one document, including a Wikipedia paragraph. It starts with the word \u201cDocument:\u201d. Then, it is followed by a paragraph with a title. Finally, we add to the prompt Entities:, asking it to start extracting a list of entities:\nDocument:\nWikipedia Title: <Page Title>\n<Paragraph Text>\nEntities:\nThe output to be generated by the LLM is supposed to be all relevant entities listed line-by-line. Overall, it looks like:\n<Entity 1>\n<Entity 2>\n...\n<Entity k>\nTo better instruct the model following the output format, we utilize few-shot in-context learning (ICL) to prompt the LLMs (Brown et al., 2020). Specifically, we add four document-entities pair as the demonstration.\nSemantic Relation Extraction To extract the semantic relations between entities, we propose two variations, one uses the multi-step extraction idea, and the second conduct joint extraction. Specifically, for multi-step extraction, the model extracts the entities first (as mentioned above), then append them to the prompt of relation extraction, and finally the prompt ends with Graph:,\nDocument:"
        },
        {
            "heading": "Wikipedia Title: <Page Title>",
            "text": "<Paragraph Text>\n<Entity 1>\\n ... \\n<Entity k>\nGraph:\nThe output to be generated would look like this:\n(<Entity a>, <R 1>, <Entity b>)\n(<Entity b>, <R 2>, <Entity c>)\n\u00b7 \u00b7 \u00b7\nWe also conduct a few-shot ICL. In this graph structure, not all pairs of entities have relations. We extract relations for pairs of entities if it exists.\nJoint Extraction of Entity and Relation Triples Different from the multi-step of extracting semantic graphs above, we also try direct prompting. Namely, we skip the entity generation process and directly generate the (subject, relation, object) triples with prompting. Basically, we directly add Graph: by the end of the paragraph context.\nDocument:"
        },
        {
            "heading": "Wikipedia Title: <Page Title>",
            "text": "<Paragraph Text>\nGraph:\nWe name the semantic graph generated through multi-step prompting \u201cSG-Multi\u201d; the semantic graph generated through the single step (joint extraction) \u201cSG-One\u201d.\nTo achieve a comprehensive understanding of how the graph structure affects the QA and reasoning process. We also build a fully connected version of the graph based on all the entities extracted in Step 1. More specifically, suppose there are k extracted entities, there would be k2 tuples, the graph would look like:\n(<Entity 1>, <Entity 2>)\n(<Entity 1>, <Entity 3>)\n\u00b7 \u00b7 \u00b7 (<Entity k-1>, <Entity k>)\nSince the fully connected graph does not leverage semantic information, we call it \u201cG-Full\u201d."
        },
        {
            "heading": "3.3 The Overall Prompt for the Question Answering Task",
            "text": "For each question, we have multiple associated passages as context. For each passage, a semantic graph is generated as described above. Then, all the graphs are combined together to form the overall prompt template. Thus, the overall prompt template includes three components: documents, an extracted graph, and a question prompt.\nThe input part consists of multiple evidence documents followed by their corresponding graphs. Then all of these structures are concatenated. Since our default setting is chain-of-thought, as suggested by Trivedi et al. (2022), we add \"Answer the following question by reasoning step-by-step\" ahead of the real question."
        },
        {
            "heading": "Documents:",
            "text": ""
        },
        {
            "heading": "Wikipedia Title: <Page Title 1>",
            "text": "<Paragraph Text 1>\n<graph 1> \u00b7 \u00b7 \u00b7"
        },
        {
            "heading": "Wikipedia Title: <Page Title n>",
            "text": "<Paragraph Text n>\n<graph n>"
        },
        {
            "heading": "Q: \u201cAnswer the question by",
            "text": "reasoning step-by-step\u201d <question>\nA:\nThen the output part would be like below:\n<Reasoning Steps>: <answer>\nFor the example question in Figure 1, the reasoning steps would be \u201cPrincess Anne was the daughter of Prince Jean. Prince Jean was the youngest child of Prince Robert. Prince Robert is the paternal grandfather of Princess Anne.\u201d One interesting difference from our reasoning chain (as specified in the demonstration) to the default CoT (Wei et al., 2022) tasks\u2019 is that, under the multi-hop setting, we can use a specific format for the multi-hop question reasoning process. More specifically, we define a reasoning chain as a list of short sentences in the correct logical order, and each sentence only represents a relation between two entities. The set of sentences naturally leads to the answer entity. Under this format, the reasoning chain can be more consistent, facilitating more fair and automatic evaluations.\nFinally, we post-process the generated <answer> by applying a regex expression to extract answers from the <answer> after it is generated \u2013 they might have a slightly different format or length to the real/gold answer. For the simple few-shot setting (non-CoT), we remove \u201cAnswer the question by ...\u201d and \u201cReasoning Steps\u201d from the few-shot examples/demonstrations."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets and Configuration",
            "text": "We evaluate our model on two multi-hop questionanswering datasets: HotpotQA (Yang et al., 2018) and 2WikiMultiHopQA (Ho et al., 2020). Both datasets have ten candidate paragraphs for each question and originally have supporting facts. We use the golden paragraphs as context for all questions. Following Trivedi et al. (2022), for each dataset, we randomly sample 100 questions from the original development set and use it as our development set for tuning hyperparameters. We randomly sample another 500 questions from the development set as our test set.\nWe use the model GPT-3.53 for the entity, relation, and graph extractions, as well as the final QA task. For entity and graph extractions, we add four demonstrations. For the question answering, we add two demonstrations. We set the maximum number of generation tokens for the entity and graph extraction process to be 300. There is no number limitation generating the reasoning chain and\n3https://platform.openai.com/docs/ models/gpt-3-5\nanswer span. But, the generation stops when it changes line. The \u201ctemperature\u201d parameter is set to 0 when generating all predictions.\nRegarding metrics, we use exact match (EM), F1, Precision, and Recall. We run the official evaluation scripts of each dataset to get the output scores."
        },
        {
            "heading": "4.2 Evaluation on the QA task",
            "text": "We first run experiments on both datasets under the CoT setting. We report results in Table 1.\nOn HotpotQA, we observe that all promptingbased methods achieve decent performance, and by adding graph structures, we see an improvement across all four metrics. Especially, the \"SG-Multi\" prompt improves the recall by 4%. When the model generates reasoning chains and answers, SG enables the LLM to \u201chop through\u201d the structured graph and context instead of inferring relations between entities purely from context. The reason that the recall score goes down (from 0.83 to 0.81) by using G-Full is that it sometimes provides spurious implicit related information (i.e., all entity pairs). Then the sequentialized graph would be too long and doesn\u2019t fit into the LLMs\u2019 input length constraint. Then it is harder for the model to capture an entire picture of the relations needed to answer the question.\nOn 2WikiMultiHopQA, we see a similar trend of improvements. The results of exact match, F1, and precision increase substantially after adding the graphs. They are improved by 9%, 6%, and 6% on average as compared to the base prompt. Recall\ngoes up in both the G-Full, SG-Multi, and SG-One settings. After manual analysis and comparisons to the Hotpot, we find that the graphs are smaller and of higher quality, which leads to a more faithful chain and final results. This will be discussed further in Section 4.3.\nFurther, we also present the recall performances for the few-shot setting (w/o CoT) in Table 2. We see recall improved by adding the structures, especially using SG-Multi for both HotPotQA and 2WikiHop. We will mention below why recall is a better metric for the multi-hop QA task when comparing prompting-based methods.\nRecall, precision and other metrics We argue that recall is a better metric for this task among the four metrics under the generative QA setting, where LLMs are used to generate the reasoning chains and answers.\nDifferent from extractive QA setting based on fine-tuned discriminative models: (1) generated answer\u2019s length varies, both wider (lower precision) or shorter (lower recall) answers can be correct; (2) LLMs sometimes generate tokens that are not shown/grounded to the original documents. \u2013 They can be in a longer-form (Fan et al., 2019) that includes tokens outside golden answers but don\u2019t exactly match (EM). From the human\u2019s judgment, most of those predicted answers may be correct. For example, the gold answer to the question \"Where was the father of Knut Hedemann born?\" is \"Stange\". But \u201cStange, Norway\u201d is also correct, which provides extra information. In this specific\nexample, the recall metric is more lenient and provides the same score. But the precision is punished. Without fine-tuning, text generation is less controllable than text span extraction. Thus, the precision metric is less suitable for the generated answers. Since F1 is affected by precision, it also punished slightly longer but correct answers.\nTo empirically verify, we conduct a brief manual study. Firstly, we manually check if generated answers are correct/acceptable under human judgment. If we believe an answer is correct, we assign 1 as its score; otherwise, 0. Then we calculate both Spearman (\u03c1) and Kendall-Tau (\u03c4 ) correlation to scores provided by each metric. As shown in Table 3, recall corresponds more to human evaluation.\nApart from recall, according to Table 1, we find that precision gets slightly increased (with hurting recall) by adding the SG-Multi (or SG-One), especially on the 2WikiMultiHopQA. As discussed previously, precision is highly influenced by the length of predicted answers. By adding graphs, the QA and reasoning process is more likely to refer to entity-relation triples in the graphs rather than starting from scratch from the long documents. According to our entity definition above and answer format, they are brief and informative. Thus, the final predicted answers are sometimes more accurate, which helps improve the precision and F1 score."
        },
        {
            "heading": "4.3 Evaluation on the Reasoning Chain",
            "text": "Apart from evaluating the quality of answers, it is also important to evaluate free-form reasoning chains under our CoT setting. We sample 100 questions in the 2WikiMultiHopQA dataset and manually annotate their reasoning chains (explanations) according to the format defined in Section 3.3. For the predicted reasoning chains, we conduct both human and automatic evaluations. We make the human correctness judgments based solely on factuality and hallucinations/faithfulness4. They are also the two most important considerations in Golovneva et al. (2022). Human judges decide whether one reasoning process is \u201ccorrect/good\u201d\n4Coherence and grammar mistakes rarely occur.\nbased on the two requirements. During our study, our two annotators fully agree.\nAlthough reliable, human evaluation is expensive. Motivated by this, recent work has also investigated automatic metrics for NL explanations evaluation, including word overlap or embedding based similarly with human written explanations (Clinciu et al., 2021). We consider two candidate automatic metrics: ROUGE measures n-gram overlap between generated and reference texts based on recall of important content. BERTScore (Zhang et al.) uses BERT embeddings to capture contextual information and computes a similarity score based on cosine similarity between generated and reference text embeddings. We first check the correlations between automatic metrics and human evaluations on whether the reasoning process is faithful or not. For each metric, they provide a score between 0 to 1 (with 1 as the perfect match). We calculate Spearman (\u03c1) and Kendall-Tau (\u03c4 ) correlations. We find that ROUGE is a slightly better metric for auto-eval (with \u03c1 being around 0.32 and \u03c4 around 0.27), as compared to BERTScore (with \u03c1 being around 0.24 and \u03c4 around 0.20). Overall, this shows a moderate agreement with human evaluations.\nOn our sampled questions, Table 4 shows the performance of generated reasoning chains by each method based on corresponding prompts. We show both human and automatic evaluation scores. We see that based on strong LLMs, all three promptingbased methods achieve good scores, but the prompt augmented with rich semantic structured (SGMulti) is of better quality. Below we look into examples and provide a more fine-grained evaluation and analysis.\nMore manual evaluation and analysis We sample and compare 100 pairs of reasoning chains generated by three prompting-based methods on the 2WikiMultiHopQA dataset. There are mainly three kinds of benefits/differences that our SGaugmented prompts bring,\n\u2022 Improvement of the faithfulness of reasoning chains. In Figure 3, there is a comparing example on the difference. Both reasoning chain starts with truthful facts (around two sentences). Later, the base prompt-based method generates Upper sing, which is either a city (\u201cGrafelfing\u201d) or a state name (\u201cUpper Bavaria\u201d), which causes the error of the final answer as well.\n\u2022 Help the model navigate through and conduct\nmore accurate reasoning (and know when to stop). For the example in Figure 4, in the question \u201cWho is the paternal grandfather of Princess Anne of Orl\u00e9ans?\u201d, the reasoning chain generated by the based prompt has an \u201cover-reasoning\u201d phenomenon (conducting more hops of reasoning). \u2013 It generates a name that is the great-greatgrandfather of Princess Anne of Orl\u00e9ans, \"Louis Philippe I\". While in the reasoning chain generated by inputting SG-Multi prompt, the reasoning process \u201cstop\u201d at the correct answer (i.e., \"Prince Robert, Duke of Chartres\").\n\u2022 When the quality of the extracted semantic graph is low, the reasoning chain\u2019s quality will be af-\nfected. Most of the time, the poor quality is caused by the incompleteness of the generated graph. Out of the 100 questions, about three examples exhibit such problems. Basically, the extracted SG is incomplete because of the output length limitation of the LLM. Thus the information is lost in the extracted graphs. This prompts the model to output \"The graph does not provide information about the question.\" instead of the reasoning chain.\nTo summarize the above, we find that generation of reasoning chains relies on SG\u2019s quality. With better-quality graphs, both final answers and reasoning chains can be more accurate."
        },
        {
            "heading": "5 Further Qualitative Analysis",
            "text": "Error Analysis We further investigate errors made by our SG-based framework. There are four major categories: (1) lack of external or commonsense knowledge (e.g. one\u2019s uncle is the brother of father), most of the wrong answers are caused by this. (2) poor quality graph (limitation of input length); (3) other problems such as metaphors/paraphrasing (e.g. relations between \u201crests\u201d and \u201cdeath\u201d) and mathematical knowledge. (4) unanswerable questions based on the given context, which is a problem of dataset creation. Around 5% of questions have this problem.\nMore about Explanabiltiy/Interpretability Our extracted semantic graph naturally provides an explanation of the reasoning process. Current work mainly relies on the generated reasoning chain, which is NLG-based and contains hallucinations (Golovneva et al., 2022). Our SG is fully grounded to the input context, as shown in the upper part of Figure 4. The red parts are the extracted nodes, and the green parts denote the relations/edges. Apart from comparing to NLGbased (reasoning chains in our setting) explanations (Narang et al., 2020). We also compare to the prevalent saliency-based method, which obtains explanations that are also grounded to the context\n(bottom of Figurer 4). It\u2019s mainly done by using attention scores to highlight the most important spans/words in the context (Lei et al., 2016). We conduct rigorous human preference studies. We invite a group of over four volunteers (with at least bachelor\u2019s degrees) to make pairwise comparisons of explanations in a blind way. They are only provided with ten questions, the corresponding contexts, and the highlighted words in the context (that models deem as most important for answering questions). Volunteers overwhelmingly vote for our SG-based extractive graphs/explanations as most grounded and informative. Beyonds, results also show a large preference for grounded explanations as compared to the reasoning chains, mainly because they have to verify the factuality/faithfulness of the generated explanations."
        },
        {
            "heading": "6 Conclusion",
            "text": "We investigate how extracted semantic graphs can contribute to explainable multi-hop question answering. We propose a prompting-based method that leverages the extracted and sequentialized graph into a prompt context. Through comprehensive experiments from multiple aspects, we find that leveraging the SGs substantially improves LLMs\u2019 reasoning capabilities in answering multiple-hop questions \u2013 with more faithful reasoning chains and better accuracy scores. Meanwhile, human studies show that the extracted graph, which itself is an interpretation that is strictly grounded to context, is preferred. As compared to the NLGbased explanations such as the generated reasoning chain and saliency-based explanations."
        },
        {
            "heading": "Limitations",
            "text": "\u2022 Since our framework adds an extra step of extracting the semantic graphs, the total inference time of the large language model is longer, and the cost is more.\n\u2022 Sometimes, the extracted semantic graph\u2019s quality is unsatisfactory. It\u2019s mainly because of the limitation of context window size, which causes the graph to be incomplete. This affects the generation of reasoning chains as well as answering the questions."
        }
    ],
    "title": "Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning",
    "year": 2023
}