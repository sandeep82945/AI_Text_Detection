{
    "abstractText": "Efficient knowledge retrieval plays a pivotal role in ensuring the success of end-to-end taskoriented dialogue systems by facilitating the selection of relevant information necessary to fulfill user requests. However, current approaches generally integrate knowledge retrieval and response generation, which poses scalability challenges when dealing with extensive knowledge bases. Taking inspiration from open-domain question answering, we propose a retrievergenerator architecture that harnesses a retriever to retrieve pertinent knowledge and a generator to generate system responses. Due to the lack of retriever training labels, we propose relying on feedback from the generator as pseudo-labels to train the retriever. To achieve this, we introduce a dual-feedback mechanism that generates both positive and negative feedback based on the output of the generator. Our method demonstrates superior performance in task-oriented dialogue tasks, as evidenced by experimental results on three benchmark datasets. Our code is available at https://github.com/Stycoo/ Dual-Feedback-TOD.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tianyuan Shi"
        },
        {
            "affiliations": [],
            "name": "Liangzhi Li"
        },
        {
            "affiliations": [],
            "name": "Zijian Lin"
        },
        {
            "affiliations": [],
            "name": "Tao Yang"
        },
        {
            "affiliations": [],
            "name": "Xiaojun Quan"
        },
        {
            "affiliations": [],
            "name": "Qifan Wang"
        }
    ],
    "id": "SP:972bc3bd78428839c89901d3f48c238637f19069",
    "references": [
        {
            "authors": [
                "Danqi Chen",
                "Adam Fisch",
                "Jason Weston",
                "Antoine Bordes."
            ],
            "title": "Reading Wikipedia to answer opendomain questions",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870\u20131879,",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Emily Dinan",
                "Stephen Roller",
                "Kurt Shuster",
                "Angela Fan",
                "Michael Auli",
                "Jason Weston."
            ],
            "title": "Wizard of wikipedia: Knowledge-powered conversational agents",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,",
            "year": 2019
        },
        {
            "authors": [
                "Mihail Eric",
                "Rahul Goel",
                "Shachi Paul",
                "Abhishek Sethi",
                "Sanchit Agarwal",
                "Shuyang Gao",
                "Adarsh Kumar",
                "Anuj Kumar Goyal",
                "Peter Ku",
                "Dilek Hakkani-T\u00fcr"
            ],
            "title": "Multiwoz 2.1: A consolidated multi-domain dialogue dataset with state corrections and state",
            "year": 2020
        },
        {
            "authors": [
                "Mihail Eric",
                "Lakshmi Krishnan",
                "Fran\u00e7ois Charette",
                "Christopher D. Manning."
            ],
            "title": "Key-value retrieval networks for task-oriented dialogue",
            "venue": "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, Saarbr\u00fccken, Germany, August 15-17,",
            "year": 2017
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Ming-Wei Chang."
            ],
            "title": "Retrieval augmented language model pre-training",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume",
            "year": 2020
        },
        {
            "authors": [
                "Zhenhao He",
                "Yuhong He",
                "Qingyao Wu",
                "Jian Chen."
            ],
            "title": "Fg2seq: Effectively encoding knowledge for end-to-end task-oriented dialog",
            "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May",
            "year": 2020
        },
        {
            "authors": [
                "Guanhuan Huang",
                "Xiaojun Quan",
                "Qifan Wang."
            ],
            "title": "Autoregressive entity generation for end-toend task-oriented dialog",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea,",
            "year": 2022
        },
        {
            "authors": [
                "Gautier Izacard",
                "Mathilde Caron",
                "Lucas Hosseini",
                "Sebastian Riedel",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Edouard Grave."
            ],
            "title": "Unsupervised dense information retrieval with contrastive learning",
            "venue": "Trans. Mach. Learn. Res., 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave."
            ],
            "title": "Distilling knowledge from reader to retriever for question answering",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.",
            "year": 2021
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave."
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick S.H. Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics, 7:452\u2013466.",
            "year": 2019
        },
        {
            "authors": [
                "Kenton Lee",
                "Ming-Wei Chang",
                "Kristina Toutanova."
            ],
            "title": "Latent retrieval for weakly supervised open domain question answering",
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August",
            "year": 2019
        },
        {
            "authors": [
                "Wenqiang Lei",
                "Xisen Jin",
                "Min-Yen Kan",
                "Zhaochun Ren",
                "Xiangnan He",
                "Dawei Yin."
            ],
            "title": "Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Patrick S.H. Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Douwe Kiela"
            ],
            "title": "Retrieval-augmented generation",
            "year": 2020
        },
        {
            "authors": [
                "Andrea Madotto",
                "Samuel Cahyawijaya",
                "Genta Indra Winata",
                "Yan Xu",
                "Zihan Liu",
                "Zhaojiang Lin",
                "Pascale Fung."
            ],
            "title": "Learning knowledge bases with parameters for task-oriented dialogue systems",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Andrea Madotto",
                "Chien-Sheng Wu",
                "Pascale Fung."
            ],
            "title": "Mem2seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL",
            "year": 2018
        },
        {
            "authors": [
                "Libo Qin",
                "Yijia Liu",
                "Wanxiang Che",
                "Haoyang Wen",
                "Yangming Li",
                "Ting Liu."
            ],
            "title": "Entity-consistent end-to-end task-oriented dialogue system with KB retriever",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Libo Qin",
                "Xiao Xu",
                "Wanxiang Che",
                "Yue Zhang",
                "Ting Liu."
            ],
            "title": "Dynamic fusion network for multidomain end-to-end task-oriented dialog",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6344\u20136354, On-",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Dinesh Raghu",
                "Atishya Jain",
                "Mausam",
                "Sachindra Joshi."
            ],
            "title": "Constraint based knowledge base distillation in end-to-end task oriented dialogs",
            "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6,",
            "year": 2021
        },
        {
            "authors": [
                "Ori Ram",
                "Gal Shachaf",
                "Omer Levy",
                "Jonathan Berant",
                "Amir Globerson."
            ],
            "title": "Learning to retrieve passages without supervision",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2022
        },
        {
            "authors": [
                "Md. Rashad Al Hasan Rony",
                "Ricardo Usbeck",
                "Jens Lehmann."
            ],
            "title": "Dialokg: Knowledge-structure aware task-oriented dialogue generation",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July",
            "year": 2022
        },
        {
            "authors": [
                "Devendra Singh Sachan",
                "Siva Reddy",
                "William L. Hamilton",
                "Chris Dyer",
                "Dani Yogatama."
            ],
            "title": "End-toend training of multi-document reader and retriever for open-domain question answering",
            "venue": "Advances in Neural Information Processing Systems 34: An-",
            "year": 2021
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "FEVER: a large-scale dataset for fact extraction and VERification",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of",
            "year": 2018
        },
        {
            "authors": [
                "Xin Tian",
                "Yingzhan Lin",
                "Mengfei Song",
                "Siqi Bao",
                "Fan Wang",
                "Huang He",
                "Shuqi Sun",
                "Hua Wu."
            ],
            "title": "QTOD: A query-driven task-oriented dialogue system",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP",
            "year": 2022
        },
        {
            "authors": [
                "Fanqi Wan",
                "Weizhou Shen",
                "Ke Yang",
                "Xiaojun Quan",
                "Wei Bi."
            ],
            "title": "Multi-grained knowledge retrieval for end-to-end task-oriented dialog",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Tsung-Hsien Wen",
                "David Vandyke",
                "Nikola Mrksic",
                "Milica Gasic",
                "Lina Maria Rojas-Barahona",
                "Pei-Hao Su",
                "Stefan Ultes",
                "Steve J. Young."
            ],
            "title": "A networkbased end-to-end trainable task-oriented dialogue system",
            "venue": "Proceedings of the 15th Conference of the",
            "year": 2017
        },
        {
            "authors": [
                "Chien-Sheng Wu",
                "Richard Socher",
                "Caiming Xiong."
            ],
            "title": "Global-to-local memory pointer networks for task-oriented dialogue",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
            "year": 2019
        },
        {
            "authors": [
                "Jie Wu",
                "Ian G. Harris",
                "Hongzhi Zhao."
            ],
            "title": "Graphmemdialog: Optimizing end-to-end task-oriented dialog systems using graph memory networks",
            "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on In-",
            "year": 2022
        },
        {
            "authors": [
                "Xiong",
                "Lingpeng Kong",
                "Rui Zhang",
                "Noah A. Smith",
                "Luke Zettlemoyer",
                "Tao Yu."
            ],
            "title": "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models",
            "venue": "Proceedings of the 2022 Conference on Empirical Meth-",
            "year": 2022
        },
        {
            "authors": [
                "Yunyi Yang",
                "Yunhao Li",
                "Xiaojun Quan."
            ],
            "title": "Ubar: Towards fully end-to-end task-oriented dialog system with gpt-2",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 35(16):14230\u201314238.",
            "year": 2021
        },
        {
            "authors": [
                "Wen-tau Yih",
                "Kristina Toutanova",
                "John C. Platt",
                "Christopher Meek."
            ],
            "title": "Learning discriminative projections for text similarity measures",
            "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 247\u2013256, Port-",
            "year": 2011
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Task-oriented dialogue (TOD) systems (Eric et al., 2020) are designed to fulfill specific tasks, such as hotel bookings, through natural language conversations with users. These systems can be integrated into applications such as chatbots and voice assistants, serving various industries like hospitality, e-commerce, and customer service. To generate informative system responses, TOD systems typically rely on an external knowledge base (KB) to retrieve relevant entity information. While large language models (LLMs) like ChatGPT have demonstrated impressive capabilities in understanding multi-turn dialogues and generating fluent responses, there are still cases where they require access to localized KBs to handle specific tasks. Therefore, knowledge\n\u2217Corresponding authors.\nretrieval remains a critical component that necessitates long-term research in dialogue systems.\nTraditional pipeline approaches in TOD systems involve multiple modules such as dialogue state tracking and dialogue policy learning, which heavily rely on annotated belief states for knowledge retrieval (Lei et al., 2018; Yang et al., 2021). In contrast, end-to-end task-oriented dialogue (E2ETOD) systems aim to generate responses in a single step without the need for intermediate retrieval annotations, thereby highlighting the importance of external knowledge retrieval. Existing E2ETOD systems can be classified into three categories based on their utilization of external knowledge. Firstly, memory networks are employed to store the knowledge, and multi-hop interactions are designed to aggregate relevant information (Madotto et al., 2018; Qin et al., 2020; Raghu et al., 2021). Secondly, pre-trained language models encode linearized KB records, which are then used as input for the response generator (Xie et al., 2022; Wu et al., 2022; Tian et al., 2022). Thirdly, the knowledge base can be embedded into model parameters through data augmentation, enabling implicit\nknowledge retrieval (Madotto et al., 2020; Huang et al., 2022). These approaches typically integrate the processes of knowledge retrieval and response generation and train them under the supervision of reference responses. However, this approach suffers from two notable shortcomings. Firstly, the system response often comprises both pure language tokens and KB-related tokens (e.g., hotel names and addresses), making it challenging to train an effective retriever using weak supervision signals from reference responses. Secondly, the efficiency of these systems may decrease as the knowledge base expands in size.\nUnlike the aforementioned works, we employ a retriever-generator architecture that explicitly separates the retrieval process from response generation. The retriever is responsible for identifying relevant information from the KB, while the generator utilizes the dialogue context and retrieved information to generate the response. Although this architecture seems straightforward, constructing an effective retriever presents significant challenges, particularly in two key aspects. Firstly, TOD systems inherently possess ground-truth responses to train the generator, but lack annotations for training the retriever. Thus, the retriever can only be trained using weak supervision signals derived from the response generator. Secondly, within a specific domain, different entities often exhibit structural and content similarities, making it challenging for the generator to learn which entities are truly relevant. Consequently, the weak supervision signals from the generator may not be reliable. Figure 1 provides a visual illustration of these challenges.\nTo tackle these challenges, we propose a dualfeedback mechanism for the retriever, consisting of positive feedback and negative feedback. Positive feedback is constructed based on the conditional generation probabilities of responses corresponding to different retrieved entities. Intuitively, if the relevance between an entity and the response is higher, the conditional generation probability of the response corresponding to this entity will also be higher. Utilizing positive feedback allows us to train the retriever based on the knowledge learned by the generator from reference responses. In order to prevent the retriever from being misled by inaccurate information, we contend that calibration is necessary. Calibration involves the initial identification and resolution of errors, which we accomplish by sampling negative samples derived\nfrom the generator\u2019s hypothesis responses. Then, we construct negative feedback based on these negative samples to calibrate the positive feedback.\nWe conducted evaluations of our system on three well-established TOD datasets: Multi-WOZ 2.1 (MWOZ) (Eric et al., 2020), Stanford MultiDomain dataset (SMD) (Eric et al., 2017), and CamRest (Wen et al., 2017). The experimental results demonstrate that our model outperforms the baseline methods, particularly in scenarios involving large-scale KB. Additionally, through extensive analysis, we have made several notable findings. Firstly, our retriever exhibits clear advantages over the baseline methods as the size of KB increases. Secondly, our dual-feedback mechanism effectively mitigates the problem of incorrect knowledge learned solely with positive feedback from the generator. Lastly, our method exhibits good compatibility with LLMs like ChatGPT."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 End-to-End Task-Oriented Dialogue",
            "text": "E2E-TOD systems employ different strategies to incorporate KB information for response generation. Firstly, memory networks are utilized to store knowledge, using multi-hop interactions to aggregate relevant information. For instance, Mem2seq (Madotto et al., 2018) employs multi-hop attention over memory cells to select KB-related tokens during response generation. GLMP (Wu et al., 2019) introduces a global-to-local memory pointer network to retrieve relevant triplets and complete the response template. CD-NET (Raghu et al., 2021) retrieves relevant KB records by computing a distillation distribution based on dialogue context.\nSecondly, the entire linearized KB is encoded by pre-trained language models and taken as input to a generator to generate the final system response. For instance, UnifiedSKG (Xie et al., 2022) uses a unified text-to-text framework, while Q-TOD (Tian et al., 2022) rewrites the dialogue context into a natural language query for knowledge retrieval. MAKER (Wan et al., 2023) introduces a multi-grained retrival with both entity and attribute selection.\nThirdly, knowledge bases are stored in model parameters for implicit retrieval during response generation. GPT-KE (Madotto et al., 2020) embeds the KB into pre-trained model parameters through data augmentation. Following GPT-KE, ECO (Huang et al., 2022) first generates the most relevant entity\nwith trie constraint before response generation to ensure entity consistency in the response."
        },
        {
            "heading": "2.2 Knowledge Retrieval",
            "text": "Previous research has extensively explored methods for knowledge retrieval across various tasks, including question answering (Chen et al., 2017; Kwiatkowski et al., 2019), fact checking (Thorne et al., 2018), and dialogue systems (Dinan et al., 2019). Recently, neural network-based dense retrievers have become popular. These retrievers commonly use a dual-encoder architecture (Yih et al., 2011), where queries and passages are encoded as separate vectors, and relevance is determined through inner product or Euclidean distance.\nSupervised retrievers, such as DPR (Karpukhin et al., 2020), have been developed for open-domain question answering. These retrievers are trained using labeled question-document pairs. To overcome the need for costly query-document annotations, researchers have explored alternative approaches that leverage signals from the answer generator. REALM (Guu et al., 2020) and RAG (Lewis et al., 2020) propose joint training of the retriever and the generator by treating documents as latent variables. FiD-KD (Izacard and Grave, 2021a) employs crossattention scores as supervision through knowledge distillation. EMDR2 (Sachan et al., 2021) models retrieval decisions as latent variables and employs an expectation-maximization algorithm to approximate the computation. Unsupervised approaches have also been explored. ICT (Lee et al., 2019) introduces the inverse cloze task for unsupervised pre-training of dense retrievers. Izacard et al. (2022) investigate contrastive learning methods for training retrievers, while Spider (Ram et al., 2022) utilizes recurring spans within a document to create pseudo-positive query-document pairs."
        },
        {
            "heading": "3 Methods",
            "text": "As illustrated in Figure 2, our system comprises a knowledge retriever and a response generator. The retriever first retrieves top-K relevant entities from a knowledge base. These retrieved entities, along with the dialogue context, are then fed into the generator model to generate the response."
        },
        {
            "heading": "3.1 Notations",
            "text": "Given a dialogue D = {u1, y1, ..., uT , yT } consisting of T turns, where ut and yt represent the user utterance and system response at the t-th turn,\nrespectively, we denote the dialogue context of the t-th turn as ct = {u1, y1, ..., ut\u22121, yt\u22121, ut}. An external KB is provided, represented as a set of entities, i.e., E = {e1, e2, ..., eB}, where each entity ei consists of N attribute-value pairs, i.e., ei = {a1, v1i , ..., aN , vNi }. E2E-TOD takes the dialogue context ct and the KB as input and directly generates a natural language response yt."
        },
        {
            "heading": "3.2 Knowledge Retriever",
            "text": "Our knowledge retriever comprises an encoder Encr that maps any input to a d-dimensional vector. The user utterances and system responses in the dialogue context ct are first concatenated and encoded by Encr as the query. To encode an entity, we concatenate the attribute-value pairs of the entity into a sequence and pass it to Encr. The similarity score between ct and ei is computed by taking the dot product of their respective vectors:\nst,i = Encr(ct)TEncr(ei). (1)\nBased on these similarity scores, we identify the top-K entities from the entity set E as the candidate entities for response generation. This set of candidates is represented as E\u0302 = {e1, e2, ..., eK}.\nWe employ BERT (Devlin et al., 2019) as the encoder and extract the representations of the [CLS] token to represent ct and ei. Prior research has highlighted that initializing the encoder directly with BERT weights can lead to collapsed representations and impact retrieval performance. Consequently, we initialize the weights via pre-training with distant supervision (Qin et al., 2019).1"
        },
        {
            "heading": "3.3 Response Generator",
            "text": "Our generator is built upon the Fusion-in-Decoder (FiD) (Izacard and Grave, 2021b) model, which is based on the pre-trained T5 (Raffel et al., 2020). The FiD model consists of an encoder Encg and a decoder Decg. The encoder is responsible for processing K different text inputs independently, where each input xt,i is formed by concatenating the dialogue context ct and a candidate entity ei. The output representations of the encoder are then concatenated to create a global representation Xt = [Encg(xt,1), . . . ,Encg(xt,K)] for the current turn.\nThe decoder takes Xt as input and generates the response autoregressively. During this process, the decoder utilizes causal attention to incorporate information from previously generated tokens, as\n1Details of the pre-training are available in Appendix D.\nwell as cross-attention to the input tokens represented by Xt. This enables the decoder to consider information from both the generated tokens and the tokens of retrieved entities. The probability of the response is defined as follows:\np(yt|Xt) = |yt|\u220f i=1 p(yt,i|yt,<i, Xt), (2)\nwhere |yt| represents the length of the response yt."
        },
        {
            "heading": "3.4 End-to-End Training",
            "text": "Entity scoring Given the retrieved entities E\u0302 = {e1, e2, ..., eK}, we employ the aforementioned response generator to assign a score to each entity. Firstly, the dialogue context ct and each entity ei are concatenated to form xt,i, which is then fed into the generator. The conditional log probability (length-normalized) of the response yt is utilized to score entity ei as follows:\ngt,i = p(yt|Xt,i)\n=\n\u2211|yt| i=1 log p(yt,i|yt,<i, Xt,i)\n|yt| ,\n(3)\nwhere Xt,i = Encg(xt,i). The rationale behind the scoring is straightforward: if an entity is pertinent to the response, the probability of generating this response given the dialogue context and the entity should also be high. Positive feedback We utilize the entity scores Gt = {gt,i}1\u2264i\u2264K obtained from the generator as pseudo-labels to train the retriever, with the objective of transferring the knowledge acquired by the generator from reference responses to the retriever. To achieve this, we enforce consistency\nbetween the retrieval scores St = {st,i}1\u2264i\u2264K obtained from Eq. (1) and the pseudo-labels Gt using KL-divergence as follows:\nLpos = DKL(Gt, St)\n= K\u2211 i=1 g\u0303t,i(log g\u0303t,i \u2212 log s\u0303t,i), (4)\nwhere g\u0303t,i = softmax(Gt)i, s\u0303t,i = softmax(St)i. The use of the generator\u2019s supervision to train the retriever is referred to as positive feedback. However, it is important to note that the generator may assimilate incorrect knowledge from the entities, leading to inaccurate entity relevancy scores. Negative feedback To address the issue of positive feedback when training the retriever, we propose incorporating negative feedback from the generator to calibrate the pseudo-labels. To achieve this, we select a negative sample for the global representation Xt during the generation process. A negative sample refers to a response that exhibits a high generation probability but is of low quality. Specifically, we generate a set {y\u0302it}1\u2264i\u2264M of responses using beam search, where M is the beam search size, and rank them based on their generation probabilities. We use Rgt (y\u0302 i t) to represent the rank of candidate response y\u0302it. Note that a high generation probability does not guarantee a highquality response. To evaluate the quality, we define a function o(y\u0302it, yt) that measures the overlap between a response and the reference response. We implement o(y\u0302it, yt) using the BLEU metric. As a result, we obtain a new sorted list of responses based on their true quality, where the rank of candidate response y\u0302it is R o t (y\u0302 i t). Finally, we identify\nthe negative sample as follows:\ny\u2212t = argminy\u0302it(R g t (y\u0302 i t)\u2212Rot (y\u0302it)). (5)\nIntuitively, it is desirable for the retriever to avoid incorporating the signal from the negative sample when updating its parameters. Therefore, we employ Eq. (3) to calculate the score of each entity in generating the negative sample, and utilize the scores G\u2212t = {g \u2212 t,i}1\u2264i\u2264K for all retrieved entities to calibrate the positive feedback by introducing a margin loss:\nLneg = max ( 0,\u2212(DKL(G\u2212t , St)\u2212DKL(Gt, St)) + \u03b7 ) . (6)\nThe training objective of the retriever comprises Lpos and Lneg. Additionally, we train the response generator by minimizing the negative loglikelihood loss:\nLNLL = \u2212 |yt|\u2211 i=1 log p(yt,i|yt,\u2264i, Xt). (7)\nThe final training objective is a combination of the objectives of the retriever and the generator:\nL = LNLL + Lpos + Lneg. (8)"
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we provide a detailed description of the experiment and present the main results."
        },
        {
            "heading": "4.1 Datasets",
            "text": "We conduct our evaluation using three publicly available TOD datasets: Multi-WOZ 2.1 (MWOZ) (Eric et al., 2020), Stanford Multi-Domain dataset (SMD) (Eric et al., 2017), and CamRest (Wen et al., 2017). These datasets contain dialogues that are associated with relevant KBs. This is referred to as the session-level KB. To construct a comprehensive and extensive knowledge base, we merge the session-level KBs corresponding to each dialog turn, resulting in a dataset-level KB. The provided train, validation, and test splits for each dataset are utilized in our evaluation. The statistics of the three datasets are summarized in Appendix A."
        },
        {
            "heading": "4.2 Evaluation Metrics",
            "text": "We evaluate the performance of our model using two metrics. Firstly, we calculate the top-K retrieval recall (Re@K), which is inspired by the widely used top-K retrieval accuracy in question\nanswering (Karpukhin et al., 2020). Re@K measures the effectiveness of the retriever by determining the percentage of gold attribute values covered by the entities retrieved in the top-K list. Secondly, we assess the overall performance of our TOD system from two aspects. To evaluate the system\u2019s capability to generate relevant entities, we employ the micro Entity-F1 metric (Eric et al., 2017). Additionally, we utilize the BLEU metric to measure the N-gram overlap between the generated response and the reference response. With these metrics, we can comprehensively evaluate the performance of our model in both retrieval and TOD systems."
        },
        {
            "heading": "4.3 Experimental Settings",
            "text": "We instantiate the knowledge retriever using the BERT-base model. As for the generator, we instantiate it with T5 of varying model sizes: T5-base and T5-large. Both the retriever and generator models are fine-tuned using the ADAM algorithm (Kingma and Ba, 2015) with different learning rate schedulers. The retriever model employs a fixed learning rate scheduler, while the generator model uses a linear learning rate scheduler. The experiments are conducted on a single 24G NVIDIA RTX 3090 GPU. We select the checkpoint that yields the best results on the validation set. For more detailed information regarding our experimental setup, please refer to Appendix B."
        },
        {
            "heading": "4.4 Baselines",
            "text": "In our comparison, we classify the existing approaches for E2E-TOD systems into three categories based on their utilization of KB.\nMemory networks: These methods store external knowledge in memory cells in the form of triplets and utilize multi-hop attention to retrieve relevant information for generating responses. Examples include DSR (Wen et al., 2017), KBRetriever (Qin et al., 2019), GLMP (Wu et al., 2019), DF-Net (Qin et al., 2020), FG2Seq (He et al., 2020), and CDNET (Raghu et al., 2021).\nLinearized KB: These approaches leverage pretrained language models to encode the entire linearized KB, along with the dialogue context, as input for assisting response generation. Examples include DialoKG (Rony et al., 2022), UnifiedSKG (Xie et al., 2022), and Q-TOD (Tian et al., 2022).\nModel parameters: These approaches encode the KB into model parameters through data augmentation of the dialogues with KB records, enabling implicit retrieval during response genera-\ntion. Examples include GPT-2+KE (Madotto et al., 2020) and ECO (Huang et al., 2022)."
        },
        {
            "heading": "4.5 Main Results",
            "text": "We conducted experiments in both session-level and dataset-level KB scenarios. In the following paragraphs, we discuss the detailed results.\nSession-level KB The results for the sessionlevel KB setting are summarized in Table 1. Our system, instantiated with T5-large, achieves stateof-the-art performance on the MWOZ and SMD datasets. Specifically, our method demonstrates an improvement of 2.56 in the Entity-F1 metric for MWOZ and 0.47 for SMD over Q-TOD. Moreover, our system achieves the highest BLEU scores, with an increase of 0.86 on MWOZ, 3.77 on SMD, and 2.25 on CamRest compared to Q-TOD. However, our system does not achieve the best performance in terms of Entity-F1 on the CamRest dataset. This can be attributed to the presence of session-level KBs containing only 1-2 entities, which poses a challenge for the retriever to perform optimally.\nNote that Q-TOD also employs a Transformerbased response generator similar to ours and utilizes manually annotated queries. However, our\nmethod outperforms Q-TOD on all three datasets using T5-large. We believe this is because Q-TOD trains the retriever independently, whereas our proposed dual-feedback method allows for joint training of the retriever and generator. This facilitates better alignment between the retrieved entities and their relevance to the current dialogue context.\nDataset-level KB Table 2 presents the results of our system on the dataset-level KB and compares it with the reimplementation of several well-known E2E-TOD systems. Our system demonstrates significant advantages, particularly in the Entity-F1 metric. It achieves an improvement of 5.83 on the MWOZ dataset and 9.7 on the CamRest dataset over the strong Q-TOD baseline.\nFurthermore, when comparing the experimental results in the two KB settings, we observe that our model exhibits more stable performance. As the KB size increases, our model experiences only a minor decrease in performance, while other baseline models show a noticeable decline. For instance, DF-Net exhibits a decrease of 7.79 in terms of Entity-F1 on the MWOZ dataset, indicating that the method struggles to adapt to large-scale KBs. Similarly, Q-TOD experiences a reduction of 3.48 in Entity-F1 on MWOZ, highlighting the less stable performance of its independently trained retriever."
        },
        {
            "heading": "5 Analysis and Discussion",
            "text": "To further investigate our method, we conduct a comprehensive analysis that includes an ablation study, an exploration of different retriever training methods, an examination of different negative sample selecting methods, and an assessment of\ncompatibility with LLMs. T5-base is used as the base model for the first three analyses."
        },
        {
            "heading": "5.1 Ablation Study",
            "text": "We conduct an ablation study on the MWOZ dataset to evaluate our method under both sessionlevel KB and dataset-level KB settings. In the session-level KB setting, we adjust the retriever evaluation metric from Re@7 to Re@3 due to the smaller size of the KB. The results of this study are presented in the upper section of Table 3.\nBy removing negative feedback, we observe a decrease in retrieval metrics (Re@7/Re@3) and TOD metrics (BLEU/Entity-F1). Notably, the retrieval metrics exhibit a more pronounced decline. This suggests that the incorporation of negative feedback allows the retriever to more effectively learn from the generator, resulting in more relevant entities for response generation.\nFurthermore, the further removal of positive feedback reduces the model to a learnable generator with a fixed retriever. In the dataset-level KB setting, all evaluated metrics exhibit a further decline. This indicates that our positive feedback successfully facilitates the transfer of knowledge learned by the generator to the retriever.\nHowever, in the session-level KB setting, while the Entity-F1 metric decreases, the Re@3 metric increases. We attribute this to the small size of the KB, causing significant fluctuations in the top-3 entities as the retriever updates. Consequently, the generator tends to prioritize the entities that occur most frequently among the retrieved results. While this improves the model\u2019s performance in dialogue scenarios that only require a single entity, which constitutes the majority, it hampers performance in scenarios that require multiple entities."
        },
        {
            "heading": "5.2 Methods for Retriever Training",
            "text": "We conduct a comparative experiment with other retriever training methods commonly used in question answering (QA), including attention-based (Izacard and Grave, 2021a) and maximum marginal likelihood (MML)-based (Sachan et al., 2021) methods. To further demonstrate the necessity of negative feedback, we perform additional experiments by incorporating negative feedback into the attention-based method. Note that the underlying model architecture remains the same across all methods, with the difference lying solely in the employed training methods. The results are presented in the lower section of Table 3. Due to the similar performance trends observed for both dataset-level and session-level KB settings, the following analysis will focus solely on the dataset-level KB setting.\nWe observe that our proposed method outperforms the attention-based method by 2.26 and the MML method by 3.82 in terms of the Re@7 metric. This indicates that in the TOD task where KB-related tokens are sparse in the response, our utilization of conditional probability of response enables better correlations between the retrieved entities and the response. Furthermore, when comparing the performance of the attention-based method before and after incorporating negative feedback, we consistently observe improvements across all metrics. This suggests that even in existing methods, negative feedback can effectively enhance the performance of the retriever and the overall model."
        },
        {
            "heading": "5.3 Methods for Selecting Negative Sample",
            "text": "To understand how different negative sample selecting methods affect the results, we conduct additional experiments. As depicted in Figure 3, these methods can be categorized into two types: argmin\u2217\u2217\u2217 and rank\u2217\u2217\u2217. The argmin\u2217\u2217\u2217 method re-\nlies solely on the scoring of the oracle function to select the response with the lowest score to construct negative feedback. On the other hand, the rank\u2217\u2217\u2217 method combines the scoring of the oracle function with the generation probabilities to select responses with high probabilities but low factual quality to construct negative feedback. Additionally, we also experiment with using BLEU and Entity-F1 as the oracle functions to assess their influence on the experiments. From Figure 3, we have made three notable observations.\nFirstly, the results in Entity-F1 and Re@7 demonstrate consistent trends, indicating that improvements in retrieval performance lead to enhanced performance in TOD. This highlights the significance of enhancing the retrieval performance. Secondly, the rank\u2217\u2217\u2217 methods outperform the argmin\u2217\u2217\u2217 methods. Therefore, we conclude that the rank\u2217\u2217\u2217 methods are more accurate in selecting appropriate negative samples that could reflect the generator\u2019s mistakes compared to methods that solely rely on the oracle function.\nLastly, it is observed that using Entity-F1 as the oracle function often results in higher Entity-F1 and Re@7 scores, but slightly lower BLEU scores. This suggests that Entity-F1 can provide more precise feedback regarding entity selection compared to BLEU. However, in practical scenarios, obtaining gold entity annotations from responses is challenging, despite their availability in the MWOZ dataset. This is why we employ BLEU as the oracle function in our experiments."
        },
        {
            "heading": "5.4 Compatibility with LLMs",
            "text": "To demonstrate the compatibility of our proposed dual-feedback mechanism with LLMs, we conduct zero-shot and few-shot experiments using ChatGPT\nas the generator. We train the retriever using positive and negative feedback provided by ChatGPT. The experimental results are presented in Table 4. Notably, since the ChatGPT API 2 does not directly provide the generation probabilities of responses, we make slight adaptations. The details of this experiment can be found in Appendix F.\nFrom Table 4, we observe that the best performance is achieved when incorporating negative feedback in both the zero-shot and few-shot scenarios. This highlights the effectiveness of the dualfeedback mechanism in training a superior retriever, even when utilizing LLMs. Moreover, the performance in the few-shot scenario surpasses that in the zero-shot scenario, emphasizing the importance of having a few demonstrating examples. These examples provide valuable guidance and enable the model to better understand the task."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose a novel dual-feedback knowledge retriever for E2E-TOD systems. Our approach separates the knowledge retrieval process from response generation, and leverages the knowledge learned by the generator to create synthetic positive and negative feedback for retriever training, eliminating the need for retrieval annotations. Through empirical evaluations, we demonstrate that our system achieves state-of-the-art performance, regardless of whether a small or largescale KB is used in each dialogue. Furthermore, ablation studies indicate that our dual-feedback mechanism effectively mitigates the problem of incorrect knowledge learned solely from positive feedback generated by the generator. Consequently, this improvement in retrieval performance directly translates to enhanced performance in E2E-TOD systems. Lastly, our method exhibits good compatibility with LLMs like ChatGPT.\n2https://openai.com/api/\nLimitations\nThere are three potential limitations to consider in our work. Firstly, the process of obtaining negative feedback through response sampling can result in decreased training efficiency. Secondly, training the retriever using feedback from ChatGPT can be costly. In each epoch, it is necessary to re-predict for every sample, and finding ways to reuse intermediate predictions is an area to explore. Thirdly, there still exists a noticeable gap between fine-tuning the generator and achieving few-shot learning with ChatGPT. Future research is needed to investigate methods for narrowing this gap.\nEthics Statement\nAll experiments in this study were conducted using publicly available datasets that do not contain any private information. Our work does not involve the analysis or utilization of identity characteristics, and we do not engage in any form of gender or racial discrimination."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Natural Science Foundation of China (No. 62176270), the Guangdong Basic and Applied Basic Research Foundation (No. 2023A1515012832)."
        },
        {
            "heading": "A Dataset Statistics",
            "text": "In the case of MWOZ, each dialogue session includes 7 candidate entities, while for SMD and CamRest, the sizes of the candidate KBs vary, ranging from 0 to 8 and 0 to 57, respectively. Table 5 shows the statistics of the three datasets: MultiWOZ 2.1 (MWOZ) (Eric et al., 2020), Stanford Multi-Domain dataset (SMD) (Eric et al., 2017), and CamRest (Wen et al., 2017). When we include the entire knowledge base (KB) as input, the length of the input text becomes significantly long, posing challenges for existing end-to-end task-oriented dialogue systems to handle."
        },
        {
            "heading": "B Hyperparameters",
            "text": "The hyperparameters of our system with sessionlevel and dataset-level KB are shown in Table 6 and Table 7, respectively."
        },
        {
            "heading": "C Traing Process",
            "text": "We provide a more comprehensive description of the training process below. During the initial training phases, we incorporate a warm-up period for the generator, facilitating dependable feedback for the retriever. Subsequent to this phase, both the retriever and generator are subjected to joint training until reaching convergence. It\u2019s important to\nemphasize that we carry out negative sampling at each training step, ensuring consistent information updates from the generator to the retriever.\nWhile the inclusion of sampling operations during training does indeed extend the overall training duration, it is important to note that these sampling operations are exclusively carried out within the training phase and, therefore, do not impact the inference time."
        },
        {
            "heading": "D Retriever Pre-training",
            "text": "Given a dialogue context and the system response, we utilize the entity with the highest frequency of its attribute values in the dialogue context and system response as the label. To optimize this process, we employ supervised contrastive learning (Gao et al., 2021). Specifically, the positive example for a dialogue context is the corresponding\nlabeled entity, while the negative examples are the labeled entities from other examples in the same mini-batch. We utilize the InfoNCE loss as the training objective, aiming to bring the sentence representations of positive samples closer together and push away those of negative samples. This pretraining procedure is performed on the MWOZ and CamRest datasets. Since the knowledge base in the SMD dataset is specific to each dialogue and lacks a global knowledge base, we do not conduct pretraining on the SMD dataset. The hyperparameters for the pre-training are detailed in Table 8."
        },
        {
            "heading": "E The Number of Retrieved Entities",
            "text": "In our experiments, the retriever retrieves the topK relevant entities from the knowledge base for response generation. We conduct an analysis to assess the performance of our system and FiD as we varied the number of retrieved documents K, as shown in Figure 4. Notably, we observe that FiD achieves the best performance when K = 7, while our model exhibits optimal performance when K = 10. This suggests that a small number of K might be inadequate to cover necessary knowledge entities for response generation. However, as K further increases, FiD\u2019s performance is significantly affected, leading to a noticeable decline in performance. In contrast, our system demonstrates only a slight decrease in performance. This finding suggests that a large value of K would inevitably introduce more noisy entities and increase the diffi-\nculty of knowledge utilization in response generation. Nevertheless, our model, empowered by the dual-feedback mechanism, still enables effective retriever training even in such circumstances."
        },
        {
            "heading": "F Train the Retriever Using ChatGPT",
            "text": "Regarding the zero-shot and few-shot experiments, our retriever underwent training on the complete MultiWOZ dataset, with a total of 2877 dialogues. Furthermore, in the few-shot context, we create prompts to simulate three main knowledge retrieval scenarios, aiming to enhance the model\u2019s comprehension of dialogue tasks. These scenarios involve knowledge base (KB) containing entities that align with user intent, KB containing similar entities for recommendations, and KB lacking similar entities and leading to query failures. Furthermore, we have also taken into account the potential inaccuracies in self-reported ChatGPT scores, which could negatively impact the feedback. Our experimental results indicate that the proposed composite scoring strategy, combining self-reported scores and BLEU metrics, effectively alleviates this issue.\nF.1 Hyperparameters\nThe hyperparameters for our retriever when using ChatGPT as the generator are presented in Table 9.\nF.2 Accuracy of ChatGPT Scores\nAs indicated in the table below, relying solely on self-reported (sr) scores for constructing feedback leads to performance degradation compared to the approach proposed in the paper. This, to some extent, validates the presence of inaccuracies in\nself-reported scores. However, quantifying the precision is challenging due to the absence of ground truth labels for ChatGPT\u2019s outputs. Furthermore, our approach combines self-reported scores with BLEU-based ranking for selecting negative samples, thereby mitigating the unfavorable effects that may arise from these inaccuracies.\nF.3 Prompt for ChatGPT Since the ChatGPT API does not directly provide the generation probabilities of responses, which are required in our method for constructing feedback and entity scoring, we make slight adaptations. For negative sample selection, we construct prompts to elicit ChatGPT to generate responses along with their corresponding confidence scores as approximations of generation probabilities. We further select hypothesis responses with low confidence scores but high actual quality as negative examples. Regarding entity scoring, we employ a similar approach by constructing prompts to assess the relevance of each (entity, response) pair. The specific prompts used in this process are shown in Table 11 and Table 12. It should be noted that the prompts used in the zero-shot and few-shot scenarios only differ in the presence of examples, and therefore the few-shot prompts are not separately displayed."
        }
    ],
    "title": "Dual-Feedback Knowledge Retrieval for Task-Oriented Dialogue Systems",
    "year": 2023
}