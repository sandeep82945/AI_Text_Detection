{
    "abstractText": "The task of related work generation aims to generate a comprehensive survey of related research topics automatically, saving time and effort for authors. Existing methods simplify this task by using human-annotated references in a large-scale scientific corpus as information sources, which is timeand cost-intensive. To this end, we propose a Unified Reference Retrieval and Related Work Generation Model (UR3WG), which combines reference retrieval and related work generation processes in a unified framework based on the large language model (LLM). Specifically, UR3WG first leverages the world knowledge of LLM to extend the abstract and generate the query for the subsequent retrieval stage. Then a lexicon-enhanced dense retrieval is proposed to search relevant references, where an importance-aware representation of the lexicon is introduced. We also propose multi-granularity contrastive learning to optimize our retriever. Since this task is not simply summarizing the main points in references, it should analyze the complex relationships and present them logically. We propose an instruction-tuning method to guide LLM to generate related work. Extensive experiments on two wide-applied datasets demonstrate that our UR3WG outperforms the state-of-the-art baselines in both generation and retrieval metrics.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhengliang Shi"
        },
        {
            "affiliations": [],
            "name": "Shen Gao"
        },
        {
            "affiliations": [],
            "name": "Zhen Zhang"
        },
        {
            "affiliations": [],
            "name": "Xiuying Chen"
        },
        {
            "affiliations": [],
            "name": "Zhumin Chen"
        },
        {
            "affiliations": [],
            "name": "Pengjie Ren"
        },
        {
            "affiliations": [],
            "name": "Zhaochun Ren"
        }
    ],
    "id": "SP:2857705af20f559549348c6cb3e9f8fd614fc031",
    "references": [
        {
            "authors": [
                "Xiuying Chen",
                "Hind Alamro",
                "Mingzhe Li",
                "Shen Gao",
                "Rui Yan",
                "Xin Gao",
                "Xiangliang Zhang."
            ],
            "title": "Target-aware abstractive related work generation with contrastive learning",
            "venue": "SIGIR.",
            "year": 2022
        },
        {
            "authors": [
                "Xiuying Chen",
                "Hind Alamro",
                "Li Mingzhe",
                "Shen Gao",
                "Xiangliang Zhang",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Capturing relations between scientific papers: An abstractive model for related work section generation",
            "venue": "ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Xiuying Chen",
                "Shen Gao",
                "Chongyang Tao",
                "Yan Song",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Iterative document representation learning towards summarization with polishing",
            "venue": "EMNLP.",
            "year": 2018
        },
        {
            "authors": [
                "Xiuying Chen",
                "Mingzhe Li",
                "Shen Gao",
                "Zhangming Chan",
                "Dongyan Zhao",
                "Xin Gao",
                "Xiangliang Zhang",
                "Rui Yan."
            ],
            "title": "Follow the timeline! generating an abstractive and extractive timeline summary in chronological order",
            "venue": "ACM Transactions on Informa-",
            "year": 2023
        },
        {
            "authors": [
                "Xiuying Chen",
                "Mingzhe Li",
                "Shen Gao",
                "Xin Cheng",
                "Qiang Yang",
                "Qishen Zhang",
                "Xin Gao",
                "Xiangliang Zhang."
            ],
            "title": "A topic-aware summarization framework with different modal side information",
            "venue": "Proc. of SIGIR.",
            "year": 2023
        },
        {
            "authors": [
                "Xiuying Chen",
                "Mingzhe Li",
                "Shen Gao",
                "Rui Yan",
                "Xin Gao",
                "Xiangliang Zhang."
            ],
            "title": "Scientific paper extractive summarization enhanced by citation graphs",
            "venue": "arXiv.",
            "year": 2022
        },
        {
            "authors": [
                "Narang",
                "Gaurav Mishra",
                "Adams Yu",
                "Vincent Zhao",
                "Yanping Huang",
                "Andrew Dai",
                "Hongkun Yu",
                "Slav Petrov",
                "Ed H. Chi",
                "Jeff Dean",
                "Jacob Devlin",
                "Adam Roberts",
                "Denny Zhou",
                "Quoc V. Le",
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language",
            "year": 2022
        },
        {
            "authors": [
                "Zekun Deng",
                "Zixin Zeng",
                "Weiye Gu",
                "Jiawen Ji",
                "Bolin Hua."
            ],
            "title": "Automatic related work section generation by sentence extraction and reordering",
            "venue": "AII@iConference.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL.",
            "year": 2019
        },
        {
            "authors": [
                "Zhengxiao Du",
                "Yujie Qian",
                "Xiao Liu",
                "Ming Ding",
                "Jiezhong Qiu",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "Glm: General language model pretraining with autoregressive blank infilling",
            "venue": "ACL.",
            "year": 2022
        },
        {
            "authors": [
                "G\u00fcnes Erkan",
                "Dragomir R. Radev."
            ],
            "title": "Lexrank: Graph-based lexical centrality as salience in text summarization",
            "venue": "Journal of Artificial Intelligence Research.",
            "year": 2004
        },
        {
            "authors": [
                "Thibault Formal",
                "Benjamin Piwowarski",
                "St\u00e9phane Clinchant."
            ],
            "title": "Splade: Sparse lexical and expansion model for first stage ranking",
            "venue": "SIGIR.",
            "year": 2021
        },
        {
            "authors": [
                "Luyu Gao",
                "Jamie Callan."
            ],
            "title": "Condenser: a pretraining architecture for dense retrieval",
            "venue": "ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Luyu Gao",
                "Zhuyun Dai",
                "Jamie Callan."
            ],
            "title": "COIL: Revisit exact lexical match in information retrieval with contextualized inverted list",
            "venue": "NAACL.",
            "year": 2021
        },
        {
            "authors": [
                "Shen Gao",
                "Zhengliang Shi",
                "Minghang Zhu",
                "Bowen Fang",
                "Xin Xin",
                "Pengjie Ren",
                "Zhumin Chen",
                "Jun Ma"
            ],
            "title": "Confucius: Iterative tool learning from introspection feedback by easy-to-difficult curriculum",
            "year": 2023
        },
        {
            "authors": [
                "Maarten Grootendorst"
            ],
            "title": "Self-supervised contextual keyword and keyphrase retrieval with selflabelling",
            "year": 2020
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Mingwei Chang."
            ],
            "title": "Retrieval augmented language model pre-training",
            "venue": "ICML.",
            "year": 2020
        },
        {
            "authors": [
                "Cong Duy Vu Hoang",
                "Min-Yen Kan."
            ],
            "title": "Towards automated related work summarization",
            "venue": "COLING.",
            "year": 2010
        },
        {
            "authors": [
                "Nabil Hossain",
                "Marjan Ghazvininejad",
                "Luke Zettlemoyer."
            ],
            "title": "Simple and effective retrieve-editrerank text generation",
            "venue": "ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Yue Hu",
                "Xiaojun Wan."
            ],
            "title": "Automatic generation of related work sections in scientific papers: An optimization approach",
            "venue": "EMNLP.",
            "year": 2014
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave."
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "EACL.",
            "year": 2021
        },
        {
            "authors": [
                "Hanqi Jin",
                "Tianming Wang",
                "Xiaojun Wan."
            ],
            "title": "Multi-granularity interaction network for extractive and abstractive multi-document summarization",
            "venue": "ACL.",
            "year": 2020
        },
        {
            "authors": [
                "S\u00f8ren Johansen",
                "Katarina Juselius."
            ],
            "title": "Maximum likelihood estimation and inference on cointegration - with applications to the demand for money",
            "venue": "Oxford Bulletin of Economics and Statistics.",
            "year": 2010
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Ledell Lewis",
                "Patrick andw Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Omar Khattab",
                "Matei Zaharia."
            ],
            "title": "Colbert: Efficient and effective passage search via contextualized late interaction over BERT",
            "venue": "SIGIR.",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2019
        },
        {
            "authors": [
                "Patrick S.H. Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Michael Lewis",
                "Wen tau Yih",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Douwe Kiela"
            ],
            "title": "Retrieval-augmented generation",
            "year": 2020
        },
        {
            "authors": [
                "Jinpeng Li",
                "Yingce Xia",
                "Rui Yan",
                "Hongda Sun",
                "Dongyan Zhao",
                "Tie-Yan Liu."
            ],
            "title": "Stylized dialogue generation with multi-pass dual learning",
            "venue": "Advances in Neural Information Processing Systems, 34:28470\u2013 28481.",
            "year": 2021
        },
        {
            "authors": [
                "Jimmy Lin",
                "Xueguang Ma."
            ],
            "title": "A few brief notes on deepimpact, coil, and a conceptual framework for information retrieval techniques",
            "venue": "arXiv.",
            "year": 2021
        },
        {
            "authors": [
                "Jimmy Lin",
                "Xueguang Ma."
            ],
            "title": "A few brief notes on deepimpact, coil, and a conceptual framework for information retrieval techniques",
            "venue": "arXiv: Information Retrieval.",
            "year": 2021
        },
        {
            "authors": [
                "Sheng-Chieh Lin",
                "Jimmy Lin."
            ],
            "title": "A dense representation framework for lexical and semantic matching",
            "venue": "ACM Transactions on Information Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Yang Liu",
                "Mirella Lapata."
            ],
            "title": "Text summarization with pretrained encoders",
            "venue": "EMNLP-IJCNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Chuan Meng",
                "Pengjie Ren",
                "Zhumin Chen",
                "Weiwei Sun",
                "Zhaochun Ren",
                "Zhaopeng Tu",
                "Maarten de Rijke."
            ],
            "title": "Dukenet: A dual knowledge interaction network for knowledge-grounded conversation",
            "venue": "SIGIR.",
            "year": 2020
        },
        {
            "authors": [
                "Stephen Mussmann",
                "Stefano Ermon."
            ],
            "title": "Learning and inference via maximum inner product search",
            "venue": "ICML.",
            "year": 2016
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "ACL.",
            "year": 2002
        },
        {
            "authors": [
                "Jiarui Qin",
                "Jiachen Zhu",
                "Bo Chen",
                "Zhirong Liu",
                "Weiwen Liu",
                "Ruiming Tang",
                "Rui Zhang",
                "Yong Yu",
                "Weinan Zhang."
            ],
            "title": "Rankflow: Joint optimization of multistage cascade ranking systems as flows",
            "venue": "SIGIR.",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research.",
            "year": 2019
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza"
            ],
            "title": "The probabilistic relevance framework: Bm25 and beyond",
            "venue": "Foundations and Trends in Information Retrieval",
            "year": 2009
        },
        {
            "authors": [
                "Keshav Santhanam",
                "Omar Khattab",
                "Jon Saad-Falcon",
                "Christopher Potts",
                "Matei Zaharia."
            ],
            "title": "Colbertv2: Effective and efficient retrieval via lightweight late interaction",
            "venue": "NAACL.",
            "year": 2023
        },
        {
            "authors": [
                "Tao Shen",
                "Xiubo Geng",
                "Chongyang Tao",
                "Can Xu",
                "Xiaolong Huang",
                "Binxing Jiao",
                "Linjun Yang",
                "Daxin Jiang."
            ],
            "title": "Lexmae: Lexicon-bottlenecked pretraining for large-scale retrieval",
            "venue": "ArXiv.",
            "year": 2022
        },
        {
            "authors": [
                "Zhengliang Shi",
                "Weiwei Sun",
                "Shuo Zhang",
                "Zhen Zhang",
                "Pengjie Ren",
                "Zhaochun Ren."
            ],
            "title": "Rade: Reference-assisted dialogue evaluation for opendomain dialogue",
            "venue": "ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Weiwei Sun",
                "Zhengliang Shi",
                "Shen Gao",
                "Pengjie Ren",
                "Maarten de Rijke",
                "Zhaochun Ren."
            ],
            "title": "Contrastive learning reduces hallucination in conversations",
            "venue": "arXiv.",
            "year": 2022
        },
        {
            "authors": [
                "Nandan Thakur",
                "Nils Reimers",
                "Andreas R\u00fcckl\u00e9",
                "Abhishek Srivastava",
                "Iryna Gurevych."
            ],
            "title": "Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models",
            "venue": "arXiv.",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Liang Wang",
                "Nan Yang",
                "Furu Wei."
            ],
            "title": "Query2doc: Query expansion with large language models",
            "venue": "arXiv preprint arXiv:2303.07678.",
            "year": 2023
        },
        {
            "authors": [
                "Yongzhen Wang",
                "Xiaozhong Liu",
                "Zheng Gao."
            ],
            "title": "Neural related work summarization with a joint context-driven attention mechanism",
            "venue": "EMNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "arXiv preprint arXiv:2109.01652.",
            "year": 2021
        },
        {
            "authors": [
                "W. Yu",
                "Dan Iter",
                "Shuohang Wang",
                "Yichong Xu",
                "Mingxuan Ju",
                "Soumya Sanyal",
                "Chenguang Zhu",
                "Michael Zeng",
                "Meng Jiang."
            ],
            "title": "Generate rather than retrieve: Large language models are strong context generators",
            "venue": "ArXiv.",
            "year": 2022
        },
        {
            "authors": [
                "Weizhe Yuan",
                "Graham Neubig",
                "Pengfei Liu."
            ],
            "title": "Bartscore: Evaluating generated text as text generation",
            "venue": "NIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Kai Zhang",
                "Chongyang Tao",
                "Tao Shen",
                "Can Xu",
                "Xiubo Geng",
                "Binxing Jiao",
                "Daxin Jiang."
            ],
            "title": "Led: Lexicon-enlightened dense retriever for large-scale retrieval",
            "venue": "Proceedings of the ACM Web Conference 2023, pages 3203\u20133213.",
            "year": 2023
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Jinming Zhao",
                "Ming Liu",
                "Longxiang Gao",
                "Yuan Jin",
                "Lan Du",
                "He Zhao",
                "He Zhang",
                "Gholamreza Haffari."
            ],
            "title": "Summpip: Unsupervised multi-document summarization with sentence graph compression",
            "venue": "SIGIR.",
            "year": 2020
        },
        {
            "authors": [
                "Xueliang Zhao",
                "Wei Wu",
                "Can Xu",
                "Chongyang Tao",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Knowledgegrounded dialogue generation with pre-trained language models",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Yufan Zhao",
                "Wei Wu",
                "Can Xu"
            ],
            "title": "2020c. Are pretrained language models knowledgeable to ground open domain dialogues? arXiv",
            "year": 2020
        },
        {
            "authors": [
                "Hao Zhou",
                "Weidong Ren",
                "Gongshen Liu",
                "Bo Su",
                "Wei Lu."
            ],
            "title": "Entity-aware abstractive multidocument summarization",
            "venue": "ACL.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Towards a Unified Framework for Reference Retrieval and Related Work Generation Zhengliang Shi1 Shen Gao1\u2217 Zhen Zhang1 Xiuying Chen3\nZhumin Chen1 Pengjie Ren1 Zhaochun Ren2 \u2217 1Shandong University, Qingdao, China 2Leiden University, Leiden, The Netherlands\n3Computational Bioscience Reseach Center, KAUST shizhl@mail.sdu.edu.cn shengao@sdu.edu.cn\nxiuying.chen@kaust.edu.sa zhen.zhang.sdu@gmail.com chenzhumin@sdu.edu.cn jay.ren@outlook.com z.ren@liacs.leidenuniv.nl\nAbstract\nThe task of related work generation aims to generate a comprehensive survey of related research topics automatically, saving time and effort for authors. Existing methods simplify this task by using human-annotated references in a large-scale scientific corpus as information sources, which is time- and cost-intensive. To this end, we propose a Unified Reference Retrieval and Related Work Generation Model (UR3WG), which combines reference retrieval and related work generation processes in a unified framework based on the large language model (LLM). Specifically, UR3WG first leverages the world knowledge of LLM to extend the abstract and generate the query for the subsequent retrieval stage. Then a lexicon-enhanced dense retrieval is proposed to search relevant references, where an importance-aware representation of the lexicon is introduced. We also propose multi-granularity contrastive learning to optimize our retriever. Since this task is not simply summarizing the main points in references, it should analyze the complex relationships and present them logically. We propose an instruction-tuning method to guide LLM to generate related work. Extensive experiments on two wide-applied datasets demonstrate that our UR3WG outperforms the state-of-the-art baselines in both generation and retrieval metrics."
        },
        {
            "heading": "1 Introduction",
            "text": "The automatic related work generation (RWG) system, which aims at generating a related work section for a target paper, may help the readers go through the cutting-edge research progress (Hoang and Kan, 2010). Two main adjacent stages are considered fundamental disciplines in RWG task: (1) selecting related reference papers and (2) figuring out the logical relation to write a summary to present the evolving process of a specific field.\n\u2217 Corresponding author.\nHowever, most existing RWG methods (Chen et al., 2022a, 2021; Wang et al., 2019) only focus on the second stage, where the human-annotated reference papers are taken as input via a summarization model. Consequently, retrieving related papers from the large-scale corpus remains underemphasized. To address this problem, we focus on amalgamating the above two stages: retrieving the related papers and generating the related work. Our unified model, unlike previous RWG research, alleviates the user\u2019s burden by requiring only a high-level abstract of the target paper as input. This approach minimizes the workload for practical purposes.\nAn intuitive retrieval practice in RWG is taking the abstract as a query and employing dense retrieval methods to obtain the related papers according to semantic similarity. However, the abstract given by the user is too vague to be a clear intention for reference. Some related topics are inherently correlated with the paper but are not explicitly mentioned in the abstract, indicating semantic mismatches. Moreover, the presence of rare proper nouns, e.g., specialized terminologies in scientific papers, makes it difficult for existing dense\nretrieval methods to capture the semantic nuances. To generate the related work section, a commonused alternative is leveraging the summarization models (Chen et al., 2023a,b). Nevertheless, an ideal related work section should reason intricate relations among various references, instead of simply enumerating their main contributions. Therefore, training the model to grasp the concept of RWG while producing a high-quality related work section remains challenging.\nIn this work, we leverage the large language model (LLM) to enhance retrieval and generation stages in the RWG task to tackle the above challenges. As shown in Figure 2, we propose an Unified Reference Retrieval and Related Work Generation framework (UR3WG). Specifically, we leverage the LLM to extend the vague abstract and generate a query containing more relevant background information since the LLM has shown strong knowledge association ability (Wang et al., 2023). The generated query is used to subsequent retrieval stage. To mitigate the diminished performance of dense semantic retrieval methods, we propose a Lexicon-Enhanced dense Retrieval (LER) method to integrate the advantages of lexical retrieval, where a learnable lexical text-matching algorithm is introduced. We also propose a multigranularity contrastive learning method to optimize our retriever, including group-wise and pairwise contrast. Since the related work section should comprehensively introduce the relationship between references, instead of simply summarizing the main points, we propose an instruction-tuning method to guide LLM to generate a high-quality related work section. Extensive experiments conducted on two benchmark datasets show that our model significantly outperforms all the strong baselines, e.g., pushing the ROUGE-1 to 31.59 (8.4% relative improvement) and BERTScore to 0.70 (6.9% relative improvement).\nOur contributions are as follows: (1) We propose the UR3WG, the first unified reference retrieval and related work generation model in the RWG task. (2) We propose a lexicon-enhanced dense retrieval method with an LLM-based query extension method to retrieve highly related reference papers, supervised by multi-granularity contrastive learning. (3) We propose an instruction-tuning method to incorporate the references and guide the LLM to generate related work logically. (4) Experimental results on two benchmark datasets show the superi-\nority of our proposed model."
        },
        {
            "heading": "2 Related work",
            "text": ""
        },
        {
            "heading": "2.1 Related work generation",
            "text": "Similar to multi-document summarization, the task of related work generation (RWG) which usually takes multiple reference papers as input and generate a related work section for a target paper (Chen et al., 2022a; Hoang and Kan, 2010) via summarizes the related information in a logical order. Existing RWG methods can be divided into extractive and abstractive methods. Specifically, extractive methods select a subset of words or sentences most relevant to the input abstract to form the final related work (Hoang and Kan, 2010; Hu and Wan, 2014; Deng et al., 2021) . With the emergence of neural-based models (Lewis et al., 2019; Raffel et al., 2019; Touvron et al., 2023), more abstractive methods are utilized to solve the RWG task (Zhao et al., 2020a; Chen et al., 2022b). For example, Chen et al. (2022a) leverage the information of the target paper and propose a target-aware graph encoder to model the relationship between reference papers. Although these methods have shown exemplary performance, they rely on the human-labeled references in the target paper, which still requires a lot of manual retrieval work."
        },
        {
            "heading": "2.2 Retrieval argument generation",
            "text": "Information retrieval has been widely used in many knowledge-intensive natural language generation tasks, e.g., question answering (Guu et al., 2020; Gao et al., 2023) and knowledge-grounded dialogues (Zhao et al., 2020b; Meng et al., 2020; Li et al., 2021). For example, Lewis et al. (2020) combine parametric and non-parametric knowledge, where the former employs the pre-train language model as a knowledge base while the latter is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. Izacard and Grave (2021) propose to first retrieve passages via sparse, e.g., BM25 or dense retrieval, e.g., DPR (Karpukhin et al., 2020) to obtain the relevant passages. Then a sequence-to-sequence (seq2seq) model is employed to generate the answer based on the concatenation of the representations of all the retrieved passages.\nThese retrieval-augmented methods are usually designed for scenarios with an explicit and clean retrieval goal. However, there have been few attempts at the RWG task. On the one hand, the model only\nuses the abstract as input, and the abstract does not indicate which topic should be surveyed in the related work. On the other hand, it is noteworthy that the citation relation among multiple documents is implicit in nature, thereby necessitating the reasoning to understand."
        },
        {
            "heading": "3 Task formulation",
            "text": "In this paper, we generate the related work section of the target paper and only use the abstract X as the input without the ground-truth references R\u2217. Specifically, we propose a unified reference retrieval and text generation framework, which consists of three steps: (1) leverage the knowledge of LLMs to extend the input abstract and generate the query used for the subsequent retrieval stage. (2) retrieve reference papers R = {r1, r2, ..., r|R|} from the document corpus D = {d1, d2, ..., d|D|} according to the similarity S to query at semantic level Ss and lexicon level Sl; (3) generate the related work section Y\u0302 of the target paper by summarizing the retrieved references R."
        },
        {
            "heading": "4 Methodology",
            "text": ""
        },
        {
            "heading": "4.1 Model overview",
            "text": "As shown in Figure 2, UR3WG first generates the reference retrieval query by extending the abstract X . Then a lexicon-enhanced dense retriever (LER) retrieves relevant references by using the extended abstract as the query. Since RWG is different from simple text summarization tasks, the logical relation between input abstract and references should be carefully considered. We employ an instructiontuning method to train LLM to understand the def-\ninition of the RWG task and generate the highquality related work section."
        },
        {
            "heading": "4.2 Query extension",
            "text": "It has been proven that achieving better retrieval performance is feasible if the query can be rewritten to a more similar form to the candidate documents (Yu et al., 2022). Since the LLMs trained on massive textual corpora have shown strong knowledge association ability (Wang et al., 2023), we leverage the knowledge of LLM to extend the abstract X and formulate pseudo references R\u0302 as an extended abstract, which contains more rich information in the RWG task. The R\u0302 is taken as a query to search relevant references accurately. Specifically, we first design an instruction IQE which contains two parts: (1) DQE is a demonstration example which describes the definition of the query extension task, and (2) X is the abstract of the target paper:\nIQE = Demonstration : {DQE} Input : {X}.\nThen, we leverage the LLM to generate pseudo references R\u0302 on the condition of instruction IQE . Since R\u0302 is highly relevant to the abstract X and can be potentially cited, we take R\u0302 as the query during the subsequent retrieval stage, assisting in query disambiguation and guiding the retriever (Wang et al., 2023). And we optimize the proposed UR3WG via standard language modeling objective:\nLQE = |R\u2217|\u2211 i=1 |ri|\u2211 t=1 logP\u03c6(r (t) i |r (<t) i , IQE), (1)\nwhere the \u03c6 is the learnable parameters and R\u2217 is the set of ground-truth references."
        },
        {
            "heading": "4.3 Lexicon-enhanced dense retrieval",
            "text": "Current trainable dense retrieval models built on language models have shown remarkable capability in capturing sentence-level similarity. Despite these advances, they exhibit diminished performance when retrieving references due to their omission of significant local phrases and entities (Zhang et al., 2023), which leads to outcomes worse than those obtained via traditional lexiconbased retrieval methods (Shen et al., 2022; Thakur et al., 2021; Lin and Ma, 2021a). To alleviate this problem, we introduce a Lexicon-Enhanced dense Retrieval (LER) method that explicitly incorporates importance-aware lexicon representations into dense semantic representations. We also design an approximation to accelerate this method for application in the large-scale corpus."
        },
        {
            "heading": "4.3.1 Semantic-oriented retrieval",
            "text": "Inspired by the dense retrieval methods (Khattab and Zaharia, 2020; Santhanam et al., 2023), we employ dual independent dense encoders: Encoderq and Encoderd, which map the query q and candidate reference d (a.k.a., document) to token-level representation, respectively. Specifically, we add the [CLS] at the beginning of q and d. The query representation Hq is obtained via:\nHq = Encoderq(q), (2)\nwhere Hq = {h1q , h2q , ...h |q| q } \u2208 R|q|\u00d7m. The hiq indicates the representation of each token, and m is the feature size. Similarly, we derive the tokenlevel representation of each document d through:\nHd = Encoderd(d), (3)\nwhere Hd = {h1d, h2d, ...h |d| d } \u2208 R |d|\u00d7m, and |d| is the length of the document. We take the embedding of [CLS] token as the semantic representation. The semantic similarity between query q and document d can be defined based on the inner product:\nSs(q, d) = Linear(h 1 q) T \u00b7 Linear(h1d), (4)\nwhere Linear denotes a linear layer with an activation function to compress and extract features from the raw representation."
        },
        {
            "heading": "4.3.2 Lexicon-oriented retrieval",
            "text": "To capture the information of local phrases and entities, we propose a token-level interaction network with a guided interaction mechanism that calculates the lexicon-level similarity. The architecture is illustrated in Figure 2. Specifically, we construct a continuous bag-of-words representation for each token in q based on Hq:\nEq = softmax(HqWq) \u2208 R|q|\u00d7|V|, (5)\nwhere Wq \u2208 Rm\u00d7|V| is the trainable parameter, and the |V| is the vocabulary size. The Eq indicates the importance of the input token to all the vocabulary. Then a lexicon representation of query q can be constructed via a max-pooling layer along each token of Eq:\n\u03b8q = MaxPooling(Eq) \u2208 R|V|. (6)\nSimilarly, we obtain the lexicon representation \u03b8d for the document d via:\nEd = softmax(HdWd) \u2208 R|d|\u00d7|V|, (7) \u03b8d = MaxPooling(Ed) \u2208 R|V|, (8)\nwhere the Wd \u2208 Rm\u00d7|V| is the trainable parameter. Although the bag-of-words representation contains fine-grained importance of lexicon, the representation \u03b8q over the entire vocabulary is too sparse and contains much noisy information. Therefore, we propose a group-wise local threshold mechanism that partitions the sparse representation \u03b8q into several groups containing k tokens, and only remains the maximum value of each group:\n\u03b8\u0303q = {max(\u03b8i\u223ci+kq )|i = nk, n \u2264 |V| k }, (9) \u03c4q = {argmax(\u03b8i\u223ci+kq )|i = nk, n \u2264 |V| k }, (10)\nwhere the \u03c4q is the index of the maximum of each group, indicating the most important lexicon for query q in this group. And the interaction between the sparse representation of q and d can be guided by \u03c4q to calculate the lexicon-level similarity:\nSl(q, d) = |\u03c4q |\u2211 i=0 \u03b8\u0303iq \u00d7 \u03b8 \u03c4 iq d . (11)\nFinally, we combine the similarity score of semantic and lexicon levels as the final matching score for the query q and document d:\nS(q, d) = Ss(q, d) + Sl(q, d). (12)\nFor each query q, we retrieve the top-k candidate reference R as input. Considering the scientific paper corpus is large-scale in real-world scenarios while the online interaction mentioned in Equation 11 is time-intensive, we design an approximate algorithm for offline matching. The details and qualitative analysis are provided in Appendix A.1."
        },
        {
            "heading": "4.3.3 Multi-granularity contrastive learning",
            "text": "Unlike the existing passage retrieval task which assumes there is only one relevant document for a query (Karpukhin et al., 2020; Khattab and Zaharia, 2020), in the RWG task, a scientific paper has multiple diverse references. We propose a multi-granularity contrastive learning method to alleviate this problem, including group-wise and pair-wise contrast. Specifically, we denote the reference cited in the target paper as R+ = {r+1 , r + 2 , . . . , r + |R|} and the irrelevant paper as R\u2212 = {r\u22121 , r \u2212 2 , . . . , r \u2212 |R|}.\nGroup-wise contrastive learning. Given the abstract X of the target paper, the goal of our groupwise contrastive learning is to minimize the distance between the abstract X and the positive reference group R+ and maximize the distance to the irrelevant document group R\u2212. Specifically, we employ negative log-likelihood as the loss function for positive references:\nLG(q,R+,R\u2212) (13)\n= \u2212 log \u2211|R+|\ni=0 e S(q,r+i )\u2211|R+|\ni=0 e S(q,r+i ) + \u2211|R\u2212| i=0 e S(q,r\u2212i ) .\nPair-wise contrastive learning. Most of the existing retrieval methods (Qin et al., 2022; Hossain et al., 2020) obtain the relevant documents by ranking the candidate documents based on the similarity to the query. They only minimize the negative log-likelihood of the positive documents, implicitly optimizing their model to binary classify the documents. However, the ranks of documents also provide a training signal to optimize the retrieval module. Specifically, we propose a pair-wise contrastive loss LP which contrasts each positive reference r+i in R + and negative reference r\u2212i in R \u2212, explicitly optimizing the model to rank r+i before r\u2212i . The LP can be formulated as:\nLP(q,R+,R\u2212) (14)\n= |R+|\u2211 i=0 |R\u2212|\u2211 j=0 max(0, S(q, r\u2212j )\u2212 S(q, r + i ) + \u03bb),\nwhere the \u03bb is the margin that we defined to explicitly constrain the gap between positive and negative references. Our contrastive loss is the combination of the LG and LP:\nLCL = \u00b5LG + (1\u2212 \u00b5)LP, (15)\nwhere \u00b5 is the weight of two losses."
        },
        {
            "heading": "4.4 Instruction-tuning for generation",
            "text": "Instruction tuning has been shown to improve the performance and generalization of LLMs in complex tasks for supervising the LLMs following concrete instructions (Chung et al., 2022; Wei et al., 2021). Since the definition of the RWG task is complex and different from summarizing text directly, we design an instruction-tuning method to help LLM understand the approach of generating related work. Specifically, we describe the definition of the RWG task via the demonstration DGEN. The DGEN paired with abstract X and reference R are concentrated to construct the instruction IGEN, which can be formulated as:\nIGEN = Demonstration : {DGEN} Input : {abstractX} Reference : {R}.\nWe then leverage the LLM to generate related work on the condition of the IGEN, prompting LLM to understand the definition of the RWG task and generate the related work section logically, which is supervised via the instruction-tuning loss:\nLGEN = \u2212 |Y |\u2211 t=1 logP\u03c6(y (t) i |y (<t) i , IGEN). (16)\nThe Y is the ground truth related work of the target paper and \u03c6 is the parameters shared with Equation 1."
        },
        {
            "heading": "4.5 Multi-task learning",
            "text": "We jointly optimize UR3WG with two tasks, i.e., query extension in \u00a7 4.2 and instruction-following generation in \u00a7 4.4. The final training objective is defined as:\nJ = \u03b1LQE + \u03b7LGEN, (17)\nwhere the \u03b1 and \u03b7 are the hyper-parameters, denoting the weights of the two losses, respectively."
        },
        {
            "heading": "5 Experiment Setup",
            "text": ""
        },
        {
            "heading": "5.1 Datasets",
            "text": "We conduct experiments on two widely-applied datasets: TAS2 and TAD (Chen et al., 2022a). Specifically, the TAS2 consists of 117,700 scientific publications from several fields, whereas TAD consists of 218,255 scientific publications from the computer science field. For each example in datasets, it includes (1) the abstract of the article, (2) a paragraph of the related work, and (3) the reference paper cited in this paragraph (four references per paragraph on average)."
        },
        {
            "heading": "5.2 Evaluation metrics",
            "text": "Following Chen et al. (2018); Zhou et al. (2021); Chen et al. (2022a), we mainly employ ROUGE (1,2,L,SU) for evaluation. Since only using the ROUGE metric to evaluate generation quality can be misleading (Shi et al., 2023), we also consider the other metrics, including the lexicon-based metric, e.g., BLEU (Papineni et al., 2002) and semantic-based metrics, e.g., BERTScore (Zhang et al., 2019) and BARTScore (Yuan et al., 2021). We evaluate the retriever with Recall@K (k=5, 10, 20) metrics. The statistical significance of differences observed between the performance of two runs is tested using a two-tailed paired t-test at \u03b1 = 0.01 and p < 0.05.\nwe also conduct the human evaluation. Three\nwell-educated Master students are invited to judge 40 randomly sampled examples with a three-scale in the four aspects: Relevance (Rel.), Fluency (Flu.), Coherence (Cohe.), Informativeness (Info.). The details of these aspects can be found in Appendix A.5."
        },
        {
            "heading": "5.3 Baselines",
            "text": "We mainly compare our UR3WG with three types of baselines: multi-document summarization methods, related work generation methods, and large language models.\nThe multi-document summarization baselines include extractive methods, such as LexRank (Erkan and Radev, 2004), BertSumExt (Liu and Lapata, 2019) and NES (Wang et al., 2019) as well as abstractive methods like BertSumAbs (Liu and Lapata, 2019) and EMS (Zhou et al., 2021). These baselines take the references as input. The related work generation methods include RRG (Chen et al., 2021) and TAG (Chen et al., 2022a), which take the abstract and references as input. The large language models include Llama-7B (Touvron et al., 2023), Vicuna 1, Claude 2, Davinci-text-003, and ChatGPT 3, which take the instruction IGEN input.\nWe also compare our retrieval method LER with strong baselines, including traditional term-based\n1https://github.com/lm-sys/FastChat 2https://www.anthropic.com/ 3https://openai.com/blog/chatgpt\nmethods like BM25 (Robertson et al., 2009), and dense retrievers, such as DPR (Karpukhin et al., 2020), ColBERT (Khattab and Zaharia, 2020), Condenser (Gao and Callan, 2021), as well as lexiconaware retrievers, e.g., DLR (Lin and Lin, 2022), SPLADE (Formal et al., 2021) and UniCOIL (Lin and Ma, 2021b). Since the abstract is too long with many irrelevant words, we take concatenation of the keywords extracted from the abstract as the query. To ensure the fairness of comparison, all the baselines are finetuned with the same datasets as our proposed method. More details about the baselines are provided in Appendix A.3."
        },
        {
            "heading": "5.4 Implementation details",
            "text": "We initialize the parameters with Llama-7B (Touvron et al., 2023). We vary the weight of contrastive learning loss \u00b5 in {0.5, 0.6, 0.7, 0.8, 0.9}, and find that 0.7 achieves the best performance. Following the Sun et al. (2022), we tune the weight \u03b1 to 0.5, \u03b7 to 0.5 at initialization, respectively, and linearly decrease to 0.3 and 0.2. We optimize the model using AdamW optimizer with parameters \u03b21 = 0.98, \u03b22 = 0.99, the learning rate of 2e\u22125, and the weight decay coefficient of 0.01. The training of our model can be done within 28 hours on the TAD dataset and 17 hours on the TAS2 dataset using four NVIDIA A100 PCIE GPUs."
        },
        {
            "heading": "6 Result and analysis",
            "text": ""
        },
        {
            "heading": "6.1 Experiment result",
            "text": "Generation performance evaluation. Table 1 shows the details of the results. Overall, UR3WG achieves the best performance compared to the other baselines. For example, UR3WG gets ROUGE-1=32.68, ROUGE-2=7.74 in TAD\ndatasets, with 8.4% and 25.6% relative improvement compared to existing state-of-the-art baseline. As shown in Table 4, we also select several strong baselines for more comprehensive evaluation. The proposed UR3WG outperforms the strong baselines in terms of the lexicon-level and semanticlevel metrics, e.g., pushing BLEU-1 to 23.22 and BERTScore to 0.70 in TAS2 dataset. These results demonstrate that our UR3WG fits well in generating high-quality related work with the assistance of curated instruction.\nRetrieval performance evaluation. As shown in Table 2, our retrieval module outperforms the strong retrieval baselines by a large margin. For example, LER reaches Recall@5=37.35 in the TAS2 dataset with 13.14 absolute improvement compared to the state-of-the-art baseline SPLADE, which illustrates that our proposed lexicon-enhanced dense retrieval is more suitable for reference retrieval scenarios. We also consider several variant models for more comprehensive comparison in Section 7."
        },
        {
            "heading": "6.2 Human evaluation",
            "text": "Table 6 shows the results of the human evaluation. We find that the UR3WG outperforms the best RWG baselines TAG in four aspects, e.g., pushing Relevance to 2.77 (0.47 absolute improvement) in TAD dataset. We also observe that UR3WG achieves comparable and better performance with ChatGPT (175B) with only 7B parameters, indicating the effectiveness of our method. The average Kappa statistics for four evaluation metrics are 0.72, 0.70, 0.71, and 0.73, illustrating agreement among the annotators. More results details are provided in Appendix A.5."
        },
        {
            "heading": "6.3 Ablation study",
            "text": "To better understand the impact of different components of our UR3WG, we employ the following modifications to the architecture.\n\u2013w/o LP. We remove the loss LP mentioned in Equation 15. We observed an average decrease in recall of 8 \u223c 10 on both datasets, e.g.,, the Recall@10 drops from 41.56 to 39.15 with 6.16% relative decrease, 40.20 to 38.01 with 5.76% relative decrease, which indicates the effect of our multi-granularity contrastive.\n\u2013w/o Sl. We remove the lexicon matching score Sl(q, d) (in \u00a7 4.3.2). From the results shown in Table 2, the recall metrics have a significant decline, which indicates that the lexicon information is critical to retrieve relevant references in the RWG task.\n\u2013w/o LQE. We remove the loss LQE for query extension. As shown in Table 2 and Table 1, both the generation and retrieval performance suffers from a significant decrease. This result indicates that the external knowledge injected via LQE assists the UR3WG in generating more informative pseudo\nreferences and final related work section. \u2013w/o LGEN. We replace the instruction-tuning loss LGEN with a standard language modeling objective (Zhao et al., 2020c; Johansen and Juselius, 2010), where we take the concatenation of abstract and references as input, supervising the model predict the ground-truth related work. As shown in Table 1, the performance significantly declines, demonstrating that the instruction we design fits the LLMs well in the RWG task and improves the capability in complex generation scenarios."
        },
        {
            "heading": "6.4 Case study",
            "text": "We conduct several case studies and find that UR3WG is more effective at retrieving relevant references and generating more informative text than baselines. Details are provided in Appendix A.6."
        },
        {
            "heading": "7 Discussion",
            "text": "Variant model for reference retrieval. For each retrieval, we take the abstract, keywords of abstract, and extended abstract as the query to search relevant documents. Specifically, the keywords are extracted via KeyBert (Grootendorst, 2020), and extended abstract is generated via the proposed UR3WG. As shown in Table 3, for each retrieval method, taking the extended abstract as the query reaches the best performance, which indicates that our query extension method can be combined with different retrieval methods. And this result further illustrates that the extended abstract contains rich information about the target paper. We also observe a significant decline when we replace the query\nfrom abstract to keywords. The potential reason is that the abstract is too long with many irrelevant words, hindering the performance of retrieval.\nThe impact of lexicon representation group size. we conduct experiments to alternate the sparse lexicon representation into different dimensions. As shown in Figure 3, we find that the recall keeps increasing and peaks at 1024 (TAD datasets) and 768 (TAS2 datasets), indicating that more representative lexicon information can be used to match relevant documents in the retrieval stage. However, when the dimension increases to 2048, the recall decreases, suggesting that some low-featured lexicon information is retained.\nQualitative analysis about the approximation We explore the impact by comparing the performance before and after approximation, and results are are shown in Table 3. Comparing with the vanilla LER, we observe that LER (w. approx.) an average decrease of about 3.13% and 3.68% in TAS2 and TAD datasets, respectively. This result illustrates that the approximation can improve retrieval efficiency at a slight performance cost. More details can be found in Appendix A.2."
        },
        {
            "heading": "8 Conclusion",
            "text": "In this paper, we propose a UR3WG, which fuses the reference retrieval and related work generation into a unified framework based on large language models (LLMs). Concretely, UR3WG first extend the input abstract and generate the query used for next retrieval stage. Then a lexicon-enhanced dense retrieval (LER) is proposed to retrieve the relevant document at the semantic and lexicon level. The LER is optimized via multi-granularity contrastive learning, including group- and pair-wise contrasts.\nSince RWG is different from simple text summarization tasks, we employ an instruction-tuning method, enabling the LLM to understand the definition of RWG task and generate a high-quality related work section. Experiments on two datasets demonstrate that our model establishes a new stateof-the-art with automatically retrieved references.\nLimitations\nThe main limitation of this paper is the requirement of massive computation resource. We will explore how to employ our method in low-resource scenarios. Another limitation is that we only consider the single-lingual retrieval scenario. In the future, we plan to extend our model to multi-lingual applications, which is common in real-world.\nEthics Statement\nThe paper proposes UR3WG, a unified reference retrieval and related work generation framework. Given the abstract as input, the UR3WG first searches relevant papers as references and generate related work. Moreover, reference papers should be cited appropriately."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Key R&D Program of China with grant No. 2020YFB1406704, the Natural Science Foundation of China ( T2293773, 62272274, 61972234, 62072279, 62102234, 62202271), the Natural Science Foundation of Shandong Province (ZR2022QF004), the Key Scientific and Technological Innovation Program of Shandong Province (2019JZZY010129), the Fundamental Research Funds of Shandong University."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Retrieving on large-scale corpus Since the scientific paper corpus is usually largescale in the real-world scenario, our UR3WG should support a billion-scale similarity search more efficiently. Following the Lewis et al. (2020), we first encode all candidate documents into vector representations using the encoder Encoderd, and utilize Faiss to store and conduct the real-time vector search. Faiss4 is a library for efficient similarity search and clustering of dense vectors. Faiss employs the MIPS (Maximum Inner Product Search) to find the document vectors with the highest inner product with the query vector (Mussmann and Ermon, 2016).\nSince the online interaction mentioned in Equation 11 is time-intensive, we can approximate it by creating an index of group-wise maximum and minimum values of \u03b8:\n\u03c4d = {argmax(\u03b8i\u223ci+kd )|i = nk, n \u2264 \u230a |V| k \u230b},\n\u03b8\u0303d,max = {max(\u03b8i\u223ci+kd )|i = nk, n \u2264 \u230a |V| k \u230b}, \u03b8\u0303d,min = {min(\u03b8i\u223ci+kd )|i = nk, n \u2264 \u230a |V| k \u230b},\nwhere the \u230a\u00b7\u230b indicates the lower rounding. The essence of our approximation method is that the bag-of-words representation is highly sparse, so we can assume that only one individual element in each group is significantly larger than the other elements, while the other elements are only slightly different. Therefore, the Equation 11 can be further simplified as:\nSl = |\u03c4q |\u2211 i=0 \u03c3i \u00b7 \u03b8\u0303iq \u00b7 \u03b8\u0303id,max + (1\u2212 \u03c3i) \u00b7 \u03b8\u0303iq \u00b7 \u03b8\u0303id,min\nwhere \u03c3i is a label defined as:\n\u03c3i = { 0, \u03c4 iq \u0338= \u03c4 id 1, \u03c4 iq = \u03c4 i d\n. (18)\nFor i-th group of \u03b8d during the matching, if the \u03c4 id equals to \u03c4 iq , there is no bias. If the \u03c4 i d not equals to \u03c4 iq , the value \u03c4 i d-th element in \u03b8d can be alternate by the minimum value of this group based on our assumption. With this approximation, we can use Faiss to perform semantic matching on the largescale corpus.\n4https://github.com/facebookresearch/faiss\nA.2 Qualitative analysis about the approximation\nTo support the retrieval on the large-scale corpus, we approximate the interaction to adapt the vector retrieval method in Faiss (in Eq A.1), which unavoidably introduces the loss of information. We explore the impact by comparing the performance before and after approximation, and the results are shown in Table 3 as LER (w. approx.). Comparing with the vanilla LER , we can observe that the performance of the algorithm has decreased, with an average decrease of about 3.13% and 3.68% in TAS2 and TAD datasets, respectively. This result illustrates that the approximation can improve retrieval efficiency at a slight performance cost. With this approximation, we can use Faiss to perform semantic matching on the large-scale corpus. Otherwise, it will be difficult for us to retrieve on a large-scale corpus with acceptable latency.\nA.3 Details for baselines\nWe mainly compare our UR3WG with three types of baselines: multi-document summarization methods, related work generation methods, and large language models.\nThe multi-document summarization baselines include: LEAD, which concatenates the first sentence of each reference; LexRank (Erkan and Radev, 2004), which extract summarization based on the graph representation of sentences; NES (Wang et al., 2019), a extractive method which measures the relevance via bibliography graph; MGSum (Jin et al., 2020), an abstractive method based on multigranularity interaction network; BertSum (Liu and Lapata, 2019), a summarization system built on a pre-trained BERT (Devlin et al., 2019); EMS (Zhou et al., 2021), which augments the encoder-decoder framework with a heterogeneous graph.\nThe related work generation methods include: RRG (Chen et al., 2021) , which enhanced by a iteratively refined relation-aware graph between references; TAG (Chen et al., 2022a), a transformerbased model with a target-aware graph encoder.\nThe large language models (LLMs) include flan-t5 (Chung et al., 2022), Llama-7B (Touvron et al., 2023), ChatGLM (Du et al., 2022), Vicuna 5, claude 6, Davinci-text-003, and ChatGPT 7, where the instruction IGEN is taken as input.\n5https://github.com/lm-sys/FastChat 6https://www.anthropic.com/ 7https://openai.com/blog/chatgpt\nWe compare our lexicon-enhance retrieval method with the following baselines: BM25 (Robertson et al., 2009), a classical sparse retrieval; DPR (Karpukhin et al., 2020), a dense retrieval method with a dual-encoder framework; SPLADE (Formal et al., 2021), a passage ranker based on sparsity regularization and a log-saturation effect; ColBERT (Khattab and Zaharia, 2020), which searches relevant passages via contextualized late interaction. UniCOIL (Lin and Ma, 2021b), an extension of classical sparse retrieval COIL (Gao et al., 2021). Since the abstract is too long with many irrelevant words, we take concatenation of the keywords extracted from the abstract as the query.\nA.4 Variant model\nSince the existing summarization baselines only use the references as input, the abstract of the target paper also contains much useful information. To make a fair comparison, we develop a variant model for each multi-document summarization method which uses the concatenation of the abstract of the target paper and references as input. We add the notion (w/ T) to each original summarization method to denote the variant model. We also implement a baseline, denoted as LEAD, which concatenates the first sentence of each ref-\nerence. As shown in Table 5, the ROUGE metrics increase when the abstract is concatenated to the reference as input, e.g., an average 4% relative improvement in ROUGE-1 metrics. However, it still lags behind the proposed UR3WG.\nA.5 Human evaluation\nA.5.1 Details for human evaluation\nWe conduct human evaluation, where three welleducate Master students are invited to judge 40 randomly sampled examples. Specifically, we show each annotator the abstract, related work, and corresponding references. Each annotator is asked to rate the related work with a three-scale in the following metrics: (1) Relevance (Rel.): the relevance between the raw abstract and the related work. (2) Fluency (Flu.): whether the related work is fluent with no grammatical errors. (3) Coherence (Cohe.): coherence refers to how well the sentences are connected and how they flow together to form a coherent and understandable text. (4) Informativeness (Info.): Whether related work covers the key information mentioned in the article or can provide more extensions. Table 6 shows the details of the human evaluation results.\nA.6 Case study Table 7 presents an example of the generated related work by UR3WG and the start-of-the-art baseline TAG. And we highlight the same key information with the same color. We first observe that UR3WG retrieves the ground-truth references correctly, even though the keyword \u201cpage rank\u201d is not mentioned in the abstract, which verifies the efficiency of our retriever. Moreover, the related work generated by UR3WG captures more highly related key information than the TAG baseline."
        }
    ],
    "title": "Towards a Unified Framework for Reference Retrieval and Related Work Generation",
    "year": 2023
}