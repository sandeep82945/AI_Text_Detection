{
    "abstractText": "Keyphrase Generation (KPG) is a longstanding task in NLP with widespread applications. The advent of sequence-to-sequence (seq2seq) pre-trained language models (PLMs) has ushered in a transformative era for KPG, yielding promising performance improvements. However, many design decisions remain unexplored and are often made arbitrarily. This paper undertakes a systematic analysis of the influence of model selection and decoding strategies on PLM-based KPG. We begin by elucidating why seq2seq PLMs are apt for KPG, anchored by an attention-driven hypothesis. We then establish that conventional wisdom for selecting seq2seq PLMs lacks depth: (1) merely increasing model size or performing task-specific adaptation is not parameter-efficient; (2) although combining in-domain pre-training with task adaptation benefits KPG, it does partially hinder generalization. Regarding decoding, we demonstrate that while greedy search achieves strong F1 scores, it lags in recall compared with samplingbased methods. Based on these insights, we propose DESEL, a likelihood-based decodeselect algorithm for seq2seq PLMs. DESEL improves greedy search by an average of 4.7% semantic F1 across five datasets. Our collective findings pave the way for deeper future investigations into PLM-based KPG.",
    "authors": [
        {
            "affiliations": [],
            "name": "Di Wu"
        },
        {
            "affiliations": [],
            "name": "Wasi Uddin Ahmad"
        },
        {
            "affiliations": [],
            "name": "Kai-Wei Chang"
        }
    ],
    "id": "SP:1faf10a0d81ae51a88540b4f85e06dbdbd7b73b3",
    "references": [
        {
            "authors": [
                "Wasi Ahmad",
                "Xiao Bai",
                "Soomin Lee",
                "Kai-Wei Chang."
            ],
            "title": "Select, extract and generate: Neural keyphrase generation with layer-wise coverage attention",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Iz Beltagy",
                "Kyle Lo",
                "Arman Cohan."
            ],
            "title": "SciBERT: A pretrained language model for scientific text",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "G\u00e1bor Berend."
            ],
            "title": "Opinion expression mining by exploiting keyphrase extraction",
            "venue": "Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1162\u20131170, Chiang Mai, Thailand. Asian Federation of Natural Language Processing.",
            "year": 2011
        },
        {
            "authors": [
                "Florian Boudin."
            ],
            "title": "A comparison of centrality measures for graph-based keyphrase extraction",
            "venue": "Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 834\u2013838, Nagoya, Japan. Asian Federation of Natural Lan-",
            "year": 2013
        },
        {
            "authors": [
                "Florian Boudin."
            ],
            "title": "Unsupervised keyphrase extraction with multipartite graphs",
            "venue": "Proceedings of the",
            "year": 2018
        },
        {
            "authors": [
                "Florian Boudin",
                "Ygor Gallina",
                "Akiko Aizawa."
            ],
            "title": "Keyphrase generation for scientific document retrieval",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1118\u20131126, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Erion \u00c7ano",
                "Ond\u0159ej Bojar."
            ],
            "title": "Two huge title and keyword generation corpora of research articles",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 6663\u20136671, Marseille, France. European Language Resources Association.",
            "year": 2020
        },
        {
            "authors": [
                "Hou Pong Chan",
                "Wang Chen",
                "Lu Wang",
                "Irwin King."
            ],
            "title": "Neural keyphrase generation via reinforcement learning with adaptive rewards",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2163\u20132174, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Jun Chen",
                "Xiaoming Zhang",
                "Yu Wu",
                "Zhao Yan",
                "Zhoujun Li."
            ],
            "title": "Keyphrase generation with correlation constraints",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4057\u20134066, Brussels, Belgium.",
            "year": 2018
        },
        {
            "authors": [
                "Wang Chen",
                "Hou Pong Chan",
                "Piji Li",
                "Lidong Bing",
                "Irwin King."
            ],
            "title": "An integrated approach for keyphrase generation via exploring the power of retrieval and extraction",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the",
            "year": 2019
        },
        {
            "authors": [
                "Wang Chen",
                "Hou Pong Chan",
                "Piji Li",
                "Irwin King."
            ],
            "title": "Exclusive hierarchical decoding for deep keyphrase generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1095\u20131105, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Wang Chen",
                "Yifan Gao",
                "Jiani Zhang",
                "Irwin King",
                "Michael R. Lyu."
            ],
            "title": "Title-guided encoding for keyphrase generation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6268\u20136275.",
            "year": 2019
        },
        {
            "authors": [
                "Md Faisal Mahbub Chowdhury",
                "Gaetano Rossiello",
                "Michael Glass",
                "Nandana Mihindukulasooriya",
                "Alfio Gliozzo"
            ],
            "title": "Applying a generic sequence-tosequence model for simple and effective keyphrase generation",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Clark",
                "Urvashi Khandelwal",
                "Omer Levy",
                "Christopher D. Manning."
            ],
            "title": "What does BERT look at? an analysis of BERT\u2019s attention",
            "venue": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
            "year": 2019
        },
        {
            "authors": [
                "Kushal S. Dave",
                "Vasudeva Varma."
            ],
            "title": "Pattern based keyword extraction for contextual advertising",
            "venue": "Proceedings of the 19th ACM International Conference on Information and Knowledge Management, CIKM \u201910, page 1885\u20131888, New York, NY, USA.",
            "year": 2010
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898, Melbourne, Australia. Association",
            "year": 2018
        },
        {
            "authors": [
                "Yifan Gao",
                "Qingyu Yin",
                "Zheng Li",
                "Rui Meng",
                "Tong Zhao",
                "Bing Yin",
                "Irwin King",
                "Michael Lyu."
            ],
            "title": "Retrieval-augmented multilingual keyphrase generation with retriever-generator iterative training",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Krishna Garg",
                "Jishnu Ray Chowdhury",
                "Cornelia Caragea"
            ],
            "title": "Data augmentation for low-resource keyphrase generation",
            "year": 2023
        },
        {
            "authors": [
                "Krishna Garg",
                "Jishnu Ray Chowdhury",
                "Cornelia Caragea."
            ],
            "title": "Keyphrase generation beyond the boundaries of title and abstract",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5809\u20135821, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Ana Marasovi\u0107",
                "Swabha Swayamdipta",
                "Kyle Lo",
                "Iz Beltagy",
                "Doug Downey",
                "Noah A. Smith."
            ],
            "title": "Don\u2019t stop pretraining: Adapt language models to domains and tasks",
            "venue": "Proceedings of the 58th Annual Meeting of the",
            "year": 2020
        },
        {
            "authors": [
                "Khaled Hammouda",
                "Diego Matute",
                "Mohamed S. Kamel."
            ],
            "title": "Corephrase: Keyphrase extraction for document clustering",
            "venue": "International workshop on machine learning and data mining in pattern recognition, pages 265\u2013274.",
            "year": 2005
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Anette Hulth."
            ],
            "title": "Improved automatic keyword extraction given more linguistic knowledge",
            "venue": "Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, EMNLP \u201903, page 216\u2013223, USA. Association for Computational",
            "year": 2003
        },
        {
            "authors": [
                "Su Nam Kim",
                "Olena Medelyan",
                "Min-Yen Kan",
                "Timothy Baldwin."
            ],
            "title": "SemEval-2010 task 5 : Automatic keyphrase extraction from scientific articles",
            "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation, pages 21\u201326, Uppsala, Sweden.",
            "year": 2010
        },
        {
            "authors": [
                "Youngsam Kim",
                "Munhyong Kim",
                "Andrew Cattle",
                "Julia Otmakhova",
                "Suzi Park",
                "Hyopil Shin."
            ],
            "title": "Applying graph-based keyword extraction to document retrieval",
            "venue": "Proceedings of the Sixth International Joint Conference on Natural Language Processing,",
            "year": 2013
        },
        {
            "authors": [
                "Aobo Kong",
                "Shiwan Zhao",
                "Hao Chen",
                "Qicheng Li",
                "Yong Qin",
                "Ruiqi Sun",
                "Xiaoyan Bai"
            ],
            "title": "Promptrank: Unsupervised keyphrase extraction using prompt",
            "year": 2023
        },
        {
            "authors": [
                "Mikalai Krapivin",
                "Aliaksandr Autaeu",
                "Maurizio Marchese."
            ],
            "title": "Large dataset for keyphrases extraction",
            "venue": "Technical report, University of Trento.",
            "year": 2009
        },
        {
            "authors": [
                "Mayank Kulkarni",
                "Debanjan Mahata",
                "Ravneet Arora",
                "Rajarshi Bhowmik."
            ],
            "title": "Learning rich representation of keyphrases from text",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 891\u2013906, Seattle, United States. Associ-",
            "year": 2022
        },
        {
            "authors": [
                "Alexandre Lacoste",
                "Alexandra Luccioni",
                "Victor Schmidt",
                "Thomas Dandres"
            ],
            "title": "Quantifying the carbon emissions of machine learning",
            "year": 2019
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang."
            ],
            "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
            "venue": "CoRR, abs/1901.08746.",
            "year": 2019
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Marina Litvak",
                "Mark Last."
            ],
            "title": "Graph-based keyword extraction for single-document summarization",
            "venue": "Coling 2008: Proceedings of the workshop Multisource Multilingual Information Extraction and Summarization, pages 17\u201324, Manchester, UK. Coling",
            "year": 2008
        },
        {
            "authors": [
                "Kyle Lo",
                "Lucy Lu Wang",
                "Mark Neumann",
                "Rodney Kinney",
                "Daniel Weld."
            ],
            "title": "S2ORC: The semantic scholar open research corpus",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969\u20134983, Online. Asso-",
            "year": 2020
        },
        {
            "authors": [
                "Yichao Luo",
                "Yige Xu",
                "Jiacheng Ye",
                "Xipeng Qiu",
                "Qi Zhang."
            ],
            "title": "Keyphrase generation with finegrained evaluation-guided reinforcement learning",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 497\u2013507, Punta Cana,",
            "year": 2021
        },
        {
            "authors": [
                "Rui Meng",
                "Xingdi Yuan",
                "Tong Wang",
                "Sanqiang Zhao",
                "Adam Trischler",
                "Daqing He."
            ],
            "title": "An empirical study on neural keyphrase generation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Rui Meng",
                "Sanqiang Zhao",
                "Shuguang Han",
                "Daqing He",
                "Peter Brusilovsky",
                "Yu Chi."
            ],
            "title": "Deep keyphrase generation",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 582\u2013592, Vancouver,",
            "year": 2017
        },
        {
            "authors": [
                "Rada Mihalcea",
                "Paul Tarau."
            ],
            "title": "TextRank: Bringing order into text",
            "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404\u2013411, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Hannaneh Hajishirzi."
            ],
            "title": "Cross-task generalization via natural language crowdsourcing instructions",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Thuy Dung Nguyen",
                "Min-Yen Kan."
            ],
            "title": "Keyphrase extraction in scientific publications",
            "venue": "Asian Digital Libraries. Looking Back 10 Years and Forging New Frontiers, pages 317\u2013326, Berlin, Heidelberg. Springer Berlin Heidelberg.",
            "year": 2007
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Martin F. Porter."
            ],
            "title": "An algorithm for suffix stripping",
            "venue": "Program, 40:211\u2013218.",
            "year": 1980
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Jishnu Ray Chowdhury",
                "Seo Yeon Park",
                "Tuhin Kundu",
                "Cornelia Caragea."
            ],
            "title": "KPDROP: Improving absent keyphrase generation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4853\u20134870, Abu Dhabi, United Arab Emirates.",
            "year": 2022
        },
        {
            "authors": [
                "Abigail See",
                "Peter J. Liu",
                "Christopher D. Manning."
            ],
            "title": "Get to the point: Summarization with pointergenerator networks",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Avinash Swaminathan",
                "Haimin Zhang",
                "Debanjan Mahata",
                "Rakesh Gosangi",
                "Rajiv Ratn Shah",
                "Amanda Stent."
            ],
            "title": "A preliminary exploration of GANs for keyphrase generation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B Hashimoto."
            ],
            "title": "Alpaca: A strong, replicable instruction-following model",
            "venue": "Stanford Center for Research on Foundation Models.",
            "year": 2023
        },
        {
            "authors": [
                "Peter D Turney."
            ],
            "title": "Learning algorithms for keyphrase extraction",
            "venue": "Information retrieval, 2:303\u2013 336.",
            "year": 2000
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Jesse Vig",
                "Yonatan Belinkov."
            ],
            "title": "Analyzing the structure of attention in a transformer language model",
            "venue": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 63\u201376, Florence, Italy. As-",
            "year": 2019
        },
        {
            "authors": [
                "Ashwin Vijayakumar",
                "Michael Cogswell",
                "Ramprasaath Selvaraju",
                "Qing Sun",
                "Stefan Lee",
                "David Crandall",
                "Dhruv Batra."
            ],
            "title": "Diverse beam search for improved description of complex scenes",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Di Wu",
                "Wasi Ahmad",
                "Sunipa Dev",
                "Kai-Wei Chang."
            ],
            "title": "Representation learning for resourceconstrained keyphrase generation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 700\u2013716, Abu Dhabi, United Arab Emi-",
            "year": 2022
        },
        {
            "authors": [
                "Di Wu",
                "Da Yin",
                "Kai-Wei Chang."
            ],
            "title": "Kpeval: Towards fine-grained semantic-based evaluation of keyphrase extraction and generation systems",
            "venue": "arXiv preprint arXiv:2303.15422.",
            "year": 2023
        },
        {
            "authors": [
                "Huanqin Wu",
                "Wei Liu",
                "Lei Li",
                "Dan Nie",
                "Tao Chen",
                "Feng Zhang",
                "Di Wang."
            ],
            "title": "UniKeyphrase: A unified extraction and generation framework for keyphrase prediction",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Huanqin Wu",
                "Baijiaxin Ma",
                "Wei Liu",
                "Tao Chen",
                "Dan Nie."
            ],
            "title": "Fast and constrained absent keyphrase generation by prompt-based learning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 11495\u201311503.",
            "year": 2022
        },
        {
            "authors": [
                "Xiaoyuan Wu",
                "Alvaro Bolivar."
            ],
            "title": "Keyword extraction for contextual advertisement",
            "venue": "Proceedings of the 17th International Conference on World Wide Web, WWW \u201908, page 1195\u20131196, New York, NY, USA. Association for Computing Machinery.",
            "year": 2008
        },
        {
            "authors": [
                "Hai Ye",
                "Lu Wang."
            ],
            "title": "Semi-supervised learning for neural keyphrase generation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4142\u20134153, Brussels, Belgium. Association for Computational",
            "year": 2018
        },
        {
            "authors": [
                "Jiacheng Ye",
                "Tao Gui",
                "Yichao Luo",
                "Yige Xu",
                "Qi Zhang."
            ],
            "title": "One2Set: Generating diverse keyphrases as a set",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Yongzheng Zhang",
                "Nur Zincir-Heywood",
                "Evangelos Milios."
            ],
            "title": "World wide web site summarization",
            "venue": "Web Intelli. and Agent Sys., 2(1):39\u201353.",
            "year": 2004
        },
        {
            "authors": [
                "Guangzhen Zhao",
                "Guoshun Yin",
                "Peng Yang",
                "Yu Yao."
            ],
            "title": "Keyphrase generation via soft and hard semantic corrections",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7757\u20137768, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Jing Zhao",
                "Yuxiang Zhang."
            ],
            "title": "Incorporating linguistic constraints into keyphrase generation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5224\u2013 5233, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Beltagy"
            ],
            "title": "2019) suggest that using a domain-specific vocabulary is crucial to downstream in-domain fine-tuning performance",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Keyphrases encapsulate the core information of a document. Due to their high information density, they have been found valuable in areas such as information retrieval (Wu and Bolivar, 2008; Dave and Varma, 2010; Kim et al., 2013; Boudin et al., 2020), document clustering (Hammouda et al., 2005), summarization (Zhang et al., 2004), and text classification (Berend, 2011). A keyphrase is termed a present keyphrase if it is explicitly found within the document and an absent keyphrase otherwise. The task of identifying present keyphrases is defined as keyphrase extraction (KPE), whereas\nkeyphrase generation (KPG) involves predicting both types of keyphrases.\nRecently, pre-trained language models (PLMs) have been widely incorporated in KPG (Chowdhury et al., 2022; Zhao et al., 2022) via sequence-tosequence (seq2seq) generation, with promising performance on zero-shot (Kulkarni et al., 2022), multilingual (Gao et al., 2022), and low-resource (Wu et al., 2022a) KPG. However, existing literature typically focuses on a specific subset of important components in this pipeline, such as data construction and loss design, while making arbitrary choices for the others (Zhao et al., 2022; Ray Chowdhury et al., 2022; Wu et al., 2022a; Garg et al., 2022). As a result, KPG systems are often compared under different assumptions and the effect of the arbitrary design choices remains unclear. To bridge this gap, this paper focuses on two crucial questions that have not been systematically explored:\n1. Which PLM leads to the best KPG performance when fine-tuned?\n2. What is the best decoding strategy? In practice, sub-optimal choices for these factors could lead to optimizing an unnecessarily large model or sub-optimal results decoded from a strong KPG model. To answer these two questions, we conduct in-depth analyses on KPG with (1) PLMs of diverse size and pre-training strategies and (2) a diverse set of decoding strategies.\nTo begin with, we posit that seq2seq PLMs are inherently suitable to KPG (\u00a73). By drawing the correlations with a strong graph-based KPE algorithm, we show that these PLMs implicitly compute phrase centrality (Boudin, 2013) in their decoder attention patterns. This knowledge is also directly translatable to a strong ranking function for KPE. On the other hand, encoder-only models fail to carry such centrality information.\nNext, we search for the best seq2seq PLM for KPG fine-tuning (\u00a74). While common strategies for other NLP tasks might advocate for (1) scaling\nup the model size, (2) in-domain pre-training, or (3) task adaptation, do these approaches hold the same merit for KPG? Our findings reveal that a singular emphasis on scaling or task adaptation does not ensure efficient performance improvement. In contrast, in-domain pre-training consistently bolsters performance across both keyphrase types and can benefit from task adaptation. A robustness analysis reveals that a proper model choice and data-oriented training approaches are complementary: without the latter, stronger PLMs are more vulnerable to perturbed input, with over 14% recall drop under named variation substitutions and over 5% recall drop under input paraphrasing.\nDecoding strategy is also an essential component in PLM-based KPG, but much under-explored by current literature. In \u00a75, we thoroughly compare six decoding strategies including greedy search, beam search, and sampling-based methods. Results suggest that when only generating a single sequence consisting of concatenated keyphrases, greedy search achieves a strong F1 score. However, aggregating the predictions from multiple sampled sequences outperforms greedy search due to a much higher recall.\nBased on these findings, we introduce DESEL, a likelihood-based selection strategy that selects from sampled phrases to augment the greedy search predictions. DESEL utilizes the probability of phrases from greedy search\u2019s predictions as the baseline to filter out noisy predictions from a set of sampled keyphrase candidates. Experiments on five KPG datasets show that DESEL consistently improves greedy decoding by 7.9% F1@M for present keyphrases, 25% F1@M for absent keyphrases, and 4.7% Semantic F1 for all keyphrases, achieving state-of-the-art KPG performance, underscoring the importance of carefully examining the design choice of KPG.\nTo summarize, our primary contributions are:\n1. An in-depth exploration of the intrinsic suitability of seq2seq PLMs for KPG. 2. A comprehensive examination of effective strategies for model enhancement in KPG, spotlighting the merits of specific combinations and their implications for robustness. 3. We establish the trade-off between accuracy and concept coverage for different decoding algorithms. Then, we introduce a probabilitybased decode-select mechanism DESEL that consistently improves over greedy search.\n4. Our research illuminates the profound impact of under-emphasized factors on KPG performance. To facilitate future research on KPG, we release our code and models at https: //github.com/uclanlp/DeepKPG."
        },
        {
            "heading": "2 Preliminaries",
            "text": ""
        },
        {
            "heading": "2.1 Keyphrase Generation",
            "text": "Problem Definition We represent an example for KPG as a tuple (X ,Y), corresponding to the input document X = (x1, x2, ..., xd) and the set of human-written reference keyphrases Y = {y1, y2, ..., yn}. Following Meng et al. (2017), yi is classified as a present keyphrase if it is a substring of X or an absent keyphrase otherwise. The KPG task requires predicting Y in any order, and the KPE task only requires predicting present keyphrases (Turney, 2000).\nEvaluation We adopt lexical-based and semanticbased evaluation to evaluate a model\u2019s predictions P = {p1, p2, ..., pm} against Y . For lexical evaluation, we follow Chan et al. (2019) and use the P@M , R@M , and F1@M scores. P and Y are stemmed with the Porter Stemmer (Porter, 1980) and the duplicates are removed before the score calculation. For semantic evaluation, we follow Wu et al. (2023) and report SemP , SemR, and SemF1. Note that lexical metrics are separately calculated for present and absent keyphrases while the semantic metrics are calculated with all the phrases. We repeat all the experiments with three random seeds and report the averaged performance.\nBenchmark Meng et al. (2017) introduce KP20k, which contains 500k Computer Science papers. Following their work, we train on KP20k and evaluate on the title and abstracts from the KP20k test set as well as four out-of-distribution testing datasets: Inspec (Hulth, 2003), Krapivin (Krapivin et al., 2009), NUS (Nguyen and Kan, 2007), and SemEval (Kim et al., 2010). Table 5 summarizes the statistics of all testing datasets.\nBaselines We consider two strong supervised encoder-decoder models from Ye et al. (2021):\n1. CopyTrans: A Transformer (Vaswani et al., 2017) with copy mechanism (See et al., 2017). 2. The SetTrans model, which performs orderagnostic KPG. The model uses control codes trained via a k-step target assignment algorithm to generate keyphrases in parallel.\nAs the goal of this work is thoroughly studying PLM-based methods, we only provide the results of the strongest baselines as a point of reference. We also include other baselines in appendix section G. In our analysis, we also use MultipartiteRank (MPRank, Boudin (2018)), a performant graphbased unsupervised KPE algorithm. More details about MPRank are discussed in \u00a73.1."
        },
        {
            "heading": "2.2 Sequence-to-Sequence PLMs",
            "text": "In this work, we focus on fine-tuning Transformerbased sequence-to-sequence PLMs BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) for KPG with the \"One2Seq\" formulation. Concretely, following Ye and Wang (2018) and Yuan et al. (2020), we use a separator token ; to join all the target keyphrases as the target sequence Y = (y1 ; ... ; yn). The models are trained with the crossentropy loss for generating Y based on X . At test time, greedy decoding is used, followed by a postprocessing stage to segment the output sequence into individual phrases. We provide implementation details and hyperparameters in appendix D."
        },
        {
            "heading": "3 Do PLMs inherently carry significant knowledge of keyphrases?",
            "text": "Existing studies have justified their use of seq2seq PLMs by drawing the close relationship between the pre-training tasks of BART (denoising language modeling) or T5 (unified text-to-text transfer) and the formulation of KPG (Gao et al., 2022; Zhao et al., 2022; Wu et al., 2022a) or KPE (Kong et al., 2023). However, there is a lack of an in-depth understanding of why seq2seq PLMs should be chosen for keyphrase-related tasks. In this section, we reason based on phrase centrality (Litvak and Last, 2008; Boudin, 2013) and show that PLMs with autoregressive decoders, including seq2seq PLMs, carry attention heads that approximately function as centrality assigners and naturally as potent keyphrase rankers."
        },
        {
            "heading": "3.1 Centrality of Phrases",
            "text": "The concept of phrase centrality originated from graph-based KPE, where keyphrase candidates are represented as nodes. Various graph centrality measures are used to determine a phrase\u2019s importance in the document. We use MPRank in our analysis, which encodes closeness-based and eigenvectorbased centrality (Boudin, 2013). MPRank first uses rules to obtain C noun phrase candidates and then\nperforms lexical clustering to group the candidates into topic clusters. Next, each candidate is represented as a graph node and connected with the candidates from other topic clusters. TextRank (Mihalcea and Tarau, 2004) is used to obtain a centrality score ci for each of the nodes ni. We refer the readers to Boudin (2018) for further details."
        },
        {
            "heading": "3.2 Attention intensities in BART and T5 decoders encode phrase centrality",
            "text": "Using MPRank as a lens, we first investigate the extent to which PLMs implicitly represent centrality information. We use the paper titles and abstracts from the KP20k test set as the probing set. Each probing instance is fed into a PLM and the attention weights from the self-attention layers are collected. For the hth attention head at layer l, we denote the attention from token i to token j as \u03b1l,hi\u2192j . For the jth token in the noun phrase candidate ni, the global attention weight on it is\nal,hij = \u2211\nk=1,...,L\n\u03b1l,hk\u2192j , (1)\nwhere L is the length of the text after tokenization. Then, the attention weight of ni is calculated as\nal,hi = |ni| \u2211 j al,hij , (2)\nwhere |ni| denotes the number of tokens in ni. We study four families of models: BART, T5, BERT, and GPT-2 (Radford et al., 2019). For\nBART and T5, we use their decoder attentions. We correlate al,hi with ci using Spearman correlation \u03c1 and Kendall\u2019s Tau \u03c4 and present the best correlation for each model in Table 1. Surprisingly, BART and T5 decoders contain attention heads that encode phrase centrality similarly as MPRank. The head with the best correlation generally appears in the lower layers, indicating that centrality understanding may be more related to low-level features. Also, the upper bound of correlation strength grows with model size for T5 while does not grow for BART. Beyond centrality assigners, these attention heads are also potent keyphrase extractors: simply ranking the noun phrase candidates by al,hi achieves similar F1@5 for present keyphrases or SemF1@5 score as MPRank (appendix B).\nEvaluating other types of PLMs, we find that BERT\u2019s attention heads only show weak centrality knowledge, with only 0.246 best Kendall Tau with MPRank. On the other hand, GPT-2 exhibits a similar pattern to the decoders from seq2seq PLMs, indicating that the observed pattern is strongly associated with autoregressive decoders. As centrality is generally correlated with global importance, our result aligns with the observations that masked language modeling tends to exploit local dependency while causal language modeling can learn long-range dependencies (Clark et al., 2019; Vig and Belinkov, 2019).\nIn summary, through attention-based analyses, we reveal novel insights into the underlying keyphrase knowledge of PLMs with autoregressive decoders. Such knowledge can be employed explicitly (via ranking) or implicitly (via fine-tuning and prompting) to facilitate KPG. In the rest of the paper, we focus on rethinking two basic designs for KPG with seq2seq PLMs."
        },
        {
            "heading": "4 Influence of PLM Choice for KPG",
            "text": "Three crucial design options exist for using seq2seq PLMs for KPG: the choice of PLM to fine-tune, the fine-tuning data and objective, and the decoding strategy. Previous work focuses on fine-tuning objective and data construction (Meng et al., 2021; Ray Chowdhury et al., 2022; Garg et al., 2023) while often making the other two choices in an ad hoc way, making it difficult to compare among approaches. This section dives into the first question by evaluating three \"conventional wisdoms\":\n1. Using PLMs with more parameters (\u00a74.1). 2. Using in-domain PLMs (\u00a74.2).\n3. Using task-adapted PLMs (\u00a74.3)."
        },
        {
            "heading": "4.1 The scaling law for keyphrase generation",
            "text": "Although the effect of model sizes has been explored for a range of tasks, it is poorly understood in the KPG literature, where most recent works employ a single PLM with 100M to 500M parameters (Kulkarni et al., 2022; Wu et al., 2022b; Zhao et al., 2022). To establish a common ground, we measure the performance of fine-tuning BART-base/large (purple line) and T5-small/base/large/3B (green line) and report the results on KP20k in Figure 1.\nSurprisingly, fine-tuning BART or T5 is extremely parameter-inefficient compared to taskspecific architectures trained from scratch1. For instance, although T5\u2019s performance consistently increases with the model size, around 8x more parameters are required to achieve the same Present F1@M on KP20k as SetTrans and 30x more parameters are required to have a better SemF1. Closer inspection shows that SetTrans excels in recall via its parallel control codes and the set loss. In comparison, limited by the learning formulation and decoding strategy, fine-tuned seq2seq PLMs fall behind in their recall of important keyphrases. In \u00a75, we will show that this problem can be alleviated with a simple decode-then-select strategy.\nBART vs. T5 BART and T5 display similar scaling for F1@M and SemF1. However, compared to T5, BART\u2019s recall scores increase more readily than the precision scores. At the same number of parameters, BART also performs better on absent keyphrases. One possible reason is that BART\u2019s text infilling objective is more advantageous for learning the knowledge for constructing spans absent from text (Wu et al., 2022a).\nWhich score is more sensitive to scaling? Compared to recall, precision is more sensitive to model size. For example, T5-small achieves 98% SemR compared to the 50x larger T5-3B. In addition, absent keyphrases scores are more sensitive. Overall, this suggests that small models are able to extract relevant keyphrases, but learn to selectively omit unimportant keyphrases and create more absent keyphrases as the model size grows. Indeed, the average number of predicted keyphrases decreases from T5-small (6.75), T5-base (5.74), and T5-large (5.66), to T5-3B (5.48), while the number of absent keyphrases increases from T5-small (0.91),\n1We note that this claim is orthogonal to the observations that PLMs are data-efficient (Wu et al., 2022a).\nT5-base (0.99), T5-large (1.01), to T5-3B (1.05)."
        },
        {
            "heading": "4.2 Domain knowledge is crucial to accurate keyphrase generation",
            "text": "In-domain pre-training has been shown effective in a wide range of tasks requiring extensive domain knowledge (Beltagy et al., 2019; Lee et al., 2019). As keyphrases often contain domain-specific terminologies, we hypothesize that the domain of a PLM greatly affects its keyphrase generation ability. To test this hypothesis, we pre-train in-domain BART models SciBART-base and SciBART-large from scratch using the paper titles and abstracts from the S2ORC dataset (Lo et al., 2020). The processed dataset contains 171.7M documents or 15.4B to-\nkens in total. The models are pre-trained on text infilling for 250k steps with batch size 2048, learning rate 3e-4, 10k warm-up steps, and polynomial learning rate decay. We present data processing and model training details in appendix C.\nThe results of fine-tuning SciBART are presented with \"+ID\" (for \"In-Domain\") in Figure 1. As expected, SciBART significantly improves over BART for all three F1 metrics, outperforming the much larger T5-3B. Notably, SciBART also has better parameter efficiency compared to general domain models: scaling from SciBART-base to SciBART-large provides a much larger growth in SemF1 compared to scaling up BART and T5."
        },
        {
            "heading": "4.3 Task-adaptive pre-training is more effective with in-domain models",
            "text": "Task-adaptive pre-training (TAPT) is another common technique for adding task-specific supervision signals (Gururangan et al., 2020). In this section, we analyze the effect on KPG performance of adding two types of TAPT stages to seq2seq PLMs: keyphrase generation or instruction following.\nKeyphrase Pre-training We directly use KeyBART (Kulkarni et al., 2022) (denoted as \"+TAPT\" in Figure 1), which is trained using the OAGKX dataset (\u00c7ano and Bojar, 2020) on KPG with keyphrases corrupted from the input. To investigate the effects of TAPT on in-domain PLMs, we also fine-tune SciBART on OAGKX with batch size 256, learning rate 3e-5, and 250k steps. We denote this model as \"+ID+TAPT\" in Figure 1.\nInstruction Pre-training Recently, instruction tuning has been introduced to improve the generalization ability of PLMs (Mishra et al., 2022; Ouyang et al., 2022). As KPG is relevant to classic NLP tasks such as information extraction and summarization, we hypothesize that training with instruction data also serves as TAPT for KPG2. To confirm, we benchmark FLAN-T5 (Chung et al., 2022), a family of T5 models fine-tuned on instruction following datasets (yellow line in Figure 1).\nTAPT struggles to improve absent KPG but is more effective with in-domain models. Figure 1 suggests that both TAPT strategies lead to a similar amount of improvement in the present keyphrase F1@M and SemF1. Surprisingly, the absolute gain is small and TAPT hardly improves absent keyphrase performance. For KeyBART, although its pre-training data (OAGKX) has a similar percentage of absent keyphrases as KP20K (32% vs. 37%), its objective (recovering present keyphrases from corrupted input) might still be different from absent keyphrase generation. For FLAN-T5, we find that the KPG-related tasks in its pre-training tasks often contain very short input text, representing a significant distribution mismatch with KP20k. However, when applied on the in-domain SciBART, TAPT can greatly improve the performance on KP20k. Combined with \u00a74.2, we conclude that in-domain pre-training is more important for KPG and TAPT serves a complementary secondary role.\n2In fact, some variants of the keyphrase extraction task are included in popular instruction datasets such as NIv2 (Wang et al., 2022) and Alpaca (Taori et al., 2023)."
        },
        {
            "heading": "4.4 Analysis: are strong KPG models sensitive to input perturbations?",
            "text": "As in-domain and task-adapted PLMs already greatly benefit KPG, are data augmentation techniques no longer necessary? In this section, we reveal that these designs increase the model\u2019s sensitivity to input perturbations, and data augmentation is still desired for better generalization."
        },
        {
            "heading": "4.4.1 Method",
            "text": "We design two input perturbations on KP20k to check the behaviors of BART-based KPG models.\nName variation substitution We construct 8905 perturbed inputs by replacing present keyphrases with their name variations linked by Chan et al. (2019). Ideally, a robust KPG model would have a similar recall for the original phrases and the name variations as they appear in the same context.\nParaphrasing We leverage gpt-3.5-turbo to paraphrase 1000 documents into a scientific writing style. A good KPG model is expected to retain similar recall for phrases that present both before and after paraphrasing since the inputs describe the same ideas. The detailed prompt and several examples are presented in appendix E."
        },
        {
            "heading": "4.4.2 Results",
            "text": "Table 2 presents the perturbation results. We observe that models often fail to predict name variations when they appear in place of the original synonyms, while successfully maintaining their predictions given paraphrased inputs. This indicates that the models may overfit to the high-frequency keyphrases in the training set and input augmentation methods such as Garg et al. (2023) are necessary to correct this pattern.\nIn addition, domain-specific or task-adapted models exhibit a larger performance drop compared to BART-large, suggesting a trade-off between domain/task specificity and generalization. Pre-trained on large-scale keyphrase data, KeyBART may rely more on syntax and position information in the data and thus be less sensitive to synonym change. On the other hand, pre-trained on a large-scale scientific corpus, SciBART is more robust than KeyBART on different scientific writing styles beyond the ones available in KP20k."
        },
        {
            "heading": "4.5 Discussion",
            "text": "We summarize the main conclusions derived from the empirical results presented in this section:\n\u2022 Naively scaling up BART and T5 is parameterinefficient on KP20k compared to SetTrans.\n\u2022 Domain knowledge is crucial for KPG performance and improves parameter efficiency.\n\u2022 Task-adaptive training with keyphrase or instruction tuning data only significantly improves KPG with in-domain models.\n\u2022 In-domain pre-training and TAPT harm generalization in different ways and data augmentation during fine-tuning is desired."
        },
        {
            "heading": "5 Decoding Strategy for KPG",
            "text": "While it is well-known that decoding strategies can strongly affect text generation quality (Fan et al., 2018; Holtzman et al., 2020), surprisingly there has been little study about decoding PLM-based KPG models. Previous studies often directly use greedy search and variants of beam search (Gao et al., 2022; Zhao et al., 2022; Wu et al., 2022a), limiting the understanding of PLMs fine-tuned for KPG. To bridge this knowledge gap, we first carefully evaluate six decoding strategies on the strongest PLM-based KPG model. We then propose a simple yet effective decode-select strategy to mitigate the observed deficiencies of greedy search."
        },
        {
            "heading": "5.1 Multi-sequence decoding: the trade-off between coverage and quality",
            "text": "We focus on decoding the SciBART-large+TAPT model fine-tuned on KP20k, under the budget varying from 1 to 20 samples. The following six decoding algorithms are compared. For each algorithm, their hyperparameters are chosen based on the KP20k validation set.\n1. Greedy search. 2. Beam search. We set the beam size to the\nnumber of desired samples. 3. Diverse beam search (Vijayakumar et al.,\n2018). We set the number of groups to the number of desired samples and the weight of the dissimilarity term to \u03bbg = 0.1. 4. Vanilla sampling. We further apply a temperature scaling \u03c4 = 0.7. 5. Top-k sampling (Fan et al., 2018). We use temperature \u03c4 = 0.7 and k = 2 as we find a large k harms the generation quality. 6. Nucleus sampling (Holtzman et al., 2020). We set p = 0.95 and temperature \u03c4 = 0.5.\nFigure 2 presents the semantic-based evaluation results as a function of sample size. In the single sample setting, greedy search achieves a strong SemF1, only slightly outperformed by diverse beam search. For other methods, we observe a clear trade-off between their information coverage (SemR) and the noise in the final output (SemP ) as the number of samples grows. Nevertheless, all these methods are able to outperform greedy search at a certain sample size, indicating that single-sequence decoding is sub-optimal."
        },
        {
            "heading": "5.2 A simple decode-select strategy boosts the performance of greedy decoding",
            "text": "Greedy search captures the correlations in humanwritten labels but suffers from local decisions and path dependency: high-quality keyphrases can be missed with improbable first tokens. However, naively outputting the union of multiple sampled sequences brings excessive noise. To achieve the balance between the two, we introduce DESEL, a simple and effective three-stage decoding strategy:\n1. Decode one sequence G via greedy search. 2. Sample n sequences {S1, ..., Sn} to collect a\nset of candidate keyphrases S. 3. Select high quality phrases {s1, ..., sm} \u2282 S\nand output the sequence (G ; s1 ; ... ; sm). For step 3, we estimate Pr(si|X ) for every phrase si in the n samples and Pr(gj |X ) for every phrase gj \u2208 G. Then, we use G as a baseline to select at most m most probable si that satisfies\nPr(si|X ) \u2265 \u03b1 |G| \u2211 gj\u2208G Pr(gj |X ), (3)\nwhere \u03b1 is a hyperparameter controlling the tradeoff between precision and recall. The probability estimation is obtained with either the original\nmodel or a newly trained \"one2one\" model3 that learns to generate a single keyphrase based on X . We use nucleus sampling with p = 0.95 and \u03c4 = 0.5 for step 2, and set m = 10, n = 10, and \u03b1 = 0.78.\nTable 3 presents the test results of important models in this paper. DESEL consistently improves the performance over the base model by a large margin. In Table 4, we compare against other selection strategies including random selection, input overlap using a sentence transformer model, and FreqFS proposed in Zhao et al. (2022). DESEL is the only method that consistently outperforms both greedy search and nucleus sampling.\nDiscussion Compared to the single-sequence decoding baselines, DESEL wins by bringing in the diversity. Compared to the baseline ranking methods, DESEL wins by capturing the correlations between labels (encoded in the greedy search outputs) and using the likelihood-based criteria to filter out\n3Starting from KeyBART, the one2one model can be efficiently trained. We provide more details in appendix F.\nhigh-quality phrases from the diverse candidates.\nEfficiency DESEL harms the inference latency as it generates multiple sequences. To improve the efficiency, one optimization is reusing the encoder\u2019s outputs for all the decoding and scoring operations. We implemented this strategy and benchmarked it with the BART (base) model. DESEL with n = 10 (1 greedy search and 10 sampling sequences decoded) takes 3.8x time compared to greedy decoding."
        },
        {
            "heading": "6 Related Work",
            "text": "Keyphrase Generation Meng et al. (2017) propose the task of Deep Keyphrase Generation and a strong baseline model CopyRNN. Later works improve the architecture by adding correlation constraints (Chen et al., 2018) and linguistic constraints (Zhao and Zhang, 2019), exploiting learning signals from titles (Ye and Wang, 2018; Chen et al., 2019b), and hierarchical modeling the phrases and words (Chen et al., 2020). Ye and Wang (2018) reformulate the problem as generating a sequence of keyphrases, while Ye et al. (2021) further uses a set generation formulation to remove the influence of difference target phrase ordering. Other works include incorporating reinforcement learning (Chan et al., 2019; Luo et al., 2021), GANs (Swaminathan et al., 2020), and unifying KPE with KPG (Chen et al., 2019a; Ahmad et al., 2021). Meng et al. (2021) conduct an empirical study on architecture, generalizability, phrase order, and decoding strategies, with the main focus on models trained from scratch instead of PLMs.\nPLMs for KPG More recently, Wu et al. (2021), Chowdhury et al. (2022), Wu et al. (2022a), Gao et al. (2022), and Wu et al. (2022b) consider finetuning prefix-LMs or seq2seq PLMs for KPG. Kulkarni et al. (2022) use KPG as a pre-training task to learn strong BART-based representations. Zhao et al. (2022) adopt optimal transport for loss design and propose frequency-based filtering for decoding to improve BART-based KPG."
        },
        {
            "heading": "7 Conclusion",
            "text": "This paper systematically investigated model selection and decoding for building KPG models with seq2seq PLMs. Our analyses suggested much more nuanced patterns beyond the \"conventional wisdom\" assumed by the majority of current literature. Our novel decoding strategy, DESEL, significantly improved the performance of greedy search across multiple datasets. More broadly, this study underscores the distinct nature of the KPG task. One should not blindly transpose conclusions or assumptions from other text generation tasks. Instead, they warrant careful re-evaluation and empirical validation. Our work also opens up exciting directions for future work, with deep groundings in keyphrase literature. For instance, making KPG models more robust, interpreting a KPG model, and designing better decoding algorithms for KPG.\nLimitations\nWhile our study sheds light on important aspects of keyphrase generation (KPG) models, several limitations present opportunities for future research.\nFirst, our analysis focuses on model selection and decoding and thus uses default cross entropy loss and original training set without data augmentations. Investigating how the discussed design choices with more recent data augmentation (Ray Chowdhury et al., 2022; Garg et al., 2022) or training strategies (Zhao et al., 2022) is an important future study. In addition, the best approach to combine the conclusions reached in this paper on long input KPG (Garg et al., 2022) or KPG models trained with reinforcement learning (Chan et al., 2019; Luo et al., 2021) worth future study.\nSecond, while in-domain pre-training combined with task adaptation was found to enhance KPG performance, we did not fully investigate the underlying mechanisms leading to these improvements. Further research could explore the interplay between these two aspects and uncover more granular insights into how they improve KPG.\nFinally, although we revealed a compromise between performance optimization and model robustness, we did not delve into designing new methods for improving the robustness of these models against perturbed inputs. Future research could further explore techniques to mitigate this trade-off, developing models that maintain high performance while being resistant to input perturbations.\nEthics Statement\nS2ORC and OAGKX are released under the Creative Commons By 4.0 License. We perform text cleaning and email/URL filtering on S2ORC to remove sensitive information, and we keep OAGKX as-is. We use the keyphrase benchmarking datasets distributed by the original authors. No additional preprocessing is performed before fine-tuning except lower-casing and tokenization. We do not re-distribute any of the datasets used in this work.\nPotential risks of SciBART include accidental leakage of (1) sensitive personal information and (2) inaccurate factual information. For (1), we carefully preprocess the data in the preprocessing stage to remove personal information, including emails and URLs. However, we had difficulties desensitizing names and phone numbers in the text because they overlapped with the informative content. For (2), since SciBART is pre-trained on scientific pa-\npers, it may generate scientific-style statements that include inaccurate information. We encourage the potential users of SciBART not to rely fully on its outputs without verifying their correctness.\nPre-training SciBART and fine-tuning the large T5 models are computationally heavy, and we estimate the total CO2 emission to be around 3000 kg using the calculation application provided by Lacoste et al. (2019). We will release the fine-tuned checkpoints and we document the hyperparameters in the appendix section D to help the community reduce the energy spent optimizing PLMs for KPG and other various NLP applications."
        },
        {
            "heading": "Acknowledgments",
            "text": "The research is supported in part by Taboola, NSF CCF-2200274, and an Amazon AWS credit award. We thank the Taboola team for the helpful discussion. We also thank anonymous reviewers, Da Yin, Tanmay Parekh, and other members of the UCLANLP group for their valuable feedback."
        },
        {
            "heading": "A Test Set Statistics",
            "text": "Table 5 summarizes the statistics of all testing datasets we use. We use the version distributed by Meng et al. (2017). In this distribution, SemEval\u2019s documents and keyphrases are already stemmed while the other datasets are not."
        },
        {
            "heading": "B Attention heads as keyphrase extractors",
            "text": "We present the KPE performance of ranking noun phrase candidate using their attention intensities in Table 6. We observe that BART and T5 contain attention heads that function well as keyphrase extractors, some of which even surpass the KPE performance of the strong MPRank algorithm."
        },
        {
            "heading": "C SciBART Pre-training Details",
            "text": "Corpus and Data Preprocessing The S2ORC dataset contains over 100M papers from a variety of disciplines (Figure 3). We train on all the titles and abstracts to increase the coverage of different topics. After removing non-English4 or title-only\n4We use guess_language for language detection.\nentries, we fix wrong Unicode characters, remove emails and URLs, and convert the text to ASCII encoding5. The final dataset contains 171.7M documents or 15.4B tokens in total. We reserve 10k documents for validation and 10k for testing and use the rest as training data.\nVocabulary Beltagy et al. (2019) suggest that using a domain-specific vocabulary is crucial to downstream in-domain fine-tuning performance. Following their observations, we build a cased BPE vocabulary in the scientific domain using the SentencePiece6 library on the cleaned training data. We set the vocabulary size to 30K.\nTraining For the pre-training objective, we only use text infilling as introduced in Lewis et al. (2020). We mask 30% of all tokens in each example, with the spans randomly sampled from a Poisson distribution (\u03bb = 3.5). For 10% of the spans selected to mask, we replace them with a random token instead of the mask token. We set the maximum sequence length to 512. The model is pre-trained for 250k steps with batch size 2048, learning rate 3e-4, 10k warm-up steps, and polynomial learning rate decay. We use the Adam optimizer for pre-training. Using 8 Nvidia A100 GPUs (40G each), the training took eight days for SciBART-base and twelve days for SciBART-large.\nD Implementation details\nD.1 Keyphrase Generation\nFor keyphrase generation with BART and T5, we use Huggingface Transformers and train for 15 epochs with early stopping. We use learning rate 6e-5, polynomial decay, batcsh size 64, and the AdamW optimizer. To fine-tune SciBART-base and SciBART-large, we use the Translation task provided by fairseq7 and train for 10 epochs. We use learning rate 3e-5, polynomial decay, and the AdamW optimizer.\nWe perform a careful hyperparameter search over the learning rate, learning rate schedule, batch size, and warm-up steps. The corresponding search spaces are {1e-5, 3e-5, 6e-5, 1e-4, 3e-4}, {linear,\n5We use clean-text for data cleaning. 6https://github.com/google/\nsentencepiece 7https://github.com/facebookresearch/ fairseq\nMedicine Biology Chemistry Engineering Computer Science Physics\nMaterials Science Mathematics\nPsychology Economics Political Science Business Geology\nSociology Geography\nEnvironmental Science Art\nHistory Philosophy\npolynomial}, {16, 32, 64, 128}, and {500, 1000, 2000, 4000}. The best hyperparameters are presented in Table 7. For DESEL, we tune the hyperparameters on the validation set of KP20k. the search space for \u03b1 is {0.5, 0.73, 0.78, 0.83, 0.88, 0.93, 0.98}.\nThe fine-tuning experiments are run on a local GPU server with RTX 2080 Ti (11G each) and A6000 GPUs (48G each). We use gradient accumulation to achieve the desired batch sizes.\nD.2 Baselines\nFor CopyTrans and SetTrans, we rerun with the original implementations to measure their performance. We use the earliest version of KeyBART available at https://zenodo.org/ record/5784384#.Y0eToNLMJcA. For MPRank, we use the implementation from pke8."
        },
        {
            "heading": "E Prompting GPT-3.5 for paraphrasing",
            "text": "To generate the paraphrased titles and abstracts used in \u00a74.4, we use the following prompt to query gpt-3.5-turbo: Paraphrase the following title and\nabstract for a scientific paper. Your paraphrase should perfectly preserve the information in the original document and should be as formal as the original text. Do not change the spelling or case of the named entities in the original sentence. If the original entity is all lower-cased, do not upper-case the first letter of these names in your paraphrase.\nTitle: {original_title}\n8https://github.com/boudinfl/pke\nAbstract: {original_abstract}\nYour Paraphrase: Title:\nWe observe that the large language model can follow the specified format when generating the paraphrased titles and abstracts. During the postprocessing, we split the titles and the abstract from the response and directly use them for model testing. Figure 4 shows two examples of original and paraphrased text. The paraphrased text has high quality, preserves the scientific writing style, and retains most of the present keyphrases."
        },
        {
            "heading": "F One2one model for DESEL",
            "text": "To better capture Pr(si|X ) and Pr(gj |X ) for DESEL without the interference from other keyphrases co-occuring in the label, we propose to train an \"one2one\" model that learns to generate a single keyphrase given an input document. Starting from the KeyBART model, we fine-tune on the KP20k training set for 0.5 epoch with learning rate 5e-5, batch size 32, and the AdamW optimizer. We use a linear learning rate decay with 1000 warmup steps."
        },
        {
            "heading": "G All model testing results",
            "text": "We present the testing scores as well as their standard deviation in Table 8. In addition to the models discussed in the main text, we also provide the performance of CatSeq (Yuan et al., 2020), CatSeqTG2RF1 (Chan et al., 2019), ExHiRD-h (Chen et al., 2020) for further reference."
        }
    ],
    "title": "Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models",
    "year": 2023
}