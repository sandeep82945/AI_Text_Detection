{
    "abstractText": "Recently, incorporating structure information (e.g. dependency syntactic tree) can enhance the performance of aspect-based sentiment analysis (ABSA). However, this structure information is obtained from off-the-shelf parsers, which is often sub-optimal and cumbersome. Thus, automatically learning adaptive structures is conducive to solving this problem. In this work, we concentrate on structure induction from pre-trained language models (PLMs) and throw the structure induction into a spectrum perspective to explore the impact of scale information in language representation on structure induction ability. Concretely, the main architecture of our model is composed of commonly used PLMs (e.g., RoBERTa, etc.), and a simple yet effective graph structure learning (GSL) module (graph learner + GNNs). Subsequently, we plug in Frequency Filters with different bands after the PLMs to produce filtered language representations and feed them into the GSL module to induce latent structures. We conduct extensive experiments on three public benchmarks for ABSA. The results and further analyses demonstrate that introducing this spectral approach can shorten Aspects-sentiment Distance (AsD) and be beneficial to structure induction. Even based on such a simple framework, the effects on three datasets can reach SOTA (state-of-the-art) or near SOTA performance. Additionally, our exploration also has the potential to be generalized to other tasks or to bring inspiration to other similar domains. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Hao Niu"
        },
        {
            "affiliations": [],
            "name": "Yun Xiong"
        },
        {
            "affiliations": [],
            "name": "Xiaosu Wang"
        },
        {
            "affiliations": [],
            "name": "Wenjing Yu"
        },
        {
            "affiliations": [],
            "name": "Yao Zhang"
        },
        {
            "affiliations": [],
            "name": "Zhonglei Guo"
        }
    ],
    "id": "SP:311fe2380678b7086bfe09cea79d9fbe96e51ec5",
    "references": [
        {
            "authors": [
                "Gianni Brauwers",
                "Flavius Frasincar."
            ],
            "title": "A survey on aspect-based sentiment classification",
            "venue": "ACM Comput. Surv., 55(4):65:1\u201365:37.",
            "year": 2023
        },
        {
            "authors": [
                "Chenhua Chen",
                "Zhiyang Teng",
                "Zhongqing Wang",
                "Yue Zhang."
            ],
            "title": "Discrete opinion tree induction for aspect-based sentiment analysis",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Chenhua Chen",
                "Zhiyang Teng",
                "Yue Zhang."
            ],
            "title": "Inducing target-specific latent structures for aspect sentiment classification",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November",
            "year": 2020
        },
        {
            "authors": [
                "Peng Chen",
                "Zhongqian Sun",
                "Lidong Bing",
                "Wei Yang."
            ],
            "title": "Recurrent attention network on memory for aspect sentiment analysis",
            "venue": "EMNLP, pages 452\u2013 461. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Yu Chen",
                "Lingfei Wu",
                "Mohammed J. Zaki."
            ],
            "title": "Iterative deep graph learning for graph neural networks: Better and robust node embeddings",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Process-",
            "year": 2020
        },
        {
            "authors": [
                "Junqi Dai",
                "Hang Yan",
                "Tianxiang Sun",
                "Pengfei Liu",
                "Xipeng Qiu."
            ],
            "title": "Does syntax matter? A strong baseline for aspect-based sentiment analysis with roberta",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT (1), pages 4171\u20134186. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Li Dong",
                "Furu Wei",
                "Chuanqi Tan",
                "Duyu Tang",
                "Ming Zhou",
                "Ke Xu."
            ],
            "title": "Adaptive recursive neural network for target-dependent twitter sentiment classification",
            "venue": "ACL (2), pages 49\u201354. The Association for Computer Linguistics.",
            "year": 2014
        },
        {
            "authors": [
                "Bing Han",
                "Cheng Wang",
                "Kaushik Roy."
            ],
            "title": "Oscillatory fourier neural network: A compact and efficient architecture for sequential processing",
            "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on In-",
            "year": 2022
        },
        {
            "authors": [
                "Binxuan Huang",
                "Kathleen M. Carley."
            ],
            "title": "Syntaxaware aspect level sentiment classification with graph attention networks",
            "venue": "EMNLP/IJCNLP (1), pages 5468\u20135476. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Lianzhe Huang",
                "Xin Sun",
                "Sujian Li",
                "Linhao Zhang",
                "Houfeng Wang."
            ],
            "title": "Syntax-aware graph attention network for aspect-level sentiment classification",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Eric Jang",
                "Shixiang Gu",
                "Ben Poole."
            ],
            "title": "Categorical reparameterization with gumbel-softmax",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.",
            "year": 2017
        },
        {
            "authors": [
                "Bo Jiang",
                "Ziyan Zhang",
                "Doudou Lin",
                "Jin Tang",
                "Bin Luo."
            ],
            "title": "Semi-supervised learning with graph learning-convolutional networks",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20,",
            "year": 2019
        },
        {
            "authors": [
                "Subhradeep Kayal",
                "George Tsatsaronis."
            ],
            "title": "Eigensent: Spectral sentence embeddings using higher-order dynamic mode decomposition",
            "venue": "Proceedings of the 57th Conference of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "ICLR (Poster).",
            "year": 2015
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling."
            ],
            "title": "Semisupervised classification with graph convolutional networks",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.",
            "year": 2017
        },
        {
            "authors": [
                "Svetlana Kiritchenko",
                "Xiaodan Zhu",
                "Colin Cherry",
                "Saif M. Mohammad."
            ],
            "title": "Nrc-canada-2014: Detecting aspects and sentiment in customer reviews",
            "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval@COLING 2014,",
            "year": 2014
        },
        {
            "authors": [
                "Ruoyu Li",
                "Sheng Wang",
                "Feiyun Zhu",
                "Junzhou Huang."
            ],
            "title": "Adaptive graph convolutional neural networks",
            "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence",
            "year": 2018
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Chris J. Maddison",
                "Andriy Mnih",
                "Yee Whye Teh."
            ],
            "title": "The concrete distribution: A continuous relaxation of discrete random variables",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Con-",
            "year": 2017
        },
        {
            "authors": [
                "Max M\u00fcller-Eberstein",
                "Rob van der Goot",
                "Barbara Plank."
            ],
            "title": "Spectral probing",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages",
            "year": 2022
        },
        {
            "authors": [
                "Matthew E. Peters",
                "Mark Neumann",
                "Mohit Iyyer",
                "Matt Gardner",
                "Christopher Clark",
                "Kenton Lee",
                "Luke Zettlemoyer."
            ],
            "title": "Deep contextualized word representations",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Associ-",
            "year": 2018
        },
        {
            "authors": [
                "Minh-Hieu Phan",
                "Philip O. Ogunbona."
            ],
            "title": "Modelling context and syntactical features for aspectbased sentiment analysis",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10,",
            "year": 2020
        },
        {
            "authors": [
                "Maria Pontiki",
                "Dimitris Galanis",
                "John Pavlopoulos",
                "Harris Papageorgiou",
                "Ion Androutsopoulos",
                "Suresh Manandhar."
            ],
            "title": "Semeval-2014 task 4: Aspect based sentiment analysis",
            "venue": "SemEval@COLING, pages 27\u201335. The Association for Computer Linguis-",
            "year": 2014
        },
        {
            "authors": [
                "Kim Schouten",
                "Flavius Frasincar."
            ],
            "title": "Survey on aspect-level sentiment analysis",
            "venue": "IEEE Trans. Knowl. Data Eng., 28(3):813\u2013830.",
            "year": 2016
        },
        {
            "authors": [
                "Kai Sun",
                "Richong Zhang",
                "Samuel Mensah",
                "Yongyi Mao",
                "Xudong Liu."
            ],
            "title": "Aspect-level sentiment analysis via convolution over dependency tree",
            "venue": "EMNLP/IJCNLP (1), pages 5678\u20135687. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Alex Tamkin",
                "Dan Jurafsky",
                "Noah D. Goodman."
            ],
            "title": "Language through a prism: A spectral approach for multiscale language representations",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Hao Tang",
                "Donghong Ji",
                "Chenliang Li",
                "Qiji Zhou."
            ],
            "title": "Dependency graph enhanced dual-transformer structure for aspect-based sentiment classification",
            "venue": "ACL, pages 6578\u20136588. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Ian Tenney",
                "Patrick Xia",
                "Berlin Chen",
                "Alex Wang",
                "Adam Poliak",
                "R. Thomas McCoy",
                "Najoung Kim",
                "Benjamin Van Durme",
                "Samuel R. Bowman",
                "Dipanjan Das",
                "Ellie Pavlick"
            ],
            "title": "What do you learn from context? probing for sentence structure in contextu",
            "year": 2019
        },
        {
            "authors": [
                "Yuanhe Tian",
                "Guimin Chen",
                "Yan Song."
            ],
            "title": "Aspect-based sentiment analysis with type-aware graph convolutional networks and layer ensemble",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Maria Mihaela Trusca",
                "Daan Wassenberg",
                "Flavius Frasincar",
                "Rommert Dekker."
            ],
            "title": "A hybrid approach for aspect-based sentiment analysis using deep contextual word embeddings and hierarchical attention",
            "venue": "Web Engineering - 20th International",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "NIPS, pages 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "Petar Velickovic",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Amir Pouran Ben Veyseh",
                "Nasim Nouri",
                "Franck Dernoncourt",
                "Quan Hung Tran",
                "Dejing Dou",
                "Thien Huu Nguyen."
            ],
            "title": "Improving aspect-based sentiment analysis with gated graph convolutional networks and syntax-based regulation",
            "venue": "Findings of the As-",
            "year": 2020
        },
        {
            "authors": [
                "Duy-Tin Vo",
                "Yue Zhang."
            ],
            "title": "Target-dependent twitter sentiment classification with rich automatic features",
            "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31,",
            "year": 2015
        },
        {
            "authors": [
                "Kai Wang",
                "Weizhou Shen",
                "Yunyi Yang",
                "Xiaojun Quan",
                "Rui Wang."
            ],
            "title": "Relational graph attention network for aspect-based sentiment analysis",
            "venue": "ACL, pages 3229\u20133238. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Nicolai Wojke",
                "Alex Bewley."
            ],
            "title": "Deep cosine metric learning for person re-identification",
            "venue": "2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018, Lake Tahoe, NV, USA, March 12-15, 2018, pages 748\u2013756. IEEE Computer",
            "year": 2018
        },
        {
            "authors": [
                "Dit-Yan Yeung",
                "Hong Chang."
            ],
            "title": "A kernel approach for semisupervised metric learning",
            "venue": "IEEE Trans. Neural Networks, 18(1):141\u2013149.",
            "year": 2007
        },
        {
            "authors": [
                "Donghan Yu",
                "Ruohong Zhang",
                "Zhengbao Jiang",
                "Yuexin Wu",
                "Yiming Yang."
            ],
            "title": "Graph-revised convolutional network",
            "venue": "Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2020, Ghent, Belgium, September 14-",
            "year": 2020
        },
        {
            "authors": [
                "Chen Zhang",
                "Qiuchi Li",
                "Dawei Song."
            ],
            "title": "Aspect-based sentiment classification with aspectspecific graph convolutional networks",
            "venue": "EMNLP/IJCNLP (1), pages 4567\u20134577. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Chen Zhang",
                "Qiuchi Li",
                "Dawei Song."
            ],
            "title": "Syntax-aware aspect-level sentiment classification with proximity-weighted convolution network",
            "venue": "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Infor-",
            "year": 2019
        },
        {
            "authors": [
                "Jiong Zhang",
                "Yibo Lin",
                "Zhao Song",
                "Inderjit S. Dhillon."
            ],
            "title": "Learning long term dependencies via fourier recurrent units",
            "venue": "Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July",
            "year": 2018
        },
        {
            "authors": [
                "Jianan Zhao",
                "Xiao Wang",
                "Chuan Shi",
                "Binbin Hu",
                "Guojie Song",
                "Yanfang Ye."
            ],
            "title": "Heterogeneous graph structure learning for graph neural networks",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on In-",
            "year": 2021
        },
        {
            "authors": [
                "Tong Zhao",
                "Yozen Liu",
                "Leonardo Neves",
                "Oliver J. Woodford",
                "Meng Jiang",
                "Neil Shah."
            ],
            "title": "Data augmentation for graph neural networks",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Ap-",
            "year": 2021
        },
        {
            "authors": [
                "Yuxiang Zhou",
                "Lejian Liao",
                "Yang Gao",
                "Zhanming Jie",
                "Wei Lu."
            ],
            "title": "To be closer: Learning to link up aspects with opinions",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event",
            "year": 2021
        },
        {
            "authors": [
                "Yanqiao Zhu",
                "Weizhi Xu",
                "Jinghao Zhang",
                "Qiang Liu",
                "Shu Wu",
                "Liang Wang."
            ],
            "title": "Deep graph structure learning for robust representations: A survey",
            "venue": "CoRR, abs/2103.03036.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Aspect-based sentiment analysis (ABSA) is designed to do fine-grained sentiment analysis for different aspects of a given sentence (Vo and Zhang, 2015; Dong et al., 2014). Specifically, one or more aspects are present in a sentence, and aspects may express different sentiment polarities. The purpose\n\u2217Corresponding author 1Our code is at https://github.com/hankniu01/FLT\nof the task is to detect the sentiment polarities (i.e., POSITIVE, NEGATIVE, NEUTRAL) of all given aspects. Given the sentence \"The decor is not a special at all but their amazing food makes up for it\" and corresponding aspects \"decor\" and \"food\", the sentiment polarity towards \"decor\" is NEGATIVE, whereas the sentiment for \"food\" is POSITIVE.\nEarly works (Vo and Zhang, 2015; Kiritchenko et al., 2014; Schouten and Frasincar, 2016) to deal with ABSA mainly relied on manually designing syntactic features, which is cumbersome and ineffective as well. Subsequently, various neural network-based models (Kiritchenko et al., 2014; Vo and Zhang, 2015; Chen et al., 2017; Zhang et al., 2019b; Wang et al., 2020; Trusca et al., 2020) have been proposed to deal with ABSA tasks, to get rid of hand-crafted feature design. In these studies, syntactic structures proved effective, helping to connect aspects to the corresponding opinion words, thereby enhancing the effectiveness of the ABSA task (Zhang et al., 2019b; Tian et al., 2021; Veyseh et al., 2020; Huang and Carley, 2019; Sun et al., 2019; Wang et al., 2020). Additionally, some research (Chen et al., 2020a; Dai et al., 2021; Zhou et al., 2021; Chen et al., 2022; Brauwers and Frasincar, 2023) suggests there should exist task-specific induced latent structures because dependency syntactic structures (following that, we refer to them as external structures for convenience) generated by off-the-shelf dependency parsers are static and sub-optimal in ABSA. The syntactic structure is not specially designed to capture the interactions between aspects and opinion words.\nConsequently, we classify these structure-based ABSA models into three categories by summarizing prior research: (1.) external structure, (2.) semi-induced structure, and (3.) full-induced structure. Works based on external structures use dependency syntactic structures generated by dependency parsers or modified dependency syntactic structures to provide structural support for ABSA (Zhang\net al., 2019b; Sun et al., 2019; Wang et al., 2020). Studies based on semi-induced structures leverage both external and induced structures, merging them to offer structural support for ABSA (Chen et al., 2020a). The first two categories require the introduction of external structures, which increases the complexity of preprocessing, while the third category directly eliminates this burdensomeness.\nOur research is based on full-induced structures. Works in this field intend to totally eliminate the reliance on external structures to aid ABSA by employing pre-trained language models (PLMs) to induce task-specific latent structures (Dai et al., 2021; Zhou et al., 2021; Chen et al., 2022). These efforts, however, aim to create a tree-based structure, then convert it into a graph structure and feed it to Graph Neural Networks (GNNs) to capture structural information. Our research follows this line of thought, but directly from the perspective of the graph, utilizing PLMs to induce a graph structure for GNNs. In addition, studies (Tamkin et al., 2020) have shown that contextual representation contains information about context tokens as well as a wide range of linguistic phenomena, including constituent labels, relationships between entities, dependencies, coreference, etc. That is, there are various scales of information (spanning from the (sub)word itself to its containing phrase, clause, sentence, paragraph, etc.) in the contextual representation. This contextual representational characteristic has rarely been explored in previous studies. Therefore, our research investigates the influence of manipulations at informational scales of contextual representation on structure induction with spectral perspective.\nSpecifically, we employ graph structure learning (GSL) based on metric learning (Zhu et al., 2021) to induce latent structures from PLMs. We investigate three commonly used metric functions (Attention-based (Attn.), Kernel-based (Knl.), and Cosine-based (Cosine)) and contrast their effects on the structure of induced graphs. Furthermore, we heuristically explore four types of Frequency Filters with corresponding band allocations (HIGH, MID-HIGH, MID-LOW, LOW) acting on contextual representations, and in this way, we can segregate the representations of different scales at the level of individual neurons. Additionally, we introduce an automatic frequency selector (AFS) to circumvent the cumbersome heuristic approaches. This allows us to investigate the impact of manipu-\nlations at scale information for structure induction in contextual representations.\nWe employ three commonly PLMs: BERTbase, RoBERTabase, RoBERTalarge. Our research is based on extensive experiments and yields some intriguing findings, which we summarize as follows:\nStructure Induction. By comparing three GSL methods (Attention-based (Attn.), Kernel-based (Knl.), and Cosine-based (Cosine)), we find that the Attention-based method is the best for structure induction on ABSA.\nFrequency Filter (FLT). Heuristic operations of information scales in the contextual representation by Frequency Filters are able to influence structure induction. Based on Attention-based GSL, the structure induction of FLT can obtain lower Aspects-sentiment Distance (AsD) and better performance.\nAutomatic Frequency Selector (AFS). Get rid of the tediousness of the heuristic method, AFS can consistently achieve better results than the Attention-based GSL method. This further demonstrates the effectiveness of manipulating scale information."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Tree Induction for ABSA",
            "text": "In ABSA, there are a lot of works that aim to integrate dependency syntactic information into neural networks (Zhang et al., 2019b; Sun et al., 2019; Wang et al., 2020) to enhance the performance of ABSA. Despite the improvement of dependency tree integration, this is still not ideal since off-theshelf dependency parsers are static, have parsing errors, and are suboptimal for a particular task. Hence, some effort is being directed toward dynamically learning task-specific tree structures for ABSA. For example, (Chen et al., 2020a) combines syntactic dependency trees and automatically induced latent graph structure by a gate mechanism. (Chen et al., 2022) propose to induce an aspectspecific latent tree structure by utilizing policybased reinforcement learning. (Zhou et al., 2021) learn an aspect-specific tree structure from the perspective of closing the distance between aspect and opinion. (Dai et al., 2021) propose to induce tree structure from fine-tuned PLMs for ABSA. However, most of them fall to take the context representational characteristic into account."
        },
        {
            "heading": "2.2 Spectral Approach in NLP",
            "text": "In NLP, one line of spectral methods is used in the study of improving efficiency (Han et al., 2022; Zhang et al., 2018). For example, (Han et al., 2022) propose a new type of recurrent neural network with the help of the discrete Fourier transformer and gain faster training. In addition, a few works investigate contextual representation learning from the standpoint of spectral methods. (Kayal and Tsatsaronis, 2019) propose a method to construct sentence embeddings by exploiting a spectral decomposition method rooted in fluid dynamics. (M\u00fcller-Eberstein et al., 2022; Tamkin et al., 2020) propose using Frequency Filters to constrain different neurons to model structures at different scales. These bring new inspiration to the research of language representation."
        },
        {
            "heading": "2.3 Metric Learning based GSL",
            "text": "The metric learning approach is one of representative graph structure learning (GSL), where edge weights are derived from learning a metric function between pairwise representations (Zhu et al., 2021). According to metric functions, the metric learning approach can be categorized into two subgroups: Kernel-based and Attention-based. Kernelbased approaches utilize traditional kernel functions as the metric function to model edge weights (Li et al., 2018; Yu et al., 2020; Zhao et al., 2021b). Attention-based approaches usually utilize attention networks or more complicated neural networks to capture the interaction between pairwise representations (Velickovic et al., 2018; Jiang et al., 2019; Chen et al., 2020b; Zhao et al., 2021a). The Cosine-based method (Chen et al., 2020b) is generally a kind of Attention-based method. In our experiments, we take it out as a representative method."
        },
        {
            "heading": "3 Method",
            "text": "To obtain induced graph structure, we propose a spectral filter (FLT) approach to select scale information when adaptively learning graph structure. In this section, we introduce a simple but effective approach (FLT) to induce graph structures from PLMs to enhance the performance of ABSA. The overall architecture is displayed in Figure 1."
        },
        {
            "heading": "3.1 Overview",
            "text": "As shown in Figure 1, the overall architecture is composed of PLMs, Graph Learner, GNNs architecture, and Prediction Head under normal cir-\ncumstances. For a given input sentence S = {w1, w2, \u00b7 \u00b7 \u00b7 , wn}, we employ a type of PLMs to serve as the contextual encoder to obtain the hidden contextual representation H \u2208 Rn\u00d7d of the input sentence S, where d is the dimension of word representations, and n is the length of the given sentence. The contextual representation H is waited for inputting into GNNs architecture as node representations. Simultaneously, it is going to feed into Graph Learner to induce latent graph structures, which serve as adjacency matrices A for GNNs architecture. Then the GNNs architecture can extract aspect-specific features ha utilizing both structural information from A and pre-trained knowledge information from H. Finally, we concatenate the representation of [CLS] token hcls from PLMs as well as ha, and send them into a Multi-layer Perception (MLP) (served as the Prediction Head) to detect the sentiment polarities (i.e., POSITIVE, NEGATIVE, NEUTRAL) for the given aspects.\nHere, we investigate the effectiveness of three common graph structure learning (GSL) methods based on metric learning: Attention-based (Attn.), Kernel-based (Knl.), and Cosine-based (Cosine) (refer to (Zhu et al., 2021) for specific descriptions of Kernel-based and Cosine-based methods). We introduce the Attention-based GSL method to adaptively induce graph structures. Firstly, we calculate the unnormalized pair-wise edge score eij for the i-th and j-th words utilizing the given representations hi \u2208 Rd and hj \u2208 Rd. Specifically, the pair-wise edge score eij is calculated as follows:\neij = (Wihi)(Wjhj)\u22a4, (1)\nwhere Wi,Wj \u2208 Rd\u00d7dh are learnable weights for i-th and j-th word representations, where dh is the hidden dimension.\nThen, relying on these pair-wise scores eij for all word pairs, we construct the adjacency matrices A for induced graph structures. Concretely,\nAij =\n{ 1 if i = j\nexp(eij)\u2211n k=1 exp(eik) otherwise , (2)\nwhere the adaptive adjacency matrix is A \u2208 Rn\u00d7n, and Aij is the weight score of the edge between the i-th and j-th words.\nFor simplicity, we employ Vallina Graph Neural Networks (GCNs) (Kipf and Welling, 2017) served as GNNs architecture (other variants of graph neural networks can also be employed here). Given the word representations H and the adaptive adjacency matrix A, we can construct an induced graph structure consisting of words (each word acts as a node in the graph) and feed it into GCNs. Specifically,\nhli = \u03c3( n\u2211\nj=1\nAijWlhl\u22121j + b l), (3)\nwhere \u03c3 is an activation function (e.g. ReLU), Wl and bl are the learnable weight and bias term of the l-th GCN layer. By stacking several layers of Graph Learner and GNNs architectures, we can obtain structure information enhanced word representations Hg for the downstream task. It should be noted that the induced graph structure is dynamically updated while training.\nAfter we get aspect representations ha from Hg, we feed them along with the pooler output hcls of PLMs (the output representation of [CLS] token) into a task-specific Prediction Head to acquire results for the downstream task."
        },
        {
            "heading": "3.2 Frequency Filter (FLT)",
            "text": "Furthermore, inspired by (Tamkin et al., 2020), we introduce a spectral analysis approach to enhance the structure induction ability of the Graph Learner. Intuitively, we tend to import a Frequency Filter on contextual word representations to manipulate on scale information, and then feed them into the Graph Learner module to improve the structure induction capability. Contextual representations have been investigated to not only convey the meaning of words in context (Peters et al., 2018), but also carry a large range of linguistic information such\nas semantic roles, coreference, and constituent labels, etc. (Tenney et al., 2019). Prism (Tamkin et al., 2020) demonstrates these word representations contain multi-scale information ranging from (sub)word to phrase, clause, sentence, and so forth. Hence in this work, we explore the impact of structure induction ability by operating on scale-specific information of contextual representations.\nTo achieve this goal, we introduce a Frequency Filter (FLT) based on Discrete Fourier Transform (DFT) to conduct disentangling operations in the frequency domain. To be specific, given word representations H \u2208 Rn\u00d7d, we feed them into the FLT before the Graph Learner. For the specific i-th and j-th word representations hi \u2208 Rd and hj \u2208 Rd, the pair-wise edge score eij is calculated as follows:\n\u03a6flt(x) = F\u22121 ( \u03a8 ( F(x) )) , (4)\neij = \u03a6 flt(Wihi)\u03a6flt(Wjhj)\u22a4, (5)\nwhere F(\u00b7) and F\u22121(\u00b7) denote the Fast Fourier Transform (FFT) and its inverse, \u03a8 indicates the filtering operation, and \u03a6flt denotes the Frequency Filter (FLT). We carry out filtering at the sentence level. Subsequent operations are consistent with Section 3.1. We conduct experiments and analyses on four band allocations (HIGH, MID-HIGH, MIDLOW, LOW)). The specific band allocations are displayed in Table 5, and the analysis experiments refer to Section 4.7 and 4.10."
        },
        {
            "heading": "4 Experiment",
            "text": "To prove the effectiveness of our approach, we demonstrate experimental results conducted on three datasets for ABSA and compare them with previous works. We show the details as follows."
        },
        {
            "heading": "4.1 Dataset",
            "text": "We conduct experiments on SemEval 2014 task (Rest14 and Laptop14) (Pontiki et al., 2014) and Twitter (Dong et al., 2014) datasets, which are widely used. Each of the three datasets contains\nthree sentiment label categories: POSITIVE, NEUTRAL, and NEGATIVE. Statistics of these datasets are displayed in Table 1, where (Train|Test) denotes the number of instances on the training, and testing set for each dataset."
        },
        {
            "heading": "4.2 Experiment Settings",
            "text": "We utilize the popular Pre-trained Language Models (PLMs) based on Transformer Encoder architecture (BERTbase (Devlin et al., 2019), RoBERTabase and RoBERTalarge (Liu et al., 2019)) for word representations. Moreover, the hidden dimensions of all Graph Learners are 60. The dropout rate is 0.2, the batch size is 32. The number of the epoch is 60 for RoBERTabase and RoBERTalarge, and 30 for BERTbase. We use Adam optimizer (Kingma and Ba, 2015) while training with the learning rate initialized by 1e-5. Following previous works, we use Accuracy and Macro-F1 scores for metrics. All experiments are conducted on NVIDIA Tesla P100."
        },
        {
            "heading": "4.3 Baselines",
            "text": "We categorize the existing structure-based ASBA models into three genres: external structure, semiinduced structure, and full-induced structure. Below, we introduce each of them in detail.\nExternal Structure. This line of works utilizes dependency syntactic structure generated by external dependency parsers (e.g. Spacy and Standford CoreNLP 2, etc.) to offer structural information supplements for ABSA. Its delegate works as follows:\ndepGCN (Zhang et al., 2019a) combines BiLSTM to capture contextual information regarding word orders with multi-layered GCNs.\nCDT (Sun et al., 2019) encodes both dependency and contextual information by utilizing GCNs and BiLSTM.\nRGAT (Wang et al., 2020) feeds reshaped syntactic dependency graph into RGAT to capture aspect-centric information.\nSAGAT (Huang et al., 2020) uses graph attention network and BERT to explore both syntax and semantic information for ABSA.\nDGEDT (Tang et al., 2020) jointly consider BERT outputs and dependency syntactic representations by utilizing GCNs.\nLCFS-ASC-CDW (Phan and Ogunbona, 2020) combine dependency syntactic embeddings, partof-speech embeddings, and contextualized embeddings to enhance the performance of ABSA.\n2https://stanfordnlp.github.io/CoreNLP/\nSemi-induced Structure. Works in this line tend to exploit both dependency syntactic structure from off-the-shelf parsers and induced structure from PLMs, the representative works are as follows:\nKumaGCN (Chen et al., 2020a) combine latent graphs induced by self-attention neural networks and dependency syntactic structure for ABSA.\nFull-induced Structure. Works in this line intend to get totally rid of external parsers and induce task-specific latent structures from PLMs for downstream tasks. Its delegate works as follows:\nFT-RoBERTa (Dai et al., 2021) induce tree structures from the fine-tuned RoBERTa (fine-tune RoBERTa on the ABSA datasets in advance) by utilizing a dependency probing approach.\ndotGCN (Chen et al., 2022) induce aspectspecific opinion tree structures by using Reinforcement learning and attention-based regularization."
        },
        {
            "heading": "4.4 Overall Performance",
            "text": "The overall results of competitive approaches and FLT on the three benchmarks are shown in Table 2. We categorize baselines according to the embedding type (static embedding (GloVe), BERTbase, RoBERTabase, and RoBERTalarge) and the structure they used (None, Dep., Semi., and Full). The parameters of PLMs are trained together with the GSL module for FLT. Compared with baselines, FLT obtains the best results except on Twitter, which obtains comparable results. We speculate that the reason is that the expression of Twitter is more casual, which leads to a limited improvement of the structure on Twitter, which is consistent with the result in (Dai et al., 2021). Compared with FT-RoBERTa-series works, the most relevant work of ours, FLT outperforms them a lot on the three datasets. And it is worth noting that FT-RoBERTaseries works need fine-tuning PLMs on the ABSA datasets in advance (Dai et al., 2021), but FLT does not need it. Therefore, FLT is simpler and more effective than FT-RoBERTa-series works."
        },
        {
            "heading": "4.5 Ablation Study",
            "text": "We conduct ablation studies to highlight the effectiveness of FLT, which is based on Attention-based (Attn.) GSL module and utilizing Frequency Filter. Thus, we compare Attn. and FLT on three PLMs (BERTbase, RoBERTabase, and RoBERTalarge) to show the impact of introducing Frequency Filter. Results are shown in Table 3. Compared to Attn., FLT has achieved significant improvements in consistency across three datasets utilizing different PLMs. Therefore, it can be seen that the manipulation of scale information is beneficial for enhancing performance."
        },
        {
            "heading": "4.6 Different Metric Function",
            "text": "In this section, we contrast the impact of three representative metric functions: Attention-based (Attn.), Kernel-based (Knl.), and Cosine-based (Cosine) on structure induction. From the insight of graph structure learning (Chen et al., 2020b; Zhu et al., 2021), the common options for metric learning include attention mechanism (Vaswani et al., 2017; Jiang et al., 2019), radial basis function kernel (Li et al., 2018; Yeung and Chang, 2007), and cosine similarity (Wojke and Bewley, 2018). We follow these previous works to implement the counterpart metric functions (Knl. and Cosine) for comparison, the results are shown in Table 4. The performance of attention-based (Attn.) on the three benchmarks gains the best results except on Twitter. But the margin between Attn. and Knl. is not big (0.29% for Accuracy and 0.06% for Macro-F1) on Twitter, thus we select the metric function Attn. for later analysis."
        },
        {
            "heading": "4.7 Different Frequency Filters",
            "text": "This section analyzes the impact of four different spectral bands (HIGH, MID-HIGH, MID-LOW, LOW) on structure induction. Each band reflects a diverse range of linguistic scales from word level to sentence level, the detailed setting is shown in Table 5. The different spectral bands are revealed by their period: the number of tokens it takes to complete a cycle. For example, the word scale suggests the period of 1 \u2192 2 tokens, thus the spectral band should be L/2 \u2192 L if the sentence\u2019s length denotes L.\nThen, we conduct analysis experiments on the three datasets to explore the impact of different spectral bands. The length L in our experiments is 100, which fits the length distribution of all samples in these datasets. We perform multiple frequency selections in different frequency bands heuristically, and the performance of our model in different frequency bands on the three datasets is summarized in Table 6. Please refer to Appendix A for\nthe detailed frequency selection and results. Our model performs better in HIGH and MID-HIGH bands on Rest14 and Laptop14 but performs better in LOW and MID-LOW bands on Twitter. Combined with Figure 2, we find that the distribution of sentence length in Twitter is very distinct from that of Rest14 and Laptop14, the sentences in Twitter are generally longer, which leads to the fact that the clause- and sentence-scale information is more beneficial to the effect improvement."
        },
        {
            "heading": "4.8 Aspects-sentiment Distance",
            "text": "To illustrate the effectiveness of induced structure, following (Dai et al., 2021), we introduce the Aspects-sentiment Distance (AsD) to quantify the average distance between aspects and sentiment words in the induced structure. The AsD is calculated as follows:\nC\u22c6 = Si \u2229 C, (6)\nAsD(Si) =\nap\u2211 A cq\u2211 C\u22c6 dist(ap, cq)\n|A||C\u22c6| , (7)\nAsD(D) =\n\u2211 D AsD(Si)\n|D| , (8)\nwhere C = \u27e8c1, \u00b7 \u00b7 \u00b7 cq\u27e9 is a sentiment words set (following the setting from (Dai et al., 2021)), Si denotes each sentence in dataset D, and A = \u27e8a1, \u00b7 \u00b7 \u00b7 , ap\u27e9 denotes the set of aspects for each sentence. We utilize dist(n1, n2) to calculate the relative distance between two nodes (n1 and n2) on the graph structure, and | \u00b7 | represent the number of elements in the given set. For a detailed setting, please refer to Appendix B.\nThe results are displayed in Table 7, and the less magnitude indicates the shorter distance between aspects and sentiment words. Compared to dependency structure (Dep.), attention-based GSL (Attn), and our method (FLT) shorten the Aspectssentiment Distance greatly, which shows that GSL method encourages the aspects to find sentiment words. Furthermore, in comparison with Attn., FLT has a lower AsD score, which proves a reasonable adjustment on the scale level can obtain better structures."
        },
        {
            "heading": "4.9 Structure Visualization and Case Study",
            "text": "Structure Visualization. As shown in Figure 3, we visualize the difference of distinct structures: (a) is from the Spacy parser, (b) is from Attn., and (c)\nis the result from FLT. This case is from the Rest14 dataset. In comparison with (a), aspects are more directly connected to important sentiment words (e.g. cooked, dried, and fine) in (b) and (c), which is consistent with the results of AsD in Section 4.8. In this case, both (b) and (c) obtained correct judgment results, hence from the perspective of structure, they are relatively similar.\nCase Study. In Figure 4, we provide a case to compare Attn. in (a) and FLT in (b). In this case, the structures induced by the two are quite different, and for the aspect (Chinese food), Attn. gives a wrong judgment. From the comparison of structures, it can be found that although the aspect word Chinese in (a) pays attention to the key information I can make better at home, they may not understand the semantics expressed by this clause. From the perspective of structure, FLT in (b) is obviously better able to understand the meaning of this clause."
        },
        {
            "heading": "4.10 Automatic Frequency Selector (AFS).",
            "text": "Furthermore, in order to illustrate the impact of the operation of the scale information on the GSL, we introduce an Automatic Frequency Selector (AFS) to select helpful frequency components along with\nthe optimization of the overall model. In this way, for different datasets, the information of the corresponding scale (HIGH, MID-HIGH, etc.) can be adaptively selected to improve the effect of structure induction. Here we briefly describe the AFS, and for a detailed description, please refer to Appendix C.\nModel Description. Following the operation of FLT, for an input sentence representation H \u2208 Rn\u00d7d, we conduct Discrete Fourier Transform (DFT) F to transform H into the frequency domain. Then, we utilize AFS \u03a6auto to adaptively select frequency components, where AFS \u03a6auto is realized by using a Multi-layer Perceptron (MLP) architecture, please refer to the Appendix C for details. After AFS and inverse Discrete Fourier Transform F\u22121, we can obtain the sentence representation Hafs \u2208 Rn\u00d7d. The subsequent operations are consistent with the attention-based GSL.\nResults. We utilize AFS instead of FLT to conduct experiments on the three datasets, the results are shown in Table 8. Compared to Attn., AFS is consistently improved. This further illustrates the operation of scale information is conducive to improving the effectiveness of GSL on ABSA. Compared with the heuristic FLT method, AFS avoids the burden brought by manual frequency selection, making the method more flexible.\nFrequency Component Analysis. Furthermore, we conducted an in-depth analysis of the intermedi-\nate results obtained from the Automatic Frequency Selector (AFS). From Table 8, we observe that incorporating AFS consistently enhances model performance without manual adjustments to Frequency Components. This suggests that the automated Frequency Components selection process is effective. Based on AFS\u2019s Frequency Component selection outcomes, we performed statistical analyses across three datasets in accordance with the spectral band distribution outlined in Table 5. Table 9 illustrates the percentage of Frequency Components selected by AFS within different spectral bands, while \"Overall\" represents the percentage of selected Frequency Components across all four bands.\nIt is evident that the results are not uniformly 100%, indicating that AFS indeed performs selection on Frequency Components, thereby adjusting information at various scales to achieve consistent improvements. Moreover, the percentage of selected Frequency Components varies across different datasets, implying adaptive adjustments by AFS to cater to the diverse demands of distinct samples. Notably, the LOW band exhibits the highest percentage of selected Frequency Components, underscoring the significance of sentence-level information for token-level tasks (such as Structure Induction for ABSA, which can be considered a tokenlevel task). This observation also aligns with the conclusion drawn in reference (M\u00fcller-Eberstein et al., 2022)."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we propose utilizing GSL to induce latent structures from PLMs for ABSA and introduce spectral methods (FLT and AFS) into this problem. We also explore the impact of manipulation on scale information of the contextual representation for structure induction. Extensive experiments and analyses have demonstrated that the operation of scale information of contextual representation\ncan enhance the effect of GSL on ABSA. Additionally, our exploration is also beneficial to provide inspiration for other similar domains.\nLimitations\nThough we verify the operation on various information scales can be beneficial to structure induction on ABSA, there are still some limitations. Although the heuristic FLT has achieved excellent results, it requires some manual intervention. The AFS method reduces manual participation, but its effect is worse than the optimal FLT method. However, it is still meaningful to explore the impact of scale information on the contextual representation of downstream tasks."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is funded in part by the National Natural Science Foundation of China Project (No.U1936213), and the Major Key Project of PCL (PCL2021A06)."
        },
        {
            "heading": "A Different Frequency Selection",
            "text": "We heuristically select spectral bands (HIGH, MIDHIGH, MID-LOW, LOW) to observe the impact of different spectral bands on structure induction for ABSA. The specific selection of spectral bands at different frequencies and their results are shown in Table 10. The range of spectral bands corresponds to the description in Table 5. Here, based on the distribution of sentence lengths in the dataset (refer to Figure 2), we set the maximum length (L) to\n100 for each dataset and place sentences of similar length in one batch, with a batch size of 32. Each batch is batched according to the maximum sentence length in that batch. For simplicity, we did not design specific spectral bands for different sentence lengths. Instead, we set the spectral bands based on the maximum sentence length (L) in each dataset. We only change the hyperparameter \u2019Bands\u2019 settings, while all other settings remain the same. For specific experimental settings, refer to Section 4.2. It can be observed that different spectral band selections indeed lead to different results, and an appropriate heuristic spectral band selection can significantly improve the results."
        },
        {
            "heading": "B The Settings of AsD analysis",
            "text": "Here, we provide a detailed introduction to the relative distance calculation dist(n1, n2) for AsD. For a given sentence Si, with its aspect words A = \u27e8a1, \u00b7 \u00b7 \u00b7 , ap\u27e9, sentiment word set C = \u27e8c1, \u00b7 \u00b7 \u00b7 cq\u27e9, and the adjacency matrix AG of the induced graph structure, we calculate the shortest hops from ap to cq. If the value of the corresponding position of ap and cq on the adjacency matrix AG is greater than the threshold \u03b3, then we call the distance between ap and cq to be 1. Otherwise, finding the shortest hops between ap and cq on the AG as its shortest path. We also use \u03b3 to judge whether there is an edge between two nodes. Here, \u03b3 is set to the average value of all values of AG. If ap and cq are not directly connected, we set the distance between ap and cq to the maximum number of hops, where the maximum number of hops is set to 10."
        },
        {
            "heading": "C Automatic Frequency Selector (AFS)",
            "text": "Furthermore, it is not affirmed that information in just one band (e.g. HIGH, MID-HIGH, etc.) is helpful, and information in other bands may also provide a gaining effect. Therefore with this in mind, we introduce an Automatic Frequency Selector (AFS) to select helpful frequency components along with the optimization of the overall model.\nTo achieve this goal, we design the Frequency Selection operation under a probabilistic scenario \u03a5. To be specific, we map each frequency component f into a Bernoulli parameter space by employing a Multi-layer Perceptron (MLP) architecture to parameterize this mapping process. Firstly, we bring in a set of learnable parameters \u03be \u2208 Rk\u00d7dk to parameterize frequency components, where k denotes the number of frequency components, and dk de-\nnotes the dimension of component representations. Then, we utilize the MLP architecture (composed of two linear projection layers Proj1 and Proj2, and an activation function \u03c3 (i.e. ReLU)) to map frequency components \u03be into the Bernoulli parameter space.\nzB = MLP (\u03be) = Proj2 ( \u03c3 ( Proj1(\u03be) )) , (9)\n\u03beB = \u03c6 (( zB \u2212 log ( \u2212 log(\u03f5) )) /\u03c4 ) (10)\nwhere \u03beB denotes the success probabilities of Bernoulli distributions, and \u03c6 denotes the Softmax function. We utilize the Gumbel reparameterization proposed by (Jang et al., 2017; Maddison et al., 2017) to address the differentiable difficulty during training, where \u03f5 \u223c U(0, 1) is random variables of a uniform distribution on the interval (0, 1). The hyperparameter \u03c4 \u2192 0 is the annealing temperature, which is adjusted to zero progressively in practice. Next, we can obtain the values of Bernoulli random variables mB \u223c Bern(\u03beB), where mB \u2208 {0, 1}k, and Bern denotes Bernoulli distributions. During the non-training phase, we set a hyperparameter threshold \u03b3 to control the sparsity of mB . (For the Rest14 dataset, we set the threshold \u03b3 to 0.65. For the other two datasets, the threshold is set at 0.75.)\nSubsequently, for the i-th and j-th word representations hi \u2208 Rd and hj \u2208 Rd, we can calculate the pair-wise edge score eij as follows:\n\u03a6afs(x) = F\u22121 ( \u03a5 ( F(x) )) , (11)\neij = \u03a6 afs(Wihi)\u03a6afs(Wjhj)\u22a4, (12)\nwhere \u03a5 indicates the Frequency Selection operation, and \u03a6afs denotes the Automatic Frequency Selector (AFS). Subsequent operations are consistent with Section 3.1."
        }
    ],
    "title": "Adaptive Structure Induction for Aspect-based Sentiment Analysis with Spectral Perspective",
    "year": 2023
}