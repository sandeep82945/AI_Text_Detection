{
    "abstractText": "The concept of a complex event schema pertains to the graph structure that represents realworld knowledge of events and their multidimensional relationships. However, previous studies on event schema induction have been hindered by challenges such as error propagation and data quality issues. To tackle these challenges, we propose a knowledge-enriched discrete diffusion model. Specifically, we distill the abundant event scenario knowledge of Large Language Models (LLMs) through an object-oriented Python style prompt. We incorporate this knowledge into the training data, enhancing its quality. Subsequently, we employ a discrete diffusion process to generate all nodes and links simultaneously in a nonauto-regressive manner to tackle the problem of error propagation. Additionally, we devise an entity relationship prediction module to complete entity relationships between event arguments. Experimental results demonstrate that our approach achieves outstanding performance across a range of evaluation metrics.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Yupu Hao"
        },
        {
            "affiliations": [],
            "name": "Pengfei Cao"
        },
        {
            "affiliations": [],
            "name": "Yubo Chen"
        },
        {
            "affiliations": [],
            "name": "Kang Liu"
        },
        {
            "affiliations": [],
            "name": "Jiexin Xu"
        },
        {
            "affiliations": [],
            "name": "Huaijun Li"
        },
        {
            "affiliations": [],
            "name": "Xiaojian Jiang"
        },
        {
            "affiliations": [],
            "name": "Jun Zhao"
        }
    ],
    "id": "SP:c547adf8a0ac22ebb09c6741655a882407cf71ea",
    "references": [
        {
            "authors": [
                "Jacob Austin",
                "Daniel D Johnson",
                "Jonathan Ho",
                "Daniel Tarlow",
                "Rianne Van Den Berg."
            ],
            "title": "Structured denoising diffusion models in discrete state-spaces",
            "venue": "Advances in Neural Information Processing Systems, 34:17981\u201317993.",
            "year": 2021
        },
        {
            "authors": [
                "Nathanael Chambers."
            ],
            "title": "Event schema induction with a probabilistic entity-driven model",
            "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1797\u20131807, Seattle, Washington, USA. Association for Computa-",
            "year": 2013
        },
        {
            "authors": [
                "Nathanael Chambers",
                "Dan Jurafsky."
            ],
            "title": "Unsupervised learning of narrative event chains",
            "venue": "Proceedings of ACL-08: HLT, pages 789\u2013797, Columbus, Ohio. Association for Computational Linguistics.",
            "year": 2008
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol."
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in neural information processing systems, 34:8780\u2013 8794.",
            "year": 2021
        },
        {
            "authors": [
                "Rotem Dror",
                "Haoyu Wang",
                "Dan Roth."
            ],
            "title": "Zeroshot on-the-fly event schema induction",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 705\u2013725, Dubrovnik, Croatia. Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Xinya Du",
                "Heng Ji."
            ],
            "title": "Retrieval-augmented generative question answering for event argument extraction",
            "venue": "arXiv preprint arXiv:2211.07067.",
            "year": 2022
        },
        {
            "authors": [
                "Yu",
                "Carl Edwards",
                "Xiaomeng Jin",
                "Yizhu Jiao",
                "Ghazaleh Kazeminejad",
                "Zhenhailong Wang",
                "Chris CallisonBurch",
                "Mohit Bansal",
                "Carl Vondrick",
                "Jiawei Han",
                "Dan Roth",
                "Shih-Fu Chang",
                "Martha Palmer",
                "Heng Ji"
            ],
            "title": "RESIN-11: Schema-guided event",
            "year": 2022
        },
        {
            "authors": [
                "Shansan Gong",
                "Mukai Li",
                "Jiangtao Feng",
                "Zhiyong Wu",
                "LingPeng Kong."
            ],
            "title": "Diffuseq: Sequence to sequence text generation with diffusion models",
            "venue": "arXiv preprint arXiv:2210.08933.",
            "year": 2022
        },
        {
            "authors": [
                "Mark Granroth-Wilding",
                "Stephen Clark."
            ],
            "title": "What happens next? event prediction using a compositional neural network model",
            "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI\u201916, page 2727\u20132733. AAAI Press.",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel."
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in neural information processing systems, 33:6840\u2013 6851.",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans."
            ],
            "title": "Classifierfree diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598.",
            "year": 2022
        },
        {
            "authors": [
                "Emiel Hoogeboom",
                "Didrik Nielsen",
                "Priyank Jaini",
                "Patrick Forr\u00e9",
                "Max Welling."
            ],
            "title": "Argmax flows and multinomial diffusion: Learning categorical distributions",
            "venue": "Advances in Neural Information Processing Systems, 34:12454\u201312465.",
            "year": 2021
        },
        {
            "authors": [
                "Xiaomeng Jin",
                "Manling Li",
                "Heng Ji."
            ],
            "title": "Event schema induction with double graph autoencoders",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Jaehyeong Jo",
                "Seul Lee",
                "Sung Ju Hwang."
            ],
            "title": "Score-based generative modeling of graphs via the system of stochastic differential equations",
            "venue": "International Conference on Machine Learning, pages 10362\u201310383. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Daniel D Johnson",
                "Jacob Austin",
                "Rianne van den Berg",
                "Daniel Tarlow."
            ],
            "title": "Beyond in-place corruption: Insertion and deletion in denoising probabilistic models",
            "venue": "arXiv preprint arXiv:2107.07675.",
            "year": 2021
        },
        {
            "authors": [
                "Gwanghyun Kim",
                "Taesung Kwon",
                "Jong Chul Ye."
            ],
            "title": "Diffusionclip: Text-guided diffusion models for robust image manipulation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2426\u20132435.",
            "year": 2022
        },
        {
            "authors": [
                "Zhifeng Kong",
                "Wei Ping",
                "Jiaji Huang",
                "Kexin Zhao",
                "Bryan Catanzaro."
            ],
            "title": "Diffwave: A versatile diffusion model for audio synthesis",
            "venue": "arXiv preprint arXiv:2009.09761.",
            "year": 2020
        },
        {
            "authors": [
                "Manling Li",
                "Sha Li",
                "Zhenhailong Wang",
                "Lifu Huang",
                "Kyunghyun Cho",
                "Heng Ji",
                "Jiawei Han",
                "Clare Voss."
            ],
            "title": "The future is not one-dimensional: Complex event schema induction by graph modeling for event prediction",
            "venue": "arXiv preprint arXiv:2104.06344.",
            "year": 2021
        },
        {
            "authors": [
                "Manling Li",
                "Qi Zeng",
                "Ying Lin",
                "Kyunghyun Cho",
                "Heng Ji",
                "Jonathan May",
                "Nathanael Chambers",
                "Clare Voss."
            ],
            "title": "Connecting the dots: Event graph schema induction with path language modeling",
            "venue": "Proceedings of the 2020 Conference on Empirical",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Li",
                "John Thickstun",
                "Ishaan Gulrajani",
                "Percy S Liang",
                "Tatsunori B Hashimoto."
            ],
            "title": "Diffusionlm improves controllable text generation",
            "venue": "Advances in Neural Information Processing Systems, 35:4328\u2013 4343.",
            "year": 2022
        },
        {
            "authors": [
                "Kiem-Hieu Nguyen",
                "Xavier Tannier",
                "Olivier Ferret",
                "Romaric Besan\u00e7on."
            ],
            "title": "Generative event schema induction with entity disambiguation",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International",
            "year": 2015
        },
        {
            "authors": [
                "Chenhao Niu",
                "Yang Song",
                "Jiaming Song",
                "Shengjia Zhao",
                "Aditya Grover",
                "Stefano Ermon."
            ],
            "title": "Permutation invariant graph generation via score-based generative modeling",
            "venue": "International Conference on Artificial Intelligence and Statistics, pages 4474\u20134484.",
            "year": 2020
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Jeff Z Pan",
                "Elspeth Edelstein",
                "Patrik Bansky",
                "Adam Wyner."
            ],
            "title": "A knowledge graph based approach to social science surveys",
            "venue": "Data Intelligence, 3(4):477\u2013 506.",
            "year": 2021
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer."
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages",
            "year": 2022
        },
        {
            "authors": [
                "Rachel Rudinger",
                "Pushpendre Rastogi",
                "Francis Ferraro",
                "Benjamin Van Durme."
            ],
            "title": "Script iuction as language modeling",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1681\u20131686, Lisbon, Portugal. As-",
            "year": 2015
        },
        {
            "authors": [
                "Yong Rui",
                "Vicente Ivan Sanchez Carmona",
                "Mohsen Pourvali",
                "Yun Xing",
                "Wei-Wen Yi",
                "Hui-Bin Ruan",
                "Yu Zhang."
            ],
            "title": "Knowledge mining: a crossdisciplinary survey",
            "venue": "Machine Intelligence Research, 19(2):89\u2013114.",
            "year": 2022
        },
        {
            "authors": [
                "Chandramouli Sastry",
                "Sri Harsha Dumpala",
                "Sageev Oore."
            ],
            "title": "Training diffusion classifiers with denoising assistance",
            "venue": "arXiv preprint arXiv:2306.09192.",
            "year": 2023
        },
        {
            "authors": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Dongsheng Li",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "Diffusionner: Boundary diffusion for named entity recognition",
            "venue": "arXiv preprint arXiv:2305.13298.",
            "year": 2023
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli."
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "International conference on machine learning, pages 2256\u20132265. PMLR.",
            "year": 2015
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Clement Vignac",
                "Igor Krawczuk",
                "Antoine Siraudin",
                "Bohan Wang",
                "Volkan Cevher",
                "Pascal Frossard."
            ],
            "title": "Digress: Discrete denoising diffusion for graph generation",
            "venue": "arXiv preprint arXiv:2209.14734.",
            "year": 2022
        },
        {
            "authors": [
                "Haitao Wang",
                "Tong Zhu",
                "Mingtao Wang",
                "Guoliang Zhang",
                "Wenliang Chen."
            ],
            "title": "A prior information enhanced extraction framework for documentlevel financial event extraction",
            "venue": "Data Intelligence, 3(3):460\u2013476.",
            "year": 2021
        },
        {
            "authors": [
                "Xiao Wang",
                "Guangyao Chen",
                "Guangwu Qian",
                "Pengcheng Gao",
                "Xiao-Yong Wei",
                "Yaowei Wang",
                "Yonghong Tian",
                "Wen Gao."
            ],
            "title": "Large-scale multi-modal pre-trained models: A comprehensive survey",
            "venue": "Machine Intelligence Research, pages 1\u201336.",
            "year": 2023
        },
        {
            "authors": [
                "Noah Weber",
                "Niranjan Balasubramanian",
                "Nathanael Chambers."
            ],
            "title": "Event representations with tensorbased compositions",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
            "year": 2018
        },
        {
            "authors": [
                "Brown",
                "Martha Palmer",
                "Chris Callison-Burch",
                "Carl Vondrick",
                "Jiawei Han",
                "Dan Roth",
                "Shih-Fu Chang",
                "Heng Ji."
            ],
            "title": "RESIN: A dockerized schemaguided cross-document cross-lingual cross-media information extraction and event tracking system",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Dongchao Yang",
                "Jianwei Yu",
                "Helin Wang",
                "Wen Wang",
                "Chao Weng",
                "Yuexian Zou",
                "Dong Yu."
            ],
            "title": "Diffsound: Discrete diffusion model for text-tosound generation",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing.",
            "year": 2023
        },
        {
            "authors": [
                "C Ying",
                "T Cai",
                "S Luo",
                "S Zheng",
                "G Ke",
                "D He",
                "Y Shen",
                "TY Liu."
            ],
            "title": "Do transformers really perform bad for graph representation? arxiv 2021",
            "venue": "arXiv preprint arXiv:2106.05234.",
            "year": 2021
        },
        {
            "authors": [
                "Hongyi Yuan",
                "Zheng Yuan",
                "Chuanqi Tan",
                "Fei Huang",
                "Songfang Huang."
            ],
            "title": "Seqdiffuseq: Text diffusion with encoder-decoder transformers",
            "venue": "arXiv preprint arXiv:2212.10325.",
            "year": 2022
        },
        {
            "authors": [
                "Quan Yuan",
                "Xiang Ren",
                "Wenqi He",
                "Chao Zhang",
                "Xinhe Geng",
                "Lifu Huang",
                "Heng Ji",
                "Chin-Yew Lin",
                "Jiawei Han."
            ],
            "title": "Open-schema event profiling for massive news corpora",
            "venue": "Proceedings of the 27th ACM International Conference on Information and",
            "year": 2018
        },
        {
            "authors": [
                "Wayne Xin Zhao",
                "Kun Zhou",
                "Junyi Li",
                "Tianyi Tang",
                "Xiaolei Wang",
                "Yupeng Hou",
                "Yingqian Min",
                "Beichen Zhang",
                "Junjie Zhang",
                "Zican Dong"
            ],
            "title": "A survey of large language models. arXiv preprint arXiv:2303.18223",
            "year": 2023
        },
        {
            "authors": [
                "T (Hoogeboom et al",
                "Yang"
            ],
            "title": "\u03b2t = (1\u2212 \u03b1t)/K and the transition matrix can be",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Event schema induction aims to summarize common patterns and structures from historical events. Current studies mainly induce the atomic schema for each independent event type and their arguments separately (e.g. \u201cAttack\u201d event with the arguments: \u201cAttacker\u201d, \u201cTarget\u201d, \u201cInstrument\u201d and \u201cPlace\u201d), without considering the correlation between events (Chambers and Jurafsky, 2008; Chambers, 2013; Nguyen et al., 2015). However, some real-world events are usually very complex, consisting of multiple events and their relations. For example in Figure 1, Bombing is a complex event, which involves some fine-grained events, such as Assemble, Detonate and Injure. Therefore, some\n1Code is available at https://github.com/ hypasd-art/KDM/\nresearchers attempt to study the complex event schema induction task, which abstracts typical structures for complex events from event data. Figure 1 illustrates an example of the complex event schema induction process for the scenario of Bombing. Initially, an information extraction (IE) tool (Du et al., 2022) is utilized to extract instance graphs from raw texts. Subsequently, we induce the event schema based on these extracted instance graphs. The resulting event schema is represented as a graph, where events are interconnected through temporal links (e.g., Damage occurs after Detonate) and their argument relations (e.g., the target\nof the Detonate event assumes the victim role in the subsequent Injure event).\nHowever, inducing complex event schema is nontrivial. As shown in Figure 1, it necessitates the model\u2019s ability to summarize the events within instance graphs and possess a profound understanding of the multi-dimensional relationships between these events. Recently, graph-based methods are proposed for this task by utilizing graph generation techniques (Li et al., 2021; Jin et al., 2022). For example, Li et al. (2021) proposes an auto-regressive generation method that generates the schema following event temporal order. Similarly, Jin et al. (2022) leverages an auto-encoder to encode the global skeleton information and decode the schema graph event by event. Despite successful efforts, these methods still face two critical challenges:\nKnowledge Coverage of Instance Graphs: The event schema induction task summarizes the instance graphs to obtain the event schema. Thus, the quality of the instance graphs is crucial for the event schema induction. However, the instance graphs are extracted via Information Extraction (IE) tools (Rui et al., 2022), whose knowledge coverage is very limited. For example, as the representative IE tool, RESIN (Wen et al., 2021) is trained on fixed datasets and can only extract predefined types of entities and events. Besides, the extraction performance of RESIN is also unsatisfactory, which only achieves approximately 64% of F1-score for event detection on the ACE dataset. It indicates that the IE tool is difficult to extract complete instance information, even for predefined event types. Therefore, how to improve the knowledge coverage of instance graphs is an important problem.\nError Propagation of Auto-regressive Decoding: Previous graph-based approaches are based on the auto-regressive generation manner (Li et al., 2021; Jin et al., 2022), generating the entire event schema graph node by node, which may lead to error accumulation over time and therefore degrade the generation performance. For example, in Figure 1, the model may mistakenly generate \u201cInjure\u201d instead of \u201cDetonate\u201d leading to the omission of subsequent events such as \u201cDamage\u201d and \u201cInvestigate\u201d in the generated schema or resulting in incorrect nodes being generated in the next. The final generated event schema graph will consist of dozens of nodes and edges at the minimum, as each instance graph used for training contains an average of 117 event nodes and 246 temporary links accord-\ning to our statistics on the Suicide-IED dataset (Li et al., 2021). The need to generate so many nodes and edges will inevitably exacerbate the problem of error accumulation. Thus, it is essential to address the error propagation problem during schema graph generation.\nIn this paper, we propose a novel method termed as Knowledge-Enriched Diffusion Model (KDM) to address aforementioned problems. Firstly, to improve the knowledge coverage of instance graphs, we devise a Instance Graph Expansion module. As Large Language Models (LLMs) are trained on vast corpora of texts (Touvron et al., 2023; Zhao et al., 2023; Wang et al., 2023) and therefore possess extensive event and entity knowledge of the real world, we leverage the LLMs (Chowdhery et al., 2022; Ouyang et al., 2022) as the knowledge databases to inject knowledge into instance graphs. The module utilizes a Python style object-oriented prompt to extract event knowledge from LLMs, and adds the knowledge into the instance graphs. Secondly, to tackle error propagation of auto-regressive decoding, we propose an Event Skeleton Generation module, which utilizes discrete diffusion model to predict all nodes and links simultaneously in non-auto-regressive manner but not generates individually based on time series, which alleviate the error propagation problem (Austin et al., 2021; Yang et al., 2023; Vignac et al., 2022). Finally, we devise an Entity Relation Prediction module, which expands the event skeleton with corresponding arguments and predicts their relations to get a complete schema.\nThe contributions of our work include: (1) We propose a Knowledge-Enriched discrete Diffusion Model (KDM) for complex event schema induction task. To the best of our knowledge, we are the first to simultaneously utilize LLMs and diffusion models to accomplish the task. (2) To improve knowledge coverage of instance graphs, we propose an Instance Graph Expansion module, which distillates the event knowledge in LLMs with python code-style prompt. To solve the error propagation problem, we design an event skeleton generation module, which predicts all nodes and links simultaneously. (3) We conduct extensive experiments on three widely used datasets. Experimental result indicates that our proposed method outperforms state-of-the-art baselines.\nForward Process \ud835\udc5e \ud835\udc3a\ud835\udc61 \ud835\udc3a\ud835\udc61\u22121 \ud835\udc5d\ud835\udf03 (\ud835\udc3a\ud835\udc61\u22121|\ud835\udc3a\ud835\udc61, \ud835\udc66) Reverse Process Transport Detonate Injure Evacuation Investigate Arrest Transport Arrest Damage Transport Detonate Injure Evacuation Investigate \ud835\udc3a\ud835\udc61 Attack threaten Transport interrogate Arrest Evacuation Damage\n\ud835\udf19\ud835\udf03 (\ud835\udc3a\ud835\udc61)\nprobability matrix \ud835\udc44\ud835\udc61\nGraph\n\ud835\udc3a\ud835\udc61 sample\nG sample Graph distribution\nNeural\u00a0network\n\ud835\udc3a\ud835\udc61\u22121 sampleOnehot\u00a0 encoding\nG sampleOnehot\u00a0 encoding \ud835\udc3a\ud835\udc61 sampleOnehot\u00a0 encoding\n\ud835\udc3a\ud835\udc61\u22121 G Uniform distribution\nFigure 2: The discrete diffusion process. In forward process, the noise changes the types of nodes and edges."
        },
        {
            "heading": "2 Preliminaries and Problem Formulation",
            "text": ""
        },
        {
            "heading": "2.1 Preliminaries",
            "text": "Discrete diffusion model preserves the discrete characteristics of each element in the training data x0, it introduces noise to each element xr0 \u2208 x0 to into the uniform distribution and reverses them by removing the noise (Austin et al., 2021). Figure 2 shows the process of graph-based discrete diffusion.\nThe forward process. This process progressively adds noise to x0 by transition probability matrix Qt at t step.\nq(xt|xt\u22121) = xt\u22121Qt (1)\nwhere |Qt|ij = q(xt = j|xt\u22121 = i) indicates the probability of transition from xt\u22121 = i to xt = j. The forward process gradually converts each xr0 \u2208 x0 to a uniform distribution when T is large enough.\nThe reverse process. Reverse process p\u03b8 with learnable parameters \u03b8 aims to convert the noise distribution xT back to the original x0:\np\u03b8(x0:T ) = p\u03b8(xT ) T\u220f t=1 p\u03b8(xt\u22121|xt) (2)\np\u03b8(xt\u22121|xt) = \u222b q(xt\u22121|xt,x0)dp\u03b8(x0|xt) (3)\nand according to Bayes formula as follows:\nq(xt\u22121|xt,x0) = q(xt|xt\u22121,x0)q(xt\u22121|x0)\nq(xt|x0) (4)\nTherefore, the task becomes predicting p\u03b8(x0|xt) using a neural network."
        },
        {
            "heading": "2.2 Problem Formulation",
            "text": "In the instance graphs about a specific topic y (e.g., Bombing), nodes represent events and entities, while edges have three types: the temporal link, the argument link and the entity relation link. The instance graph is denoted as G = (N , E), We\ndefine the distribution of graph as G = (N ,E), where node set N is sampled from node feature distribution N \u2208 Rn\u00d7a and edge set E is sampled from edge feature distribution E \u2208 Rn\u00d7n\u00d7b. Here, c(p) represents the one-hot vector (category scalar) sampled based on probability distribution p. At time t, the instance graph is defined as Gt = (N t, E t) = (c(N t), c(Et)).\nThe objective of this task is to learn an event schema Sy from a set of instance graphs Dy = {G(1),G(2), . . . ,G(m)} that belong to the same specific topic."
        },
        {
            "heading": "3 Our Approach",
            "text": "To solve the complex event schema induction task, we propose a Knowledge-Enriched Discrete Diffusion Model, as shown in Figure 3. Our method mainly consists of three modules: (1) Instance Graph Expansion, which expands the instance graphs using the complex event knowledge obtained from LLMs atomically. (2) Event Skeleton Induction, which summarizes the event evolution skeleton using a discrete diffusion model. (3) Entity Relation Prediction, which decorates the arguments to the event skeleton and then use a simple graph transformer to predict the entity relations. We will illustrate each component in detail."
        },
        {
            "heading": "3.1 Instance Graph Expansion",
            "text": "In this section, we will illustrate how to obtain knowledge about event schemas from LLMs and inject them into instance graphs. Complex event schemas involve intricate graph structures, while LLMs are good at processing unstructured language tasks. To retain structured information of instance graphs, we need LLMs to be able to handle structured inputs and outputs. Considering the powerful coding capabilities of LLMs, we treat events as Python objects. In detail, events, entities, and their intricate relations can correspond to classes, attributes, and instances in the object-\noriented paradigm, respectively. This module includes three aspects: event knowledge expansion, temporal relation expansion, and entity relation expansion.\nIn event knowledge expansion, we select frequently occurring event sequences from the training instance graphs and write them into Python classes. Then we ask the LLM to enrich Python code. In this way, we will obtain new classes which represent new events that have a high correlation with the scenario. We filter out new events that occur less frequently than a hyperparameter K and are not in the predefined event categories. In temporal relation expansion, we write the obtained events as multiple-choice questions to establish their temporal relation with existing event sequences. In entity relation expansion, we obtain the argument connection relation between the new and existing events by encoding new events into Python code and instantiating the class. By effectively leveraging the complex event knowledge contained in LLMs, our approach enhances the event schema generation process. The details and examples refer to Appendix E."
        },
        {
            "heading": "3.2 Event Skeleton Generation",
            "text": "In this section, we will introduce the forward and reverse processes of discrete diffusion based on the instance graphs through Instance Graph Expansion module. We have adopted the diffusion framework of Vignac et al.(2022) and made improvements based on it. Here, we denote the distribution G at\ntime t as Gt. The forward diffusion process. In this process, we apply noise separately on each node and edge. This is achieved by multiplying the node and edge distributions with the transition probability matrix Q. By doing so, we can obtain the graph Gt from the previous graph Gt\u22121. Mathematically, this can be expressed as:\nq(Gt|Gt\u22121) = (Nt\u22121QNt ,Et\u22121QEt )\n= (NQ\u0304Nt ,EQ\u0304 E t )\n(5)\nWhere Q\u0304t = Q0Q1 \u00b7 \u00b7 \u00b7 Qt. The transition probability matrix Qt is defined as \u03b1tI + (1 \u2212 \u03b1t)1(1)T /K, where 1 is a column vector of all ones, and \u03b1t varies from 1 to 0 (Austin et al., 2021). This formulation ensures that the distribution q(Gt|G0) consistent with uniform distribution when time t becomes sufficiently large.\nNext, we sample the node and edge types from these probability distributions to obtain a discrete graph:\nGt = c(q(Gt|Gt\u22121)) (6)\nThe reverse diffusion process. We aim to remove the noise from the graphs using a parameterized reverse process p\u03b8. Following the formulation presented by Austin et al.(2021), we can express the posterior p\u03b8(Gt\u22121|Gt) as:\np\u03b8(Gt\u22121|Gt) = \u222b q(Gt\u22121|Gt,G0)dp\u03b8(G0|Gt) (7)\nTo predict the clean graph distribution Gtp = p\u03b8(G0|Gt) at time t given the noisy input Gt, we\ntrain a graph transformer \u03d5\u03b8 that outputs the clean graph representation:\nGtp = (N t p,E t p) = \u03d5\u03b8(Gt) (8)\nOur model \u03d5\u03b8 adopt transformer structure (Vaswani et al., 2017). Previous graph transformer model (Ying et al., 2021) is not appropriate for encoding directed graphs based on time series, because the relative position information between nodes is lost during the noise adding process. For instance, the self-attention mechanism module in the transformer cannot differentiate two \u201ctransport\u201d events that occur in different time periods. To address this issue, we encode the depth information of event nodes as a fixed-feature embedding ndep into the model. Before inputting the graph into the transformer, we add the depth feature to the corresponding node feature.\nThe depth fixed-feature embedding is encoded as follows:\nndep = { sin(wk \u00b7 nd), i = 2k cos(wk \u00b7 nd), i = 2k + 1\n(9)\nwhere wk = 1/100002k/nd and nd is the average depth of node n, i is the index of the depth embedding.\nInspired by Vignac et al.(2022), our transformer model comprises several layers, each of which consists of a self-attention module and a feed-forward network. For layer l, the self-attention module takes as input time features t, node features N tl , edge features Etl , and updates their representation as follows:\nAttl = E t lWa +E t lWm \u2299N tl WQ(N tl WK)T (10)\nN tl+1 = MLP (tWtn +AttlN t l WV ) (11)\nEtl+1 = MLP (tWte +Attl) (12)\nwhere \u2299 denotes the pairwise multiplication. Wa, Wm, Wtn, Wte, WQ,WK ,WV are trainable parameters.\nTo optimize our model, we use the cross-entropy loss LCE weighted by \u03bb:\nLCE(Gtp,G) = \u2211 G\u2208D CE(N tp,N) + \u03bbCE(E t p,E) (13)\nOnce we obtain the clean graph distribution Gtp, we can infer the node distribution p\u03b8(nt\u22121|nt) and edge distribution p\u03b8(et\u22121|et) using the equations:\np\u03b8(nt\u22121|nt) = Kn\u2211\nn0=1\nq(nt\u22121|nt, n0)p\u03b8(n0|nt)\np\u03b8(et\u22121|et) = Ke\u2211\ne0=1\nq(et\u22121|et, e0)p\u03b8(e0|et)\n(14)\nwhere Kn is the node type number, and Ke is edge type number. Before the next reverse process, we will get the discrete graph Gt\u22121 from its distribution by probability sampling.\nOur model obtains the final event schema G through T-step reversing process in non-AR manner. For further algorithm and derivation details, please refer to Appendix C.\nConditional Generation. Previous approaches (Li et al., 2020, 2021; Jin et al., 2022) need to train separate models for each scenario to ensure accurate generation. However, in order to generate event schemas for various scenarios using a single model and improve the model\u2019s generalization capabilities, we also propose the conditional diffusion model named as KDMall as a supplement.\nWe incorporate the category information y of the instance graphs as an additional attribute to control the training process of the model (Ho and Salimans, 2022; Dhariwal and Nichol, 2021). This allows us to influence the category of the generated schema. The formulation is as follows:\np\u03b8(Gt\u22121|Gt,y) = \u222b\nq(Gt\u22121|Gt,G0)p\u03b8(G0|Gt,y) (15)\nTherefore, we only need to encode the category information y into the neural network. We simply concatenate it into temporal features, enabling the conditional diffusion model to generate event schemas of different categories:\nGtp = (N t p,E t p) = \u03d5\u03b8(Gt,y) (16)"
        },
        {
            "heading": "3.3 Entity Relation Prediction",
            "text": "In this module, we have developed a simple architecture that combines a graph transformer for obtaining node representations with an MLP layer for relation prediction. This module takes the event skeleton, expanded with event argument roles, as input and generates the complete event schema by predicting the relations between entities.\nWhile previous models have primarily focused on entity types in the classification process, neglecting the significance of events and event roles, we address this limitation by artificially aggregating them together. We initialize the node features using BERT model (Devlin et al., 2018). Specifically, for each event or entity node ni, BERT (ni) represents its type embedding encoded by BERT. For entity nodes, nei indicates the event node that entity node ni belongs to, and nri is a fixed embedding representing the role played by entity node ni in\nevent nei , The encoding formula of embeddings n r i is the same as that of equation 9.\nni = concat(BERT (ni), BERT (nei )) (17)\nOur transformer encoder is the same as the model used for Event Skeleton Generation, except that it lacks the time feature. The graph transformer outputs n\u0302i corresponding to the input ni, which is then passed to the MLP predictor.\nThe predicted relation type rij of entity node ni and nj is then computed as:\nrij = MLP ([n\u0302i + n r i , n\u0302j + n r j ]) (18)\nIt is worth noting that the classification problem is highly unbalanced. To address this issue, we set different weights for different categories of the loss function, defined as:\nL = \u2211\nentity i,j\nH(r\u0302ij)CE(r\u0302ij , rij) (19)\nwhere r\u0302ij denotes the true relation between entity i and j and H(\u00b7) is a scalar function that assigns balanced weights to different relationships, with each relationship corresponding to a specific value."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We conduct experiments using the IED Schema Learning Corpus released by Li et al.(2021). The dataset utilizes the DARPA KAIROS ontology. The corpus specifically focuses on three sub-types of complex events related to Improvised Explosive Devices (IEDs): General-IED, Mass-Car-BombingIED, and Suicide-IED.\nHowever, the test data in the corpus has data quality issues since it is also extracted through IE tools. To address this, we manually modify the test data, generating golden test event schemas based on the modified data. Additionally, to ensure the objective evaluation of our model\u2019s effectiveness, we record the test results using the original, unmodified data, which are provided in Appendix 7."
        },
        {
            "heading": "4.2 Baselines",
            "text": "In this work, we compare the proposed event schema induction model with two baselines:\nFrequency-Based Sampling (FBS) model which constructs the event schema according to frequency distributions of temporal links in the training data. At each timestamp, FBS samples a pair of event types according to their frequency and\nadds the sampled edge into the schema graph. The procedure is repeated until FBS detects a cycle in the schema graph after adding a new edge.\nDouble Graph Auto-encoders Model (DoubleGAE) (Jin et al., 2022), the state-of-the-art schema induction model which designs a variational directed acyclic graph auto-encoder to extract the event skeleton. Then it uses another GCN based auto-encoder to reconstruct entity-entity relations.\nLarge Language Model (LLM) have strong understanding and generation abilities, We ask the large language model (ChatGPT) to directly generate the event schema and use it as the baseline."
        },
        {
            "heading": "4.3 Evaluation Metrics",
            "text": "To evaluate the quality of the generated schema, we compare the schema with test instance graphs in terms of the following metrics to see how well the schema match real world instance graphs, the following evaluation metrics are employed:\n(1) Event type match. we calculate the F1 score between the event types present in the schema graph and test instance graphs.\n(2) Event sequence match. A good schema is able to track events through a timeline. we calculate the F1 score between the event sequences of length 2 or 3 present in the schema graph and the test instance graphs.\n(3) Node/edge type distribution. we compare the Kullback-Leibler (KL) divergence of the node and edge type distributions between the schema graph and each test instance graph.\n(4) Event Argument Connection Match (CM). Complex event graph schema includes entities and their relations, representing how events are connected through arguments. Because there is a serious long tail issue with the data, we calculate the macro F1-score for every pair of relationships between entities."
        },
        {
            "heading": "4.4 Overall Results",
            "text": "As shown in Table 1, the result demonstrates the effectiveness of KDM in capturing important events and their relationships. Specifically, our approach outperforms the baseline methods in terms of event sequence matching, particularly for longer path lengths (l=3).\nThese improvements can be attributed to the discrete diffusion process employed in our model. This process allows our model to simultaneously predict the categories of all nodes and edges, making it well-suited for graph generation. Addition-\nally, the Transformer architecture leveraged in our model effectively utilizes global features through the self-attention mechanism, resulting in improved prediction accuracy.\nFurthermore, our model shows remarkable improvements in the Connection Match evaluation, indicating the effectiveness of our graph transformer model than GCN graph auto-encoder in DoubleGAE."
        },
        {
            "heading": "4.5 Conditional Generation Results",
            "text": "Building upon the aforementioned diffusion model, in order to improve the possibility of the model\u2019s generalization ability, we present an extension in the form of a conditional diffusion model as a supplement. This model enables the generation of event schemas for various scenarios using a single model.\nAs shown in Table 2, when comparing KDMall with our model trained on a specific-dataset,\nwe find that KDMall shows improved generalization capabilities and better understanding of event relationships, particularly in the \u201cGeneralIED\u201d scenario. Additionally, in other datasets, KDMall demonstrates comparable results to the model trained on a single dataset, indicating the potential of our conditional generation process. The incorporation of diverse training data enables the model to learn common patterns and associations across different scenarios, leading to improved performance and broader applicability (Sastry et al., 2023; Kim et al., 2022)."
        },
        {
            "heading": "4.6 Ablation Experiment",
            "text": "To demonstrate the effectiveness of our approach, we conduct ablation studies on the \u201cSuicide-IED\u201d dataset. (1) IGE Module Ablation Experiment: To prove the effectiveness of our approach, we conduct experiments as shown in Table 3. JSON is a prevalent format for representing structured data. We encode the data in JSON format and instruct the LLMs to perform expansion, while maintaining the rest of the process consistent with the Python prompt approach. As shown in the Table, the results obtained through the use of Python prompts are noticeably better than those achieved with JSON prompts. And after filtering, the event types generated by the Python prompt are significantly more numerous than those generated by the JSON prompt. This observation underscores the effectiveness of the Python prompt approach. (2) Diffusion Model Ablation Experiment: In Table 4, comparing our KDM model with a variant\nthat removes the Instance Graph Expansion module, Our model achieves a 4.1% increase in node matching accuracy, proving the effectiveness of Instance Graph Expansion module. Additionally, by incorporating depth information, we observe a notable 5.9% improvement in sequence matching. These results demonstrate that the inclusion of depth information enhances our model\u2019s ability to capture the structural characteristics of graph, proving the effectiveness of adding depth features. (3) Entity Predictor Ablation Experiment: In Figure 4, Compared to not setting weight hyperparameters, our model achieves a significant 5.57% improvement in the macro F1 index, demonstrating that our weight scalar function significantly addresses the long-tail data problem. Moreover, when comparing the results of \u201cw/o RE\u201d and \u201cw/o IGE\u201d, the improvements highlight the effectiveness of adding role and event features and Instance Graph Expansion module."
        },
        {
            "heading": "4.7 Case Study",
            "text": "In Figure 5, we observe that our Instance Graph Expansion module successfully generates a schema that encompasses a broader range of events and exhibits more comprehensive temporal relationships within complex events. This outcome support the effectiveness of leveraging object-oriented coding to distill knowledge from LLMs. Additionally, we provide a case study showcasing the diffusion process on the \u201cSuicide-IED\u201d dataset in Figure 6 in Appendix.\nAttack\nArrest\nTransport"
        },
        {
            "heading": "5 Related Work",
            "text": "Event Schema Event schema induction is a comprehensive graphical pattern composed of temporal and multi hop argument relationships (Li et al., 2020; Jin et al., 2022). It is actually a combination of atomic schema induction (Chambers, 2013; Yuan et al., 2018; Du and Ji, 2022; Wang et al., 2021) and script learning (Rudinger et al., 2015; Granroth-Wilding and Clark, 2016; Weber et al., 2018). Clearly, the event schema induction has broad application significance. For example, the event schema facilitates analysis and prediction of future events, aiding in the development of reaction plans for relevant scenarios (Li et al., 2021; Dror et al., 2023; Pan et al., 2021). Event schemas can be used as guidance information in information extraction, which helps people understand the internal logic of events (Wen et al., 2021).\nDiffusion models Diffusion model (SohlDickstein et al., 2015; Ho et al., 2020) has achieved impressive results on image, text and audio generation (Rombach et al., 2022; Shen et al., 2023; Li et al., 2022; Gong et al., 2022; Kong et al., 2020; Yuan et al., 2022). Recently, Vignac et al. (2022) have shown great potential in graph generation field. Previous graph diffusion models embedded graphs in a continuous space by adding Gaussian noise to the nodes and edges feature (Niu et al., 2020; Jo et al., 2022). However, this approach destroys the graph\u2019s sparsity and makes it hard to capture the node connections (Vignac et al., 2022). Discrete diffusion model (Austin et al., 2021; Yang et al., 2023; Vignac et al., 2022; Johnson et al., 2021) overcomes this problem by utilizing Markov process that can occur independently on each node or edge."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we identify the limitations of previous works and proposed a Knowledge-enriched discrete diffusion model. To enhance the quality and coherence of the generated schemas, we harness the potential and rich knowledge present in LLMs by utilizing them for Instance Graph Expansion. Our model leverages a discrete diffusion process to learn and generate event skeletons, while incorporating an entity relationship predictor to predict the relationships between event arguments. Additionally, we propose a conditional diffusion model with the purpose of generating schemas for mul-\ntiple diverse topics. We achieved the best results among multiple different evaluation indicators.\nLimitations\nWe only consider the temporal relationship between events here and do not consider the hierarchical structure of the event schema, which may result in not perfect event schemas generated by us.\nDue to the limited availability of datasets, our conditional diffusion model KDMall has only undergone unified training and testing on three highly related explosive events, requiring more categories and quantities of data for the comprehensive ability testing of the model.\nEthics Statement\nWe use a discrete diffusion model to generate event skeletons and design an entity relationship predictor. At the same time, we have fully explored the potential rich knowledge in LLM for knowledge expansion. Our work has improved the effectiveness of event schema induction, helping people better summarize the logic and ontology knowledge of events, making contributions to this field."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by the National Key Research and Development Program of China (No. 2020AAA0106400), the National Natural Science Foundation of China (No. 62176257, 61976211). This work is also supported by the Strategic Priority Research Program of Chinese Academy of Sciences (Grant No.XDA27020100 ), the Youth Innovation Promotion Association CAS, and Yunnan Provincial Major Science and Technology Special Plan Projects (No.202202AD080004)."
        },
        {
            "heading": "A Data Preprocessing",
            "text": "In the data preprocessing stage. Firstly, for each complex event, we constructed an instance graph by merging coreferential events or entities. Isolated events were excluded from the instance graphs during the graph construction process.\nSpecifically, we followed the cleaning strategy outlined in Jin et al. (2022). We deleted links with the same start and end types, as well as event-event links such as (DIE, INJURE), (ARRESTJAILDETAIN, ATTACK), (ENDPOSITION, STARTPOSITION), (DEFEAT, EXCHANGEBUYSELL), (SENTENCE, DIE), (ENDPOSITION, SENTENCE), and (THREATENCOERCE, RELEASEPAROLE) from the instance graphs. The maximum number of graph nodes m is set to 50."
        },
        {
            "heading": "B Training And Evaluation Details",
            "text": "In our event skeleton induction process, we utilize a 12-layer Transformer model. Additionally, we employ a 3-layer Transformer as our entity relation predictor. To balance the trade-off between nodes and edges, we set \u03bb to 3. The learning rate is set to 1e-4, and the number of diffusion training epochs is set to 2500. The scalar function H(r\u0302ij) is set to 0.1 if rij indicates \u201cNo-Relation\u201d, otherwise the function is set to 0.9. We conduct evaluations using 500 randomly generated event schemas for each performance metric. The node number is sampled from a range of 25 to 35. We choose the model checkpoint from the last epoch for evaluation.\nIn the Instance Graph Expansion process, we select the top 10 frequently occurring event sequences from the training data as inputs for ChatGPT. Each event sequence is input to ChatGPT 10 times to obtain the final result. Furthermore, we use a hyperparameter K of 3 to filter out events generated by ChatGPT that occur less frequently.\nIn the Entity Relation Prediction module, each event has a predetermined set of argument roles. For example, the \u201cInjure\u201d event may have the argument role \u201cVictim\u201d limited to entity types \u201cPER\u201d and \u201cAML\u201d. We count the occurrences of entity categories for each role in all instance graphs. The entity category with the highest occurrence in the corresponding role is then inserted into the event skeleton.\nTo modify the test data, we made the following modifications: 1. Merge the same path: For all subsequent nodes of each event node, merge event\nnodes with the same type, starting from the START node and merging in the order of the BFS algorithm. 2. Supplementary event nodes: Based on human judgment, randomly add possible missing events that may occur in the schema."
        },
        {
            "heading": "C Conditional Discrete Diffusion Model",
            "text": "Transition probability matrix in forward process. In discrete diffusion model, a transition probability matrix Q is defined to corrupt data for each step. Here Qt = \u03b1tI + (1 \u2212 \u03b1t)1(1)T /K, where 1 is a column vector of all ones, \u03b1t varies from 1 to 0 making sure the node and edge feature sampled from is a uniform distribution at time T (Hoogeboom et al., 2021; Yang et al., 2023). \u03b2t = (1\u2212 \u03b1t)/K and the transition matrix can be represent as:\nQt =  \u03b1t + \u03b2t \u03b2t . . . \u03b2t \u03b2t \u03b1t + \u03b2t . . . \u03b2t\n\u00b7 \u00b7 . . . \u00b7 \u00b7 \u00b7 . . . \u00b7 \u03b2t \u03b2t . . . \u03b1t + \u03b2t  we can calculate q(xt|x0) according to follow-\ning formula:\nx0Q\u0304t = \u03b1\u0304 tx0 + \u03b2\u0304 t. (20)\nas t is enough large, \u03b1t is close to 0, the graph distribution Gt is confirm to uniform distribution.\nReverse discrete diffusion process, we convert the noise GT into G, whose joint probability having a Markovian structure follows (Vignac et al., 2022):\nlog p\u03b8(G0:T ) = log p(GT ) T\u2211 t=1 log p\u03b8(Gt\u22121|Gt)\nwhere p\u03b8 is the process of the reverse with learnable parameters \u03b8. and for each discrete elements x in graph G0:T posterior probability is :\np\u03b8(xt\u22121|xt) = \u2211 x q(xt\u22121|xt, x)p(x)\n= \u2211 x0 q(xt\u22121|xt, x0)p\u03b8(x0|xt)\nThe posterior q(xt\u22121|xt, x0) can be derived according to Bayes formula as follows (Austin et al.,\n2021):\nq(xt\u22121|xt, x0) = q(xt|xt\u22121, x0)q(xt\u22121|x0)\nq(xt|x0)\n= xt(Qt) T \u2299 x0Q\u0304t\u22121 x0Q\u0304t(xt) T\nTo train the discrete diffusion process, we minimize the negative logarithmic likelihood of the predicted distribution of the model using variational lower bound (VLB), We use G0 here to represent G:\nL = \u2212Eq(G0)[logp\u03b8(G0)] \u2264 LV LB = \u2212Eq(G0:G)[log p\u03b8(G0)] = Eq(G0:T )[DKL(q(GT |G0) \u2225 p\u03b8(GT ))\ufe38 \ufe37\ufe37 \ufe38\nLT + T\u2211\nt=2 DKL(q(Gt\u22121|Gt,G0) \u2225 p\u03b8(Gt\u22121|Gt))\ufe38 \ufe37\ufe37 \ufe38 Lt\u22121\n\u2212 log p\u03b8(G0|G1)\ufe38 \ufe37\ufe37 \ufe38 L0 ]\n= Eq(G0:T )[DKL(q(GT |G0) \u2225 p\u03b8(GT |Gn,Gd))\ufe38 \ufe37\ufe37 \ufe38 LT\n+ log p\u03b8(Gn,Gd)\n+ T\u2211 t=2 DKL(q(Gt\u22121|Gt,G0) \u2225 p\u03b8(Gt\u22121|Gt))\ufe38 \ufe37\ufe37 \ufe38 Lt\u22121 \u2212 log p\u03b8(G0|G1)\ufe38 \ufe37\ufe37 \ufe38 L0 ]\nPlease note that Gt is sampled from the node number distribution Gn and the corresponding depth distribution Gd. Therefore, the probability p\u03b8(GT ) can be expressed as p\u03b8(GT ) = p\u03b8(GT |Gn,Gd)p\u03b8(Gn,Gd).\nThe terms LT and Lt\u22121 represent the KullbackLeibler (KL) divergences between graph categorical distributions, while L0 represent the predicted probabilities of the graph G0 based on the noisy graph G1. The algorithms 1 and 2 is the training and generating algorithms about KDM."
        },
        {
            "heading": "D Supplement Experiment",
            "text": "As presented in Table 5, we evaluate our model on the original testing data used by (Jin et al., 2022) and observe consistent outperformance of our model compared to the baselines specially in sequence match. This result highlights the strong capability of our discrete diffusion model in generating high-quality event schemas. Interestingly, we\nnote that models without knowledge expansion exhibited better performance in this evaluation. This finding also suggests that the original test data may not effectively measure the quality of the generated schemas.\nAlgorithm 1: Training KDM 1 for G in D do 2 sample t from Uniform(T); 3 sample Gt from distribution\n(NQ\u0304Nt ,EQ\u0304 E t );\n4 estimate distribution Gtp = \u03d5\u03b8(Gt); 5 calculate loss LCE(Gtp,G); 6 update network \u03d5\u03b8 7 end\nAlgorithm 2: Sampling from KDM 1 sample GT from Uniform distribution; 2 for t = T to 1 do 3 convert Gt to distribution Gt; 4 Gtp = \u03d5\u03b8(Gt); 5 estimate distribution p\u03b8(Gt\u22121|Gt); 6 sample Gt\u22121 from distribution; 7 end\nE Instance Graph Expansion Details\nThe process of the Instance Graph Expansion is shown in Figure 7, we also present our event knowledge expansion prompt in Figure 8, temporal relation expansion prompt in Figure 10 and entity relation expansion prompt in Figure 9."
        }
    ],
    "title": "Complex Event Schema Induction with Knowledge-Enriched Diffusion Model",
    "year": 2023
}