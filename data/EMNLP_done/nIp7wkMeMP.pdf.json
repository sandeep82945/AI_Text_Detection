{
    "abstractText": "Named Entity Recognition (NER) frequently suffers from the problem of insufficient labeled data, particularly in fine-grained NER scenarios. Although K-shot learning techniques can be applied, their performance tends to saturate when the number of annotations exceeds several tens of labels. To overcome this problem, we utilize existing coarse-grained datasets that offer a large number of annotations. A straightforward approach to address this problem is prefinetuning, which employs coarse-grained data for representation learning. However, it cannot directly utilize the relationships between finegrained and coarse-grained entities, although a fine-grained entity type is likely to be a subcategory of a coarse-grained entity type. We propose a fine-grained NER model with a Fineto-Coarse(F2C) mapping matrix to leverage the hierarchical structure explicitly. In addition, we present an inconsistency filtering method to eliminate coarse-grained entities that are inconsistent with fine-grained entity types to avoid performance degradation. Our experimental results show that our method outperforms both K-shot learning and supervised learning methods when dealing with a small number of fine-grained annotations. Code is available at https://github.com/sue991/CoFiNER.",
    "authors": [
        {
            "affiliations": [],
            "name": "Su Ah Lee"
        },
        {
            "affiliations": [],
            "name": "Seokjin Oh"
        },
        {
            "affiliations": [],
            "name": "Woohwan Jung"
        }
    ],
    "id": "SP:097858867ecd7231a2b23870ff093cc9313364c0",
    "references": [
        {
            "authors": [
                "Armen Aghajanyan",
                "Anchit Gupta",
                "Akshat Shrivastava",
                "Xilun Chen",
                "Luke Zettlemoyer",
                "Sonal Gupta."
            ],
            "title": "Muppet: Massive multi-task representations with pre-finetuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Sarkar Snigdha Sarathi Das",
                "Arzoo Katiyar",
                "Rebecca J Passonneau",
                "Rui Zhang."
            ],
            "title": "Container: Fewshot named entity recognition via contrastive learning",
            "venue": "arXiv preprint arXiv:2109.07589.",
            "year": 2021
        },
        {
            "authors": [
                "Leon Derczynski",
                "Eric Nichols",
                "Marieke van Erp",
                "Nut Limsopatham."
            ],
            "title": "Results of the WNUT2017 shared task on novel and emerging entity recognition",
            "venue": "Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 140\u2013147, Copenhagen,",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Ning Ding",
                "Guangwei Xu",
                "Yulin Chen",
                "Xiaobin Wang",
                "Xu Han",
                "Pengjun Xie",
                "Haitao Zheng",
                "Zhiyuan Liu."
            ],
            "title": "Few-NERD: A few-shot named entity recognition dataset",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Maximilian Hofer",
                "Andrey Kormilitzin",
                "Paul Goldberg",
                "Alejo Nevado-Holgado"
            ],
            "title": "Few-shot learning for named entity recognition",
            "year": 2018
        },
        {
            "authors": [
                "Yucheng Huang",
                "Kai He",
                "Yige Wang",
                "Xianli Zhang",
                "Tieliang Gong",
                "Rui Mao",
                "Chen Li."
            ],
            "title": "Copner: Contrastive learning with prompt guiding for fewshot named entity recognition",
            "venue": "Proceedings of the 29th International conference on computational",
            "year": 2022
        },
        {
            "authors": [
                "Haoming Jiang",
                "Danqing Zhang",
                "Tianyu Cao",
                "Bing Yin",
                "Tuo Zhao"
            ],
            "title": "Named entity recognition with small strongly labeled and large weakly labeled data",
            "year": 2021
        },
        {
            "authors": [
                "Woohwan Jung",
                "Kyuseok Shim."
            ],
            "title": "Dual supervision framework for relation extraction with distant supervision and human annotation",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 6411\u20136423, Barcelona, Spain (On-",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Lefteris Loukas",
                "Manos Fergadiotis",
                "Ilias Chalkidis",
                "Eirini Spyropoulou",
                "Prodromos Malakasiotis",
                "Ion Androutsopoulos",
                "Georgios Paliouras."
            ],
            "title": "Finer: Financial numeric entity recognition for xbrl tagging",
            "venue": "arXiv preprint arXiv:2203.06482.",
            "year": 2022
        },
        {
            "authors": [
                "Jie Ma",
                "Miguel Ballesteros",
                "Srikanth Doss",
                "Rishita Anubhai",
                "Sunil Mallya",
                "Yaser Al-Onaizan",
                "Dan Roth."
            ],
            "title": "Label semantics for few shot named entity recognition",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1956\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Dublin",
                "Ireland"
            ],
            "title": "Association for Computational Linguistics",
            "year": 1971
        },
        {
            "authors": [
                "Tingting Ma",
                "Huiqiang Jiang",
                "Qianhui Wu",
                "Tiejun Zhao",
                "Chin-Yew Lin."
            ],
            "title": "Decomposed metalearning for few-shot named entity recognition",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1584\u20131596.",
            "year": 2022
        },
        {
            "authors": [
                "Alexis Mitchell",
                "Stephanie Strassel",
                "Shudong Huang",
                "Ramez Zakhary."
            ],
            "title": "Ace 2004 multilingual training corpus",
            "venue": "Linguistic Data Consortium, Philadelphia, 1:1\u20131.",
            "year": 2005
        },
        {
            "authors": [
                "Seongwoong Oh",
                "Woohwan Jung",
                "Kyuseok Shim."
            ],
            "title": "Thunder: Named entity recognition using a teacher-student model with dual classifiers for strong and weak supervisions",
            "venue": "Proceedings of 26th European Conference on Artificial Intelligence.",
            "year": 2023
        },
        {
            "authors": [
                "Alan Ritter",
                "Sam Clark",
                "Mausam",
                "Oren Etzioni."
            ],
            "title": "Named entity recognition in tweets: An experimental study",
            "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1524\u20131534, Edinburgh, Scotland,",
            "year": 2011
        },
        {
            "authors": [
                "Yongliang Shen",
                "Xiaobin Wang",
                "Zeqi Tan",
                "Guangwei Xu",
                "Pengjun Xie",
                "Fei Huang",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "Parallel instance query network for named entity recognition",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Mujeen Sung",
                "Minbyul Jeong",
                "Yonghwa Choi",
                "Donghyeon Kim",
                "Jinhyuk Lee",
                "Jaewoo Kang."
            ],
            "title": "Bern2: an advanced neural biomedical named entity recognition and normalization tool",
            "venue": "Bioinformatics, 38(20):4837\u20134839.",
            "year": 2022
        },
        {
            "authors": [
                "Erik F. Tjong Kim Sang."
            ],
            "title": "Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition",
            "venue": "COLING-02: The 6th",
            "year": 2002
        },
        {
            "authors": [
                "Christopher Walker",
                "Stephanie Strassel",
                "Julie Medero",
                "Kazuaki Maeda."
            ],
            "title": "Ace 2005 multilingual training corpus",
            "venue": "Linguistic Data Consortium, Philadelphia, 57:45.",
            "year": 2006
        },
        {
            "authors": [
                "Peiyi Wang",
                "Runxin Xu",
                "Tianyu Liu",
                "Qingyu Zhou",
                "Yunbo Cao",
                "Baobao Chang",
                "Zhifang Sui."
            ],
            "title": "An enhanced span-based decomposition method for few-shot sequence labeling",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of",
            "year": 2022
        },
        {
            "authors": [
                "Ralph Weischedel",
                "Martha Palmer",
                "Mitchell Marcus",
                "Eduard Hovy",
                "Sameer Pradhan",
                "Lance Ramshaw",
                "Nianwen Xue",
                "Ann Taylor",
                "Jeff Kaufman",
                "Michelle Franchini"
            ],
            "title": "Ontonotes release 5.0 ldc2013t19",
            "venue": "Linguistic Data",
            "year": 2013
        },
        {
            "authors": [
                "Deming Ye",
                "Yankai Lin",
                "Peng Li",
                "Maosong Sun."
            ],
            "title": "Packed levitated marker for entity and relation extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4904\u20134917, Dublin,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Named Entity Recognition (NER) is a fundamental task in locating and categorizing named entities in unstructured texts. Most research on NER has been conducted on coarse-grained datasets, including CoNLL\u201903 (Tjong Kim Sang, 2002), ACE04 (Mitchell et al., 2005), ACE05 (Walker et al., 2006), and OntoNotes (Weischedel et al., 2013), each of which has less than 18 categories. As the applications of NLP broaden across diverse fields, there is increasing demand for finegrained NER that can provide more precise and\n\u2020Major in Bio Artificial Intelligence\ndetailed information extraction. Nonetheless, detailed labeling required for large datasets in the context of fine-grained NER presents several significant challenges. It is more cost-intensive and time-consuming than coarse-grained NER. In addition, it requires a high degree of expertise because domain-specific NER tasks such as financial NER (Loukas et al., 2022) and biomedical NER (Sung et al., 2022) require fine-grained labels. Thus, finegrained NER tasks typically suffer from the data scarcity problem. Few-shot NER approaches (Ding et al., 2021) can be applied to conduct fine-grained NER with scarce fine-grained data. However, these methods do not exploit existing coarse-grained datasets that can be leveraged to improve finegrained NER because fine-grained entities are usually subtypes of coarse-grained entities. For instance, if a model knows what Organization is, it could be easier for it to understand the concept of Government or Company. Furthermore, these methods often experience early performance saturation, necessitating the training of a new supervised learning model if the annotation extends beyond several tens of labels.\nA pre-finetuning strategy (Aghajanyan et al., 2021; Ma et al., 2022a) was proposed to overcome the aforementioned problem. This strategy first learns the feature representations using a coarsegrained dataset before training the fine-grained model on a fine-grained dataset. In this method, coarse-grained data are solely utilized for representation learning; thus, it still does not explicitly utilize the relationships between the coarse- and fine-grained entities.\nOur intuition for fully leveraging coarse-grained datasets comes mainly from the hierarchy between coarse- and fine-grained entity types. Because a coarse-grained entity type typically comprises multiple fine-grained entity types, we can enhance lowresource fine-grained NER with abundant coarsegrained data. To jointly utilize both datasets, we\ndevise a method to build a mapping matrix called F2C (short for \u2018Fine-to-Coarse\u2019), which connects fine-grained entity types to their corresponding coarse-grained entity types and propose a novel approach to train a fine-grained NER model with both datasets.\nSome coarse-grained entities improperly match a fine-grained entity type because datasets may be created by different annotators for different purposes. These mismatched entities can reduce the performance of the model during training. To mitigate this problem, coarse-grained entities that can degrade the performance of fine-grained NER must be eliminated. Therefore, we introduce a filtering method called \u2018Inconsistency Filtering\u2019. This approach is designed to identify and exclude any inconsistent coarse-to-fine entity mappings, ensuring a higher-quality training process, and ultimately, better model performance. The main contributions of our study are as follows:\n\u2022 We propose an F2C mapping matrix to directly leverage the intimate relation between coarseand fine-grained types.\n\u2022 We present an inconsistency filtering method to screen out coarse-grained data that are inconsistent with the fine-grained types.\n\u2022 The empirical results show that our method achieves state-of-the-art performance by utilizing the proposed F2C mapping matrix and the inconsistency filtering method."
        },
        {
            "heading": "2 Related Work",
            "text": "Fine-grained NER. NER is a key task in information extraction and has been extensively studied. Traditional NER datasets (Tjong Kim Sang, 2002; Ritter et al., 2011; Weischedel et al., 2013; Derczynski et al., 2017) address coarse-grained entity types in the general domain. Recently, domain-specific NER has been investigated in various fields (Hofer et al., 2018; Loukas et al., 2022; Sung et al., 2022). The domain-specific NER tasks typically employ fine-grained entity types. In addition, Ding et al. (2021) proposed a general domain fine-grained NER.\nN -way K-shot learning for NER. Since labeling domain-specific data is an expensive process, few-shot NER has gained attention. Most fewshot NER studies are conducted using an N -way K-shot episode learning (Das et al., 2021; Huang\net al., 2022; Ma et al., 2022b; Wang et al., 2022). The objective of this approach is to train a model that can correctly classify new examples into one of N classes, using only K examples per class. To achieve this generalization performance, a large number of episodes need to be generated. Furthermore, to sample these episodes, the training data must contain a significantly larger number of classes than N . In contrast, in our problem context, the number of fine-grained classes we aim to identify is substantially larger than the number of coarse-grained classes in the existing dataset. Consequently, the episode-based few-shot learning approaches are unsuitable for addressing this problem. Leveraging auxiliary data for information extraction. Several studies have employed auxiliary data to overcome the data scarcity problem. Jiang et al. (2021); Oh et al. (2023) propose NER models trained with small, strongly labeled, and large weakly labeled data. Jung and Shim (2020) used strong and weak labels for relation extraction. However, in the previous studies, the main data and auxiliary data shared the same set of classes, which is not the case for fine-grained NER with coarse-grained labels. While the approaches of Aghajanyan et al. (2021) and Ma et al. (2022a) can be applied to our problem setting, they utilize auxiliary data for representation learning rather than explicitly utilizing the relationship between the two types of data."
        },
        {
            "heading": "3 Proposed Method",
            "text": "In this section, we introduce the notations and define the problem of fine-grained NER using coarsegrained data. Then, we introduce the proposed CoFiNER model, including the creation of the F2C mapping matrix."
        },
        {
            "heading": "3.1 Problem definition",
            "text": "Given a sequence of n tokens X = {x1, x2, ..., xn}, the NER task involves assigning type yi \u2208 E to each token xi where E is a predefined set of entity types. In our problem setting, we used a fine-grained dataset DF = {(XF1 ,YF1 ), ..., (XF|DF |,X F |DF |)} with a predefined entity type set EF . Additionally, we possess a coarse-grained dataset DC = {(XC1 ,YC1 ), ..., (XC|DC |,X C |DC |)} characterized by a coarse-grained entity set EC (i.e. |EF | > |EC |). A coarse-grained dataset typically\nhas a smaller number of types than a fine-grained dataset. Throughout this study, we use F and C to distinguish between these datasets. It should be noted that our method can be readily extended to accommodate multiple coarse-grained datasets, incorporating an intrinsic multi-level hierarchy. However, our primary discussion revolves around a single coarse-grained dataset for simplicity and readability."
        },
        {
            "heading": "3.2 Training CoFiNER",
            "text": "We aim to utilize both coarse- and fine-grained datasets directly in a single model training. Figure 1 illustrates an overview of the fine-grained NER model training process using both coarse- and finegrained datasets. The CoFiNER training process consists of the following four steps: Step 1- Training a fine-grained model. In the first step, we train a fine-grained model fF (\u03b8) with the low-resource fine-grained datasetDF . This process follows a typical supervised learning approach for NER. For a training example (XF ,YF ), XF is fed into a PLM (Pre-trained Language Model), such as BERT, RoBERTa, to generate a contextual representations hi \u2208 Rd of each token xi.\nH = [h1, ...,hn] = PLM([x F 1 , ..., x F n ]). (1)\nThen, we apply a softmax layer to obtain the label probability distribution:\npFi = softmax(Whi + b)\nwhere W\u2208R|EF |\u00d7d and b\u2208R|EF | represent the weights and bias of the classification head, respectively. To train the model using a fine-grained dataset, we optimize the cross-entropy loss function:\nLF = \u2212 1\nn n\u2211 i=1 logpFi [y F i ] (2)\nwhere yi \u2208 EF is the fine-grained label for the token xi. Step 2 - Generating an F2C matrix. To fully leverage the hierarchy between coarse- and finegrained entity types, we avoid training separate NER models for each dataset. Instead, we utilize a single model that incorporates an F2C mapping matrix that transforms a fine-grained output into a corresponding coarse-grained output. The F2C mapping matrix assesses the conditional probability of a coarse-grained entity type s \u2208 EC given\na fine-grained label \u2113\u2208EF (i.e., M\u2113,s = p(yC = s|yF =\u2113)).\nGiven a fine-grained probability distribution pFi computed using the proposed model, the marginal probability of a coarse-grained type s can be computed as\npCi [s] = \u2211 \u2113\u2208EF p(yC=s|yF =\u2113) \u00b7 pFi [\u2113].\nThus, the coarse-grained output probabilities are simply computed as follows:\npCi = p F i \u00b7M (3)\nwhere M \u2208 R|EF |\u00d7|EC | is the F2C mapping matrix whose row-wise sum is 1. By introducing this F2C mapping matrix, we can train a single model using multiple datasets with different granularity levels.\nManual annotation is a straightforward approach that can be used when hierarchical information is unavailable. However, it is not only cost-intensive but also noisy and subjective, especially when there are multiple coarse-grained datasets or a large number of fine-grained entity types. We introduce an efficient method for automatically generating an F2C matrix in \u00a73.3. Step 3 - Filtering inconsistent coarse labels. Although a fine-grained entity type is usually a subtype of a coarse-grained type, there can be some misalignments between the coarse- and finegrained entity types. For example, an entity \"Microsoft\" in a financial document can either be tagged as Company or Stock which are not hierarchical. This inconsistency can significantly degrade the model\u2019s performance.\nTo mitigate the effect of inconsistent labeling, we devise an inconsistency filtering method aimed at masking less relevant coarse labels. By automatically filtering out the inconsistent labels from the coarse-grained dataset, we investigate the coarsegrained labels using the fine-grained NER model trained in Step 1. For each token in the coarsegrained dataset, we predict the coarse-grained label using the fine-grained model and the mapping matrix as follows:\ny\u0303Ci = argmaxp C i . (4)\nIf the predicted label is the same as the coarsegrained label (i.e., yCi = y\u0303 C i ), we assume that the coarse-grained label is consistent with the finegrained one and can benefit the model. Otherwise,\nwe regard the label as inconsistent with fine-grained types and do not utilize the coarse-grained label in Step 4. Note that the fine-grained NER model is frozen during this phase.\nStep 4 - Jointly training CoFiNER with both datasets . The model is trained by alternating between epochs using coarse- and fine-grained data. This is an effective learning strategy for training models using heterogeneous datasets (Jung and Shim, 2020).\nFor the fine-grained batches, CoFiNER is trained by minimizing the loss function defined in Equation (2), as described in Step 1. Meanwhile, we use the F2C mapping matrix to generate coarsegrained outputs and utilize inconsistency filtering for coarse-grained batches. Thus, we compute the cross-entropy loss between the coarse-grained label yCi and the predicted probabilities p C i when the coarse-grained label is consistent with our model (i.e.,yCi = y\u0303 C i ) as follows:\nLC = \u2212 1\nm m\u2211 i=1 logpCi [y C i ] \u00b7 I[yCi = y\u0303Ci ] (5)\nwhere m is a length of XC . For example, suppose that the coarse label of token xi is yCi =ORG. If the estimated coarse label y\u0303Ci is ORG, the loss for the token is \u2212 logpCi [yCi ]. Otherwise, it is zero."
        },
        {
            "heading": "3.3 Construction of the F2C mapping matrix",
            "text": "The F2C mapping matrix assesses the conditional probability of a coarse-grained entity type s\u2208EC given a fine-grained label \u2113 \u2208 EF (i.e., M\u2113,s = p(yC = s|yF = \u2113)). As there are only a few identical texts with both coarse- and fine-grained annotations simultaneously, we can not directly calculate this conditional probability using the data alone. Thus, we approximate the probability using a coarse-grained NER model and fine-grained labeled data as follows: M\u2113,s = p(y\nC=s|yF =\u2113) \u2248 p(y\u0303C=s|yF =\u2113). To generate the mapping matrix, we first train a coarse-grained NER model fC(\u03b8) with the coarsegrained dataset DC . Then, we reannotate the finegrained dataset DF by using the coarse-grained model fC(\u03b8). As a result, we obtain parallel annotations for both coarse- and fine-grained types in the fine-grained data DF . By using the parallel annotations, we can compute the co-occurrence matrix C \u2208 N|EF |\u00d7|EC | where each cell C\u2113,s is the number of tokens that are labeled as fine-grained type \u2113 and coarse-grained type s together.\nBecause some labels generated by the coarsegrained model fC(\u03b8) can be inaccurate, we refine the co-occurrence matrix by retaining only the topk counts for each fine-grained type and setting the rest to zero. Our experiments show that our model performs best when k = 1. This process effec-\ntively retains only the most frequent coarse-grained categories for each fine-grained type, thereby improving the precision of the resulting mapping. Finally, we compute the conditional probabilities for all \u2113 \u2208 EF , s \u2208 EC by using the co-occurrence counts as follows:\nM\u2113,s = p(y\u0303 C=s|yF =\u2113) = C\u2113,s\u2211 s\u2032\u2208EC C\u2113,s\u2032 . (6)\nThe F2C mapping matrix M is used to predict the coarse-grained labels using Equation (3)."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We conduct experiments using a fine-grained NER dataset, Few-NERD (SUP) (Ding et al., 2021), as well as two coarse-grained datasets, namely, OntoNotes (Weischedel et al., 2013) and CoNLL\u201903 (Tjong Kim Sang, 2002). The finegrained dataset Few-NERD comprises 66 entity types, whereas the coarse-grained datasets CoNLL\u201903 and OntoNotes consist of 4 and 18 entity types, respectively. The statistics for the datasets are listed in Table 1. K-shot sampling for the fine-grained dataset. Because we assumed a small number of examples for each label in the fine-grained dataset, we evaluated the performance in the K-shot learning setting. Although Few-NERD provides few-shot samples, they are obtained based on an N -way K-shot scenario, where N is considerably smaller than the total number of entity types. However, our goal is to identify named entities across all possible entity types. For this setting, we resampled K-shot examples to accommodate all-way K-shot scenarios.\nSince multiple entities exist in a single sentence, we cannot strictly generate exact K-shot samples for all the entity types. Therefore, we adopt the\nK\u223c(K + 5)-shot setting. In the K\u223c(K+5)-shot setting, there are at least K examples and at most K+5 examples for each entity type. See Appendix A for more details. In our experiments, we sampled fine-grained training data for K = 10, 20, 40, 80, and 100."
        },
        {
            "heading": "4.2 Experimental Settings",
            "text": "In experiments, we use transformer-based PLM, including BERTBASE, RoBERTaBASE, and RoBERTaLARGE. In CoFiNER, we follow RoBERTaLARGE to build a baseline model. The maximum sequence length is set to 256 tokens. The AdamW optimizer (Loshchilov and Hutter, 2019) is used to train the model with a learning rate of 2e\u22125 and a batch size of 16. The number of epochs is varied for each model. We train the fine-grained model, CoFiNER, over 30 epochs. To construct the F2C mapping matrix, the coarse-grained model is trained for 50 epochs using both CoNLL\u201903 and OntoNotes. To train the inconsistency filtering model, we set different epochs based on the number of shots: For 10, 20, 40, 80, and 100 shot settings, the epochs are 150, 150, 120, 50, and 30, respectively. We report the results using span-level F1. The dropout with a probability of 0.1 is applied. All the models were trained on NVIDIA RTX 3090 GPUs."
        },
        {
            "heading": "4.3 Compared Methods",
            "text": "In this study, we compare the performance of CoFiNER with that of both the supervised and few-shot methods. We modified the existing methods for our experimental setup and re-implemented them accordingly.\nSupervised method. We use BERTBASE, RoBERTaBASE, and RoBERTaLARGE as the supervised baselines, each including a fine-grained classifier on the head. In addition, PIQN (Shen et al., 2022) and PL-Marker (Ye et al., 2022) are methods that have achieved state-of-the-art performance in a supervised setting using the full Few-NERD dataset. All models are trained using only a FewNERD dataset.\nFew-shot method. The LSFS (Ma et al., 2022a) leverages a label encoder to utilize the semantics of label names, thereby achieving state-of-the-art results in low-resource NER settings. The LSFS applies a pre-finetuning strategy to learn prior knowledge from the coarse-grained dataset, OntoNotes. For a fair comparison, we also conducted pre-\nfinetuning on OntoNotes and performed fine-tuning on each shot of the Few-NERD dataset.\nProposed method. We trained our CoFiNER model as proposed in \u00a73. In each epoch, CoFiNER is first trained on two coarse-grained datasets: OntoNotes and CoNLL\u201903. Subsequently, it is trained on the fine-grained dataset Few-NERD. We used RoBERTaLARGE as the pre-trained language model for CoFiNER in Equation (1)."
        },
        {
            "heading": "4.4 Main Results",
            "text": "Table 2 reports the performance of CoFiNER and existing methods. The result shows that CoFiNER outperforms both supervised learning and few-shot learning methods. Because supervised learning typically needs a large volume of training data, these models underperform in low-resource settings. This demonstrates that our method effectively exploits coarse-grained datasets to enhance the performance of the fine-grained NER model. In other words, the proposed F2C mapping matrix significantly reduces the amount of fine-grained dataset required to train supervised NER models. In particular, CoFiNER achieves significant performance improvements compared to the state-ofthe-art model PL-Marker, which also utilizes the same pre-trained language model RoBERTaLARGE as CoFiNER.\nThe few-shot method LSFS yields the highest F1 score for the 10-shot case. However, this fewshot method suffers from early performance saturation, resulting in less than a 2.4 F1 improvement with an additional 90-shot. By contrast, the F1 score of CoFiNER increases by 12.2. Consequently, CoFiNER outperforms all the compared methods except for the 10-shot case. In summary, the proposed method yields promising results for a wide range of data sample sizes by explicitly leveraging the inherent hierarchical structure."
        },
        {
            "heading": "4.5 Ablation Study",
            "text": "An ablation study is conducted to validate the effectiveness of each component of the proposed method. The results are presented in Table 3. First, we remove the inconsistency filtering (w/o filtering) and observe a significant decrease in the F1 score, ranging from 2.05 to 7.27. These results demonstrate the effectiveness of our filtering method, which excludes mislabeled entities. Second, we provide the results using a single coarse-grained dataset (w/o OntoNotes and w/o CoNLL\u201903). Even with a single coarse-grained dataset, our proposed method significantly outperforms w/o coarse, which is trained solely on the fine-grained dataset (i.e. RoBERTaLARGE in Table 2).\nThis indicates the effectiveness of using a wellaligned hierarchy through the F2C mapping matrix and inconsistency filtering. Although we achieve a significant improvement even with a single coarsegrained dataset, we achieve a more stable result with two coarse-grained datasets. This implies that our approach effectively utilizes multiple coarsegrained datasets, although the datasets contain different sets of entity types."
        },
        {
            "heading": "4.6 Analysis",
            "text": "In this section, we experimentally investigate how the F2C mapping matrix and inconsistency filtering improve the accuracy of the proposed model."
        },
        {
            "heading": "4.6.1 F2C Mapping Matrix",
            "text": "Mapping the entity types between the coarse- and fine-grained datasets directly affects the model performance. Hence, we investigate the mapping outcomes between the coarse- and fine-grained datasets. Figures 2 and 3 show the F2C matrices for FewNERD-OntoNotes and FewNERD-CoNLL\u201903, respectively. In both figures, the x- and y-axis represent the fine-grained and coarse-grained entity types, respectively. The colored text indicates the corresponding coarse-grained entity types in FewNERD, which were not used to find the mapping matrix. The mapping is reliable if we compare the y-axis and the colored types (coarse-grained types in Few-NERD). Even without manual annotation of the relationship between coarse- and fine-grained entities, our method successfully obtains reliable mapping from fine-grained to coarse-grained types. Our model can be effectively trained with both types of datasets using accurate F2C mapping.\nFigure 4 provides the F1 scores by varying the hyperparameter k to refine the F2C mapping matrix described in \u00a73.3. \u2018all\u2019 refers to the usage of the complete frequency distribution when creating an\nF2C mapping matrix. We observe that the highest performance is achieved when k is set to 1, and as k increases, the performance gradually decreases. Although the optimal value of k can vary depending on the quality of the coarse-grained data and the performance of the coarse-grained NER model, the results indicate that a good F2C mapping matrix can be obtained by ignoring minor co-occurrences.\nAdditionally, We conducted an experiment by setting the F2C mapping matrix to be learnable and comparing it with our non-learnable F2C matrix. The non-learnable approach showed better\nperformance, hence we adopted this approach for CoFiNER. Detailed analysis and experiment results are in Appendix B.2."
        },
        {
            "heading": "4.6.2 Inconsistency Filtering",
            "text": "We aim to examine whether inconsistency filtering successfully screened out inconsistent entities between OntoNotes and Few-NERD datasets. To conduct this analysis, we describe three inconsistent examples. Table 4 shows the predicted values and target labels, which correspond to the coarse-grained output of the fine-grained NER model trained as described in \u00a73.2 and the golden labels of the coarsegrained dataset.\nThe first example illustrates the inconsistency in entity types between mislabeled entities. In the original coarse-grained dataset, \"Palestinian\" is labeled as NORP, but the model trained on the finegrained dataset predicts \"Palestinian rebellion\" as its appropriate label, EVENT. However, annotators of the OntoNotes labeled \"Palestinian\" as NORP, whereas the fine-grained NER model correctly predicts the highly informative actual label span. The inconsistency caused by a label mismatch between the coarse-grained and fine-grained datasets can result in performance degradation.\nIn the second example, both \"Canada\" and \"Toronto\" are consistently labeled as GPE; thus, the model is not confused when training on these two entities. However, in the case of \"All-star\", we can observe a mismatch. This example in the coarse-grained dataset is labeled O instead of the correct entity type EVENT, indicating a mismatch. Through inconsistency filtering, unlabeled \"Allstar\" is masked out of the training process.\nAs shown in the examples, inconsistency filtering is necessary to mitigate the potential noise arising from mismatched entities. We analyze the filtering results for each coarse-grained label to assess its impact on model performance. Figure 5\nillustrates the correlation between the filtering proportion and the performance improvement for each coarse-grained label mapped to the fine-grained labels. In this figure, a higher filtering proportion indicates a greater inconsistency between the two datasets. F1 improvements indicate the difference in performance when filtering is applied and when it is not. Each data point refers to a fine-grained label mapped onto a coarse-grained label. As the proportion of the filtered entities increases, the F1 scores also increase. These improvements indicate that inconsistency filtering effectively eliminates noisy entities, enabling the model to be well-trained on consistent data only."
        },
        {
            "heading": "5 Conclusion",
            "text": "We proposed CoFiNER, which explicitly leverages the hierarchical structure between coarse- and finegrained types to alleviate the low-resource problem of fine-grained NER. We devised the F2C mapping matrix that allows for fine-grained NER model\ntraining using additional coarse-grained datasets. However, because not all coarse-grained entities are beneficial for the fine-grained NER model, the proposed inconsistency filtering method is used to mask out noisy entities from being used in the model training process. We found through experiments that using a smaller amount of consistent data is better than using a larger amount of data without filtering, thus demonstrating the crucial role of inconsistency filtering. Empirical results confirmed the superiority of CoFiNER over both the supervised and few-shot methods.\nLimitations\nDespite the promising empirical results of this study, there is still a limitation. The main drawback of CoFiNER is that the token-level F2C mapping matrix and inconsistency filtering may not be directly applicable to nested NER tasks. Nested NER involves identifying and classifying certain overlapping entities that exceed the token-level scope. Because CoFiNER addresses fine-grained NER at the token level, it may not accurately capture entities with nested structures. Therefore, applying our token-level approach to nested NER could pose challenges and may require further adaptations or other modeling techniques to effectively handle the hierarchical relations between nested entities."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. NRF2022R1G1A1013549). This work was supported by Institute of Information & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (No. RS2023-00261068, Development of Lightweight Multimodal Anti-Phishing Models and Split-Learning Techniques for Privacy-Preserving Anti-Phishing) and (No.RS-2022-00155885, Artificial Intelligence Convergence Innovation Human Resources Development (Hanyang University ERICA)). This research was also supported by the Ministry of Trade, Industry, and Energy (MOTIE), Korea, under the \u201cProject for Research and Development with Middle Markets Enterprises and DNA(Data, Network, AI) Universities\u201d (Smart Home Based LifeCare Service & Solution Development)(P0024555) supervised by the Korea Institute for Advancement of Technology (KIAT)."
        },
        {
            "heading": "A Sampling Algorithm",
            "text": "Algorithm 1 K\u223c(K+5) sampling algorithm Input: Dataset X, labeled set Y, K Output: Fine-grained dataset DF\n1: Sort classes in Y based on their freq. in X 2: DF \u2190 \u2205; \u25b7 Init the train dataset\n\u25b7 Init the count of entity classes in DF\n3: for i = 1 to |Y| do 4: Count [i] = 0; 5: for i = 1 to |Y| do 6: if \u2200 Count[i\u2032] \u2265K then 7: break; 8: Randomly sample (x, y) \u2208 X s.t. Yi \u2208 y; 9: for j = 1 to |y| do\n10: CountY[j] += 1; 11: if \u2203 Count[i] + CountY[i] > K + 5 then 12: Continue; 13: else 14: DF \u2190 DF \u222a {(x, y)}; 15: Count\u2190 Count + CountY; 16: return DF\nUnlike other tasks, NER involves multiple entity occurrences within a sentence, making it too restrictive to sample an exact count. We adopt a\nK\u223c(K+5)-shot setting to minimize differences in the number of entity types. Additionally, we sample the entity types, starting from those with fewer occurrences to ensure a balanced distribution of multiple entity types within the sentences. Algorithm 1 presents the K-shot sampling algorithm used in this study."
        },
        {
            "heading": "B Additional Experiments",
            "text": "B.1 Generalization of Inconsistency filtering in diverse dataset settings\nTo assess the generalization of the proposed method, we conducted experiments under various coarse- and fine-grained dataset settings.\nFirst, we verified the robustness of inconsistency filtering through experiments using different coarse-grained datasets. The fine-grained dataset, Few-NERD, remains unchanged. Since Few-NERD has both coarse- and fine-grained labels, we constructed a coarse-grained dataset Few-NERD_coarse using the coarse-grained labels from Few-NERD. When compared to Few-NERD_coarse, entities in OntoNotes and CoNLL\u201903 are inconsistently labeled with FewNERD because they were independently created. In Table 5, Few-NERD_coarse exhibits a higher F1 score in the w/o filtering setting due to its consistency with fine-grained labels of Few-NERD. However, the performance improvement achieved with filtering is more substantial in the inconsistent datasets, when compared to the consistent dataset. This result indicates that inconsistency filtering improves performance by filtering out the mismatching labels. Therefore, we have demonstrated the importance of using inconsistency filtering to filter out noise when working with datasets that employ different labeling schemes. Furthermore, by achieving effective performance improvements across various coarse-grained datasets, we have provided evidence of the robustness of the filtering method.\nSecond, we validate the generalization performance through experiments conducted in different coarse- and fine-grained dataset settings. We\nset up the CoNLL\u201903, which has 4 entity types, as the coarse-grained dataset, and the 100-shot OntoNotes, which has a finer label with 19 entity types, as the fine-grained dataset. In Table 6, when compared to the two top-performing models in the main results, RoBERTaLARGE and the state-of-theart PL-Marker, CoFiNER show consistently higher performance. Furthermore, as shown in the ablation study that was conducted following the same methodology in \u00a74.5, CoFiNER exhibited superior performance.\nIn conclusion, through above the two experiments, our method has been demonstrated to work robustly across different coarse- and fine-grained dataset settings.\nB.2 Comparison with learnable F2C mapping matrix\nTo find the optimal F2C mapping matrix, we conducted experiments to explore the impact of making the F2C mapping matrix learnable. We use FewNERD as a fine-grained dataset and OntoNotes as a coarse-grained dataset. Table 7 shows no performance gains when the F2C mapping matrix was set to be learnable. We found that the learnable matrix tends to form a pattern similar to what is shown in Figure 4 with k=all. This result suggests that taking minor co-occurrences into account leads to an overall decrease in performance. Based on this analysis, the non-learnable mapping matrix is used in our experiments."
        }
    ],
    "title": "Enhancing Low-resource Fine-grained Named Entity Recognition by Leveraging Coarse-grained Datasets",
    "year": 2023
}