{
    "abstractText": "Multimodal machine translation (MMT) simultaneously takes the source sentence and a relevant image as input for translation. Since there is no paired image available for the input sentence in most cases, recent studies suggest utilizing powerful text-to-image generation models to provide image inputs. Nevertheless, synthetic images generated by these models often follow different distributions compared to authentic images. Consequently, using authentic images for training and synthetic images for inference can introduce a distribution shift, resulting in performance degradation during inference. To tackle this challenge, in this paper, we feed synthetic and authentic images to the MMT model, respectively. Then we minimize the gap between the synthetic and authentic images by drawing close the input image representations of the Transformer Encoder and the output distributions of the Transformer Decoder. Therefore, we mitigate the distribution disparity introduced by the synthetic images during inference, thereby freeing the authentic images from the inference process. Experimental results show that our approach achieves state-ofthe-art performance on the Multi30K En-De and En-Fr datasets, while remaining independent of authentic images during inference.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wenyu Guo"
        },
        {
            "affiliations": [],
            "name": "Qingkai Fang"
        },
        {
            "affiliations": [],
            "name": "Dong Yu"
        },
        {
            "affiliations": [],
            "name": "Yang Feng"
        }
    ],
    "id": "SP:1fbc90c42466cd0981ac7a4e8cc470e264b79025",
    "references": [
        {
            "authors": [
                "Rameen Abdal",
                "Peihao Zhu",
                "John Femiani",
                "Niloy J. Mitra",
                "Peter Wonka."
            ],
            "title": "Clip2stylegan: Unsupervised extraction of stylegan edit directions",
            "venue": "SIGGRAPH \u201922: Special Interest Group on Computer Graphics and Interactive Techniques Confer-",
            "year": 2022
        },
        {
            "authors": [
                "Iacer Calixto",
                "Qun Liu",
                "Nick Campbell."
            ],
            "title": "Doubly-attentive decoder for multi-modal neural machine translation",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 -",
            "year": 2017
        },
        {
            "authors": [
                "Yen-Chun Chen",
                "Linjie Li",
                "Licheng Yu",
                "Ahmed El Kholy",
                "Faisal Ahmed",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu."
            ],
            "title": "UNITER: universal image-text representation learning",
            "venue": "Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, Au-",
            "year": 2020
        },
        {
            "authors": [
                "Desmond Elliott."
            ],
            "title": "Adversarial evaluation of multimodal machine translation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 2974\u20132978. Association",
            "year": 2018
        },
        {
            "authors": [
                "Desmond Elliott",
                "Stella Frank",
                "Lo\u00efc Barrault",
                "Fethi Bougares",
                "Lucia Specia."
            ],
            "title": "Findings of the second shared task on multimodal machine translation and multilingual image description",
            "venue": "Proceedings of the Second Conference on Machine Transla-",
            "year": 2017
        },
        {
            "authors": [
                "Desmond Elliott",
                "Stella Frank",
                "Khalil Sima\u2019an",
                "Lucia Specia"
            ],
            "title": "Multi30k: Multilingual englishgerman image descriptions",
            "venue": "In Proceedings of the 5th Workshop on Vision and Language,",
            "year": 2016
        },
        {
            "authors": [
                "Desmond Elliott",
                "\u00c1kos K\u00e1d\u00e1r."
            ],
            "title": "Imagination improves multimodal translation",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei, Taiwan, November 27 - December 1, 2017 - Volume 1: Long",
            "year": 2017
        },
        {
            "authors": [
                "Qingkai Fang",
                "Yang Feng."
            ],
            "title": "Neural machine translation with phrase-level universal visual representations",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland,",
            "year": 2022
        },
        {
            "authors": [
                "Qingkai Fang",
                "Yang Feng."
            ],
            "title": "Understanding and bridging the modality gap for speech translation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Qingkai Fang",
                "Rong Ye",
                "Lei Li",
                "Yang Feng",
                "Mingxuan Wang."
            ],
            "title": "Stemm: Self-learning with speechtext manifold mixup for speech translation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Hongcheng Guo",
                "Jiaheng Liu",
                "Haoyang Huang",
                "Jian Yang",
                "Zhoujun Li",
                "Dongdong Zhang",
                "Zheng Cui."
            ],
            "title": "LVP-M3: Language-aware visual prompt for multilingual multimodal machine translation",
            "venue": "Proceedings of the 2022 Conference on Empirical Meth-",
            "year": 2022
        },
        {
            "authors": [
                "Baijun Ji",
                "Tong Zhang",
                "Yicheng Zou",
                "Bojie Hu",
                "Si Shen."
            ],
            "title": "Increasing visual awareness in multimodal neural machine translation from an information theoretic perspective",
            "venue": "CoRR, abs/2210.08478.",
            "year": 2022
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila."
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "CoRR, abs/1812.04948.",
            "year": 2018
        },
        {
            "authors": [
                "Wonjae Kim",
                "Bokyung Son",
                "Ildoo Kim."
            ],
            "title": "Vilt: Vision-and-language transformer without convolution or region supervision",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume",
            "year": 2021
        },
        {
            "authors": [
                "Matt J. Kusner",
                "Yu Sun",
                "Nicholas I. Kolkin",
                "Kilian Q. Weinberger."
            ],
            "title": "From word embeddings to document distances",
            "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of",
            "year": 2015
        },
        {
            "authors": [
                "Bei Li",
                "Chuanhao Lv",
                "Zefan Zhou",
                "Tao Zhou",
                "Tong Xiao",
                "Anxiang Ma",
                "Jingbo Zhu."
            ],
            "title": "On vision features in multimodal machine translation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2022
        },
        {
            "authors": [
                "Yi Li",
                "Rameswar Panda",
                "Yoon Kim",
                "Chun-Fu Richard Chen",
                "Rog\u00e9rio Feris",
                "David D. Cox",
                "Nuno Vasconcelos."
            ],
            "title": "VALHALLA: visual hallucination for machine translation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR",
            "year": 2022
        },
        {
            "authors": [
                "Yunlong Liang",
                "Fandong Meng",
                "Jinan Xu",
                "Yufeng Chen",
                "Jie Zhou."
            ],
            "title": "MSCTD: A multimodal sentiment chat translation dataset",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL",
            "year": 2022
        },
        {
            "authors": [
                "Huan Lin",
                "Fandong Meng",
                "Jinsong Su",
                "Yongjing Yin",
                "Zhengyuan Yang",
                "Yubin Ge",
                "Jie Zhou",
                "Jiebo Luo."
            ],
            "title": "Dynamic context-guided capsule network for multimodal machine translation",
            "venue": "MM \u201920: The 28th ACM International Conference on Multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Quanyu Long",
                "Mingxuan Wang",
                "Lei Li."
            ],
            "title": "Generative imagination elevates machine translation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen."
            ],
            "title": "GLIDE: towards photorealistic image generation and editing with text-guided diffusion models",
            "venue": "International",
            "year": 2022
        },
        {
            "authors": [
                "Myle Ott",
                "Sergey Edunov",
                "Alexei Baevski",
                "Angela Fan",
                "Sam Gross",
                "Nathan Ng",
                "David Grangier",
                "Michael Auli."
            ],
            "title": "fairseq: A fast, extensible toolkit for sequence modeling",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Asso-",
            "year": 2019
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Ru Peng",
                "Yawen Zeng",
                "Junbo Zhao."
            ],
            "title": "Distill the image to nowhere: Inversion knowledge distillation for multimodal machine translation",
            "venue": "CoRR, abs/2210.04468.",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen."
            ],
            "title": "Hierarchical textconditional image generation with CLIP latents",
            "venue": "CoRR, abs/2204.06125.",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever."
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24",
            "year": 2021
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer."
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans,",
            "year": 2022
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-",
            "year": 2016
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "C\u00e9dric Villani."
            ],
            "title": "Optimal transport: old and new",
            "venue": "volume 338. Springer.",
            "year": 2009
        },
        {
            "authors": [
                "Patrick von Platen",
                "Suraj Patil",
                "Anton Lozhkov",
                "Pedro Cuenca",
                "Nathan Lambert",
                "Kashif Rasul",
                "Mishig Davaadorj",
                "Thomas Wolf."
            ],
            "title": "Diffusers: Stateof-the-art diffusion models",
            "venue": "https://github.com/ huggingface/diffusers.",
            "year": 2022
        },
        {
            "authors": [
                "Zhiyong Wu",
                "Lingpeng Kong",
                "Wei Bi",
                "Xiang Li",
                "Ben Kao."
            ],
            "title": "Good for misconceived reasons: An empirical revisiting on the need for visual context in multimodal machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Tao Xu",
                "Pengchuan Zhang",
                "Qiuyuan Huang",
                "Han Zhang",
                "Zhe Gan",
                "Xiaolei Huang",
                "Xiaodong He."
            ],
            "title": "Attngan: Fine-grained text to image generation with attentional generative adversarial networks",
            "venue": "2018 IEEE Conference on Computer Vision and Pattern",
            "year": 2018
        },
        {
            "authors": [
                "Zhe Yang",
                "Qingkai Fang",
                "Yang Feng."
            ],
            "title": "Lowresource neural machine translation with cross-modal alignment",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10134\u201310146, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Shaowei Yao",
                "Xiaojun Wan."
            ],
            "title": "Multimodal transformer for multimodal machine translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 4346\u20134350. Associa-",
            "year": 2020
        },
        {
            "authors": [
                "Junjie Ye",
                "Junjun Guo",
                "Yan Xiang",
                "Kaiwen Tan",
                "Zhengtao Yu."
            ],
            "title": "Noise-robust cross-modal interactive learning with text2image mask for multimodal neural machine translation",
            "venue": "Proceedings of the 29th International Conference on Computational",
            "year": 2022
        },
        {
            "authors": [
                "Shaolei Zhang",
                "Qingkai Fang",
                "Zhuocheng Zhang",
                "Zhengrui Ma",
                "Yan Zhou",
                "Langlin Huang",
                "Mengyu Bu",
                "Shangtong Gui",
                "Yunji Chen",
                "Xilin Chen",
                "Yang Feng"
            ],
            "title": "Bayling: Bridging cross-lingual alignment and instruction following through interactive",
            "year": 2023
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Kehai Chen",
                "Rui Wang",
                "Masao Utiyama",
                "Eiichiro Sumita",
                "Zuchao Li",
                "Hai Zhao."
            ],
            "title": "Neural machine translation with universal visual representation",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis",
            "year": 2020
        },
        {
            "authors": [
                "Yan Zhou",
                "Qingkai Fang",
                "Yang Feng."
            ],
            "title": "CMOT: Cross-modal mixup via optimal transport for speech translation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7873\u20137887, Toronto,",
            "year": 2023
        },
        {
            "authors": [
                "Yucheng Zhou",
                "Guodong Long."
            ],
            "title": "Improving cross-modal alignment for text-guided image inpainting",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Multimodal machine translation (MMT) integrates visual information into neural machine translation (NMT) to enhance language understanding with visual context, leading to improvement in translation quality (Li et al., 2022a; Guo et al., 2022; Fang and Feng, 2022). However, most existing MMT models require an associated image with the input sentence, which is difficult to satisfy at inference and limits the usage scenarios. Hence, overcoming\n\u2217Corresponding author: Yang Feng. Code is publicly available at https://github.com/\nictnlp/SAMMT.\nthe dependence on images at inference stands as a pivotal challenge in MMT.\nTo tackle this challenge, some researchers have proposed to employ text-to-image generation models (Long et al., 2021; Li et al., 2022b) to generate synthetic images to use as the associated image for the source text at inference, while during training MMT models still use the available authentic images as the visual context to generate translation. Despite the remarkable capabilities of textto-image generation models in generating highly realistic images from textual descriptions (Ramesh et al., 2022; Rombach et al., 2022), the synthetic images may exhibit different distribution patterns compared to the authentic (ground truth) images. As shown in Figure 1, the synthetic images may depict counterfactual scenes, omit information in the text, or add information irrelevant from the text. Therefore, training with authentic images but inferring with synthetic images causing the distribution shift in the images, leading to a decline in translation performance.\nIn this paper, we embrace the utilization of textto-image generation models in MMT and propose a method to bridge the gap between synthetic and authentic images. In our method, we also introduce synthetic images during training and feed synthetic images and authentic images to the MMT model, respectively. Then we minimize the gap\nbetween the two images by drawing close the following two terms of the MMT model when the two images are inputted respectively: the input image representations to the Transformer Encoder and the output distributions from the Transformer Decoder. Regarding the input representations, we leverage the Optimal Transport (OT) theory to mitigate the disparity of the representations. Kullback-Leibler (KL) divergence is utilized to ensure the consistency of the output distributions. Consequently, we effectively eliminate the disparities introduced by synthetic images used during the inference process, thereby freeing the authentic images from the inference process.\nExperimental results show that our approach achieves state-of-the-art performance on the Multi30K En-De and En-Fr datasets, even in the absence of authentic images. Further analysis demonstrates that our approach effectively enhances the representation and prediction consistency between synthetic and authentic images."
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 Stable Diffusion",
            "text": "Stable Diffusion (Rombach et al., 2022) is a textto-image generation model based on the latent diffusion model (LDM), enabling the generation of highly realistic images from textual descriptions. It mainly consists of a VAE model, a U-Net model and a CLIP text encoder.\nThe training of the conditional LDM involves a forward diffusion process and a backward denoising process. During the diffusion process, the images are firstly compressed into the latent space using the VAE encoder, followed by the addition of Gaussian noise to the latent representations. In the denoising process, the U-Net is iteratively employed to progressively eliminate noise from the noisy representations. To integrate textual guidance during the denoising process, the text description is encoded using the CLIP text encoder and incorporated into the U-Net through the cross-attention mechanism. Finally, the VAE decoder reconstructs images from the latent representations. By leveraging image-text pairs, the conditional LDM can be optimized by:\nLldm = EE(a),x,\u03f5\u223cN (0,1),t [ \u2225\u03f5\u2212 \u03f5\u03b8(zt, t, \u03c4\u03b8(x))\u222522 ] , (1)\nwhere x and a represent the input text and image, respectively. \u03f5 denotes the Gaussian noise. t and\nzt refer to the time step and the latent representation at the t-th time step. E represents the VAE encoder. \u03f5\u03b8 and \u03c4\u03b8 represent the U-Net and the CLIP text encoder, respectively. In this paper, we utilize the pre-trained Stable Diffusion model to generate images for the source sentence."
        },
        {
            "heading": "2.2 Multimodal Transformer",
            "text": "Multimodal Transformer (Yao and Wan, 2020) is a powerful architecture designed for MMT. It replaces the self-attention layer in the Transformer (Vaswani et al., 2017) Encoder with a multimodal self-attention layer, which can learn multimodal representations from text representations under the guidance of the image-aware attention.\nFormally, given the input text x and image a, the textual and visual representations can be denoted as Hx = (hx1 , .., hxN ) and H a = (ha1, ..., h a P ), respectively. Here, N represents the length of the source sentence, and P denotes the size of visual features. In each multimodal self-attention layer, the textual and visual representations are concatenated together as the query vectors:\nH\u0303 = [Hx;HaW a] \u2208 R(N+P )\u00d7d, (2)\nwhere W a are learnable weights. The key and value vectors are the textual representations Hx. Finally, the output of the multimodal self-attention layer is calculated as follows:\nci = N\u2211 j=1 \u03b1\u0303ij(h x jW V ), (3)\nwhere \u03b1\u0303ij is the weight coefficient computed by the softmax function:\n\u03b1\u0303ij = softmax\n( (h\u0303iW Q)(hxjW K)T\n\u221a dk\n) , (4)\nwhere WQ,WK , and W V are learnable weights, and dk is the dimension of the key vector. The output of the last Multimodal Transformer Encoder layer is fed into the Transformer Decoder to generate the target translation. In this paper, we use the Multimodal Transformer as our base model architecture."
        },
        {
            "heading": "3 Method",
            "text": "The original Multimodal Transformer typically requires a paired image of the input sentence to provide visual context, which is often impractical in\nmany scenarios. To address this limitation, we leverage the Stable Diffusion model during inference to generate a synthetic image based on the source sentence. However, as synthetic images may follow different distributions compared to the authentic (ground truth) images, using authentic images for training and synthetic images for inference could result in a performance decline in translation quality due to the distribution shift in the images. Next, we will introduce our proposed approach for bridging the gap between synthetic and authentic images."
        },
        {
            "heading": "3.1 Training with both Synthetic and",
            "text": "Authentic Images\nTo address the issue of distribution shifts in images, we propose training the MMT model using a combination of synthetic and authentic images. Specifically, we enhance the training set by augmenting each sentence with an additional synthetic image, and train the model with both synthetic and authentic images. By incorporating synthetic images during training, we can alleviate the inconsistency between training and inference phases.\nTechnically, we utilize the Stable Diffusion model to generate an image s corresponding to the input text x. We employ the pre-trained CLIP (Radford et al., 2021) image encoder to encode images into visual embeddings. Subsequently, we incorporate a feed-forward network (FFN) to adjust the dimension of visual embeddings to align with the dimension of word embeddings.\nHa = FFN(CLIPImageEncoder(a)), (5) Hs = FFN(CLIPImageEncoder(s)), (6)\nwhere Ha \u2208 R1\u00d7d and Hs \u2208 R1\u00d7d denote visual representations of the authentic and synthetic images, respectively, and d is the dimension of word embeddings. Next, Ha and Hs are fed into the Multimodal Transformer to perform the MMT task.\nLet us denote the target sentence as y = (y1, ..., yM ). The loss functions for training with synthetic and authentic images can be formulated as follows:\nLsyn = \u2212 M\u2211 j=1 log p (yj |y<j , x, s) , (7) Laut = \u2212 M\u2211 j=1 log p (yj |y<j , x, a) . (8)\nThe final training objective is as follows:\nLtrans = 1\n2 (Lsyn + Laut) . (9)"
        },
        {
            "heading": "3.2 Consistency Training",
            "text": "Incorporating synthetic images into the training process helps mitigate the distribution shift between training and inference. However, the model\u2019s behaviour may still differ when handling synthetic and authentic images. To enhance the inherent consistency of the model when dealing with these two types of images, we introduce two consistency losses during training. Optimal transport loss is a common training object used to encourage word-image alignment in vision-language pretraining tasks (Chen et al., 2020; Kim et al., 2021). The Kullback\u2013Leibler divergence loss can also be used to improve visual representation consistency in text-guided image inpainting tasks (Zhou and Long, 2023). Following previous works, at the encoder side, we incorporate an optimal transportbased training objective to encourage consistency between representations of synthetic and authentic\nimages. At the decoder side, we encourage consistency between the predicted target distributions based on both types of images. Figure 2 illustrates the overview of our proposed method.\nRepresentation Consistency Intuitively, we hope the visual representations of synthetic and authentic images to be similar. However, directly optimizing the cosine similarity or L2 distance between Hs and Ha may not be optimal, as the visual representations of two images may exhibit different distributions across the feature dimension. Therefore, we propose measuring the representation similarity between synthetic and authentic images based on the optimal transport distance.\nThe Optimal Transport (OT) problem is a wellknown mathematical problem that seeks to find the optimal mapping between two distributions with minimum cost. It has been widely employed as a loss function in various machine learning applications. Following the principles of optimal transport theory (Villani, 2009), the two probability distributions P and Q can be formulated as:\nP = {(wi,mi)}Ki=1, s.t. \u2211 i mi = 1;\nQ = {(w\u0302j , m\u0302j)}K\u0302j=1, s.t. \u2211 j m\u0302j = 1, (10)\nwhere each data point wi has a probability mass mi \u2208 [0,\u221e). Given a transfer cost function c (wi, wj), and using Tij to represent the mass transported from wi to w\u0302j , the transport cost can be defined as:\nD (P,Q) = min T\u22650 \u2211 i,j Tijc (wi, w\u0302j) ,\ns.t. K\u0302\u2211 j=1 Tij = mi,\u2200i \u2208 {1, ...,K},\nK\u2211 i=1 Tij = m\u0302j , \u2200j \u2208 {1, ..., K\u0302}.\n(11)\nWe regard two visual representations Hs = (hs1, ..., h s d) and H a = (ha1, ..., h a d) as two independent distributions. Each hsi and h a j here is a scalar value. We formulate the distance between Hs and\nHa as an optimal transport problem, given by:\nD (Hs,Ha) = min T\u22650 \u2211 i,j Tijc ( hsi , h a j ) , s.t. d\u2211\nj=1\nTij = mi, \u2200i \u2208 {1, ..., d},\nd\u2211 i=1 Tij = m\u0302j ,\u2200j \u2208 {1, ..., d}.\n(12)\nWe use the L2 distance as the cost function c, and the probability mass is defined as:\nmi = |hsi |\u2211 i |hsi | , m\u0302j = |haj |\u2211 j |haj | .\n(13)\nFollowing Kusner et al. (2015), we remove the second constraint to obtain a lower bound of the accurate OT solution. The relaxed OT improves the training speed without performance decline, which can be defined as:\nD (Hs,Ha) = min T\u22650 \u2211 i,j Tijc ( hsi , h a j ) , s.t. d\u2211\nj=1\nTij = mi, \u2200i \u2208 {1, ..., d}. (14)\nThe final OT loss is defined as follows:\nLot = 1\n2 (D (Hs,Ha) +D (Ha,Hs)) , (15)\nPrediction Consistency In addition to ensuring the consistency of representations between synthetic and authentic images, we also aim to enhance the model\u2019s consistency in the predicted probability distributions based on both two types of images. Inspired by previous works on speech translation (Fang et al., 2022; Zhou et al., 2023; Fang and Feng, 2023), we introduce a prediction consistency loss, defined as the Kullback-Leibler (KL) divergence between the two probability distributions:\nLkl = M\u2211 j=1 KL [p(yj |y<j , x, s)\u2225p(yj |y<j , x, a)] .\n(16) Finally, the training objective is as follows:\nL = Ltrans + \u03bbLkl + \u03b3Lot, (17)\nwhere \u03bb and \u03b3 are the hyperparameters that control the contribution of the KL divergence loss and the optimal transport loss."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "We conduct experiments on the Multi30K dataset (Elliott et al., 2016). Multi30K contains bilingual parallel sentence pairs with image annotations, where the English description of each image is manually translated into German (De) and French (Fr). The training and validation sets consist of 29,000 and 1,014 instances, respectively. We report the results on the Test2016, Test2017 and ambiguous MSCOCO test sets (Elliott et al., 2017), which contain 1,000, 1,000 and 461 instances respectively. We apply the byte-pair encoding (BPE) (Sennrich et al., 2016) algorithm with 10K merge operations to segment words into subwords, resulting in a shared vocabulary of 9,712 and 9,544 entries for the En-De and En-Fr translation tasks, respectively."
        },
        {
            "heading": "4.2 System Settings",
            "text": "Stable Diffusion We build our own inference pipeline with diffusers (von Platen et al., 2022). We use the pre-trained VAE and U-Net models1.\n1https://huggingface.co/CompVis/ stable-diffusion-v1-4\nThe pre-trained CLIP (Radford et al., 2021) text encoder2 is used to encode the input text. The number of denoising steps is set to 50. The seed of the generator is set to 0. The scale of classifier-free guidance is 7.5 and the batch size is 1.\nVisual Features For both synthetic and authentic images, we use the pre-trained ViT-B/32 CLIP model3 to extract the visual features, which have a dimension of [1\u00d7 512].\nTranslation Model We follow Wu et al. (2021) to conduct experiments with the Transformer-Tiny configuration, which proves to be more effective on the small dataset like Multi30K. The translation model consists of 4 encoder and decoder layers. The hidden size is 128 and the filter size of FFN is 256. There are 4 heads in the multi-head selfattention module. The dropout is set to 0.3 and the label smoothing is 0.1. Our implementation is based on the open-source framework fairseq4 (Ott et al., 2019). Each training batch contains 2,048 tokens and the update frequency is set to 4.\n2https://huggingface.co/openai/ clip-vit-large-patch14\n3https://github.com/openai/CLIP 4https://github.com/facebookresearch/fairseq\nFor evaluation, we average the last 10 checkpoints following previous works. The beam size is set to 5. We measure the results with BLEU (Papineni et al., 2002) scores for all test sets. All models are trained and evaluated using 1 Tesla V100 GPU. We set the KL loss weight \u03bb to 0.5. The OT loss weight \u03b3 is set to 0.1 for En-De and 0.9 for En-Fr."
        },
        {
            "heading": "4.3 Basline Systems",
            "text": "We conduct experiments in two different settings: image-must and image-free. In the image-must setting, authentic images from the test set are used to provide the visual context. In contrast, the authentic images are not used in the image-free setting. We implement the following three baseline systems for comparision:\nTEXT-ONLY Text-only Transformer is implemented under the Transformer-Tiny configuration. It follows an encoder-decoder paradigm (Vaswani et al., 2017), taking only texts as input.\nMULTITRANS Multimodal Transformer (Yao and Wan, 2020) trained on the original Multi30K dataset, as described in Section 2.2.\nINTEGRATED Multimodal Transformer trained on our augmented Multi30K dataset, where each sentence is paired with an authentic image and a synthetic image, as described in Section 3.1.\nBesides, in the image-must setting, we include DCCN (Lin et al., 2020), RMMT (Wu et al., 2021), Doubly-ATT (Calixto et al., 2017), Gated Fusion (Wu et al., 2021), Selective Attention (Li et al., 2022a), Noise-robust (Ye et al., 2022), and VALHALLA (Li et al., 2022b) for comparison. In the image-free setting, we include UVR-NMT (Zhang et al., 2020), Imagination (Elliott and K\u00e1d\u00e1r, 2017), Distill (Peng et al., 2022), and VALHALLA (Li et al., 2022b) for comparison."
        },
        {
            "heading": "4.4 Main Results on the Multi30K Dataset",
            "text": "Table 1 summarizes the results in both image-must and image-free settings. Each model is evaluated on three test sets for two language pairs.\nFirstly, our method and the majority of the baseline systems demonstrate a substantial performance advantage over the TEXT-ONLY baseline, underscoring the importance of the visual modality.\nSecondly, for the MULTITRANS baseline, it is evident that the synthetic and authentic images yield different translation results. Overall, the authentic images outperform the synthetic images. Directly substituting authentic images with synthetic images results in a performance decline due to the distribution shift in images.\nThirdly, for our INTEGRATED baseline, we incorporate synthetic images into the training process. Experimental results show some improvements compared to the MULTITRANS baseline. The utilization of synthetic or authentic images during inference does not significantly affect the results, indicating the effectiveness of this approach in addressing the distribution shift between training and inference.\nFinally, our method significantly outperforms all baseline systems in both image-must and image-free settings. It achieves state-of-the-art performance on all test sets, while remaining independent of the authentic images. The superior performance demonstrates the effectiveness of encouraging the representation and prediction consistency between synthetic and authentic images in the MMT training."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Effects of Training Objectives",
            "text": "To investigate the impact of the KL divergence (KL) loss and the optimal transport (OT) loss, we conduct ablation experiments on the training objectives. The results are presented in Table 2. When only applying the OT loss (#2), we observe a performance decline. When only using the KL loss (#3), we observe an improvement of 0.64 BLEU, highlighting the benefits of promoting prediction consistency. When both KL loss and OT loss are utilized (#4), we achieve a more significant improvement\nof 1.10 BLEU, demonstrating the benefits of simultaneously encouraging the representation and prediction consistency."
        },
        {
            "heading": "5.2 Representation Consistency",
            "text": "To investigate the effectiveness of our method in aligning the representations of synthetic and authentic images, we measure the cosine similarity between their respective representations (Hs and Ha) on the test set. Specifically, we measure the cosine similarity of the visual representations after the shared-weights FFN network on Test2016 of En-De translation task during inference. Table 3 presents the results. For the MULTITRANS baseline, a significant disparity exists between the visual representations of synthetic and authentic images. However, when the synthetic images are integrated into the training process (INTEGRATED), the similarity between synthetic and authentic images increases to 86.60%. In contrast, our method achieves an impressive similarity of 100.00%, highlighting the effectiveness of our approach in bridging the representation gap between synthetic and authentic images. The optimal transport loss plays a crucial role in reducing the representation gap."
        },
        {
            "heading": "5.3 Apply Our Method to Other Model Architecture",
            "text": "Our approach focuses on improving the training methodology for MMT, which is not restricted to a specific model architecture. To validate the generality of our method, we conduct experiments using an alternative model architecture: Selective Attention\n(SELECTIVE) (Li et al., 2022a), which incorporating visual representations with a selective attention module and a gated fusion module. We apply the optimal transport loss before the selective attention module, and apply the KL loss in the decoder. The model settings and hyperparameters are the same as our experiments on the Multimodal Transformer.\nThe results are shown in Table 4. For the INTEGRATED baseline, integrating synthetic images into the training process can eliminate the gap and bring slight improvements. When we add the KL loss and OT loss during training, we observe a 1.43 BLEU improvements compared with the SELECTIVE baseline, demonstrating the effectiveness and generalizability of our method across different model architectures."
        },
        {
            "heading": "5.4 Comparison with Regularization Methods",
            "text": "We employ the Stable Diffusion model to generate images from textual descriptions as visual contexts. To verify the necessity of the text-to-image generation model, we compare its performance with other two regularization methods: RANDOM and NOISE. RANDOM means shuffling the correspondences between synthetic images and textual descriptions in the training set. NOISE means using noise vectors as the visual representations. Results in Table 5 show that both two strategies perform worse than our approach, demonstrating that our superior performance is not solely attributed to regularization effects but rather to the utilization of appropriate images and semantic information."
        },
        {
            "heading": "5.5 Comparison with Other Loss Functions",
            "text": "We measure the representation similarity between synthetic and authentic images, and minimize their distance during training. To assess the significance of the optimal transport loss, we conduct experiments using various loss functions. Table 6 presents the results when substituting the optimal transport loss with the COSINE embedding loss and L2 loss functions in order to improve representation consistency. The hyperparameters are the\nsame as our experiments on the optimal transport loss. The drop of results indicates the effectiveness of the optimal transport loss in mitigating the representation disparity."
        },
        {
            "heading": "5.6 Results on Different Languages and Datasets",
            "text": "To verify that our method can be utilized in different language pairs and datasets, we conduct experiments on the En-Cs task of the Multi30K dataset and the En-De translation task of the MSCTD dataset (Liang et al., 2022). The MSCTD dataset is a multimodal chat translation dataset, including 20,240, 5,063 and 5,047 instances for the training, validation and test sets respectively. The results are shown in Table 7. Our method still achieves significant improvements among all the baselines on the En-Cs task and the MSCTD dataset, showing the generality of our method among different languages and datasets."
        },
        {
            "heading": "5.7 Incongruent Decoding",
            "text": "We follow method (Elliott, 2018) to use an adversarial evaluation method to test if our method is more sensitive to the visual context. We set the congruent image\u2019s feature to all zeros. Then we observe the value \u2206BLEU by calculating the difference between the congruent data and the incongruent one. A larger value means the model is more sensitive to the image context. As shown in Table 8, we conduct experiment on the three test sets of the En-De translation task and calculate the average incongruent results. Notably, our method achieves the highest score, providing substantial evidence of its sensitivity to visual information."
        },
        {
            "heading": "5.8 Case Study",
            "text": "Table 9 presents two translation cases in the image-free scenario involving the four systems. In the first case, our model accurately translates the phrase \"im maul\" (in the mouth) and \"rennt im schnee\" (run in the snow), whereas the other systems provide incomplete translations. In the second\ncase, the phrases \"spielt\" (play) and \"auf einer roten gitarre\" (on a red guitar) are correctly translated by our proposed model while other systems translate inaccurately. To verify that our system benefits from the incorporation of visual information, we also compute the BLEU score for each sentence in the En-De Test2016 test set and conduct a comparison between our model and the TEXT-ONLY baseline. Among 1,000 data instances, 34% achieve higher blue scores upon incorporating visual information, whereas 24.9% exhibit lower blue scores. We also manually annotated the En-De Test2016 test set, enlisting a German language expert who is certified at the professional eighth level for the annotation task. The annotation results reveal that 35.7% of the instances benefit from the visual features, while 30.8% show no improvement from visual features. These results demonstrate that our model enhances translation performance when visual information is incorporated, further confirming the improvement in translation results achieved by aligning synthetic and authentic images."
        },
        {
            "heading": "6 Related Work",
            "text": ""
        },
        {
            "heading": "6.1 Multimodal Machine Translation",
            "text": "Multimodal Machine translation (MMT) has drawn great attention in the research community. Existing methods mainly focus on integrating visual information into Neural Machine Translation (NMT) (Zhang et al., 2023). Multimodal Transformer (Yao and Wan, 2020) employs a graph perspective to fuse visual information into the translation model. Gated and RMMT, introduced by Wu et al. (2021), are two widely-used architectures for integrating visual and textual modalities. Selective Attention (Li et al., 2022a) is a method that utilizes texts to select useful visual features. Additionally, Ji et al. (2022) proposes additional training objectives to enhance the visual awareness of the MMT model. Yang et al. (2022) introduces cross-modal contrastive learning to enhance low-resource NMT.\nIn addition to methods that rely on authentic images, there is a growing interest in developing\nMMT systems in image-free scenarios due to the difficulty of obtaining paired images during inference. The image retrieval-based approaches (Zhang et al., 2020; Fang and Feng, 2022) utilize images retrieved based on textual information as visual representations. The proposed method (Li et al., 2022b) generates discrete visual representations from texts and incorporate them in the training and inference process. Peng et al. (2022) proposes distillation techniques to transfer knowledge from image representations to text representations, enabling the acquisition of useful multimodal features during inference. In this paper, we focus on using synthetic visual representations generated from texts and aim to bridge the gap between synthetic and authentic images."
        },
        {
            "heading": "6.2 Text-to-image Generation",
            "text": "Text-to-image generation models have made significant progress in generating highly realistic images from textual descriptions. Early approaches (Karras et al., 2018; Xu et al., 2018) rely on training Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) to generate images. CLIP (Radford et al., 2021), a powerful vision-and-language model, has also been utilized to guide the image generation process in various methods (Ramesh et al., 2022; Abdal et al., 2022). Recently, researchers have achieved impressive results in zeroshot text-to-image generation scenarios, where images are generated based on textual descriptions without specific training data (Ramesh et al., 2021; Yu et al., 2022). The adoption of diffusion-based methods (Nichol et al., 2022; Saharia et al., 2022) has further pushed the boundaries of text-to-image\ngeneration, resulting in high-quality image synthesis. In this work, we employ the Stable Diffusion (Rombach et al., 2022) model, a diffusion-based approach, as our text-to-image generation model to generate content-rich images from texts."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we strive to address the disparity between synthetic and authentic images in multimodal machine translation. Firstly, we feed synthetic and authentic images to the MMT model, respectively. Secondly, we minimize the gap between the synthetic and authentic images by drawing close the input image representations of the Transformer Encoder and the output distributions of the Transformer Decoder through two loss functions. As a result, we mitigate the distribution disparity introduced by the synthetic images during inference, thereby freeing the authentic images from the inference process. Through extensive experiments, we demonstrate the effectiveness of our proposed method in improving the quality and coherence of the translation results.\nLimitations\nThe utilization of the text-to-image generation process may introduce additional computational overhead. Moreover, our method is currently limited to translation tasks from English to other languages, as the pre-trained text-to-image model lacks support for languages other than English. The application of our method to other tasks and languages remains unexplored, and we consider these limitations as potential areas for future research."
        }
    ],
    "title": "Bridging the Gap between Synthetic and Authentic Images for Multimodal Machine Translation",
    "year": 2023
}