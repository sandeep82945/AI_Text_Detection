{
    "abstractText": "One of the fundamental goals in code search is to retrieve a functionally correct code for a given natural language query. As annotating for correctness requires executing test cases (i.e. obtaining execution feedback), existing code search training datasets approximate text-code co-occurrences as positive execution feedback. However, this approximation may misalign models\u2019 retrieval decisions from ground-truth correctness. To address such limitation, we propose Code Intervention-based Reinforcement Learning (CIRL) that perturbs training code to result in misalignment (i.e. code intervention), then tests models\u2019 decisions and corrects them with the execution feedback by reinforcement learning. The first technical contribution of CIRL is to induce the execution feedback from perturbation, without actual execution. Secondly, CIRL introduces structural perturbations using abstract syntax trees, going beyond simple lexical changes. Experimental results on various datasets demonstrate the effectiveness of CIRL compared to conventional approaches.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hojae Han"
        },
        {
            "affiliations": [],
            "name": "Minsoo Kim"
        },
        {
            "affiliations": [],
            "name": "Seung-won Hwang"
        },
        {
            "affiliations": [],
            "name": "Nan Duan"
        },
        {
            "affiliations": [],
            "name": "Shuai Lu"
        }
    ],
    "id": "SP:e21a6729f6b267c221e19a486777a7649dc1bcba",
    "references": [
        {
            "authors": [
                "Sid Black",
                "Leo Gao",
                "Phil Wang",
                "Connor Leahy",
                "Stella Biderman"
            ],
            "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow",
            "year": 2021
        },
        {
            "authors": [
                "Nghi DQ Bui",
                "Yijun Yu",
                "Lingxiao Jiang."
            ],
            "title": "Selfsupervised contrastive learning for code retrieval and summarization via semantic-preserving transformations",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in",
            "year": 2021
        },
        {
            "authors": [
                "Nitay Calderon",
                "Eyal Ben-David",
                "Amir Feder",
                "Roi Reichart."
            ],
            "title": "DoCoGen: Domain counterfactual generation for low resource domain adaptation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Carta",
                "Cl\u00e9ment Romac",
                "Thomas Wolf",
                "Sylvain Lamprier",
                "Olivier Sigaud",
                "Pierre-Yves Oudeyer."
            ],
            "title": "Grounding large language models in interactive environments with online reinforcement learning",
            "venue": "Proceedings of the 40th International Conference",
            "year": 2023
        },
        {
            "authors": [
                "Hannah Chen",
                "Yangfeng Ji",
                "David Evans."
            ],
            "title": "Balanced adversarial training: Balancing tradeoffs between fickleness and obstinacy in NLP models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 632\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Seungtaek Choi",
                "Myeongho Jeong",
                "Hojae Han",
                "Seung-won Hwang."
            ],
            "title": "C2l: Causally contrastive learning for robust text classification",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.",
            "year": 2022
        },
        {
            "authors": [
                "Yangruibo Ding",
                "Luca Buratti",
                "Saurabh Pujar",
                "Alessandro Morari",
                "Baishakhi Ray",
                "Saikat Chakraborty."
            ],
            "title": "Towards learning (dis)-similarity of source code from program contrasts",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Zhangyin Feng",
                "Daya Guo",
                "Duyu Tang",
                "Nan Duan",
                "Xiaocheng Feng",
                "Ming Gong",
                "Linjun Shou",
                "Bing Qin",
                "Ting Liu",
                "Daxin Jiang",
                "Ming Zhou."
            ],
            "title": "CodeBERT: A pre-trained model for programming and natural languages",
            "venue": "Findings of the Association",
            "year": 2020
        },
        {
            "authors": [
                "Xiaodong Gu",
                "Hongyu Zhang",
                "Sunghun Kim."
            ],
            "title": "Deep code search",
            "venue": "2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE), pages 933\u2013944. IEEE.",
            "year": 2018
        },
        {
            "authors": [
                "Daya Guo",
                "Shuai Lu",
                "Nan Duan",
                "Yanlin Wang",
                "Ming Zhou",
                "Jian Yin."
            ],
            "title": "UniXcoder: Unified crossmodal pre-training for code representation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2022
        },
        {
            "authors": [
                "Hojae Han",
                "Seungtaek Choi",
                "Myeongho Jeong",
                "Jin-woo Park",
                "Seung-won Hwang."
            ],
            "title": "Counterfactual generative smoothing for imbalanced natural language classification",
            "venue": "Proceedings of the 30th ACM International Conference on Information & Knowl-",
            "year": 2021
        },
        {
            "authors": [
                "Hojae Han",
                "Seung-won Hwang",
                "Shuai Lu",
                "Nan Duan",
                "Seungtaek Choi."
            ],
            "title": "Towards compositional generalization in code search",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10743\u201310750, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Saurav Kadavath",
                "Mantas Mazeika",
                "Akul Arora",
                "Ethan Guo",
                "Collin Burns",
                "Samir Puranik",
                "Horace He",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring coding challenge competence with APPS",
            "venue": "Proceedings of the Neural",
            "year": 2021
        },
        {
            "authors": [
                "Yujing Hu",
                "Qing Da",
                "Anxiang Zeng",
                "Yang Yu",
                "Yinghui Xu."
            ],
            "title": "Reinforcement learning to rank in e-commerce search engine: Formalization, analysis, and application",
            "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowl-",
            "year": 2018
        },
        {
            "authors": [
                "Hamel Husain",
                "Ho-Hsiang Wu",
                "Tiferet Gazit",
                "Miltiadis Allamanis",
                "Marc Brockschmidt."
            ],
            "title": "Codesearchnet challenge: Evaluating the state of semantic code search",
            "venue": "arXiv preprint arXiv:1909.09436.",
            "year": 2019
        },
        {
            "authors": [
                "Jeevana Priya Inala",
                "Chenglong Wang",
                "Mei Yang",
                "Andres Codas",
                "Mark Encarnaci\u00f3n",
                "Shuvendu K Lahiri",
                "Madanlal Musuvathi",
                "Jianfeng Gao."
            ],
            "title": "Faultaware neural code rankers",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Paras Jain",
                "Ajay Jain",
                "Tianjun Zhang",
                "Pieter Abbeel",
                "Joseph Gonzalez",
                "Ion Stoica."
            ],
            "title": "Contrastive code representation learning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5954\u20135971, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Akshita Jha",
                "Chandan K Reddy."
            ],
            "title": "Codeattack: Code-based adversarial attacks for pre-trained programming language models",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence.",
            "year": 2023
        },
        {
            "authors": [
                "Divyansh Kaushik",
                "Eduard H. Hovy",
                "Zachary Chase Lipton."
            ],
            "title": "Learning the difference that makes A difference with counterfactually-augmented data",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April",
            "year": 2020
        },
        {
            "authors": [
                "Hung Le",
                "Yue Wang",
                "Akhilesh Deepak Gotmare",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "CodeRL: Mastering code generation through pretrained models and deep reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Sanghack Lee",
                "Elias Bareinboim."
            ],
            "title": "Characterizing optimal mixed policies: Where to intervene and what to observe",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages 8565\u20138576. Curran Associates, Inc.",
            "year": 2020
        },
        {
            "authors": [
                "Sergey Levine",
                "Aviral Kumar",
                "George Tucker",
                "Justin Fu."
            ],
            "title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
            "venue": "arXiv preprint arXiv:2005.01643.",
            "year": 2020
        },
        {
            "authors": [
                "Haochen Li",
                "Chunyan Miao",
                "Cyril Leung",
                "Yanxian Huang",
                "Yuan Huang",
                "Hongyu Zhang",
                "Yanlin Wang."
            ],
            "title": "Exploring representation-level augmentation for code search",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natu-",
            "year": 2022
        },
        {
            "authors": [
                "Xiaonan Li",
                "Yeyun Gong",
                "Yelong Shen",
                "Xipeng Qiu",
                "Hang Zhang",
                "Bolun Yao",
                "Weizhen Qi",
                "Daxin Jiang",
                "Weizhu Chen",
                "Nan Duan."
            ],
            "title": "CodeRetriever: A large scale contrastive pre-training method for code search",
            "venue": "Proceedings of the 2022 Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Yujia Li",
                "David Choi",
                "Junyoung Chung",
                "Nate Kushman",
                "Julian Schrittwieser",
                "R\u00e9mi Leblond",
                "Tom Eccles",
                "James Keeling",
                "Felix Gimeno",
                "Agustin Dal Lago"
            ],
            "title": "Competition-level code generation with alphacode",
            "year": 2022
        },
        {
            "authors": [
                "Tie-Yan Liu."
            ],
            "title": "Learning to rank for information retrieval",
            "venue": "Foundations and Trends\u00ae in Information Retrieval, 3(3):225\u2013331.",
            "year": 2009
        },
        {
            "authors": [
                "Shuai Lu",
                "Nan Duan",
                "Hojae Han",
                "Daya Guo",
                "Seungwon Hwang",
                "Alexey Svyatkovskiy."
            ],
            "title": "Reacc: A retrieval-augmented code completion framework",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "daresan",
                "Shao Kun Deng",
                "Shengyu Fu",
                "Shujie Liu"
            ],
            "title": "Codexglue: A machine learning benchmark dataset for code understanding and generation",
            "venue": "In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks",
            "year": 2021
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Chatgpt: Optimizing language models for dialogue",
            "venue": "openai.",
            "year": 2022
        },
        {
            "authors": [
                "Bhargavi Paranjape",
                "Matthew Lamm",
                "Ian Tenney."
            ],
            "title": "Retrieval-guided counterfactual generation for QA",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland,",
            "year": 2022
        },
        {
            "authors": [
                "Ruchir Puri",
                "David S Kung",
                "Geert Janssen",
                "Wei Zhang",
                "Giacomo Domeniconi",
                "Vladmir Zolotov",
                "Julian Dolby",
                "Jie Chen",
                "Mihir Choudhury",
                "Lindsey Decker"
            ],
            "title": "Project codenet: a large-scale ai for code dataset for learning a diversity",
            "year": 2021
        },
        {
            "authors": [
                "Shuo Ren",
                "Daya Guo",
                "Shuai Lu",
                "Long Zhou",
                "Shujie Liu",
                "Duyu Tang",
                "Neel Sundaresan",
                "Ming Zhou",
                "Ambrosio Blanco",
                "Shuai Ma."
            ],
            "title": "Codebleu: a method for automatic evaluation of code synthesis",
            "venue": "arXiv preprint arXiv:2009.10297.",
            "year": 2020
        },
        {
            "authors": [
                "Ensheng Shi",
                "Yanlin Wang",
                "Wenchao Gu",
                "Lun Du",
                "Hongyu Zhang",
                "Shi Han",
                "Dongmei Zhang",
                "Hongbin Sun."
            ],
            "title": "Cocosoda: Effective contrastive learning for code search",
            "venue": "Proceedings of the 45th International Conference on Software Engineering, ICSE",
            "year": 2023
        },
        {
            "authors": [
                "Parshin Shojaee",
                "Aneesh Jain",
                "Sindhu Tipirneni",
                "Chandan K. Reddy."
            ],
            "title": "Execution-based code generation using deep reinforcement learning",
            "venue": "Transactions on Machine Learning Research.",
            "year": 2023
        },
        {
            "authors": [
                "Xin Wang",
                "Fei Mi Yasheng Wang",
                "Pingyi Zhou",
                "Yao Wan",
                "Xiao Liu",
                "Li Li",
                "Hao Wu",
                "Jin Liu",
                "Xin Jiang."
            ],
            "title": "Syncobert: Syntax-guided multi-modal contrastive pre-training for code representation",
            "venue": "arXiv preprint arXiv:2108.04556.",
            "year": 2021
        },
        {
            "authors": [
                "Zeng Wei",
                "Jun Xu",
                "Yanyan Lan",
                "Jiafeng Guo",
                "Xueqi Cheng."
            ],
            "title": "Reinforcement learning to rank with markov decision process",
            "venue": "Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval, pages 945\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Jun Xu",
                "Zeng Wei",
                "Long Xia",
                "Yanyan Lan",
                "Dawei Yin",
                "Xueqi Cheng",
                "Ji-Rong Wen."
            ],
            "title": "Reinforcement learning to rank with pairwise policy gradient",
            "venue": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in",
            "year": 2020
        },
        {
            "authors": [
                "Zhou Yang",
                "Jieke Shi",
                "Junda He",
                "David Lo."
            ],
            "title": "Natural attack for pre-trained models of code",
            "venue": "Proceedings of the 44th International Conference on Software Engineering, ICSE \u201922, page 1482\u20131493, New York, NY, USA. Association for Computing",
            "year": 2022
        },
        {
            "authors": [
                "Hang Zhang",
                "Yeyun Gong",
                "Yelong Shen",
                "Jiancheng Lv",
                "Nan Duan",
                "Weizhu Chen."
            ],
            "title": "Adversarial retriever-ranker for dense text retrieval",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Xiangyu Zhao",
                "Liang Zhang",
                "Zhuoye Ding",
                "Long Xia",
                "Jiliang Tang",
                "Dawei Yin."
            ],
            "title": "Recommendations with negative feedback via pairwise deep reinforcement learning",
            "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge",
            "year": 2018
        },
        {
            "authors": [
                "Feng"
            ],
            "title": "2020) directly mimic NLP architectures and objectives. Later approaches additionally utilize code specific information such as data flow (Guo et al., 2021), abstract syntax tree (AST; Wang et al., 2021",
            "year": 2022
        },
        {
            "authors": [
                "tacks (Jha",
                "Reddy"
            ],
            "title": "2023), or learning to capture the semantics (Ding et al., 2022). Meanwhile, several approaches like CODERANKER (Inala et al., 2022) augment generated code snippets to train code search models",
            "year": 2022
        },
        {
            "authors": [
                "2022 Paranjape et al",
                "2022 Choi et al",
                "Calderon"
            ],
            "title": "Our distinction. Applying \u03b5 lexical changes to flip EF for code intervention",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Code search aims to retrieve code snippets that are most relevant to a given query. However, the notion of relevance often falls short of capturing the ultimate goal of functional correctness, focusing on whether the code would execute as the programmer intended (Ding et al., 2022). For instance, programmers would prefer c (in Figure 1a) over c1 (in Figure 1b), as the entire \u201celse\u201d clause is missing in c1.\nIdeally, ground truth labels for functional correctness should be obtained from execution feedback, collected after executing each code c with a sufficient amount of test cases for each query q. We denote such execution feedback as EFpq, cq. However, conventional training datasets, such as\n\u02daCorresponding author.\nfrom bisect import bisect_left\ndef find_LIS(array): dp = [] for num in array:\nidx = bisect_left(dp, num) if idx == len(dp):\ndp.append(num) else:\ndp[idx] = num return len(dp)\nQuery (\ud835\udc5e): Find the longest increasing subsequence (LIS) of a given array. Code (\ud835\udc50):\n\ud835\udc04\ud835\udc05 = \ud835\udfcf \ud835\udc79\ud835\udf3d = \ud835\udfcf\n(a) A positive code c to a query q in conventional dataset.\n\ud835\udc04\ud835\udc05 = \ud835\udfce \ud835\udc79\ud835\udf3d = \ud835\udfcf\nfrom bisect import bisect_left\ndef find_LIS(array): dp = [] for num in array:\nidx = bisect_left(dp, num) if idx == len(dp):\ndp.append(num) return len(dp)\nCode (\ud835\udc50\"):\n(b) A negative code c1 with misaligned R\u03b8 with EF.\nFigure 1: Example code snippets with the execution feedback (EF) from a running environment and the model\u2019s retrieval decisions (R\u03b8) trained on a conventional code search dataset. The code line difference between the code pair is highlighted as blue.\nCodeSearchNet (CSN; Husain et al., 2019), collect data from GitHub by regarding commented text descriptions as queries then labeling code snippets by query-code co-occurrences. This collection process can introduce an observation bias of lexical dissimilarity between positive and negative training snippets. Consequently, models can learn such observation bias, resulting in misaligned retrieval decisions with EF of returning a negative code with high lexical similarity as a false positive. For example, Figure 1b shows that a code search model\n\u03b8, trained from a conventional dataset, decides to retrieve c1, or R\u03b8pq, c1q \u201c 1, but this model decision is misaligned with the execution feedback, or EFpq, c1q \u201c 0.\nThe objective of this paper is to resolve the misalignment, i.e., R\u03b8 \u2030 EF. A straightforward solution is to expose models to misaligned code pairs and correct the model decisions by EF. In reinforcement learning (RL), this trial-and-error process is achieved by interacting with agents while perturbing input states, referred to as intervention (Lee and Bareinboim, 2020; Carta et al., 2023). We employ this intervention technique by perturbing the positive training code c to generate its negative counterpart c1, where both code snippets yield the same model decision. Formally, R\u03b8pq, c1q \u201c R\u03b8pq, cq \u201c 1 but EFpq, cq \u0105 EFpq, c1q.\nHowever, obtaining EF to correct decisions for each c1 is expensive, and to make matters worse, there are infinite possible perturbations satisfying the above misalignment condition. To identify a representative c1, existing solution known as counterfactual perturbation (Kaushik et al., 2020; Han et al., 2021; Choi et al., 2022; Chen et al., 2022), suggests to find c1 that is lexically closest to c, but with different EF. However, adopting this strategy would expose models solely to trivial syntax errors, such as omitting a colon from Figure 1a, causing a syntax error (and thus different EF). A realistic intervention, such as Figure 1a to Figure 1b, induces a larger lexical change, such as removing the \u201celse\u201d clause.\nOur key technical contribution is a sampleefficient code intervention for RL. Our proposed approach, namely Code Intervention-based Reinforcement Learning (CIRL), perturbs a positive code into negative with \u03f5 structural changes, which is designed to include the intuition of minimal lexical edit, but generalizes beyond to enfold structural similarity.\nSpecifically, we leverage the abstract syntax tree (AST) representation of the code, treating subtrees such as statements, clauses and expressions, as units of structural changes. A structural perturbation is done by masking out a subtree from a positive code then filling it out unless model prediction changes. We stress that CIRL subsumes counterfactual perturbation (when subtree is a leaf), and also adapts to the test distribution, by selecting subtrees with a realistic granularity (e.g. \u201celse\u201d clause in Figure 1a). Further, we ensure syntac-\ntic validity while replacing subtrees, to discourage exposures to trivial syntax errors.\nWe summarize our contributions as follows: First, we employ reinforcement learning with intervention that performs perturbation to simulate misaligned code without feedback from real code execution. Second, we propose CIRL, which utilizes AST information to inject \u03f5 structural changes to code to alter EF while preserving model decision. Third, our experimental results on various datasets show that CIRL effectively contributes to aligning code search with EF.\nAll our implementation and datasets are publicly available1 for future research purposes."
        },
        {
            "heading": "2 Preliminary",
            "text": "In this section, we will begin by defining the code search task and addressing the misalignment issue in existing supervised code search training."
        },
        {
            "heading": "2.1 Code Search",
            "text": "For a universe of queries Q and code snippets C, the ultimate goal of this task is to find functionally correct snippets for each query.2 The functional correctness between a query q P Q and a code c P C can be confirmed by execution feedback (EF) from an environment using a set of test cases TCq:\nEFpq, cq \u201c # 1, if @pti, toq P TCq, cptiq \u201c to, 0, otherwise,\n(1)\nwhere pti, toq P TCq is a test input-output pair and cptiq signifies running c by feeding ti. Our goal is to build a code search model \u03b8 making its retrieval decision R\u03b8 P t0, 1u to align with EF for any q and c, i.e., @q P Q @c P C, R\u03b8pq, cq \u201c EFpq, cq."
        },
        {
            "heading": "2.2 Supervised Baseline",
            "text": "Ideally, a training dataset D\u02da should comprise query-code pairs along with EF as labels, encompassing both positive and negative examples, i.e., D\u02da \u201c tpq, c, EFpq, cqq|q P Q, c P Cu. However, building such a dataset at scale is nontrivial due to the high costs and efforts involved in manually annotating test cases for each query, executing the code with test cases, and creating a secure\n1https://github.com/stovecat/CIRL 2Although code search typically aims to retrieve relevant code snippets, pursuing functional correctness is valuable, e.g., in scenarios where generated codes are directly executed, as relevance is a subset of functional correctness.\nof a query and a set of candidate code snippets, a code search policy \u03b8 takes an action at to retrieve a code ct. 2 CIRL perturbs the retrieved code ct into c1t with Abstract Syntax Tree (AST) information. 3 CIRL intervenes the state st to s1t by replacing ct into c 1 t and makes \u03b8 take another action a 1 t from s 1 t. If c 1 t is not retrieved, we go back to the second stage. 4 When c1t is retrieved by a 1 t, though we expect to receive the EF from an execution environment as a reward\u2014rt`1 \u201c EFpq, ctq and r1t`1 \u201c EFpq, c1tq\u2014CIRL can skip this costly stage by inducing the EF during perturbation. 5 \u03b8 is updated to align its decision with the EF.\nenvironment for possibly unsafe code when executed (Chen et al., 2021; Li et al., 2022c). Due to this reason, a traditional training dataset such as CodeSearchNet (CSN; Husain et al., 2019) is gathered from GitHub by assuming that each function implementation c and its corresponding commented description q (i.e. Rpq, cq \u201c 1) can substitute EF: D \u201c tpq, c, 1q|q P Q, c P C, Rpq, cq \u201c 1u.\nIn training time, negative code snippets for a query are sampled from other queries\u2019 positive snippets. However, these sampled negative snippets are both lexically and structurally dissimilar to positive ones. To illustrate, we examine that \u201e 97% of positive-negative code pairs in CSN Python training set score very low in lexical/structural similarity.3 As a consequence, dataset D exhibits bias towards positive code, leading to functionally misaligned retrieval decisions by a model \u03b8 trained on D when comparing positive code to lexically similar negative code. Figure 1 is an example of such bias\u2014retrieving both c and c1 despite their different EF due to a missing \u201celse\u201d clause."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Contribution I: Reinforcement Learning with Intervention",
            "text": "We employ RL with code intervention to resolve the misalignment of model decisions and EF. Although RL behavior may resemble supervised or curriculum learning in specific scenarios where rules align with model decisions, our proposed RL\n3\u0103 0.3 CodeBLEU (Ren et al., 2020) scores, which considers both lexical and structural similarity.\nabstraction enables to extend beyond these cases to situations where the RL-based model can choose to contradict (lesser-effective) curriculum rules.4\nRL Formulation. In the process of retrieving positive code snippets from a pool of M candidates in D by a model \u03b8, we regard \u03b8 as a policy and formulate code search as a Markov decision process, which consists of following elements:\n\u2022 States: Given a query q, the state st in time step t is defined as pq, Ctq where C0 is the set of M candidate code snippets.\n\u2022 Actions: Apstq is a set of possible actions from the state st, and at P Apstq is a retrieval decision of a code ct P Ct, i.e., R\u03b8pq, ctq \u201c 1.\n\u2022 Reward: The reward of at is defined as the EF of ct, i.e., EFpq, ctq.\n\u2022 Transition: The transition from st to st`1 by at is done by removing ct from Ct.\n\u2022 Discount factor: \u03b3 P r0, 1s is a discount factor to estimate the current value of future rewards, i.e., Gt \u201c \u0159M\u00b4t\u00b41 k\u201c0 \u03b3 kEFpq, ct`kq.\nCode Intervention. To resolve the misalignment, we intervene a retrieved code ct in step t by replacing it into c1t such that R\u03b8pq, c1tq \u201c R\u03b8pq, ctq \u201c 1 but EFpq, ctq \u0105 EFpq, c1tq. The overall process of our RL with code intervention is illustrated in Figure 2. Formally, the pairwise policy gradient of \u03b8 in step t from two actions at and a1t respectively\n4Refer to Appendix B for details.\nretrieving code ct and c1t is,\n\u2206\u03b8 \u201c pGt \u00b4 G1tqp\u2207 log p\u03b8pat|stq \u00b4\u2207 log p\u03b8pa1t|s1tqq, (2)\nwhere p\u03b8 is the probability distribution of actions taken by \u03b8, and s1t is the state after intervention by replacing ct into c1t. Gt and G 1 t is the cumulative rewards starting from t for at and a1t respectively. As we replace the retrieved code ct into c1t to generate s1t from st, both states have the same next state, or, st`1 \u201c s1t`1, thus Gt\u00b4G1t \u201c EFpq, ctq\u00b4EFpq, c1tq. Then Eq (2) becomes,\n\u2206\u03b8 \u201c pEFpq, ctq \u00b4 EFpq, c1tqqp\u2207 log p\u03b8pat|stq \u00b4\u2207 log p\u03b8pa1t|s1tqq.\n(3)\nThis equation suggests that gradients can be calculated efficiently, in a single transition. Lastly, to avoid directly comparing the policy gradients from different states st and s1t, we approximate two states into a single virtual state st \u201c pq, Ct Y tc1tuq. As \u03b8 can take both at and a1t in st, we only need to maximize at in p\u03b8:\n\u2206\u03b8 \u201c pEFpq, ctq \u00b4 EFpq, c1tqq\u2207 log p\u03b8pat|stq. (4)\nFrom the perspective of Levine et al. (2020), our approach can be understood as an online off-policy RL, where a1t acts as an action produced by an offpolicy and \u03b8 is the target policy. In this light, code intervention is akin to importance sampling that leverages predictions of the target policy to choose actions likely to be undertaken."
        },
        {
            "heading": "3.2 Contribution II: Structural Perturbation",
            "text": "However, computing \u2206\u03b8 in Eq (4) is still expensive, due to the following challenges: First, we need to compute EFpq, c1q for each gradient calculation. Second, from the perspective of off-policy RL, we need to pursue sample efficiency, as the search space of c1t satisfying R\u03b8pq, c1tq \u201c R\u03b8pq, ctq \u201c 1 yet EFpq, ctq \u0105 EFpq, c1tq is enormously broad.\nA conservative approach is applying a minimal lexical edit, e.g., changing \u2018+\u2019 operator to \u2018-\u2019, to guarantee to alter the EF, keeping R\u03b8 unchanged as two code snippets are lexically near-identical, which we denote as lexical counterfactual perturbation (Kaushik et al., 2020; Han et al., 2021; Choi et al., 2022; Chen et al., 2022). As we intended\nthe perturbation to flip EF from 1 to 0, we can label EF=0 for the perturbed code without incurring actual execution.\nHowever, the counterfactual objective of minimizing lexical edits is too restrictive for compositional nature of code (Han et al., 2022). To illustrate, Figure 1a and 1b are lexically rather distant, but semantically close, or, parse tree structure is only \u03f5 away.\nInspired, we propose to relax lexical minimization to train more realistic EF-contrastive pairs, with the following goals:\n\u2022 A pair should be free of syntax errors. Using lexical perturbation often incurs syntax errors such as omitting a semicolon.\n\u2022 A pair must maintain contextual consistency. For example, when we change the if condition in Figure 3, lexical perturbation can break the context of \u2018try statement - if statement\u2019.\n\u2022 A pair should match the target distribution such as large lexical edits. Lexical perturbation with minimal edits results in deviate from the desired distribution.\nToward the goals, we propose subtree perturbation for augmenting a pair, ensuring the above criteria:\n\u2022 Controlled Syntax and EF Error: We model code revisions utilizing Abstract Syntax Trees (ASTs), to consider subtrees like statements and clauses as elements for perturbations.5\n5Empirical evidence suggests that less than 1% of the code snippets augmented by our method (sourced from the Codeforces training set) contain syntax errors and potential EF noises play negligible roles for our contrastive training objectives.\n\u2022 Maintaining Contextual Consistency: As shown in Figure 3, the augmented code satisfies contextual consistency by matching the ancestor nodes during subtree replacement.\n\u2022 Target Distribution Matching: The newly inserted subtrees, being derived from actual code snippets, match the target distribution.\nNote that we can avoid expensive EFpq, c1q invocation, as it subsumes counterfactual perturbation as a special case where the subtree is a leaf node."
        },
        {
            "heading": "3.3 CIRL",
            "text": "We present CIRL by providing a summary of the steps involved in the code perturbation from the original code c to c1. We begin with a positive query-code pair pq, cq in the dataset D where EFpq, cq \u201c 1. We initialize a perturbation ratio 0 \u0103 \u03b4 \u0103 100%, which determines the number of nodes to be changed. This ratio is gradually increased through iterations using a schedule function s.6 Note that a structural change of magnitude \u03f5 can lead to significant lexical modifications with a magnitude of \u03b4, impacting multiple leaf nodes.\nDuring each iteration, we perform the following two steps and verify whether the model\u2019s decision on code c is maintained for the perturbed code c1, i.e., R\u03b8pq, cq \u201c R\u03b8pq, c1q. If the decision is preserved, we calculate \u2206\u03b8 in Eq (4) using c1, increase \u03b4 using the schedule function s, and proceed with another iteration using the updated \u03b4. The iteration process terminates either when R\u03b8pq, cq \u2030 R\u03b8pq, c1q is satisfied or when the maximum number of iterations n is reached.\nStep 1: Subtree Removal. We convert c into its AST representation. Then, based on the node perturbation ratio \u03b4, we randomly select a set of subtrees to remove. Each subtree\u2019s root node is a statement, clause, or expression. The total number of nodes for the selected subtrees is approximately equal to \u03b4% of the AST nodes for c.\nStep 2: Subtree Insertion. For each removed subtree, we sample a new subtree from other code snippets in the dataset D. To maintain syntactic validity, we adopt a conservative guideline: we select a new subtree only if it shares the same ancestor AST node sequence with the removed subtree as depicted in Figure 3. To prevent name errors, we\n6We empirically tune \u03b4 to adapt to the test distribution. Please refer to Appendix C.2 for more details.\nuniformly sample names from the code c for variables and functions in the injected subtrees. We ensure consistent replacements for repeated names. Finally, we convert the perturbed AST back into its code form, resulting in c1."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "The evaluation is performed on three categories of benchmarks.\nThe first category involves large-scale public code competition datasets, namely Codeforces (Caballero et al., 2016) and CodeNet (Puri et al., 2021). These datasets consist of natural language descriptions paired with code submissions including Python. We use the cleansed version from CodeContests (Li et al., 2022c), filter examples not written in Python language, code/description longer than 512 tokens, and those which cannot be parsed into ASTs.7 We train on Codeforces and conduct evaluations on both sets. To see the performance across diverse difficulty, we split CodeNet into 10 subsets by accept-ratios (the number of answer code snippets among submissions) as an additional evaluation set, to complement Codeforces that is relatively too easy (accept ratio is mostly 0.5 or higher).\nThe second category involves AdvTest (Lu et al., 2021), which evaluates the misalignment of positive code snippets that are not retrieved by code search models. This dataset is created by perturbing variable and function names while preserving EF from the test set of CodeSearchNet Python (CSN-Python; Husain et al., 2019) .\nThe third category focuses on CSN benchmark, which includes six different programming languages. The evaluation specifically examines the performance on the Python and Ruby subset of CSN, which is filtered from previous work (Guo et al., 2021).\nOverall, these benchmark categories provide comprehensive evaluations of CIRL and its impact on aligning model decisions with EF across various datasets and scenarios."
        },
        {
            "heading": "4.2 Code Search Baselines",
            "text": "We consider two code search baselines in our experiments. First baseline is GraphCodeBERT (Guo et al., 2021), a popular code search model that uses data flow, a graph structure for representing the\n7Detailed statistics are shown in Appendix D.\nrelation among variables\u2013declaration, assignment, usage, and removal. Second baseline is ContraCode (Jain et al., 2021), which conducts contrastive pretraining on conventional datasets. To minimize truncation, we use a bi-encoder for encoding the description and code separately, following Guo et al. (2021) and Jain et al. (2021).8 We obtain the last layer\u2019s representation of the [CLS] token for each query and code, then compute the prediction score.9"
        },
        {
            "heading": "4.3 Code Intervention Baselines",
            "text": "We compare CIRL with different code augmentation approaches as code intervention baselines instead of our structural perturbation.10 Note that\n8Note that CIRL maintains a model-agnostic nature, as it acknowledges the potential enhancement of cross-encoder models by intervened data, such as iteratively sampled hard negatives in dense text retrieval, as demonstrated in Zhang et al. (2022).\n9Refer to Appendix C.1 for implementation details. 10Refer to Appendix C.2 for implementation details.\nwe compare CI-HUMAN and CI-PLM on Codeforces and CodeNet, because of the prerequisites in both baselines\u2013 the existence of test cases and human generated negative code snippets.\nCI-HUMAN This baseline utilizes human efforts by generating additional code snippets that receive negative EF. Unlike conventional distribution such as CSN, Codeforces provides not only the set of positive code for each query, but also the set of negative code for each query. This approach additionally utilizes the latter set for code intervention.\nCI-PLM Following Inala et al. (2022), we can augment negative code snippets generated by code generation models instead of expensive human efforts. We use GPT-Neo 2.7B (Black et al., 2021) finetuned on APPS (Hendrycks et al., 2021) dataset to generate snippets in Codeforces training set, then calculate the EF for each generated code.\nCI-ADV Like in AdvTest, code intervention may tackle misaligned model decisions caused by positive code that are not retrieved, by semanticpreserving perturbation of each positive code c from D to c1 such that R\u03b8pq, cq \u0105 R\u03b8pq, c1q but EFpq, c1q \u201c EFpq, cq \u201c 1. Following Bui et al. (2021) and Lu et al. (2022), heuristic rules such as the replacement of variable and function names, or the insertion of dead code lines can be seen as defining such adversarial perturbation."
        },
        {
            "heading": "4.4 Main Results",
            "text": "For evaluation, we used Mean Reciprocal Rank (MRR), a standard code search metric from previous studies (Feng et al., 2020; Guo et al., 2021; Lu et al., 2021; Wang et al., 2021), with respect to the ground-truth of ranking the correct answer as the top.\nCodeforces and CodeNet Table 1 presents improved MRR scores for evaluating the alignment of model decisions with EF using different code intervention approaches. Among the low-cost methods, CIRL outperformed ContraCode, GraphCodeBERT, and CI-ADV. Even when compared to the costly methods, CIRL still significantly outperformed CI-PLM and generally outperformed CI-HUMAN. Regarding the code difficulty, CIHuman enhanced MRR by over 2% in difficult CodeNet ranges (0-10 to 30-40), but less in easier ranges (40-50 to 90-100). Conversely, CIRL consistently boosted MRR across all code difficulties, showcasing its widespread efficacy. Additionally, CIRL combined with CI-HUMAN, achieved the best performance, showing the augmentation from\nCIRL can complement human efforts. Lastly, the \u2018- Code Intervention\u2019 ablation replaced RL with a basic curriculum contrastive learning, following settings in Appendix B. Its underperformance compared to CIRL highlights our RL mechanism\u2019s effectiveness.\nAdvTest To evaluate whether CIRL contributes to aligned model decisions for adversarially perturbed positive code, we applied CIRL on CSNPython training set and evaluate on AdvTest. The training epochs were increased from 2 to 10, resulting in improved GraphCodeBERT performance. The results in Table 2 confirm that applying CIRL on ContraCode maintained performance, while it enhanced performance on GraphCodeBERT, indicating that CIRL is reliable and potentially beneficial for correcting misaligned decisions in adversarial positive code. In contrast, CI-ADV exhibited a drop of approximately 5% compared to GraphCodeBERT, which is further discussed in Section 5.4.\nCSN The results on the conventional distribution, where observation bias is strongly correlated with the label, are presented in Table 3. CIRL enhanced\nthe MRR score on ContraCode and showed only a marginal decrease on GraphCodeBERT. Additionally, Table 4 demonstrates CIRL\u2019s effectiveness across programming languages, showcasing results on Ruby code snippets."
        },
        {
            "heading": "5 Discussion",
            "text": ""
        },
        {
            "heading": "5.1 Is CIRL Perturbation Sample-efficient?",
            "text": "We conducted a stress test by applying CIRL on 5 test subsets of CodeNet (0.0-0.1 to 0.4-0.5).11 The results in Table 5 demonstrate that both ContraCode and GraphCodeBERT experienced drops in MRR scores after augmenting the intervened code snippets, indicating confusion between intervened negative code and positive code. Although CIHUMAN reduced this confusion, the model still suffered from MRR drops. However, applying CIRL successfully resolved this confusion and prevented MRR drops."
        },
        {
            "heading": "5.2 Does CIRL Generalize Over Counterfactual Perturbation?",
            "text": "We examined the impact of CIRL allowing for more structural changes, compared to counterfactual perturbation with minimal lexical changes. To ensure a fair comparison, we implemented a COUNTERFACTUAL baseline from CIRL by fixing the perturbation ratio \u03b4 to a small value (\u03b4 \u201c 2) to focus on small lexical changes. Figure 4 presents a comparison of GraphCodeBERT, COUNTERFACTUAL, and CIRL on Codeforces and CodeNet benchmarks. As expected, COUNTERFACTUAL exhibited lower MRR scores compared to CIRL, and even lower than GraphCodeBERT in some test sets.\n11Refer to Appendix C.3 for details."
        },
        {
            "heading": "Codeforces",
            "text": ""
        },
        {
            "heading": "5.3 Does CIRL Generalize Over Different Programming Languages?",
            "text": "Table 4 shows that CIRL successfully improved the MRR performance on CSN-Ruby. Though our AST perturbation mechanism may look more complex than lexical perturbation, it straightforwardly generalizes to other languages, as AST parser (e.g., tree-sitter) easily applies to other languages for extracting a language-generic representation to perturb."
        },
        {
            "heading": "5.4 Can Augmentation Negatively Affect Performance?",
            "text": "It may seem counter-intuitive that one of the existing augmentation approaches, CI-ADV, negatively affects the MRR scores throughout our experiments. We speculate that CI-ADV relies on heuristic rules for perturbation, which inherently have limitations\nin terms of the coverage of perturbations in order to preserve EF. For instance, augmenting code perturbation by variable renaming results in high lexical and structural similarity with the original code. Consequently, this augmentation may amplify the observation bias from the original code, leading to more misaligned decisions in trained models."
        },
        {
            "heading": "5.5 Can LLMs Align better with EF?",
            "text": "We explore whether general-purpose large language models, benefiting from enormous parameter scales, can detect misalignments. To confirm this, we use GPT-3.5 (OpenAI, 2022) to evaluate querycode pairs from CodeNet 0.0-0.1 and determine whether the code is correct for the given query, in the following two settings: (1) zero-shot; (2) in-context learning (ICL) by providing a positive pq, cq and a negative pair pq, c1q randomly sampled from the Codeforces training set.12\nFail to Align Human Perturbations. GPT-3.5 inaccurately identified negative code snippets by CI-HUMAN as false positives, performing worse than random guessing of 50% (see the \u2018Negative Codes (CI-HUMAN)\u2019 graphs in Figure 5).\nFail to Align Subtree Removals. To illustrate the weakness of GPT-3.5 for negative code with subtree removals, we implemented CIRLRM doing the removal step only. GPT-3.5 failed to identify omitted implementations such as handling edge cases13, as shown in Figure 5 that \u2018Negative Codes (CIRLRM)\u2019 graphs were lower than random guessing (50%).\nSucceed When Code Lines are Replaced. GPT3.5 showed 100% aligned decisions with EF for negative code snippets consisting of replaced subtrees, shown as \u2018Negative Codes (CIRL)\u2019 graphs in Figure 5. GPT-3.5 often explained the reason for its decision as the presence of unnecessary or unrelated code lines.13\nIn-Context Learning Fails. A naive solution of employing ICL with positive and negative examples did not improve the alignment, as contrasted by zero-shot and ICL groups in Figure 5. Against our expectation, it even increased misalignment for negative code by CI-HUMAN and CIRLRM. We leave the alignment of LLMs\u2019 decisions with EF for future work.\n12Refer to Appendix C.4 for details. 13Example outputs are shown in Appendix F."
        },
        {
            "heading": "6 Conclusion",
            "text": "This paper has explored the issue of misalignment of model decisions and EF due to observation bias in traditional training datasets. To overcome this limitation, we have introduced a novel RL framework with code intervention called CIRL. The primary contribution of CIRL is to expose models to misaligned code snippets, which can be subsequently corrected through EF. CIRL is sample efficient by utilizing ASTs to simulate structural perturbations for code intervention, allowing us to bypass actual execution for EF. Extensive experimental results on various datasets demonstrate that CIRL enhances the alignment of model decisions and EF in code search compared to conventional approaches."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was partially supported by Microsoft Research Asia, and Electronics and Telecommunications Research Institute (ETRI) grant funded by ICT R&D program of MSIT/IITP (2022-0-00995, Automated reliable source code generation from natural language descriptions)."
        },
        {
            "heading": "Limitations",
            "text": "Despite our achievements, there are following limitations in this work. First, we mainly tackled Python language, appending with Ruby. Second, while CIRL is model-agnostic, we applied it to\nContraCode and GraphCodeBERT, conducting preliminary analysis on LLMs, and leaving extensive application on LLMs for future work."
        },
        {
            "heading": "A Related Work",
            "text": ""
        },
        {
            "heading": "A.1 Deep Code Search Models",
            "text": "Recently, deep learning models have shown dramatic improvements in the conventional code search task. Early approaches (Gu et al., 2018; Feng et al., 2020) directly mimic NLP architectures and objectives. Later approaches additionally utilize code specific information such as data flow (Guo et al., 2021), abstract syntax tree (AST; Wang et al., 2021; Guo et al., 2022, and template (Han et al., 2022). For pretraining, several approaches (Jain et al., 2021; Wang et al., 2021; Guo et al., 2022; Li et al., 2022b) employ contrastive learning of semantically related code-code and text-code pairs. Instead of random sampling, existing works (Li et al., 2022a; Shi et al., 2023) have proposed negative code sampling methods in expense of extensive model predictions or additional memory usage.\nOur distinction. Existing approaches are trained by (self-)supervised learning in conventional datasets where execution feedback (EF) is approximated by text-code coocurrences, suffering from misalignment of model decisions with EF. Our distinction is to employ reinforcement learning and intervene code to test and correct misaligned model decisions. Regarding negative code sampling, our approach plays a similar role as hard negative mining, all while circumventing the need for extensive computational resources or memory usage."
        },
        {
            "heading": "A.2 Reinforcement Learning in IR and Code Generation",
            "text": "Reinforcement Learning to Rank. In information retrieval (IR) and recommender system, which are related to code search, recent approaches have been applied reinforcement learning (RL), formulating document retrieval and item recommendation as markov decision process (MDP). In early approaches like PPG (Wei et al., 2017) a policy estimates the absolute rank score of each candidate (i.e. pointwise). Inspired by pairwise learning to rank (Liu, 2009), later approaches (Zhao et al., 2018; Xu et al., 2020; Hu et al., 2018) use pairwise loss between candidates sharing certain features with different feedback, improving sample efficiency."
        },
        {
            "heading": "Reinforcement Learning on Code Generation.",
            "text": "Several approaches have employed RL to generate code snippets. CodeRL (Le et al., 2022)\nuses an actor-critic algorithm that gives rewards for policy gradients by a trained reward model. PPOCoder (Shojaee et al., 2023) utilizes Proximal Policy Optimization (PPO) to reduce syntax errors and improve functional correctness.\nOur distinction. Unlike IR, recommender system, and code generation domains, traditional code search training sets do not contain misaligned code pairs. Our distinction is to generate misalignment pairs with intervention, by perturbing a given positive code to a negative code while not changing model decision."
        },
        {
            "heading": "A.3 Synthetic Code Augmentation",
            "text": "To automatically supplement training code distribution, existing approaches (Bui et al., 2021; Yang et al., 2022) utilize semantic-preserving perturbations by changing semantically ineffective lexicons (e.g., variable and function names). Other approaches target software vulnerability by semanticpreserving perturbations to conduct adversarial attacks (Jha and Reddy, 2023), or learning to capture the semantics (Ding et al., 2022). Meanwhile, several approaches like CODERANKER (Inala et al., 2022) augment generated code snippets to train code search models.\nOur distinction. Unlike existing methods, we directly target to code intervention in RL to align code search with EF, while managing both sample efficiency and zero-cost for EF calculation."
        },
        {
            "heading": "A.4 Counterfactual Text Augmentation",
            "text": "In NLP, early approaches (Kaushik et al., 2020) pair each instance with its label-flipped augmentation, obtained from perturbing labels by human efforts with \u03f5 lexical changes. Later approaches propose automatic syntheses to alleviate human efforts (Han et al., 2021; Paranjape et al., 2022; Choi et al., 2022; Calderon et al., 2022).\nOur distinction. Applying \u03f5 lexical changes to flip EF for code intervention is not sufficient as it only exposes to trivial lexical errors such as forgetting colon from Figure 1a, thus fails to generalize in misaligned decisions over larger lexical changes like Figure 1b. Instead, we produce \u03f5 structural changes, which subsume \u03f5 lexical changes as a special case of replacing a single leaf node, thus broaden model exposures for generalization."
        },
        {
            "heading": "B Motivation of RL framework",
            "text": "Both supervised learning and curriculum learning can be viewed as specific instances of the RL framework. Thus, it shouldn\u2019t be surprising that in certain fortuitous scenarios when R\u03b8pq, cq \u201c R\u03b8pq, c1q for every c1, the RL framework may resemble to them. However, this does not undermine the value of our RL framework. To illustrate, curriculum learning utilizes a predetermined rule for data selection during augmentation (like measuring AST perturbation levels). Our proposed RL framework encompasses such fortuitous scenarios where the rule strongly correlates with agent decision, making both RL and curriculum learning optimal.\nHowever, our framework extends beyond these specific cases, into general scenarios when (a) agent disagrees with the rules from curriculum learning, contributing (b) disagreements to gains:\n(a) Agent-Curriculum Disagreements. We sampled 100 positive code snippets from the Codeforces (Caballero et al., 2016) training set and corresponding generated negative snippets with five different perturbation ratios (same as in Appendix C.2). A Spearman rank correlation analysis between selection orderings of these negative snippets by curriculum and by RL with intervention revealed that 17% of the rank correlations are below 0.5. This underscores that curriculum learning and RL-based intervention often have differing data selection preferences.\n(b) Disagreements Contribute to Gains. An analysis of cases where curriculum learning faltered (i.e., top 1 ranked code is negative) on the CodeNet (Puri et al., 2021) 0.0-0.1 test set revealed frequent rank discrepancies. For instance, the top 1 ranked negative code typically shared only one line with the positive code, whereas the negative code with the most line overlaps had 19 lines in common with its positive counterpart. Table 6 provides the qualitative evidence. Here, CIRL differs from the curriculum baseline by intervening code based on\ntarget agent predictions. From the online off-policy RL perspective mentioned in Section 3.1, this data selection driven by the target agent can enhance the quality of decisions made by the agent during testing.\nC Implementation Detail"
        },
        {
            "heading": "C.1 Code Search Baselines",
            "text": "GraphCodeBERT For all scenarios except CSNRuby, we finetuned the pretrained checkpoint14 of GraphCodeBERT (Guo et al., 2021) on the CSN (Guo et al., 2021) Python training set for 10 epochs using AdamW optimizer, a 2e-5 learning rate, max token lengths 128 (description) and 256 (code), a max data flow length of 64, and a batch size of 32. The total training time was 1.5 days with 4 NVIDIA GeForce RTX 3090 (24GB) GPUs. In Codeforces (Caballero et al., 2016) training set, we further fine-tuned GraphCodeBERT using 16 NVIDIA Tesla V100 (32GB) GPUs for 3 days with the following settings: using AdamW optimizer, a 2e-5 learning rate, a batch size of 480, a max training epoch of 10, max token lengths 512 for both description and code, and a max data flow length of 64.\nContraCode We finetune the pretrained checkpoint of ContraCode (Jain et al., 2021) on Codeforces training set using 8 NVIDIA GeForce RTX 3090 (24GB) GPUs for 3 days with the same setting of GraphCodeBERT, except for the batch size of 96 and not using data flow. For CSN-Python and CSN-Ruby, we used the same setting of GraphCodeBERT but not using data flow."
        },
        {
            "heading": "C.2 Code Intervention Approaches",
            "text": "Throughout all approaches, we use GraphCodeBERT as a code search policy.\nCIRL In Codeforces training set, CIRL intervenes each positive code with maximum n \u201c 5 iterations, where the initial perturbation ratio \u03b4\n14https://huggingface.co/microsoft/graphcodebert-base\nis initialized by \u03b4 \u201c 2 and updated by a schedule function sp\u03b4, \u00b5q \u201c \u03b4\u00b5 where \u00b5 P r1, 2, ..., ns is the current iteration. The structural element is one of the following AST subtrees: future import statement, import statement, import from statement, print statement, assert statement, expression statement, return statement, delete statement, raise statement, pass statement, break statement, continue statement, global statement, nonlocal statement, exec statement, if statement, for statement, while statement, try statement, with statement, function definition, class definition, decorated definition, elif clause, else clause, except clause, with clause, and block. In CSN training set, CIRL conducts a single intervention per code (n \u201c 1), where the perturbation ratio is set by \u03b4 \u201c 32. For other hyperparameters, we use the same values with GraphCodeBERT, except for changing the batch size to 40 training with 8 NVIDIA GeForce RTX 3090 (24GB) GPUs for 10 days on Codeforces training set.\nCI-HUMAN We augment 1 negative code for each positive code to avoid overfitting, as the size of negative code snippets is 30.70% of that of positive snippets in Codeforces training set.\nCI-PLM We use GPT-Neo 2.7B (Black et al., 2021) finetuned on APPS (Hendrycks et al., 2021) dataset to generate 5 code snippets per problem description in Codeforces training set, then calculate the execution feedback (EF) for each synthetic code using the provided test cases from CodeContests (Li et al., 2022c). We train this baseline on the same setting with our approach, including the batch size of 40.\nCI-ADV To implement CI-ADV, we use the available implementation from Lu et al. (2022) for variable/function renaming and and dead code insertion. In both Codeforces and CSN training sets, we augment the same number of iterations for each positive code, along with the same training configuration."
        },
        {
            "heading": "C.3 Test Time Code Augmentation by CIRL",
            "text": "We intervene with a single iteration for each positive code in 5 CodeNet (Puri et al., 2021) subsets (0.0-0.1 to 0.4-0.5), where each of the perturbation ratio \u03b4 \u201c 5, 15, 25, 35, and 45 is set by the average of min-max accept ratio for each subset."
        },
        {
            "heading": "C.4 Misalignment in LLMs",
            "text": "We use GPT-3.5-turbo-0613 (OpenAI, 2022)15, with the top_p as 1 and the temperature as 0 for reproducibility. Appendix F shows examples of input prompts for both zero-shot and in-context learning settings.\nCIRLRM For each code, we remove a single structural element, where element is one of the following AST subtrees that do not incur syntax errors when removed: print statement, assert statement, if statement, for statement, while statement, try statement, with statement, function definition, class definition, else clause, except clause, and with clause.\n15https://platform.openai.com/docs/model-index-forresearchers"
        },
        {
            "heading": "E CIRL: Case Study",
            "text": ""
        },
        {
            "heading": "D Dataset",
            "text": ""
        },
        {
            "heading": "Prompt",
            "text": ""
        },
        {
            "heading": "F GPT-3.5: Case Study",
            "text": ""
        },
        {
            "heading": "Prompt",
            "text": ""
        },
        {
            "heading": "Prompt",
            "text": ""
        },
        {
            "heading": "Prompt",
            "text": ""
        }
    ],
    "title": "Intervention-Based Alignment of Code Search with Execution Feedback",
    "year": 2023
}