{
    "abstractText": "General-purpose text decoding approaches are usually adopted for dialogue response generation. Although the quality of the generated responses can be improved with dialogue-specific encoding methods, conversational decoding methods are still under-explored. Inspired by Wu et al. (2023) that a good dialogue feature space should follow the rules of locality and isotropy, we present a fine-grained conversational decoding method, termed isotropic and proximal search (IPS). Our method is designed to generate the semantic-concentrated response, while still maintaining informativeness and discrimination against the context. Experiments show that our approach outperforms existing decoding strategies in the dialogue field across both automatic and human evaluation metrics. More in-depth analyses further confirm the effectiveness of our approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuxuan Yao"
        },
        {
            "affiliations": [],
            "name": "Han Wu"
        },
        {
            "affiliations": [],
            "name": "Qiling Xu"
        },
        {
            "affiliations": [],
            "name": "Linqi Song"
        }
    ],
    "id": "SP:b1b04ccbd667fec609b93636c25f7d69fd32d122",
    "references": [
        {
            "authors": [
                "Sourya Basu",
                "Govardana Sachitanandam Ramachandran",
                "Nitish Shirish Keskar",
                "Lav R. Varshney."
            ],
            "title": "Mirostat: A neural text decoding algorithm that directly controls perplexity",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Hongshen Chen",
                "Xiaorui Liu",
                "Dawei Yin",
                "Jiliang Tang."
            ],
            "title": "A survey on dialogue systems",
            "venue": "ACM SIGKDD Explorations Newsletter, 19(2):25\u201335.",
            "year": 2017
        },
        {
            "authors": [
                "Kawin Ethayarajh."
            ],
            "title": "How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898, Melbourne, Australia. Association",
            "year": 2018
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Maxwell Forbes",
                "Antoine Bosselut",
                "David Golub",
                "Yejin Choi."
            ],
            "title": "Learning to write with cooperative discriminators",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "year": 2017
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jiwei Li",
                "Michel Galley",
                "Chris Brockett",
                "Jianfeng Gao",
                "Bill Dolan."
            ],
            "title": "A diversity-promoting objective function for neural conversation models",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Jiwei Li",
                "Will Monroe",
                "Alan Ritter",
                "Dan Jurafsky",
                "Michel Galley",
                "Jianfeng Gao."
            ],
            "title": "Deep reinforcement learning for dialogue generation",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1192\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Yanran Li",
                "Hui Su",
                "Xiaoyu Shen",
                "Wenjie Li",
                "Ziqiang Cao",
                "Shuzi Niu."
            ],
            "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu"
            ],
            "title": "G-eval: Nlg evaluation using gpt-4 with better human alignment",
            "year": 2023
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Krishna Pillutla",
                "Swabha Swayamdipta",
                "Rowan Zellers",
                "John Thickstun",
                "Sean Welleck",
                "Yejin Choi",
                "Zaid Harchaoui."
            ],
            "title": "MAUVE: Measuring the gap between neural text and human text using divergence frontiers",
            "venue": "Advances in Neural Information Pro-",
            "year": 2021
        },
        {
            "authors": [
                "Verena Rieser",
                "Oliver Lemon",
                "Simon Keizer."
            ],
            "title": "Natural language generation as incremental planning under uncertainty: Adaptive information presentation for statistical dialogue systems",
            "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc., 22(5):979994.",
            "year": 2014
        },
        {
            "authors": [
                "Alan Ritter",
                "Colin Cherry",
                "William B. Dolan."
            ],
            "title": "Data-driven response generation in social media",
            "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 583\u2013 593, Edinburgh, Scotland, UK. Association for Com-",
            "year": 2011
        },
        {
            "authors": [
                "Yixuan Su",
                "Nigel Collier."
            ],
            "title": "Contrastive search is what you need for neural text generation",
            "venue": "Transactions on Machine Learning Research.",
            "year": 2023
        },
        {
            "authors": [
                "Yixuan Su",
                "Tian Lan",
                "Yan Wang",
                "Dani Yogatama",
                "Lingpeng Kong",
                "Nigel Collier."
            ],
            "title": "A contrastive framework for neural text generation",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Yixuan Su",
                "Yan Wang",
                "Deng Cai",
                "Simon Baker",
                "Anna Korhonen",
                "Nigel Collier."
            ],
            "title": "Prototype-tostyle: Dialogue generation with style-aware editing on retrieval memory",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:2152\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Yida Wang",
                "Pei Ke",
                "Yinhe Zheng",
                "Kaili Huang",
                "Yong Jiang",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "A large-scale chinese short-text conversation dataset",
            "venue": "Natural Language Processing and Chinese Computing: 9th CCF International Conference, NLPCC",
            "year": 2020
        },
        {
            "authors": [
                "Sam Wiseman",
                "Stuart Shieber",
                "Alexander Rush."
            ],
            "title": "Challenges in data-to-document generation",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253\u20132263, Copenhagen, Denmark. Association for",
            "year": 2017
        },
        {
            "authors": [
                "Han Wu",
                "Haochen Tan",
                "Mingjie Zhan",
                "Gangming Zhao",
                "Shaoqing Lu",
                "Ding Liang",
                "Linqi Song."
            ],
            "title": "Learning locality and isotropy in dialogue modeling",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi"
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "In International Conference on Learning Representations",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Dialogue response generation (Li et al., 2017; Wang et al., 2020) aims to generate the utterance that forms a coherent and fluent continuation given a dialogue context. Generic text decoding strategies (Rieser et al., 2014; Ritter et al., 2011; Chen et al., 2017) are usually adopted to produce grammatical and contextual responses. As an independent technique, decoding strategy can also enhance the generation quality of large language models.\nExisting text decoding methods have been explored in various generic text generation tasks, but lack tailoring for dialogue generation, e.g., capturing dialogue-specific features and generating an informative and discriminative dialogue response (Su et al., 2021; Wu et al., 2023). Early maximizationbased methods, e.g., greedy search (Li et al., 2016b) and beam search (Wiseman et al., 2017), may lead to dullness and degeneration (Fan et al., 2018; Holtzman et al., 2018). Later sampling-based improvements are proposed to tackle these problems,\n\u2217Corresponding author: linqi.song@cityu.edu.hk\nincluding top-k sampling (Fan et al., 2018) and nucleus search (Holtzman et al., 2018). While alleviating degeneration, these sampling methods introduce critical semantic inconsistency and are not aligned with human-written prefix (Basu et al., 2021). Specifically, a bunch of studies (Ethayarajh, 2019; Su and Collier, 2023) have asserted that the problem of anisotropy, i.e., a distribution pattern in the latent space with features occupying a narrow cone in the space, leads to inconsistency and degradation of the generation. Although contrastive search (Su et al., 2022) has been proposed correspondingly to mitigate the issue, as a generalized text decoding strategy, it still ignores dialoguespecific features, such as utterance dependencies and conversational structure information. Therefore, research on conversational decoding methods is warmly needed.\nIn this work, we propose a fine-grained conversational decoding method, namely isotropic and proximal search (IPS). Different from traditional approaches, we consider the previous tokens and contexts separately from a granular perspective. Acknowledging that locality and isotropy are two important properties for refining the dialogue feature space, we design our IPS following these rules: (i) the generated output should be selected from the most probable candidate set predicted by the dialogue model; (ii) the generated tokens in the same utterance should be proximal to each other for expressing a concentrated idea; and (iii) the newly generated utterance should be discriminative enough with respect to the context utterances. In this way, our method encourages informativeness and discrimination among different utterances as well as maintains a concentrated idea within an utterance. We evaluate our approach on two commonly-used dialogue datasets, DailyDialog (Li et al., 2017) in English and LCCC (Wang et al., 2020) in Chinese. Both human and automatic evaluation results, i.e., indicators based on GPT3.5,\nconsistently show that IPS can generate more fluent, coherent, and human-like responses than existing decoding methods."
        },
        {
            "heading": "2 Methodology",
            "text": ""
        },
        {
            "heading": "2.1 Preliminary",
            "text": "Dialogue response generation Given a dialogue context D = {u1, u2, ..., uN} composed of N utterances, where ui = { xi,1, xi,2, ..., xi,|ui| } is a sequence of consecutive words, the task of dialogue response generation is to produce the continuation utterance ur = {w1, w2, ..., w|ur|}, (r = N + 1).\nThere are generally two key steps to finish the task, including context encoding and response decoding. For the first step, we obtain the context representations H from the language model by concatenating the utterances into a sequence.\nH = PrLM(u1 [EOU] u2 [EOU] ... uN [EOU]),\nwhere [EOU] is the special token inserted as the last token of each utterance.\nFor the decoding step, the response is generally produced in an auto-regressive manner as follows\np(w1:|ur|) = \u220f|ur|\ni=1 p(wi|w<i, D) (1)\nDialogue modeling Wu et al. (2023) has demonstrated that locality and isotropy are two key properties for building a good conversational feature space. Specifically, locality encourages the model to aggregate the representations of tokens within an utterance while isotropy pushes away the representations of distinct utterances."
        },
        {
            "heading": "2.2 Isotropic and Proximal Search",
            "text": "We present a fine-grained conversational decoding method, i.e., isotropic and proximal search (IPS). Specifically, we expect the generated response to satisfy two requirements: 1) representations of the response tokens are nearby to convey a concentrated idea, saying proximity; 2) the response representation is discriminative to the context utterance representations, saying isotropy.\nDuring the decoding stage, for proximal search, we try to select the candidate token having the shortest average distance to the existing generated tokens. For isotropic search, we try to choose the token that enables the response representation most discriminative to representations of context utterances. As the response representation cannot be\ndetermined during the decoding stage, we calculate it in an approximate way, i.e., averaging the representations of the already generated tokens, as follows:\nhRT = 1\nT \u2211T i=1 hwi (2)\nwhere hRT is the response representation which will be dynamically updated along with the generation process, and T is the number of already generated tokens.\nUp to now, the problem changes to how to generate the first token for starting the isotropic and proximal search since the method is heavily dependent on the previous tokens. To address this problem, we attempt to finish the first n-steps generation by traditional decoding methods, such as beam search, top-k sampling or nucleus sampling. On the other hand, as IPS is essentially a deterministic decoding strategy, this solution also enables it to produce diverse responses by using different decoding strategies in the first n steps. Therefore, in each step t after the first sampling stage, we calculate the proximal and isotropic values as follows:\np_valuet = 1 t\u2212 1 \u2211t\u22121 i=1 s(hwt ,hwi) (3)\ni_valuet = 1\nN \u2211N i=1 s(hRT ,hui) (4)\nwhere s is the cosine similarity. hui are the utterance representations obtained from the special token [EOU]. The proximal value measures the average distance between the candidate token and the already generated tokens while the isotropic value stands for the average similarity between the undergoing response representation and all utterance representations. Next, the selection of the candidate token wt is formulated as,\nwt = argmax wt\u2208V (m) {\u03b1\u00d7 p(wt | w<t, D)\ufe38 \ufe37\ufe37 \ufe38 model confidence\n+ (1\u2212 \u03b1)\u00d7 (p_valuet \u2212 i_valuet)\ufe38 \ufe37\ufe37 \ufe38 isotropic and proximal penalty } (5)\nwhere V (m) is the set of top-m predictions from the model\u2019s probability distribution p(wt | w<t, D) and m, is typically set as 4 \u223c 8. In Eq. (5), the first term, model confidence, is the probability of the candidate wt predicted by the model. The second term, isotropic and proximal penalty, aims to maximize the discrimination between the undergoing response and previous utterances and minimize the\ntoken difference within the response. The hyperparameter \u03b1 \u2208 [0, 1] regulates the importance of these two components. When \u03b1 = 1, our method degenerates to the greedy search method.\nWe claim our method is fine-grained because the generic auto-regressive generation predicts the next token by jointly considering the already generated tokens w<t and the context D, formulated as p(wt|w<t, D) while IPS splits these two factors. Specifically, proximity value only focuses on the effects of the already generated tokens, i.e., p_valuet \u223c p(wt|w<t), and isotropy value pays more attention to the context, i.e., i_valuet \u223c p(wt|D, (w<t)) wherein w<t is just used to obtain the undergoing response representation hRT ."
        },
        {
            "heading": "3 Experiments",
            "text": "Dataset We evaluate our method on two commonly-used datasets, DailyDialog (Li et al., 2017) in English and LCCC (Wang et al., 2020) in Chinese. Both of them are open-domain multi-turn dialogue datasets, collected from social media. For LCCC, owing to the academic-level computing resource, we follow previous work (Su et al., 2022), and sample a subset of the dataset, consisting of 100,000 dialogue examples.\nBaselines Following Wu et al. (2023), we use BART (Lewis et al., 2020) as our backbone. We\nevaluate the performance of decoding strategies with different models, including vanilla BART, BART with SimCTG (Su et al., 2022), and BART with SimDRC (Wu et al., 2023). We compare IPS to greedy search, beam search, top-k sampling (Fan et al., 2018), nucleus sampling (Holtzman et al., 2018) and contrastive search (Su et al., 2022).\nSettings We fine-tune the models on DailyDialog and LCCC datasets for 6k steps and 7k steps, respectively. We use a batch size of 64 and truncate the training samples to a maximum length of 256. The parameters of the models are initialized from HuggingFace libraries and updated by Adam optimizer (Kingma and Ba, 2017) with a learning rate of 3e-5. We adopt the margin values of SimCTG and SimDRC suggested in their work, i.e., \u03c1 = 0.5 for SimCTG and \u03b4 = 0.7, \u03b1 = 0.3 for SimDRC. We conduct the isotropic and proximal search with the first n = 2 steps adopting top-k sampling (k = 7). The weight \u03b1 is 0.6. We run all experiments with five different seeds and report the average score.\nEvaluation Metrics Traditional n-gram overlap and text matching metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) are not proper to evaluate plausible output diversity for open-domain dialog systems. Therefore, for automatic evaluation, we choose the following metrics, including BERTScore (Zhang* et al., 2020),\nMAUVE (Pillutla et al., 2021), Distinct2/4 (Li et al., 2016a), and G-Eval, an automatic evaluation metric based on GPT3.5 (Liu et al., 2023).\nWe also conduct a human evaluation with the help of recruited proficient English/Chinese speakers. We randomly sample 100 dialogue examples from DailyDialog and LCCC test sets. For each dialogue context, we generate responses using the aforementioned backbone models (BART, BART+SimCTG, BART+SimDRC) with six different inference strategies. Five annotators are hired independently to measure these samples. Annotators are instructed to give a score ranging from 1 to 5 over the following aspects, including fluency, informativeness, coherence, and semantic coverage1.\nResults and Discussion Table 1 lists the automatic evaluation results of the different methods with different decoding strategies. Similar results can be also found in human evaluation, as shown in Table 2. We can see that the models, collaborating with IPS, can produce more semantically consistent(high BERTScores and MAUVE scores) and human-like (high G-Eval scores) responses. Although contrastive search can generate more novel and diverse tokens (high Distinct scores), it usually suffers from the problem of prediction deviation, i.e., the predicted token being weakly related to the main idea of the response. This is also in line with the worse performance of contrastive search on other metrics, such as BERTScore, and G-Eval, indicating that the diverse responses produced by contrastive search are not accurate and human-like enough. Different from contrastive search, IPS tries to concentrate on the core meaning of the response and express it clearly, thus a slightly lower Distinct score is acceptable and expected. Note that IPS still has better distinct scores than other traditional decoding methods since it encourages discrimination and isotropy among utterances.\n1Details of human evaluation are in Appendix A.1.\nAlthough IPS can be directly used with different models and achieve good performance, the models trained with SimDRC are the best testbed for IPS. We can see that SimDRC+IPS can mostly achieve the best performance across the board on both automatic and human evaluation. This is reasonable because the training process of SimDRC is greatly consistent with the search criterion of IPS, and they both push away the inter-utterance features and pull close the intra-utterance features.\nAblation Study Figure 1 shows the ablation studies on different components of the method, including the first n steps, the sampling strategy for the first n-step decoding, and the weight \u03b1. As shown in Figure 1(a), our method consistently outperforms the contrastive search no matter the number of first steps. We find some performance drops with the increase of the first-stage sampling steps. We think this is because more generic tokens are selected by traditional search methods, thus weakening the proximity and isotropy of the response. For strategies in the first n steps, we attempt beam search, top-k sampling, and nucleus sampling. We finally select top-k sampling as our first stage\u2019s strategy owing to its better performance in the comparisons. Figure 1(b) shows the results of different k values adopted in top-k sampling. We can see that our method exceeds the baseline by a large margin when k > 5. The effect of weight \u03b1 is also studied, as shown in Figure 1(c). Our method consistently outperforms the baseline with the different weights, suggesting the robustness of our method.\nHyperparameter Analysis To explore the effects of isotropy and proximity, in our experiments, we introduced a hyperparameter \u03b2 to balance the p_value and i_value as:\n(1\u2212 \u03b2)\u00d7 p_value \u2212 \u03b2 \u00d7 i_value (6)\nWe tried the effects of \u03b2 ranging from 0.2 to 0.8. We surprisingly found that the balance of proximal\nvalue and isotropy value leads to the best performance, saying \u03b2 equals 0.5. This finding is a bit different from the observations in SimDRC(Wu et al., 2023) which suggests that larger isotropy loss weight is needed to balance the two properties in the training stage. We think this is because our method is a decoding strategy, rather than the training optimization process. The sparse isotropy values would not cause the model bias in the decoding stage. So, the harmonious balance of proximity and isotropy can be simply achieved by giving a moderate value of \u03b2."
        },
        {
            "heading": "4 Conclusion",
            "text": "In this work, we present a fine-grained conversational decoding strategy, namely isotropic and proximal search (IPS) to encourage the generation of isotropic and conversational tokens. Superior to existing decoding methods, IPS decouples the previous tokens and the context. Experiments show that our method achieves impressive performance on both automatic and human evaluation."
        },
        {
            "heading": "Ackonwledgements",
            "text": "This work was supported in part by the InnoHK initiative, the Government of the HKSAR, Laboratory for AI-Powered Financial Technologies."
        },
        {
            "heading": "Limitations",
            "text": "During the experiments, we found that for a single piece of data in the DailyDialog test set, traditional text decoding methods such as beam search, top-k sampling and beam search take less than 1 second, the contrastive search takes about 5.07s, and the decoding time required by our proposed IPS is about 2.16s. Although our approach takes longer than the traditional text decoding method, our calculation speed is obviously faster than contrastive search. How to further improve the computing speed is still the direction we need to work on."
        },
        {
            "heading": "Ethics Statement",
            "text": "In this work, we use publicly released datasets to auxiliary our dialogue response generation. Generally, these previous works have considered ethical issues when creating the datasets. We have manually checked some samples for the datasets we used in this work, and do not find any obvious ethical concerns, such as violent or offensive content. We will also release the source decoding code\nwith friendly instructions to support its correct use. However, we still need to emphasize that text generation is not as controllable as we think. It still would generate some novel or unexpected words occasionally. We may take actions to decrease generation diversity to alleviate this problem."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Human Evaluation Instructions",
            "text": "Please rate the quality of the generated response based on the given dialogue context and the target response over the following aspects: (1) Fluency; (2) Informativeness; (3) Coherence; (4) Semantic Coverage. We provide some instructions for your rating."
        },
        {
            "heading": "A.1.1 Fluency",
            "text": "This measures whether the generated text has no formatting problems, capitalization errors, or obviously ungrammatical sentences (e.g., fragments, missing components) that make the text difficult to read. The definitions of different scores are:\n\u2022 5: The text is fluent, grammatically correct, and has no errors. It is easy to read.\n\u2022 4: The text is grammatically correct but has a few spelling or capitalization errors, which does not affect your understanding.\n\u2022 3: The text has minor errors in both grammar and spelling. The errors slightly affect your understanding.\n\u2022 2: The text has major errors in both grammar and spelling. The errors make the text hard to read.\n\u2022 1: The text does not make sense and it is unreadable.\nA.1.2 Informativeness This measures whether the generated text has diverse, informative, novel, or logically related content. The definitions of different scores are:\n\u2022 5: The text contains very diverse, informative, and novel content. It is enjoyable to read the text.\n\u2022 4: The text contains many informative and novel contents. (Choose this score when you hesitate between 3 and 5.)\n\u2022 3: The text contains some new information but also contains a few repetitions of the context.\n\u2022 2: The text only contains a few informative and new terms. (Choose this score when you hesitate between 1 and 3.)\n\u2022 1: The text is dull, repetitive, and has no new information. All contents are from the dialogue context."
        },
        {
            "heading": "A.1.3 Coherence",
            "text": "This measures whether the generated text is semantically and factually consistent with the dialogue context. The definitions of different scores are:\n\u2022 5: The text is semantically, factually, and topically consistent with the dialogue context. All contents of the text are related to the source text or can be inferred from the source.\n\u2022 4: The text is very related to the context but has minor inconsistencies or contradictions that do not affect its overall relevance.\n\u2022 3: The text is related to the context but has some obvious inconsistencies and contradictions.\n\u2022 2: The text is slightly consistent with the context. Many inconsistencies and contradictions in the context can be found.\n\u2022 1: The text is totally inconsistent with the context. It semantically or factually contradicted the context."
        },
        {
            "heading": "A.1.4 Semantic Coverage",
            "text": "This measures how many semantic content units from the target response are covered by the generated text. The definitions of different scores are:\n\u2022 5: All semantic content units of the target text can be found in the generated text. They are semantically consistent.\n\u2022 4: Most of the content units of the target text can be found from the generated text while a few missing units do not affect the overall coverage.\n\u2022 3: Some semantic content units can be found in the generated text but also miss some important units.\n\u2022 2: Most of the semantic content units are not covered. Only a few insignificant units can be found in the generated text.\n\u2022 1: The text does not have any overlapping semantic content units with the target text.\nWe recruit five human workers to annotate 3,600 samples. To make sure the workers are fairly paid, we pay 0.1 dollars for each sample. Therefore, the total amount spent on participant compensation is 360 dollars. The annotators take 24 hours to finish the task, suggesting the hourly wage for each worker is 15 dollars."
        },
        {
            "heading": "A.2 More Details of the Task",
            "text": ""
        },
        {
            "heading": "A.2.1 Evaluation of G-EVAL Score",
            "text": "The API we used to test G-EVAl is gpt-3.5-turbo, and the following is the prompt (Liu et al., 2023):\nYou will be given a conversation between two individuals. You will then be given one potential response for the next turn in the conversation. Your task is to give a final score for utterance. Please make sure you read and understand these instructions carefully.\nThe evaluation aspects are:\n1. Engagingness: Is the response dull or interesting?\n2. Naturalness: This measures whether the generated text has no formatting problems, capitalization errors, or obviously ungrammatical sentences to read.\n3. Informativeness: This measures whether the generated text has diverse, informative, novel, or logically related content.\n4. Coherence: This measures whether the generated text is semantically and factually consistent with the dialogue context.\nThe evaluation steps are:\n1. Read the conversation, the corresponding label, and the response carefully.\n2. Considering the above evaluation aspects, return a comprehensive final score ranging from 1 to 5 for each conversation.\n3. Please only return 1 overall score, without any extra text descriptions. The return format should be like Score:1.\nNow please read the following conversation, and return the score."
        },
        {
            "heading": "A.2.2 More Experimental Results",
            "text": "Table 2 lists the results of human evaluation."
        },
        {
            "heading": "A.3 Surface-level Analysis",
            "text": ""
        },
        {
            "heading": "A.3.1 Score Distribution According to the Length of the Previous Context",
            "text": "Table 3 and Table 4 illustrate the relations between the context length and the human evaluation metrics while using the IPS (the above one) and beam search (the below one) decoding strategies. Observing the table, when the context length is particularly short (<10), we speculate that the context may consist of simple greetings or introductions, resulting in lower difficulty of generation and thus higher scores. When the context length varies in the range of approximately 10 to 40, due to differences in the complexity of context content and semantics, the scores exhibit a fluctuating trend. As the length continues to increase, the information provided by the previous context becomes richer, leading to improved effectiveness of both decoding methods. We also note that when faced with exceptionally long contexts, the generation quality of IPS is superior to the baselines."
        },
        {
            "heading": "A.3.2 Utterance Length Analysis",
            "text": "Table 5 shows that both IPS and contrastive search tend to produce shorter sentences than traditional methods. We explain in the main text that by incorporating isotropy, achieved through contrastive\nsearch and IPS, redundancy is minimized, resulting in more concise generated text compared to previous methods. Considering the nature of the conversation, our IPS strategy expects proximity and does not enlarge the token distance in the same utterance, thus responses of IPS are slightly longer than that of contrastive search."
        },
        {
            "heading": "A.4 Qualitative Analysis",
            "text": "A.4.1 Instances Illustration\nSome examples are presented to illustrate the effect of our IPS search.\nIn summation, according to Table 6 and Table 7, some qualitative observations are as follows:\n\u2022 Replies generated by IPS are more natural and accurate.\n\u2022 IPS tends to generate relatively concise responses.\n\u2022 With more complex previous contexts, we observed that IPS does not prioritize shortening the length of response. IPS can generate responses that are more in line with the situation based on the characteristics of the conversation."
        },
        {
            "heading": "A.5 Cosine Similarity Heatmap",
            "text": "To ensure utterances generated by our IPS are isotropic and proximal, and observe the representations produced by different decoding methods, we showcase the cosine similarity matrix of token representations correspondingly.\nThe larger color difference between different sentences represents greater isotropy, indicating discrimination among utterances; while the darker the color within the same sentence, the greater the proximity, conveying a more concentrated thought.\nChoosing SimDRC as the backbone model, cosine similarity heatmaps of different inference methods are shown as follows. Tokens generated by IPS exhibit brighter colors in the heatmap, indicating increased proximity within the same sentence, while tokens from IPS showcase darker colors for different sentences, signifying greater isotropy. Contrastingly, traditional methods like beam search showed anisotropy(i.e. features occupy a narrow cone in the vector space, thus leading to the problem of degeneration.) in the figures."
        },
        {
            "heading": "A.6 Examples of Generated Texts",
            "text": "For non-native Chinese speakers, translations of Table 9 are presented in Table 10. The quality of the LCCC dataset still requires optimization, as it contains numerous colloquial and slang expressions. We are not professional translators, and in our attempts, we noticed that the translated meanings sometimes diverged from the original Chinese. We apologize for the inconvenience."
        }
    ],
    "title": "Fine-grained Conversational Decoding via Isotropic and Proximal Search",
    "year": 2023
}