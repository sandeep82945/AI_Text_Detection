{
    "abstractText": "The ability to detect intent in dialogue systems has become increasingly important in modern technology. These systems often generate a large amount of unlabeled data, and manually labeling this data requires substantial human effort. Semi-supervised methods attempt to remedy this cost by using a model trained on a few labeled examples and then by assigning pseudolabels to further a subset of unlabeled examples that has a model prediction confidence higher than a certain threshold. However, one particularly perilous consequence of these methods is the risk of picking an imbalanced set of examples across classes, which could lead to poor labels. In the present work, we describe Top-K K-Nearest Neighbor (TK-KNN), which uses a more robust pseudo-labeling approach based on distance in the embedding space while maintaining a balanced set of pseudo-labeled examples across classes through a ranking-based approach. Experiments on several datasets show that TK-KNN outperforms existing models, particularly when labeled data is scarce on popular datasets such as CLINC150 and Banking77. Code is available at https://github. com/ServiceNow/tk-knn",
    "authors": [
        {
            "affiliations": [],
            "name": "Nicholas Botzer"
        },
        {
            "affiliations": [],
            "name": "David Vasquez"
        },
        {
            "affiliations": [],
            "name": "Tim Weninger"
        },
        {
            "affiliations": [],
            "name": "Issam Laradji"
        }
    ],
    "id": "SP:b433159a9b060b4cfbccef929ec2b2341272280e",
    "references": [
        {
            "authors": [
                "E. Arazo",
                "D. Ortego",
                "P. Albert",
                "N.E. O\u2019Connor",
                "K. McGuinness"
            ],
            "title": "Pseudo-labeling and confirmation bias in deep semi-supervised learning",
            "venue": "In IJCNN,",
            "year": 2020
        },
        {
            "authors": [
                "M. Arjovsky",
                "L. Bottou"
            ],
            "title": "Towards principled methods for training generative adversarial networks",
            "venue": "arXiv preprint arXiv:1701.04862,",
            "year": 2017
        },
        {
            "authors": [
                "S. Basu",
                "A. Sharaf",
                "A. Fischer",
                "V. Rohra",
                "M. Amoake",
                "H. ElHammamy",
                "E. Nosakhare",
                "V. Ramani",
                "B. Han"
            ],
            "title": "Semisupervised few-shot intent classification and slot filling",
            "venue": "arXiv preprint arXiv:2109.08754,",
            "year": 2021
        },
        {
            "authors": [
                "I. Casanueva",
                "T. Tem\u010dinas",
                "D. Gerz",
                "M. Henderson",
                "I. Vuli\u0107"
            ],
            "title": "Efficient intent detection with dual sentence encoders",
            "venue": "In Workshop on Natural Language Processing for Conversational AI,",
            "year": 2020
        },
        {
            "authors": [
                "P. Cascante-Bonilla",
                "F. Tan",
                "Y. Qi",
                "V. Ordonez"
            ],
            "title": "Curriculum labeling: Revisiting pseudo-labeling for semisupervised learning",
            "venue": "In AAAI,",
            "year": 2021
        },
        {
            "authors": [
                "J. Chen",
                "Z. Yang",
                "D. Yang"
            ],
            "title": "Mixtext: Linguisticallyinformed interpolation of hidden space for semi-supervised text classification",
            "venue": "In ACL,",
            "year": 2020
        },
        {
            "authors": [
                "J. Chen",
                "D. Tam",
                "C. Raffel",
                "M. Bansal",
                "D. Yang"
            ],
            "title": "An empirical survey of data augmentation for limited data learning in nlp",
            "venue": "arXiv preprint arXiv:2106.07499,",
            "year": 2021
        },
        {
            "authors": [
                "J. Chen",
                "R. Zhang",
                "Y. Mao",
                "J. Xu"
            ],
            "title": "Contrastnet: A contrastive learning framework for few-shot text classification",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Z. Cheng",
                "Z. Jiang",
                "Y. Yin",
                "C. Wang",
                "Q. Gu"
            ],
            "title": "Learning to classify open intent via soft labeling and manifold mixup",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "D. Croce",
                "G. Castellucci",
                "R. Basili"
            ],
            "title": "Gan-bert: Generative adversarial learning for robust text classification with a bunch of labeled examples",
            "venue": "In ACL,",
            "year": 2020
        },
        {
            "authors": [
                "J. Devlin",
                "M.-W. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "G. Ding",
                "S. Zhang",
                "S. Khan",
                "Z. Tang",
                "J. Zhang",
                "F. Porikli"
            ],
            "title": "Feature affinity-based pseudo labeling for semi-supervised person re-identification",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2019
        },
        {
            "authors": [
                "R. Dror",
                "G. Baumer",
                "S. Shlomov",
                "R. Reichart"
            ],
            "title": "The hitchhiker\u2019s guide to testing statistical significance in natural language processing",
            "year": 2018
        },
        {
            "authors": [
                "A. El-Nouby",
                "N. Neverova",
                "I. Laptev",
                "H. J\u00e9gou"
            ],
            "title": "Training vision transformers for image retrieval",
            "venue": "arXiv preprint arXiv:2102.05644,",
            "year": 2021
        },
        {
            "authors": [
                "G. Elsayed",
                "D. Krishnan",
                "H. Mobahi",
                "K. Regan",
                "S. Bengio"
            ],
            "title": "Large margin deep networks for classification",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "A. Gera",
                "A. Halfon",
                "E. Shnarch",
                "Y. Perlitz",
                "L. Ein-Dor",
                "N. Slonim"
            ],
            "title": "Zero-shot text classification with self-training",
            "venue": "arXiv preprint arXiv:2210.17541,",
            "year": 2022
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. WardeFarley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM,",
            "year": 2020
        },
        {
            "authors": [
                "P. Khosla",
                "P. Teterwak",
                "C. Wang",
                "A. Sarna",
                "Y. Tian",
                "P. Isola",
                "A. Maschinot",
                "C. Liu",
                "D. Krishnan"
            ],
            "title": "Supervised contrastive learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "L.F. Kozachenko",
                "N.N. Leonenko"
            ],
            "title": "Sample estimate of the entropy of a random vector",
            "venue": "Problemy Peredachi Informatsii,",
            "year": 1987
        },
        {
            "authors": [
                "I. Laradji",
                "P. Rodr\u00edguez",
                "D. Vazquez",
                "D. Nowrouzezahrai"
            ],
            "title": "Ssr: Semi-supervised soft rasterizer for single-view 2d to 3d reconstruction",
            "venue": "arXiv preprint arXiv:2108.09593,",
            "year": 2021
        },
        {
            "authors": [
                "S. Larson",
                "A. Mahendran",
                "J.J. Peper",
                "C. Clarke",
                "A. Lee",
                "P. Hill",
                "J.K. Kummerfeld",
                "K. Leach",
                "M.A. Laurenzano",
                "L. Tang"
            ],
            "title": "An evaluation dataset for intent classification and out-of-scope prediction",
            "venue": "EMNLP-IJCNLP,",
            "year": 2019
        },
        {
            "authors": [
                "D.-H. Lee"
            ],
            "title": "Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks",
            "venue": "In Workshop on challenges in representation learning,",
            "year": 2013
        },
        {
            "authors": [
                "X. Liu",
                "A. Eshghi",
                "P. Swietojanski",
                "V. Rieser"
            ],
            "title": "Benchmarking natural language understanding services for building conversational agents",
            "year": 1903
        },
        {
            "authors": [
                "M. Marcuzzo",
                "A. Zangari",
                "M. Schiavinato",
                "L. Giudice",
                "A. Gasparetto",
                "A. Albarelli"
            ],
            "title": "A multi-level approach for hierarchical ticket classification",
            "venue": "W-NUT,",
            "year": 2022
        },
        {
            "authors": [
                "K. Nigam",
                "R. Ghani"
            ],
            "title": "Analyzing the effectiveness and applicability of co-training",
            "venue": "In Proceedings of the ninth international conference on Information and knowledge management,",
            "year": 2000
        },
        {
            "authors": [
                "Y. Ouali",
                "C. Hudelot",
                "M. Tami"
            ],
            "title": "An overview of deep semisupervised learning",
            "venue": "arXiv preprint arXiv:2006.05278,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Ouali",
                "C. Hudelot",
                "M. Tami"
            ],
            "title": "Semi-supervised semantic segmentation with cross-consistency training",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "L. Prechelt"
            ],
            "title": "Early stopping-but when? In Neural Networks: Tricks of the trade, pages 55\u201369",
            "year": 1998
        },
        {
            "authors": [
                "N. Reimers",
                "I. Gurevych"
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 2019
        },
        {
            "authors": [
                "M.N. Rizve",
                "K. Duarte",
                "Y.S. Rawat",
                "M. Shah"
            ],
            "title": "In defense of pseudo-labeling: An uncertainty-aware pseudo-label selection framework for semi-supervised learning",
            "venue": "arXiv preprint arXiv:2101.06329,",
            "year": 2021
        },
        {
            "authors": [
                "A. Sablayrolles",
                "M. Douze",
                "C. Schmid",
                "H. J\u00e9gou"
            ],
            "title": "Spreading vectors for similarity search",
            "venue": "In ICLR 2019-7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "T. Salimans",
                "I. Goodfellow",
                "W. Zaremba",
                "V. Cheung",
                "A. Radford",
                "X. Chen"
            ],
            "title": "Improved techniques for training gans",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "K. Sohn",
                "D. Berthelot",
                "N. Carlini",
                "Z. Zhang",
                "H. Zhang",
                "C.A. Raffel",
                "E.D. Cubuk",
                "A. Kurakin",
                "C.-L. Li"
            ],
            "title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "F. Taherkhani",
                "A. Dabouei",
                "S. Soleymani",
                "J. Dawson",
                "N.M. Nasrabadi"
            ],
            "title": "Self-supervised wasserstein pseudolabeling for semi-supervised image classification",
            "year": 2021
        },
        {
            "authors": [
                "R. Wang",
                "X. Dai"
            ],
            "title": "Contrastive learning-enhanced nearest neighbor mechanism for multi-label text classification",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
            "year": 2022
        },
        {
            "authors": [
                "T. Wolf",
                "L. Debut",
                "V. Sanh",
                "J. Chaumond",
                "C. Delangue",
                "A. Moi",
                "P. Cistac",
                "T. Rault",
                "R. Louf",
                "M. Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "year": 1910
        },
        {
            "authors": [
                "I.Z. Yalniz",
                "H. J\u00e9gou",
                "K. Chen",
                "M. Paluri",
                "D. Mahajan"
            ],
            "title": "Billion-scale semi-supervised learning for image classification",
            "year": 1905
        },
        {
            "authors": [
                "X. Zhai",
                "A. Oliver",
                "A. Kolesnikov",
                "L. Beyer"
            ],
            "title": "S4l: Selfsupervised semi-supervised learning",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "L.-M. Zhan",
                "H. Liang",
                "B. Liu",
                "L. Fan",
                "X.-M. Wu",
                "A. Lam"
            ],
            "title": "Out-of-scope intent detection with self-supervision and discriminative training",
            "venue": "arXiv preprint arXiv:2106.08616,",
            "year": 2021
        },
        {
            "authors": [
                "B. Zhang",
                "Y. Wang",
                "W. Hou",
                "H. Wu",
                "J. Wang",
                "M. Okumura",
                "T. Shinozaki"
            ],
            "title": "Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "H. Zhang",
                "M. Cisse",
                "Y.N. Dauphin",
                "D. Lopez-Paz"
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "arXiv preprint arXiv:1710.09412,",
            "year": 2017
        },
        {
            "authors": [
                "H. Zhang",
                "H. Xu",
                "T.-E. Lin"
            ],
            "title": "Deep open intent classification with adaptive decision boundary",
            "venue": "In AAAI,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhou",
                "P. Liu",
                "X. Qiu"
            ],
            "title": "Knn-contrastive learning for out-of-domain intent classification",
            "venue": "In ACL,",
            "year": 2022
        },
        {
            "authors": [
                "X. Zhu",
                "A.B. Goldberg"
            ],
            "title": "Introduction to semi-supervised learning",
            "venue": "Synthesis lectures on artificial intelligence and machine learning,",
            "year": 2009
        },
        {
            "authors": [
                "Z. Zhu",
                "Z. Dong",
                "Y. Liu"
            ],
            "title": "Detecting corrupted labels without training a model to predict",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zou",
                "Z. Yu",
                "B. Kumar",
                "J. Wang"
            ],
            "title": "Unsupervised domain adaptation for semantic segmentation via class-balanced self-training",
            "venue": "In Proceedings of the European conference on computer vision (ECCV), pages 289\u2013305,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "The ability to detect intent in dialogue systems has become increasingly important in modern technology. These systems often generate a large amount of unlabeled data, and manually labeling this data requires substantial human effort. Semi-supervised methods attempt to remedy this cost by using a model trained on a few labeled examples and then by assigning pseudolabels to further a subset of unlabeled examples that has a model prediction confidence higher than a certain threshold. However, one particularly perilous consequence of these methods is the risk of picking an imbalanced set of examples across classes, which could lead to poor labels. In the present work, we describe Top-K K-Nearest Neighbor (TK-KNN), which uses a more robust pseudo-labeling approach based on distance in the embedding space while maintaining a balanced set of pseudo-labeled examples across classes through a ranking-based approach. Experiments on several datasets show that TK-KNN outperforms existing models, particularly when labeled data is scarce on popular datasets such as CLINC150 and Banking77. Code is available at https://github. com/ServiceNow/tk-knn"
        },
        {
            "heading": "1 Introduction.",
            "text": "Large language models like BERT [Devlin et al., 2018] have significantly pushed the boundaries of Natural Language Understanding (NLU) and created interesting applications such as automatic ticket resolution [Marcuzzo et al., 2022]. A key component of such systems is a virtual agent\u2019s ability to understand a user\u2019s intent to respond appropriately. Successful implementation and deployment of models for these systems require a large amount of labeled data to be effective. Although deployment of these systems often generate a large amount of data that could be used for fine-tuning, the cost of labeling this data is high. Semi-supervised learning methodologies are an ob-\nvious solution because they can significantly reduce the amount of human effort required to train these kinds of models [Laradji et al., 2021, Zhu and Goldberg, 2009] especially in image classification tasks [Zhai et al., 2019, Ouali et al., 2020b, Yalniz et al., 2019]. However, as well shall see, applications of these models is difficult for NLU and intent classification because of the label distribution.\nIndeed, research most closely realted to the present work is the Slot-List model by Basu et al. [2021], which focuses on the meta-learning aspect of semi-supervised learning rather than using unlabeled data. In a similar vein the GANBERT [Croce et al., 2020] model shows that using an adversarial learning regime can be devised to ensure that the extracted BERT features are similar amongst the unlabeled and the labeled data sets and substantially boost classification performance. Other methods have investigated how data augmen-\ntation can be applied to the NLP domain to enforce consistency in the models [Chen et al., 2020], and several other methods have been proposed from the computer vision community. However, a recent empirical study found that many of these methods do not provide the same benefit to NLU tasks as they provide to computer vision tasks [Chen et al., 2021] and can even hinder performance.\nIntent classification remains a challenging problem for multiple reasons. Generally, the number of intents a system must consider is relatively large, with sixty classes or more. On top of that, most queries consists of only a short sentence or two. This forces models to need many examples in order to learn nuance between different intents within the same domain. In the semi-supervised setting, many methods set a confidence threshold for the model and assign pseudo-labels to the unlabeled data if their confidence is above the threshold [Sohn et al., 2020]. This strategy permits high-confidence pseudo-labeled data elements to be included in the training set, which typically results in performance gains. Unfortunately, this approach also causes the model to become overconfident for classes that are easier to predict.\nIn the present work, we describe the Top-K KNearest Neighbor (TK-KNN) method for training semi-supervised models. The main idea of this method is illustrated in Figure 1. TK-KNN makes two improvements over other pseudo-labeling approaches. First, to address the model overconfidence problem, we use a top-k sampling strategy when assigning pseudo-labels to enforce a balanced set of classes by taking the top-k predictions per class, not simply the predictions that exceed a confidence threshold overall predictions. Furthermore, when selecting the top-k examples the sampling strategy does not simply rely on the model\u2019s predictions, which tend to be noisy. Instead we leverage the embedding space of the labeled and unlabeled examples to find those with similar embeddings and combine them with the models\u2019 predictions. Experiments using standard performance metrics of intent classification are performed on three datasets: CLINC150 [Larson et al., 2019], Banking77 [Casanueva et al., 2020], and Hwu64 [Liu et al., 2019]. We find that the TKKNN method outperforms existing methods in all scenarios and performs exceptionally well in the low-data scenarios."
        },
        {
            "heading": "2 Related Work",
            "text": "Intent Classification The task of intent classification has attracted much attention in recent years due to the increasing use of virtual customer service agents. Recent research into intent classification systems has mainly focused on learning out of distribution data [Zhan et al., 2021, Zhang et al., 2021b, Cheng et al., 2022, Zhou et al., 2022]. These techniques configure their experiments to learn from a reduced number of the classes and treat the remaining classes as out-of-distribution during testing. Although this research is indeed important in its own regard, it deviates from the present work\u2019s focus on semi-supervised learning.\nPseudo Labeling Pseudo labeling is a mainstay in semi-supervised learning [Lee et al., 2013, Rizve et al., 2021, Cascante-Bonilla et al., 2021]. In simple terms, pseudo labelling uses the model itself to acquire hard labels for each of the unlabeled data elements. This is achieved by taking the argmax of the models\u2019 output and treating the resulting label as the example\u2019s label. In this learning regime, the hard labels are assigned to the unlabeled examples without considering the confidence of the model\u2019s predictions. These pseudo-labeled examples are then combined with the labeled data to train the model iteratively. The model is then expected to iteratively improve until convergence. The main drawback of this method is that mislabeled data elements early in training can severely degrade the performance of the system.\nA common practice to help alleviate mislabeled samples is to use a threshold \u03c4 to ensure that only high-quality (i.e., confident) labels are retained [Sohn et al., 2020]. The addition of confidence restrictions into the training process [Sohn et al., 2020] has shown improvements but also restricts the data used at inference time and introduces the confidence threshold value as yet another hyperparameter that needs to be tuned.\nAnother major drawback of this selection method is that the model can become very biased towards the easy classes in the early iterations of learning [Arazo et al., 2020]. Recent methods, such as FlexMatch [Zhang et al., 2021a], have discussed this problem at length and attempted to address this issue with a curriculum learning paradigm that allows each class to have its own threshold. These thresholds tend to be higher for majority classes lower for less-common classes. However, this only\nserves to exacerbate the problem because the lesscommon classes will have less-confident labels. A previous work by Zou et al. [2018] proposes a similar class balancing parameter to be learned per class, but is applied to the task of unsupervised domain adaptation. A close previous work to ours is co-training [Nigam and Ghani, 2000] that iteratively adds a single example from each class throughout the self-training. Another more recent work [Gera et al., 2022] also proposes a balanced sampling mechanism for self-training, but starts from a zero-shot perspective and limits to two cycles of self-training.\nAnother pertinent work is by Chen et al. [2022], who introduce ContrastNet, a framework that leverages contrastive learning for few-shot text classification. This is particularly relevant to our study considering the challenges posed by datasets with a scarce number of labeled examples per class. A notable work by Wang et al. [2022] employs a contrastive learning-enhanced nearest neighbor mechanism for multi-label text classification, which bears some resemblance to the KNN strategies discussed in our work.\nThe TK-KNN strategy described in the present work addresses these issues by learning the decision boundaries for all classes in a balanced way while still giving preference to accurate labels by considering the proximity between the labeled and the unlabeled examples in the embedding space.\nDistance-based Pseudo labeling Another direction explored in recent work is to consider the smoothness and clustering assumptions found in semi-supervised learning [Ouali et al., 2020a] for pseudo labeling. The smoothness assumption states that if two points lie in a high-density region their outputs should be the same. The clustering assumption similarly states that if points are in the same cluster, they are likely from the same class. Recent work by Zhu et al. [2022] propose a training-free approach to detect corrupted labels. They use a kstyle approach to detect corrupted labels that share similar features. The results of this work show that the smoothness and clustering assumptions are also applicable in a latent embedding space and therefore data elements that are close in the latent space are likely to share the same clean label.\nTwo other recent works have made use of these assumptions in semi-supervised learning to improve their pseudo-labeling process. First, Taherkhani et al. [2021] use the Wasserstein dis-\ntance to match clusters of unlabeled examples to labeled clusters for pseudo-labeling.\nSecond, the aptly-named feature affinity based pseudo-labeling [Ding et al., 2019] method uses the cosine similarity between unlabeled examples and cluster centers that have been discovered for each class. The selected pseudo label is determined based on the highest similarity score calculated for the unlabeled example.\nResults from both of these works demonstrate that distance-based pseudo-labeling strategies yield significant improvements over previous methods. However, both of these methods depend on clusters formed from the labeled data. In the intent classification task considered in the current study, the datasets sometimes have an extremely limited number of labeled examples per class, with instances where there is only one labeled example per class. This scarcity of labeled data makes forming reliable clusters quite challenging. Therefore, the TK-KNN model described in the present work adapted the K-Nearest Neighbors search strategy to help guide our pseudo-labeling process."
        },
        {
            "heading": "3 Top-K KNN Semi-Supervised Learning",
            "text": ""
        },
        {
            "heading": "3.1 Problem Definition",
            "text": "We formulate the problem of semi-supervised intent classification as follows:\nGiven a set of labeled intents X = (xn, yn) : n \u2208 (1, ..., N) where xn represents the intent example and yn the corresponding intent class c \u2208 C and a set of unlabeled intents U = um : m \u2208 (1, ...,M), where each instance um is an intent example lacking a label. Intents are fed to the model as input and the model outputs a predicted intent class, denoted as pmodel(c|x, \u03b8), where \u03b8 represents some pre-trained model parameters. Our goal is to learn the optimal parameters for \u03b8."
        },
        {
            "heading": "3.2 Method Overview",
            "text": "As described above, we first employ pseudo labeling to iteratively train (and re-train) a model based on its most confident-past predictions. In the first training cycle, the model is trained on only the small portion of labeled data X . In the subsequent cycles, the model is trained on the union of X and a subset of the unlabeled data U that has been pseudo-labeled by the model in the previous cycle. Figure 2 illustrates an example of this training regime with the TK-KNN method.\nWe use the BERT-base [Devlin et al., 2018] model with an added classification head to the top. The classification head consists of a dropout layer followed by a linear layer with dropout and ends with an output layer that represents the dataset\u2019s class set C. We select the BERT-base model for fair comparison with other methods."
        },
        {
            "heading": "3.3 Top-K Sampling",
            "text": "When applying pseudo-labeling, it is often observed that some classes are easier to predict than others. In practice, this causes the model to become biased towards the easier classes [Arazo et al., 2020] and perform poorly on the more difficult ones. The Top-K sampling process within the TKKNN system seeks to alleviate this issue by growing the pseudo-label set across all labels together.\nWhen we perform pseudo labeling, we select the top-k predictions per class from the unlabeled data. This selection neither uses nor requires any threshold; instead, it limits each class to choose the predictions with the highest confidence. We rank each predicted data element with a score based on the models predicted probability.\nscore(um) = pmodel(y = c|um; \u03b8) (1)\nAfter each training cycle, the number of pseudo labels in the dataset will have increased by k times the number of classes. This process continues until all examples are labeled or some number of pre-defined cycles has been reached. We employ standard early stopping criteria [Prechelt, 1998] during each training cycle to determine whether or not to stop training."
        },
        {
            "heading": "3.4 KNN-Alignment",
            "text": "Although our top-k selection strategy helps alleviate the model\u2019s bias, it still relies entirely on the model predictions. To enhance our top-k selection strategy, we utilize a KNN search to modify the scoring function that is used to rank which pseudolabeled examples should be included in the next training iteration. The intuition for the use of the KNN search comes from the findings in [Zhu et al., 2022] where \"closer\" instances are more likely to share the same label based on the neighborhood information when some labels are corrupted, which often occurs in semi-supervised learning from the pseudo-labeling strategy.\nSpecifically, we extract a latent representation from each example in our training dataset, both the labeled and unlabeled examples. We formulate this latent representation in the same way as SentenceBERT [Reimers and Gurevych, 2019] to construct a robust sentence representation. This representation is defined as the mean-pooled representation of the final BERT layer that we formally define as:\nz = mean([CLS], T1, T2, ..., TM ) (2)\nWhere CLS is the class token, T is each token in the sequence, M is the sequence length, and z is the extracted latent representation. When we perform our pseudo labeling process we extract the latent representation for all of our labeled data X as well as our unlabeled data U .\nFor each unlabeled example, we calculate the cosine similarity between its latent representation and the latent representations of the labeled counterparts belonging to the predicted class.\nThe highest cosine similarity score between the unlabeled example and its labeled neighbors is used to calculate the score of an unlabeled example. Let zm and zn be the latent representations of the unlabeled data point um and a labeled data point xn, respectively. An additional hyperparameter, \u03b2, permits the weighing of the model\u2019s prediction and the cosine similarity for the final scoring function.\nscore(um) = (1\u2212 \u03b2)\u00d7 pmodel(y|um; \u03b8)+ \u03b2 \u00d7 sim(zn, zm) (3)\nIn this equation sim(zn, zm) computes the cosine similarity between the latent representations of um and its closest labeled counterpart in the predicted class. With these scores we then follow the previously discussed top-k selection strategy to ensure balanced classes. The addition of the K-nearest neighbor search helps us to select more accurate labels early in the learning process. We provide pseudo code for our pseudo-labeling strategy in Appendix Algorithm 1."
        },
        {
            "heading": "3.5 Loss Function",
            "text": "As we use the cosine similarity to help our ranking method we want to ensure that similar examples are grouped together in the latent space.While the cross entropy loss is an ideal choice for classification, as it incentivizes the model to produce accurate predictions, it does not guarantee that discriminative features will be learned [Elsayed et al., 2018],\nwhich our pseudo labeling relies on. To address this issue, we supplemented the cross-entropy loss with a supervised contrastive loss [Khosla et al., 2020] and a differential entropy regularization loss [Sablayrolles et al., 2019], and trained the model using all three losses jointly.\nLCE = \u2212 C\u2211 i=1 yi log(y\u0302i) (4)\nWe select the supervised contrastive loss as shown in [Khosla et al., 2020]. This ensures that our model with learn good discriminative features in the latent space that separate examples belonging to different classes. The supervised contrastive loss relies on augmentations of the original examples. To get this augmentation we simply apply a dropout layer of 0.2 to the representations that we extract from the model. As is standard for the supervised contrastive loss we add a separate projection layer to our model to align the representations. The representations fed to the projection layer is the mean-pooled BERT representation as shown in Eq. 2.\nLSCL = \u2211 i\u2208I \u22121 |P (i)| \u2211 p\u2208P (i) log sim(zi, zp)/\u03c4\u2211 a\u2208A(i) sim(zi, za)/\u03c4\n(5)\nWhen adopting the contrastive loss previous works [El-Nouby et al., 2021] have discussed how the model can collapse in dimensions as a result of the loss. We follow this work in adopting a differential entropy regularizer in order to spread the representations our more uniformly. The method we use is based on the Kozachenko and Leonenko [1987] differential entropy estimator:\nLKoLeo = \u2212 1\nN N\u2211 i=1 log(pi) (6)\nWhere pi = min(i \u0338=j)||f(xi) \u2212 f(xj)||. This regularization helps to maximize the distance between each point and its neighbors. By doing so it helps to alleviate the rank collapse issue. We combine this term with the cross-entropy and contrastive objectives, weighting it using a coefficient \u03b3.\nLALL = LCE + LSCL + \u03b3LKoLeo (7)\nThe joint training of these individual components leads our model to have better discriminative\nfeatures that are more robust, that results in improved generalization and prediction accuracy."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Datasets We use three well-known benchmark datasets to test and compare the TK-KNN model against other models on the intent classification task. Our intent classification datasets are CLINC150 [Larson et al., 2019] that contains 150 in-domain intents classes from ten different domains and one out-of-domain class. BANKING77 [Casanueva et al., 2020] that contains 77 intents, all related to the banking domain. HWU64 [Liu et al., 2019] which includes 64 intents coming from 21 different domains. Banking77 and Hwu64 do not provide validation sets, so we created our own from the original training sets. All datasets are in English. A breakdown of each dataset is shown in Table 1.\nWe conducted our experiments with varying amounts of labeled data for each dataset. All methods are run with five random seeds and the mean average accuracy of their results are reported along with their 95% confidence intervals [Dror et al., 2018]."
        },
        {
            "heading": "4.2 Baselines",
            "text": "To perform a proper and thorough comparison of TK-KNN with existing methods, we implemented and repeated the experiments on the following models and strategies.\n\u2022 Supervised: Use only labeled portion of dataset to train the model without any semisupervised training. This model constitutes a competitive lower bound of performance because of the limits in the amount of labeled data.\n\u2022 Pseudo Labeling (PL) [Lee et al., 2013]: This strategy trains the model to convergence then makes predictions on all of the unlabeled\ndata examples. These examples are then combined with the labeled data and used to re-train the model in an iterative manner.\n\u2022 Pseudo Labeling with Threshold (PL-T) [Sohn et al., 2020]: This process follows the pseudo labeling strategy but only selects unlabeled data elements which are predicted above a threshold \u03c4 . We use a \u03c4 of 0.95 based on the findings from previous work.\n\u2022 Pseudo Labeling with Flexmatch (PL-Flex) [Zhang et al., 2021a]: Rather than using a static threshold across all classes, a dynamic threshold is used for each class based on a curriculum learning framework.\n\u2022 GAN-BERT [Croce et al., 2020]: This method applies generative adversarial networks [Goodfellow et al., 2020] to a pretrained BERT model. The generator is an MLP that takes in a noise vector. The output head added to the BERT model acts as the discriminator and includes an extra class for predicting whether a given data element is real or not.\n\u2022 MixText [Chen et al., 2020]: This method extends the MixUp [Zhang et al., 2017] framework to NLP and uses the hidden representation of BERT to mix together. The method also takes advantage of consistency regularization in the form of back translated examples.\n\u2022 TK-KNN : The method described in the present work using top-k sampling with a weighted selection based on model predictions and cosine similarity to the labeled samples."
        },
        {
            "heading": "4.3 Implementation Details",
            "text": "Each method uses the BERT base model with a classification head attached. We use the base BERT implementation provided by Huggingface Wolf et al., 2019, that contains a total of 110M parameters. All models are trained for 30 cycles of self-training. The models are optimized with the AdamW optimizer with a learning rate of 5e-5. Each model is trained until convergence by early stopping applied according to the validation set. We use a batch size of 256 across experiments and limit the sequence length to 64 tokens. For TK-KNN, we set k = 6, \u03b2 = 0.75, and \u03b3 = 0.1 and report the results for\nthese settings. An ablation study of these two hyperparameters is presented later. For details on the settings used for MixText please see Appendix B.\nComputational Use. In total we estimate that we used around 18,000 GPU hours for this project. For the final experiments and ablation studies we estimate that the TK-KNN model used 4400 GPU hours. Experiments were carried out on Nvidia Tesla P100 GPUs that each had 12GB of memory and 16GB of memory."
        },
        {
            "heading": "5 Results",
            "text": "Results from these experiments are shown in Table 2. These quantitative results demonstrate that TK-KNN yielded the best performance on the benchmark datasets. We observed the most significant performance gains for CLINC150 and BANKING77, where these datasets have more classes. For instance, on the CLINC150 dataset with 1% labeled data, our method performs 10.92% better than the second best strategy, FlexMatch. As the portion of labeled data used increases, we notice that the effectiveness of TK-KNN diminishes.\nAnother observation from these results is that the GAN-BERT model tends to be unstable when the\nlabeled data is limited. However, GAN-BERT does improve as the proportion of labeled data increases. This is consistent with previous findings on training GANs from computer vision [Salimans et al., 2016, Arjovsky and Bottou, 2017]. We also find that while the MixText method shows improvements the benefits of consistency regularization are not as strong compared to works from the computer vision domain.\nThese results demonstrate the benefits of TKKNN\u2019s balanced sampling strategy and its use of the distances in the latent space."
        },
        {
            "heading": "5.1 Overconfidence in Pseudo-Labelling Regimes",
            "text": "A key observation we found throughout selftraining was that the performance of existing pseudo-labelling methods tended to degrade as the number of cycles increased. An example of this is illustrated in Figure 3. Here we see that when a pre-defined threshold is used, the model tends to improve performance for the first few training cycles. After that point, the pseudo-labeling becomes heavily biased towards the easier classes. This causes the model to become overconfident in predictions for those classes and neglect more difficult classes. PL-Flex corrects this issue but converges much earlier in the learning process. TK-KNN achieves the best performance thanks to the slower balanced pseudo-labeling approach. This process helps the model learn clearer decision boundaries for all classes simultaneously and prevent overconfidence in the model in some classes."
        },
        {
            "heading": "5.2 Ablation Study",
            "text": "Because TK-KNN is different from existing methods in two distinct ways: (1) top-k balanced sampling and (2) KNN ranking, we perform a set of ablation experiments to better understand how each of these affects performance. Specifically, we test TK-KNN under three scenarios, top-k sampling without balancing the classes, top-k sampling with balanced classes, and top-k KNN without balancing for classes. When we perform top-k sampling in an unbalanced manner, we ensure that the total data sampled is still equal to k \u2217 C, where C is the number of classes. We report the results for these experiments in Table 3. We also conduct experiments on the addition of the different loss objectives detailed in Appendix C\nThe results from the ablation study demonstrate both the effectiveness of top-k sampling and KNN ranking. A comparison between our unbalanced sampling top-k sampling and balanced versions show a drastic difference in performance across all datasets. We highlight again that the performance difference is greatest in the lowest resource setting, with a 12.47% increase in accuracy for CLINC150 in the 1% setting.\nResults from the TK-KNN method with unbalanced sampling also show an improvement over unbalanced sampling alone. This increase in performance is smaller than the difference between unbalanced and balanced sampling but still highlights the benefits of leveraging the geometry for selective pseudo-labeling. We also present an ablation of the Top-k sampling methodology when passing correctly labeled examples in the Appendix A."
        },
        {
            "heading": "5.3 Parameter Search",
            "text": "TK-KNN relies on two hyperparameters k and \u03b2 that can affect performance based on how they are configured. We explore experiments to gauge their effect on learning by testing k \u2208 (4, 6, 8) and \u03b2 \u2208 (0.0, 0.25, 0.50, 0.75, 1.00). When varying k we hold \u03b2 at 0.75. For \u03b2 experiments we keep k = 6. When \u03b2 = 0.0, this is equivalent to just topk sampling based on Equation 3. Alternatively, when \u03b2 = 1.0, this is equivalent to only using the KNN similarity for ranking. Results from our experiments are shown in Figures 4 for \u03b2 and 5 for k.\nAs we varied the \u03b2 parameter, we noticed that all configurations tended to have similar training patterns. After we trained the model for the first five cycles, the model tended to move in small jumps between subsequent cycles. From the illustration, we can see that no single method was always the best, but the model tended to perform worse when\n\u03b2 = 0.0, highlighting the benefits of including our KNN similarity for ranking. The model reached the best performance when \u03b2 = 0.75, which occurs about a third of the way through the training process.\nComparison of values for k show that TK-KNN is robust to adjustments in this hyperparameter. We notice slight performance benefits from selecting a higher k of 6 and 8 in comparison to 4. When a higher value of k is used the model will see an increase in performance earlier in the self-training process, as it has more examples to train from. This is only acheivable though when high quality correct samples are selected across the entire class distribution. If a k value was selected that is too large, more bad examples will be included early in the training process and may result in poor model performance."
        },
        {
            "heading": "6 Conclusions",
            "text": "This paper introduces TK-KNN, a balanced distance-based pseudo-labeling approach for semisupervised intent classification. TK-KNN deviates from previous pseudo-labeling methods as it does not rely on a threshold to select the samples. Instead, we show that a balanced approach that takes the model prediction and K-Nearest Neighbor similarity measure allows for more robust decision boundaries to be learned. Experiments on three popular intent classification datasets, CLINC150, Banking77, and Hwu64, demonstrate that our method improved performance in all scenarios."
        },
        {
            "heading": "7 Limitations",
            "text": "While our method shows noticeable improvements, it is not without limitations. Our method does not require searching for a good threshold but instead requires two different hyperparameters, k and \u03b2, that must be found. We offer a reasonable method and findings for selecting both of these but others may want to search for other combinations depending on the dataset. A noticeable drawback from our self-training method is that more cycles of training will need to be done, especially if the value of k is small. This requires much more GPU usage to converge to a good point. Further, we did not explore any heavily imbalanced datasets, so we are unaware of how TK-KNN would perform under those scenarios."
        },
        {
            "heading": "8 Ethics Statement",
            "text": "This work can be used to help improve current virtual assistant systems for many of the businesses that may be seeking to use one. As the goal of these systems is to understand a persons request, failures can lead to wrong actions being taken that potentially impact an individual."
        },
        {
            "heading": "A Upper Bound Analysis",
            "text": "We further ran experiments to gauge the performance of top-k sampling when ground truth labels are fed to the model instead of predicted pseudo labels. This experiment gives us an indicator as to how performance should increase throughout the self-training process in an ideal pseudo-labeling scenario. We present the results of this in Figure. 6. As expected, the model tends to converge towards a fully supervised performance as the cycle increases and more data is (pseudo-)labeled. Another point of interest is that the method\u2019s upper bound can continue learning with proper labels, while TKKNN method tends to converge earlier. The upper bound method also takes a significant increase in the first few cycles as well. This highlights a need to investigate methods for accurate pseudo label selection further, so that the model can continue to improve."
        },
        {
            "heading": "B MixText Experiments",
            "text": "The MixText method requires a number of hyperparameters to be selected in order to acheive good performance. The method also relies on data augmentation of the original text. For our experiments we used the Helsinki-NLP models available on the huggingface repository 1 to perform back translation. Following the original paper we performed back translation into German (de) and Russian (ru). Our experiments used a labeled batch size of 4 and unlabeled batch size of 8, the same as the original paper. The Temperature parameter was set to 0.5.\n1https://huggingface.co/Helsinki-NLP/opus-mt-en-de https://huggingface.co/Helsinki-NLP/opus-mt-en-ru"
        },
        {
            "heading": "C Ablation Table",
            "text": "We present a detailed breakdown of our full ablation results in Table 4. These results include the performance when using only the cross-entropy loss, as well as various combinations of the supervised contrastive loss and differential entropy regularizer. The results in this table demonstrate that the inclusion of additional loss objectives improves performance. This is particularly evident when we add our KNN objective, as we observe an increase of approximately 1-4%. Although the addition of the differential entropy regularizer yields smaller performance improvements, it remains beneficial to our method overall."
        }
    ],
    "title": "TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for Semi-Supervised Intent Classification",
    "year": 2023
}