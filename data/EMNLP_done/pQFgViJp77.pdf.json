{
    "abstractText": "Instruction tuned large language models (LLMs), such as ChatGPT, demonstrate remarkable performance in a wide range of tasks. Despite numerous recent studies that examine the performance of instruction-tuned LLMs on various NLP benchmarks, there remains a lack of comprehensive investigation into their ability to understand cross-lingual sociopragmatic meaning (SM), i.e., meaning embedded within social and interactive contexts. This deficiency arises partly from SM not being adequately represented in any of the existing benchmarks. To address this gap, we present SPARROW, an extensive multilingual benchmark specifically designed for SM understanding. SPARROW comprises 169 datasets covering 13 task types across six primary categories (e.g., anti-social language detection, emotion recognition). SPARROW datasets encompass 64 different languages originating from 12 language families representing 16 writing scripts. We evaluate the performance of various multilingual pretrained language models (e.g., mT5) and instruction-tuned LLMs (e.g., BLOOMZ, ChatGPT) on SPARROW through fine-tuning, zero-shot, and/or few-shot learning. Our comprehensive analysis reveals that existing opensource instruction tuned LLMs still struggle to understand SM across various languages, performing close to a random baseline in some cases. We also find that although ChatGPT outperforms many LLMs, it still falls behind task-specific finetuned models with a gap of 12.19 SPARROW score. Our benchmark is available at: https://github.com/",
    "authors": [
        {
            "affiliations": [],
            "name": "Chiyu Zhang"
        },
        {
            "affiliations": [],
            "name": "Khai Duy Doan\u03bb"
        },
        {
            "affiliations": [],
            "name": "Qisheng Liao\u03bb"
        },
        {
            "affiliations": [],
            "name": "Muhammad Abdul-Mageed"
        }
    ],
    "id": "SP:a4cf104ea20db3b5874aa744f0aae848a3e57c03",
    "references": [
        {
            "authors": [
                "Muhammad Abdul-Mageed",
                "Mona T. Diab",
                "Sandra K\u00fcbler."
            ],
            "title": "SAMAR: subjectivity and sentiment analysis for arabic social media",
            "venue": "Comput. Speech Lang., 28(1):20\u201337.",
            "year": 2014
        },
        {
            "authors": [
                "Muhammad Abdul-Mageed",
                "AbdelRahim A. Elmadany",
                "El Moatez Billah Nagoudi."
            ],
            "title": "ARBERT & MARBERT: deep bidirectional transformers for arabic",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Muhammad Abdul-Mageed",
                "Lyle Ungar."
            ],
            "title": "EmoNet: Fine-grained emotion detection with gated recurrent neural networks",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2017
        },
        {
            "authors": [
                "Muhammad Abdul-Mageed",
                "Chiyu Zhang",
                "AbdelRahim Elmadany",
                "Houda Bouamor",
                "Nizar Habash."
            ],
            "title": "NADI 2022: The third nuanced Arabic dialect identification shared task",
            "venue": "Proceedings of the The Seventh Arabic Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Muhammad Abdul-Mageed",
                "Chiyu Zhang",
                "Azadeh Hashemi",
                "El Moatez Billah Nagoudi."
            ],
            "title": "AraNet: A deep learning toolkit for Arabic social media",
            "venue": "Proceedings of the 4th Workshop on 9https://alliancecan.ca",
            "year": 2020
        },
        {
            "authors": [
                "Ibrahim Abu Farha",
                "Walid Magdy."
            ],
            "title": "From Arabic sentiment analysis to sarcasm detection: The ArSarcasm dataset",
            "venue": "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language",
            "year": 2020
        },
        {
            "authors": [
                "Akshita Aggarwal",
                "Anshul Wadhawan",
                "Anshima Chaudhary",
                "Kavita Maurya."
            ],
            "title": "did you really mean what you said?\u201d : Sarcasm detection in Hindi-English code-mixed data using bilingual word embeddings",
            "venue": "Proceedings of the Sixth Workshop",
            "year": 2020
        },
        {
            "authors": [
                "Kabir Ahuja",
                "Rishav Hada",
                "Millicent Ochieng",
                "Prachi Jain",
                "Harshita Diddee",
                "Samuel Maina",
                "Tanuja Ganu",
                "Sameer Segal",
                "Maxamed Axmed",
                "Kalika Bali",
                "Sunayana Sitaram."
            ],
            "title": "MEGA: multilingual evaluation of generative AI",
            "venue": "CoRR, abs/2303.12528.",
            "year": 2023
        },
        {
            "authors": [
                "Azalden Alakrot",
                "Liam Murray",
                "Nikola S. Nikolov."
            ],
            "title": "Dataset construction for the detection of antisocial behaviour in online communication in arabic",
            "venue": "Fourth International Conference On Arabic Computational Linguistics, ACLING 2018, Novem-",
            "year": 2018
        },
        {
            "authors": [
                "Ali Alshehri",
                "El Moatez Billah Nagoudi",
                "Muhammad Abdul-Mageed."
            ],
            "title": "Understanding and detecting dangerous speech in social media",
            "venue": "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared",
            "year": 2020
        },
        {
            "authors": [
                "Adam Amram",
                "Anat Ben David",
                "Reut Tsarfaty."
            ],
            "title": "Representations and architectures in neural sentiment analysis for morphologically rich languages: A case study from Modern Hebrew",
            "venue": "Proceedings of the 27th International Conference on",
            "year": 2018
        },
        {
            "authors": [
                "Seyed Arad Ashrafi Asli",
                "Behnam Sabeti",
                "Zahra Majdabadi",
                "Preni Golazizian",
                "reza fahmi",
                "Omid Momenzadeh"
            ],
            "title": "Optimizing annotation effort using active learning strategies: A sentiment analysis case study in Persian",
            "year": 2020
        },
        {
            "authors": [
                "Stefano Cresci",
                "Heike Trautmann",
                "Frank Neumann",
                "Christian Grimme."
            ],
            "title": "Benchmarking crisis in social media analytics: A solution for the datasharing problem",
            "venue": "Social Science Computer Review, 40(6):1496\u20131522.",
            "year": 2022
        },
        {
            "authors": [
                "David Bamman",
                "Noah A. Smith."
            ],
            "title": "Contextualized sarcasm detection on twitter",
            "venue": "Proceedings of the Ninth International Conference on Web and Social Media, ICWSM 2015, University of Oxford, Oxford, UK, May 26-29, 2015, pages 574\u2013577. AAAI",
            "year": 2015
        },
        {
            "authors": [
                "Yejin Bang",
                "Samuel Cahyawijaya",
                "Nayeon Lee",
                "Wenliang Dai",
                "Dan Su",
                "Bryan Wilie",
                "Holy Lovenia",
                "Ziwei Ji",
                "Tiezheng Yu",
                "Willy Chung",
                "Quyet V. Do",
                "Yan Xu",
                "Pascale Fung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of chatgpt on",
            "year": 2023
        },
        {
            "authors": [
                "Francesco Barbieri",
                "Valerio Basile",
                "Danilo Croce",
                "Malvina Nissim",
                "Nicole Novielli",
                "Viviana Patti."
            ],
            "title": "Overview of the evalita 2016 sentiment polarity classification task",
            "venue": "Proceedings of Third Italian Conference on Computational Linguistics",
            "year": 2016
        },
        {
            "authors": [
                "Francesco Barbieri",
                "Jose Camacho-Collados",
                "Luis Espinosa Anke",
                "Leonardo Neves."
            ],
            "title": "TweetEval: Unified benchmark and comparative evaluation for tweet classification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Francesco Barbieri",
                "Luis Espinosa Anke",
                "Jose Camacho-Collados."
            ],
            "title": "XLM-T: Multilingual language models in Twitter for sentiment analysis and beyond",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages",
            "year": 2022
        },
        {
            "authors": [
                "Valerio Basile",
                "Andrea Bolioli",
                "Viviana Patti",
                "Paolo Rosso",
                "Malvina Nissim."
            ],
            "title": "Overview of the evalita 2014 sentiment polarity classification task",
            "venue": "Overview of the Evalita 2014 SENTIment POLarity Classification Task, pages 50\u201357.",
            "year": 2014
        },
        {
            "authors": [
                "Valerio Basile",
                "Cristina Bosco",
                "Elisabetta Fersini",
                "Debora Nozza",
                "Viviana Patti",
                "Francisco Manuel Rangel Pardo",
                "Paolo Rosso",
                "Manuela Sanguinetti"
            ],
            "title": "SemEval-2019 task 5: Multilingual detection of hate speech against immigrants",
            "year": 2019
        },
        {
            "authors": [
                "Federico Bianchi",
                "Debora Nozza",
                "Dirk Hovy"
            ],
            "title": "FEEL-IT: Emotion and sentiment classification",
            "year": 2021
        },
        {
            "authors": [
                "Federico Bianchi",
                "Debora Nozza",
                "Dirk Hovy."
            ],
            "title": "XLM-EMO: Multilingual emotion prediction in social media text",
            "venue": "Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis, pages 195\u2013203,",
            "year": 2022
        },
        {
            "authors": [
                "Vladislav Blinov",
                "Valeria Bolotova-Baranova",
                "Pavel Braslavski."
            ],
            "title": "Large dataset and language model fun-tuning for humor recognition",
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Cristina Bosco",
                "Felice Dell\u2019Orletta",
                "Fabio Poletto",
                "Manuela Sanguinetti",
                "Maurizio Tesconi"
            ],
            "title": "Overview of the EVALITA 2018 hate speech detection task",
            "venue": "In Proceedings of the Sixth Evaluation Campaign of Natural Language Processing",
            "year": 2018
        },
        {
            "authors": [
                "Diana Boxer",
                "Florencia Cort\u00e9s-Conde."
            ],
            "title": "Social Groups and Relational Networks, Cambridge Handbooks in Language and Linguistics, page 227\u2013246",
            "venue": "Cambridge University Press.",
            "year": 2021
        },
        {
            "authors": [
                "Amodei."
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.",
            "year": 2020
        },
        {
            "authors": [
                "Henrico Bertini Brum."
            ],
            "title": "Expans\u00e3o de recursos para an\u00e1lise de sentimentos usando aprendizado semi-supervisionado",
            "venue": "Ph.D. thesis, Universidade de S\u00e3o Paulo.",
            "year": 2018
        },
        {
            "authors": [
                "Henrico Bertini Brum",
                "Maria das Gra\u00e7as Volpe Nunes."
            ],
            "title": "Building a sentiment corpus of tweets in Brazilian Portuguese",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki,",
            "year": 2018
        },
        {
            "authors": [
                "Neil Vicente Cabasag",
                "Vicente Raphael Chan",
                "Sean Christian Lim",
                "Mark Edward Gonzales",
                "Charibeth Cheng"
            ],
            "title": "Hate speech in philippine election-related tweets: Automatic detection and classification using natural language processing",
            "year": 2019
        },
        {
            "authors": [
                "Bharathi Raja Chakravarthi",
                "Ruba Priyadharshini",
                "Vigneshwaran Muralidaran",
                "Navya Jose",
                "Shardul Suryawanshi",
                "Elizabeth Sherly",
                "John P. McCrae"
            ],
            "title": "Dravidiancodemix: sentiment analysis and offensive language identification dataset for dravid",
            "year": 2022
        },
        {
            "authors": [
                "Qianben Chen",
                "Richong Zhang",
                "Yaowei Zheng",
                "Yongyi Mao."
            ],
            "title": "Dual contrastive learning: Text classification via label-aware data augmentation",
            "venue": "CoRR, abs/2201.08702.",
            "year": 2022
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Patricia Chiril",
                "V\u00e9ronique Moriceau",
                "Farah Benamara",
                "Alda Mari",
                "Gloria Origgi",
                "Marl\u00e8ne CoulombGully."
            ],
            "title": "An annotated corpus for sexism detection in french tweets",
            "venue": "Proceedings of The 12th Language Resources and Evaluation Confer-",
            "year": 2020
        },
        {
            "authors": [
                "Luis Chiruzzo",
                "Santiago Castro",
                "Santiago G\u00f3ngora",
                "Aiala Ros\u00e1",
                "J.A. Meaney",
                "Rada Mihalcea."
            ],
            "title": "Overview of HAHA at IberLEF 2021: Detecting, rating and analyzing humor in Spanish",
            "venue": "Proces. del Leng. Natural, 67:257\u2013268.",
            "year": 2021
        },
        {
            "authors": [
                "Paul F. Christiano",
                "Jan Leike",
                "Tom B. Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei."
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Pro-",
            "year": 2017
        },
        {
            "authors": [
                "Alessandra Teresa Cignarella",
                "Simona Frenda",
                "Valerio Basile",
                "Cristina Bosco",
                "Viviana Patti",
                "Paolo Rosso."
            ],
            "title": "Overview of the EVALITA 2018 task on irony detection in italian tweets (ironita)",
            "venue": "Proceedings of the Sixth Evaluation Campaign of",
            "year": 2018
        },
        {
            "authors": [
                "Alexandra Ciobotaru",
                "Liviu P. Dinu."
            ],
            "title": "Red: A novel dataset for romanian emotion detection from tweets",
            "venue": "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021), pages 296\u2013305, Varna, Bul-",
            "year": 2021
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Douwe Kiela."
            ],
            "title": "SentEval: An evaluation toolkit for universal sentence representations",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language",
            "year": 2018
        },
        {
            "authors": [
                "Diogo Cortiz",
                "Jefferson O. Silva",
                "Newton Calegari",
                "Ana Lu\u00edsa Freitas",
                "Ana Ang\u00e9lica Soares",
                "Carolina Botelho",
                "Gabriel Gaudencio R\u00eago",
                "Waldir Sampaio",
                "Paulo Sergio Boggio"
            ],
            "title": "A weak supervised dataset of fine-grained emotions in portuguese",
            "year": 2021
        },
        {
            "authors": [
                "Mithun Das",
                "Saurabh Kumar Pandey",
                "Animesh Mukherjee."
            ],
            "title": "Evaluating chatgpt\u2019s performance for multilingual and emoji-based hate speech detection",
            "venue": "CoRR, abs/2305.13276.",
            "year": 2023
        },
        {
            "authors": [
                "Thomas Davidson",
                "Dana Warmsley",
                "Michael W. Macy",
                "Ingmar Weber."
            ],
            "title": "Automated hate speech detection and the problem of offensive language",
            "venue": "Proceedings of the Eleventh International Conference on Web and Social Media, ICWSM 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Alexandra DeLucia",
                "Shijie Wu",
                "Aaron Mueller",
                "Carlos Aguirre",
                "Mark Dredze",
                "Philip Resnik."
            ],
            "title": "Bernice: A multilingual pre-trained encoder for Twitter",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Process-",
            "year": 2022
        },
        {
            "authors": [
                "Dorottya Demszky",
                "Dana Movshovitz-Attias",
                "Jeongwoo Ko",
                "Alan Cowen",
                "Gaurav Nemade",
                "Sujith Ravi."
            ],
            "title": "GoEmotions: A dataset of finegrained emotions",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Jiawen Deng",
                "Jingyan Zhou",
                "Hao Sun",
                "Fei Mi",
                "Minlie Huang."
            ],
            "title": "COLD: A benchmark for chinese offensive language detection",
            "venue": "CoRR, abs/2201.06025.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Mountaga Diallo",
                "Chayma Fourati",
                "Hatem Haddad"
            ],
            "title": "Bambara language dataset for sentiment analysis",
            "year": 2021
        },
        {
            "authors": [
                "Alexiei Dingli",
                "Nicole Sant."
            ],
            "title": "Sentiment analysis on maltese using machine learning",
            "venue": "Proceedings of The Tenth International Conference on Advances in Semantic Processing (SEMAPRO 2016), pages 21\u201325.",
            "year": 2016
        },
        {
            "authors": [
                "Stefan Daniel Dumitrescu",
                "Andrei-Marius Avram",
                "Sampo Pyysalo."
            ],
            "title": "The birth of romanian BERT",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings",
            "year": 2020
        },
        {
            "authors": [
                "AbdelRahim A. Elmadany",
                "El Moatez Billah Nagoudi",
                "Muhammad Abdul-Mageed."
            ],
            "title": "ORCA: A challenging benchmark for arabic language understanding",
            "venue": "CoRR, abs/2212.10758.",
            "year": 2022
        },
        {
            "authors": [
                "Ibrahim Abu Farha",
                "Wajdi Zaghouani",
                "Walid Magdy."
            ],
            "title": "Overview of the WANLP 2021 shared task on sarcasm and sentiment detection in arabic",
            "venue": "Proceedings of the Sixth Arabic Natural Language Processing Workshop, WANLP 2021, Kyiv,",
            "year": 2021
        },
        {
            "authors": [
                "Bjarke Felbo",
                "Alan Mislove",
                "Anders S\u00f8gaard",
                "Iyad Rahwan",
                "Sune Lehmann."
            ],
            "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
            "venue": "Proceedings of the 2017 Conference on",
            "year": 2017
        },
        {
            "authors": [
                "Mauajama Firdaus",
                "Hardik Chauhan",
                "Asif Ekbal",
                "Pushpak Bhattacharyya"
            ],
            "title": "MEISD: A multimodal multi-label emotion, intensity and sentiment",
            "year": 2020
        },
        {
            "authors": [
                "Paula Fortuna",
                "Jo\u00e3o Rocha da Silva",
                "Juan SolerCompany",
                "Leo Wanner",
                "S\u00e9rgio Nunes."
            ],
            "title": "A hierarchically-labeled Portuguese hate speech dataset",
            "venue": "Proceedings of the Third Workshop on Abusive Language Online, pages 94\u2013104, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Zou"
            ],
            "title": "A framework for few-shot language model evaluation",
            "year": 2021
        },
        {
            "authors": [
                "Bilal Ghanem",
                "Jihen Karoui",
                "Farah Benamara",
                "V\u00e9ronique Moriceau",
                "Paolo Rosso."
            ],
            "title": "IDAT at FIRE2019: overview of the track on irony detection in arabic tweets",
            "venue": "FIRE \u201919: Forum for Information Retrieval Evaluation, Kolkata, India, De-",
            "year": 2019
        },
        {
            "authors": [
                "Preni Golazizian",
                "Behnam Sabeti",
                "Seyed Arad Ashrafi Asli",
                "Zahra Majdabadi",
                "Omid Momenzadeh",
                "Reza Fahmi."
            ],
            "title": "Irony detection in Persian language: A transfer learning approach using emoji prediction",
            "venue": "Proceedings of the 12th Language",
            "year": 2020
        },
        {
            "authors": [
                "Xiaochang Gong",
                "Qin Zhao",
                "Jun Zhang",
                "Ruibin Mao",
                "Ruifeng Xu."
            ],
            "title": "The design and construction of a chinese sarcasm dataset",
            "venue": "Proceedings of The 12th Language Resources and Evaluation Conference, LREC 2020, Marseille, France, May 11-16,",
            "year": 2020
        },
        {
            "authors": [
                "Raymond G Gordon Jr."
            ],
            "title": "Ethnologue, languages of the world",
            "venue": "http://www. ethnologue. com/.",
            "year": 2005
        },
        {
            "authors": [
                "Tushar Goswamy",
                "Ishika Singh",
                "Ahsan Barkati",
                "Ashutosh Modi."
            ],
            "title": "Adapting a language model for controlled affective text generation",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 2787\u20132801,",
            "year": 2020
        },
        {
            "authors": [
                "Zekeriya G\u00fcven",
                "Banu Diri",
                "Tolgahan \u00c7akalo\u011flu."
            ],
            "title": "Comparison of n-stage latent dirichlet allocation versus other topic modeling methods for emotion analysis",
            "venue": "Journal of the Faculty of Engineering and Architecture of Gazi University, 35(4).",
            "year": 2020
        },
        {
            "authors": [
                "Thuy Nguyen"
            ],
            "title": "Emotion recognition",
            "year": 2019
        },
        {
            "authors": [
                "Pei Ke",
                "Haozhe Ji",
                "Siyang Liu",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "SentiLARE: Sentiment-aware language representation learning with linguistic knowledge",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
            "year": 2020
        },
        {
            "authors": [
                "Jan Koco\u0144",
                "Piotr Mi\u0142kowski",
                "Monika Za\u015bkoZieli\u0144ska"
            ],
            "title": "Multi-level sentiment analysis of PolEmo 2.0: Extended corpus of multi-domain consumer reviews",
            "venue": "In Proceedings of the 23rd Conference on Computational Natural Language Learning",
            "year": 2019
        },
        {
            "authors": [
                "Petra Kralj Novak",
                "Igor Mozeti\u010d",
                "Nikola Ljube\u0161i\u0107."
            ],
            "title": "Slovenian twitter hate speech dataset IMSyPP-sl",
            "venue": "Slovenian language resource repository CLARIN.SI.",
            "year": 2021
        },
        {
            "authors": [
                "Atharva Kulkarni",
                "Meet Mandhane",
                "Manali Likhitkar",
                "Gayatri Kshirsagar",
                "Raviraj Joshi."
            ],
            "title": "L3cubemahasent: A marathi tweet-based sentiment analysis dataset",
            "venue": "CoRR, abs/2103.11408.",
            "year": 2021
        },
        {
            "authors": [
                "Ritesh Kumar",
                "Aishwarya N. Reganti",
                "Akshit Bhatia",
                "Tushar Maheshwari."
            ],
            "title": "Aggressionannotated corpus of hindi-english code-mixed data",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC",
            "year": 2018
        },
        {
            "authors": [
                "Viet Dac Lai",
                "Nghia Trung Ngo",
                "Amir Pouran Ben Veyseh",
                "Hieu Man",
                "Franck Dernoncourt",
                "Trung Bui",
                "Thien Huu Nguyen."
            ],
            "title": "Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning",
            "venue": "CoRR,",
            "year": 2023
        },
        {
            "authors": [
                "Md. Tahmid Rahman Laskar",
                "M. Saiful Bari",
                "Mizanur Rahman",
                "Md Amran Hossen Bhuiyan",
                "Shafiq Joty",
                "Jimmy Xiangji Huang."
            ],
            "title": "A systematic study and comprehensive evaluation of chatgpt on benchmark datasets",
            "venue": "CoRR, abs/2305.18486.",
            "year": 2023
        },
        {
            "authors": [
                "Sophia Lee",
                "Zhongqing Wang."
            ],
            "title": "Emotion in code-switching texts: Corpus construction and analysis",
            "venue": "Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing, pages 91\u2013 99, Beijing, China. Association for Computational",
            "year": 2015
        },
        {
            "authors": [
                "Haonan Li",
                "Fajri Koto",
                "Minghao Wu",
                "Alham Fikri Aji",
                "Timothy Baldwin."
            ],
            "title": "Bactrian-x : A multilingual replicable instruction-following model with low-rank adaptation",
            "venue": "CoRR, abs/2305.15011.",
            "year": 2023
        },
        {
            "authors": [
                "Mounika Marreddy",
                "Subba Reddy Oota",
                "Lakshmi Sireesha Vakada",
                "Venkata Charan Chinni",
                "Radhika Mamidi."
            ],
            "title": "Am i a resource-poor language? data sets, embeddings, models and analysis for four different nlp tasks in telugu language",
            "venue": "ACM",
            "year": 2022
        },
        {
            "authors": [
                "J.A. Meaney",
                "Steven Wilson",
                "Luis Chiruzzo",
                "Adam Lopez",
                "Walid Magdy."
            ],
            "title": "SemEval 2021 task 7: HaHackathon, detecting and rating humor and offense",
            "venue": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),",
            "year": 2021
        },
        {
            "authors": [
                "Youssef Mohamed",
                "Faizan Farooq Khan",
                "Kilichbek Haydarov",
                "Mohamed Elhoseiny."
            ],
            "title": "It is okay to not be okay: Overcoming emotional bias in affective image captioning by contrastive data collection",
            "venue": "IEEE/CVF Conference on Computer Vision and",
            "year": 2022
        },
        {
            "authors": [
                "Saif Mohammad",
                "Felipe Bravo-Marquez",
                "Mohammad Salameh",
                "Svetlana Kiritchenko."
            ],
            "title": "SemEval-2018 task 1: Affect in tweets",
            "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation, pages 1\u201317, New Orleans, Louisiana.",
            "year": 2018
        },
        {
            "authors": [
                "Jihyung Moon",
                "Won Ik Cho",
                "Junbum Lee."
            ],
            "title": "BEEP! Korean corpus of online news comments for toxic speech detection",
            "venue": "Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media, pages 25\u201331, Online. As-",
            "year": 2020
        },
        {
            "authors": [
                "Igor Mozeti\u010d",
                "Miha Gr\u010dar",
                "Jasmina Smailovi\u0107."
            ],
            "title": "Multilingual twitter sentiment classification: The role of human annotators",
            "venue": "PloS one, 11(5):e0155036.",
            "year": 2016
        },
        {
            "authors": [
                "Hamdy Mubarak",
                "Kareem Darwish",
                "Walid Magdy",
                "Tamer Elsayed",
                "Hend Al-Khalifa."
            ],
            "title": "Overview of OSACT4 Arabic offensive language detection shared task",
            "venue": "Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Hagos Tesfahun Gebremichael",
                "Bernard Opoku",
                "Steven Arthur."
            ],
            "title": "Afrisenti: A twitter sentiment analysis benchmark for african languages",
            "venue": "CoRR, abs/2302.08956.",
            "year": 2023
        },
        {
            "authors": [
                "Pavel Brazdil."
            ],
            "title": "Naijasenti: A nigerian twitter sentiment corpus for multilingual sentiment analysis",
            "venue": "Proceedings of the 13th Language Resources and Evaluation Conference, pages 590\u2013602, Marseille, France. European Language Resources Asso-",
            "year": 2022
        },
        {
            "authors": [
                "Hala Mulki",
                "Hatem Haddad",
                "Chedi Bechikh Ali",
                "Halima Alshabani."
            ],
            "title": "L-HSAB: A Levantine Twitter dataset for hate speech and abusive language",
            "venue": "Proceedings of the Third Workshop on Abusive Language Online, pages 111\u2013118, Flo-",
            "year": 2019
        },
        {
            "authors": [
                "Sebastian Nordhoff",
                "Harald Hammarstr\u00f6m."
            ],
            "title": "Glottolog/langdoc: Defining dialects, languages, and language families as collections of resources",
            "venue": "Proceedings of the First International Workshop on Linked Science 2011, Bonn, Germany, October 24,",
            "year": 2011
        },
        {
            "authors": [
                "Emily \u00d6hman",
                "Marc P\u00e0mies",
                "Kaisla Kajava",
                "J\u00f6rg Tiedemann."
            ],
            "title": "XED: A multilingual dataset for sentiment analysis and emotion detection",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, COLING",
            "year": 2020
        },
        {
            "authors": [
                "Birgitta Ojamaa",
                "P\u00e4ivi Kristiina Jokinen",
                "Kadri Muischenk."
            ],
            "title": "Sentiment analysis on conversational texts",
            "venue": "Proceedings of the 20th Nordic Conference of Computational Linguistics, NODALIDA 2015, Institute of the Lithuanian Language,",
            "year": 2015
        },
        {
            "authors": [
                "Shereen Oraby",
                "Vrindavan Harrison",
                "Lena Reed",
                "Ernesto Hernandez",
                "Ellen Riloff",
                "Marilyn Walker."
            ],
            "title": "Creating and characterizing a diverse corpus of sarcasm in dialogue",
            "venue": "Proceedings of the 17th Annual Meeting of the Special Interest Group",
            "year": 2016
        },
        {
            "authors": [
                "Reynier Ortega-Bueno",
                "Francisco Rangel",
                "D Hern\u00e1ndez Far\u0131as",
                "Paolo Rosso",
                "Manuel Montes-y G\u00f3mez",
                "Jos\u00e9 E Medina Pagola."
            ],
            "title": "Overview of the task on irony detection in spanish variants",
            "venue": "Proceedings of the Iberian Languages Evaluation Fo-",
            "year": 2019
        },
        {
            "authors": [
                "Nedjma Ousidhoum",
                "Zizheng Lin",
                "Hongming Zhang",
                "Yangqiu Song",
                "Dit-Yan Yeung."
            ],
            "title": "Multilingual and multi-aspect hate speech analysis",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
            "year": 2019
        },
        {
            "authors": [
                "Welinder",
                "Paul F. Christiano",
                "Jan Leike",
                "Ryan Lowe."
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Wuraola Fisayo Oyewusi",
                "Olubayo Adekanmbi",
                "Olalekan Akinsande"
            ],
            "title": "Semantic enrichment of nigerian pidgin english for contextual sentiment classification",
            "year": 2020
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
            "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, 21-26 July, 2004,",
            "year": 2004
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
            "venue": "Proceedings of the 43rd Annual Meeting of the Association for",
            "year": 2005
        },
        {
            "authors": [
                "Braja Gopal Patra",
                "Dipankar Das",
                "Amitava Das."
            ],
            "title": "Sentiment analysis of code-mixed indian languages: An overview of sail_code-mixed shared task @icon-2017",
            "venue": "CoRR, abs/1803.06745.",
            "year": 2018
        },
        {
            "authors": [
                "Flor Miriam Plaza del Arco",
                "Carlo Strapparava",
                "L. Alfonso Urena Lopez",
                "Maite Martin."
            ],
            "title": "EmoEvent: A multilingual emotion corpus based on different events",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages",
            "year": 2020
        },
        {
            "authors": [
                "Soujanya Poria",
                "Devamanyu Hazarika",
                "Navonil Majumder",
                "Rada Mihalcea."
            ],
            "title": "Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research",
            "venue": "arXiv preprint arXiv:2005.00357.",
            "year": 2020
        },
        {
            "authors": [
                "Pavel Prib\u00e1n",
                "Josef Steinberger."
            ],
            "title": "Czech dataset for cross-lingual subjectivity classification",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference, LREC 2022, Marseille, France, 20-25 June 2022, pages 1381\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Tom\u00e1\u0161 Pt\u00e1\u010dek",
                "Ivan Habernal",
                "Jun Hong."
            ],
            "title": "Sarcasm detection on Czech and English Twitter",
            "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 213\u2013223, Dublin, Ireland.",
            "year": 2014
        },
        {
            "authors": [
                "Michal Ptaszynski",
                "Agata Pieciukiewicz",
                "Pawe\u0142 Dyba\u0142a."
            ],
            "title": "Results of the PolEval 2019 shared task 6: First dataset and open shared task for automatic cyberbullying detection in Polish twitter",
            "venue": "Proceedings of the PolEval 2019 Workshop, page 89.",
            "year": 2019
        },
        {
            "authors": [
                "Chengwei Qin",
                "Aston Zhang",
                "Zhuosheng Zhang",
                "Jiaao Chen",
                "Michihiro Yasunaga",
                "Diyi Yang"
            ],
            "title": "Is chatgpt a general-purpose natural language processing task solver? CoRR, abs/2302.06476",
            "year": 2023
        },
        {
            "authors": [
                "Ashwin Rajadesingan",
                "Reza Zafarani",
                "Huan Liu."
            ],
            "title": "Sarcasm detection on twitter: A behavioral modeling approach",
            "venue": "Proceedings of the Eighth ACM International Conference on Web Search and Data Mining, WSDM 2015, Shanghai,",
            "year": 2015
        },
        {
            "authors": [
                "Luis Rei",
                "Dunja Mladenic",
                "Simon Krek."
            ],
            "title": "A multilingual social media linguistic corpus",
            "venue": "Proceedings of the 4th Conference on CMC and Social Media Corpora for the Humanities, Ljubljana, Slovenia.",
            "year": 2016
        },
        {
            "authors": [
                "Ellen Riloff",
                "Ashequl Qadir",
                "Prafulla Surve",
                "Lalindra De Silva",
                "Nathan Gilbert",
                "Ruihong Huang."
            ],
            "title": "Sarcasm as contrast between a positive sentiment and negative situation",
            "venue": "Proceedings of",
            "year": 2013
        },
        {
            "authors": [
                "der M. Rush"
            ],
            "title": "Multitask prompted training",
            "year": 2022
        },
        {
            "authors": [
                "Noah A. Smith"
            ],
            "title": "The risk of racial bias",
            "year": 2019
        },
        {
            "authors": [
                "Alexander G. Sboev",
                "Aleksandr Naumov",
                "Roman B. Rybka."
            ],
            "title": "Data-driven model for emotion detection in russian texts",
            "venue": "Proceedings of the 2020 Annual International Conference on BrainInspired Cognitive Architectures for Artificial Intel-",
            "year": 2020
        },
        {
            "authors": [
                "Mou",
                "Chris Emezue",
                "Christopher Klamm",
                "Colin Leong",
                "Daniel van Strien",
                "David Ifeoluwa Adelani"
            ],
            "title": "BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100",
            "year": 2022
        },
        {
            "authors": [
                "Iyanuoluwa Shode",
                "David Ifeoluwa Adelani",
                "Anna Feldman."
            ],
            "title": "YOSM: A new Yor\u00f9b\u00e1 Sentiment Corpus for Movie Reviews",
            "venue": "AfricaNLP 2022 @ICLR.",
            "year": 2022
        },
        {
            "authors": [
                "Debaditya Shome."
            ],
            "title": "Emohind: Fine-grained multilabel emotion recognition from hindi texts with deep learning",
            "venue": "12th International Conference on Computing Communication and Networking Technologies, ICCCNT 2021, Kharagpur, India, July 6-8,",
            "year": 2021
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on",
            "year": 2013
        },
        {
            "authors": [
                "Kirubarajan",
                "Asher Mullokandov",
                "Ashish Sabharwal",
                "Austin Herrick",
                "Avia Efrat",
                "Aykut Erdem",
                "Ayla Karakas"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615",
            "year": 2022
        },
        {
            "authors": [
                "Yu Sun",
                "Shuohuan Wang",
                "Yu-Kun Li",
                "Shikun Feng",
                "Hao Tian",
                "Hua Wu",
                "Haifeng Wang"
            ],
            "title": "ERNIE 2.0: A continual pre-training framework for language understanding",
            "venue": "In The Thirty-Fourth AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Varsha Suresh",
                "Desmond C. Ong."
            ],
            "title": "Not all negatives are equal: Label-aware contrastive loss for fine-grained text classification",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Vir-",
            "year": 2021
        },
        {
            "authors": [
                "Haruya Suzuki",
                "Yuto Miyauchi",
                "Kazuki Akiyama",
                "Tomoyuki Kajiwara",
                "Takashi Ninomiya",
                "Noriko Takemura",
                "Yuta Nakashima",
                "Hajime Nagahara"
            ],
            "title": "A Japanese dataset for subjective and objective sentiment polarity classification in micro blog domain",
            "year": 2022
        },
        {
            "authors": [
                "Sali A Tagliamonte."
            ],
            "title": "Making waves: The story of variationist sociolinguistics",
            "venue": "John Wiley & Sons.",
            "year": 2015
        },
        {
            "authors": [
                "Songbo Tan",
                "Jin Zhang."
            ],
            "title": "An empirical study of sentiment analysis for chinese documents",
            "venue": "Expert Syst. Appl., 34(4):2622\u20132629.",
            "year": 2008
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Mike Thelwall",
                "Kevan Buckley",
                "Georgios Paltoglou."
            ],
            "title": "Sentiment strength detection for the social web",
            "venue": "J. Assoc. Inf. Sci. Technol., 63(1):163\u2013 173.",
            "year": 2012
        },
        {
            "authors": [
                "Hao Tian",
                "Can Gao",
                "Xinyan Xiao",
                "Hao Liu",
                "Bolei He",
                "Hua Wu",
                "Haifeng Wang",
                "Feng Wu."
            ],
            "title": "SKEP: Sentiment knowledge enhanced pre-training for sentiment analysis",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Cynthia Van Hee",
                "Els Lefever",
                "V\u00e9ronique Hoste."
            ],
            "title": "SemEval-2018 task 3: Irony detection in English tweets",
            "venue": "Proceedings of The 12th International Workshop on Semantic Evaluation, pages 39\u2013 50, New Orleans, Louisiana. Association for Com-",
            "year": 2018
        },
        {
            "authors": [
                "Erik Velldal",
                "Lilja \u00d8vrelid",
                "Eivind Alexander Bergem",
                "Cathrine Stadsnes",
                "Samia Touileb",
                "Fredrik J\u00f8rgensen."
            ],
            "title": "NoReC: The Norwegian review corpus",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation",
            "year": 2018
        },
        {
            "authors": [
                "Bertie Vidgen",
                "Leon Derczynski."
            ],
            "title": "Directions in abusive language training data, a systematic review: Garbage in, garbage out",
            "venue": "Plos one, 15(12):e0243300.",
            "year": 2020
        },
        {
            "authors": [
                "Deepanshu Vijay",
                "Aditya Bohra",
                "Vinay Singh",
                "Syed Sarfaraz Akhtar",
                "Manish Shrivastava."
            ],
            "title": "A dataset for detecting irony in hindi-english code-mixed social media text",
            "venue": "Proceedings of 4th Workshop on Sentic Computing, Sentiment Analysis,",
            "year": 2018
        },
        {
            "authors": [
                "Marilyn Walker",
                "Jean Fox Tree",
                "Pranav Anand",
                "Rob Abbott",
                "Joseph King."
            ],
            "title": "A corpus for research on deliberation and debate",
            "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC\u201912), pages",
            "year": 2012
        },
        {
            "authors": [
                "Harald G Wallbott",
                "Klaus R Scherer."
            ],
            "title": "How universal and specific is emotional experience? evidence from 27 countries on five continents",
            "venue": "Social science information, 25(4):763\u2013795.",
            "year": 1986
        },
        {
            "authors": [
                "Shuo Wan",
                "Bohan Li",
                "Anman Zhang",
                "Wenhuan Wang",
                "Donghai Guan."
            ],
            "title": "S2ap: Sequential sentiweibo analysis platform",
            "venue": "Database Systems for Advanced Applications - 25th International Conference, DASFAA 2020, Jeju, South Korea, September",
            "year": 2020
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "7th International Conference on Learning Representa-",
            "year": 2019
        },
        {
            "authors": [
                "Zengzhi Wang",
                "Qiming Xie",
                "Zixiang Ding",
                "Yi Feng",
                "Rui Xia."
            ],
            "title": "Is chatgpt a good sentiment analyzer? A preliminary study",
            "venue": "CoRR, abs/2304.04339.",
            "year": 2023
        },
        {
            "authors": [
                "Zeerak Waseem",
                "Dirk Hovy."
            ],
            "title": "Hateful symbols or hateful people? predictive features for hate speech detection on Twitter",
            "venue": "Proceedings of the NAACL Student Research Workshop, pages 88\u201393, San Diego, California. Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Bryan Wilie",
                "Karissa Vincentio",
                "Genta Indra Winata",
                "Samuel Cahyawijaya",
                "Xiaohong Li",
                "Zhi Yuan Lim",
                "Sidik Soleman",
                "Rahmad Mahendra",
                "Pascale Fung",
                "Syafri Bahar",
                "Ayu Purwarianti"
            ],
            "title": "Indonlu: Benchmark and resources for evaluating indonesian",
            "year": 2020
        },
        {
            "authors": [
                "Ruder."
            ],
            "title": "Nusax: Multilingual parallel sentiment dataset for 10 indonesian local languages",
            "venue": "CoRR, abs/2205.15960.",
            "year": 2022
        },
        {
            "authors": [
                "Minghao Wu",
                "Abdul Waheed",
                "Chiyu Zhang",
                "Muhammad Abdul-Mageed",
                "Alham Fikri Aji."
            ],
            "title": "Lamini-lm: A diverse herd of distilled models from large-scale instructions",
            "venue": "CoRR, abs/2304.14402.",
            "year": 2023
        },
        {
            "authors": [
                "Rong Xiang",
                "Xuefeng Gao",
                "Yunfei Long",
                "Anran Li",
                "Emmanuele Chersoni",
                "Qin Lu",
                "Chu-Ren Huang."
            ],
            "title": "Ciron: a new benchmark dataset for chinese irony detection",
            "venue": "Proceedings of The 12th Language Resources and Evaluation Conference, LREC",
            "year": 2020
        },
        {
            "authors": [
                "Zuoyu Tian",
                "Yiwen Zhang",
                "He Zhou",
                "Shaoweihua Liu",
                "Zhe Zhao",
                "Qipeng Zhao",
                "Cong Yue",
                "Xinrui Zhang",
                "Zhengliang Yang",
                "Kyle Richardson",
                "Zhenzhong Lan."
            ],
            "title": "CLUE: A chinese language understanding evaluation benchmark",
            "venue": "Proceed-",
            "year": 2020
        },
        {
            "authors": [
                "Lu Xu",
                "Lidong Bing",
                "Wei Lu",
                "Fei Huang."
            ],
            "title": "Aspect sentiment classification with aspect-specific opinion spans",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3561\u20133567, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mt5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North",
            "year": 2021
        },
        {
            "authors": [
                "Seid Muhie Yimam",
                "Hizkiel Mitiku Alemayehu",
                "Abinew Ayele",
                "Chris Biemann."
            ],
            "title": "Exploring Amharic sentiment analysis from social media texts: Building annotation tools and classification models",
            "venue": "Proceedings of the 28th International Conference",
            "year": 2020
        },
        {
            "authors": [
                "Marcos Zampieri",
                "Shervin Malmasi",
                "Preslav Nakov",
                "Sara Rosenthal",
                "Noura Farra",
                "Ritesh Kumar."
            ],
            "title": "Predicting the type and target of offensive posts in social media",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of",
            "year": 2019
        },
        {
            "authors": [
                "Marcos Zampieri",
                "Preslav Nakov",
                "Sara Rosenthal",
                "Pepa Atanasova",
                "Georgi Karadzhov",
                "Hamdy Mubarak",
                "Leon Derczynski",
                "Zeses Pitenis",
                "\u00c7agri \u00c7\u00f6ltekin"
            ],
            "title": "Semeval-2020 task 12: Multilingual offensive language identification in social media (offenseval",
            "year": 2020
        },
        {
            "authors": [
                "Chiyu Zhang",
                "Muhammad Abdul-Mageed."
            ],
            "title": "Improving social meaning detection with pragmatic masking and surrogate fine-tuning",
            "venue": "Proceedings of the 12th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analy-",
            "year": 2022
        },
        {
            "authors": [
                "Chiyu Zhang",
                "Muhammad Abdul-Mageed",
                "Ganesh Jawahar."
            ],
            "title": "Contrastive learning of sociopragmatic meaning in social media",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 2405\u20132439, Toronto, Canada. Associa-",
            "year": 2023
        },
        {
            "authors": [
                "Chiyu Zhang",
                "Muhammad Abdul-Mageed",
                "El Moatez Billah Nagoudi."
            ],
            "title": "Decay no more: A persistent twitter dataset for learning social meaning",
            "venue": "Workshop Proceedings of the 16th International AAAI Conference on Web and Social",
            "year": 2022
        },
        {
            "authors": [
                "Wenxuan Zhang",
                "Yue Deng",
                "Bing Liu",
                "Sinno Jialin Pan",
                "Lidong Bing."
            ],
            "title": "Sentiment analysis in the era of large language models: A reality check",
            "venue": "CoRR, abs/2305.15005.",
            "year": 2023
        },
        {
            "authors": [
                "Qihuang Zhong",
                "Liang Ding",
                "Juhua Liu",
                "Bo Du",
                "Dacheng Tao."
            ],
            "title": "Can chatgpt understand too? A comparative study on chatgpt and fine-tuned BERT",
            "venue": "CoRR, abs/2302.10198.",
            "year": 2023
        },
        {
            "authors": [
                "Caleb Ziems",
                "William Held",
                "Omar Shaikh",
                "Jiaao Chen",
                "Zhehao Zhang",
                "Diyi Yang"
            ],
            "title": "Can large language models transform computational social science? CoRR, abs/2305.03514",
            "year": 2023
        },
        {
            "authors": [
                "ules. Li"
            ],
            "title": "2023) translate the English 67K",
            "year": 2023
        },
        {
            "authors": [
                "hoff",
                "Hammarstr\u00f6m"
            ],
            "title": "The column # dataset shows the number of datasets covered by SPARROW per language",
            "year": 2011
        },
        {
            "authors": [
                "Offe-T-kanCha Chakravarthi"
            ],
            "title": "2022) kan YouTube comment",
            "year": 2019
        },
        {
            "authors": [
                "M-F1 Offe-T-malCha Chakravarthi"
            ],
            "title": "2022) mal YouTube comment 2019 4 {Group, Individual, Not, Untargeted} 14,723/1,836/1,844 M-F1 Offe-T-tamCha Chakravarthi et al. (2022) tam YouTube comment",
            "year": 2019
        },
        {
            "authors": [
                "Basile"
            ],
            "title": "2016) Iron-engHee F1-iro",
            "venue": "SoTA SoTA study Iron-itaBas",
            "year": 2014
        },
        {
            "authors": [
                "Muhammad"
            ],
            "title": "2023b) Sent-arqMuh W-F1",
            "venue": "Sent-aryMuh",
            "year": 2023
        },
        {
            "authors": [
                "Muhammad"
            ],
            "title": "2023b) Sent-swhMuh W-F1",
            "venue": "Sent-kinMuh",
            "year": 2022
        },
        {
            "authors": [
                "Barbieri"
            ],
            "title": "2016) Subj-cesPri Acc",
            "venue": "Subj-spaBar",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Multilingual LLMs have recently transformed NLP, due to their powerful capabilities on a\n\u22c6 Equal contribution\nwide range of tasks (Xue et al., 2021; Scao et al., 2022). Methods such instruction tuning and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) have further enhanced the zero-shot generalizability of these models. Notably, ChatGPT exhibits impressive capabilities in this regard. Human language, however, is intrinsically ambiguous and far from solved. In fact, some forms of meaning are deeply embedded in social interactions. We collectively refer to this type of meaning as sociopragmatic meaning (SM). To appreciate SM, consider how the meaning of an utterance in social interaction (e.g., on social media) can be highly subtle and how it incorporates both the social variation related to language users (from a sociolinguistics perspective) (Tagliamonte, 2015) and their communicative intentions (from a pragmatics perspective) (Boxer and Cort\u00e9s-Conde, 2021). Although SM is quite established within linguistics, NLP systems still struggle with this type of meaning that is intertwined in social and interactive contexts (Zhang and Abdul-Mageed, 2022). The extent to which instruction tuned models such as ChatGPT can capture SM across languages re-\nmains largely unclear as these models are yet to be evaluated on appropriate datasets under standardized conditions easy to replicate.\nTo facilitate evaluation of LLMs and enhance fairness of model comparisons and reproducibility, early work introduces evaluation benchmarks. Most existing benchmarks, however, focus on the monolingual setting. These include GLUE (Wang et al., 2019), SentEval (Conneau and Kiela, 2018), and TweetEval (Barbieri et al., 2020) for English, ARLUE (Abdul-Mageed et al., 2021) for Arabic, CLUE (Xu et al., 2020a) for Chinese, and IndoNLU (Wilie et al., 2020) for Indonesian. Although XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) introduce multilingual benchmarks, they only include a few SM tasks for a limited number of languages. They are also limited to standard language use (e.g., Wikipedia). Barbieri et al. (2022) propose a multilingual sentiment analysis benchmark (UMSAB), but it solely contains tweet sentiment analysis datasets in only eight languages. As such, absence of a unified, diverse, and comprehensive benchmark and a fragmented evaluation landscape hamper NLP work for cross-lingual SM.\nAnother challenge for SM research is the issue of data inaccessibility (Assenmacher et al., 2022). Although many studies release the IDs of posts (e.g., tweets), substantial amounts of these social posts become inaccessible over time due to deletion, etc. (Zhang et al., 2022). In our benchmark, we attempt to re-collect text contents of 25 datasets by using their IDs but can only retrieve 58% samples on average (see Table 8 in Appendix). This data decay also hinders fair comparisons in NLP for SM research. This issue has already become worse as corporations such as Twitter tighten access to their API, making it even harder to collect historical data. To address this bottleneck, we introduce a massively multilingual SM evaluation benchmark, dubbed SPARROW, that comprises 169 datasets covering 64 languages from 12 language families, 16 types of scripts, across diverse online platforms (e.g., Twitter, YouTube, and Weibo). We then perform an extensive evaluation of ChatGPT, comparing it to 13 other models ranging in size between 110M-7B parameters. Our evaluations allow us to answer multiple questions related to how it is that LLMs fare across languages on SM. To facilitate future comparisons, we also design a modular, interac-"
        },
        {
            "heading": "Studies Lang. Tasks SM Tasks Dataset Models LeaderBrd",
            "text": "Zhong et al. (2023) en 5 1 8 5 \u2717 Qin et al. (2023) en 7 1 20 29 \u2717 Ahuja et al. (2023) 70 10 3 16 11 \u2717 Laskar et al. (2023) 12 12 2 140 27 \u2717 Bang et al. (2023) 8 8 1 23 3 \u2717 Lai et al. (2023) 37 7 0 8 7 \u2717 Das et al. (2023) 11 2 2 2 1 \u2717 Wang et al. (2023) en 5 5 18 3 \u2717 Zhang et al. (2023b) en 13 13 26 5 \u2713 Ziems et al. (2023) en 24 18 24 13 \u2717 Ours 64 13 13 169 14 \u2713"
        },
        {
            "heading": "2 Related Work",
            "text": "SM is still not adequately represented in existing benchmarks, hindering comprehensive evaluations on more languages. As we summarize in Table 1, benchmarks used for listed evaluations only include a few SM tasks focusing on sentiment analysis. Wang et al. (2023); Zhang et al. (2023b) investigate LLMs on a number of SM tasks (e.g., offensive language detection), but only on English. Ziems et al. (2023) investigate ChatGPT performance on a range of computational social science tasks covering subjects such as sociology, psychology, and linguistics, but they again focus only on English. Das et al. (2023) extend evaluation of ChatGPT on hate speech detection to 11 languages. Compared to these works, our objective is to investigate more diverse SM tasks on a massively multilingual setting.\nSociopragmatic Meaning Benchmarks. Many previous works introduce unified benchmarks such as GLUE (Wang et al., 2019), SentEval (Conneau and Kiela, 2018), XTREME (Hu et al., 2020), and XGLUE (Liang et al., 2020). These benchmarks include a wide range of NLP tasks, but comprise a sole SM task (i.e., sentiment analysis). Some recent studies started to construct benchmarks focusing on SM: Barbieri et al. (2020) introduce TweetEval benchmark that contains seven English datasets of six SM tasks; Zhang et al. (2023b) develop SentiEval that involves 26 English datasets of 13 sentimentrelated tasks. Beyond English, NusaX (Winata et al., 2022), NaijaSenti (Muhammad et al., 2022), and AfriSenti (Muhammad et al., 2023a) propose benchmarks for sentiment analysis with eight Indonesian languages, four African languages, and 14 African languages, respectively. UMSAB introduced by Barbieri et al. (2022) contains 11 sentiment analysis datasets in 11 languages. For detecting antisocial online comments, Risch et al. (2021) introduces a toxic comment collection that contains 43 datasets of six antisocial detection tasks in 14 languages. Compared to these works, our SPARROW benchmark includes significantly more SM tasks and languages, from more diverse sources (refer to Figure 1 for a comparison)."
        },
        {
            "heading": "3 SPARROW Benchmark",
            "text": "In this section, we describe clusters of tasks in our benchmark as well as our preprocessing and unification. SPARROW consists of 13 types of tasks in six main categories. It contains 169 datasets\nTasks Dataset Lang. LF Scr\nA nt\nis oc\nia l Aggressive 1 1 1 1 Dangerous 1 1 1 1 Hate 16 11 6 5 Offense 7 6 3 3 H/O-Group 3 3 2 3 H/O-Target 8 8 4 7 Antisocial 36 20 7 10\nEmotion 26 17 7 5 Humor 4 4 1 2\nIr n&\nSa rc Irony 9 7 3 3\nSarcasm 10 4 3 3 Irony-Type 1 1 1 1 Irony&Sarcasm 20 8 3 3 Sentiment 77 58 10 15 Subjectivity 6 5 2 2 SPARROW 169 64 12 16"
        },
        {
            "heading": "3.1 Task Clusters",
            "text": "Emotion Recognition. Emotion affects our decision-making as well as mental and physical health (Abdul-Mageed and Ungar, 2017). SPARROW includes 26 emotion datasets in 17 languages (e.g., Kajava (2018); Bianchi et al. (2021)).\nHumor Detection. Humor is a type of figurative language which induces amusing effects, such as laughter or well-being sensations. We include four humor detection datasets in four languages (e.g., Blinov et al. (2019); Meaney et al. (2021)).\nIrony & Sarcasm Detection. Irony and sarcasm also involve figurative language. An ironic/sarcastic expression intentionally uses diametric language to signify implied meaning. We include (1) nine irony detection datasets in seven languages (e.g., Xiang et al. (2020)), (2) ten sarcasm detection datasets in four languages (e.g., Walker et al. (2012)), and (3) an irony type identification dataset (Van Hee et al., 2018).\nSubjectivity and Sentiment Analysis. Subjectivity analysis aims to understand the opinions, feelings, judgments, and speculations expressed via language (Abdul-Mageed et al., 2014). Our benchmark includes six subjectivity analysis datasets in five different languages (e.g., Pang and Lee (2004); Prib\u00e1n and Steinberger (2022)). Subjectivity incorporates sentiment. Sentiment analysis (Poria et al., 2020) is one of the most popular tasks in SM understanding where the objective is to identify the underlying sentiment of a given text. Our benchmark contains 77 sentiment analysis datasets in 58 languages (e.g., Pang and Lee (2005); Marreddy et al. (2022))."
        },
        {
            "heading": "3.2 Preprocessing, Splits, and Metrics",
            "text": "We apply light normalization on all the samples by converting user mentions and hyperlinks to \u2018USER\u2019 and \u2018URL\u2019, respectively. We standardize label names for consistency across datasets without reassigning nor aggregating the original labels of the datasets. For instance, in certain sentiment analysis datasets, we map \u20180\u2019 and \u20181\u2019 to \u2018Negative\u2019 and \u2018Positive\u2019 respectively. Regarding data splits, if the dataset already has Train, Dev, and Test sets, we maintain the same splits. If the original dataset does not include a Dev set, we randomly select 10% of training data to be a Dev set. In cases without pre-defined splits, we use an 80% Train, 10% Dev, and 10% Test random split. For computing constraints, we also prepare a smaller Test set for\neach dataset by randomly sampling 500 samples from Test (if its size exceeds 500). We refer to this smaller test set as Test-S.\nWe evaluate on each dataset using its original metric as Tables 9, 10, 11, 12, 13, and 14 in Appendix summarize.1 We report the performance on individual datasets, aggregate datasets into 13 tasks, and report an average score over each task. Moreover, we introduce a metric for each main category, calculated as the average of dataset-specific metrics within that category. Inspired by previous evaluation benchmarks like GLUE (Wang et al., 2019), we define a global metric called SPARROW score, which represents the unweighted average of all dataset-specific metrics. The SPARROW score provides an overall indication of performance on SM tasks."
        },
        {
            "heading": "4 Evaluation Methods",
            "text": ""
        },
        {
            "heading": "4.1 Finetuning on Encoder-Only Models",
            "text": "We evaluate the following Transformer-encoderbased multilingual models on SPARROW: (1) Multilingual-BERT (mBERT) (Devlin et al., 2019) (Base, 110M parameters), (2) XLMRoBERTaBase (XLM-R) (Conneau et al., 2020) (270M parameters), (3) Bernice (DeLucia et al., 2022), a 270M-parameter model trained with 2.5B tweets in 66 languages, and (4) InfoDCL (Zhang et al., 2023a), a SoTA for SM understanding, which further trains XLM-R with 100M tweets in 66 languages with contrastive learning. More details about all models are in Appendix B."
        },
        {
            "heading": "4.2 Zero- and Few-Shot on LLMs",
            "text": "We investigate zero-shot performance on a wide range of generative models, including pretrained generative models: (1) BLOOM (Scao et al., 2022), (2) mT5 (Xue et al., 2021), (3) LLaMA (Touvron et al., 2023), instruction tuned models: (4) BLOOMZ (Scao et al., 2022), a BLOOM-initialized model tuned with multilingual xP3 corpus, (5) BLOOMZ-P3 (Muennighoff et al., 2022), a BLOOM-initialized model tuned with English-only P3 corpus, (6) BLOOMBactrian (Li et al., 2023), a BLOOM-initialized model tuned with 3.4M instruction-following samples in 52 languages, (7) mT0 (Muennighoff et al., 2022), an mT5 model tuned with xP3 corpus, (8) Alpaca (Taori et al., 2023), a\n1We select the macro-average F1 score as the main metric if the original paper utilizes more than one metric.\nChatGPT ( green ). We use an English prompt (Figures a, c) and machine translated the prompt in the corresponding language (Figures b, d), repectively. The prompts construct each task as question-and-answer tasks. The actual input sample is in blue, and the label options are in red.\nLLaMA-initialized model tuned with 52K English instruction-following samples, (9) Vicuna (Chiang et al., 2023), a LLaMA-initialized model on 70K conversational data, and (10) ChatGPT, for which we use the gpt-3.5-turbo-0301 version via OpenAI API.2 We use 7B-size version of BLOOM- and LLaMA-based models and 4B-size version of mT5-based models. We also evaluate six open-source LLMs (i.e, BLOOM, BLOOMZP3, mT5, mT0, LLaMA, and Vicuna) via few-shot in-context learning."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Implementation",
            "text": "Finetuning. To keep computation reasonable, we randomly select 45 datasets for hyperparameter tuning and only tune the learning rate of each model.3 For all experiments, we finetune a pretrained model with an arbitrary batch size of 32 and sequence length of 128 tokens. Each model is finetuned on the full Train set of each dataset for 20 epochs (with patience = 5) based on performance on Dev set. We run each experiment three times with different random seeds and identify the best model on Dev in each run. We report the average performance on Test-S over three runs.4\nZero-shot Evaluation. We perform a zero-shot evaluation on SPARROW for BLOOM-, mT5-, and LLaMA-based models using language model evaluation harness (lm-evaluation-harness Gao et al. (2021)).5 While we do not tailor prompts\n2In rest of this paper, ChatGPT refers to gpt-3.5-turbo-0301.\n3For more information, refer to Section C.1 in Appendix. 4We also report the performance of Dev and standard de-\nviations in Appendix Table 16. 5https://github.com/EleutherAI/ lm-evaluation-harness\nspecifically for each model, customized prompts are employed for each set of tasks. These prompts follow the structure of question-and-answer tasks, where we present sample content alongside a taskspecific question, as shown in Figure 2. The prompts are summarized in Appendix Table 15. We then instruct the model to generate an answer based on the provided option labels. Each option label represents a potential answer, and we calculate the log-likelihood for each candidate. The prediction with the highest log-likelihood is chosen as the model\u2019s final prediction. For the evaluation of ChatGPT, we draw inspiration from previous practices for prompt design (Ziems et al., 2023), and incorporate additional instructions to guide its generation of the desired labels. As shown in Figure 2, we provide an instruction that compels ChatGPT to select a single label for the given input text without providing any additional explanation. We set the temperature to 0 to generate deterministic and reproducible results from ChatGPT. For a few instances, we observe that ChatGPT is unable to provide a direct answer. In these cases, we randomly assign a false label to each sample. In addition, we also use machine translation to translate English prompts and label names to the corresponding language of each dataset.6\nFew-shot Evaluation. We utilize lm-evaluationharness tool with the same prompts employed in zero-shot evaluation to explore the few-shot incontext learning abilities of open-source LLMs. Before the actual test examples, we prepend m examples from the Train set. Each example consists\n6We use Google Translate for most languages. NLLB model is used to translate the languages of ace, ban, bjn, bug, and min because Google Translate does not cover these. The prompts of pcm datasets are translated by a native speaker.\nof an input text, task-specific instruction, and the corresponding answer. We set m to either 3 or 5."
        },
        {
            "heading": "5.2 Results",
            "text": "We present the aggregated performance of TestS on each task and main category, respectively, in Table 3. We also present test results on all datasets and compare to dataset-specific SoTA performance in Tables 17, 18, 19, 20, 21, and 22 in Appendix.\n(1) How is the overall performance over different models? All the fully finetuned models surpass the zero-shot generative models as well as ChatGPT, as shown in Table 3. The most superior among the finetuned models is InfoDCL, which achieves a SPARROW score of 71.60 and outperforms ChatGPT with 11.56 points SPARROW score. On the other hand, the open-source models (i.e., BLOOM, mT5 and LLaMA) still significantly lag behind on multilingual SM understanding with performance close to a random baseline. Meanwhile, the instruction tuned multilingual LLMs (BLOOMZ and mT0) only slightly perform better than the random baseline.\n(2) Can instruction tuning enhance LLMs\u2019 ability on SM understanding? Yes, but it depends on the instruction training data. Following instruction tuning on the English-only P3 dataset, BLOOMZ-P3 demonstrates an improvement of 7.76 SPARROW score compared to BLOOM. Also, BLOOMZ improves 5.85 points over BLOOM (but falls short of BLOOMZ-P3). MT0 also outperforms mT5. However, there remains a substantial gap between all instruction tuned models and finetuned models. BLOOMBactrian performs worse than BLOOMZ and BLOOMZ-P3, which are instruction tuned with NLP tasks. This indicates that the general purpose instruction-response dataset is not very useful for SM understanding.\nTo further probe how instruction tuning improves BLOOM-based models, we compare BLOOM with BLOOMZ-P3 and BLOOMZ in terms of individual tasks, finding sentiment analysis to exhibit the most significant improvement. BLOOMZ-P3 and BLOOMZ achieve a sentiment score improvement of 16.37 and 12.36, respectively, based on average calculation across 77 sentiment analysis datasets. However, BLOOMBactrian obtains an improvement of only 1.79 sentiment score, perhaps implying that the Bactrian\ninstruction-response data is not all that useful for some SM tasks. After tuning mT5 on xP3 dataset, mT0 also experiences a 13.88 improvement in the sentiment score. These may be stemming from inclusion of five English sentiment analysis datasets in both P3 and xP3 during the training phase. For example, we observe that BLOOM, BLOOMZ, BLOOMZ-P3, mT5, and mT0 obtain an accuracy of 56.4, 92.2, 93.00, 49.00, and 76.8 on SentengSoc (not included in either xP3 or P3), respectively and that BLOOM-Bactrian still performs poorly (accuracy= 53.60) after instruction tuning. Again, these results indicate that it is still important to include task-related datasets in the instruction tuning stage.\n(3) How do LLMs perform across different SM tasks? They are inferior at humor and antisocial language detection while being relatively better at sentiment and emotion recognition tasks. BLOOMZ-P3, BLOOMZ, and mT0 exhibit considerable enhancements (> 5 points) on sentiment and emotion when compared to their respective initialization models. On the other hand, we find that instruction tuned models perform significantly worse on aggressive language detection and humor detection tasks. BLOOMZ-P3, BLOOMZ, BLOOM-Bactrian, and mT0 all incur a loss of more than 5 points on these two tasks. Upon investigating the predictions of instruction tuned models, we find that they tend to assign negative labels (i.e., non-aggressive or non-humor) which results in many false negative predictions. For a concrete example, we show that BLOOMZ-P3 predict most samples as non-humor in Figure 3a shows.\nChatGPT outperforms the open-source LLMs on all tasks except dangerous language detection. Comparing ChatGPT to InfoDCL, we find gaps favoring InfoDCL in subjectivity analysis (a difference of 9.47), emotion recognition (a difference of 10.68), and irony & sarcasm detection (a difference of 10.70). ChatGPT also largely lags behind InfoDCL in humor detection (a difference of 15.40) and antisocial language detection (a difference of 14.06). As the example shows in Figure 3b, ChatGPT makes more false positive errors (classifies non-hateful as hateful).\n(4) How do LLMs perform across different languages? We now examine the impact of instruction finetuning on the model\u2019s language-wise performance. We categorize the performance of each"
        },
        {
            "heading": "BMZ",
            "text": "dataset based on language and calculate the average language scores across all datasets within a language. Since each language contains different tasks and datasets, a direct comparison across languages is not feasible. Therefore, we compare the relative performance between different models for each language. By comparing the instruction tuned models to their initial models, we observe that most languages experience improvement. However, we also observe a significant decline in performance for the Amharic (amh) dataset among these models. Specifically, BLOOMZ-P3, BLOOMZ, and mT0 experience a deterioration of 36.07, 24.99, and 26.12 points, respectively, compared to their respective initial models. We hypothesize that this deterioration can be attributed to catastrophic forgetting after instruction tuning, where Amharic was not included in the training set and does not share the writing scripts with the other included languages.\nand the second best is in green highlight . The red font denotes a performance lower than the random baseline.\nSimilarly, the Filipino (fil) tasks exhibit an average decline of approximately 11 points on both BLOOMZ-P3 and BLOOMZ, as Filipino is not included in the xP3 dataset. Although Hindi is included in the xP3 dataset, the three instruction tuned models still show a decline in performance. Upon examining the individual performance of Hindi datasets, we find that the major deteriorations occur in the aggressive language detection and humor detection tasks, while the emotion recognition and sentiment analysis tasks show improvement. The instruction-response data for training Alpaca and Vicuna consist solely of English language. Therefore, we compare the performance of Alpaca and Vicuna to that of LLaMA using both English and non-English datasets. We observe that Alpaca and Vicuna outperform LLaMA when evaluated on English datasets, achieving\nTask #Sample CG (sample MT) GPT-4\nHate-engWas 96 \u2014 28.82 Hate-engDav 168 \u2014 76.30 Hate-araAla 153 38.82 35.76 Hate-itaBos 104 29.75 38.44 Hate-filCab 145 15.76 23.96 Hate-araMul 127 29.49 36.33 Hate-engBas 178 \u2014 19.23 Hate-spaBas 174 21.06 17.35 Hate-porFor 142 35.37 37.34 Hate-polPta 54 35.62 49.57 Hate-korMoo 250 14.20 15.85 Hate-araMub 89 20.54 16.74 Hate-zhoDen 125 29.24 39.24 Hate-korJeo 169 27.75 44.84 Hate-telMar 2 100.00 100.00 Sexi-fraChi 113 34.10 43.69 Humo-hinAgg 220 22.73 30.91 Humo-rusBli 136 26.89 47.31 Humo-spaChi 152 30.68 36.80 Humo-engMea 48 \u2014 50.34\nTasks Zero-shot Three-shot Five-shot\nBM BMZP3 mT5 mT0 LLa. Vic. BM BMZ P3 mT5 mT0 LLa. Vic. BM BMZ P3 mT5 mT0 LLa. Vic.\nA nt\nis oc\nia l Aggressive 51.06 18.72 53.67 15.82 18.31 25.07 46.43 43.93 40.73 41.88 43.70 44.53 47.92 47.63 33.80 37.52 50.66 55.27 Dangerours 46.87 46.87 49.31 46.87 46.87 46.87 46.87 46.87 45.68 46.87 46.87 46.87 46.87 46.87 48.91 46.87 46.87 46.87 Hate 39.83 38.52 23.29 37.33 37.80 41.59 38.83 38.30 39.43 37.82 43.51 49.17 37.95 37.14 39.53 37.70 41.87 48.37 Offense 41.06 38.59 24.99 39.90 39.85 48.70 43.94 40.25 21.99 41.53 46.36 54.49 42.42 40.59 34.10 41.67 43.83 51.72 H/O-Group 13.63 21.23 7.02 16.25 12.35 9.26 11.43 13.04 7.92 15.98 11.81 14.19 9.68 11.76 7.23 14.50 12.58 16.27 H/O-Target 18.73 16.89 6.69 20.58 19.32 17.01 17.09 18.41 10.48 16.55 20.56 24.84 17.44 17.45 9.32 16.56 20.09 23.60 AS 33.70 31.97 20.14 32.02 31.68 34.50 33.14 32.55 27.19 32.36 36.42 41.69 32.43 31.88 29.17 32.09 35.35 40.99 Emotion 9.71 15.07 7.75 27.87 15.14 18.12 17.08 12.17 10.66 23.12 32.12 40.28 18.48 12.35 10.07 25.57 34.20 41.79 Humor 41.78 33.04 43.60 33.12 39.78 46.19 33.67 33.12 44.70 38.19 55.20 57.15 34.06 33.12 40.20 37.08 53.86 58.75\nI& S Irony 36.63 44.46 36.52 34.69 40.78 47.48 42.21 41.58 42.61 35.18 36.76 39.78 44.34 44.14 39.67 34.82 38.40 41.61 Sarcasm 43.00 41.68 36.09 41.62 41.17 47.67 46.14 42.91 46.72 48.42 49.75 52.55 45.43 43.05 45.75 39.88 49.03 52.51 Irony-Type 18.83 18.83 18.83 18.83 18.83 18.83 18.83 18.83 18.83 18.83 18.83 18.83 18.83 18.83 18.83 18.83 18.83 18.83 I&S 38.92 41.79 35.42 37.36 39.87 46.14 43.01 41.11 43.48 40.98 42.36 45.12 43.61 42.33 41.67 36.55 42.74 45.92 Sentiment 26.67 43.03 20.77 34.65 27.55 25.02 34.81 35.79 24.76 31.37 37.73 34.53 33.15 37.71 23.17 29.25 40.88 39.37 Subjectivity 44.12 30.73 37.35 41.64 42.30 38.73 36.50 30.66 37.11 34.36 44.20 54.77 33.70 30.72 39.36 31.42 46.53 56.15 SPARROW 27.94 35.70 21.45 33.63 28.75 29.36 32.76 31.91 26.12 31.71 37.82 39.44 32.03 32.84 25.47 30.44 39.48 41.97"
        },
        {
            "heading": "7 Conclusion",
            "text": ""
        },
        {
            "heading": "6 Public Leaderboard",
            "text": ""
        },
        {
            "heading": "8 Limitations",
            "text": "Benchmark Construction. Our SPARROW benchmark only includes text classification tasks related to SM. Despite our best efforts, we acknowledge that our benchmark has not covered existing SM datasets exhaustively. We will continue expanding this benchmark and welcome future datasets or metric contributions to it. We also plan to extend SPARROW to more types of tasks related to SM, such as span-based sentiment analysis (Xu et al., 2020b), affective language generation (Goswamy et al., 2020), and conversational sentiment analysis (Ojamaa et al., 2015). We only include text-based SM tasks. Another improvement direction is to extend this benchmark to more tasks that involve more modalities, such affective image captioning (Mohamed et al., 2022) and multi-modal emotion recognition (Firdaus et al., 2020).\nModel Selection. Due to computation constraints, we cannot evaluate on model sizes > 7B. However, we hope SPARROW will be used in the future to evaluate larger-sized models. Again, due to budget constraints, we only conduct a relatively small case study on GPT-4 and do not evaluate more diverse commercial instruction tuned models that are more expensive (e.g., text-davinci-003 by OpenAI).\nExperiments. While we customize prompts employed for each task, we do not tailor prompts specifically for each model. We acknowledge that the performance of models may be influenced by different prompt variants. In future work, we will test diverse prompt variations for more robust results. We only experiment with machine translated prompts in our analyses and acknowledge that the performance drop may stem from the poor quality of machine translation. We will investigate the utility of human translated prompts in a future study. In this paper, we only evaluate LLMs on zero-shot learning. The adoption of few-shot incontext learning may enhance performance, which we also leave to future work."
        },
        {
            "heading": "Ethics Statement and Broad Impacts",
            "text": "Data Collection and Releasing. All the 169 datasets are produced by previous research. Since there are large numbers of datasets and languages in SPARROW, it is hard to manually verify the\nquality of all the datasets. As a quality assurance measure, we only include in SPARROW datasets that are introduced in peer-reviewed published research. To facilitate access to information about each dataset, we link to each published paper describing each of these datasets inTables 9, 10, 11, 12, 13, and 14.\nFollowing privacy protection policies, we anonymize all SPARROW data as described in Section 3.2. With reference to accessibility of the original individual dataset, SPARROW data can be categorized into three releasing strategies: (1) In the case of datasets requiring approval by the original authors, we require future researchers to obtain approval first and will share our splits once approval has been obtained. We indicate these nine datasets in our data description tables. (2) For the 25 datasets (see Table 8 in Appendix) that are shared via tweet IDs, we share our obtained data for research use. By doing so, we expect to mitigate the issue of data decay and allow fair comparisons. (3) We will share the other 135 publicly accessible datasets upon request. We will also require a justification for responsible use of the datasets. Each dataset will be shared in our Train, Dev, and Test splits along with a dataset card to indicate the original publication of the dataset.\nIntended Use. The intended use of SPARROW benchmark is to construct a scoring board to facilitate model comparisons as well as enhance fairness and reproducibility across different languages and tasks. We also aim to mitigate data decay issues in social media research. SPARROW could help researchers investigate model\u2019s capacity on SM tasks across languages. SPARROW may also be used to investigate model transferability across a wide range of tasks and diverse languages in different settings (such as zero- or few-shot settings and prompting).\nPotential Misuse and Bias. We notice that some annotations in the datasets of SPARROW (e.g., for hate speech task (Waseem and Hovy, 2016)) can carry annotation and temporal biases. We recommend that any dataset in SPARROW not be used for research or in applications without careful consideration of internal biases of the datasets and potential biases of the resulting systems. We also suggest that users of SPARROW not only focus on the overall SPARROW score but also a model performance on each task and\ndataset. The SPARROW score is an unweighted average score over all the dataset-specific metrics, which may lose the fine-grained information and be dominated by the largest task cluster (i.e., sentiment analysis) or languages (e.g., languages from Indo-European language family)."
        },
        {
            "heading": "Acknowledgements",
            "text": "We acknowledge support from Canada Research Chairs (CRC), the Natural Sciences and Engineering Research Council of Canada (NSERC; RGPIN-2018-04267), the Social Sciences and Humanities Research Council of Canada (SSHRC; 435-2018-0576; 895-2020- 1004; 895-2021-1008), Canadian Foundation for Innovation (CFI; 37771), Digital Research Alliance of Canada,9 and UBC ARC-Sockeye.10"
        },
        {
            "heading": "Appendices",
            "text": ""
        },
        {
            "heading": "A Benchmark",
            "text": "Table 7 summarizes language distribution of datasets in SPARROW and taxonomy of these language according to Ethnologue (Gordon Jr, 2005) and Glottolog (Nordhoff and Hammarstr\u00f6m, 2011). Tables 9, 10, 11, 12, 13, and 14 describe the datasets in tasks of antisocial language detection, emotion recognition, humor detection, irony and sarcasm detection, sentiment analysis, and subjectivity analysis, respectively.\nWe empirically characterize the issue of data inaccessibility by re-collecting tweets content via tweet IDs. Table 8 shows the data decay issue of 25 datasets."
        },
        {
            "heading": "B Models",
            "text": ""
        },
        {
            "heading": "B.1 Finetuning on Encoder-only LLMs",
            "text": "We evaluate the following Transformer-encoderbased multilingual PLMs on SPARROW. We finetune each PLMs on the full training set and update all the parameters of the model during the training.\n(1) Multilingual-BERT (mBERT) (Devlin et al., 2019) is trained on a Wikipedia corpus including 104 languages with masked language modelling (MLM) and next sentence prediction objectives. It contains 110M parameters. mBERT tokenizes text by using WordPiece with a vocabulary size of 172K.\n(2) XLM-RoBERTaBase (XLM-R) (Conneau et al., 2020) is trained on CommonCrawl data involving 100 languages with MLM objective. It uses a SentencePiece tokenizer with a vocabulary size of 250K and contains 270M parameters.\n(3) Bernice (DeLucia et al., 2022) is trained with 2.5B tweets in 66 languages and MLM objective. Bernice consists of 270M parameters and a tweet-specific SentencePiece tokenizer including a vocabulary size of 250K.\n(4) InfoDCL (Zhang et al., 2023a) further trains XLM-R with 100M tweets in 66 languages with two contrastive learning, MLM, and distant label prediction objectives. InfoDCL shows that it effectively learns language representations for understanding SM."
        },
        {
            "heading": "B.2 Zero-shot Setting on LLMs",
            "text": "We also investigate the zero-shot performance on a wide range of LLMs:\n(1) BLOOM (Scao et al., 2022) is a Transformer decoder-only model trained on the ROOTS corpus consisting of 46 natural and 13 programming languages. BLOOM uses a multilingual vocabulary with 250K tokens and is trained with auto-regressive language modelling objectives.\n(2) Multilingual T5 (mT5) (Xue et al., 2021) is Transformer encoder-decoder model trained on CommonCrawl data involving 101 languages and contains a vocabulary with 250K tokens. It trained with sequence-to-sequence MLM objective.\n(3) LLaMA (Touvron et al., 2023) is a Transformer decoder-only model pretrained on 1.4T tokens where the majority are English and a small amount of data in 20 other languages. We utilize LLaMA with 7B parameters and a vocabulary with 30K tokens.\n(4) BLOOMZ (Muennighoff et al., 2022) is also an instruction finetune model. It further finetunes BLOOM on xP3 corpus that contains 13 type of tasks in 46 languages with English prompt. We benchmark SPARROW on the BLOOM-based models with a size of 7.1B parameters.\n(5) BLOOMZ-P3 (Muennighoff et al., 2022) is an instruction finetuned model. It is initialized by BLOOM and further finetunes on English-only P3 corpus (Sanh et al., 2022) containing 2, 073 natural language prompts for eight types of NLP tasks.\n(6) BLOOM-Bactrian (Li et al., 2023) tune BLOOM on a 3.4M instruction-following dataset in 52 languages with low-rank adaptation modules. Li et al. (2023) translate the English 67K instructions from Alpaca and Dolly datasets into 51 languages and utilize ChatGPT API to generate responses in the corresponding language.\n(7) mT0 (Muennighoff et al., 2022) is instruction fine-tuned mT5 model with xP3 corpus. We evaluate the mT5-based models with XL size (with 3.7B parameters).\n(8) Alpaca (Taori et al., 2023) further tune LLaMA on a 52K instruction-following dataset that is generated by gpt-3.5-turbo of OpenAI API. The dataset includes diverse English instruction-following tasks, e.g., question answering and programming.\n(9) Vicuna (Chiang et al., 2023) further tune LLaMA on 70K diverse user-shared conversations with ChatGPT in English.\n(10) ChatGPT is a conversation-based LLM trained GTP-3 (Brown et al., 2020) through reinforcement learning with human feedback (Ouyang\net al., 2022; Christiano et al., 2017). We expolit gpt-3.5-turbo-0301 via OpenAI API.11"
        },
        {
            "heading": "C Experiments",
            "text": ""
        },
        {
            "heading": "C.1 Hyperparameters",
            "text": "To be computation friendly, we only tune the peak learning rate of each model in a set of {1e\u22124, 5e\u2212 5, 3e\u2212 5, 1e\u2212 5} and randomly select 45 datasets for hyper-parameter tuning. We fine-tune a PLM with an arbitrary batch size of 32, sequence length of 128 tokens, and 20 epochs with patience of five epochs based on the model performance on Dev set. We fine-tune each dataset three time with different seeds and identify the best model based on Dev set performance. The best learning rate for each model is identified based on the average score of Dev set of the 45 datasets. The best peak learning rate is 3e\u22125 for mBERT, XLM-T, and Bernice and 1e\u2212 5 for other models."
        },
        {
            "heading": "C.2 Prompts",
            "text": "The prompts we use in our experiments are summarized in Table 15."
        },
        {
            "heading": "C.3 Results",
            "text": "Table 16 shows aggregated performance of finetuned models on Dev and Test-S. We report the average of dataset-specific metrics and standard deviation in a task and a category. We also report the Test-S performance of tasks of antisocial language detection, emotion recognition, humor detection, irony and sarcasm detection, sentiment analysis, and subjectivity analysis in Tables 17, 18, 19, 20, 21, and 22, respectively.\nWe provide a concise study to probe the sensitivity of open-source LLMs to prompts and present the results in Table 24.\n11https://openai.com/"
        },
        {
            "heading": "Dataset Study Lang. Source / Domain Year #Lb Labels Data Slipt Metric",
            "text": ""
        },
        {
            "heading": "Dataset Study Lang Source / Domain Year #Lb Labels Data Slipt Metric",
            "text": "Dataset Metric Random mBERT XLM-R Bernice InfoDCL BLOOM BLOOMZ BLOOMZ(MT) BLOOMZP3 BLOOMZBactrian mT5 mT0 mT0 (MT) LLaMA Alpaca Vicuna ChatGPT ChatGPT (MT) SoTA SoTA study\nDataset Metric Random mBERT XLM-R Bernice InfoDCL BLOOM BLOOMZ BLOOMZ(MT) BLOOMZP3 BLOOMZBactrian mT5 mT0 mT0 (MT) LLaMA Alpaca Vicuna ChatGPT ChatGPT (MT) SoTA SoTA study\nDataset Metric Random mBERT XLM-R Bernice InfoDCL BLOOM BLOOMZ BLOOMZ(MT) BLOOMZP3 BLOOMZBactrian mT5 mT0 mT0 (MT) LLaMA Alpaca Vicuna ChatGPT ChatGPT (MT) SoTA SoTA study\nDataset Metric Random mBERT XLM-R Bernice InfoDCL BLOOM BLOOMZ BLOOMZ(MT) BLOOMZP3 BLOOMZBactrian mT5 mT0 mT0 (MT) LLaMA Alpaca Vicuna ChatGPT ChatGPT (MT) SoTA SoTA study"
        },
        {
            "heading": "Dataset Metric Random mBERT XLM-R Bernice InfoDCL BLOOM BLOOMZ BLOOMZ(MT)",
            "text": "Dataset Metric Random mBERT XLM-R Bernice InfoDCL BLOOM BLOOMZ BLOOMZ(MT) BLOOMZP3 BLOOMZBactrian mT5 mT0 mT0 (MT) LLaMA Alpaca Vicuna ChatGPT ChatGPT (MT) SoTA SoTA study\nis in green highlight . The red font denotes a performance lower than the random baseline. BMZ: BLOOMZ, CG: ChatGPT, MT: using machine translated prompts."
        }
    ],
    "title": "The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages",
    "year": 2023
}