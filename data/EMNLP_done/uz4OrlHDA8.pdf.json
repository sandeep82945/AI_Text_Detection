{
    "abstractText": "Temporal data distribution shift is prevalent in the financial text. How can a financial sentiment analysis system be trained in a volatile market environment that can accurately infer sentiment and be robust to temporal data distribution shifts? In this paper, we conduct an empirical study on the financial sentiment analysis system under temporal data distribution shifts using a real-world financial social media dataset that spans three years. We find that the fine-tuned models suffer from general performance degradation in the presence of temporal distribution shifts. Furthermore, motivated by the unique temporal nature of the financial text, we propose a novel method that combines out-of-distribution detection with time series modeling for temporal financial sentiment analysis. Experimental results show that the proposed method enhances the model\u2019s capability to adapt to evolving temporal shifts in a volatile financial market.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yue Guo"
        },
        {
            "affiliations": [],
            "name": "Chenxi Hu"
        },
        {
            "affiliations": [],
            "name": "Yi Yang"
        }
    ],
    "id": "SP:52cb2c61ade281771573ec57ef95e0cc7ec37b77",
    "references": [
        {
            "authors": [
                "Khrystyna Bochkay",
                "Stephen V. Brown",
                "Andrew J. Leone",
                "Jennifer Wu Tucker."
            ],
            "title": "Textual analysis in accounting: What\u2019s next?",
            "venue": "Contemporary Accounting Research, 40(2):765\u2013805.",
            "year": 2023
        },
        {
            "authors": [
                "Chengyu Chuang",
                "Yi Yang."
            ],
            "title": "Buy tesla, sell ford: Assessing implicit stock market preference in pre-trained language models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL",
            "year": 2022
        },
        {
            "authors": [
                "Keith Cortis",
                "Andr\u00e9 Freitas",
                "Tobias Daudert",
                "Manuela H\u00fcrlimann",
                "Manel Zarrouk",
                "Siegfried Handschuh",
                "Brian Davis."
            ],
            "title": "Semeval-2017 task 5: Finegrained sentiment analysis on financial microblogs and news",
            "venue": "Proceedings of the 11th International",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Lin Lawrence Guo",
                "Stephen R Pfohl",
                "Jason Fries",
                "Alistair EW Johnson",
                "Jose Posada",
                "Catherine Aftandilian",
                "Nigam Shah",
                "Lillian Sung"
            ],
            "title": "Evaluation of domain generalization and adaptation on improving model robustness to temporal dataset shift in clinical",
            "year": 2022
        },
        {
            "authors": [
                "Lin Lawrence Guo",
                "Ethan Steinberg",
                "Scott Lanyon Fleming",
                "Jose Posada",
                "Joshua Lemmon",
                "Stephen R Pfohl",
                "Nigam Shah",
                "Jason Fries",
                "Lillian Sung."
            ],
            "title": "Ehr foundation models improve robustness in the presence of temporal distribution shift",
            "venue": "Scientific",
            "year": 2023
        },
        {
            "authors": [
                "Yue Guo",
                "Zian Xu",
                "Yi Yang."
            ],
            "title": "Is chatgpt a financial expert? evaluating language models on financial natural language processing",
            "venue": "CoRR, abs/2310.12664.",
            "year": 2023
        },
        {
            "authors": [
                "Allen H Huang",
                "Hui Wang",
                "Yi Yang."
            ],
            "title": "Finbert: A large language model for extracting information from financial text",
            "venue": "Contemporary Accounting Research.",
            "year": 2022
        },
        {
            "authors": [
                "Siavash Kazemian",
                "Shunan Zhao",
                "Gerald Penn."
            ],
            "title": "Evaluating sentiment analysis in the context of securities trading",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin,",
            "year": 2016
        },
        {
            "authors": [
                "Mark Kritzman",
                "Sebastien Page",
                "David Turkington."
            ],
            "title": "Regime shifts: Implications for dynamic strategies (corrected)",
            "venue": "Financial Analysts Journal, 68(3):22\u201339.",
            "year": 2012
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Tim Loughran",
                "Bill McDonald."
            ],
            "title": "Textual analysis in accounting and finance: A survey",
            "venue": "Journal of Accounting Research, 54(4):1187\u20131230.",
            "year": 2016
        },
        {
            "authors": [
                "Pekka Malo",
                "Ankur Sinha",
                "Pekka J. Korhonen",
                "Jyrki Wallenius",
                "Pyry Takala."
            ],
            "title": "Good debt or bad debt: Detecting semantic orientations in economic texts",
            "venue": "J. Assoc. Inf. Sci. Technol., 65(4):782\u2013796.",
            "year": 2014
        },
        {
            "authors": [
                "Peter Nystrup",
                "Henrik Madsen",
                "Erik Lindstr\u00f6m."
            ],
            "title": "Dynamic portfolio optimization across hidden market regimes",
            "venue": "Quantitative Finance, 18(1):83\u201395.",
            "year": 2018
        },
        {
            "authors": [
                "Raj Shah",
                "Kunal Chawla",
                "Dheeraj Eidnani",
                "Agam Shah",
                "Wendi Du",
                "Sudheer Chava",
                "Natraj Raman",
                "Charese Smiley",
                "Jiaao Chen",
                "Diyi Yang."
            ],
            "title": "When FLUE meets FLANG: Benchmarks and large pretrained language model for financial domain",
            "venue": "Pro-",
            "year": 2022
        },
        {
            "authors": [
                "Tianlu Wang",
                "Rohit Sridhar",
                "Diyi Yang",
                "Xuezhi Wang."
            ],
            "title": "Identifying and mitigating spurious correlations for improving robustness in NLP models",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, Seattle, WA, United States,",
            "year": 2022
        },
        {
            "authors": [
                "Zhao Wang",
                "Aron Culotta."
            ],
            "title": "Robustness to spurious correlations in text classification via automatically generated counterfactuals",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications",
            "year": 2021
        },
        {
            "authors": [
                "Shijie Wu",
                "Ozan Irsoy",
                "Steven Lu",
                "Vadim Dabravolski",
                "Mark Dredze",
                "Sebastian Gehrmann",
                "Prabhanjan Kambadur",
                "David S. Rosenberg",
                "Gideon Mann."
            ],
            "title": "Bloomberggpt: A large language model for finance",
            "venue": "CoRR, abs/2303.17564.",
            "year": 2023
        },
        {
            "authors": [
                "Yi Yang",
                "Mark Christopher Siy Uy",
                "Allen Huang."
            ],
            "title": "Finbert: A pretrained language model for financial communications",
            "venue": "CoRR, abs/2006.08097.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Temporal data distribution shift is prevalent in the financial text. How can a financial sentiment analysis system be trained in a volatile market environment that can accurately infer sentiment and be robust to temporal data distribution shifts? In this paper, we conduct an empirical study on the financial sentiment analysis system under temporal data distribution shifts using a real-world financial social media dataset that spans three years. We find that the fine-tuned models suffer from general performance degradation in the presence of temporal distribution shifts. Furthermore, motivated by the unique temporal nature of the financial text, we propose a novel method that combines out-of-distribution detection with time series modeling for temporal financial sentiment analysis. Experimental results show that the proposed method enhances the model\u2019s capability to adapt to evolving temporal shifts in a volatile financial market."
        },
        {
            "heading": "1 Introduction",
            "text": "Natural language processing (NLP) techniques have been widely adopted in financial applications, such as financial sentiment analysis, to facilitate investment decision-making and risk management (Loughran and McDonald, 2016; Kazemian et al., 2016; Bochkay et al., 2023). However, the nonstationary financial market environment can bring about significant changes in the data distribution between model development and deployment, which can degrade the model\u2019s performance over time and, consequently, its practical value. For example, a regime shift in the stock market refers to a significant change in the underlying economic or financial conditions. A regime shift, which may be triggered by changes in interest rates or political events, can significantly affect the market behavior and investor sentiment (Kritzman et al., 2012; Nystrup et al., 2018).\nThere has been limited research on the temporal dataset shift in the financial context. Existing NLP works on financial sentiment analysis follow the conventional approach that randomly splits a dataset into training and testing so that there is no distribution shift between training and testing (Malo et al., 2014; Cortis et al., 2017). However, in a real-world financial sentiment analysis system, there could be unpredictable distribution shifts between the data that is used to build the model (in-sample data) and the data that the model runs inference on (out-of-sample data). As a result, the practitioners often face a dilemma. If the model fits too well to the in-sample data, it may experience a drastic drop in the out-of-sample data if a distribution shift happens (such as a regime shift from a bull market to a bear market); if the model is built to minimize performance disruption, its performance may be unsatisfactory on the in-sample data as well as the out-of-sample data.\nIn this paper, we raise our first research question RQ1: how does temporal data shift affect the robustness of financial sentiment analysis? The question is not as trivial as it seems. For example, Guo et al. (2023a) find that language models are robust to temporal shifts in healthcare prediction tasks. However, financial markets may exhibit even more drastic changes. To answer this question, we systematically assess several language models, from BERT to GPT-3.5, with metrics that measure both the model capacity and robustness under temporal distribution shifts. In our monthly rollingbased empirical analysis, this dilemma between in-sample performance and out-of-sample performance is confirmed. We find that fine-tuning a pretrained language model (such as BERT) fails to produce robust sentiment classification performance in the presence of temporal distribution shifts.\nMoreover, we are interested in RQ2: how to mitigate the performance degradation of financial sentiment analysis in the existence of temporal dis-\ntribution shift? Motivated by the unique temporal nature of financial text data, we propose a novel method that combines out-of-distribution (OOD) detection with autoregressive (AR) time series modeling. Experiments show that OOD detection can effectively identify the samples causing the model performance degradation (we refer to those samples as OOD data). Furthermore, the model performance on the OOD data is improved by an autoregressive time series modeling on the historical model predictions. As a result, the model performance degradation from in-sample data to out-ofsample data is alleviated.\nThis work makes two contributions to the literature. First, while sentiment analysis is a very well-studied problem in the financial context, the long-neglected problem is how to build a robust financial sentiment analysis model under the pervasive distribution shift. To our knowledge, this paper provides the first empirical evidence of the impact of temporal distribution shifts on financial sentiment analysis. Second, we propose a novel approach to mitigate the out-of-sample performance degradation while maintaining in-sample sentiment analysis utility. We hope this study contributes to the continuing efforts to build a more robust and accountable financial NLP system."
        },
        {
            "heading": "2 Temporal Distribution Shift in Financial Sentiment Analysis",
            "text": "In this section, we first define the task of financial sentiment analysis on temporal data. We then introduce two metrics for model evaluation under the data distribution shift."
        },
        {
            "heading": "2.1 Problem Formulation",
            "text": "The financial sentiment analysis model aims to classify a text input, such as a social media post or financial news, into positive or negative classes 1. It can be expressed as a text classification model M : M(X) 7\u2192 Y . Conventionally, this task is modeled and evaluated on a non-temporal dataset, i.e., (X,Y ) consists of independent examples unrelated to each other in chronological order.\nIn the real world, financial text data usually exhibits temporal patterns corresponding to its occurrence time. To show this pattern, we denote (X,Y ) = {(X1, Y1), ..., (XN , YN )}, where\n1We consider binary positive/negative prediction in this paper. Other financial analysis systems may have an additional neutral label (Huang et al., 2022).\n(Xt, Yt) denotes a set of text and associated sentiment label collected from time t. Here t could be at various time horizons, such as hourly, daily, monthly, or even longer horizon.\nIn the real-world scenarios, at time t, the sentiment classification model can only be trained with the data that is up to t, i.e., {(X1, Y1), ..., (Xt, Yt))}. We denote the model trained with data up to period t as Mt. In a continuous production system, the model is applied to the data (Xt+1, Yt+1) in the next time period t+ 1.\nThe non-stationary financial market environment leads to different data distributions at different periods, i.e., there is a temporal distribution shift. However, the non-stationary nature of the financial market makes it difficult to predict how data will be distributed in the next period. Without loss of generality, we assume p(Xt, Yt) \u0338= p(Xt+1, Yt+1) for any time t."
        },
        {
            "heading": "2.2 Evaluation Metrics",
            "text": "Unlike traditional financial sentiment analysis, temporal financial sentiment analysis trains the model on in-sample data and applies the model to the out-of-sample data. Therefore, in addition to the in-sample sentiment analysis performance, we also care about its generalization performance on outof-sample data. In other words, we hope the model experiences minimal performance degradation even under significant temporal distribution shifts. Specifically, we use the standard classification metric F1-Score to measure the model performance. To measure the model generalization, we use \u2206F1 = F1in\u2212F1out, where F1in and F1out are the F1-Score on in-sample and out-of-sample data respectively. An ideal financial sentiment analysis model would achieve high F1in and F1out and low \u2206F1 at the same time."
        },
        {
            "heading": "3 Experiment Setup",
            "text": "This section describes the evaluation setups on the dataset and models for temporal model analysis."
        },
        {
            "heading": "3.1 Dataset",
            "text": "We collect a time-stamped real-world financial text dataset from StockTwits2, a Twitter-like social media platform for the financial and investing community. StockTwits data is also used in prior NLP work for financial sentiment analysis (Cortis et al., 2017), though the data is used in a conventional\n2https://stocktwits.com/\nnon-temporal setting. In our experiment, we collect all posts from StockTwits spanning from 2014-1-1 to 2016-12-31 3. We then filter the dataset by selecting those posts that contain a user-generated sentiment label bullish (\"1\") or bearish (\"0\"). The final dataset contains 418,893 messages, each associated with a publish date, providing helpful information for our temporal-based empirical analysis.\nWe provide some model-free evidence on the temporal distribution shift. First, we plot the \"sentiment positivity\" score for the monthly sentiment label distributions of the whole dataset in Figure 1 (a). The sentiment positivity is defined as the percentage of the positive samples, i.e., #pos/(#pos+#neg). It shows that while most messages are positive each month, the ratio between positive and negative samples fluctuates.\nWe then choose two representative companies, Apple Inc. and Tesla Inc., and filter the messages with the token \"Apple\" or \"Tesla\". We plot their sentiment positivity score in the solid lines in Figure 1 (b) and (c), respectively. We also plot the\n3More recent year data is not easily accessible due to API restriction.\nmonthly stock price return of Apple Inc. and Tesla Inc. in the dashed line in Figure 1 (b) and (c). It shows that the sentiment and the stock price movement are highly correlated, and the Spearman Correlation between the sentiment and the monthly return is 0.397 for Apple and 0.459 for Tesla. Moreover, the empirical conditional probability of labels given the specific token (i.e., \"Apple\", \"Tesla\") varies in different months. Taking together, we observe a temporal distribution shift in the financial social media text."
        },
        {
            "heading": "3.2 Sentiment Classification Models",
            "text": "We choose several standard text classification methods for sentiment classification, including (1) a simple logistic regression classifier that uses bag-ofwords features of the input sentences, (2) an LSTM model with a linear classification layer on top of the LSTM hidden output, (3) three pretrained language models: BERT (base, uncased) (Devlin et al., 2019), RoBERTa (base) (Liu et al., 2019) and a finance domain specific pretrained model FinBERT (Yang et al., 2020); (4) the large language model GPT-3.5 (text-davinci-003) (Brown et al., 2020) with two-shot in-context learning."
        },
        {
            "heading": "4 Empirical Evaluation of Temporal Data Shift in Financial Sentiment Analysis",
            "text": "Our first research question aims to empirically examine if temporal data shift affects the robustness of financial sentiment analysis and to what extent. Prior literature has studied temporal distribution shifts in the healthcare domain and finds that pretrained language models are robust in the presence of temporal distribution shifts for healthcarerelated prediction tasks such as hospital readmission (Guo et al., 2023a). However, financial markets and the financial text temporal shift are much more volatile. We empirically answer this research question using the experiment setup discussed in Section 3."
        },
        {
            "heading": "4.1 Training Strategy",
            "text": "Training a sentiment classification model on the time-series data is not trivial, as different utilization of the historical data and models would lead to different results in model performance and generalization. To comprehensively understand the model behavior under various settings, we summarize three training strategies by the different incorporation of the historical data and models.\nOld Data, New Model (ODNM): This training strategy uses all the available data up to time t, i.e.{(X1, Y1), ..., (Xt, Yt)} to train a new model Mt. With this training strategy, the sentiment analysis model is trained with the most diverse financial text data that is not restricted to the most recent period.\nNew Data, New Model (NDNM): For each time period t, a new model Mt is trained with the latest data (Xt, Yt) collected in time t. This training strategy fits the model to the most recent data, which may have the most similar distribution to the outof-sample data if there is no sudden change in the market environment.\nNew Data, Old Model (NDOM): Instead of training a new model from scratch every time, we update the model trained at the previous time with the latest data. Specifically, in time t, the parameters of the model Mt are initialized with the parameters from Mt\u22121 and continuously learn from (Xt, Yt). This training strategy inherits the knowledge from past data but still adapts to more recent data.\nFor GPT-3.5, we use two-shot in-context learning to prompt the model. The in-context examples are randomly selected from (Xt, Yt). The prompt is \"Perform financial sentiment classification: text:{a positive example} label:positive; text:{a negative example} label:negative; text:{testing example} label:\"."
        },
        {
            "heading": "4.2 Rolling-based Empirical Test",
            "text": "To empirically examine temporal sentiment classification models, we take a rolling-based approach. We divide the dataset by month based on the timestamp of each text. Since we have three years of StockTwits data, we obtain 36 monthly subsets. For each month t, we train a sentiment classification model Mt (Section 3.2, except GPT-3.5) using a training strategy in Section 4.1 that uses data up to month t. For evaluation, we consider the testing samples from (Xt, Yt) as the in-sample and the testing samples from (Xt+1, Yt+1) as out-ofsample. This rolling-based approach simulates a real-world continuous production setup. Since we have 36 monthly datasets, our temporal-based empirical analysis is evaluated on 36\u22121 = 35 monthly datasets (except the last month). We report the average performance in F1-score and \u2206F1 as 2.2. The train/validate/test split is by 7:1.5:1.5 randomly."
        },
        {
            "heading": "4.3 Empirical Results",
            "text": "We evaluate different sentiment classification models using different training strategies 4. We present the main empirical results on the original imbalanced dataset in Table 1, averaged over the 35 monthly results. An experiment using the balanced dataset after up-sampling the minor examples is presented in Appendix A, from which similar conclusions can be drawn. To better understand the model performance over time, we plot the results by month in Figure 2, using BERT and NDOM strategy as an example. The monthly results of NDNM and ODNM on BERT are shown in Appendix B. Furthermore, to compare the performance and the robustness against data distribution shift among the training strategies, we plot the in-sample performance F1in(avg) in Figure 3, and the performance drop \u2206F1(avg) in Figure 4 by the training strategies. The F1out(avg) is supplemented in Appendix B.\nWe have the following observations from the experimental results: The performance drop in the out-of-sample prediction is prevalent, especially for the negative label. All \u2206F1 of the finetuned models in table 1 are positive, indicating that all fine-tuned models suffer from performance degradation when serving the models to the next month\u2019s data. Such performance drop is especially significant for the negative label, indicating that the minority label suffers even more in the model generalization. The results remain after up-sampling the minority label, as shown in Appendix A.\nNDOM training strategy achieves the best insample performance, yet fails to generalize on outof-sample. Table 1 and Figure 3 show that NDOM has the highest F1-score among the three training strategies, especially for the in-sample prediction. Training with ODNM strategy is most robust against data distribution shift. As shown in Table 1 and Figure 4, among the three training strategies, ODNM has the smallest performance drop in outof-sample predictions. It suggests that using a long range of historical data can even out the possible distribution shift in the dataset and improve the model\u2019s robustness.\nThere is a trade-off between in-sample performance and out-of-sample performance. As stated above, the NDOM strategy achieves the best\n4Since LR and LSTM are trained using dataset-specific vocabulary, the NDOM training strategy, which uses the old model\u2019s vocabulary, is not applicable.\nin-sample performance but suffers significant outof-sample degradation. ODNM, conversely, has the slightest performance degradation, yet its overall prediction capability is limited. From a practical perspective, both strategies are not ideal. First, accurately identifying the market sentiments is essential for a financial sentiment classification model to build a portfolio construction. Second, it is also important that financial sentiment classification models produce stable prediction performance so that the subsequent trading portfolio, driven by the prediction outcomes, can have the least disruption.\nIn GPT-3.5, the performance drop is not observed, but the in-sample and out-of-sample performance falls behind the fine-tuned models. It indicates that increasing the model size and training corpus may reduce the performance drop when facing a potential distribution shift. However, GPT3.5 is not ideal, as its performance on the in-sample and out-of-sample data is significantly worse than the fine-tuned models, which aligns with the findings from (Guo et al., 2023b). Moreover, as the training corpus in GPT-3.5 covers the data from 2014 to 2017, our test set may not be genuinely out-of-distribution regarding GPT-3.5, which may also result in alleviating the performance drop."
        },
        {
            "heading": "4.4 Additional Analysis",
            "text": "We conduct additional analysis to understand the problem of performance degradation further. We examine the relationship between distribution shift and performance drop. To measure the distribution shift between in-sample data (Xt, Yt) in month t and out-of-sample data (Xt+1, Yt+1) in month t+1, we follow the steps: 1) For each token v in the vocabulary V ,5 we estimate the empirical probability\n5V is the vocabulary from the pretrained tokenizer.\npt(y|v), pt+1(y|v), pt(v), pt+1(v). 2) We measure the distribution shift from (Xt, Yt) to (Xt+1, Yt+1) as the weighted sum of the KL-divergence (KLD) between the two conditional probability distributions: \u2211 v pt(v)KLD(pt(y|v), pt+1(y|v)).\nWe then compute the Spearman correlation between the performance drop \u2206F1 and the distribution shift from in-sample data (Xt, Yt) to out-ofsample data (Xt+1, Yt+1). The result is shown in Table 2, showing a significant positive correlation between performance drop and distribution shift. Therefore, a more significant distribution shift, primarily caused by financial market turbulence, can lead to more severe model performance degradation. This problem is especially problematic because the performance degradation may exacerbate sentiment-based trading strategy during a volatile market environment, leading to significant investment losses.\nSecond, we provide empirical evidence that the models are prone to be influenced by spurious correlations. Generally, a sentiment classification model makes predictions on the conditional probability p(y|v) based on some sentiment words v. Ideally, such predictions are effective if there is no distribution shift and the model can successfully capture the sentiment words (e.g., v = bearish, bullish, and so on). However, if a model is affected by spurious correlations, it undesirably associates the words with no sentiment with the label. The model generalization will be affected when the correlations between the spurious words and\nthe sentiment label change in the volatile financial market. For example, suppose the model makes predictions based on the spurious correlated words p(y|\"Tesla\"). When the market sentiment regarding Tesla fluctuates, the model\u2019s robustness will be affected.\nWe use the most explainable logistic regression model as an example to provide evidence for spurious correlations. The logistic regression model assigns a coefficient to each word in the vocabulary, suggesting the importance of the word contributed to the prediction. We fit a logistic regression model to our dataset and then extract the top 30 words with the highest coefficients (absolute value). The extracted words are listed in Table 3, with bold words indicating the sentiment words and the unbolded words as the spurious words. We can see that most words the model regards as important are not directly connected to the sentiments. As a result, the performance of model prediction p(y|v) is likely influenced by the changing financial sentiment in the volatile markets."
        },
        {
            "heading": "5 Mitigating Model Degradation under Temporal Data Shift",
            "text": "In the previous section, our analysis reveals a consistent performance degradation in financial sentiment classification models on out-of-sample data. In this section, we explore possible ways to mitigate the degradation. As our previous analysis shows that the performance degradation is correlated with the distribution shift, it is reasonable to infer that the performance degradation is caused by the failure of the out-of-distribution (OOD) examples. This observation motivates us to propose a two-stage method to improve the model robustness under temporal data distribution shifts. Firstly, we train an OOD sample detector to detect whether an upcoming sample is out of distribution. If not, we still use the model trained on in-sample data on this sample. If yes, we propose using an autoregressive model from time series analysis to simulate the model prediction towards the future."
        },
        {
            "heading": "5.1 Mitigation Method",
            "text": "This subsection introduces the mitigation method for temporal financial sentiment analysis."
        },
        {
            "heading": "5.1.1 Detecting OOD Samples",
            "text": "As the model trained on the historical data experiences performance degradation on the future data under distribution shift, we first employ an OOD\ndetection mechanism to determine the ineffective future data. To train the OOD detector, we collect a new dataset that contains the in-distribution (ID) data (label=0) and OOD data (label=1) regarding a model Mt. The labeling of the OOD dataset is based on whether Mt can correctly classify the sample, given the condition that the in-sample classifier can correctly classify the sample.\nSpecifically, let i,j denote the indexes of time satisfying i < j, given a target sentiment classifier Mi, and a sample (xj , yj) which can be correctly classified by the in-sample sentiment model Mj , the OOD dataset assigns the label by the rule\nOOD(Mi, xj) = { 0, if Mi(xj) = yj 1, Mi(xj) \u0338= yj\n(1)\nAfter collecting the OOD dataset, we train an OOD classifier f(M,x) on the OOD dataset to detect whether a sample x is OOD concerning the model M . The classifier is a two-layer multi-layer perceptron (MLP) on the [CLS] token of M(x), i.e.,\nf(M,x) = W2(GELU(W1(M [CLS](x))+b1))+b2\n(2) The classifier is optimized through the crossentropy loss on the OOD dataset. During training, the sentiment model M parameters are fixed, and only the parameters of the MLP (i.e., W1,W2, b1, b2) are updated by gradient descent. The parameters of the OOD classifier are used across all sentiment models M \u2208 {M1, ...,MN} regardless of time.\nDuring inference, given a sentiment model Mt and an out-of-sample xt+1 , we compute f(Mt, xt+1) to detect whether xt+1 is OOD regarding to Mt. If xt+1 is not an OOD sample, we infer the sentiment of xt+1 by Mt. Otherwise, xt+1 is regarded as an OOD sample, and Mt may suffer from model degradation. Therefore, we predict the sentiment of xt+1 by the autoregressive model from time-series analysis to avoid potential ineffectiveness."
        },
        {
            "heading": "5.1.2 Autoregressive Modeling",
            "text": "In time series analysis, an autoregressive (AR) model assumes the future variable can be expressed by a linear combination of its previous values and on a stochastic term. Motivated by this, as the distribution in the future is difficult to estimate directly, we assume the prediction from a future model can\nalso be expressed by the combination of the past models\u2019 predictions. Specifically, given an OOD sample xt+1 detected by the OOD classifier, the prediction y\u0302t+1 is given by linear regression on the predictions from the past models Mt, ...,Mt\u2212p+1, i.e.,\ny\u0302t+1 = p\u22121\u2211 k=0 \u03b1kMt\u2212k(xt+1) + \u03f5 (3)\n, where \u03b1k is the regression coefficient and \u03f5 is the error term estimated from the past data. Moreover, p is the order of the autoregressive model determined empirically.\nFor temporal data in financial sentiment classification, the future distribution is influenced by an aggregation of recent distributions and a stochastic term. Using an AR model on previous models\u2019 predictions can capture this feature. The AR modeling differs from a weighted ensemble method that assigns each model a fixed weight. In our method, weights assigned to past models are determined by how recently they were trained or used, with more recent models receiving higher weights."
        },
        {
            "heading": "5.2 Experiment Setup",
            "text": "To train the OOD detector and estimate the parameters in the AR model, we use data from 2014-01 to 2015-06. We split the data each month by 7:1.5:1.5 for sentiment model training, detector/AR model training, and model testing, respectively. The data from 2015-07 to 2016-12 is used to evaluate the effectiveness of the mitigation method.\nTo train the OOD detector, we use AdamW optimizer and grid search for the learning rate in [2\u00d710\u22123, 2\u00d710\u22124, 2\u00d710\u22125], batch size in [32, 64]. When building the OOD dataset regarding Mt, we use the data that happened within three months starting from t.\nTo estimate the parameters in the AR model, for a training sample (xt, yt), we collect the predictions of xt from Mt\u22121, ..,Mt\u2212p, and train a regression model to predict yt, for t from 2014-01+p to 2015-06. We empirically set the order of the AR model p as 3 in the experiment."
        },
        {
            "heading": "5.3 Baselines",
            "text": "Existing NLP work has examined model robustness on out-of-domain data in a non-temporal shift setting. We experiment with two popular methods, spurious tokens masking (Wang et al., 2022) and counterfactual data augmentation (Wang and Culotta, 2021), to examine their capability in miti-\ngating the performance drop under temporal data distribution shift.\nSpurious Tokens Masking (Wang et al., 2022) (STM) is motivated to improve the model robustness by reducing the spurious correlations between some tokens and labels. STM identifies the spurious tokens by conducting cross-dataset stability analysis. While genuine and spurious tokens have high importance, \"spurious\" tokens tend to be important for one dataset but fail to generalize to others. Therefore, We identify the \"spurious tokens\" as those with high volatility in the attention score across different months from 2014-01 to 2015-06. Then, we mask the identified spurious tokens during training and inference on the data from 2015-07 to 2016-12.\nCounterfactual Data Augmentation (Wang and Culotta, 2021) (CDA) improves the model robustness by reinforcing the impact of the causal clues. It first identifies the causal words by a matching algorithm and then generates the counterfactual data by replacing the identified causal word with its antonym. Like STM, we identify causal words on the monthly datasets from 2014-01 to 2015-06.\nMore details of the setup of the two baseline methods are presented in Appendix C."
        },
        {
            "heading": "5.4 Mitigation Results",
            "text": "First, we analyze the performance of the OOD detector. Table 4 shows the classification reports of the OOD detector of BERT and FinBERT on the test set. The recall of OOD data is the most crucial indicator of the model performance, as we want to retrieve as much OOD data as possible before applying it to the AR model to avoid potential model degradation. As shown in table 4, the detector can\nachieve the recall of 0.86 and 0.91 for OOD data in BERT and FinBERT, respectively, indicating the adequate capability to identify the data that the sentiment models will wrongly predict. As the dataset is highly unbalanced towards the ID data, the relatively low precision in OOD data is expected. Nevertheless, the detector can achieve an accuracy of around 0.8 on the OOD dataset.\nTable 5 shows the results of the mitigation methods under the NDOM training strategy. We only apply our mitigation method to the out-of-sample prediction. For BERT, RoBERTa, and FinBERT, our method reduces the performance drop by 31%, 26%, and 32%, respectively. Our results show that The AR model can improve the model performance on OOD data. As a result, the overall out-of-sample performance is improved, and the model degradation is alleviated.\nAnother advantage of our methods is that, unlike the baseline methods, our method does not require re-training the sentiment models. Both baseline methods re-train the sentiment models on the newly generated datasets, either by data augmentation or spurious tokens masking, at the cost of influencing the model performance. Our proposed methods avoid re-training the sentiment models and improve the out-of-sample prediction with aggregation on the past models."
        },
        {
            "heading": "6 Related Works",
            "text": "Temporal Distribution Shift. While temporal distribution shift has been studied in other contexts such as healthcare (Guo et al., 2022), there is no systematic empirical study of temporal distribution shift in the finance domain. Moreover, although a prior study in the healthcare domain has shown\nthat the large language models can significantly mitigate temporal distribution shifts on healthcarerelated tasks such as readmission prediction (Guo et al., 2023a), financial markets are more volatile, so is the financial text temporal shift. Our empirical study finds that fine-tuned models suffer from model degradation under temporal distribution shifts. Financial Sentiment Analysis. NLP techniques have gained widespread adoption in the finance domain (Loughran and McDonald, 2016; Yang et al., 2020; Bochkay et al., 2023; Shah et al., 2022; Wu et al., 2023; Chuang and Yang, 2022). One of the essential applications is financial sentiment classification, where the inferred sentiment is used to guide trading strategies and financial risk management (Kazemian et al., 2016). However, prior NLP work on financial sentiment classification has not explored the temporal distribution shift problem, a common phenomenon in financial text. This work aims to investigate the financial temporal distribution shift empirically and proposes a mitigation method."
        },
        {
            "heading": "7 Conclusions",
            "text": "In this paper, we empirically study the problem of distribution shift over time and its adverse impacts on financial sentiment classification models. We find a consistent yet significant performance degradation when applying a sentiment classification model trained using in-sample (past) data to the out-of-sample (future) data. The degradation is driven by the data distribution shift, which, unfortunately, is the nature of dynamic financial markets. To improve the model\u2019s robustness against the ubiquitous distribution shift over time, we propose a novel method that combines out-of-distribution detection with autoregressive (AR) time series modeling. Our method is effective in alleviating the out-of-sample performance drop.\nGiven the importance of NLP in real-world financial applications and investment decision-making, there is an urgent need to understand the weaknesses, safety, and robustness of NLP systems. We raise awareness of this problem in the context of financial sentiment classification and conduct a temporal-based empirical analysis from a practical perspective. The awareness of the problem can help practitioners improve the robustness and accountability of their financial NLP systems and also calls for developing effective NLP systems that are\nrobust to temporal data distribution shifts.\nLimitations\nThis paper has several limitations to improve in future research. First, our temporal analysis is based on the monthly time horizon, so we only analyze the performance degradation between the model built in the current month t and the following month t + 1. Future analysis can investigate other time interval granularity, such as weekly or annual. Second, our data is collected from a social media platform. The data distribution on social media platforms may differ from other financial data sources, such as financial news articles or analyst reports. Thus, the robustness of language models on those types of financial text data needs to be examined, and the effectiveness of our proposed method on other types of financial text data warrants attention. Future research can follow our analysis pipeline to explore the impact of data distribution shifts on financial news or analyst reports textual analysis. Third, our analysis focuses on sentiment classification performance degradation. How performance degradation translates into economic losses is yet to be explored. Trading simulation can be implemented in a future study to understand the economic impact of the problem better."
        }
    ],
    "title": "Predict the Future from the Past? On the Temporal Data Distribution Shift in Financial Sentiment Classifications"
}