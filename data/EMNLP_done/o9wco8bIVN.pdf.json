{
    "abstractText": "State-of-the-art grammatical error correction (GEC) systems rely on parallel training data (ungrammatical sentences and their manually corrected counterparts), which are expensive to construct. In this paper, we employ the Break-It-Fix-It (BIFI) method to build an unsupervised GEC system. The BIFI framework generates parallel data from unlabeled text using a fixer to transform ungrammatical sentences into grammatical ones, and a critic to predict sentence grammaticality. We present an unsupervised approach to build the fixer and the critic, and an algorithm that allows them to iteratively improve each other. We evaluate our unsupervised GEC system on English and Chinese GEC. Empirical results show that our GEC system outperforms previous unsupervised GEC systems, and achieves performance comparable to supervised GEC systems without ensemble. Furthermore, when combined with labeled training data, our system achieves new state-of-the-art results on the CoNLL-2014 and NLPCC-2018 test sets.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Hannan Cao"
        },
        {
            "affiliations": [],
            "name": "Liping Yuan"
        },
        {
            "affiliations": [],
            "name": "Yuchen Zhang"
        },
        {
            "affiliations": [],
            "name": "Hwee Tou Ng"
        }
    ],
    "id": "SP:689cb3481fcf68eda466650863f0b6b318c6e034",
    "references": [
        {
            "authors": [
                "Dimitris Alikaniotis",
                "Vipul Raheja."
            ],
            "title": "The unreasonable effectiveness of transformer language models in grammatical error correction",
            "venue": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 127\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Abhijeet Awasthi",
                "Sunita Sarawagi",
                "Rasna Goyal",
                "Sabyasachi Ghosh",
                "Vihari Piratla."
            ],
            "title": "Parallel iterative edit models for local sequence transduction",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th",
            "year": 2019
        },
        {
            "authors": [
                "Ond\u0159ej Bojar",
                "Christian Federmann",
                "Mark Fishel",
                "Yvette Graham",
                "Barry Haddow",
                "Matthias Huck",
                "Philipp Koehn",
                "Christof Monz."
            ],
            "title": "Findings of the 2018 conference on machine translation (WMT18)",
            "venue": "Proceedings of the Third Conference on Machine",
            "year": 2018
        },
        {
            "authors": [
                "Adriane Boyd",
                "Jirka Hana",
                "Lionel Nicolas",
                "Detmar Meurers",
                "Katrin Wisniewski",
                "Andrea Abel",
                "Karin Sch\u00f6ne",
                "Barbora \u0160tindlov\u00e1",
                "Chiara Vettori."
            ],
            "title": "The MERLIN corpus: Learner language and the CEFR",
            "venue": "Proceedings of the Ninth International",
            "year": 2014
        },
        {
            "authors": [
                "Christopher Bryant",
                "Mariano Felice",
                "\u00d8istein E. Andersen",
                "Ted Briscoe."
            ],
            "title": "The BEA-2019 shared task on grammatical error correction",
            "venue": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages",
            "year": 2019
        },
        {
            "authors": [
                "Christopher Bryant",
                "Zheng Yuan",
                "Muhammad Reza Qorib",
                "Hannan Cao",
                "Hwee Tou Ng",
                "Ted Briscoe."
            ],
            "title": "Grammatical Error Correction: A Survey of the State of the Art",
            "venue": "Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Hannan Cao",
                "Wenmian Yang",
                "Hwee Tou Ng."
            ],
            "title": "Grammatical error correction with contrastive learning in low error density domains",
            "venue": "Findings of the",
            "year": 2021
        },
        {
            "authors": [
                "Hannan Cao",
                "Wenmian Yang",
                "Hwee Tou Ng."
            ],
            "title": "Mitigating exposure bias in grammatical error correction with data augmentation and reweighting",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Ciprian Chelba",
                "Tomas Mikolov",
                "Mike Schuster",
                "Qi Ge",
                "Thorsten Brants",
                "Phillipp Koehn",
                "Tony Robinson"
            ],
            "title": "One billion word benchmark for measuring progress in statistical language modeling",
            "year": 2014
        },
        {
            "authors": [
                "Shamil Chollampatt",
                "Hwee Tou Ng."
            ],
            "title": "A multilayer convolutional encoder-decoder neural network for grammatical error correction",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, pages 5755\u20135762.",
            "year": 2018
        },
        {
            "authors": [
                "Shamil Chollampatt",
                "Kaveh Taghipour",
                "Hwee Tou Ng."
            ],
            "title": "Neural network translation models for grammatical error correction",
            "venue": "Proceedings of IJCAI, pages 2768\u20132774.",
            "year": 2016
        },
        {
            "authors": [
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language models",
            "year": 2022
        },
        {
            "authors": [
                "Steven Coyne",
                "Keisuke Sakaguchi",
                "Diana Galvan-Sosa",
                "Michael Zock",
                "Kentaro Inui"
            ],
            "title": "Analyzing the performance of GPT-3.5 and GPT-4 in grammatical error correction",
            "year": 2023
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Shijin Wang",
                "Guoping Hu."
            ],
            "title": "Revisiting pre-trained models for Chinese natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Dahlmeier",
                "Hwee Tou Ng."
            ],
            "title": "Better evaluation for grammatical error correction",
            "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2012
        },
        {
            "authors": [
                "Daniel Dahlmeier",
                "Hwee Tou Ng",
                "Siew Mei Wu."
            ],
            "title": "Building a large annotated corpus of learner English: The NUS corpus of learner English",
            "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,",
            "year": 2013
        },
        {
            "authors": [
                "Roman Grundkiewicz",
                "Marcin Junczys-Dowmunt."
            ],
            "title": "The wiked error corpus: A corpus of corrective wikipedia edits and its application to grammatical error correction",
            "venue": "Advances in Natural Language Processing, pages 478\u2013490.",
            "year": 2014
        },
        {
            "authors": [
                "Roman Grundkiewicz",
                "Marcin Junczys-Dowmunt",
                "Kenneth Heafield."
            ],
            "title": "Neural grammatical error correction systems with unsupervised pre-training on synthetic data",
            "venue": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building",
            "year": 2019
        },
        {
            "authors": [
                "Shun Kiyono",
                "Jun Suzuki",
                "Masato Mita",
                "Tomoya Mizumoto",
                "Kentaro Inui."
            ],
            "title": "An empirical study of incorporating pseudo data into grammatical error correction",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Process-",
            "year": 2019
        },
        {
            "authors": [
                "Shaopeng Lai",
                "Qingyu Zhou",
                "Jiali Zeng",
                "Zhongli Li",
                "Chao Li",
                "Yunbo Cao",
                "Jinsong Su."
            ],
            "title": "Typedriven multi-turn corrections for grammatical error correction",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 3225\u20133236.",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jared Lichtarge",
                "Chris Alberti",
                "Shankar Kumar",
                "Noam Shazeer",
                "Niki Parmar",
                "Simon Tong."
            ],
            "title": "Corpora generation for grammatical error correction",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Yu Meng",
                "Yunyi Zhang",
                "Jiaxin Huang",
                "Chenyan Xiong",
                "Heng Ji",
                "Chao Zhang",
                "Jiawei Han."
            ],
            "title": "Text classification using label names only: A language model self-training approach",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Tomoya Mizumoto",
                "Mamoru Komachi",
                "Masaaki Nagata",
                "Yuji Matsumoto."
            ],
            "title": "Mining revision log of language learning SNS for automated Japanese error correction of second language learners",
            "venue": "Proceedings of 5th International Joint Conference on Natural",
            "year": 2011
        },
        {
            "authors": [
                "Courtney Napoles",
                "Maria N\u0103dejde",
                "Joel Tetreault."
            ],
            "title": "Enabling Robust Grammatical Error Correction in New Domains: Data Sets, Metrics, and Analyses",
            "venue": "Transactions of the Association for Computational Linguistics, pages 551\u2013566.",
            "year": 2019
        },
        {
            "authors": [
                "Hwee Tou Ng",
                "Siew Mei Wu",
                "Ted Briscoe",
                "Christian Hadiwinoto",
                "Raymond Hendy Susanto",
                "Christopher Bryant."
            ],
            "title": "The CoNLL-2014 shared task on grammatical error correction",
            "venue": "Proceedings of the Eighteenth Conference on Computational Natural",
            "year": 2014
        },
        {
            "authors": [
                "Hwee Tou Ng",
                "Siew Mei Wu",
                "Yuanbin Wu",
                "Christian Hadiwinoto",
                "Joel Tetreault."
            ],
            "title": "The CoNLL2013 shared task on grammatical error correction",
            "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared",
            "year": 2013
        },
        {
            "authors": [
                "Jingcheng Niu",
                "Gerald Penn."
            ],
            "title": "Grammaticality and language modelling",
            "venue": "Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems, pages 110\u2013119.",
            "year": 2020
        },
        {
            "authors": [
                "Muhammad Qorib",
                "Seung-Hoon Na",
                "Hwee Tou Ng."
            ],
            "title": "Frustratingly easy system combination for grammatical error correction",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Sascha Rothe",
                "Jonathan Mallinson",
                "Eric Malmi",
                "Sebastian Krause",
                "Aliaksei Severyn."
            ],
            "title": "A simple recipe for multilingual grammatical error correction",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th",
            "year": 2021
        },
        {
            "authors": [
                "Alla Rozovskaya",
                "Dan Roth."
            ],
            "title": "Grammar Error Correction in Morphologically Rich Languages: The Case of Russian",
            "venue": "Transactions of the Association for Computational Linguistics, 7:1\u201317.",
            "year": 2019
        },
        {
            "authors": [
                "Holger Schwenk",
                "Guillaume Wenzek",
                "Sergey Edunov",
                "Edouard Grave",
                "Armand Joulin",
                "Angela Fan."
            ],
            "title": "CCMatrix: Mining billions of high-quality parallel sentences on the web",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Yunfan Shao",
                "Zhichao Geng",
                "Yitao Liu",
                "Junqi Dai",
                "Fei Yang",
                "Li Zhe",
                "Hujun Bao",
                "Xipeng Qiu."
            ],
            "title": "CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation",
            "venue": "CoRR.",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Sorokin."
            ],
            "title": "Improved grammatical error correction by ranking elementary edits",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11416\u201311429, Abu Dhabi, United Arab Emirates.",
            "year": 2022
        },
        {
            "authors": [
                "Felix Stahlberg",
                "Shankar Kumar."
            ],
            "title": "Synthetic data generation for grammatical error correction with tagged corruption models",
            "venue": "Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, pages 37\u201347.",
            "year": 2021
        },
        {
            "authors": [
                "Xin Sun",
                "Tao Ge",
                "Shuming Ma",
                "Jingjing Li",
                "Furu Wei",
                "Houfeng Wang."
            ],
            "title": "A unified strategy for multilingual grammatical error correction with pretrained cross-lingual language model",
            "venue": "Proceedings of the Thirty-First International Joint Confer-",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, pages 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "Xiuyu Wu",
                "Yunfang Wu."
            ],
            "title": "From spelling to grammar: A new framework for Chinese grammatical error correction",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 889\u2013902.",
            "year": 2022
        },
        {
            "authors": [
                "Junyuan Xie",
                "Ross Girshick",
                "Ali Farhadi."
            ],
            "title": "Unsupervised deep embedding for clustering analysis",
            "venue": "Proceedings of the 33rd International Conference on International Conference on Machine Learning Volume 48, ICML\u201916, page 478\u2013487.",
            "year": 2016
        },
        {
            "authors": [
                "Qizhe Xie",
                "Minh-Thang Luong",
                "Eduard H. Hovy",
                "Quoc V. Le."
            ],
            "title": "Self-training with noisy student improves imagenet classification",
            "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19,",
            "year": 2020
        },
        {
            "authors": [
                "Ziang Xie",
                "Guillaume Genthial",
                "Stanley Xie",
                "Andrew Ng",
                "Dan Jurafsky."
            ],
            "title": "Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the",
            "year": 2018
        },
        {
            "authors": [
                "Helen Yannakoudakis",
                "Ted Briscoe",
                "Ben Medlock."
            ],
            "title": "A new dataset and method for automatically grading ESOL texts",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2011
        },
        {
            "authors": [
                "Michihiro Yasunaga",
                "Jure Leskovec",
                "Percy Liang."
            ],
            "title": "LM-critic: Language models for unsupervised grammatical error correction",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7752\u20137763.",
            "year": 2021
        },
        {
            "authors": [
                "Michihiro Yasunaga",
                "Percy Liang."
            ],
            "title": "Break-itfix-it: Unsupervised learning for program repair",
            "venue": "International Conference on Machine Learning.",
            "year": 2021
        },
        {
            "authors": [
                "Zheng Yuan",
                "Mariano Felice."
            ],
            "title": "Constrained Grammatical Error Correction using Statistical Machine Translation",
            "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 52\u201361.",
            "year": 2013
        },
        {
            "authors": [
                "Baolin Zhang."
            ],
            "title": "Features and functions of the HSK dynamic composition corpus (in Chinese)",
            "venue": "International Chinese Language Education, pages 71\u201379.",
            "year": 2009
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in Neural Information Processing Systems, volume 28.",
            "year": 2015
        },
        {
            "authors": [
                "Yue Zhang",
                "Zhenghua Li",
                "Zuyi Bao",
                "Jiacheng Li",
                "Bo Zhang",
                "Chen Li",
                "Fei Huang",
                "Min Zhang."
            ],
            "title": "MuCGEC: a multi-reference multi-source evaluation dataset for Chinese grammatical error correction",
            "venue": "Proceedings of the 2022 Conference of",
            "year": 2022
        },
        {
            "authors": [
                "Yue Zhang",
                "Bo Zhang",
                "Zhenghua Li",
                "Zuyi Bao",
                "Chen Li",
                "Min Zhang."
            ],
            "title": "SynGEC: Syntax-enhanced grammatical error correction with a tailored GECoriented parser",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Yuanyuan Zhao",
                "Nan Jiang",
                "Weiwei Sun",
                "Xiaojun Wan."
            ],
            "title": "Overview of the NLPCC 2018 shared task: Grammatical error correction",
            "venue": "CCF International Conference on Natural Language Processing and Chinese Computing, pages 439\u2013445.",
            "year": 2018
        },
        {
            "authors": [
                "Zewei Zhao",
                "Houfeng Wang."
            ],
            "title": "Maskgec: Improving neural grammatical error correction via dynamic masking",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 34(01):1226\u20131233.",
            "year": 2020
        },
        {
            "authors": [
                "Micha\u0142 Ziemski",
                "Marcin Junczys-Dowmunt",
                "Bruno Pouliquen"
            ],
            "title": "The United Nations parallel corpus v1.0",
            "venue": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Grammatical Error Correction (GEC) (Chollampatt et al., 2016; Chollampatt and Ng, 2018; Qorib et al., 2022; Bryant et al., 2023) is the task of correcting errors in a source sentence and generating a grammatically correct target sentence. Current state-of-the-art (SOTA) systems (Rothe et al., 2021) have reached good performance using sequence-tosequence (seq2seq) models. However, a common drawback of these systems is their extensive reliance on a significant quantity of labeled data. For instance, Rothe et al. (2021) utilized over 2 million sentence pairs, which are time-consuming and costly to obtain as they require human manual correction. Unsupervised GEC systems aim to over-\n\u2217Work done during Cao\u2019s internship at ByteDance. 1Source code available at https://github.com/nusnl\np/ugec.\ncome this limitation. However, the current performance of unsupervised GEC systems (Alikaniotis and Raheja, 2019; Yasunaga et al., 2021) is much lower than supervised systems. Moreover, they still require manually defined or extracted confusion sets to generate synthetic data and assess sentence grammaticality. As a result, this greatly hinders the applicability of unsupervised GEC systems.\nThe SOTA unsupervised GEC system, LM-critic (Yasunaga et al., 2021), uses the Break-It-Fix-It (BIFI) framework (Yasunaga and Liang, 2021) to extract realistic parallel data from unlabeled data. Specifically, the BIFI framework utilizes a fixer and a critic. The fixer is designed to perform the GEC task, while the critic is designed for the grammatical error detection (GED) task, which classifies an input sentence as grammatical or ungrammatical. Given a critic which classifies each unlabeled sentence as grammatical or ungrammatical, BIFI generates parallel data to train a better fixer by the following four steps. (1) Correct ungrammatical sentences with the existing fixer and collect outputs that are classified as grammatical by the critic. (2) Train a grammatical error generator (called a breaker) using the sentence pairs obtained in (1). (3) Corrupt the grammatical sentences with the breaker and collect the outputs that the critic classifies as ungrammatical. (4) Obtain parallel data by combining outputs of (1) and (3). LM-Critic uses local neighborhood information and perplexity (PPL) to build the critic and uses synthetic data to initialize the fixer. However, the synthetic data relies on the edit pairs provided by Awasthi et al. (2019), which are extracted from labeled sentences. Moreover, a significant performance gap remains between LM-critic and supervised systems (See Section 4).\nIn this paper, we propose a novel method for generating synthetic data and building a critic, with the aim of building an unsupervised GEC system that can rival supervised systems. By examining\nthe grammatical errors in labeled data, we identified several language-independent error patterns. Using these patterns, we propose a synthetic data generation method based on a masked language model (MLM) to build a fixer. Subsequently, we use this fixer as a basis for building our critic. The critic is trained using grammaticality labels obtained from high-confidence fixer predictions. To address the data scarcity problem that arises from high-confidence filtering, we propose a maskingbased approach and a self-knowledge distillation method for data augmentation. The unsupervised GEC system is trained using the BIFI framework, with the fixer and the critic being refined repeatedly in iterations.\nWe evaluate the performance of our system on both English and Chinese GEC tasks. Specifically, we evaluate our system on the CoNLL-2014 (Ng et al., 2014) and BEA-2019 (Bryant et al., 2019) test sets for English GEC, and on the NLPCC-2018 (Zhao et al., 2018) test set for Chinese GEC. Our unsupervised system outperforms the prior unsupervised SOTA by 12.5 F0.5 and 13.8 F0.5 on the CoNLL-2014 and BEA-2019 test sets, respectively. Our unsupervised system also compares favorably with the best-performing supervised systems for both languages. Furthermore, when we further train our system with labeled data, we surpass the SOTA results on both CoNLL-2014 and NLPCC2018 test sets.\nThe contributions of our paper are as follows:\n\u2022 We introduce a novel method for unsupervised synthetic data generation, based on MLM and language-independent error patterns. Compared to existing approaches, our method generates more realistic synthetic data, and provides a better unsupervised fixer.\n\u2022 We propose a new method to build an unsupervised critic with high-confidence predictions from the fixer model. This approach enables the critic model to continually enhance its performance over iterations, demonstrating better performance than prior methods."
        },
        {
            "heading": "2 Related Work",
            "text": "Unsupervised grammatical error correction. Prior research (Alikaniotis and Raheja, 2019) builds an unsupervised GEC system by leveraging manually constructed confusion sets to provide possible corrections, and uses language models (LMs)\nto validate these corrections. Yasunaga et al. (2021) utilize the confusion sets and LM in a different way. Instead of constructing a GEC model directly, Yasunaga et al. (2021) use them to create a GED model. This GED model is then combined with the BIFI method to build an unsupervised GEC system. In contrast to these works, our method does not rely on any manually constructed confusion sets, making it easy to extend to low-resource languages. Synthetic data generation. Synthetic data generation for GEC commonly adopts two strategies: backtranslation-based corruption methods using labeled data (Kiyono et al., 2019; Stahlberg and Kumar, 2021; Xie et al., 2018), and error injection corruption methods via edit pairs or confusion sets extracted from labeled data (Awasthi et al., 2019; Lichtarge et al., 2019; Yuan and Felice, 2013). Methods that do not require labeled GEC data have been explored by Grundkiewicz et al. (2019) and Sun et al. (2022). The former utilizes spellcheckerbased confusion sets to generate erroneous sentences, while the latter applies machine translation pairs and a pre-trained cross-lingual language model (XLM) for sentence corruption. Our method avoids external dependencies, such as confusion sets, spellcheckers, or translation pairs. Text evaluation. Prior work in GEC (Bryant et al., 2019; Dahlmeier and Ng, 2012; Niu and Penn, 2020) assesses sentence grammaticality through reference text or syntactic information, such as partof-speech tags. Yasunaga et al. (2021) mitigate this reliance with an LM-based method, yet it still needs pre-defined confusion sets. Our method constructs a critic using high-confidence predictions from the fixer model, thereby completely eliminating the need for external information."
        },
        {
            "heading": "3 Method",
            "text": "Figure 1 illustrates our method to build an unsupervised GEC system. It contains two key components: initial fixer2 construction (\u00a73.2) and the critic construction (\u00a73.3)."
        },
        {
            "heading": "3.1 Problem Setup",
            "text": "Grammatical error correction aims to correct an ungrammatical sentence x(i) into its grammatical version y(i) while preserving the original semantics. In the supervised setting with annotated data available, the GEC model leverages labeled sentence pairs Dl = {(x(i), y(i))} to learn a mapping from x to y. However, in the unsupervised setting, the GEC model must infer this mapping from a monolingual corpus Dm = {x(i)}. The BIFI framework offers a mechanism to extract realistic parallel data from unlabeled sentences using a fixer f and a critic c. The fixer maps x to y, and the critic evaluates the grammaticality of a given sentence. Our goal is to construct a good initial fixer f0 (\u00a73.2) and critic (\u00a73.3) through unsupervised methods and utilize them to develop the final fixer fn (\u00a73.4)."
        },
        {
            "heading": "3.2 Training an Initial Fixer",
            "text": "The BIFI framework relies on a good initial fixer f0. Intuitively, f0 could be obtained by training a model with synthetic data generated via unsupervised approaches. However, how to generate realistic synthetic data without reliance on supervised information (e.g., edit pairs) remains an open problem. To tackle this problem, we analyze the parallel data in English and Chinese to identify some language-independent error patterns (\u00a73.2.1). Leveraging these patterns, we propose an unsupervised synthetic data generation method (\u00a73.2.2)."
        },
        {
            "heading": "3.2.1 Exploiting Error Patterns",
            "text": "We carry out analysis on the GEC validation set and categorize the errors into three categories: insertion errors, deletion errors, and replacement errors. Inspired by context-free spell-checkers, we plot the edit distance distribution between erroneous source tokens and their corresponding target tokens for replacement errors. For both deletion and insertion errors, we plot the frequency distribution of each erroneous token of the vocabulary.\nAs depicted in Figure 2, it is evident that the edit distance between an erroneous token and its\n2To differentiate between the fixer obtained via synthetic data and the fixer obtained via paired data through BIFI, we name the former fixer as the initial fixer.\ntarget token is typically small for both English and Chinese replacement errors. In either language, the majority of the edit distances are confined by the typical length of a \u201cword\u201d. In Figure 3, we can see that the vast majority of incorrect tokens resulting from insertion and deletion errors are found within the top 5% of the vocabulary. This leads to the conclusion that these errors are commonly associated with high-frequency tokens. Based on these observations, we define two language-independent error patterns: Replacement errors. The edit distance between an erroneous token and its corresponding target token is typically small. Insertion and deletion errors. The erroneous token usually has a high frequency in the vocabulary.\nLeveraging these two patterns, we outline our unsupervised synthetic data generation approach in \u00a73.2.2."
        },
        {
            "heading": "3.2.2 Unsupervised Synthetic Data Generation",
            "text": "We synthesize erroneous sentences from a clean corpus using the following steps: for each sentence x(i) from the seed corpus Dseedm , we first sample the error count per sentence from a pre-defined dis-\ntribution (Awasthi et al., 2019). We introduce each error by performing one of these three operations: (1) delete a token wv \u2208 x(i) with probability pdel; (2) insert a token wv at a random position with probability pins; (3) replace a token wj \u2208 x(i) with wr by probability prep.3\nSpecifically, to generate the replacement token wr, we replace a randomly selected token wj \u2208 x(i) with the mask token [MASK] and utilize MLM to predict a set of candidate tokens at the masked position based on its surrounding context. In this work, we choose RoBERTa as the MLM in our implementation. As described in Section 3.2.1, only candidates with a low edit distance from wj are appropriate replacements. Therefore, we eliminate candidate tokens that have an edit distance exceeding a certain threshold. Finally, we sample wr from the remaining candidates using a pre-defined distribution solely based on the edit distance.\nTo circumvent the problem of consistently sampling the same high-frequency tokens for insertion and deletion errors, we design a smoothing function to smooth the frequency of tokens in the vocabulary. This process is detailed in Algorithm 1. In Algorithm 1, LISTID represents a list of breakpoints (idi), which are positive integers in ascending order used for comparing against the rank of a token. Note that the tokens of the vocabulary are organized in descending order of frequency, where a token with a smaller rank occurs more frequently. This design ensures that high-frequency tokens in a collection possess an equal chance of being sampled, while maintaining a higher frequency than the less frequent tokens. We diverge from sampling based on the raw frequency of tokens in the vocabulary, opting to sample according to the smoothed frequency fsmooth.\nAlgorithm 1: Smoothing Function Input: LISTID = [id0, id1, ... , idn]\nwv: a token in the vocabulary Output: Smoothed probability fsmooth of wv\n1: Find the rank k for wv in the vocabulary 2: Find the smallest i such that k \u2264 idi 3: if i = 0 then 4: fsmooth = 1/id0 5: else 6: fsmooth = 1/(idi \u2212 idi\u22121) 7: end if\n3The sum of pdel, pins, and prep equals to one"
        },
        {
            "heading": "3.3 Training a Critic",
            "text": "LM-Critic integrates word-level perturbations with sentence perplexity to define the critic. However, the efficacy of word-level perturbations relies on pre-defined confusion sets. To circumvent this reliance, an intuitive approach is to extract the GED pseudo-labels from the existing fixer and then train a binary classifier from such pseudo-labels as the critic. Specifically, we begin by randomly choosing a subset D\u2032m from Dm. For each sentence x(i) \u2208 D\u2032m, we use the fixer to make corrections and obtain the output y\u0302(i). If y\u0302(i) is different from x(i), then we assign a pseudo-label z(i) = 0, meaning that x(i) is \u201cungrammatical\u201d. Otherwise, we assign z(i) = 1, meaning that x(i) is \u201cgrammatical\u201d.\nSince the initial fixer is far from optimal, the pseudo-labels assigned by the initial fixer may have low precision. To address this problem, we analyze the relation between the confidence of y\u0302(i) and the precision of z(i). In Figure 4, we observe that highconfidence predictions (i.e., y\u0302(i) predicted with a high probability) are associated with more accurate grammaticality labels. Therefore, we propose to select a highly confident subset Dsub from D\u2032m such that for every x(i) \u2208 Dsub, the fixer predicts y\u0302(i) with probability greater than 0.9.\nIt is worth noting that when the critic is trained on fixer predictions, it may unintentionally cause over-fitting to the fixer, which undermines the critic\u2019s ability to enhance the fixer further through iterations. Xie et al. (2020) has demonstrated the importance of introducing noise throughout the self-training process. Accordingly, we propose a masking-based data augmentation approach when building the critic. Specifically, for each sentence x(i) \u2208 Dsub, we generate an augmented sentence x (i) masked by randomly replacing p% tokens with the [MASK] token, and minimize the loss function Lmasked with respect to the critic\u2019s model parame-\nters \u03b8cr:\nLmasked = \u2212 1 |Dsub| \u2211\nx(i)\u2208Dsub\n\u2211 c\u2208{0,1} 1{z(i) = c}\u00b7\n(logP (c|x(i); \u03b8cr) + logP (c|x(i)masked; \u03b8cr)) (1)\nAnother issue of selecting high-confidence pseudo-labels is data scarcity. With the initial fixer, only 20% of the sentences from D\u2032m are selected. To mitigate this issue, we utilize a self-knowledge distillation (SKD) technique to gather additional training data and enhance the model\u2019s generalizability. Specifically, for each x(i) \u2208 D\u2032m, we follow the method used by (Xie et al., 2016; Meng et al., 2020) to construct soft pseudo-labels z\u0303(i)c 4:\nz\u0303(i)c = [P (c|x(i); \u03b8\u2032cr)]2/fc\u2211\nc\u2032\u2208{0,1}{[(P (c\u2032|x(i); \u03b8\u2032cr)]2/fc\u2032} (2)\nwhere fc = \u2211\nx(i)\u2208Dm\u2032 P (c|x(i); \u03b8\u2032cr) is the sum\nover soft frequencies for class c, and \u03b8\u2032cr is the critic\u2019s model parameters in the previous epoch. In the first epoch, \u03b8\u2032cr represents the critic\u2019s model parameters obtained by minimizing (1). Once the soft pseudo-labels are obtained, we train a new critic model by minimizing the following loss function:\nLcritic = Lmasked + Lskd where\nLskd = \u2212 1 |Dm\u2032 | \u2211\nx(i)\u2208Dm\u2032\n\u2211 c\u2208{0,1} z\u0303(i)c \u00b7\n(logP (c|x(i); \u03b8cr) + logP (c|x(i)masked; \u03b8cr))\n(3)\nAlgorithm 2: Break-It-Fix-It (BIFI) Input: Fixer f , critic c, grammatical sentences\nDgm, and ungrammatical sentences D ug m\nOutput: (erroneous, corrected) sentence pairs. 1: Correct Dugm using the fixer f and retain\noutput deemed grammatical by the critic c. 2: Train a breaker (error generator) on the\nresulting paired data. 3: Corrupt Dgm using the breaker and retain\noutput deemed ungrammatical by the critic c. 4: Combine the parallel data obtained in Step 1\nand 3.\n4The intuition is to (1) strengthen predictions; (2) emphasize data with high confidence; and (3) normalize the loss contribution of each centroid. Refer to (Xie et al., 2016) for details."
        },
        {
            "heading": "3.4 Iteratively Refining the Fixer and Critic",
            "text": "Algorithm 3 provides a high-level overview of our unsupervised grammatical error correction (GEC) system. We start by applying the unsupervised technique outlined in \u00a73.2.2 to corrupt Dseedm and yield synthetic data. This synthetic data is then employed to train an initial fixer, denoted by f0. In the next phase, we leverage f0 and Dm to derive pseudo labels and train a RoBERTa-based critic, as described in \u00a73.3. By utilizing this critic, we segregate Dm into grammatically correct (D g m) and incorrect (Dugm ) subsets. We then use the BIFI mechanism to generate realistic parallel data that is then employed to train a new fixer f1. We subsequently substitute f0 with f1 and repeat this procedure until the fixer achieves satisfactory performance.\nAlgorithm 3: Unsupervised GEC system Input: Monolingual corpora Dseedm , Dm\n1: Generate synthetic data using the method described in \u00a73.2.2 to corrupt Dseedm 2: Build f0 with synthetic data 3: for t = 1, 2, 3, . . . do 4: Extract GED pseudo-labels with ft\u22121 5: Train a critic (grammaticality classifier)\nby minimizing Eqn (3), then use it to split Dm into D g m and D ug m\n6: Use BIFI (Algorithm 2) to generate parallel data to train a new fixer ft. 7: end for"
        },
        {
            "heading": "4 Experiments on English GEC",
            "text": ""
        },
        {
            "heading": "4.1 Data and Model Configuration",
            "text": "Following prior work (Awasthi et al., 2019; Grundkiewicz et al., 2019), we use the combination of WMT NewsCrawl corpus (Bojar et al., 2018) and One-Billion-Word corpus (Chelba et al., 2014) as the seed monolingual corpus Dseedm . We generate 145 million synthetic sentence pairs with the method described in \u00a73.2.2. These synthetic pairs are used to fine-tune the Flan-T5-xxl model (Chung et al., 2022) to create the initial fixer f0.\nFollowing Yasunaga et al. (2021), our monolingual dataset Dm contains both grammatical and ungrammatical sentences. Concretely, we randomly select 10 million unlabeled sentences from various sources: Yahoo!Answer corpus (Zhang et al., 2015), Wikipedia history (Grundkiewicz and Junczys-Dowmunt, 2014), Lang8 (Mizumoto et al.,\n2011), NUCLE (Dahlmeier et al., 2013), and FCE (Yannakoudakis et al., 2011) datasets. Notably, as Wikipedia history, Lang8, NUCLE, and FCE are labeled datasets, we only take sentences from the source side of these datasets5. When constructing the critic, we use the Lang8 dataset as D\u2032m and choose RoBERTa-base as our classifier model.\nWe evaluate the performance of the English GEC system on the CoNLL-2014 and BEA-2019 test sets with the MaxMatch scorer (Dahlmeier and Ng, 2012) and the ERRANT scorer (Bryant et al., 2019), respectively. Following Cao et al. (2021), we use a one-tailed sign test with bootstrap resampling to carry out statistical significance tests. Refer to Appendix A.3 for the detailed experimental settings."
        },
        {
            "heading": "4.2 Main Results",
            "text": "Table 1 shows the performance of our system on both CoNLL-2014 and BEA-2019 test sets, including a comparison with existing supervised and unsupervised systems on the leaderboard. Our un-\n5The source side sentences are not annotated sentences, and they could be grammatical or ungrammatical.\nsupervised system achieves F0.5 score of 68.0 and 75.4 on the CoNLL-2014 and BEA-2019 test set, respectively, surpassing the current leading unsupervised system (Yasunaga et al., 2021) by 12.5 points on the CoNLL-2014 and 13.8 points on the BEA-2019 test set. Our system also exceeds the zero-shot performance of the GPT4 model by 8.1 points and 16.3 points on the CoNLL-2014 and BEA-2019 test set, respectively. Notably, our system compares favorably with the state-of-the-art supervised single system (Rothe et al., 2021), lagging behind by just 0.9 points on the CoNLL-2014 test set and 0.5 points on the BEA-2019 test set.\nTo enable a fair comparison with Yasunaga et al. (2021), we replace the Flan-T5-xxl model with the smaller BART-base (Lewis et al., 2020) model when building the fixer. With BART-base, our unsupervised system still outperforms Yasunaga et al. (2021), with a 5.2 F0.5 increase on CoNLL-2014 and a 2.2 F0.5 increase on BEA-2019. This highlights the superiority of our unsupervised training algorithm.\nWhen we further fine-tune our model using supervised data, the cLang8 (Rothe et al., 2021)\ndataset, our system achieves an F0.5 of 69.6 on CoNLL-2014 and 76.5 on BEA-2019. This sets a new SOTA result on the CoNLL-2014 test set."
        },
        {
            "heading": "4.3 Analysis",
            "text": "Synthetic data. We compare our synthetic data generation method with relevant methods proposed by (Grundkiewicz et al., 2019; Sun et al., 2022), and the method by Awasthi et al. (2019) which was used by (Yasunaga et al., 2021). To enable a fair comparison with the aforementioned data synthesis methods, we randomly select 8 million sentences from the UN Parallel Corpus v1.0 (Ziemski et al., 2016) and corrupt the same monolingual data using each method. We then train a Transformer-base model (Vaswani et al., 2017) on the resulting synthetic data.\nTable 2 shows that our method outperforms competing approaches. As demonstrated in Table 3, the erroneous sentences generated by the competing methods tend to either be grammatically correct or change the intended meaning of the original sentences. This observation explains the better performance of our method relative to these competing approaches. Notably, Sun et al. (2022) implements an approach similar to ours, which also generates replacement errors by inserting masks and then uses XLM to predict the mask. The difference is that they use translation pairs to guide the creation of candidate tokens, while our method relies on edit distance and frequency information.\nIn our ablation study (Table 2), we find that edit distance and frequency controls are crucial to generate realistic synthetic data, confirming the effectiveness of the error patterns reported in \u00a73.2.1. Critic\u2019s training methods. Following (Yasunaga et al., 2021), we randomly sample 600 grammatical sentences and 600 ungrammatical sentences from GEC validation sets and use the averaged F0.5 score over 5 runs to measure the performance of the critic. Specifically, to measure the performance across various domains, we assemble our\nGEC validation set from the BEA-2019 dev set, the CoNLL-2013 dev set (Ng et al., 2013), and the GMEG-wiki/Yahoo/FCE validation set (Napoles et al., 2019).\nWe analyze the performance of our critic and compare it to LM-Critic in Table 4. We conduct an ablation study using the following configurations: (1) without employing the self-knowledge distillation method (SKD); (2) without applying the data augmentation approach (DA); and (3) without utilizing the high-confidence subset Dsub (CF). Results indicate that all three methods are crucial in enhancing the critic\u2019s performance. Notably, our critic outperforms LM-Critic by a significant margin, exhibiting a 13.4 F0.5 increase in grammatical and a 14.1 F0.5 increase in ungrammatical sentences. Our statistical significance test shows that our critic significantly improves over LM-Critic, and our critic without each individual component (SKD, DA and CF) still significantly improves over LM-Critic.\nFixer\u2019s performance through iterations. In Figure 5, the performance of the fixer across BIFI\niterations is shown. It is observed that the fixer\u2019s improvement is stagnant in the absence of the highconfidence subset (CF). Additionally, the fixer\u2019s improvement is considerably smaller when data augmentation (DA) or self-knowledge distillation (SKD) is excluded. Moreover, similar to LM-critic, the fixer\u2019s improvement comes to a halt after the first iteration without updating the critic. This demonstrates the significance of updating both the critic and the fixer throughout the process.\nCritic\u2019s performance through iterations. In Figure 6, we observe a consistent improvement in the performance of the critic throughout the iterations. This indicates a mutually beneficial learning process between the critic and the fixer: the critic improves the fixer, which in turn refines the critic even further. The plot on the right shows a correlation between pseudo-label precision and fixer iteration. This suggests that the fixer enhances the critic by providing more accurate GED pseudo-labels.\nExamples. In Table 5, we provide qualitative examples to compare the sentences generated by our system with those of GPT4 and LM-Critic. We find that both GPT4 and LM-Critic tend to make unnecessary edits, while our system does\nnot. The advantage of our system over LM-Critic could be attributed to two components: a better initial fixer which corrects more errors, and a better critic which assesses sentence grammaticality more precisely, as illustrated in Table 2 and Table 4."
        },
        {
            "heading": "5 Experiments on Chinese GEC",
            "text": ""
        },
        {
            "heading": "5.1 Data and Model Configuration",
            "text": "We generate 10 million synthetic sentence pairs using 10 million monolingual sentences crawled from the Toutiao website6. We train the Chinese BART-large model (Shao et al., 2021) on this data to create the initial fixer f0. To build the monolingual dataset Dm, we randomly select 4 million sentences from the CCMatrix corpus (Schwenk et al., 2021), Chinese Lang8 (Zhao et al., 2018), and HSK (Zhang, 2009). For both Lang8 and HSK datasets, we only take the sentences from the source side. When creating the critic, we use the HSK dataset as D\u2032m and use RoBERTa-wwm-ext (Cui et al., 2020) as our classifier model.\nWe evaluate the performance of our Chinese GEC system on the NLPCC-2018 test set with the MaxMatch scorer. Following Cao et al. (2021), we use the one-tailed sign test with bootstrap resampling to carry out statistical significance tests."
        },
        {
            "heading": "5.2 Results",
            "text": "Since no unsupervised results are available for Chinese GEC, we compare our model with existing supervised models on the NLPCC-2018 test set. Ta-\n6https://www.toutiao.com/\nble 6 shows that our model achieves 44.7 F0.5 score, surpassing Wu and Wu (2022) and Sun et al. (2022). It is only 0.6 points below the best-performing supervised single system. When we further finetune our unsupervised GEC system with labeled data, the combination of the Chinese Lang8 dataset, and the HSK dataset, our system achieves 47.8 F0.5 score, setting a new SOTA on NLPCC-2018. It demonstrates that our unsupervised model can serve as a strong initial checkpoint for supervised training."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we present innovative unsupervised techniques to produce synthetic parallel data and train a critic to evaluate the grammaticality of sentences. By combining our methods with BIFI, we develop an unsupervised GEC system that achieves results comparable to models utilizing substantial labeled data. The core idea is to employ languageindependent erroneous models to construct realistic synthetic data, and then create an unsupervised critic utilizing high-confidence predictions from the fixer model. Our system does not require any manually defined or extracted confusion sets, making it an ideal solution for developing GEC models for low-resource languages."
        },
        {
            "heading": "7 Limitations",
            "text": "We identified and utilized error patterns in both English and Chinese labeled corpora. While we believe such patterns are language-agnostic, we have not explored their application to other lowresource languages. Future research may delve\nfurther into this area. Additionally, we trained our models using extensive GPU resources, up to 32 A100 GPUs, though similar results can be achieved with just 8 V100 GPUs."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the anonymous reviewers for their helpful comments. This research is supported by a research grant from TikTok (WBS No. A8000972-00-00). The computational work for this article was partially performed on resources of the National Supercomputing Centre, Singapore (https://www.nscc.sg)."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Details on Exploiting Error Patterns Validation set selection. We carry out error pattern analysis on the validation set. Specifically, we follow previous work (Cao et al., 2023; Wu and Wu, 2022) to use BEA-2019 dev set (Bryant et al., 2019) and randomly sample 5,000 sentences from the NLPCC-2018 training set (Zhao et al., 2018) as the validation set for English and Chinese, respectively. Vocabulary creation. We derive the vocabulary from the C4 corpus (Raffel et al., 2020) and the\nUN Parallel Corpus v1.0 (Ziemski et al., 2016) for English and Chinese, respectively. Error type creation. We use the ERRANT toolkit7 to extract edits. Specifically, we use the \u2018all-split\u2019 configuration, which merges nothing, when extracting edit pairs from the labeled data. In this way, both the target side and the source side of an edit pair contain at most one token. If the source side of an edit pair is empty, the edit is categorized as an insertion error. If the target side of an edit pair is empty, the edit is categorized as a deletion error. For the rest of the cases, the edit is categorized as a replacement error. Complete figures. We show the insertion and deletion error pattern for English in Figure 7. The insertion and deletion error pattern for Chinese is shown in Figure 8. The replacement error pattern for English is shown in Figure 9. The replacement error pattern for Chinese is shown in Figure 10.\nA.2 Extracting GED Pseudo-Labels from the Fixer\nThe complete correlation between the probability of producing y\u0302(i) and precision of z(i) is shown in Figure 11.\nA.3 Detailed Experimental Settings\nImplementation details and training configuration.\nWe build our fixer using both the fairseq8 and transformers9 toolkit. Specifically, since the FlanT5-xxl model has around 11B parameters, we use the transformers toolkit with DeepSpeed10 ZeROOffload to build the fixer for English and use the fairseq toolkit to build the rest of the components. For English GEC, we use 32 NVIDIA A100 GPUs. For Chinese GEC, we use 8 NVIDIA A100 GPUs. The experiments took 14 days for English and 2 days in total for Chinese. We use the default training configuration under different toolkits unless otherwise stated. The detailed training configurations for English and Chinese are shown in Table 8 and Table 9, respectively. The best checkpoint is selected based on the performance on the validation set. Specifically, when building the fixer, we follow Yasunaga and Liang (2021) to randomly sample 5,000 sentences from the obtained training\n7https://github.com/chrisjbryant/errant 8https://github.com/facebookresearch/fairseq 9https://github.com/huggingface/transformers\n10https://github.com/microsoft/DeepSpeed\nsentence pairs as the validation data for both English and Chinese. When building the critic, we follow the approach used by Yasunaga et al. (2021) to randomly select 600 grammatical sentences and 600 ungrammatical sentences from the BEA-2019 dev set and Chinese Lang8 dataset as the validation set for English and Chinese, respectively. Hyper-parameter settings. We tune two hyperparameters in our system, the edit distance threshold, as mentioned in \u00a73.2.2, and the masking percentage, denoted as p%, which is outlined in \u00a73.3. We select the edit distance threshold from {1, 2, 3, 4, 5} for English GEC and select the the edit distance threshold from {0, 1, 2} for Chinese. For both English and Chinese p is selected from {5, 10, 15}. For English, the edit distance threshold 2 and p equals 5% give the best performance on the validation set. For Chinese, the edit distance threshold 1 and p% equals 10% give the best performance on the validation set. Parameters for synthetic data generation. Table 10 shows the parameter values used when generating the synthetic data. Note that these values are set to mimic the error distribution in real erroneous corpora.\nA.4 Experiments on German and Russian We use German (Falko-MERLIN dataset) and Russian (RULEC-GEC dataset) to demonstrate our method\u2019s performance in additional languages.\nFor both languages, we use mT5-xxl instead of Flan-T5-xxl as the base model and generate 10 million synthetic sentence pairs by corrupting the sentences from UN-Corpus v1.0. Following the setup in Section 4.1 and Section 5.1, we randomly collect 10 million sentences from the CCMatrix (Schwenk et al., 2021) corpus, Falko-MERLIN (Boyd et al., 2014) dataset, and cLang8(Rothe et al., 2021) dataset for German. For both FalkoMERLIN dataset and cLang8 dataset, we take the sentences from the source side (not annotated sentences), which could be grammatical or ungrammatical. We randomly collect 10 million sentences from the CCMatrix (Schwenk et al., 2021) corpus, RULEC-GEC (Rozovskaya and Roth, 2019) dataset, and cLang8 (Rothe et al., 2021) dataset for Russian. For both RULEC-GEC dataset and cLang8 dataset, we also take the sentences from the source side. The results are shown in the Table 7. Note that no unsupervised baselines exist in German and Russian GEC."
        }
    ],
    "title": "Unsupervised Grammatical Error Correction Rivaling Supervised Methods",
    "year": 2023
}