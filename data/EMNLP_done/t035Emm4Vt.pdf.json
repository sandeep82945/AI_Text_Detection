{
    "abstractText": "In recent years, we witness the explosion of false and unconfirmed information (i.e., rumors) that went viral on social media and shocked the public. Rumors can trigger versatile, mostly controversial stance expressions among social media users. Rumor verification and stance detection are different yet relevant tasks. Fake news debunking primarily focuses on determining the truthfulness of news articles, which oversimplifies the issue as fake news often combines elements of both truth and falsehood. Thus, it becomes crucial to identify specific instances of misinformation within the articles. In this research, we investigate a novel task in the field of fake news debunking, which involves detecting sentence-level misinformation. One of the major challenges in this task is the absence of a training dataset with sentence-level annotations regarding veracity. Inspired by the Multiple Instance Learning (MIL) approach, we propose a model called Weakly Supervised Detection of Misinforming Sentences (WSDMS). This model only requires bag-level labels for training but is capable of inferring both sentence-level misinformation and article-level veracity, aided by relevant social media conversations that are attentively contextualized with news sentences. We evaluate WSDMS on three real-world benchmarks and demonstrate that it outperforms existing stateof-the-art baselines in debunking fake news at both the sentence and article levels.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ruichao Yang"
        },
        {
            "affiliations": [],
            "name": "Wei Gao"
        },
        {
            "affiliations": [],
            "name": "Jing Ma"
        },
        {
            "affiliations": [],
            "name": "Hongzhan Lin"
        },
        {
            "affiliations": [],
            "name": "Zhiwei Yang"
        }
    ],
    "id": "SP:cb558f1a82c3f6f9c4241c49112487f5d41190cb",
    "references": [
        {
            "authors": [
                "Stefanos Angelidis",
                "Mirella Lapata."
            ],
            "title": "Multiple instance learning networks for fine-grained sentiment analysis",
            "venue": "Transactions of the Association for Computational Linguistics, 6:17\u201331.",
            "year": 2018
        },
        {
            "authors": [
                "Lucas Azevedo",
                "Mathieu d\u2019Aquin",
                "Brian Davis",
                "Manel Zarrouk"
            ],
            "title": "Lux (linguistic aspects under examination): Discourse analysis for automatic fake news classification",
            "venue": "In Findings of the Association for Computational Linguistics: ACL-IJCNLP",
            "year": 2021
        },
        {
            "authors": [
                "Tian Bian",
                "Xi Xiao",
                "Tingyang Xu",
                "Peilin Zhao",
                "Wenbing Huang",
                "Yu Rong",
                "Junzhou Huang."
            ],
            "title": "Rumor detection on social media with bi-directional graph convolutional networks",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, vol-",
            "year": 2020
        },
        {
            "authors": [
                "Carlos Castillo",
                "Marcelo Mendoza",
                "Barbara Poblete."
            ],
            "title": "Information credibility on twitter",
            "venue": "Proceedings of the 20th international conference on World wide web, pages 675\u2013684.",
            "year": 2011
        },
        {
            "authors": [
                "Yixuan Chen",
                "Dongsheng Li",
                "Peng Zhang",
                "Jie Sui",
                "Qin Lv",
                "Lu Tun",
                "Li Shang."
            ],
            "title": "Cross-modal ambiguity learning for multimodal fake news detection",
            "venue": "Proceedings of the ACM Web Conference 2022, pages 2897\u20132905.",
            "year": 2022
        },
        {
            "authors": [
                "Zheqian Chen",
                "Ben Gao",
                "Huimin Zhang",
                "Zhou Zhao",
                "Haifeng Liu",
                "Deng Cai."
            ],
            "title": "User personalized satisfaction prediction via multiple instance deep learning",
            "venue": "Proceedings of the 26th International Conference on World Wide Web, pages 907\u2013915.",
            "year": 2017
        },
        {
            "authors": [
                "Kevin Clark",
                "Urvashi Khandelwal",
                "Omer Levy",
                "Christopher D Manning."
            ],
            "title": "What does bert look at? an analysis of bert\u2019s attention",
            "venue": "ACL 2019, page 276.",
            "year": 2019
        },
        {
            "authors": [
                "Ronan Collobert",
                "Jason Weston",
                "L\u00e9on Bottou",
                "Michael Karlen",
                "Koray Kavukcuoglu",
                "Pavel Kuksa."
            ],
            "title": "Natural language processing (almost) from scratch",
            "venue": "Journal of machine learning research, 12(ARTICLE):2493\u20132537.",
            "year": 2011
        },
        {
            "authors": [
                "Zhuyun Dai",
                "Chenyan Xiong",
                "Jamie Callan",
                "Zhiyuan Liu"
            ],
            "title": "Convolutional neural networks for soft-matching n-grams in ad-hoc search",
            "year": 2018
        },
        {
            "authors": [
                "Thomas G Dietterich",
                "Richard H Lathrop",
                "Tom\u00e1s Lozano-P\u00e9rez."
            ],
            "title": "Solving the multiple instance problem with axis-parallel rectangles",
            "venue": "Artificial intelligence, 89(1-2):31\u201371.",
            "year": 1997
        },
        {
            "authors": [
                "Yaqian Dun",
                "Kefei Tu",
                "Chen Chen",
                "Chunyan Hou",
                "Xiaojie Yuan."
            ],
            "title": "Kan: Knowledge-aware attention network for fake news detection",
            "venue": "AAAI.",
            "year": 2021
        },
        {
            "authors": [
                "Song Feng",
                "Ritwik Banerjee",
                "Yejin Choi."
            ],
            "title": "Syntactic stylometry for deception detection",
            "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 171\u2013175.",
            "year": 2012
        },
        {
            "authors": [
                "James Foulds",
                "Eibe Frank."
            ],
            "title": "A review of multi-instance learning assumptions",
            "venue": "The knowledge engineering review, 25(1):1\u201325.",
            "year": 2010
        },
        {
            "authors": [
                "Yi Fung",
                "Christopher Thomas",
                "Revanth Gangi Reddy",
                "Sandeep Polisetty",
                "Heng Ji",
                "Shih-Fu Chang",
                "Kathleen McKeown",
                "Mohit Bansal",
                "Avirup Sil"
            ],
            "title": "Infosurgeon: Cross-media fine-grained information consistency checking for fake news",
            "year": 2021
        },
        {
            "authors": [
                "Max Glockner",
                "Yufang Hou",
                "Iryna Gurevych."
            ],
            "title": "Missing counter-evidence renders nlp fact-checking unrealistic for misinformation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5916\u20135936.",
            "year": 2022
        },
        {
            "authors": [
                "Quentin Grail",
                "Julien Perez",
                "Eric Gaussier."
            ],
            "title": "Globalizing bert-based transformer architectures for long document summarization",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main",
            "year": 2021
        },
        {
            "authors": [
                "Linmei Hu",
                "Tianchi Yang",
                "Luhao Zhang",
                "Wanjun Zhong",
                "Duyu Tang",
                "Chuan Shi",
                "Nan Duan",
                "Ming Zhou."
            ],
            "title": "Compare to the knowledge: Graph neural fake news detection with external knowledge",
            "venue": "Proceedings of the 59th Annual Meeting of",
            "year": 2021
        },
        {
            "authors": [
                "Yiqiao Jin",
                "Xiting Wang",
                "Ruichao Yang",
                "Yizhou Sun",
                "Wei Wang",
                "Hao Liao",
                "Xing Xie."
            ],
            "title": "Towards fine-grained reasoning for fake news detection",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 5746\u20135754.",
            "year": 2022
        },
        {
            "authors": [
                "Zhiwei Jin",
                "Juan Cao",
                "Yongdong Zhang",
                "Jianshe Zhou",
                "Qi Tian."
            ],
            "title": "Novel visual and statistical image features for microblogs news verification",
            "venue": "IEEE transactions on multimedia, 19(3):598\u2013608.",
            "year": 2016
        },
        {
            "authors": [
                "Ankur Joshi",
                "Saket Kale",
                "Satish Chandel",
                "D Kumar Pal."
            ],
            "title": "Likert scale: Explored and explained",
            "venue": "British journal of applied science & technology, 7(4):396.",
            "year": 2015
        },
        {
            "authors": [
                "Rohit Kumar Kaliyar",
                "Anurag Goswami",
                "Pratik Narang."
            ],
            "title": "Fakebert: Fake news detection in social media with a bert-based deep learning approach",
            "venue": "Multimedia Tools and Applications, 80(8):11765\u2013 11788.",
            "year": 2021
        },
        {
            "authors": [
                "S Sathiya Keerthi",
                "Chih-Jen Lin."
            ],
            "title": "Asymptotic behaviors of support vector machines with gaussian kernel",
            "venue": "Neural computation, 15(7):1667\u20131689.",
            "year": 2003
        },
        {
            "authors": [
                "Ling Min Serena Khoo",
                "Hai Leong Chieu",
                "Zhong Qian",
                "Jing Jiang."
            ],
            "title": "Interpretable rumor detection in microblogs by attending to user interactions",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 8783\u20138790.",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Sejeong Kwon",
                "Meeyoung Cha",
                "Kyomin Jung",
                "Wei Chen",
                "Yajun Wang."
            ],
            "title": "Prominent features of rumor propagation in online social media",
            "venue": "2013 IEEE 13th international conference on data mining, pages 1103\u20131108. IEEE.",
            "year": 2013
        },
        {
            "authors": [
                "Hongzhan Lin",
                "Jing Ma",
                "Mingfei Cheng",
                "Zhiwei Yang",
                "Liangliang Chen",
                "Guang Chen."
            ],
            "title": "Rumor detection on twitter with claim-guided hierarchical graph attention networks",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural",
            "year": 2021
        },
        {
            "authors": [
                "Yusan Lin",
                "Maryam Moosaei",
                "Hao Yang."
            ],
            "title": "Outfitnet: Fashion outfit recommendation with attentionbased multiple instance learning",
            "venue": "Proceedings of The Web Conference 2020, pages 77\u201387.",
            "year": 2020
        },
        {
            "authors": [
                "Zhenghao Liu",
                "Chenyan Xiong",
                "Maosong Sun",
                "Zhiyuan Liu."
            ],
            "title": "Fine-grained fact verification with kernel graph attention network",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7342\u20137351.",
            "year": 2020
        },
        {
            "authors": [
                "Yi-Ju Lu",
                "Cheng-Te Li."
            ],
            "title": "Gcan: Graphaware co-attention networks for explainable fake news detection on social media",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 505\u2013514.",
            "year": 2020
        },
        {
            "authors": [
                "Xiong Luo",
                "Xiaohui Chang",
                "Xiaojuan Ban."
            ],
            "title": "Regression and classification using extreme learning machine based on l1-norm and l2-norm",
            "venue": "Neurocomputing, 174(PA):179\u2013186.",
            "year": 2016
        },
        {
            "authors": [
                "Jing Ma",
                "Wei Gao",
                "Shafiq Joty",
                "Kam-Fai Wong."
            ],
            "title": "Sentence-level evidence embedding for claim verification with hierarchical attention networks",
            "venue": "Proceedings of the 57th Annual Meeting of the",
            "year": 2019
        },
        {
            "authors": [
                "Jing Ma",
                "Wei Gao",
                "Shafiq Joty",
                "Kam-Fai Wong."
            ],
            "title": "An attention-based rumor detection model with tree-structured recursive neural networks",
            "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 11(4):1\u201328.",
            "year": 2020
        },
        {
            "authors": [
                "Jing Ma",
                "Wei Gao",
                "Prasenjit Mitra",
                "Sejeong Kwon",
                "Bernard J Jansen",
                "Kam-Fai Wong",
                "Meeyoung Cha"
            ],
            "title": "Detecting rumors from microblogs with recurrent neural networks",
            "year": 2016
        },
        {
            "authors": [
                "Jing Ma",
                "Wei Gao",
                "Zhongyu Wei",
                "Yueming Lu",
                "Kam-Fai Wong."
            ],
            "title": "Detect rumors using time series of social context information on microblogging websites",
            "venue": "Proceedings of the 24th ACM international on conference on information",
            "year": 2015
        },
        {
            "authors": [
                "Jing Ma",
                "Wei Gao",
                "Kam-Fai Wong."
            ],
            "title": "Detect rumors in microblog posts using propagation structure via kernel learning",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2017
        },
        {
            "authors": [
                "Jing Ma",
                "Wei Gao",
                "Kam-Fai Wong."
            ],
            "title": "Rumor detection on twitter with tree-structured recursive neural networks",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1980\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Nikhil Mehta",
                "Mar\u00eda Leonor Pacheco",
                "Dan Goldwasser."
            ],
            "title": "Tackling fake news detection by continually improving social context representations using graph neural networks",
            "venue": "Proceedings of the 60th Annual Meeting of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Erxue Min",
                "Yu Rong",
                "Yatao Bian",
                "Tingyang Xu",
                "Peilin Zhao",
                "Junzhou Huang",
                "Sophia Ananiadou."
            ],
            "title": "Divide-and-conquer: Post-user interaction network for fake news detection on social media",
            "venue": "Proceedings of the ACM Web Conference 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Dat Quoc Nguyen",
                "Thanh Vu",
                "Anh-Tuan Nguyen."
            ],
            "title": "Bertweet: A pre-trained language model for english tweets",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,",
            "year": 2020
        },
        {
            "authors": [
                "Jeff Z Pan",
                "Siyana Pavlova",
                "Chenxi Li",
                "Ningxi Li",
                "Yangmei Li",
                "Jinshuo Liu."
            ],
            "title": "Content based fake news detection using knowledge graphs",
            "venue": "17th International Semantic Web Conference, ISWC 2018, pages 669\u2013683. Springer Verlag.",
            "year": 2018
        },
        {
            "authors": [
                "Nikolaos Pappas",
                "Andrei Popescu-Belis."
            ],
            "title": "Explicit document modeling through weighted multipleinstance learning",
            "venue": "Journal of Artificial Intelligence Research, 58:591\u2013626.",
            "year": 2017
        },
        {
            "authors": [
                "Sungkyu Park",
                "Jamie Yejean Park",
                "Hyojin Chin",
                "Jeonghan Kang",
                "Meeyoung Cha."
            ],
            "title": "An experimental study to understand user experience and perception bias occurred by fact-checking messages",
            "venue": "Proceedings of the Web Conference 2021, pages",
            "year": 2021
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning",
            "year": 2019
        },
        {
            "authors": [
                "Kashyap Popat",
                "Subhabrata Mukherjee",
                "Andrew Yates",
                "Gerhard Weikum."
            ],
            "title": "Declare: Debunking fake news and false claims using evidenceaware deep learning",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural",
            "year": 2018
        },
        {
            "authors": [
                "Martin Potthast",
                "Johannes Kiesel",
                "Kevin Reinartz",
                "Janek Bevendorff",
                "Benno Stein."
            ],
            "title": "A stylometric inquiry into hyperpartisan and fake news",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2018
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentencebert: Sentence embeddings using siamese bertnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on",
            "year": 2019
        },
        {
            "authors": [
                "Todd Rogers",
                "Richard Zeckhauser",
                "Francesca Gino",
                "Michael I Norton",
                "Maurice E Schweitzer."
            ],
            "title": "Artful paltering: The risks and rewards of using truthful statements to mislead others",
            "venue": "Journal of personality and social psychology, 112(3):456.",
            "year": 2017
        },
        {
            "authors": [
                "Natali Ruchansky",
                "Sungyong Seo",
                "Yan Liu."
            ],
            "title": "Csi: A hybrid deep model for fake news detection",
            "venue": "Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pages 797\u2013806.",
            "year": 2017
        },
        {
            "authors": [
                "Giovanni C Santia",
                "Jake Ryland Williams."
            ],
            "title": "Buzzface: A news veracity dataset with facebook user commentary and egos",
            "venue": "Twelfth international AAAI conference on web and social media.",
            "year": 2018
        },
        {
            "authors": [
                "Qiang Sheng",
                "Juan Cao",
                "Xueyao Zhang",
                "Rundong Li",
                "Danding Wang",
                "Yongchun Zhu."
            ],
            "title": "Zoom out and observe: News environment perception for fake news detection",
            "venue": "Proceedings of the 60th Annual Meeting of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Kai Shu",
                "Limeng Cui",
                "Suhang Wang",
                "Dongwon Lee",
                "Huan Liu."
            ],
            "title": "defend: Explainable fake news detection",
            "venue": "Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 395\u2013405.",
            "year": 2019
        },
        {
            "authors": [
                "Kai Shu",
                "Deepak Mahudeswaran",
                "Suhang Wang",
                "Dongwon Lee",
                "Huan Liu."
            ],
            "title": "Fakenewsnet: A data repository with news content, social context, and spatiotemporal information for studying fake news on social media",
            "venue": "Big data, 8(3):171\u2013188.",
            "year": 2020
        },
        {
            "authors": [
                "Kai Shu",
                "Amy Sliva",
                "Suhang Wang",
                "Jiliang Tang",
                "Huan Liu."
            ],
            "title": "Fake news detection on social media: A data mining perspective",
            "venue": "ACM SIGKDD explorations newsletter, 19(1):22\u201336.",
            "year": 2017
        },
        {
            "authors": [
                "Kai Shu",
                "Xinyi Zhou",
                "Suhang Wang",
                "Reza Zafarani",
                "Huan Liu."
            ],
            "title": "The role of user profiles for fake news detection",
            "venue": "Proceedings of the 2019 IEEE/ACM international conference on advances in social networks analysis and mining, pages 436\u2013439.",
            "year": 2019
        },
        {
            "authors": [
                "Amila Silva",
                "Ling Luo",
                "Shanika Karunasekera",
                "Christopher Leckie."
            ],
            "title": "Embracing domain differences in fake news: Cross-domain fake news detection using multi-modal data",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, vol-",
            "year": 2021
        },
        {
            "authors": [
                "Kirill Solovev",
                "Nicolas Pr\u00f6llochs."
            ],
            "title": "Moral emotions shape the virality of covid-19 misinformation on social media",
            "venue": "Proceedings of the ACM web conference 2022, pages 3706\u20133717.",
            "year": 2022
        },
        {
            "authors": [
                "Yun-Zhu Song",
                "Yi-Syuan Chen",
                "Yi-Ting Chang",
                "Shao-Yu Weng",
                "Hong-Han Shuai."
            ],
            "title": "Adversary-aware rumor detection",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1371\u20131382.",
            "year": 2021
        },
        {
            "authors": [
                "Edson C Tandoc Jr."
            ],
            "title": "Five ways buzzfeed is preserving (or transforming) the journalistic field",
            "venue": "Journalism, 19(2):200\u2013216.",
            "year": 2018
        },
        {
            "authors": [
                "Wei Wang",
                "Yue Ning",
                "Huzefa Rangwala",
                "Naren Ramakrishnan."
            ],
            "title": "A multiple instance learning framework for identifying key sentences and detecting events",
            "venue": "Proceedings of the 25th ACM International on Conference on Information and",
            "year": 2016
        },
        {
            "authors": [
                "Yaqing Wang",
                "Fenglong Ma",
                "Zhiwei Jin",
                "Ye Yuan",
                "Guangxu Xun",
                "Kishlay Jha",
                "Lu Su",
                "Jing Gao."
            ],
            "title": "Eann: Event adversarial neural networks for multi-modal fake news detection",
            "venue": "Proceedings of the 24th acm sigkdd international conference on",
            "year": 2018
        },
        {
            "authors": [
                "Yaqing Wang",
                "Fenglong Ma",
                "Haoyu Wang",
                "Kishlay Jha",
                "Jing Gao."
            ],
            "title": "Multimodal emergent fake news detection via meta neural process networks",
            "venue": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pages",
            "year": 2021
        },
        {
            "authors": [
                "Ke Wu",
                "Song Yang",
                "Kenny Q Zhu."
            ],
            "title": "False rumors detection on sina weibo by propagation structures",
            "venue": "2015 IEEE 31st international conference on data engineering, pages 651\u2013662. IEEE.",
            "year": 2015
        },
        {
            "authors": [
                "Liang Wu",
                "Fred Morstatter",
                "Kathleen M Carley",
                "Huan Liu."
            ],
            "title": "Misinformation in social media: definition, manipulation, and detection",
            "venue": "ACM SIGKDD Explorations Newsletter, 21(2):80\u201390.",
            "year": 2019
        },
        {
            "authors": [
                "Yang Wu",
                "Pengwei Zhan",
                "Yunjian Zhang",
                "Liming Wang",
                "Zhen Xu."
            ],
            "title": "Multimodal fusion with co-attention networks for fake news detection",
            "venue": "Findings of the association for computational linguistics: ACL-IJCNLP 2021, pages 2560\u20132569.",
            "year": 2021
        },
        {
            "authors": [
                "Chenyan Xiong",
                "Zhuyun Dai",
                "Jamie Callan",
                "Zhiyuan Liu",
                "Russell Power."
            ],
            "title": "End-to-end neural ad-hoc ranking with kernel pooling",
            "venue": "Proceedings of the 40th International ACM SIGIR conference on research and development in information retrieval,",
            "year": 2017
        },
        {
            "authors": [
                "Weizhi Xu",
                "Junfei Wu",
                "Qiang Liu",
                "Shu Wu",
                "Liang Wang."
            ],
            "title": "Evidence-aware fake news detection with graph neural networks",
            "venue": "Proceedings of the ACM Web Conference 2022, pages 2501\u20132510.",
            "year": 2022
        },
        {
            "authors": [
                "Ruichao Yang",
                "Jing Ma",
                "Hongzhan Lin",
                "Wei Gao."
            ],
            "title": "A weakly supervised propagation model for rumor verification and stance detection with multiple instance learning",
            "venue": "SIGIR \u201922: The 45th International ACM SIGIR Conference on Research",
            "year": 2022
        },
        {
            "authors": [
                "Ruichao Yang",
                "Xiting Wang",
                "Yiqiao Jin",
                "Chaozhuo Li",
                "Jianxun Lian",
                "Xing Xie."
            ],
            "title": "Reinforcement subgraph reasoning for fake news detection",
            "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages",
            "year": 2022
        },
        {
            "authors": [
                "Chunyuan Yuan",
                "Qianwen Ma",
                "Wei Zhou",
                "Jizhong Han",
                "Songlin Hu."
            ],
            "title": "Jointly embedding the local and global relations of heterogeneous graph for rumor detection",
            "venue": "2019 IEEE international conference on data mining (ICDM), pages 796\u2013805. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Jiaqi Zheng",
                "Xi Zhang",
                "Sanchuan Guo",
                "Quan Wang",
                "Wenyu Zang",
                "Yongdong Zhang."
            ],
            "title": "Mfan: Multi-modal feature-enhanced attention networks for rumor detection",
            "venue": "IJCAI.",
            "year": 2022
        },
        {
            "authors": [
                "Yongchun Zhu",
                "Qiang Sheng",
                "Juan Cao",
                "Shuokai Li",
                "Danding Wang",
                "Fuzhen Zhuang."
            ],
            "title": "Generalizing to the future: Mitigating entity bias in fake news detection",
            "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research",
            "year": 2022
        },
        {
            "authors": [
                "Arkaitz Zubiaga",
                "Alex Voss",
                "Rob Procter",
                "Maria Liakata",
                "Bo Wang",
                "Adam Tsakalidis."
            ],
            "title": "Towards realtime, country-level location classification of worldwide tweets",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, 29(9):2053\u20132066.",
            "year": 2017
        },
        {
            "authors": [
                "HAN (Ma"
            ],
            "title": "2019) aims similarly to DeClarE to the claim verification task and the provided evidence set is collected from multiple documents relevant to the claim",
            "year": 2019
        },
        {
            "authors": [
                "GCAN (Lu",
                "Li"
            ],
            "title": "2020) aims at debunking rumors only using the corresponding sequence of retweet users without text comments of a source tweet. The source tweet it accepts as a claim is also short",
            "year": 2020
        },
        {
            "authors": [
                "KAN (Dun"
            ],
            "title": "2021) detects fake news by identifying entity mentions in news contents and align them with the entities in the knowledge graph, which are used to learn news-entity co-attentions for better representing news",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Misinformation, such as fake news, poses tremendous risks and threats to contemporary society. The detection of fake news entails various technical challenges (Glockner et al., 2022), and one of them is accurately identifying false elements within news articles. This challenge arises due to the blending of authentic and fabricated content by creators\n\u2217Jing Ma is the corresponding author.\nof fake news, thereby complicating the determination of overall veracity (Solovev and Pr\u00f6llochs, 2022). Such instances have been prevalent during the Covid-19 pandemic1.\nFake news detection aims to determine the veracity of a given news article (Shu et al., 2017). Previous analysis has revealed that users often share comments and provide evidence about fake news on social media platforms (Zubiaga et al., 2017), which has led to a growing stream of research that leverages these social engagements, along with the content of news articles, to aid in fake news detection (Pan et al., 2018; Shu et al., 2019a; Min et al., 2022). This approach bears analogies to rumor detection, where the focus is on assessing as a specific statement rather than an entire news article (Wu et al., 2015; Ma et al., 2018; Bian et al., 2020; Lin et al., 2021; Song et al., 2021; Park et al., 2021; Zheng et al., 2022; Xu et al., 2022). Many studies in this domain aims to train supervised classifiers using features extracted from the social context and the content of the claim or article. However, the existing fake news detection models predominately focus on coarse-level classification of the entire article, which oversimplifies the problem. Misinformation can be strategically embedded within an article by manipulating portions of its content to enhance its credibility (Feng et al., 2012; Rogers et al., 2017; Zhu et al., 2022) Therefore, we target a fine-grained task that aims to identify sentences containing misinformation within an article, which can be jointly learned with article-level fake news detection.\nFigure 1 shows an illustrative example of a fake news article titled \u201cNASA will pay 100,000 USD\n1https://ahmedabadmirror.com/gujaratplans-to-give-world-a-wonder-drug-to-battlecorona/76017951.html. This article combines factual information about the historical use of cow urine in India\u2019s traditional medicine with false assertions that cow urine contains active ingredients capable of treating Covid-19 and has been used in hospitals in South Korea and China.\nto participants staying in bed for 60 days!\u201d, where the sentences in the article can be linked to a set of social conversations organized as propagation trees of posts. These sentences contain opinions and evidence that can aid in the veracity classification at the sentence and article levels, specifically in spotting misinformation sentences. For instance, sentence s3 can be debunked by referring to trees t1 and t3, as they provide evidence that contradicts the incorrect reward amount and duration mentioned in the sentence. This information helps in determining that the article is fake. Conversely, if we already know that the article is fake, we can infer that there must be misinforming sentences present within it.\nHowever, existing methods are not readily applicable for the identification of sentence-level misinformation due to two main reasons: 1) Obtaining veracity labels for sentences in an article is costly, as it requires annotators to exhaustively factcheck each sentence. 2) While rumor detection models can predict the label of a given claim, they often assume the availability of social conversations that correspond to the claim. However, it is difficult to establish a correspondence between social conversations and specific sentences within a news article. Inspired by multiple instance learning (MIL) (Foulds and Frank, 2010), we attempt to develop an approach for debunking fake news via weakly supervised detection of misinforming sentences (i.e., instances), called WSDMS2, only using available article-level veracity annotations (i.e., bag-level labels) and a handful of social conversations related to the news.\nTo gather the relevant social conversations associated with an article, we employ established\n2https://github.com/HKBUNLP/WSDMS-EMNLP2023\nmethods used in fake news detection that rely on social news engagement data collection (Shu et al., 2020), which provides the necessary conversation trees linked to the article in question. We devise a hierarchical embedding model to establish connections between each sentence in the article and its corresponding conversations, facilitating the identification of sentence-level misinformation. Standard MIL determines the bag-level label as positive if one or more instances within the bag are positive, and negative otherwise (Dietterich et al., 1997). To improve its tolerance on sentence-level prediction errors, we further develop a collective attention mechanism for a more accurate article veracity inference on top of the sentence-level predictions. The entire framework is trained end-to-end by optimizing a loss function that aims to alleviate prediction bias by considering both sentence- and articlelevel consistencies. Our approach ensures that the model captures the nuances of misinformation at both levels of granularity. Our contributions are summarized as follows:\n\u2022 Unlike existing fake news detection approaches, we introduce a new task that is focused on spotting misinforming sentences in news articles while simultaneously detecting article-level fake news.\n\u2022 We develop WSDMS, a MIL-based model, to contextualize news sentences aided by social conversations about the news and use only article veracity annotations to weakly supervise sentence representation and model training.\n\u2022 Our method achieves superior performance over state-of-the-art baselines on sentenceand article-level misinformation detection."
        },
        {
            "heading": "2 Related Work",
            "text": "Early studies on fake news detection have attempted to exploit various approaches to extract features from news content and social context information, including linguistic features (Potthast et al., 2018; Azevedo et al., 2021), visual clues (Jin et al., 2016), temporal traits (Kwon et al., 2013; Ma et al., 2015), user behaviors and profiles (Castillo et al., 2011; Ruchansky et al., 2017; Shu et al., 2019b). Subsequent studies have employed neural networks to automatically learn deep feature representations from similar sources of data (Ma et al., 2016; Popat et al., 2018; Ma et al., 2019; Nguyen et al., 2020; Kaliyar et al., 2021; Sheng et al., 2022). Furthermore, researchers have incorporated external knowledge sources(Pan et al., 2018; Dun et al., 2021; Hu et al., 2021) and combined multi-modal data (Wang et al., 2018, 2021; Fung et al., 2021; Wu et al., 2021; Silva et al., 2021; Chen et al., 2022) to enhance learning and improve fake news detection performance. Notably, social context information has played a crucial role in debunking fake news and rumors (Yuan et al., 2019; Khoo et al., 2020; Yang et al., 2022a; Ma et al., 2020; Mehta et al., 2022). The utilization of social context structures has spurred the development of Graph Neural Networks (GNNs) such as Kernel Graph Attention Networks (KGAT) (Liu et al., 2020) and Graphaware Co-Attention Networks (GCAN) (Lu and Li, 2020), which have demonstrated effectiveness in various fake news-related tasks. However, existing approaches (Shu et al., 2019a; Jin et al., 2022; Yang et al., 2022b) generally aim to detect article-level fake news, which lack the capability to tell which specific sentences contain misinformation.\nMIL is a weakly supervised approach that infers instance-level labels (e.g., sentence or pixel) when training data is annotated with bag-level labels (e.g., document or image) (Dietterich et al., 1997). Several MIL variants have been developed based on threshold-based MIL assumption (Foulds and Frank, 2010) and weighted collective MIL assumption (Pappas and Popescu-Belis, 2017), successfully applied in various downstream tasks such as recommendation systems (Lin et al., 2020) sentiment analysis (Angelidis and Lapata, 2018), keywords extraction (Wang et al., 2016), community question answering (Chen et al., 2017), and more recently joint detection of stances and rumors (Yang et al., 2022a). We adopt the weighted collective MIL assumption (Pappas and Popescu-\nBelis, 2017) to incorporate a weight function over the sentence space to calculate the article veracity probability. This assumption allows us to achieve a more robust prediction, as it avoids bias introduced by less important instances."
        },
        {
            "heading": "3 Problem Definition",
            "text": "We define a fake news dataset as a set of news articles {A}, where each article consists of a set of n sentences A = {si}ni=1 and si is the i-th sentence. For each article, we assume there is a set of m social conversation trees relevant to it denoted as T = {tj}mj=1, where tj is the j-th conversation tree containing posts (i.e., nodes) and message propagation paths (i.e., edges) which can provide the social context information for A. Our task is to predict the veracity of information at both sentence level and article level in a unified model:\n\u2022 Sentence-level Veracity Prediction aims to determine whether each si \u2208 A is a misinforming sentence or not given its relevant social context information T . That is to learn a function f(A) : s1, s2, . . . , sn \u2192 p1, p2, \u00b7 \u00b7 \u00b7 pn, where pi is the sentence-level prediction probability as to whether si is misinforming or not.\n\u2022 Article-level Veracity Prediction aims to classify the veracity of the article A on top of the sentence-level misinformation detection. That is to learn a function g(A) : p1, p2, \u00b7 \u00b7 \u00b7 pn \u2192 y\u0302, where y\u0302 denotes the prediction as to whether A is fake or true. Note that we have only article-level ground truth for model training."
        },
        {
            "heading": "4 WSDMS: Our MIL-based Model",
            "text": "Detecting more nuanced instances of misinformation at the sentence level solely based on article content is challenging (Feng et al., 2012). Previous studies have demonstrated that social media posts contain valuable opinions, conjectures, and evidence that can be leveraged to debunk claimlevel misinformation, such as rumors (Ma et al., 2017, 2018; Wu et al., 2019), where claims, typically presented as short sentences, share similar characteristics with sentences in news articles. We hypothesize that the detection of misinforming sentences can be done by incorporating relevant information from social context associated with the article. We try to establish connections between social conversations and specific news sentences in the article, enabling the contextualization of social\nwisdom to enrich the representation of sentences, in order to better capture the veracity of sentences.\nThe architecture of our MIL-based weakly supervised model WSDMS is illustrated in Figure 2. WSDMS consists of four closely coupled components: input embedding, sentence and conversation tree linking, misinforming sentence detection, and article veracity prediction. We describe them with detail in this section."
        },
        {
            "heading": "4.1 Input Embeddings",
            "text": "We represent the word sequence of each news sentence and social post using SBERT (Reimers and Gurevych, 2019) which maps the sequence into a fixed-size vector. Let a sequence S = w1w2 \u00b7 \u00b7 \u00b7w|S| consist of |S| tokens, where S could optionally denote a news title, a news sentence, or a post in conversation tree. Then, the SBERT embedding of S can be represented by S\u0304 = SBERT(w1, \u00b7 \u00b7 \u00b7 , w|S|). In the rest of the paper, given an article A, we will use additional notations T to denote the news title, p and q to denote posts in a conversation tree. And then T\u0304 , s\u0304i, p\u0304 and q\u0304 will denote the respective SBERT embeddings of T , si, p and q."
        },
        {
            "heading": "4.2 Linking Sentences to Conversation Trees",
            "text": "To mine the discernible relationship between sentences and social posts trees, we want to design a sentence-tree linking mechanism between the sentence set {si}ni=1 and post tree set {tj}mj=1, both associated with A. There are clearly different designs to create links across the elements between them, such as 1) using a fully connected graph that links any si and tj regardless of their similarity,\nfollowed by a model to fix the closeness of each connection; 2) creating a link according to the similarity between si and tj based on a threshold. Our preliminary experiments indicate that the different designs of interaction indeed influence the performance. Given that the number of sentences and trees associated with articles varies significantly, we opt for the threshold-based approach to avoid the overhead of computing on a fully connected graph. We begin with modeling posts interaction in each tree to learn its representation before linking the sentences and trees.\nPost Interaction Embedding: To represent a tree accurately, we use a generic kernel-based graph model KernelGAT (Liu et al., 2020) to measure the importance of each post in a tree by modeling the interactions between each post and its neighboring posts.\nWe first construct a translation matrix M to represent the similarity of each post with its neighbors, where each Mpq \u2208 M is the cosine similarity between post p and q:\nMpq = { p\u0304\u00b7q\u0304 |p\u0304||q\u0304| if q \u2208 N (p) 0 otherwise\n(1)\nwhere N (p) is the set of neighboring nodes of p. We then define a kernel function G\u20d7(Mp) to represent the features considering the interactions between p and its neighbors based on K Gaussian kernels (Keerthi and Lin, 2003), and this yields:\nG\u20d7(Mp) = {G1(Mp), \u00b7 \u00b7 \u00b7 ,GK(Mp)} (2)\nwhere Gk(Mp) = log \u2211\nq\u2208N (p)\nexp ( \u2212(Mpq \u2212 \u00b5k) 2\n2\u03c32k\n)\nand \u00b5k and \u03c3k are parameters in the k-th kernel to capture the node interactions at different levels (Xiong et al., 2017). Note that if \u03c3k \u2192 \u221e, the kernel function degenerates to the mean pooling.\nThen, we update the representation p\u0303 of each post p by considering all its neighbors with their identified importance, which is given as:\n\u03b3q = softmax ( W1 ( G\u20d7(Mp) ) + b1 ) [q]\np\u0303 = \u2211\nq\u2208N (p)\n\u03b3q \u00b7 q\u0303 (3)\nwhere \u03b3q is a scalar representing the post-level attention coefficient between p and its neighbor q, W1 and b1 are trainable parameters used to transform K kernels into a vector of all nodes in the tree, [q] takes the value corresponding to post q, and p\u0303 and q\u0303 are initialized respectively with the BERT-based post embeddings p\u0304 and q\u0304.\nLink Sentences and Trees. With the obtained interaction-enhanced post representations, we use a mean pooling function to represent a conversation tree tj , i.e., t\u0303j = mean( \u2211 p p\u0303) for all p \u2208 tj . For each pair of sentences and tree (si, tj) associated with an article, we then create a link between them if the cosine similarity of s\u0304i and t\u0303j is above a global threshold \u03c4 , where \u03c4 is determined according to the global range of similarity scores between sentences and trees by mapping \u03c4 to the median of the range of scores. We fix this setting empirically."
        },
        {
            "heading": "4.3 Detecting Misinforming Sentences",
            "text": "To spot misinforming sentences based on the graph with the sentence-tree links, we propose a graph attention model to detect whether a sentence si contains misinformation. Each sentence can be linked to multiple conversation trees and vice versa. In Figure 1, for example, two trees t1 and t3 are linked to s3, where t1 provides more specific evidence (e.g., the right reward amount and the number of experimental days) indicating that s3 is misinforming, while t3 just implies the sentence is not credible without providing specific clues. Hence, we design an attention mechanism to update the representation of each sentence by considering the importance of all its corresponding trees.\nMore specifically, let Ti denote the set of trees linked to si. We aggregate the representation of corresponding trees according to their attention weights, and then update the sentence representa-\ntion. This is achieved by:\n\u03b2i,j = exp(t\u0303j \u00b7 s\u0304i)\u2211\nt\u2032j\u2208Ti exp(t\u0303\u2032j \u00b7 s\u0304i)\ns\u0303i = \u2211 tj\u2208Ti \u03b2i,j \u00b7 t\u0303j \u2295 s\u0304i (4) where s\u0303i denotes the socially contextualized representation of si, \u03b2i,j is the importance of tj \u2208 Ti with respect to si, and \u2295 denotes concatenation operation.\nWe then use a fully-connected softmax layer to predict the probability of si containing misinformation based on its BERT-based embedding s\u0304i and socially contextualized embedding s\u0303i:\np\u0302i = softmax(W2s\u0303i +W3s\u0304i + b2) (5)\nwhere W2, W3 and b2 are trainable parameters and p\u0302i is the class probability distribution of si provided that the bag-level class labels are fake and real, based on the MIL (Foulds and Frank, 2010; Angelidis and Lapata, 2018)."
        },
        {
            "heading": "4.4 Inferring Article Veracity",
            "text": "We can simply predict an article as fake if there is at least one misinforming sentence is detected, which conforms to the original threshold-based MIL assumption. However, the assumption is overly strong because there can be inaccuracies in sentence-level prediction. Based on the weighted collective MIL assumptions (Foulds and Frank, 2010), we design a context-based attention mechanism to bridge the inconsistency between sentenceand article-level predictions.\nSpecifically, we first learn a global representation for the article utilizing a pre-trained transformer (Grail et al., 2021):\n[T\u0302 , s\u03021, \u00b7 \u00b7 \u00b7 , s\u0302n] = Trans ( [T\u0304 , s\u03031, \u00b7 \u00b7 \u00b7 , s\u0303n] ) (6)\nwhere T\u0304 is the initial SBERT embedding of the article title. We then adopt an attention mechanism to measure the importance of sentences w.r.t the article veracity prediction, which yields:\n\u03b1i = exp(s\u0302i \u00b7 T\u0302 )\u2211n i=1 exp(s\u0302i \u00b7 T\u0302 )\ny\u0302 = n\u2211\ni=1\n\u03b1i \u00b7 p\u0302i (7)\nwhere \u03b1i denotes the attention weight of s\u0302i relative to the title representation T\u0302 , and y\u0302 is the class probability distribution of A being fake or real."
        },
        {
            "heading": "4.5 Model Training",
            "text": "Intuitively, the more similar two sentences are, the more similar their corresponding predictions should be. We define the following loss function considering pairwise consistency between sentence representation and prediction, with only articlelevel ground truth:\nL(A) = \u03bb \u00b7 C(A) + (1\u2212 \u03bb) \u00b7 ||yA \u2212 y\u0302A||22 (8)\nwhere C(A) = n\u2211\ni=1 n\u2211 j=1 exp ( \u2212||s\u0302i \u2212 s\u0302j ||22 \u00b7 ||p\u0302i \u2212 p\u0302j ||22 ) Here C(.) \u2208 [0, 1] is the function measuring the consistency between pairwise sentence similarity (i.e., s\u0302i and s\u0302j) and the prediction (i.e., p\u0302i and p\u0302j), yA and y\u0302A denote respectively the ground-truth and predicted class probability distributions of A, ||.||22 is an efficient kernel based on the L2 norm (Luo et al., 2016) as a non-negative penalty function, and \u03bb is the trade-off coefficient."
        },
        {
            "heading": "5 Experiments and Results",
            "text": ""
        },
        {
            "heading": "5.1 Datasets and Setup",
            "text": "We employ two public real-world datasets PolitiFact and GossipCop (Shu et al., 2020) respectively related to politics and entertainment fake news, where relevant social conversations are collected from Twitter. We also construct an open-domain fake news dataset BuzzNews by extending BuzzFeed (Tandoc Jr, 2018), for which we gather social conversations of the articles via Twitter API3.\nWe recruit three annotators to label misinforming sentences of the articles in the test sets of the three datasets. We train the annotators by providing them with a unified set of annotation rules referring to the detailed guide from several fact-checking websites such as snopes.com and politifact.com, where specific rationales on how each claim was judged are provided. Then, we take a majority vote for determining the label of each sentence, and the inter-annotator agreement is 0.793. Table 1 shows the statistics of these three datasets.\nWe use precision (Pre), recall (Rec), F1, and accuracy (Acc) as evaluation metrics. All the baselines and our methods are implemented with PyTorch (Paszke et al., 2019) (see Appendix A.2 for implementation details)."
        },
        {
            "heading": "5.2 Article-level Fake News Detection",
            "text": "We compare the following models at the article level. Some original settings of baselines might not suit the data in this task, which have to be specifically customized (see Appendix A.1). 1) DeClarE (Popat et al., 2018): An evidence-aware network using news title to attend over words in relevant posts for verifying news claims. 2) HAN (Ma et al., 2019): A hierarchical attention network using the news title to attend over relevant posts as evidences. 3) dEFEND (Shu et al., 2019a): A sentence-post co-attention network for fake news detection. 4) BerTweet (Nguyen et al., 2020): A language model pre-trained on 850M tweets, which is applied here for article verification using article and relevant posts. 5) GCAN (Lu and Li, 2020): A graph-aware co-attention model trained on user profile and post propagation structure without using post content to verify the news given title. 6) Bi-GCN (Bian et al., 2020): A bi-directional graph convolutional network using news title and propagation structure of posts for verifying the news. 7) KAN (Dun et al., 2021): An attention network utilizing entities in article content and entity contexts for fake news detection. 8) SureFact (Yang et al., 2022b): A reinforcement subgraph reasoning method using the topic connection between article and relevant posts for fake news detection. 9) WSDMS: Our proposed weakly supervised method. 10) WSDMS-FC: A variant of our method that fully connects sentences and post trees. Table 2 presents the following observations:\n\u2022 In the first group of structured models, dEFEND performs the best. This is because DeClearE and HAN are designed to only use the external relevant context of a claim and BerTweet is trained to represent social posts. dEFEND leverages features extracted from both article content and external posts that are complementary.\n3https://developer.twitter.com/en/docs\n\u2022 In the second group of non-structured models, the graph-based models GCAN and Bi-GCN mainly rely on propagation structures of fake news and perform comparably with KAN using entities and their contexts extracted from the social media content, suggesting that social conversations embed a good amount of human wisdom useful for detecting fake news. SureFact performs best among all the baselines because it groups social posts into the topics discovered from article content, suggesting that creating a connection between them at the topic level is helpful.\n\u2022 WSDMS consistently defeats the best baseline SureFact on the three datasets, demonstrating that our explicit and fine-grained linking between sentence and social context is superior, and the sentence-level detection can help article veracity prediction. In addition, WSDMS does not sacrifice its performance compared to WSDMSFC that uses full connections between sentences and trees, while we find that WSDMS significantly reduces training time from 4.5 to 2 hours. This indicates our sentence-tree linking method is cost-effective."
        },
        {
            "heading": "5.3 Misinforming Sentence Detection",
            "text": "For misinforming sentence detection, the baselines are deployed by treating each sentence as a claim and the conversation trees linked to the sentence (see Section 4.2) as the source of evidence. SureFact is excluded as it cannot classify specific sentences. More details are in Appendix A.1.\nSince all baselines are supervised methods that need sentence labels for training, we split the three test sets with sentence-level annotation into train and test parts with a 70%-30% ratio. Due to the large number of sentences in the original test sets (6,300/6,480/2,480), we end up with three workable sentence-level training and test sets. We then\ntrain all models on the same training data. But this intentionally disadvantages our WSDMS since it can only use article labels. Therefore, we also present the performance of WSDMS (o) trained on the original training sets without sentence labels, which baselines cannot take advantage of. Table 3 conveys the following findings:\n\u2022 Similar to article-level prediction, dEFEND outperforms DeClarE and HAN because it effectively models the sentence and social context correlations via the co-attention mechanism. BERTweet is more advantageous at representing social media posts, demonstrating better performance at the sentence level.\n\u2022 Among the structured models, KAN performs best because it incorporates both content and propagation information and has a co-attention mechanism between sentence and entity contexts extracted from social conversations. This may enhance sentence representation better than Bi-GCN and GCAN that can only utilize propagation-based features.\n\u2022 Weakly supervised WSDMS performs better than DeClarE and comparably with HAN, which are fully supervised. This is because WSDMS considers the propagation structure while DeClarE and HAN can only leverage unstructured posts. The overall performance of WSDMS is clearly compromised due to weak supervision. However, when it is trained on the original datasets, WSDMS (o) can enjoy the large volume of article labels to beat all baselines that cannot be weakly supervised. To reach the same level of performance, the baselines may need tremendous sentence annotations which are infeasible to get. Again, it performs comparably well as WSDMSFC (o), implying that our sentence-tree linking\nreserves vital information for spotting misinforming sentences efficiently.\n\u2022 WSDMS effectively enhances sentence-level performance by utilizing publicly accessible article-level labels. To achieve comparable performance, baseline systems generally require massive fine-grained sentence-level annotations. Consequently, sentence-level prediction remains a pivotal contribution of our study."
        },
        {
            "heading": "5.4 Ablation Study",
            "text": "We ablate WSDMS based on the PolitiFact dataset by varying some component(s): 1) w/o \u03c4 : Fully connect sentences and trees by removing \u03c4 , i.e., WSDMS-FC. 2) w/o NLL: Replace the loss with an ordinary negative log-likelihood loss function. 3) w/o wc: Infer article veracity based on the original MIL assumption without weighted collective attention. 4) Title as sent: Treat the title as a common sentence. 5) w/o kernel: Reduce the kernelbased post interaction embedding to dot-product attention between sentence and conversation trees. 6) w/o tree: Remove conversation trees.\nFigure 3 shows that most of the ablations make the result worse. w/o tree implies that only using article content is insufficient for the task. w/o kernel supports that embedding post interactions with kernel can help post and tree representation. Experiment in the Appendix A.3 also echoes the advantages of the kernel. Title as sent means that the news title may attract the most attention from the trees, which can hurt the representation of other sentences, and should be specially treated. w/o wc indicates adopting weighted collective MIL is better. w/o NLL confirms that our designed loss is necessary and effective. Only w/o \u03c4 is marginally better due to fully connected sentences and trees, which is however more costly and less efficient."
        },
        {
            "heading": "5.5 Case Study",
            "text": "To gain a deeper insight, we visualize two news articles checked by PolitiFact in Figure 4 which are predicted as fake (left) and true (right) correctly by WSDMS. The spotted misinforming and true sentences are also shown. We observe that 1) WSDMS can associate a sentence with multiple trees using attention weights (arrow lines indicate highweight trees) to help determine its veracity. 2) The posts in the conversations provide useful clues for indicating how credible each sentence is by aggregating collective opinions of users in the trees; 3) The article-level veracity is not determined simply by whether there is a misinforming sentence detected, because the prediction might be inaccurate. For example, if s4 is incorrectly predicted as fake, the article will also be determined as fake under the standard MIL. Our approach increases the chance of correcting such an error by giving higher attention weights to other sentences, which may indicate that the article is overall more likely to be true. Thus, the attention weights of sentences can collectively aggregate sentence-level predictions to improve the final prediction."
        },
        {
            "heading": "5.6 User Study Experiment",
            "text": "We conduct a user study to evaluate the quality of the model output. We sample 120 articles from PolitiFact and present them in two forms: Baseline (article, posts) and WSDMS (article, misinforming\nsentences, trees). We then ask 6 users to label the articles and give their confidence in a 5-point Likert Scale (Joshi et al., 2015), and each person is given only one form to avoid cross influence.\nTable 4 shows that 1) users determine the articlelevel veracity more accurately with WSDMS; 2) users spent 70% less time identifying fake news; and 3) users show higher confidence with the results of WSDMS, suggesting that users tend to be more sure about their decision when specific misinforming sentences and relevant evidence are provided."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "We propose a MIL-based model called WSDMS to debunk fake news in a finer-grained manner via weakly supervised detection of misinforming sentences with only article veracity labels for model training. WSDMS uses the attention mechanism to associate news sentences with their relevant social news conversations to identify misinforming sentences and determine the article\u2019s veracity by aggregating sentence-level predictions. WSDMS outperforms a set of strong baselines at the article level and sentence level on three datasets.\nIn the future, we will incorporate more intersentence features, such as discourse relations, to detect composition-level misinformation.\nLimitations\nFake news is one type of misinformation, which also includes disinformation, rumors, and propaganda. WSDMS can be well-generalized to detect\nthese various forms of misinformation. Whereas, we simplify some techniques in this paper. For example, the representation of conversation trees can be learned by considering the direction of message propagation and combining top-down and bottomup propagation trees. In addition, it cannot deal with more complex situations, where multiple true sentences combined constitute some kind of logical falsehoods or inconsistencies. This can be strengthened by considering sentence-level relations such as discourse information in the model. Despite this limitation, WSDMS encounters no such situation in the three datasets used according to our observation. Nevertheless, this suggests that the existing fake news datasets and detection models lack consideration of discourse-level fakes or logically inconsistent compositions, which are presumably not uncommon in real-world fake news. Lastly, we only use social context data collected from Twitter, which might have platform bias. To mitigate the issue, we can introduce additional data from different social media platforms, such as BuzzFace (Santia and Williams, 2018) from Facebook."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was partially supported by Hong Kong RGC ECS (Ref. 22200722), National Natural Science Foundation of China Young Scientists Fund(No. 62206233) and HKBU One-off Tier 2 Start-up Grant (Ref. RCOFSGT2/20-21/SCI/004).\nEthics Statement\nData Privacy: Although the datasets used in our research are publicly accessible, the utilization of social media conversations for debunking fake news may raise concerns regarding user privacy. To address this issue, we took measures to anonymize all social media posts during the data processing and experiments, ensuring that user information remains invisible and unusable. Additionally, our proposed approach does not require access to any sensitive user information, therefore eliminating the risk of privacy infringement. The collection of social media conversations in the BuzzNews dataset was conducted in compliance with the privacy regulations set by the platform. Social Implications: The detection and debunking of fake news can carry significant social and political implications. One critical consideration is the potential impact on the reliability of the system and the possibility of misleading users by mislabeling\ninformation as misinformation or vice versa. In light of this concern, we have taken precautions to carefully assess the model we developed and restrict their distribution to the general public. We are committed to designing a responsible policy regarding the dissemination of codes and datasets within research community, and ensure that they are used responsibly in a manner that aligns with ethical standards and societal well-being."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Detailed Baseline Settings\nExisting fake news detection and rumor detection methods predominately focus on coarse-level classification on the entire article and claim, respectively, while our goals include identifying misinforming sentences within an article at a fine-grained level. When comparing with the baselines that are originally designed to either classify a news article or a claim, the required (and available) inputs may differ from our study. Therefore, we need to specifically customize the data inputs to make the baselines applicable to the article-level and sentence-level detection tasks while maintaining the implementation of baseline models intact. In this section, we will provide more details about baseline models and the information they used.\nA.1.1 Article-level Task 1) DeClarE (Popat et al., 2018) is designed to classify a claim with relevant news content obtained from external sources as evidence, such as web search results. The claim it used is short and there are many relevant articles providing evidence. In our fake news detection dataset, however, what is available includes a single long-form article which is the target to be checked, and the relevant social conversation trees providing external assistance. Since DeClarE can only accept short claims as input, we use the title of the news article as an input claim and the posts in conversations as evidence.\n2) HAN (Ma et al., 2019) aims similarly to DeClarE to the claim verification task and the provided evidence set is collected from multiple documents relevant to the claim. In our case, article text is the target to be verified, while HAN assumes a short claim as the target which cannot be fed into HAN directly. So, we use the news title as the input claim and posts in conversations as evidence.\n3) dEFEND (Shu et al., 2019a) is a fake news detection model using news article as the target of verification and the related user comments as evidence. This is mostly consistent with our setting. Thus, it does not require any special treatment.\n4) BerTweet (Nguyen et al., 2020) is a pretrained language model trained on large English posts corpus. It is designed to encode short text. To apply BerTweet for article-level verification, we use the posts in conversation trees to fine-tune the model, and then treat the news title as a claim to be verified because BerTweet cannot accept the\nlong-form article as input. 5) GCAN (Lu and Li, 2020) aims at debunking rumors only using the corresponding sequence of retweet users without text comments of a source tweet. The source tweet it accepts as a claim is also short. To apply it to our data, we use the news title as source tweet and the post user profiles and propagation structure without post content as evidence.\n6) Bi-GCN (Bian et al., 2020) utilizes bidirectional Graph Convolutional Network to accommodate top-down and bottom-up post propagation structure to detect rumors taking a short source post as input. Similarly, we use news title as a source post and post propagation structure as evidence.\n7) KAN (Dun et al., 2021) detects fake news by identifying entity mentions in news contents and align them with the entities in the knowledge graph, which are used to learn news-entity co-attentions for better representing news text. While there are news articles in our data, we have only related posts from social media but no knowledge graph. For this issue, we use the social conversions of the article as the source to extract entities as entity contexts of the entities in the article.\n8) SureFact (Yang et al., 2022b) groups related posts based on specific topics extracted from news content to implicitly connect news and social media content for fake news detection. It can be directly applied to our datasets.\nA.1.2 Sentence-level Task\nFor misinforming sentence detection, the baselines are deployed by treating a sentence in article as a claim or source post and the conversation trees linked to the sentence (see Section 4.2) as the source of evidence. In such a setting, most of the baselines can be applied to this sentence-level task in a more straightforward manner. See Table 5 for specific details.\nA.2 Implementation Details Our model parameters are updated by backpropagation (Collobert et al., 2011) with Adam (Kingma and Ba, 2015) optimizer. We set the maximum epoch to 100, the dimension of embeddings to 512 for sentences and posts, and empirically initialize the learning rate as 0.001, and the hyperparameter \u03bb is set to 0.5 which is validated on a small hold-out dataset.\nAs for Gaussian kernels in Equation 2, we set K = 10. Here one kernel with parameter \u00b5k = 1 and \u03c3k = 0.001 is designed for exact matching (Dai et al., 2018). The other kernels\u2019 parameter \u03c3k = 0.01, and their parameter \u00b5k is distributed within [-1, 1] evenly.\nThe training process is controlled to end when the loss value converges or the maximum epoch number is met.\nA.3 Experiment on Kernel Attention Concentration\nWe conduct an experiment to compute the entropy values of kernel attention weights used in WSDMS and compare it with dot-product attention used in GCAN, to reflect whether the learned attention weights are more focused or scattered. The lower the entropy, the more focused the attention mechanism (Clark et al., 2019). The entropy results are given in Table 6.\nWe find that kernel attention bears a smaller entropy than the dot-product attention. It suggests that kernel attention has a stronger ability to be focused on a few more vital posts. This is also the reason why we use kernel attention in our method."
        }
    ],
    "title": "WSDMS: Debunk Fake News via Weakly Supervised Detection of Misinforming Sentences with Contextualized Social Wisdom",
    "year": 2023
}