{
    "abstractText": "Harvesting question-answer (QA) pairs from customer service chatlog in the wild is an efficient way to enrich the knowledge base for customer service chatbots in the cold start or continuous integration scenarios. Prior work attempts to obtain 1-to-1 QA pairs from a growing customer service chatlog, which fails to integrate the incomplete utterances from the dialog context for composite QA retrieval. In this paper, we propose N-to-N QA extraction task in which the derived questions and corresponding answers might be separated across different utterances. We introduce a suite of generative/discriminative tagging based methods with end-to-end and two-stage variants that perform well on 5 customer service datasets and for the first time setup a benchmark for N-to-N DialogQAE with utterance and session level evaluation metrics. With a deep dive into extracted QA pairs, we find that the relations between and inside the QA pairs can be indicators to analyze the dialogue structure, e.g. information seeking, clarification, barge-in and elaboration. We also show that the proposed models can adapt to different domains and languages, and reduce the labor cost of knowledge accumulation in the real-world product dialogue platform. *.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xin Zheng"
        },
        {
            "affiliations": [],
            "name": "Tianyu Liu"
        },
        {
            "affiliations": [],
            "name": "Haoran Meng"
        },
        {
            "affiliations": [],
            "name": "Xu Wang"
        },
        {
            "affiliations": [],
            "name": "Yufan Jiang"
        },
        {
            "affiliations": [],
            "name": "Mengliang Rao"
        },
        {
            "affiliations": [],
            "name": "Binghuai Lin"
        },
        {
            "affiliations": [],
            "name": "Yunbo Cao"
        },
        {
            "affiliations": [],
            "name": "Zhifang Sui"
        }
    ],
    "id": "SP:737056a1fa0f3834bcf38b85b8903c8e4e5ffcca",
    "references": [
        {
            "authors": [
                "Malykh",
                "Maxim Petrov",
                "Vadim Polulyakh",
                "Leonid Pugachev",
                "Alexey Sorokin",
                "Maria Vikhreva",
                "Marat Zaynutdinov."
            ],
            "title": "DeepPavlov: Open-source library for dialogue systems",
            "venue": "Proceedings of ACL 2018, System Demonstrations, pages 122\u2013127, Mel-",
            "year": 2018
        },
        {
            "authors": [
                "Bowen Zhou."
            ],
            "title": "The JDDC corpus: A largescale multi-turn Chinese dialogue dataset for Ecommerce customer service",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 459\u2013466, Marseille, France. European",
            "year": 2020
        },
        {
            "authors": [
                "Yi-Ting Chen",
                "Hen-Hsen Huang",
                "Hsin-Hsi Chen."
            ],
            "title": "MPDD: A multi-party dialogue dataset for analysis of emotions and interpersonal relationships",
            "venue": "Proceedings of the 12th Language Resources and Evaluation Conference, pages 610\u2013614, Marseille,",
            "year": 2020
        },
        {
            "authors": [
                "Lei Cui",
                "Shaohan Huang",
                "Furu Wei",
                "Chuanqi Tan",
                "Chaoqun Duan",
                "Ming Zhou."
            ],
            "title": "SuperAgent: A customer service chatbot for E-commerce websites",
            "venue": "Proceedings of ACL 2017, System Demonstrations, pages 97\u2013102, Vancouver, Canada. Association for",
            "year": 2017
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Shijin Wang",
                "Guoping Hu."
            ],
            "title": "Revisiting pre-trained models for Chinese natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings,",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Xinya Du",
                "Claire Cardie."
            ],
            "title": "Harvesting paragraph-level question-answer pairs from Wikipedia",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1907\u20131917, Mel-",
            "year": 2018
        },
        {
            "authors": [
                "Xinya Du",
                "Junru Shao",
                "Claire Cardie."
            ],
            "title": "Learning to ask: Neural question generation for reading comprehension",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1342\u20131352,",
            "year": 2017
        },
        {
            "authors": [
                "Nan Duan",
                "Duyu Tang",
                "Peng Chen",
                "Ming Zhou."
            ],
            "title": "Question generation for question answering",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 866\u2013874, Copenhagen, Denmark. Association for",
            "year": 2017
        },
        {
            "authors": [
                "Joshua Eisenberg",
                "Michael Sheriff."
            ],
            "title": "Automatic extraction of personal events from dialogue",
            "venue": "Proceedings of the First Joint Workshop on Narrative",
            "year": 2020
        },
        {
            "authors": [
                "Hao Fei",
                "Jingye Li",
                "Shengqiong Wu",
                "Chenliang Li",
                "Donghong Ji",
                "Fei Li."
            ],
            "title": "Global inference with explicit syntactic and discourse structures for dialogue-level relation extraction",
            "venue": "Proceedings of the Thirty-First International Joint Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Boris Galitsky",
                "Dmitry Ilvovsky."
            ],
            "title": "Building dialogue structure from discourse tree of a question",
            "venue": "Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 17\u201323, Brussels, Belgium.",
            "year": 2018
        },
        {
            "authors": [
                "Matthew Henderson",
                "Ivan Vuli\u0107",
                "Daniela Gerz",
                "I\u00f1igo Casanueva",
                "Pawe\u0142 Budzianowski",
                "Sam Coope",
                "Georgios Spithourakis",
                "Tsung-Hsien Wen",
                "Nikola Mrk\u0161i\u0107",
                "Pei-Hao Su."
            ],
            "title": "Training neural response selection for task-oriented dialogue systems",
            "venue": "Pro-",
            "year": 2019
        },
        {
            "authors": [
                "dan Damoc",
                "Aurelia Guy",
                "Simon Osindero",
                "Karen Simonyan",
                "Erich Elsen",
                "Oriol Vinyals",
                "Jack William Rae",
                "Laurent Sifre"
            ],
            "title": "An empirical analysis of compute-optimal large language model training",
            "venue": "In Advances in Neural Information Processing Sys-",
            "year": 2022
        },
        {
            "authors": [
                "Qi Jia",
                "Mengxue Zhang",
                "Shengyao Zhang",
                "Kenny Q. Zhu."
            ],
            "title": "Matching questions and answers in dialogues from online forums",
            "venue": "ECAI 2020 - 24th European Conference on Artificial Intelligence, 29 August-8 September 2020, Santiago de Compostela,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.",
            "year": 2015
        },
        {
            "authors": [
                "Hua Liang",
                "Tianyu Liu",
                "Peiyi Wang",
                "Mengliang Rao",
                "Yunbo Cao."
            ],
            "title": "Smartsales: Sales script extraction and analysis from sales chatlog",
            "venue": "arXiv preprint arXiv:2204.08811.",
            "year": 2022
        },
        {
            "authors": [
                "Ruixue Liu",
                "Meng Chen",
                "Hang Liu",
                "Lei Shen",
                "Yang Song",
                "Xiaodong He."
            ],
            "title": "Enhancing multiturn dialogue modeling with intent information for e-commerce customer service",
            "venue": "CCF International Conference on Natural Language Processing and",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Liu",
                "Fuli Luo",
                "Pengcheng Yang",
                "Wei Wu",
                "Baobao Chang",
                "Zhifang Sui."
            ],
            "title": "Towards comprehensive description generation from factual attribute-value tables",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Liu",
                "Xin Zheng",
                "Baobao Chang",
                "Zhifang Sui."
            ],
            "title": "Towards faithfulness in open domain tableto-text generation from an entity-centric view",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on In-",
            "year": 2021
        },
        {
            "authors": [
                "Xinwei Long",
                "Shuzi Niu",
                "Yucheng Li."
            ],
            "title": "Consistent inference for dialogue relation extraction",
            "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 3885\u20133891. International Joint Conferences on Arti-",
            "year": 2021
        },
        {
            "authors": [
                "Haoran Meng",
                "Zheng Xin",
                "Tianyu Liu",
                "Zizhen Wang",
                "He Feng",
                "Binghuai Lin",
                "Xuemin Zhao",
                "Yunbo Cao",
                "Zhifang Sui."
            ],
            "title": "Dialogusr: Complex dialogue utterance splitting and reformulation for multiple intent detection",
            "venue": "arXiv preprint arXiv:2210.11279.",
            "year": 2022
        },
        {
            "authors": [
                "Amit Moryossef",
                "Yoav Goldberg",
                "Ido Dagan."
            ],
            "title": "Step-by-step: Separating planning from realization in neural data-to-text generation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2019
        },
        {
            "authors": [
                "Guntis B\u0101rzdin"
            ],
            "title": "Human-in-the-loop conversation agent for customer service",
            "venue": "In International Conference on Applications of Natural Language to Information Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Shuang Peng",
                "Mengdi Zhou",
                "Minghui Yang",
                "Haitao Mi",
                "Shaosheng Cao",
                "Zujie Wen",
                "Teng Xu",
                "Hongbin Wang",
                "Lei Liu."
            ],
            "title": "A dialogue-based information extraction system for medical insurance assessment",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Denis Peskov",
                "Nancy Clarke",
                "Jason Krone",
                "Brigi Fodor",
                "Yi Zhang",
                "Adel Youssef",
                "Mona Diab."
            ],
            "title": "Multi-domain goal-oriented dialogues (MultiDoGO): Strategies toward curating and annotating large scale dialogue data",
            "venue": "Proceedings of the 2019 Confer-",
            "year": 2019
        },
        {
            "authors": [
                "Liang Qiu",
                "Yizhou Zhao",
                "Weiyan Shi",
                "Yuan Liang",
                "Feng Shi",
                "Tao Yuan",
                "Zhou Yu",
                "Song-Chun Zhu."
            ],
            "title": "Structured attention for unsupervised dialogue structure induction",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Ashwin Ram",
                "Rohit Prasad",
                "Chandra Khatri",
                "Anu Venkatesh",
                "Raefer Gabriel",
                "Qing Liu",
                "Jeff Nunn",
                "Behnam Hedayatnia",
                "Ming Cheng",
                "Ashish Nagar"
            ],
            "title": "Conversational ai: The science behind the alexa prize",
            "venue": "arXiv preprint arXiv:1801.03604",
            "year": 2018
        },
        {
            "authors": [
                "Zhouxing Shi",
                "Minlie Huang."
            ],
            "title": "A deep sequential model for discourse parsing on multi-party dialogues",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7007\u20137014.",
            "year": 2019
        },
        {
            "authors": [
                "Kazutoshi Shinoda",
                "Saku Sugawara",
                "Akiko Aizawa."
            ],
            "title": "Improving the robustness of QA models to challenge sets with variational question-answer pair generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Anna Tigunova",
                "Paramita Mirza",
                "Andrew Yates",
                "Gerhard Weikum."
            ],
            "title": "PRIDE: Predicting Relationships in Conversations",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4636\u20134650, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Siyuan Wang",
                "Zhongyu Wei",
                "Zhihao Fan",
                "Yang Liu",
                "Xuanjing Huang."
            ],
            "title": "A multi-agent communication framework for question-worthy phrase extraction and question generation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):7168\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Yu Wu",
                "Wei Wu",
                "Chen Xing",
                "Ming Zhou",
                "Zhoujun Li."
            ],
            "title": "Sequential matching network: A new architecture for multi-turn response selection in retrievalbased chatbots",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Lin-",
            "year": 2017
        },
        {
            "authors": [
                "Runxin Xu",
                "Tianyu Liu",
                "Lei Li",
                "Baobao Chang."
            ],
            "title": "Document-level event extraction via heterogeneous graph-based interaction model with a tracker",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
            "year": 2021
        },
        {
            "authors": [
                "Runxin Xu",
                "Peiyi Wang",
                "Tianyu Liu",
                "Shuang Zeng",
                "Baobao Chang",
                "Zhifang Sui."
            ],
            "title": "A two-stream AMR-enhanced model for document-level event argument extraction",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the As-",
            "year": 2022
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Zhilin Yang",
                "Junjie Hu",
                "Ruslan Salakhutdinov",
                "William Cohen."
            ],
            "title": "Semi-supervised QA with generative domain-adaptive nets",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Dian Yu",
                "Kai Sun",
                "Claire Cardie",
                "Dong Yu."
            ],
            "title": "Dialogue-based relation extraction",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4927\u20134940, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Chunyuan Yuan",
                "Wei Zhou",
                "Mingming Li",
                "Shangwen Lv",
                "Fuqing Zhu",
                "Jizhong Han",
                "Songlin Hu."
            ],
            "title": "Multi-hop selector network for multi-turn response selection in retrieval-based chatbots",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in",
            "year": 2019
        },
        {
            "authors": [
                "Changchang Zeng",
                "Shaobo Li",
                "Qin Li",
                "Jie Hu",
                "Jianjun Hu."
            ],
            "title": "A survey on machine reading comprehension\u2014tasks, evaluation metrics and benchmark datasets",
            "venue": "Applied Sciences, 10(21):7640.",
            "year": 2020
        },
        {
            "authors": [
                "Xiangyang Zhou",
                "Lu Li",
                "Daxiang Dong",
                "Yi Liu",
                "Ying Chen",
                "Wayne Xin Zhao",
                "Dianhai Yu",
                "Hua Wu."
            ],
            "title": "Multi-turn response selection for chatbots with deep attention matching network",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The development of natural language processing and conversational intelligence has radically redefined the customer service landscape. The customer service chatbots empowered by knowledge bases or frequently asked questions (FAQs) drastically enhance the efficiency of customer support (e.g. Cui et al., 2017; Ram et al., 2018; Burtsev et al., 2018; Liu et al., 2020; Paikens et al., 2020) In the cold\n*Our code and data are available at https://github. com/MrZhengXin/DialogQAE\n*Equal contribution. \u2020Corresponding authors.\nstart or continuous integration scenarios, harvesting QA pairs from existing or growing customer service chatlog is an efficient way to enrich knowledge bases. Besides the retrieved QA pairs can be valuable resources to improve dialogue summarization (Lin et al., 2021), gain insights into the prevalent customer concerns and figure out new customer intents or sales trends (Liang et al., 2022), which are of vital importance to business growth.\nPrior work on question-answer extraction follows the utterance matching paradigm, e.g., matching the answers to the designated questions in a dialogue session (Jia et al., 2020) in the offline setting or figuring out the best response to the specific user query (Wu et al., 2017; Zhou et al., 2018; Yuan et al., 2019) in the online setting. Within this framework, 1-to-1 QA extraction has been explored by Jia et al. (2020), however, we argue that users might not cover all the details in a single query while interacting with the customer service agents, which means that a certain QA pair might involve multiple utterances in the dialogue session.\nIn this paper, we extend 1-to-1 QA matching to N-to-N QA extraction, where the challenges are two-fold: 1) cluster-to-cluster QA matching with no prior knowledge of the number of utterances involved in each QA pair and the number of QA pairs in each dialogue session, as the question might be distributed in single or multiple user queries. 2) session-level representation learning with a longer context, as the paired questions and answers might be separated within the dialogue, and the model shall detect multiple same-topic questions and then check if the answer is related to any one of the questions. We propose session-level tagging-based methods to deal with the two challenges. Our method is not only compatible with the N-N QA extraction task setting but also 1-1 and 1-N, which is generic. Switching from matching to tagging, we feed the entire dialogue session into powerful pre-trained models, like BERT (Devlin et al., 2019)\n[1, 3\uff0c6] [8\uff0c9] N-N [10\uff0c13] [11\uff0c14] N-N [17] [18, 21] 1-N [22, 24, 25] [26] N-1 [27] [28] 1-1\n[1, 3, 10] [8, 11] N-N [13] [14] 1-1 [17] [18,19, 21] 1-N [22, 24, 25] [26] N-1 [27] [28] 1-1 [6] [9] 1-1\nor mT5 (Xue et al., 2021), and design a set of QA tags that empowers N-to-N QA matching. Through careful analysis, we find that DialogQAE can serve as a powerful tool in the dialogue structure analysis, as the relations between and within QA pairs are implicit signals for dialogue actions like information seeking, clarification, barge-in and elaboration. From a pragmatic perspective, we show that the proposed models can be easily adapted to different domains and languages, and largely accelerate the knowledge acquisition on FAQs of real-world users in the product dialogue system. We summarize our contributions below:\n\u2022 We setup a benchmark for DialogQAE with end-to-end and two-stage baselines that support N-to-N QA extraction, as well as the utterance and session-level evaluation metrics.\n\u2022 We show that DialogQAE is an effective\nparadigm for dialogue analysis by summarizing 5 between-QA-pairs and 3 in-QA-pair relations that characterize the dialogue structure in the conversation flow.\n\u2022 Through careful analysis of domain and language adaptation, as well as real-world applications, we show that the proposed DialogQAE model effectively automates and accelerates the cold-start or upgrade of a commercial dialogue system."
        },
        {
            "heading": "2 Task Overview",
            "text": "A complete snippet of customer service conversation between human service representatives and customers, which is canonically termed as a dialogue session S, consists of multiple, i.e. n, dialogue utterances. Formally we have S = {(u1, r1), (u2, r2), \u00b7 \u00b7 \u00b7 , (un, rn)}, in which ri sig-\nnifies the speaker role of the i-th utterance urii . In this paper we focus on two-party dialogue, more concretely we have ri \u2208 {C,A} in which \u2018C\u2019, \u2018A\u2019 represents the roles of speakers: customers and human agents respectively.\nAfter feeding the dialogue session into the DialogQAE model, we expect the model to extract m QA pairs R = {(UQ1 ,UA1), (UQ2 ,UA2), \u00b7 \u00b7 \u00b7 , (UQm ,UAm)}.\nUQj = {(uq1 , rq1), (uq2 , rq2), \u00b7 \u00b7 \u00b7 , (uqs , rqs)} (1) UAj = {(ua1 , ra1), (ua2 , ra2), \u00b7 \u00b7 \u00b7 , (uat , rat)} (2) UQj ,UAj represent the unions of question and answer dialogue utterances in S 2, respectively.\nTo better characterize the proposed n-to-n dialogue QA extraction, we introduce two notions which are conceptually related to the mapping between dialogue utterances and extracted QA pairs. 1) Exclusive dialogue utterance: each utterance in S can only be exclusively mapped to one single question or answer union in R, i.e. the mapping between S and R is a one-to-one (injection) function. 2) Speaker role consistency: a common assumption for most two-party conversations is that the customers raise questions while the agents answer the questions. Formally for each extracted QA pair, e.g. UQj and UAj in R, {rq1:s} = {C}, {ra1:t} = {A}. In our setting, the rule of exclusive dialogue utterance strictly holds for all the datasets we used. However, although most datasets in this paper exhibit speaker role consistency, we still observe the customer queries in the answer union or the agent responses in the question unions, for example in Fig 1 the 23-rd utterance from the agent is included in the question union Q3.\nAs shown in Fig 1, depending on the sizes of question and answer unions, e.g., UQj ,UAj , in the certain QA pair, we categorize the DialogQAE task into four types: 1-to-1, 1-to-N, N-to-1 and N-to-N, in which the former and latter numbers indicate the size of question and answer unions respectively."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Tagging as Extraction",
            "text": "The prior mainstream research on dialogue QA matching are based on the text segment alignment, such as matching the specific answers with the\n2In equation 1 and 2, the upper indexes, i.e. j, of q1:s and a1:t are omitted for simplicity.\ngiven questions in QA extraction (Jia et al., 2020), measuring the similarity of the user query and candidate answers in the response selection (Wu et al., 2017; Henderson et al., 2019), or extracting relations with entity matching in the dialogue (Tigunova et al., 2021).\nThe key to successfully excavating n-to-n QA pairs from customer service chatlog is to figure out the cluster-to-cluster mapping among utterances in a dialogue session."
        },
        {
            "heading": "3.2 End-to-end QA Extraction",
            "text": "We convert QA extraction into\u201cfill-in-the-blank\u201d sequence labeling task, hoping that the model would quickly learn to predict the label li \u2208 {O,Qj , Aj} based on the corresponding utterance Ui. After label prediction, we collect the QA pairs R = {(UQj , UAj )|UQj = {uk|lk = Qj , 1 \u2264 k \u2264 n}, UAj = {uk|lk = Aj , 1 \u2264 k \u2264 n}, 1 \u2264 j \u2264 m} from the labels L = {li|1 \u2264 i \u2264 n}.\nAs depicted in Figure 2, the input of the model is \u201cr1 : (u1, r1) [MASK] [SEP] ...rn : urnn [MASK] [SEP] \u201d, where [MASK], [SEP] signifies mask token, separation token and both of which are in the vocabulary of the masked language model. We formulate the QA sequence labeling task as either a generative, i.e. mT5-style (Xue et al., 2021), or a discriminative, i.e. BERT-style (Devlin et al., 2019)), classifier.\nFrom the generative perspective, i.e., spancorruption model mT5, the [MASK] token symbolizes the <extra_id_i> for each utterance ui, and we use the semicolon (;) as the replacement for the separation token [SEP]. The output of the QA extraction model is a list of Q/A labels L, where for the encoder-only model each prediction is exactly on the masked position, and for encoder-decoder model mT5 the prediction is a sequence \u201c<extra_id_0> l1... <extra_id_n\u2212 1> ln\u201d. For the discriminative tagging model (BERTstyle), we use [unusedX] to denote the label set {O,Q1, ..., O1, ...} and for span-corruption encoder-decoder model, we just use their text form \u201cO, Q1, ..., O1, ...\u201d to represent the label."
        },
        {
            "heading": "3.3 Two-stage QA Extraction",
            "text": "Instead of predicting the label of utterance in a single round, we could decouple the process into two steps (Moryossef et al., 2019; Liu et al., 2019, 2021), which firstly figures out questions then extract corresponding answers. We illustrate the twostage workflow in Figure 6: in the first stage, the\nmodel input is the same as the end-to-end QA extraction (Sec 3.2), however the dialogue question extractor only predicts lstage1 \u2208 {O,Q1, ...}, to determine whether each utterance in the dialogue session is a question or not. In the second stage, we fill in the labels where the stage-1 model predicts as questions. Then we feed the filled utterances sequence to the dialog answer extractor, which predicts the remaining utterances within the label set lstage2 \u2208 {O,A1, ...}, to decide whether they are the answer Aj to the question Qj .\nMoreover, in the 1-to-N (including 1-to-1) scenario, where a question covers only a single utterance, we could further break down the question labeling process in a context-less way. As shown in Figure 7, at stage 1, we separately perform binary classification {Q,O} for each utterance with the input format of \u201c [CLS] Ui\u201d, where [CLS] is the classification token, and at stage 2, we relabel those predicted as Q in the sequential order Q1, Q2, ... to fill in the blank, and then apply the same dialog answer extracting strategy."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We conduct the experiments on 5 Chinese customer service multi-turn dialogue datasets, namely CSDS (Lin et al., 2021), MedQA (Jia et al., 2020), EduQA, CarsaleQA and ExpressQA. CSDS and MedQA are two public dialogue chatlog datasets while the latter three datasets are internal datasets which are accumulated through genuine customer-agent interactions in a commercial industrial dialogue platform. CSDS is derived from JDDC (Chen et al., 2020a) corpus and tailored for dialogue summarization where n-to-n QAs are also provided as the clues for the summaries. MedQA is accumulated on a medical QA platform3 that covers con-\n3https://www.120ask.com/\nversations between doctors and patients. EduQA, CarsaleQA and ExpressQA, as indicated by their names, come from real-world conversations in the education, carsales and express delivery domains. As shown in Table 1, EduQA, CarsaleQA and ExpressQA are composed exclusively of 1-1 QA pairs while CSDS and MedQA involve 1-N, N-1 and N-N mappings in the extracted QA pairs."
        },
        {
            "heading": "4.2 Evaluation Metrics",
            "text": "To evaluate the performance at the utterance level, we apply the traditional precision (P), recall (R) and F1 metrics, which ignore the non-QA label \"O\":\nP =\n\u2211 i \u2211 j IPred(i)j !=O,Pred(i)j =Ref (i)j\u2211\n1\u2264i\u2264N \u2211\n1\u2264j\u2264n(i) IPred(i)j !=O ,\nR =\n\u2211 i \u2211 j IRef (i)j !=O,Pred(i)j =Ref (i)j\u2211\n1\u2264i\u2264N \u2211\n1\u2264j\u2264n(i) IRef (i)j !=O ,\nF1 = 2 \u2217 P \u2217R P +R ,\n(3)\nwhere N is the number of instances, and Pred(i), Ref (i) denote the prediction and reference label sequences of the i-th instance.\nSimilarly, for QA-pair level evaluation, we propose adoption rate (AR), hit rate (HR) and session F1 (S-F1):\nAR =\n\u2211 i \u2211 j |R_pred(i) \u2229R_ref (i)|\u2211 1\u2264i\u2264N |R_pred(i)| ,\nHR =\n\u2211 i \u2211 j |R_pred(i) \u2229R_ref (i)|\u2211\n1\u2264i\u2264N |R_ref (i)| ,\nS-F1 = 2 \u2217HR \u2217AR HR+AR ,\n(4)\nwhere R_pred(i), R_ref (i) denote the prediction and reference QA-pair set of the i-th instance.\nFrom the perspective of FAQ database population by extracting QA pairs from the customer service chatlog, the predicted QA pairs would serve as an automated module in the workflow, followed by the human verification. The adoption rate (AR) corresponds to the ratio of \u201caccepted\u201d QA pairs by human judges within the predicted QAs, which is analogous to the utterance-level precision. The hit rate (HR), on the other hand, signifies the proportion of predicted QAs in all annotated QAs within the dialogue session which corresponds to the utterance-level recall."
        },
        {
            "heading": "4.3 Experimental Settings",
            "text": "We experiment with a variety of pre-trained models via Hugging Face Transformers (Wolf et al., 2020), which are encoder-decoder model mT5 (Xue et al., 2021) with three different parameter scales, namely T5-base (580M), T5-large (1.2B), T5-xl (3.7B), and encoder-only model including chinese-bert-wwm-ext (110M), chinese-robertawwm-ext (110M), chinese-roberta-wwm-ext-large (330M) (Cui et al., 2020), Deberta-Chinese-Large (304M) 4, Erlangshen-MegatronBert (1.3B) 5, as the backbones for the end-to-end and two-stage models. For contextless question classification and question-answer matching, we use chinese-robertawwm-ext-large (330M) (Cui et al., 2020). We use the Adam optimizer (Kingma and Ba, 2015) with the learning rate of 3e-5 and train the models for at most 9 epochs on 4 Nvidia A100 GPUs.\n4https://huggingface.co/WENGSYX/ Deberta-Chinese-Large\n5https://huggingface.co/IDEA-CCNL/ Erlangshen-MegatronBert-1.3B"
        },
        {
            "heading": "5 Analysis and Discussions",
            "text": ""
        },
        {
            "heading": "5.1 Baseline Performance",
            "text": "Table 2 and 3 illustrate the utterance-level and session-level performance of QA extraction on the MedQA and CSDS datasets respectively. For both end-to-end and two-stage models, enlarging the model parameters leads to a considerable performance gain, which indicates that the dialogue session encoders with higher capacity are of vital importance for extracting QA pairs. In terms of the comparisons between the end-to-end and twostage models, we observe that the two-stage models outperform end-to-end models on the MedQA dataset while it is the other way around on the CSDS dataset, which shows that end-to-end methods are more favorable in the N-to-N QA extraction that requires reasoning over longer dialogue context, such as CSDS (6.88 in Dist_QA and 65.57% non-1-to-1 QAs as shown in Fig 1), as presumably complex Q-A mapping exaggerates the error propagation of aligning the potential answers to the given predicted questions in the two-stage models. For the model performance on the N-to-N mapping shown in Fig 3, as we expect, the models get higher scores on 1-to-1 mapping than N-to-N mapping.\nWe also highlight the comparison between the generative (mT5-style, \u2018Gen\u2019) and the discriminative (BERT-style, \u2018Tag\u2019) models in Table 2 and 3. We observe that with comparable pre-trained model size, i.e. DeBERTa-large (304M) versus mT5-base (580M) and MegatronBERT (1.3B) versus mT5large (1.5B), generative models perform better on the CSDS dataset while discriminative models win on the MedQA dataset, showing that T5 models might be a promising option on the dialogue analysis with long context (Meng et al., 2022). We believe that model size is an important factor in performance, since intuitively model with more\nparameters would fit the data better, and yet discriminative models with Masked Language Model pre-training task may not enjoy the same scaling law (Hoffmann et al., 2022) as the generative models do."
        },
        {
            "heading": "5.2 Dialogue Structure Analysis",
            "text": "Prior research tried to extract and analyze the structure of a given dialogue session through latent dialogue states (Qiu et al., 2020), discourse parsing (Galitsky and Ilvovsky, 2018) or event extraction (Eisenberg and Sheriff, 2020). However,\nthose methods are specific to the predefined semantic/information schema or ontology, i.e., discourse, dependency, AMR parsing trees (Xu et al., 2021, 2022), dialogue actions or event/entity labels (Liang et al., 2022). Through the analysis of the extracted QA pairs of a dialogue session, we summarize a more general schema to categorize the dialogue structure according to the customer-agent interaction in the dialogue flow.\nFig 4 demonstrates the typical \u2018between-QApairs\u2019 relations based on the extracted QA mappings. The most common case is Sequential\nQA Flow, where Position(A1) < Position(Q2) and Role(Q1) = Role(Q2); in this case, one complete QA pair is after another. For Follow-up Information Seeking, here Position(A1) < Position(Q2) but Role(Q1) \u0338= Role(Q2), indicating the answer leads to a new question. For elaboration/Detailing, Position(Q2) < Position(A1) and Role(Q1) = Role(Q2), which means one person asked two questions in a row, and in turn, the other answered consecutively. In the example, the doctor sequentially answers the consecutive questions raised by the patients, with the second answer elaborating on the first one. For Clarification/Confirmation, Position(Q2) < Position(A1) and Role(Q1) \u0338= Role(Q2) and Position(A2) < Position(A1), which implies the first question can not get the answer yet, and more information is needed from the questioner; once provided, the first question can finally be answered correctly. In the example, the doctor asked a clarification question on when the symptoms occurred after the inquiry of the patient instead of answering the inquiry instantly. For Barge-in/Interruption, which is not common, is the case of Position(Q2) < Position(A1) and Role(Q1) = Role(Q2) and Position(A2) < Position(A1), where the second question is answered first. As shown in Fig 3, the QA extraction models perform better on SF, FIS, and BI than CC and ED, presumably the interleaving QA pairs pose a bigger challenge to the dialogue information extraction.\nWe delve into the relative position of the question and answer utterances within an N-to-N QA pair in Fig 5. Most questions and answers are disjoint within a QA pair while the overlapping questions and answers account for 26.91% in the CSDS dataset. We take a further step to split the overlapping QAs into two circumstances: in-pair Q-A and in-pair Q-A-Q, depending on the role (Q or A) of the last utterance in the QA pair. As illustrated in Fig 3, all three QAE models perform better on the disjoint QA pairs than overlapping ones."
        },
        {
            "heading": "5.3 Domain and Language Adaptation",
            "text": "We illustrate the domain and language adaptation of our dialogQAE models in Table 4 and 5 respectively, which highlight the real-world utility of our models.\nIn Table 4, we observe that mixing the datasets from different domains is a simple but effective way to boost the overall performance. The potential\ncorrelation between different domains is the key factor of the model performance on the domain transfer, e.g. the bidirectional transfer between the carsale and the education domains gets higher scores than other domain pairs.\nThanks to the multilingual nature of mT5, the models trained on the Chinese datasets can be easily applied to datasets in other languages, e.g. English. We test the Chinese DialogQAE model on different domains of the MultiDOGO dataset (Peskov et al., 2019). As the MultiDOGO dataset does not have Q-A-pair annotations, we ask the human annotator to decide whether the recognized QA pairs by the MedQA-DialogQAE model are eligible according to the semantics in the dialogue flow. We use majority votes among 3 human judges and the inter-annotator agreement (the Krippendorf\u2019s alpha) is 0.89."
        },
        {
            "heading": "6 Related Work",
            "text": ""
        },
        {
            "heading": "6.1 QA Extraction",
            "text": "For text-based QA extraction, Rajpurkar et al. (2016) proposed the dataset SQuAD 1.1, in which the 100k+ questions were created by crowdworkers on 536 Wikipedia articles. Subsequently, Du and Cardie (2018) created 1M+ paragraph-level\nquestion-answer pairs over 10,000 Wikipedia articles. For question generation, Yang et al. (2017) use a trained model to generate questions on unlabeled data. Later, Wang et al. (2019) proposed to identify key phrases first and then generate questions accordingly. For machine reading comprehension (MRC), research on dialogues MRC aims to teach machines to read dialogue contexts and make response Zeng et al. (2020) aims to answer the question based on a passage as context. Shinoda et al. (2021) leveraged variational question-answer pair generation for better robustness on MRC. However, extraction methods that can work on 1-1, 1-N, and N-N scenario is under-explored."
        },
        {
            "heading": "6.2 Dialogue Analysis",
            "text": "For dialogue information extraction (IE), in order to save the efforts of the assessor in the medical insurance industry, Peng et al. (2021) proposed a dialogue IE system to extract keywords and generate insurance reports. To figure out the semantics in the dialogue flow, Galitsky and Ilvovsky (2018) proposed a dialogue structure-building method from the discourse tree of questions. Qiu et al. (2020) incorporated structured attention into a Variational Recurrent Neural Network for dialogue structure induction in an unsupervised way. Eisenberg and Sheriff (2020) introduced a new problem, extracting events from dialogue, annotated the dataset Personal Events in Dialogue Corpus, and trained a support vector machine model.\nRelation Extraction over Dialogue is a newly defined task by DialogRE (Yu et al., 2020), which fo-\ncuses on extracting relations between speakers and arguments in a dialogue. DialogRE is an English dialogue relation extraction dataset, consisting of 1788 dialogues and 36 relations. MPDD (Chen et al., 2020b) is a Multi-Party Dialogue Dataset built on five Chinese TV series, with both emotion and relation labels on each utterance. Long et al. (2021) proposed a consistent learning and inference method for dialogue relation extraction, which aims to minimize possible contradictions. Fei et al. (2022) introduced a dialogue-level mixed dependency graph. Shi and Huang (2019) proposed a deep sequential model for discourse parsing on multi-party dialogues. The model predicts dependency relations and constructs a discourse structure jointly and alternately."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we propose N-to-N question and answer (QA) pair extraction from customer service dialogue history, where each question or answer may involve more than one dialogue utterance. We introduce a suite of end-to-end and two-stage tagging-based methods that perform well on 5 customer service datasets, as well as utterance and session level evaluation metrics for DialogQAE. With further analysis, we find that the extracted QA pairs characterize the dialogue structure, e.g. information seeking, clarification, barge-in, and elaboration. Extensive experiments show that the proposed models can adapt to different domains and languages and largely accelerate knowledge accumulation in the real-world dialogue platform.\nLimitations\nThis work focuses on the N-to-N question and answer extraction from a dialogue session and does not touch the relevant tasks such as question generation (e.g. Du et al., 2017; Duan et al., 2017) and dialogue summarization (Lin et al., 2021). The proposed task can be seen as a preparation for the subsequent tasks by decomposing the entire procedure of question generation and dialogue summarization into two steps: extraction before generation. The extracted QA pairs can also be further processed in order to visualize some important factors for customer service, like common customer concerns about the products, winning sales scripts to persuade the customers and emerging or trending user intents (Liang et al., 2022), by a set of atomic natural language processing modules like keyword extraction, sentiment analysis and semantic parsing and clustering.\nEthics Statement\nThe internal datasets we used in this paper, i.e. EduQA, CarsalesQA and ExpressQA, have gone through a strict data desensitization process, with the guarantee that no user privacy or any other sensitive data is being exposed by a hybrid of automatic and human verification. Human verification also eliminates the dialogue sessions with gender/ethnic biases or profanities. The other two datasets, CSDS and MedQA, are publicly available and we use them with any modification. The model for extracting questions and answers in the dialogue paves the for N-to-N dialogue QA extraction, without any risk of violating the EMNLP ethics policy."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Model Performance on the Internal Datasets\nWe show the model performance on the internal datasets, i.e. EduQA, CarsalesQA and ExpressQA in Table 6. In terms of comparison between endto-end and two-stage models, end-to-end models are clear winner with respect to session level F1 on the Carsales and EndQA datasets, while two-stage models take the lead on the ExpressQA dataset. According to the dataset statistics in Table 1, we guess this is because ExpressQA has longer dialogue session (14.13 for the average number of utterances) and require longer context (2.57 versus 2.23/1.36 in Dist_QA) for extracting QA pairs.\nThe DialogQAE models have been deployed in a commercial platform for conversational intelligence. The module serves as an automatic dialogue information extractor, followed by human verification and modification on the extracted QA pairs so that they can serve as standard and formal FAQs in the customer service. According to the user feedbacks from the online customer service department of an international express company, the assistance of dialogue QA extraction has largely accelerated the information enrichment for customer service FAQs, reducing from around 8 days per update to 2 days per update.\nA.2 Between-QA-Pairs Relations Examples We show more examples on the dialogue structure from the MedQA datasets below.\n!\"#$%&'()* Is there anything wrong with your neck?\n!+,-. Cervical stiff neck.\n/0123423$%567 What department should I go for in the hospital?\n89!+: Take a cervical spine X-ray first.\nUID=u1Q1\nUID=u2A1\nUID=u3Q2\nUID=u4A2\nFollow-up Information Seeking (14.29%)\n;<=>?@ABCD#EF%7G Now it's chest tightness and retching, does it matter?\nH$%IJK;L* since when did it occur?\nMNOPQ When i woke up in the morning.\nRSTUVW#X May be related to urticaria.\nUID=u1Q1\nUID=u2Q2\nUID=u4A1\nUID=u3A2\nClarification/Confirmation (12.34%)\nYZDY>[23L\\])* Hello, did you have your liver function checked during your physical examination? ^_>`ab\\cd)* Have you ever had the hepatitis B vaccine?\nNefa.ghi. Got my second shot last week.\n\\]Sjkl Liver function is normal.\nUID=u1Q1\nUID=u3A2\nBarge-in/Interruption (1.80%)\nUID=u2Q2\nUID=u4A1\nR^mZ)* Can it be cured?\nnopq>r$%L* What is the therapeutic effect of AD capsules?\nR^mZ It can be cured\nstuvwxyz It includes prevention of night blindness and calcium supplementation.\nUID=u1Q1\nUID=u3A1\nElaboration/Detailing (12.62%)\nUID=u2Q2\nUID=u4A2\nSequential QA Flow (58.95%)\n{|#$%|}~4 \u00c4)7G Do you have any questions?\n\u00c5\u00c7\u00c9Q\u00d1\u00d6&'( I have a bad stomach lately.\n\u00dc%&'(.7G How bad is your stomach?\n&>\u00e1D=>#\u00e0\u00e2\u00e2 Not painful, just a little bloated.\nUID=u1Q1\nUID=u2A1\nUID=u4A2\nUID=u3Q2\nCaption: The analysis of the dialogue structures based on the between-QA-pairs relations. Given a snippet of dialogue utterance, we roughly categorize the dialogue structures into 5 different types according to the betweenQA interactions between customers and human agents.\nUID=uk\nIcon: SpeakerA/SpeakerB, can be either customer or human agent.\nUID=ut Dotted Box: The question/answer unions, might comprise >1 utterances. UID= ut/uk: We use the 1st utterance in the union to represent its position in the conversational flow.\nExplanation for the illustrations of utterances\nUID=u4A2\nUID=u2Q2AjQi QA Tag: Each utterance (union) can be tagged as either Q or A, no matter what the speaker\u2019s role is.\nFigure 4: The demonstration for the between-QA-pairs relations and their proportion in the MedQA dataset. Given a snippet of consecutive dialogue utterances (for UIDs, u1 < u2 < u3 < u4), we roughly categorize the dialogue flows into 5 different types according to the between-QA interactions between customers and human agents.\nCaption: The demonstration for the in-QA-pair relations and their propotion in the CSDS dataset. According to the\nrelative positions of Q and A utterances in a n-to-n QA pair, we categorize the interleaving utterances into 3 types.\nIn\nQ UID=u1\nUID=3A\nUID=4A\nUID=2\nUID=u4A\nDisjoint In-pair Q-A (83.09%)\nQ UID=u1\nUID=u2A\nQ UID=u3\nUID=u4A\nOverlap In-pair Q-A (14.70%)\nQ UID=u1\nUID=u2A\nQ UID=u3\nOverlap In-pair Q-A-Q (2.21%)\n{|\u00e4\u00e3\u00e5\u00e7\u00e9\u00e8<\u00ea7G Where is the location of the local warehouse?\n\u00eb\u00ed\u00ec\u00ee\u00ef\u00f1R^)7 I'm in a hurry, can I pick it up by myself?\nYZ\u00ee&R^\u00f3\u00e5\u00e7L\u00f2\u00f4 Hello, you can't go to the warehouse.\n\u00c79>R^\u00f3\u00f6\u00f5\u00ef\u00f1L\u00f4 You can pick it up at the delivery site.\n\u00faL\u00f9>\u00fbL7G Is it raw or cooked?\n\u00faL\u00f4 It\u2019s raw.\n\u00fc\u2020)7G Is it fresh?\n\u00fc\u2020L\u00f4 It is fresh.\n\u00dc%\u00b0{7G How to apply?\n\u00a2\u00a3\u00a7\u2022\u00b6\u00df\u00ae\u00a9\u2122\u00a3\u00b0{\u00b4I will apply for you after your order is completed.\n\u00b0{\u00df]\u00a9\u00ee\u00a8\u2260\u00c6\u00d8\u2122\u00c5)7 After the application is successful, will the money be returned to me?\nQ UID=u2\nUID=u3A\nFigure 5: The demonstration of the in-QA-pair relations and their proportion in the CSDS dataset. According to the relative positions of Q and A utterances in an n-to-n QA pair, we categorize the interleaving utterances into 3 types.\nDataset Training Strategy Utterance Level(%) Session Level(%) P R F1 AR HR S-F1 CarsalesQA End-to-End (Gen) 83.67 61.50 70.89 42.11 44.44 43.24 EduQA End-to-End (Gen) 88.96 83.81 86.31 55.50 65.98 60.29 ExpressQA End-to-End (Gen) 96.26 78.26 86.33 63.46 71.74 67.35 CarsalesQA Two-Stage (G+G) 85.96 73.13 79.03 43.31 53.54 47.89 EduQA Two-Stage (G+G) 98.44 69.23 81.29 59.62 67.39 63.27 ExpressQA Two-Stage (G+G) 92.99 76.54 83.97 59.66 67.31 63.25\nTable 6: The performances of the end-to-end and two-stage models (mT5-large) for the QA extraction task on the internal datasets.\nRole Utterance Doctor \u5403\u8fc7\u4ec0\u4e48\u836f\u5462? What medicine have you taken? Doctor \u73b0\u5728\u8fd8\u5403\u7740\u5417? Are you still taking them? Patient \u8fd8\u5728\u5403 Still taking. Patient \u915a\u9ebb\u7f8e\u654f\u7247\nParacetamol, Pseudoephedrine Hydrochloride, Dextromethorphan Hydrobromide and Chlorpheniramine Maleate Tablets\nDoctor \u591a\u957f\u65f6\u95f4\u4e86? How long has it been like this? Doctor \u591a\u5927\u5e74\u9f84? How old are you? Patient \u4e09\u5341\u4e09\u4e86 I\u2019m thirty-three. Patient \u5341\u591a\u5e74\u4e86 It has been more than ten years.\nDoctor \u4f60\u7684\u5b9d\u5b9d\u73b0\u5728\u6709\u54b3\u55fd\u5417? Does your baby have a cough right now? Doctor \u6709\u53d1\u70ed\u7684\u60c5\u51b5\u5417? Does it have a fever? Patient \u6ca1\u6709\u53d1\u70ed No fever Patient \u6709\u54b3\u55fd Have a cough.\nTable 8: Barge-in/Interruption examples in MedQA\nDialog Question Classifier\nU1 U2 U3 Stage 1\nInput\nStage 2 Input\nQ1 Q2U1 [SEP] U2 [SEP] U3 [SEP] U4 [SEP]\nDialog Answer Extractor\nO A2 Stage 2 Output\n[CLS] [CLS] [CLS] U4[CLS]\n[MASK] [MASK]\nFigure 7: The model workflow for the two-stage QA extraction where the first stage is the binary question classifier. The workflow only works for 1-to-1 or 1-to-N QA extraction.\nRole Utterance Doctor \u80c3\u75db\u591a\u957f\u65f6\u95f4\u4e86? How long have you had a stomach ache? Doctor \u662f\u4e00\u76f4\u75db\u8fd8\u662f\u4e00\u9635\u4e00\u9635\u75bc\u75db? Is it constant pain or bouts of pain? Patient \u4e8c,\u4e09\u5929 two or three days Patient \u75bc\u75db\u8d77\u6765\u7279\u522b\u96be\u53d7"
        }
    ],
    "title": "DialogQAE: N-to-N Question Answer Pair Extraction from Customer Service Chatlog",
    "year": 2023
}