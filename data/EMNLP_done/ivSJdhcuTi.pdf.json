{
    "abstractText": "Machine learning (ML) systems in natural language processing (NLP) face significant challenges in generalizing to out-of-distribution (OOD) data, where the test distribution differs from the training data distribution. This poses important questions about the robustness of NLP models and their high accuracy, which may be artificially inflated due to their underlying sensitivity to systematic biases. Despite these challenges, there is a lack of comprehensive surveys on the generalization challenge from an OOD perspective in natural language understanding. Therefore, this paper aims to fill this gap by presenting the first comprehensive review of recent progress, methods, and evaluations on this topic. We further discuss the challenges involved and potential future research directions. By providing convenient access to existing work, we hope this survey will encourage future research in this area.",
    "authors": [
        {
            "affiliations": [],
            "name": "Linyi Yang"
        },
        {
            "affiliations": [],
            "name": "Yaoxiao Song"
        },
        {
            "affiliations": [],
            "name": "Xuan Ren"
        },
        {
            "affiliations": [],
            "name": "Chenyang Lyu"
        },
        {
            "affiliations": [],
            "name": "Yidong Wang"
        },
        {
            "affiliations": [],
            "name": "Jingming Zhuo"
        },
        {
            "affiliations": [],
            "name": "Lingqiao Liu"
        },
        {
            "affiliations": [],
            "name": "Jindong Wang"
        },
        {
            "affiliations": [],
            "name": "Jennifer Foster"
        },
        {
            "affiliations": [],
            "name": "Yue Zhang"
        }
    ],
    "id": "SP:4ae3c44b2b969733258e3262184c7b93ad483e44",
    "references": [
        {
            "authors": [
                "Kartik Ahuja",
                "Karthikeyan Shanmugam",
                "Kush Varshney",
                "Amit Dhurandhar."
            ],
            "title": "Invariant risk minimization games",
            "venue": "International Conference on Machine Learning, pages 145\u2013155. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Uri Alon",
                "Frank Xu",
                "Junxian He",
                "Sudipta Sengupta",
                "Dan Roth",
                "Graham Neubig."
            ],
            "title": "Neuro-symbolic language modeling with automaton-augmented retrieval",
            "venue": "International Conference on Machine Learning, pages 468\u2013485. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Andreas."
            ],
            "title": "Good-enough compositional data augmentation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7556\u20137566.",
            "year": 2020
        },
        {
            "authors": [
                "Naveen Arivazhagan",
                "Ankur Bapna",
                "Orhan Firat",
                "Roee Aharoni",
                "Melvin Johnson",
                "Wolfgang Macherey."
            ],
            "title": "The missing ingredient in zero-shot neural machine translation",
            "venue": "arXiv preprint arXiv:1903.07091.",
            "year": 2019
        },
        {
            "authors": [
                "Martin Arjovsky",
                "L\u00e9on Bottou",
                "Ishaan Gulrajani",
                "David Lopez-Paz."
            ],
            "title": "Invariant risk minimization",
            "venue": "arXiv preprint arXiv:1907.02893.",
            "year": 2019
        },
        {
            "authors": [
                "Udit Arora",
                "William Huang",
                "He He."
            ],
            "title": "Types of out-of-distribution texts and how to detect them",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10687\u201310701.",
            "year": 2021
        },
        {
            "authors": [
                "Anton Bakhtin",
                "Sam Gross",
                "Myle Ott",
                "Yuntian Deng",
                "Marc\u2019Aurelio Ranzato",
                "Arthur Szlam"
            ],
            "title": "Real or fake? learning to discriminate machine from human",
            "year": 2019
        },
        {
            "authors": [
                "Yejin Bang",
                "Samuel Cahyawijaya",
                "Nayeon Lee",
                "Wenliang Dai",
                "Dan Su",
                "Bryan Wilie",
                "Holy Lovenia",
                "Ziwei Ji",
                "Tiezheng Yu",
                "Willy Chung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity",
            "year": 2023
        },
        {
            "authors": [
                "Max Bartolo",
                "Alastair Roberts",
                "Johannes Welbl",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Beat the ai: Investigating adversarial human annotation for reading comprehension",
            "venue": "Transactions of the Association for Computational Linguistics, 8:662\u2013678.",
            "year": 2020
        },
        {
            "authors": [
                "Max Bartolo",
                "Tristan Thrush",
                "Robin Jia",
                "Sebastian Riedel",
                "Pontus Stenetorp",
                "Douwe Kiela."
            ],
            "title": "Improving question answering model robustness with synthetic adversarial data generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in",
            "year": 2021
        },
        {
            "authors": [
                "Max Bartolo",
                "Tristan Thrush",
                "Sebastian Riedel",
                "Pontus Stenetorp",
                "Robin Jia",
                "Douwe Kiela."
            ],
            "title": "Models in the loop: Aiding crowdworkers with generative annotation assistants",
            "venue": "arXiv preprint arXiv:2112.09062.",
            "year": 2021
        },
        {
            "authors": [
                "Jasmijn Bastings",
                "Sebastian Ebert",
                "Polina Zablotskaia",
                "Anders Sandholm",
                "Katja Filippova."
            ],
            "title": " will you find these shortcuts?\" a protocol for evaluating the faithfulness of input salience methods for text classification",
            "venue": "arXiv preprint arXiv:2111.07367.",
            "year": 2021
        },
        {
            "authors": [
                "Markus Bayer",
                "Marc-Andr\u00e9 Kaufhold",
                "Christian Reuter."
            ],
            "title": "A survey on data augmentation for text classification",
            "venue": "arXiv preprint arXiv:2107.03158.",
            "year": 2021
        },
        {
            "authors": [
                "Yonatan Belinkov",
                "Yonatan Bisk."
            ],
            "title": "Synthetic and natural noise both break neural machine translation",
            "venue": "arXiv preprint arXiv:1711.02173.",
            "year": 2017
        },
        {
            "authors": [
                "Shai Ben-David",
                "John Blitzer",
                "Koby Crammer",
                "Alex Kulesza",
                "Fernando Pereira",
                "Jennifer Wortman Vaughan."
            ],
            "title": "A theory of learning from different domains",
            "venue": "Machine learning, 79(1):151\u2013175.",
            "year": 2010
        },
        {
            "authors": [
                "John Blitzer",
                "Ryan McDonald",
                "Fernando Pereira."
            ],
            "title": "Domain adaptation with structural correspondence learning",
            "venue": "Proceedings of the 2006 conference on empirical methods in natural language processing, pages 120\u2013128.",
            "year": 2006
        },
        {
            "authors": [
                "Ben Bogin",
                "Sanjay Subramanian",
                "Matt Gardner",
                "Jonathan Berant."
            ],
            "title": "Latent compositional representations improve systematic generalization in grounded question answering",
            "venue": "Transactions of the Association for Computational Linguistics, 9:195\u2013210.",
            "year": 2021
        },
        {
            "authors": [
                "Tolga Bolukbasi",
                "Kai-Wei Chang",
                "James Y Zou",
                "Venkatesh Saligrama",
                "Adam T Kalai."
            ],
            "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
            "venue": "Advances in neural information processing systems, 29.",
            "year": 2016
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Zheng Cai",
                "Lifu Tu",
                "Kevin Gimpel."
            ],
            "title": "Pay attention to the ending: Strong neural baselines for the roc story cloze task",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 616\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Aylin Caliskan",
                "Joanna J Bryson",
                "Arvind Narayanan."
            ],
            "title": "Semantics derived automatically from language corpora contain human-like biases",
            "venue": "Science, 356(6334):183\u2013186.",
            "year": 2017
        },
        {
            "authors": [
                "Yupeng Chang",
                "Xu Wang",
                "Jindong Wang",
                "Yuan Wu",
                "Kaijie Zhu",
                "Hao Chen",
                "Linyi Yang",
                "Xiaoyuan Yi",
                "Cunxiang Wang",
                "Yidong Wang"
            ],
            "title": "A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109",
            "year": 2023
        },
        {
            "authors": [
                "Changyu Chen",
                "Xiting Wang",
                "Yiqiao Jin",
                "Victor Ye Dong",
                "Li Dong",
                "Jie Cao",
                "Yi Liu",
                "Rui Yan."
            ],
            "title": "Semi-offline reinforcement learning for optimized text generation",
            "venue": "arXiv preprint arXiv:2306.09712.",
            "year": 2023
        },
        {
            "authors": [
                "Howard Chen",
                "Jacqueline He",
                "Karthik Narasimhan",
                "Danqi Chen"
            ],
            "title": "2022a. Can rationalization improve robustness? arXiv preprint arXiv:2204.11790",
            "year": 2022
        },
        {
            "authors": [
                "Jiaao Chen",
                "Derek Tam",
                "Colin Raffel",
                "Mohit Bansal",
                "Diyi Yang."
            ],
            "title": "An empirical survey of data augmentation for limited data learning in nlp",
            "venue": "arXiv preprint arXiv:2106.07499.",
            "year": 2021
        },
        {
            "authors": [
                "Jiawei Chen",
                "Qing Liu",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun."
            ],
            "title": "Few-shot named entity recognition with self-describing networks",
            "venue": "arXiv preprint arXiv:2203.12252.",
            "year": 2022
        },
        {
            "authors": [
                "Shuguang Chen",
                "Gustavo Aguilar",
                "Leonardo Neves",
                "Thamar Solorio."
            ],
            "title": "Data augmentation for crossdomain named entity recognition",
            "venue": "arXiv preprint arXiv:2109.01758.",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Chen",
                "Ningyu Zhang",
                "Lei Li",
                "Xin Xie",
                "Shumin Deng",
                "Chuanqi Tan",
                "Fei Huang",
                "Luo Si",
                "Huajun Chen."
            ],
            "title": "Lightner: A lightweight generative framework with prompt-guided attention for low-resource ner",
            "venue": "arXiv preprint arXiv:2109.00720.",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Chen",
                "Ningyu Zhang",
                "Xin Xie",
                "Shumin Deng",
                "Yunzhi Yao",
                "Chuanqi Tan",
                "Fei Huang",
                "Luo Si",
                "Huajun Chen."
            ],
            "title": "Knowprompt: Knowledgeaware prompt-tuning with synergistic optimization for relation extraction",
            "venue": "Proceedings of the ACM",
            "year": 2022
        },
        {
            "authors": [
                "Xilun Chen",
                "Claire Cardie."
            ],
            "title": "Multinomial adversarial networks for multi-domain text classification",
            "venue": "arXiv preprint arXiv:1802.05694.",
            "year": 2018
        },
        {
            "authors": [
                "Xinyun Chen",
                "Chen Liang",
                "Adams Wei Yu",
                "Dawn Song",
                "Denny Zhou."
            ],
            "title": "Compositional generalization via neural-symbolic stack machines",
            "venue": "Advances in Neural Information Processing Systems, 33:1690\u2013 1701.",
            "year": 2020
        },
        {
            "authors": [
                "Yongqiang Chen",
                "Kaiwen Zhou",
                "Yatao Bian",
                "Binghui Xie",
                "Bingzhe Wu",
                "Yonggang Zhang",
                "MA KAILI",
                "Han Yang",
                "Peilin Zhao",
                "Bo Han"
            ],
            "title": "2023b. Pareto invariant risk minimization: Towards mitigating the optimization dilemma in out-of-distribution",
            "year": 2023
        },
        {
            "authors": [
                "Yong Cheng",
                "Lu Jiang",
                "Wolfgang Macherey."
            ],
            "title": "Robust neural machine translation with doubly adversarial inputs",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4324\u20134333, Florence, Italy. Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Yo Joong Choe",
                "Jiyeon Ham",
                "Kyubyong Park."
            ],
            "title": "An empirical study of invariant risk minimization",
            "venue": "arXiv preprint arXiv:2004.05007.",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan H Choi",
                "Kristin E Hickman",
                "Amy Monahan",
                "Daniel Schwarcz."
            ],
            "title": "Chatgpt goes to law school",
            "venue": "Available at SSRN.",
            "year": 2023
        },
        {
            "authors": [
                "Prafulla Kumar Choubey",
                "Anna Currey",
                "Prashant Mathur",
                "Georgiana Dinu."
            ],
            "title": "Improving gender translation accuracy with filtered self-training",
            "venue": "arXiv preprint arXiv:2104.07695.",
            "year": 2021
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Tom Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei."
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Jacob Hilton",
                "Reiichiro Nakano",
                "Christopher Hesse",
                "John Schulman."
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "arXiv preprint arXiv:2110.14168.",
            "year": 2021
        },
        {
            "authors": [
                "Leyang Cui",
                "Yu Wu",
                "Jian Liu",
                "Sen Yang",
                "Yue Zhang."
            ],
            "title": "Template-based named entity recognition using bart",
            "venue": "arXiv preprint arXiv:2106.01760.",
            "year": 2021
        },
        {
            "authors": [
                "Paula Czarnowska",
                "Sebastian Ruder",
                "\u00c9douard Grave",
                "Ryan Cotterell",
                "Ann Copestake."
            ],
            "title": "Don\u2019t forget the long tail! a comprehensive analysis of morphological generalization in bilingual lexicon induction",
            "venue": "Proceedings of the 2019 Conference on",
            "year": 2019
        },
        {
            "authors": [
                "Verna Dankers",
                "Elia Bruni",
                "Dieuwke Hupkes."
            ],
            "title": "The paradox of the compositionality of natural language: a neural machine translation case study",
            "venue": "arXiv preprint arXiv:2108.05885.",
            "year": 2021
        },
        {
            "authors": [
                "Sarkar Snigdha Sarathi Das",
                "Arzoo Katiyar",
                "Rebecca J Passonneau",
                "Rui Zhang."
            ],
            "title": "Container: Fewshot named entity recognition via contrastive learning",
            "venue": "arXiv preprint arXiv:2109.07589.",
            "year": 2021
        },
        {
            "authors": [
                "Hal Daum\u00e9 III."
            ],
            "title": "Frustratingly easy domain adaptation",
            "venue": "arXiv preprint arXiv:0907.1815.",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Mark Diaz",
                "Isaac Johnson",
                "Amanda Lazar",
                "Anne Marie Piper",
                "Darren Gergle."
            ],
            "title": "Addressing agerelated bias in sentiment analysis",
            "venue": "Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. Association for Computing Ma-",
            "year": 2018
        },
        {
            "authors": [
                "Lucas Dixon",
                "John Li",
                "Jeffrey Scott Sorensen",
                "Nithum Thain",
                "Lucy Vasserman."
            ],
            "title": "Measuring and mitigating unintended bias in text classification",
            "venue": "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society.",
            "year": 2018
        },
        {
            "authors": [
                "Li Dong",
                "Mirella Lapata."
            ],
            "title": "Language to logical form with neural attention",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 33\u201343.",
            "year": 2016
        },
        {
            "authors": [
                "Li Dong",
                "Mirella Lapata."
            ],
            "title": "Coarse-to-fine decoding for neural semantic parsing",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018), pages 731\u2013 742.",
            "year": 2018
        },
        {
            "authors": [
                "Yana Dranker",
                "He He",
                "Yonatan Belinkov."
            ],
            "title": "Irm\u2014when it works and when it doesn\u2019t: A test case of natural language inference",
            "venue": "Advances in Neural Information Processing Systems, 34.",
            "year": 2021
        },
        {
            "authors": [
                "Iddo Drori",
                "Sunny Tran",
                "Roman Wang",
                "Newman Cheng",
                "Kevin Liu",
                "Leonard Tang",
                "Elizabeth Ke",
                "Nikhil Singh",
                "Taylor L Patti",
                "Jayson Lynch"
            ],
            "title": "A neural network solves and generates mathematics problems by program synthesis: Calculus, differential",
            "year": 2021
        },
        {
            "authors": [
                "Chunning Du",
                "Haifeng Sun",
                "Jingyu Wang",
                "Qi Qi",
                "Jianxin Liao."
            ],
            "title": "Adversarial and domain-aware bert for cross-domain sentiment analysis",
            "venue": "Proceedings of the 58th annual meeting of the Association for Computational Linguistics, pages 4019\u20134028.",
            "year": 2020
        },
        {
            "authors": [
                "Mengnan Du",
                "Varun Manjunatha",
                "Rajiv Jain",
                "Ruchi Deshpande",
                "Franck Dernoncourt",
                "Jiuxiang Gu",
                "Tong Sun",
                "Xia Hu"
            ],
            "title": "Towards interpreting and mitigating shortcut learning behavior of nlu models",
            "year": 2021
        },
        {
            "authors": [
                "Yuntao Du",
                "Jindong Wang",
                "Wenjie Feng",
                "Sinno Pan",
                "Tao Qin",
                "Renjun Xu",
                "Chongjun Wang."
            ],
            "title": "Adarnn: Adaptive learning and forecasting of time series",
            "venue": "Proceedings of the 30th ACM International Conference on Information & Knowledge Manage-",
            "year": 2021
        },
        {
            "authors": [
                "Akiko Eriguchi",
                "Melvin Johnson",
                "Orhan Firat",
                "Hideto Kazawa",
                "Wolfgang Macherey."
            ],
            "title": "Zeroshot cross-lingual classification using multilingual neural machine translation",
            "venue": "arXiv preprint arXiv:1809.04686.",
            "year": 2018
        },
        {
            "authors": [
                "T Fagni",
                "F Falchi",
                "M Gambini",
                "A Martella",
                "M Tesconi"
            ],
            "title": "Tweepfake: About detecting deepfake tweets",
            "venue": "PLOS ONE,",
            "year": 2021
        },
        {
            "authors": [
                "Caoyun Fan",
                "Wenqing Chen",
                "Jidong Tian",
                "Yitian Li",
                "Hao He",
                "Yaohui Jin."
            ],
            "title": "Improving the out-of-distribution generalization capability of language models: Counterfactually-augmented data is not enough",
            "venue": "ICASSP 2023-2023 IEEE Interna-",
            "year": 2023
        },
        {
            "authors": [
                "Shi Feng",
                "Eric Wallace",
                "Jordan Boyd-Graber."
            ],
            "title": "Misleading failures of partial-input baselines",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5533\u2013 5538.",
            "year": 2019
        },
        {
            "authors": [
                "Steven Y Feng",
                "Varun Gangal",
                "Jason Wei",
                "Sarath Chandar",
                "Soroush Vosoughi",
                "Teruko Mitamura",
                "Eduard Hovy."
            ],
            "title": "A survey of data augmentation approaches for nlp",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Stanislav Fort",
                "Jie Ren",
                "Balaji Lakshminarayanan."
            ],
            "title": "Exploring the limits of out-of-distribution detection",
            "venue": "Advances in Neural Information Processing Systems, 34.",
            "year": 2021
        },
        {
            "authors": [
                "Dan Friedman",
                "Ben Dodge",
                "Danqi Chen."
            ],
            "title": "Single-dataset experts for multi-dataset question answering",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6128\u20136137.",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Furrer",
                "Marc van Zee",
                "Nathan Scales",
                "Nathanael Sch\u00e4rli."
            ],
            "title": "Compositional generalization in semantic parsing: Pre-training vs",
            "venue": "specialized architectures. arXiv preprint arXiv:2007.08970.",
            "year": 2020
        },
        {
            "authors": [
                "Jean-Christophe Gagnon-Audet",
                "Kartik Ahuja",
                "Mohammad-Javad Darvishi-Bayazi",
                "Guillaume Dumas",
                "Irina Rish."
            ],
            "title": "Woods: Benchmarks for out-of-distribution generalization in time series tasks",
            "venue": "arXiv preprint arXiv:2203.09978.",
            "year": 2022
        },
        {
            "authors": [
                "Yaroslav Ganin",
                "Evgeniya Ustinova",
                "Hana Ajakan",
                "Pascal Germain",
                "Hugo Larochelle",
                "Fran\u00e7ois Laviolette",
                "Mario Marchand",
                "Victor Lempitsky."
            ],
            "title": "Domain-adversarial training of neural networks",
            "venue": "The journal of machine learning research, 17(1):2096\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen."
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "ArXiv, abs/2012.15723.",
            "year": 2021
        },
        {
            "authors": [
                "Matt Gardner",
                "Yoav Artzi",
                "Victoria Basmov",
                "Jonathan Berant",
                "Ben Bogin",
                "Sihao Chen",
                "Pradeep Dasigi",
                "Dheeru Dua",
                "Yanai Elazar",
                "Ananth Gottumukkala"
            ],
            "title": "Evaluating models\u2019 local decision boundaries via contrast sets",
            "venue": "In Findings of the Association",
            "year": 2020
        },
        {
            "authors": [
                "Matt Gardner",
                "William Merrill",
                "Jesse Dodge",
                "Matthew E Peters",
                "Alexis Ross",
                "Sameer Singh",
                "Noah A Smith."
            ],
            "title": "Competency problems: On finding and removing artifacts in language data",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods",
            "year": 2021
        },
        {
            "authors": [
                "Saurabh Garg",
                "Sivaraman Balakrishnan",
                "Zachary Lipton."
            ],
            "title": "Domain adaptation under open set label shift",
            "venue": "Advances in Neural Information Processing Systems, 35:22531\u201322546.",
            "year": 2022
        },
        {
            "authors": [
                "Robert Geirhos",
                "J\u00f6rn-Henrik Jacobsen",
                "Claudio Michaelis",
                "Richard Zemel",
                "Wieland Brendel",
                "Matthias Bethge",
                "Felix A Wichmann."
            ],
            "title": "Shortcut learning in deep neural networks",
            "venue": "Nature Machine Intelligence, 2(11):665\u2013673.",
            "year": 2020
        },
        {
            "authors": [
                "Abbas Ghaddar",
                "Philippe Langlais."
            ],
            "title": "Winer: A wikipedia annotated corpus for named entity recognition",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 413\u2013422.",
            "year": 2017
        },
        {
            "authors": [
                "Hila Gonen",
                "Yoav Goldberg."
            ],
            "title": "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy."
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "CoRR, abs/1412.6572.",
            "year": 2015
        },
        {
            "authors": [
                "Jonathan Gordon",
                "David Lopez-Paz",
                "Marco Baroni",
                "Diane Bouchacourt."
            ],
            "title": "Permutation equivariant models for compositional generalization in language",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Roberto Gozalo-Brizuela",
                "Eduardo C GarridoMerchan."
            ],
            "title": "Chatgpt is not all you need",
            "venue": "a state of the art review of large generative ai models. arXiv preprint arXiv:2301.04655.",
            "year": 2023
        },
        {
            "authors": [
                "Yu Gu",
                "Sue Kase",
                "Michelle Vanni",
                "Brian Sadler",
                "Percy Liang",
                "Xifeng Yan",
                "Yu Su."
            ],
            "title": "Beyond iid: three levels of generalization for question answering on knowledge bases",
            "venue": "Proceedings of the Web Conference 2021, pages 3477\u20133488.",
            "year": 2021
        },
        {
            "authors": [
                "Biyang Guo",
                "Xin Zhang",
                "Ziyuan Wang",
                "Minqi Jiang",
                "Jinran Nie",
                "Yuxuan Ding",
                "Jianwei Yue",
                "Yupeng Wu."
            ],
            "title": "How close is chatgpt to human experts? comparison corpus, evaluation, and detection",
            "venue": "arXiv preprint arXiv:2301.07597.",
            "year": 2023
        },
        {
            "authors": [
                "Fang Guo",
                "Yun Luo",
                "Linyi Yang",
                "Yue Zhang."
            ],
            "title": "Scimine: An efficient systematic prioritization model based on richer semantic information",
            "venue": "Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information",
            "year": 2023
        },
        {
            "authors": [
                "Han Guo",
                "Ramakanth Pasunuru",
                "Mohit Bansal."
            ],
            "title": "Multi-source domain adaptation for text classification via distancenet-bandits",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7830\u20137838.",
            "year": 2020
        },
        {
            "authors": [
                "Shivanshu Gupta",
                "Sameer Singh",
                "Matt Gardner."
            ],
            "title": "Structurally diverse sampling reduces spurious correlations in semantic parsing datasets",
            "venue": "arXiv preprint arXiv:2203.08445.",
            "year": 2022
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Swabha Swayamdipta",
                "Omer Levy",
                "Roy Schwartz",
                "Samuel Bowman",
                "Noah A Smith."
            ],
            "title": "Annotation artifacts in natural language inference data",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Kyle Hamilton",
                "Aparna Nayak",
                "Bojan Bo\u017ei\u0107",
                "Luca Longo."
            ],
            "title": "Is neuro-symbolic ai meeting its promises in natural language processing? a structured review",
            "venue": "Semantic Web, (Preprint):1\u201342.",
            "year": 2022
        },
        {
            "authors": [
                "Xiaochuang Han",
                "Jacob Eisenstein."
            ],
            "title": "Unsupervised domain adaptation of contextualized embeddings for sequence labeling",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Michael A Hedderich",
                "Lukas Lange",
                "Heike Adel",
                "Jannik Str\u00f6tgen",
                "Dietrich Klakow."
            ],
            "title": "A survey on recent approaches for natural language processing in low-resource scenarios",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Saurav Kadavath",
                "Akul Arora",
                "Steven Basart",
                "Eric Tang",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring mathematical problem solving with the math dataset",
            "venue": "arXiv preprint arXiv:2103.03874.",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel."
            ],
            "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Xiaoyuan Liu",
                "Eric Wallace",
                "Adam Dziedzic",
                "Rishabh Krishnan",
                "Dawn Song."
            ],
            "title": "Pretrained transformers improve out-of-distribution robustness",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
            "year": 2020
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Mantas Mazeika",
                "Thomas Dietterich."
            ],
            "title": "Deep anomaly detection with outlier exposure",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "John Hewitt",
                "Xiang Lisa Li",
                "Sang Michael Xie",
                "Benjamin Newman",
                "Percy Liang."
            ],
            "title": "Ensembles and cocktails: Robust finetuning for natural language generation",
            "venue": "NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications.",
            "year": 2021
        },
        {
            "authors": [
                "Paul Holland."
            ],
            "title": "Statistics and causal inference",
            "venue": "Journal of the American Statistical Association, 81:945\u2013960.",
            "year": 1986
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "International Conference on Machine Learning, pages",
            "year": 2019
        },
        {
            "authors": [
                "Junjie Hu",
                "Sebastian Ruder",
                "Aditya Siddhant",
                "Graham Neubig",
                "Orhan Firat",
                "Melvin Johnson."
            ],
            "title": "Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation",
            "venue": "International Conference on Machine Learn-",
            "year": 2020
        },
        {
            "authors": [
                "Xiusheng Huang",
                "Yubo Chen",
                "Kang Liu",
                "Yuantao Xie",
                "Weijian Sun",
                "Jun Zhao."
            ],
            "title": "Nsrl: Named entity recognition with noisy labels via selective review learning",
            "venue": "China Conference on Knowledge Graph and Semantic Computing, pages 157\u2013170. Springer.",
            "year": 2021
        },
        {
            "authors": [
                "Yinya Huang",
                "Meng Fang",
                "Yu Cao",
                "Liwei Wang",
                "Xiaodan Liang."
            ],
            "title": "Dagn: Discourse-aware graph network for logical reasoning",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "Dieuwke Hupkes",
                "Verna Dankers",
                "Mathijs Mul",
                "Elia Bruni"
            ],
            "title": "Compositionality decomposed: How do neural networks generalise",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2020
        },
        {
            "authors": [
                "Dieuwke Hupkes",
                "Mario Giulianelli",
                "Verna Dankers",
                "Mikel Artetxe",
                "Yanai Elazar",
                "Tiago Pimentel",
                "Christos Christodoulopoulos",
                "Karim Lasri",
                "Naomi Saphra",
                "Arabella Sinclair"
            ],
            "title": "State-of-the-art generalisation research in nlp: a taxonomy and review",
            "year": 2022
        },
        {
            "authors": [
                "Ben Hutchinson",
                "Vinodkumar Prabhakaran",
                "Emily Denton",
                "Kellie Webster",
                "Yu Zhong",
                "Stephen Denuyl."
            ],
            "title": "Social biases in NLP models as barriers for persons with disabilities",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Guido Imbens",
                "Donald B. Rubin"
            ],
            "title": "Causal inference for statistics, social, and biomedical sciences: An introduction",
            "year": 2015
        },
        {
            "authors": [
                "Srinivasan Iyer",
                "Ioannis Konstas",
                "Alvin Cheung",
                "Jayant Krishnamurthy",
                "Luke Zettlemoyer."
            ],
            "title": "Learning a neural semantic parser from user feedback",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2017
        },
        {
            "authors": [
                "Baijun Ji",
                "Zhirui Zhang",
                "Xiangyu Duan",
                "Min Zhang",
                "Boxing Chen",
                "Weihua Luo."
            ],
            "title": "Cross-lingual pre-training based transfer for zero-shot neural machine translation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages",
            "year": 2020
        },
        {
            "authors": [
                "Chen Jia",
                "Xiaobo Liang",
                "Yue Zhang."
            ],
            "title": "Crossdomain ner using cross-domain language modeling",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2464\u2013 2474.",
            "year": 2019
        },
        {
            "authors": [
                "Chen Jia",
                "Yue Zhang."
            ],
            "title": "Multi-cell compositional lstm for ner domain adaptation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5906\u20135917.",
            "year": 2020
        },
        {
            "authors": [
                "Haoming Jiang",
                "Pengcheng He",
                "Weizhu Chen",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Tuo Zhao."
            ],
            "title": "Smart: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization",
            "venue": "Proceedings of the 58th",
            "year": 2020
        },
        {
            "authors": [
                "Zhongtao Jiang",
                "Yuanzhe Zhang",
                "Zhao Yang",
                "Jun Zhao",
                "Kang Liu."
            ],
            "title": "Alignment rationale for natural language inference",
            "venue": "ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Rie Johnson",
                "Tong Zhang."
            ],
            "title": "A highperformance semi-supervised learning method for text chunking",
            "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 1\u20139.",
            "year": 2005
        },
        {
            "authors": [
                "Alexander Jung."
            ],
            "title": "Machine learning: The basics",
            "venue": "Springer Nature.",
            "year": 2022
        },
        {
            "authors": [
                "Jaehun Jung",
                "Lianhui Qin",
                "Sean Welleck",
                "Faeze Brahman",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "Maieutic prompting: Logically consistent reasoning with recursive explanations",
            "venue": "arXiv preprint arXiv:2205.11822.",
            "year": 2022
        },
        {
            "authors": [
                "Divyansh Kaushik",
                "Eduard Hovy",
                "Zachary Lipton."
            ],
            "title": "Learning the difference that makes a difference with counterfactually-augmented data",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Divyansh Kaushik",
                "Zachary C Lipton."
            ],
            "title": "How much reading does reading comprehension require? a critical investigation of popular benchmarks",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5010\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Divyansh Kaushik",
                "Amrith Setlur",
                "Eduard H Hovy",
                "Zachary Chase Lipton."
            ],
            "title": "Explaining the efficacy of counterfactually augmented data",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Kenji Kawaguchi",
                "Leslie Pack Kaelbling",
                "Yoshua Bengio."
            ],
            "title": "Generalization in deep learning",
            "venue": "arXiv preprint arXiv:1710.05468.",
            "year": 2017
        },
        {
            "authors": [
                "Katherine Keith",
                "David Jensen",
                "Brendan O\u2019Connor"
            ],
            "title": "Text and causal inference: A review of using text to remove confounding from causal estimates",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Phillip Keung",
                "Yichao Lu",
                "Vikas Bhardwaj."
            ],
            "title": "Adversarial learning with contextual embeddings for zero-resource cross-lingual classification and NER",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Keysers",
                "Nathanael Sch\u00e4rli",
                "Nathan Scales",
                "Hylke Buisman",
                "Daniel Furrer",
                "Sergii Kashubin",
                "Nikola Momchev",
                "Danila Sinopalnikov",
                "Lukasz Stafiniak",
                "Tibor Tihon"
            ],
            "title": "Measuring compositional generalization: A comprehensive method",
            "year": 2020
        },
        {
            "authors": [
                "Huda Khayrallah",
                "Philipp Koehn."
            ],
            "title": "On the impact of various types of noise on neural machine translation",
            "venue": "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 74\u201383, Melbourne, Australia. Association for Com-",
            "year": 2018
        },
        {
            "authors": [
                "Juyong Kim",
                "Pradeep Ravikumar",
                "Joshua Ainslie",
                "Santiago Onta\u00f1\u00f3n."
            ],
            "title": "Improving compositional generalization in classification tasks via structure annotations",
            "venue": "arXiv preprint arXiv:2106.10434.",
            "year": 2021
        },
        {
            "authors": [
                "Najoung Kim",
                "Tal Linzen"
            ],
            "title": "Cogs: A compositional generalization challenge based on",
            "year": 2020
        },
        {
            "authors": [
                "Yoon Kim."
            ],
            "title": "Sequence-to-sequence learning with latent neural grammars",
            "venue": "Advances in Neural Information Processing Systems, 34.",
            "year": 2021
        },
        {
            "authors": [
                "Svetlana Kiritchenko",
                "Saif M Mohammad."
            ],
            "title": "Examining gender and race bias in two hundred sentiment analysis systems",
            "venue": "arXiv preprint arXiv:1805.04508.",
            "year": 2018
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Shiori Sagawa",
                "Henrik Marklund",
                "Sang Michael Xie",
                "Marvin Zhang",
                "Akshay Balsubramani",
                "Weihua Hu",
                "Michihiro Yasunaga",
                "Richard Lanas Phillips",
                "Irena Gao"
            ],
            "title": "Wilds: A benchmark of in-the-wild distribution shifts",
            "year": 2021
        },
        {
            "authors": [
                "Ananya Kumar",
                "Aditi Raghunathan",
                "Robbie Jones",
                "Tengyu Ma",
                "Percy Liang."
            ],
            "title": "Fine-tuning can distort pretrained features and underperform out-ofdistribution",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Yuxuan Lai",
                "Chen Zhang",
                "Yansong Feng",
                "Quzhe Huang",
                "Dongyan Zhao"
            ],
            "title": "Why machine reading comprehension models learn shortcuts? In Findings of the Association for Computational Linguistics: ACL-IJCNLP",
            "year": 2021
        },
        {
            "authors": [
                "Brenden Lake",
                "Marco Baroni."
            ],
            "title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
            "venue": "International conference on machine learning, pages 2873\u20132882. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "Brenden M Lake."
            ],
            "title": "Compositional generalization through meta sequence-to-sequence learning",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Egoitz Laparra",
                "Steven Bethard",
                "Timothy A Miller."
            ],
            "title": "Rethinking domain adaptation for machine learning over clinical language",
            "venue": "JAMIA open, 3(2):146\u2013150.",
            "year": 2020
        },
        {
            "authors": [
                "Angeliki Lazaridou",
                "Adhi Kuncoro",
                "Elena Gribovskaya",
                "Devang Agrawal",
                "Adam Liska",
                "Tayfun Terzi",
                "Mai Gimenez",
                "Cyprien de Masson d\u2019Autume",
                "Tomas Kocisky",
                "Sebastian Ruder"
            ],
            "title": "Mind the gap: Assessing temporal generalization in neural language",
            "year": 2021
        },
        {
            "authors": [
                "Hang Le",
                "Juan Pino",
                "Changhan Wang",
                "Jiatao Gu",
                "Didier Schwab",
                "Laurent Besacier."
            ],
            "title": "Lightweight adapter tuning for multilingual speech translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th",
            "year": 2021
        },
        {
            "authors": [
                "Ronan Le Bras",
                "Swabha Swayamdipta",
                "Chandra Bhagavatula",
                "Rowan Zellers",
                "Matthew Peters",
                "Ashish Sabharwal",
                "Yejin Choi."
            ],
            "title": "Adversarial filters of dataset biases",
            "venue": "International Conference on Machine Learning, pages 1078\u20131088. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Dong-Ho Lee",
                "Mahak Agarwal",
                "Akshen Kadakia",
                "Jay Pujara",
                "Xiang Ren."
            ],
            "title": "Good examples make a faster learner: Simple demonstrationbased learning for low-resource ner",
            "venue": "arXiv preprint arXiv:2110.08454.",
            "year": 2021
        },
        {
            "authors": [
                "Seanie Lee",
                "Donggyu Kim",
                "Jangwon Park."
            ],
            "title": "Domain-agnostic question-answering with adversarial training",
            "venue": "Proceedings of the 2nd Workshop on Machine Reading for Question Answering. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059.",
            "year": 2021
        },
        {
            "authors": [
                "Omer Levy",
                "Minjoon Seo",
                "Eunsol Choi",
                "Luke Zettlemoyer."
            ],
            "title": "Zero-shot relation extraction via reading comprehension",
            "venue": "Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333\u2013342.",
            "year": 2017
        },
        {
            "authors": [
                "Patrick Lewis",
                "Barlas Oguz",
                "Ruty Rinott",
                "Sebastian Riedel",
                "Holger Schwenk."
            ],
            "title": "Mlqa: Evaluating cross-lingual extractive question answering",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7315\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Lewis",
                "Pontus Stenetorp",
                "Sebastian Riedel."
            ],
            "title": "Question and answer test-train overlap in opendomain question answering datasets",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main",
            "year": 2021
        },
        {
            "authors": [
                "Bohan Li",
                "Yutai Hou",
                "Wanxiang Che."
            ],
            "title": "Data augmentation approaches in natural language processing: A survey",
            "venue": "AI Open.",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Pro-",
            "year": 2021
        },
        {
            "authors": [
                "Yafu Li",
                "Yongjing Yin",
                "Yulong Chen",
                "Yue Zhang."
            ],
            "title": "On compositional generalization of neural machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Yuanpeng Li",
                "Liang Zhao",
                "Jianyu Wang",
                "Joel Hestness."
            ],
            "title": "Compositional generalization for primitive substitutions",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference",
            "year": 2019
        },
        {
            "authors": [
                "Zheng Li",
                "Xin Li",
                "Ying Wei",
                "Lidong Bing",
                "Yu Zhang",
                "Qiang Yang."
            ],
            "title": "Transferable end-to-end aspect-based sentiment analysis with selective adversarial learning",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Danni Liu",
                "Jan Niehues",
                "James Cross",
                "Francisco Guzm\u00e1n",
                "Xian Li."
            ],
            "title": "Improving zero-shot translation by disentangling positional information",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
            "year": 2021
        },
        {
            "authors": [
                "Evan Z Liu",
                "Behzad Haghgoo",
                "Annie S Chen",
                "Aditi Raghunathan",
                "Pang Wei Koh",
                "Shiori Sagawa",
                "Percy Liang",
                "Chelsea Finn."
            ],
            "title": "Just train twice: Improving group robustness without training group information",
            "venue": "International Conference on Ma-",
            "year": 2021
        },
        {
            "authors": [
                "Jian Liu",
                "Leyang Cui",
                "Hanmeng Liu",
                "Dandan Huang",
                "Yile Wang",
                "Yue Zhang."
            ],
            "title": "Logiqa: a challenge dataset for machine reading comprehension with logical reasoning",
            "venue": "Proceedings of the Twenty-Ninth International Conference on Interna-",
            "year": 2021
        },
        {
            "authors": [
                "Kun Liu",
                "Yao Fu",
                "Chuanqi Tan",
                "Mosha Chen",
                "Ningyu Zhang",
                "Songfang Huang",
                "Sheng Gao."
            ],
            "title": "Noisy-labeled ner with confidence estimation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Linqing Liu",
                "Patrick Lewis",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Challenges in generalization in open domain question answering",
            "venue": "arXiv preprint arXiv:2109.01156.",
            "year": 2021
        },
        {
            "authors": [
                "Nelson F. Liu",
                "Matt Gardner",
                "Yonatan Belinkov",
                "Matthew E. Peters",
                "Noah A. Smith."
            ],
            "title": "Linguistic knowledge and transferability of contextual representations",
            "venue": "Proceedings of the Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig"
            ],
            "title": "2021f. Pretrain, prompt, and predict: A systematic survey",
            "year": 2021
        },
        {
            "authors": [
                "Qi Liu",
                "Yue Zhang",
                "Jiangming Liu."
            ],
            "title": "Learning domain representation for multi-domain sentiment classification",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2018
        },
        {
            "authors": [
                "Tianyu Liu",
                "Zheng Xin",
                "Baobao Chang",
                "Zhifang Sui."
            ],
            "title": "HypoNLI: Exploring the artificial patterns of hypothesis-only bias in natural language inference",
            "venue": "Proceedings of the 12th Language Resources and Evaluation Conference, pages 6852\u20136860, Marseille,",
            "year": 2020
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
            "venue": "arXiv preprint arXiv:2110.07602.",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zihan Liu",
                "Yan Xu",
                "Tiezheng Yu",
                "Wenliang Dai",
                "Ziwei Ji",
                "Samuel Cahyawijaya",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Crossner: Evaluating crossdomain named entity recognition",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Jinghui Lu",
                "Linyi Yang",
                "Brian Mac Namee",
                "Yue Zhang."
            ],
            "title": "A rationale-centric framework for human-in-the-loop machine learning",
            "venue": "arXiv preprint arXiv:2203.12918.",
            "year": 2022
        },
        {
            "authors": [
                "Kaiji Lu",
                "Piotr Mardziel",
                "Fangjing Wu",
                "Preetam Amancharla",
                "Anupam Datta."
            ],
            "title": "Gender bias in neural natural language processing",
            "venue": "Logic, Language, and Security, pages 189\u2013202. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Yun Luo",
                "Fang Guo",
                "Zihan Liu",
                "Yue Zhang."
            ],
            "title": "Mere contrastive learning for cross-domain sentiment analysis",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 7099\u20137111.",
            "year": 2022
        },
        {
            "authors": [
                "Yun Luo",
                "Zihan Liu",
                "Yuefeng Shi",
                "Stan Z Li",
                "Yue Zhang."
            ],
            "title": "Exploiting sentiment and common sense for zero-shot stance detection",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 7112\u20137123.",
            "year": 2022
        },
        {
            "authors": [
                "Chenyang Lyu",
                "Jennifer Foster",
                "Yvette Graham."
            ],
            "title": "Extending the scope of out-of-domain: Examining qa models in multiple subdomains",
            "venue": "arXiv preprint arXiv:2204.04534.",
            "year": 2022
        },
        {
            "authors": [
                "Ruotian Ma",
                "Xin Zhou",
                "Tao Gui",
                "Yiding Tan",
                "Qi Zhang",
                "Xuanjing Huang."
            ],
            "title": "Templatefree prompt tuning for few-shot ner",
            "venue": "arXiv preprint arXiv:2109.13532.",
            "year": 2021
        },
        {
            "authors": [
                "Robin Manhaeve",
                "Sebastijan Dumancic",
                "Angelika Kimmig",
                "Thomas Demeester",
                "Luc De Raedt."
            ],
            "title": "Deepproblog: Neural probabilistic logic programming",
            "venue": "advances in neural information processing systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Rowan Hall Maudslay",
                "Hila Gonen",
                "Ryan Cotterell",
                "Simone Teufel."
            ],
            "title": "It\u2019s all in the name: Mitigating gender bias with name-based counterfactual data substitution",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Chandler May",
                "Alex Wang",
                "Shikha Bordia",
                "Samuel Bowman",
                "Rachel Rudinger."
            ],
            "title": "On measuring social biases in sentence encoders",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2019
        },
        {
            "authors": [
                "Tom McCoy",
                "Ellie Pavlick",
                "Tal Linzen."
            ],
            "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428\u20133448.",
            "year": 2019
        },
        {
            "authors": [
                "Zhengjie Miao",
                "Yuliang Li",
                "Xiaolan Wang",
                "WangChiew Tan."
            ],
            "title": "Snippext: Semi-supervised opinion mining with augmented data",
            "venue": "Proceedings of The Web Conference 2020, pages 617\u2013628.",
            "year": 2020
        },
        {
            "authors": [
                "Paul Michel",
                "Tatsunori Hashimoto",
                "Graham Neubig."
            ],
            "title": "Modeling the second player in distributionally robust optimization",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Sewon Min",
                "Eric Wallace",
                "Sameer Singh",
                "Matt Gardner",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer."
            ],
            "title": "Compositional questions do not necessitate multi-hop reasoning",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Eric Mitchell",
                "Yoonho Lee",
                "Alexander Khazatsky",
                "Christopher D. Manning",
                "Chelsea Finn."
            ],
            "title": "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
            "venue": "CoRR, abs/2301.11305.",
            "year": 2023
        },
        {
            "authors": [
                "Takeru Miyato",
                "Andrew M. Dai",
                "Ian J. Goodfellow."
            ],
            "title": "Adversarial training methods for semisupervised text classification",
            "venue": "arXiv: Machine Learning.",
            "year": 2017
        },
        {
            "authors": [
                "Milad Moradi",
                "Kathrin Blagec",
                "Matthias Samwald."
            ],
            "title": "Deep learning models are not robust against noise in clinical text",
            "venue": "arXiv preprint arXiv:2108.12242.",
            "year": 2021
        },
        {
            "authors": [
                "Milad Moradi",
                "Matthias Samwald."
            ],
            "title": "Evaluating the robustness of neural language models to input perturbations",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1558\u20131570.",
            "year": 2021
        },
        {
            "authors": [
                "Stephen L Morgan",
                "Christopher Winship."
            ],
            "title": "Counterfactuals and causal inference",
            "venue": "Cambridge University Press.",
            "year": 2015
        },
        {
            "authors": [
                "Aakanksha Naik",
                "Abhilasha Ravichander",
                "Norman Sadeh",
                "Carolyn Rose",
                "Graham Neubig."
            ],
            "title": "Stress test evaluation for natural language inference",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 2340\u20132353.",
            "year": 2018
        },
        {
            "authors": [
                "Jakub N\u00e1plava",
                "Martin Popel",
                "Milan Straka",
                "Jana Strakov\u00e1."
            ],
            "title": "Understanding model robustness to user-generated noisy texts",
            "venue": "Proceedings of the Seventh Workshop on Noisy User-generated Text (WNUT 2021), pages 340\u2013350.",
            "year": 2021
        },
        {
            "authors": [
                "Hoang Nguyen",
                "Francesco Gelli",
                "Soujanya Poria."
            ],
            "title": "Dozen: Cross-domain zero shot named entity recognition with knowledge graph",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval.",
            "year": 2021
        },
        {
            "authors": [
                "Jianmo Ni",
                "Jiacheng Li",
                "Julian McAuley."
            ],
            "title": "Justifying recommendations using distantly-labeled reviews and fine-grained aspects",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela."
            ],
            "title": "Adversarial NLI: A new benchmark for natural language understanding",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "CoRR, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Yonatan Oren",
                "Shiori Sagawa",
                "Tatsunori B Hashimoto",
                "Percy Liang."
            ],
            "title": "Distributionally robust language modeling",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference",
            "year": 2019
        },
        {
            "authors": [
                "Ji Ho Park",
                "Jamin Shin",
                "Pascale Fung."
            ],
            "title": "Reducing gender bias in abusive language detection",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2799\u20132804.",
            "year": 2018
        },
        {
            "authors": [
                "Judea Pearl"
            ],
            "title": "Models, reasoning and inference",
            "venue": "Cambridge, UK: CambridgeUniversityPress, 19:2.",
            "year": 2000
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander H Miller",
                "Sebastian Riedel"
            ],
            "title": "Language models as knowledge bases? arXiv preprint arXiv:1909.01066",
            "year": 2019
        },
        {
            "authors": [
                "Maxime Peyrard",
                "Sarvjeet Singh Ghotra",
                "Martin Josifoski",
                "Vidhan Agarwal",
                "Barun Patra",
                "Dean Carignan",
                "Emre Kiciman",
                "Robert West."
            ],
            "title": "Invariant language modeling",
            "venue": "arXiv preprint arXiv:2110.08413.",
            "year": 2021
        },
        {
            "authors": [
                "Pouya Pezeshkpour",
                "Sarthak Jain",
                "Sameer Singh",
                "Byron C Wallace."
            ],
            "title": "Combining feature and instance attribution to detect artifacts",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022), pages 5533\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Iryna Gurevych",
                "Sebastian Ruder."
            ],
            "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2020
        },
        {
            "authors": [
                "Sundar Pichai."
            ],
            "title": "An important next step on our ai journey",
            "venue": "Google Blog, The Keyword.",
            "year": 2023
        },
        {
            "authors": [
                "Barbara Plank."
            ],
            "title": "Cross-lingual cross-domain nested named entity evaluation on english web texts",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1808\u20131815.",
            "year": 2021
        },
        {
            "authors": [
                "Adam Poliak",
                "Jason Naradowsky",
                "Aparajita Haldar",
                "Rachel Rudinger",
                "Benjamin Van Durme."
            ],
            "title": "Hypothesis only baselines in natural language inference",
            "venue": "Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 180\u2013",
            "year": 2018
        },
        {
            "authors": [
                "Reid Pryzant",
                "Kelly Shen",
                "Dan Jurafsky",
                "Stefan Wagner."
            ],
            "title": "Deconfounded lexicon induction for interpretable social science",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2018
        },
        {
            "authors": [
                "Guanghui Qin",
                "Jason Eisner."
            ],
            "title": "Learning how to ask: Querying LMs with mixtures of soft prompts",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.",
            "year": 2021
        },
        {
            "authors": [
                "Joaquin Quionero-Candela",
                "Masashi Sugiyama",
                "Anton Schwaighofer",
                "Neil D Lawrence"
            ],
            "title": "Dataset shift in machine learning",
            "year": 2009
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "year": 2018
        },
        {
            "authors": [
                "Alan Ramponi",
                "Barbara Plank."
            ],
            "title": "Neural unsupervised domain adaptation in nlp\u2014a survey",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 6838\u20136855.",
            "year": 2020
        },
        {
            "authors": [
                "Vipula Rawte",
                "Amit Sheth",
                "Amitava Das."
            ],
            "title": "A survey of hallucination in large foundation models",
            "venue": "arXiv preprint arXiv:2309.05922.",
            "year": 2023
        },
        {
            "authors": [
                "Sylvestre-Alvise Rebuffi",
                "Hakan Bilen",
                "Andrea Vedaldi."
            ],
            "title": "Learning multiple visual domains with residual adapters",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Anna Rogers",
                "Matt Gardner",
                "Isabelle Augenstein."
            ],
            "title": "Qa dataset explosion: A taxonomy of nlp resources for question answering and reading comprehension",
            "venue": "arXiv preprint arXiv:2107.12708.",
            "year": 2021
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Barbara Plank."
            ],
            "title": "Strong baselines for neural semi-supervised learning under domain shift",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1044\u20131054.",
            "year": 2018
        },
        {
            "authors": [
                "Rachel Rudinger",
                "Jason Naradowsky",
                "Brian Leonard",
                "Benjamin Van Durme."
            ],
            "title": "Gender bias in coreference resolution",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2018
        },
        {
            "authors": [
                "Jake Russin",
                "Jason Jo",
                "Randall C O\u2019Reilly",
                "Yoshua Bengio"
            ],
            "title": "Compositional generalization in a deep seq2seq model by separating syntax and semantics. arXiv preprint arXiv:1904.09708",
            "year": 2019
        },
        {
            "authors": [
                "Shiori Sagawa",
                "Pang Wei Koh",
                "Tatsunori B Hashimoto",
                "Percy Liang."
            ],
            "title": "Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Koustuv Saha",
                "Benjamin Sugar",
                "John B Torous",
                "Bruno D. Abrahao",
                "Emre K\u0131c\u0131man",
                "Munmun De Choudhury."
            ],
            "title": "A social media study on the effects of psychiatric medication use",
            "venue": "Proceedings of the ... International AAAI Conference on Weblogs",
            "year": 2019
        },
        {
            "authors": [
                "Keisuke Sakaguchi",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi."
            ],
            "title": "Winogrande: An adversarial winograd schema challenge at scale",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8732\u20138740.",
            "year": 2020
        },
        {
            "authors": [
                "Viktor Schlegel",
                "Goran Nenadic",
                "Riza BatistaNavarro."
            ],
            "title": "Beyond leaderboards: A survey of methods for revealing weaknesses in natural language inference data and models",
            "venue": "arXiv preprint arXiv:2005.14709.",
            "year": 2020
        },
        {
            "authors": [
                "Bernhard Sch\u00f6lkopf",
                "Francesco Locatello",
                "Stefan Bauer",
                "Nan Rosemary Ke",
                "Nal Kalchbrenner",
                "Anirudh Goyal",
                "Yoshua Bengio."
            ],
            "title": "Toward causal representation learning",
            "venue": "Proceedings of the IEEE, 109(5):612\u2013634.",
            "year": 2021
        },
        {
            "authors": [
                "Yiqiu Shen",
                "Laura Heacock",
                "Jonathan Elias",
                "Keith D Hentel",
                "Beatriu Reig",
                "George Shih",
                "Linda Moy"
            ],
            "title": "Chatgpt and other large language models are double-edged swords",
            "year": 2023
        },
        {
            "authors": [
                "Zheyan Shen",
                "Jiashuo Liu",
                "Yue He",
                "Xingxuan Zhang",
                "Renzhe Xu",
                "Han Yu",
                "Peng Cui."
            ],
            "title": "Towards out-of-distribution generalization: A survey",
            "venue": "CoRR, abs/2108.13624.",
            "year": 2021
        },
        {
            "authors": [
                "Jake Snell",
                "Kevin Swersky",
                "Richard Zemel."
            ],
            "title": "Prototypical networks for few-shot learning",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Gabriel Stanovsky",
                "Noah A Smith",
                "Luke Zettlemoyer."
            ],
            "title": "Evaluating gender bias in machine translation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1679\u20131684.",
            "year": 2019
        },
        {
            "authors": [
                "Saku Sugawara",
                "Kentaro Inui",
                "Satoshi Sekine",
                "Akiko Aizawa"
            ],
            "title": "What makes reading comprehension questions easier",
            "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Saku Sugawara",
                "Pontus Stenetorp",
                "Kentaro Inui",
                "Akiko Aizawa."
            ],
            "title": "Assessing the benchmarking capacity of machine reading comprehension datasets",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8918\u20138927.",
            "year": 2020
        },
        {
            "authors": [
                "Marcus Tomalin",
                "Bill Byrne",
                "Shauna Concannon",
                "Danielle Saunders",
                "Stefanie Ullmann."
            ],
            "title": "The practical ethics of bias reduction in machine translation: why domain adaptation is better than data debiasing",
            "venue": "Ethics and Information Technology,",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Prasetya Ajie Utama",
                "Nafise Sadat Moosavi",
                "Victor Sanh",
                "Iryna Gurevych."
            ],
            "title": "Avoiding inference heuristics in few-shot prompt-based finetuning",
            "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2021
        },
        {
            "authors": [
                "Eva Vanmassenhove",
                "Christian Hardmeier",
                "Andy Way."
            ],
            "title": "Getting gender right in neural machine translation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3003\u20133008.",
            "year": 2018
        },
        {
            "authors": [
                "Giorgos Vernikos",
                "Katerina Margatina",
                "Alexandra Chronopoulou",
                "Ion Androutsopoulos."
            ],
            "title": "Domain adversarial fine-tuning as an effective regularizer",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3103\u20133112.",
            "year": 2020
        },
        {
            "authors": [
                "Thuy Vu",
                "Dinh Phung",
                "Gholamreza Haffari."
            ],
            "title": "Effective unsupervised domain adaptation with adversarially trained language models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6163\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Cunxiang Wang",
                "Xiaoze Liu",
                "Yuanhao Yue",
                "Xiangru Tang",
                "Tianhang Zhang",
                "Cheng Jiayang",
                "Yunzhi Yao",
                "Wenyang Gao",
                "Xuming Hu",
                "Zehan Qi",
                "Yidong Wang",
                "Linyi Yang",
                "Jindong Wang",
                "Xing Xie",
                "Zheng Zhang",
                "Yue Zhang"
            ],
            "title": "Survey on factuality in large",
            "year": 2023
        },
        {
            "authors": [
                "Cunxiang Wang",
                "Boyuan Zheng",
                "Yuchen Niu",
                "Yue Zhang."
            ],
            "title": "Exploring generalization ability of pretrained language models on arithmetic and logical reasoning",
            "venue": "CCF International Conference on Natural Language Processing and Chinese Computing,",
            "year": 2021
        },
        {
            "authors": [
                "Huazheng Wang",
                "Zhe Gan",
                "Xiaodong Liu",
                "Jingjing Liu",
                "Jianfeng Gao",
                "Hongning Wang"
            ],
            "title": "Adversarial domain adaptation for machine reading comprehension",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Jindong Wang",
                "Xixu Hu",
                "Wenxin Hou",
                "Hao Chen",
                "Runkai Zheng",
                "Yidong Wang",
                "Linyi Yang",
                "Haojun Huang",
                "Wei Ye",
                "Xiubo Geng"
            ],
            "title": "On the robustness of chatgpt: An adversarial and out-of-distribution perspective",
            "year": 2023
        },
        {
            "authors": [
                "Tianlu Wang",
                "Diyi Yang",
                "Xuezhi Wang."
            ],
            "title": "Identifying and mitigating spurious correlations for improving robustness in nlp models",
            "venue": "arXiv preprint arXiv:2110.07736.",
            "year": 2021
        },
        {
            "authors": [
                "Xiao Wang",
                "Shihan Dou",
                "Limao Xiong",
                "Yicheng Zou",
                "Qi Zhang",
                "Tao Gui",
                "Liang Qiao",
                "Zhanzhan Cheng",
                "Xuanjing Huang."
            ],
            "title": "Miner: Improving out-of-vocabulary named entity recognition from an information theoretic perspective",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Yuan Hu",
                "Qiyuan Bian",
                "Zhihua Liu",
                "Shan Qin",
                "Bolin Zhu",
                "Xiaoyu Xing",
                "Jinlan Fu",
                "Yue Zhang",
                "Minlong Peng",
                "Xiaoqing Zheng",
                "Yaqian Zhou",
                "Zhongyu Wei",
                "Xipeng Qiu",
                "Xuanjing Huang"
            ],
            "title": "TextFlint: Unified multilingual robustness evaluation toolkit",
            "year": 2021
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Haohan Wang",
                "Diyi Yang."
            ],
            "title": "Measure and improve robustness in nlp models: A survey",
            "venue": "arXiv preprint arXiv:2112.08313.",
            "year": 2021
        },
        {
            "authors": [
                "Zhao Wang",
                "Aron Culotta."
            ],
            "title": "Identifying spurious correlations for robust text classification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3431\u20133440.",
            "year": 2020
        },
        {
            "authors": [
                "Zhao Wang",
                "Aron Culotta."
            ],
            "title": "Robustness to spurious correlations in text classification via automatically generated counterfactuals",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14024\u201314031.",
            "year": 2021
        },
        {
            "authors": [
                "Alex Warstadt",
                "Alicia Parrish",
                "Haokun Liu",
                "Anhad Mohananey",
                "Wei Peng",
                "Sheng-Fu Wang",
                "Samuel R Bowman."
            ],
            "title": "Blimp: The benchmark of linguistic minimal pairs for english",
            "venue": "Transactions of the Association for Computational Linguistics, 8:377\u2013392.",
            "year": 2020
        },
        {
            "authors": [
                "Jason Wei",
                "Kai Zou."
            ],
            "title": "Eda: Easy data augmentation techniques for boosting performance on text classification tasks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference",
            "year": 2019
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Jim Winkens",
                "Rudy Bunel",
                "Abhijit Guha Roy",
                "Robert Stanforth",
                "Vivek Natarajan",
                "Joseph R Ledsam",
                "Patricia MacWilliams",
                "Pushmeet Kohli",
                "Alan Karthikesalingam",
                "Simon Kohl"
            ],
            "title": "Contrastive training for improved out-of-distribution detection",
            "year": 2020
        },
        {
            "authors": [
                "Qianhui Wu",
                "Zijia Lin",
                "B\u00f6rje Karlsson",
                "Jian-Guang Lou",
                "Biqing Huang."
            ],
            "title": "Single-/multi-source crosslingual ner via teacher-student learning on unlabeled data in target language",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Tongshuang Wu",
                "Marco Tulio Ribeiro",
                "Jeffrey Heer",
                "Daniel S Weld."
            ],
            "title": "Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models",
            "venue": "arXiv preprint arXiv:2101.00288.",
            "year": 2021
        },
        {
            "authors": [
                "Yuxiang Wu",
                "Matt Gardner",
                "Pontus Stenetorp",
                "Pradeep Dasigi."
            ],
            "title": "Generating data to mitigate spurious correlations in natural language inference datasets",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Qizhe Xie",
                "Zihang Dai",
                "Eduard Hovy",
                "Thang Luong",
                "Quoc Le."
            ],
            "title": "Unsupervised data augmentation for consistency training",
            "venue": "Advances in Neural Information Processing Systems, 33:6256\u20136268.",
            "year": 2020
        },
        {
            "authors": [
                "Albert Xu",
                "Xiang Ren",
                "Robin Jia."
            ],
            "title": "Conal: Anticipating outliers with large language models",
            "venue": "arXiv preprint arXiv:2211.15718.",
            "year": 2022
        },
        {
            "authors": [
                "Linyi Yang",
                "Jiazheng Li",
                "P\u00e1draig Cunningham",
                "Yue Zhang",
                "Barry Smyth",
                "Ruihai Dong."
            ],
            "title": "Exploring the efficacy of automatically generated counterfactuals for sentiment analysis",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Linyi Yang",
                "Shuibai Zhang",
                "Libo Qin",
                "Yafu Li",
                "Yidong Wang",
                "Hanmeng Liu",
                "Jindong Wang",
                "Xing Xie",
                "Yue Zhang."
            ],
            "title": "Glue-x: Evaluating natural language understanding models from an outof-distribution generalization perspective",
            "venue": "arXiv",
            "year": 2022
        },
        {
            "authors": [
                "Shunyu Yao",
                "Dian Yu",
                "Jeffrey Zhao",
                "Izhak Shafran",
                "Thomas L Griffiths",
                "Yuan Cao",
                "Karthik Narasimhan."
            ],
            "title": "Tree of thoughts: Deliberate problem solving with large language models",
            "venue": "arXiv preprint arXiv:2305.10601.",
            "year": 2023
        },
        {
            "authors": [
                "Nanyang Ye",
                "Kaican Li",
                "Lanqing Hong",
                "Haoyue Bai",
                "Yiting Chen",
                "Fengwei Zhou",
                "Zhenguo Li."
            ],
            "title": "Ood-bench: Benchmarking and understanding outof-distribution generalization datasets and algorithms",
            "venue": "arXiv preprint arXiv:2106.03721.",
            "year": 2021
        },
        {
            "authors": [
                "Wentao Ye",
                "Mingfeng Ou",
                "Tianyi Li",
                "Xuetao Ma",
                "Yifan Yanggong",
                "Sai Wu",
                "Jie Fu",
                "Gang Chen",
                "Junbo Zhao"
            ],
            "title": "Assessing hidden risks of llms: An empirical study",
            "year": 2023
        },
        {
            "authors": [
                "Yongjing Yin",
                "Yafu Li",
                "Fandong Meng",
                "Jie Zhou",
                "Yue Zhang."
            ],
            "title": "Categorizing semantic representations for neural machine translation",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 5227\u20135239, Gyeongju,",
            "year": 2022
        },
        {
            "authors": [
                "Tao Yu",
                "Rui Zhang",
                "Michihiro Yasunaga",
                "Yi Chern Tan",
                "Xi Victoria Lin",
                "Suyi Li",
                "Heyang Er",
                "Irene Li",
                "Bo Pang",
                "Tao Chen"
            ],
            "title": "2019a. Sparc: Crossdomain semantic parsing in context",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Weihao Yu",
                "Zihang Jiang",
                "Yanfei Dong",
                "Jiashi Feng."
            ],
            "title": "Reclor: A reading comprehension dataset requiring logical reasoning",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Xiang Yue",
                "Shuang Zhou."
            ],
            "title": "Phicon: Improving generalization of clinical text de-identification models via data augmentation",
            "venue": "Proceedings of the 3rd Clinical Natural Language Processing Workshop, pages 209\u2013214.",
            "year": 2020
        },
        {
            "authors": [
                "Rowan Zellers",
                "Yonatan Bisk",
                "Roy Schwartz",
                "Yejin Choi."
            ],
            "title": "SWAG: A large-scale adversarial dataset for grounded commonsense inference",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2018
        },
        {
            "authors": [
                "Biao Zhang",
                "Philip Williams",
                "Ivan Titov",
                "Rico Sennrich."
            ],
            "title": "Improving massively multilingual neural machine translation and zero-shot translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1628\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N Dauphin",
                "David Lopez-Paz."
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Kai Zhang",
                "Hefu Zhang",
                "Qi Liu",
                "Hongke Zhao",
                "Hengshu Zhu",
                "Enhong Chen."
            ],
            "title": "Interactive attention transfer network for cross-domain sentiment classification",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 5773\u20135780.",
            "year": 2019
        },
        {
            "authors": [
                "Wen Zhang",
                "Yang Feng",
                "Fandong Meng",
                "Di You",
                "Qun Liu."
            ],
            "title": "Bridging the gap between training and inference for neural machine translation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4334\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Jieyu Zhao",
                "Tianlu Wang",
                "Mark Yatskar",
                "Ryan Cotterell",
                "Vicente Ordonez",
                "Kai-Wei Chang."
            ],
            "title": "Gender bias in contextualized word embeddings",
            "venue": "Proceedings of NAACL-HLT, pages 629\u2013634.",
            "year": 2019
        },
        {
            "authors": [
                "Jieyu Zhao",
                "Tianlu Wang",
                "Mark Yatskar",
                "Vicente Ordonez",
                "Kai-Wei Chang."
            ],
            "title": "Gender bias in coreference resolution: Evaluation and debiasing methods",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association",
            "year": 2018
        },
        {
            "authors": [
                "Jieyu Zhao",
                "Yichao Zhou",
                "Zeyu Li",
                "Wei Wang",
                "KaiWei Chang."
            ],
            "title": "Learning gender-neutral word embeddings",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4847\u20134853.",
            "year": 2018
        },
        {
            "authors": [
                "Hao Zheng",
                "Mirella Lapata."
            ],
            "title": "Disentangled sequence to sequence learning for compositional generalization",
            "venue": "arXiv preprint arXiv:2110.04655.",
            "year": 2021
        },
        {
            "authors": [
                "Yinhe Zheng",
                "Guanyi Chen",
                "Minlie Huang."
            ],
            "title": "Out-of-domain detection for natural language understanding in dialog systems",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:1198\u20131209.",
            "year": 2020
        },
        {
            "authors": [
                "Ruiqi Zhong",
                "Kristy Lee",
                "Zheng Zhang",
                "Dan Klein."
            ],
            "title": "Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections",
            "venue": "EMNLP.",
            "year": 2021
        },
        {
            "authors": [
                "Wanjun Zhong",
                "Siyuan Wang",
                "Duyu Tang",
                "Zenan Xu",
                "Daya Guo",
                "Jiahai Wang",
                "Jian Yin",
                "Ming Zhou",
                "Nan Duan."
            ],
            "title": "Ar-lsat: Investigating analytical reasoning of text",
            "venue": "arXiv preprint arXiv:2104.06598.",
            "year": 2021
        },
        {
            "authors": [
                "Chunting Zhou",
                "Daniel Levy",
                "Xian Li",
                "Marjan Ghazvininejad",
                "Graham Neubig."
            ],
            "title": "Distributionally robust multilingual machine translation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Ran Zhou",
                "Ruidan He",
                "Xin Li",
                "Lidong Bing",
                "Erik Cambria",
                "Luo Si",
                "Chunyan Miao."
            ],
            "title": "Melm: Data augmentation with masked entity language modeling for cross-lingual ner",
            "venue": "arXiv preprint arXiv:2108.13655.",
            "year": 2021
        },
        {
            "authors": [
                "Wenxuan Zhou",
                "Fangyu Liu",
                "Muhao Chen."
            ],
            "title": "Contrastive out-of-distribution detection for pretrained transformers",
            "venue": "arXiv preprint arXiv:2104.08812.",
            "year": 2021
        },
        {
            "authors": [
                "Chen Zhu",
                "Yu Cheng",
                "Zhe Gan",
                "Siqi Sun",
                "Tom Goldstein",
                "Jingjing Liu."
            ],
            "title": "Freelb: Enhanced adversarial training for natural language understanding",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Kaijie Zhu",
                "Jindong Wang",
                "Jiaheng Zhou",
                "Zichen Wang",
                "Hao Chen",
                "Yidong Wang",
                "Linyi Yang",
                "Wei Ye",
                "Neil Zhenqiang Gong",
                "Yue Zhang"
            ],
            "title": "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts",
            "year": 2023
        },
        {
            "authors": [
                "Zining Zhu",
                "Aparna Balagopalan",
                "Marzyeh Ghassemi",
                "Frank Rudzicz."
            ],
            "title": "Quantifying the taskspecific information in text-based classifications",
            "venue": "arXiv preprint arXiv:2110.08931.",
            "year": 2021
        },
        {
            "authors": [
                "Ran Zmigrod",
                "Sebastian J Mielke",
                "Hanna Wallach",
                "Ryan Cotterell."
            ],
            "title": "Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Pre-trained Language Models (PLMs) (Devlin et al., 2018; Liu et al., 2019b; Radford et al., 2018) have revolutionized natural language processing (NLP) and enabled remarkable advances in Large-scale Language Models (LLMs) (Touvron et al., 2023; Gozalo-Brizuela and Garrido-Merchan, 2023; Pichai, 2023) . Despite substantial progress in developing accurate models in several natural language understanding tasks, including sentiment analysis (Kaushik et al., 2019; Ni et al., 2019; Yang et al., 2021; Lu et al., 2022; Luo et al., 2022a,b), natural language inference (Williams et al., 2018), and machine reading comprehension (Kaushik and Lipton, 2018; Sugawara et al., 2020), a major challenge persists \u2013 out-of-distribution (OOD) generalization \u2013 which entails the ability of a model to\n1These authors contributed equally to this work. 2\u201cLarge Language Models (LLMs)\u201d refers to recent generative models while \u201cPre-trained Language Models refers to small-scale pre-trained models\u201d in this paper.\naccurately classify text instances from distributions different from those of the training data (Ben-David et al., 2010; Hendrycks and Gimpel, 2017; Hupkes et al., 2022). This paper aims to provide a comprehensive overview of the current state of research in OOD generalization for natural language understanding, highlighting key methodologies, advancements, and unique challenges.\nThe importance of OOD generalization in NLP cannot be overstated, as real-world data often exhibit diversity and unpredictability. Numerous applications, such as sentiment analysis, document categorization, and spam detection (Shen et al., 2021; Yang et al., 2022), necessitate models capable of adapting to novel and unforeseen data distributions. While machine learning models generally demonstrate strong in-distribution performance, their performance frequently deteriorates when confronted with OOD instances, underscoring the need for effective strategies that facilitate generalization beyond the training distribution.\nAlthough research on OOD generalization in NLP is emerging, it is not on the scale of other tasks like computer vision (Ye et al., 2021; Koh et al., 2021) and time series (Du et al., 2021b; Gagnon-Audet et al., 2022). Furthermore, most related surveys in NLP focus on measuring and improving model robustness against adversarial attacks (Schlegel et al., 2020; Arora et al., 2021), or providing causal explanations (Keith et al., 2020). Among them, Wang et al. (2021d) is the most relevant review to this paper, but their work does not differentiate between data-level variance and shortcut features and also not discuss LLMs.\nTo address these limitations, this survey provides an extensive examination of the existing literature on OOD generalization in NLP, covering a diverse array of techniques and approaches. We focus on two perspectives of OOD generalization: the data distribution, which is model-independent and the feature distribution, which is model-oriented. Ad-\nditionally, we discuss the evaluation metrics and benchmarks employed to assess the effectiveness of these techniques, as well as the limitations and drawbacks of current methodologies.\nThroughout this survey, we trace the evolution of OOD generalization techniques in natural language processing, from the early approaches based on traditional machine learning algorithms to more recent advancements driven by deep learning architectures, also including the discussion of the most recent emergent abilities of LLMs. We identify the key innovations and breakthroughs that have shaped the field, while also highlighting areas where progress has been relatively slow or incremental. Our analysis emphasizes the interconnected nature of these advancements and the importance of driving fundamental research in the generalization problem towards unforeseen data distributions. In addition, this survey aims to identify open challenges and future directions for OOD generalization in NLP, especially for LLMs. We discuss the limitations of current techniques, potential avenues for improving model robustness and adaptability, and emerging research trends that may contribute to the development of more effective OOD generalization strategies.\nThe remainder of this survey is organized as follows: we formalize the scope of OOD generalization in Section 2. Then, we propose a novel taxonomy towards OOD robustness and review existing methodologies developed for addressing OOD issues in Section 3. In particular, we identify two salient aspects of OOD generalization, namely Data Variance and Shortcut Features. We outline two representative application scenarios in Section 4, namely Deployment in High-stake Domains and Social Bias. We also introduce the methods for improving the OOD robustness in Section 5 before discussing the redefinition of OOD in the era of large language models."
        },
        {
            "heading": "2 The Scope of OOD Generalization",
            "text": "Denote a set of labeled data as D = {(xi, yi)}Ni=1, where an input x \u2208 X , output y \u2208 Y , and N is the number of datasets. A training dataset Dtrain = {(Xtrain, Ytrain)} is generated by sampling from D with distribution Ptrain, and the test dataset Dtest = {(Xtest, Ytest)} is sampled from D with distribution Ptest. Out-of-distribution (OOD) refers to the circumstance when Ptrain \u2260 Ptest.\nIn the context of text classification, let X be the\nset of all possible documents, Y be the set of all possible labels, and D be a training distribution defined on X \u00d7 Y . Suppose the true target distribution is PX ,Y , which is close to but not identical to D with PX ,Y \u2260 D. When we encounter a document that is drawn from a distribution QX that is significantly different from PX , we refer to it as an out-of-distribution (OOD) sample. An OOD sample may have a vocabulary or language not presenting in PX .\nA text classification model f \u2236 X \u2192 Y is considered OOD if its performance on QX is significantly worse than on PX due to the distribution shift. The OOD detection function can be derived from a probabilistic perspective using Bayesian inference. In this case, we can estimate the posterior probability of a document being OOD given its bag-of-words features through Bayesian model averaging:\nP (OOD\u2223x) = \u2211 \u03b8 P (OOD\u2223\u03b8,x)P (\u03b8\u2223x)\n= \u2211 \u03b8\nP (x\u2223OOD, \u03b8)P (OOD\u2223\u03b8)P (\u03b8) P (x)\nwhere \u03b8 denotes the model parameters, x is the bag-of-words representation of a document, P (OOD\u2223\u03b8) is the prior probability of the model being OOD assuming the model parameter \u03b8, P (x\u2223OOD, \u03b8) is the likelihood of observing the bag-of-words features x given that the document is OOD and the model parameter \u03b8, P (\u03b8) is the prior probability of the model parameter \u03b8, and P (x) is the marginal likelihood of observing the bag-of-words features x.\nThe conditional OOD probability of model f\u03b8 on input x given the parameter \u03b8 is defined as:\nP (OOD\u2223\u03b8,x) = P (x\u2223OOD, \u03b8)P (OOD\u2223\u03b8) P (x\u2223\u03b8) .\nAs can be seen from the above equations, OOD can be perceived in terms of both the data- and model- levels, robust models can be more resistant to data variances. The OOD detection function can be defined as a threshold on the posterior OOD probability:\ng(x) = [max y P (y\u2223x) < \u03f5],\nwhere P (y\u2223x) is the posterior probability of the document belonging to class y given the bag-ofwords features x, and \u03f5 is a threshold parameter that determines the confidence of the prediction.\nThe OOD detection performance can be evaluated using metrics such as the Receiver Operating Characteristic (ROC) curve or the KolmogorovSmirnov (KS) statistic, which capture the trade-off between true positive rate and false positive rate, or the maximum distance between the cumulative distribution functions of the OOD and in-distribution predictions, respectively."
        },
        {
            "heading": "3 Taxonomy of Out-of-Distribution Problems",
            "text": "We classify OOD problems into two perspectives, as depicted in Figure 1, namely Data and Features. Data variance encompasses the domain generalization problem, while \u201cshortcut features\" represent a range of issues typically caused by shortcut learning, which cannot be avoided from inductive reasoning."
        },
        {
            "heading": "3.1 Data",
            "text": "Data variance can be seen as a typical problem of domain generalization methods, assuming the unavailability of labeled or unlabeled data from the target domain. Previous studies have explored this approach in sentiment analysis (SA) (Kaushik et al., 2019; Ni et al., 2019; Yang et al., 2021; Lu et al., 2022), natural language inference (NLI) (Williams et al., 2018; Hendrycks et al., 2020), and named entity recognition (NER) (Jia and Zhang, 2020; Plank, 2021). Different domains have intrinsically different feature distributions, and instances from different domains have different predicted vocabulary distributions, which leads to the OOD generalization challenge, as shown in Figure 1.\nNumerous NLP studies aim to tackle systematic variations between training and testing distributions, encompassing a vast body of literature on domain generalization (Blitzer et al., 2006; Ganin et al., 2016; Ruder and Plank, 2018; Han and Eisenstein, 2019; Guo et al., 2020) and cross-task trans-\nfer (Johnson et al., 2017; Levy et al., 2017; Eriguchi et al., 2018; Wang et al., 2022). These studies can be broadly categorized into input-level variation and output-level variation. Notable comprehensive surveys dedicated to this task include those by Ramponi and Plank (2020) and Wang et al. (2021d) but fail to decouple data and features.\nCompositional generalization refers to the challenge of learning the distribution of atoms given the surface distributions of their compositions. It has garnered significant attention in NLP research, encompassing areas such as semantic parsing (Iyer et al., 2017; Gupta et al., 2022), QA (Gu et al., 2021; Lewis et al., 2021), machine translation (Li et al., 2021), and general natural language understanding (NLU) tasks (Lake and Baroni, 2018; Keysers et al., 2020). Researchers (Keysers et al., 2020; Kim et al., 2021) have found that state-of-the-art neural models struggle to generalize to novel compounds in a manner similar to human performance. Several benchmarks have been introduced to evaluate compositional generalization. For example, the SCAN dataset by Lake and Baroni (2018) is designed for sequence-to-sequence generalization (Russin et al., 2019; Li et al., 2019a; Gordon et al., 2019; Andreas, 2020). Additionally, Keysers et al. (2020) and Kim and Linzen (2020) propose the CFQ and COGS benchmarks, respectively, for semantic parsing. Li et al. (2021) propose the CoGnition dataset to assess how neural machine translation models generalize to novel compounds (Hupkes et al., 2020; Zheng and Lapata, 2021; Dankers et al., 2021; Jung, 2022).\nTo address the challenges of compositional generalization, achieving OOD robustness is highly desirable as current NLP models have shown fragility to variations in expression, where even minor punctuation changes can lead to different outputs (Wang et al., 2021c). Furthermore, Moradi et al. (2021) observe significant performance decay of NLP mod-\nels in domain-specific tasks, such as the clinical domain, due to noise, grammar errors, missing words, punctuation, typos, and other factors. Additionally, Wang et al. (2021c) develop a unified multilingual robustness evaluation platform for NLP tasks to provide comprehensive robustness analysis.\nAnother source of OOD data is human-crafted adversarial data. For example, the recently proposed contrast sets (Kaushik et al., 2019; Gardner et al., 2020; Warstadt et al., 2020) reveal the failure of capturing true underlying distributions, which show the fragility of models against small variations of input expressions. In addition, although researchers also propose a benchmark to reveal the importance of OOD detection (Hendrycks and Gimpel, 2017; Hendrycks et al., 2020; Fort et al., 2021), there is a consensus that we still lack a standard definition of OOD examples and fine-grained evaluations. A full survey of current available OOD datasets can be found in Appendix A."
        },
        {
            "heading": "3.2 Features",
            "text": "Models\u2019 predictions are often influenced by shortcut features learned from spurious patterns between training data and labels, as well as existing shortcuts in the dataset. For instance, as illustrated in Figure 1 (Model Perspective), sentence length has inadvertently become a learned feature during training, where 60% of the hypotheses in entailment examples have 7 or fewer tokens and half of the hypotheses with more than 12 or fewer than 5 tokens are neutral or contradiction, respectively (Gururangan et al., 2018).\nIdeally, a model should learn rational (Jiang et al., 2021; Lu et al., 2022) features for robust generalization. Take sentiment classification for example. In order to decide a positive polarity for the sentence \u201cI like this movie.\u201d, a rationale feature should be \u201clike\u201d rather than \u201cmovie\u201d. The latter is referred to as a spurious feature (Kaushik et al., 2020), which leads to reduced generalization. Other cases of feature issues include shortcut features (Geirhos et al., 2020). For instance, in machine reading comprehension, if the question asks for a date and the input passage contains only one date, a model can bypass a reasoning process and directly use the date feature for output (Lai et al., 2021). For numerical (Hendrycks et al., 2020; Wang et al., 2021a; Cobbe et al., 2021) and logical (Yu et al., 2019b; Liu et al., 2021c) reasoning tasks, the rationale feature should be the underlying\nalgebraic and logic deduction, which turn out to be extremely challenging to learn using existing pre-trained models, leading to weak generalization.\nCurrent NLP methods tend to learn implicitly superficial cues instead of the causal associations between the input and labels, as evidenced by (Geirhos et al., 2020; Guo et al., 2023b), and thus usually show their brittleness when deployed in real-world scenarios. Recent work (Sugawara et al., 2018, 2020; Lai et al., 2021; Wang et al., 2021b; Du et al., 2021a; Zhu et al., 2021; Bastings et al., 2021) indicates that current PLMs unintentionally learn shortcuts to trick specific benchmarks and such tricks (i.e., syntactic heuristics, lexical overlap, and relevant words) that use partial evidence to produce unreliable output, which is particularly serious in the open domain."
        },
        {
            "heading": "4 Application Scenarios",
            "text": "We highlight the importance of OOD generalization in two real-world application scenarios, in which low OOD robustness may lead to serious consequences."
        },
        {
            "heading": "4.1 Deployment in Practical Domains",
            "text": "Despite the generalization ability of LLMs, such as ChatGPT (OpenAI, 2023b), the relatively low generalization ability of medium-size models hinders the deployment of NLP systems, especially for high-stake domains, from health and medicine to finance and business (Imbens and Rubin, 2015; Choi et al., 2023), and should be taken more seriously. Notably, a recent comprehensive evaluation of OOD generalization in text classification named GLUE-X (Yang et al., 2022) shows that the average accuracy of PLMs on cross-domain evaluations falls significantly short of human performance, even for the highest-performing model (80.1% \u2013 human versus 74.6% \u2013 model). In contrast to GLUE, where over 20 single-model results outperform human baselines, none of the baselines, including InstructGPT and ChatGPT, considered in GLUE-X is able to surpass human performance using OOD tests. The lack of sufficient OOD generalization ability is also related to social bias."
        },
        {
            "heading": "4.2 Social Bias",
            "text": "Recent studies (Gardner et al., 2020) have uncovered a problematic tendency for gender bias in sentiment analysis (Zmigrod et al., 2019; Maudslay et al., 2019; Lu et al., 2020). Bias exists\nin different forms of language representations, including word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2017; Zhao et al., 2018b; Gonen and Goldberg, 2019), contextualized word embeddings (Zhao et al., 2019) and sentence embeddings (May et al., 2019). Some found that the embeddings of feminine words and masculine words are often clustered into different groups (e.g., occupation) (Bolukbasi et al., 2016; Caliskan et al., 2017; Zhao et al., 2018b, 2019; Gonen and Goldberg, 2019). Gender bias also affects coreference resolution systems, which tend to link a pronoun to occupations dominated by the pronoun gender (Rudinger et al., 2018; Zhao et al., 2018a). In machine translation, Vanmassenhove et al. (2018) and Stanovsky et al. (2019) find that models tend to make stereotypical assignments of gender roles when translating occupation words. Apart from gender bias, there are other forms of social bias in NLP data, such as disability (Hutchinson et al., 2020), race (Kiritchenko and Mohammad, 2018), age (Diaz et al., 2018), etc."
        },
        {
            "heading": "5 Methods",
            "text": "Existing work to address OOD issues in NLP can be categorized into three groups: data augmentation (Sec.5.1), model-level control (Sec.5.2), and training approaches (Sec.5.3) shown in Fig. 2. Descriptions of current OOD generalization methods categorized by tasks are introduced in the Appendix."
        },
        {
            "heading": "5.1 Data Augmentation",
            "text": "Data augmentation (DA) techniques are employed to enhance the diversity of training data without the need for explicitly collecting new data (Feng et al., 2021). This approach proves beneficial for improving the generalization of NLP models by reducing overfitting and enhancing robustness. Several existing surveys have discussed data augmentation in low-resource NLP scenarios from different perspectives (Hedderich et al., 2021; Feng et al., 2021;\nBayer et al., 2021; Chen et al., 2021a; Li et al., 2022). In this study, our focus is on data augmentation regarding OOD generalization. Semi-fact Data Augmentation. One common type of data augmentation method in NLP involves substituting part of the content or introducing perturbations to the original data, primarily focusing on enhancing the diversity without altering the semantic meaning or label. Synonym substitution has been explored by Zhang et al. (2015), Miao et al. (2020) and Yue and Zhou (2020) to replace words or entities. Perturbation techniques typically involve manipulating tokens within sentences (Zhang et al., 2018; Wei and Zou, 2019; Miao et al., 2020; Xie et al., 2020; Zhao et al., 2019, 2018a), as well as adversarial perturbations (Miyato et al., 2017; Cheng et al., 2019; Zhu et al., 2019; Jiang et al., 2020; Zheng et al., 2020), which employ large pre-trained models (e.g., GPT-2, BART, BERT) for generating conditional data augmentations. Lu et al. (2022) apply the human-in-the-loop technique incorporated with semi-fact data augmentation for improving the OOD robustness of PLMs in sentiment analysis. Counterfactual data augmentation (CDA) is widely adopted to mitigate bias in neural NLP tasks by operating on biased text (Maudslay et al., 2019; Zmigrod et al., 2019). A counterfactual example constructed by flipping the label helps to learn real associations between input and label. For instance, Lu et al. (2020) proposes a CDA method to mitigate gender bias in neural coreference resolution, which is a generic methodology for corpus augmentation via causal interventions (i.e., breaking associations between gendered and gender-neutral words). In text classification, Kaushik et al. (2019), Kaushik et al. (2020), and Wang and Culotta (2020) employ humans for generating counterfactual data, which has been shown to be effective to mitigate the influence of spurious patterns. Automatic counterfactual generation aims to change the data distribution of the training data so that models can alleviate reliance on dataset-specific bias and exclude spurious correlations (Yang et al., 2021; Wang and Culotta, 2021; Wu et al., 2021) and has been improved in a recent work (Fan et al., 2023) by using data-level and sentence-level constraints."
        },
        {
            "heading": "5.2 Model-level Operations",
            "text": "Feature representation learning holds a pivotal role in OOD generalization. In this section, we evaluate model-level approaches, focusing on two critical\naspects: invariance and the causal factor.\nInvariant Common Features Research on invariant features as a means to facilitate transfer learning has been an enduring pursuit in the field. In the context of discrete linear models, various methods have been developed to harness data from the target domain to aid representation learning. For instance, Structured Correspondence Learning utilizes unlabeled target-domain data to establish mappings between features across different domains (Blitzer et al., 2006). On a similar note, Daum\u00e9 III (2009) employs labeled data for this purpose.\nAdditionally, Johnson and Zhang (2005) also uses unlabeled data, but in a different setting. Transitioning to neural models, adversarial learning emerges as a prevalent technique (Goodfellow et al., 2015; Ganin et al., 2016; Zhang et al., 2019a). In this approach, an adversarial loss function is employed to train a domain classifier. This classifier attempts to eliminate domain-specific information in the hidden layers, thereby producing representations that are more amenable for cross-domain (Liu et al., 2018; Li et al., 2019b; Du et al., 2020) or cross-task decision making (Johnson et al., 2017; Levy et al., 2017; Eriguchi et al., 2018; Lee et al., 2019; Wang et al.; Keung et al., 2019; Vernikos et al., 2020; Wang et al., 2022). In sentiment analysis, Liu et al. (2018) and Du et al. (2020) conduct adversarial training to derive enhanced domaininvariant features for cross-domain classification.\nFeature clustering and other techniques are also adopted to learn invariant features, which requires OOD generalization on unseen tasks. For instance, Johnson et al. (2017), Arivazhagan et al. (2019), Ji et al. (2020), Liu et al. (2021a) train translation models for better learning of language-independent representations, which help the model generalize to unseen language pairs. More recently, Yin et al. (2022) categorize source contextualized representations to boost compositional generalization.\nCausal-based Features Causal inference aims to determine the effectiveness of one variable on another variable (Holland, 1986; Morgan and Winship, 2015; Imbens and Rubin, 2015; Pearl et al., 2000). Because the relationships between the causal features and the labels are invariant under distribution shift (Pearl et al., 2000; QuioneroCandela et al., 2009), learning causal relationships allows a model to acquire robust knowledge that holds beyond the distribution of a set of training tasks or the observed data (Sch\u00f6lkopf et al., 2021).\nIn addition, learning a causal model requires fewer examples to adapt to new environments (Sch\u00f6lkopf et al., 2021).\nThere has been much research on using causal inference to improve OOD generalization. For instance, in social media, Pryzant et al. (2018) induce a lexicon that is helpful for target label prediction yet uncorrelated to a set of confounding variables, and Saha et al. (2019) perform propensity scorebased causal analysis on social media posts for evaluating the effect of psychiatric medications."
        },
        {
            "heading": "5.3 Training Approaches",
            "text": "In the presence of distribution shifts, optimization tends to be influenced by irrelevant signals, resulting in severe failures when applied to OOD test data (Liu et al., 2021b). Consequently, there has been significant interest in recent work regarding training techniques. Distributionally Robust Optimization (DRO) aims to learn a model on the worst-case distribution scenario (domain) while expected to generalize well on test data. To improve the worst-case domain, Sagawa et al. (2020) propose a group DRO method that requires explicit group annotation of samples. Methods based on group DRO and its variants have recently been applied in NLP tasks, such as NLI (Sagawa et al., 2020; Liu et al., 2021b), machine translation (Zhou et al., 2021a), spoken language understanding (Broscheit et al.), and toxicity detection (Michel et al., 2020). For example, Oren et al. (2019) design a DRO procedure for generative modeling that minimizes the simulated worst-case distribution scenario over the mixture of topics. Zhou et al. (2021c) consider the worst-case with language pairs to optimize multilingual neural machine translation. Invariance Risk Minimization (IRM) Different from DRO, which focuses on domain shift robustness, IRM methods focus on learning invariant representations. IRM (Arjovsky et al., 2019) is a recently proposed learning paradigm that estimates non-linear, invariant, causal predictors from multiple training environments for improving OOD generalization. It has several advantages. For example, it does not need extra knowledge to manipulate the original data (e.g., human intervention or rule-based methods) and extra large computation. Existing work has studied the IRM and its variants in NLP. Choe et al. (2020) investigate IRM on synthetic settings and simple MLP and machine\nlearning models in sentiment analysis. Dranker et al. (2021) study OOD generalization for NLI by IRM, in which environments are constructed by ensuring whether the dataset and bias are synthetic or naturalistic. Peyrard et al. (2021) propose a language framework based on IRM-games (Ahuja et al., 2020) for learning invariant representations that generalize better across multiple environments. The OOD objective in learning the causal invariance can also be viewed as a multi-objective optimization problem, which has been explored by Chen et al. (2023b) using a pareto learning strategy.\nTuning Three popular tuning approaches for preserving the pre-trained features are reviewed: prompt tuning, adapter tuning, and linear probing.\nAdapter tuning (Rebuffi et al., 2017; Houlsby et al., 2019) contains a few task-specific trainable parameters and are injected between layers of frozen pre-trained models. Training only the adapter modules can help models achieve competitive performance on various tasks, such as multi-task text classification (Houlsby et al., 2019), NER (Pfeiffer et al., 2020), multi-task QA (Friedman et al., 2021), and multilingual speech translation (Le et al., 2021).\nPrompt tuning (Liu et al., 2021f) methods convert the downstream problems into language modeling problems. It adds prompt tokens as the prefix to the questions and converts them to input texts, then use a pre-trained language model to process the input texts in order to generate the answer sequences. There are two variations of prompt tokens, hard prompt tokens, and soft prompt tokens. Tuning hard prompt tokens requires fine-tuning the pre-trained models (Petroni et al., 2019; Cui et al., 2021). Tuning soft prompt tokens only need to finetune the prompt tokens, thus preserving the pretrained features (Li and Liang, 2021; Lester et al., 2021; Qin and Eisner, 2021; Liu et al., 2021g). Soft prompt tuning is helpful for a wide range of crossdomain tasks, such as NER (Chen et al., 2021c, 2022b), text classification (Gao et al., 2021; Zhong et al., 2021a; Utama et al., 2021), table-to-text (Li and Liang, 2021), QA and paraphrase detection (Lester et al., 2021) and NLU (Liu et al., 2021g).\nLinear probing (Liu et al., 2019a) fine-tunes the top layers while keeping the lower layers frozen. Compared to full fine-tuning, linear probing performs better for OOD generalization but reaches lower accuracy on IID data. Kumar et al. (2022) propose a two-step strategy, which first trains the\nmodel with linear probing and then performs finetuning (LP-FT). This approach has been theoretically proven to improve both in-domain and OOD performance for deep neural models."
        },
        {
            "heading": "6 Large Language Models",
            "text": "Large language models (LLMs) have attracted increasing attention in the field of artificial intelligence recently. However, as a crucial property towards artificial general intelligence (AGI), the OOD robustness is still under-explored (Wang et al., 2023b). Given its importance, we review the recent work on the OOD generalization of LLMs. OOD Definition It is of imminent importance to reframe the OOD definition in the era of LLM dominance since the pre-trained corpora of LLMs are not publicly available. The absence of pre-trained corpus information makes it hard to define OOD examples for LLMs in NLP. Although providing an accurate and strict definition remains challenging for large foundation models, researchers make attempts to build label-sharing OOD data for LLMs from two perspectives, namely, synthetic data, and distribution shift over time. Synthetic data is generally defined as artificially annotated information generated by algorithms or simulations, which can be hand-crafted as challenging OOD examples for LLMs. Distribution shift over time refers to the idea of using real-world datasets collected after 2021 as OOD test data, which is the latest data collection time of ChatGPT (Wang et al., 2023b).\nAnother type of OOD data refers to the task of generalizing to unseen classes. For instance, in open-set label shift (Garg et al., 2022), the test data includes examples from novel classes not present in the training data, making it impossible for classical small models to predict correctly. LLMs such as ChatGPT can alleviate this issue by using incontext learning, as evidenced by recent research (Xu et al., 2022). This means that LLMs can be used to improve robustness with minimal human intervention but they cannot fully solve this problem and open-set label shift remains challenging. OOD Detection Previous research on OOD detection has employed models to identify test examples that come from a different distribution (Hendrycks and Gimpel, 2017; Hendrycks et al., 2018). Some of these approaches introduce new training objectives, such as using a contrastive objective (Winkens et al., 2020; Zhou et al., 2021c). When the type of distribution shift is known, the\nmodel can be trained to exhibit uncertainty when presented with known OOD examples (Hendrycks et al., 2020). However, the distribution of SOTA LLMs, such as ChatGPT and GPT-4 is hidden and cannot be inferred. Very recently, CoNAL (Xu et al., 2022) provides an alternative for generating novel examples which simulate open-set shifts and has proven to be effective for OOD detection.\nRegarding language models (LLMs), the deepfake detectors aimed at distinguishing content generated by humans or LLMs is closely related to previous algorithms designed for OOD detection (Guo et al., 2023a). When it comes to deepfake detection, one intuitive approach is to employ statistical boundaries that differentiate linguistic patterns between human-written and machine-generated text (Mitchell et al., 2023). However, these statistical methods have a limitation: they assume access to the model\u2019s prediction distributions is possible, which hinders their application to models behind APIs, such as ChatGPT. An alternative paradigm involves training neural-based detectors (Bakhtin et al., 2019; Fagni et al., 2021), including the official implementation of OpenAI (OpenAI, 2023a).\nOOD Robustness Previous studies have extensively examined various aspects of ChatGPT in the domains of law (Choi et al., 2023), ethics (Shen et al., 2023), reasoning (Bang et al., 2023) and planning (Yao et al., 2023). However, limited attention has been given to its robustness (Kawaguchi et al., 2017) against out-of-distribution (OOD) inputs. Evaluating OOD robustness in a reliable manner poses a significant challenge due to the massive and unknown training data of LLMs. Wang et al. (2023b) offers an initial investigation into the robustness of ChatGPT by presenting OOD results on Flipkart and DDXPlus. Building on this work, Ye et al. (2023) delimit the robustness of LLMs in comparison to conventional models, with a focus on aligning the threat model to the realistic deployment of LLMs. Additionally, Zhu et al. (2023) measures LLMs\u2019 resilience to adversarial prompts using adversarial textual attacks on character, word, sentence, and semantic levels. Collectively, these evaluations raise similar concerns regarding the limited robustness of ChatGPT. Among different attack levels, character-level attacks demonstrate higher robustness, while word-level attacks pose the greatest vulnerability. Recently, we have noticed that several papers survey the robustness issue in Large Language Models (LLMs) from differ-\nent perspectives, including factuality (Wang et al., 2023a), hallucination (Rawte et al., 2023), and evaluation methods (Chang et al., 2023)."
        },
        {
            "heading": "7 Future Directions and Conclusion",
            "text": "We consider multiple promising directions for improving the OOD robustness from four perspectives: (1) enhancing the learning of such salient causal features, either by the help of human guidance (Kaushik et al., 2019; Lu et al., 2022) via human-in-the-loop or through psychologically inspired neural structures (Chowdhery et al., 2022), can be worthy of consideration; (2) data-centric AI: both the selection of training data and the careful design of prompt learning have proven effective in domain generalization (Chen et al., 2022c). In addition, the emerging ability of large-scale language models holds a huge potential for OOD generalization, benefiting from the instruction tuning, which requires a high-quality data construction process; (3) alignment methods: this can be effectuated through the deployment of reinforcement learning algorithms, be it in an online or offline setting (Christiano et al., 2017; Chen et al., 2023a); (4) neuro-symbolic modeling for NLP: purely neural models like ChatGPT can possess incredibly powerful generalization abilities. While it is more-or-less accepted that purely neural models face challenges of reasoning beyond surface-level patterns. In order to avoid picking up spurious correlations, neuro-symbolic approaches are proposed to improve the models\u2019 OOD robustness by combining the learning capabilities of neural networks with the expressive power of symbolic reasoning (Alon et al., 2022; Jung et al., 2022; Manhaeve et al., 2018; Hamilton et al., 2022).\nThis paper presents an ambitious attempt to categorize the challenges of OOD generalization, focusing on both data and model levels. By undertaking this categorization, our aim is to shed light on the limitations of current methods, emphasize the crucial nature of OOD robustness, and provide quick access to existing references for further exploration. In addition, we emphasize the ongoing significance of OOD robustness in the era of large language models, emphasizing the need to address this aspect. We call upon researchers in the NLP community to delve deeper into the proper definition of OOD in the context of large models and develop appropriate benchmark tests that accurately measure the OOD generalization ability of LLMs."
        },
        {
            "heading": "8 Limitations",
            "text": "When we categorise OOD-related NLP work, we mostly focus on the recently appearing papers, which can be retrospected to classical generalization studies. Moreover, the literature on domain generalization and domain adaptation has not been distinguished in this work. Lastly, the introduction of classical transfer learning algorithms has not been included for the time being."
        },
        {
            "heading": "Acknowledgement",
            "text": "This publication has emanated from research conducted with the financial support of the Pioneer and \u201cLeading Goose\u201d R&D Program of Zhejiang under Grant Number 2022SDXHDX0003 and the 72nd round of the Chinese Post-doctoral Science Foundation project 2022M722836. Yue Zhang is the corresponding author."
        },
        {
            "heading": "A Appendix",
            "text": "In an effort to clearly outline the various challenges tied to OOD generalization, we divide our discussion into two distinct aspects, represented by Table 1 (Data Distribution) and Table 2 (Feature Distribution), respectively. In Table 1, we focus on issues that arise due to differences and changes in data. It systematically lists the ways in which variability in data attributes can make it difficult for models to effectively generalize to out-of-distribution samples in different tasks. In general, we categorize the reference materials from two perspectives: annotation artifacts (label-sharing generalization) and output variance (label-different generalization). Different from label-sharing generalization approaches, which rely on few-shot or unlabeled data from the target domain, label-different generalization is based on zero-shot learning using clustering and other techniques. We also outline the typical datasets for each task and corresponding representative methods.\nOn the other hand, we concentrate on a different set of issues from the feature perspective. These problems originate from the models\u2019 tendencies to learn from spurious patterns or \u201cshortcut features\u201d in the data, which might not reflect the true underlying relationships between inputs and labels, leading to the generalization challenge. Ideally, a model should learn rational features. However, inductive reasoning naturally relies on patterns and trends from the training data. This reliance can result in models performing well on familiar data but poorly when faced with new, OOD examples. The OOD generalization challenge can not be avoided when using deep learning based approaches, yet it can be alleviated by several techniques as illustrated in Table 2. Collectively, Table 1 and Table 2 provide a thorough understanding of the challenges in OOD generalization, and set the stage for developing strategies to address these issues.\nTo provide a fine-grained description of OOD generalization methods in NLP, we introduce key points of representative methods in different tasks from Tables 3-5, ranging from the scope and method to dataset and metric. We hope these materials can serve as a quick access to existing references for further exploration."
        }
    ],
    "title": "Out-of-Distribution Generalization in Natural Language Processing: Past, Present, and Future",
    "year": 2023
}