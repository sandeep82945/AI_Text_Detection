{
    "abstractText": "In speech translation, leveraging multimodal data to improve model performance and address limitations of individual modalities has shown significant effectiveness. In this paper, we harness the complementary strengths of speech and text to improve speech translation. However, speech and text are disparate modalities, we observe three aspects of modality gap that impede their integration in a speech translation model. To tackle these gaps, we propose Fuse-Speech-Text (FuseST), a crossmodal model which supports three distinct input modalities for translation: speech, text and fused speech-text. We leverage multiple techniques for cross-modal alignment and conduct a comprehensive analysis to assess its impact on speech translation, machine translation and fused speech-text translation. We evaluate FuseST on MuST-C, GigaST and newstest benchmark. Experiments show that the proposed FuseST achieves an average 34.0 BLEU on MuST-C En\u2192De/Es/Fr (vs SOTA +1.1 BLEU). Further experiments demonstrate that FuseST does not degrade on MT task, as observed in previous works. Instead, it yields an average improvement of 3.2 BLEU over the pre-trained MT model. Code is available at https://github.com/WenbiaoYin/FuseST.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wenbiao Yin"
        },
        {
            "affiliations": [],
            "name": "Zhicheng Liu"
        },
        {
            "affiliations": [],
            "name": "Chengqi Zhao"
        },
        {
            "affiliations": [],
            "name": "Tao Wang"
        },
        {
            "affiliations": [],
            "name": "Jian Tong"
        },
        {
            "affiliations": [],
            "name": "Rong Ye"
        }
    ],
    "id": "SP:335f2988721492535f34e3a9764f118e1c29204f",
    "references": [
        {
            "authors": [
                "Alexei Baevski",
                "Steffen Schneider",
                "Michael Auli."
            ],
            "title": "vq-wav2vec: Self-supervised learning of discrete speech representations",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Alexei Baevski",
                "Yuhao Zhou",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "2020b. wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:12449\u201312460",
            "year": 2020
        },
        {
            "authors": [
                "Parnia Bahar",
                "Albert Zeyer",
                "Ralf Schl\u00fcter",
                "Hermann Ney."
            ],
            "title": "On using SpecAugment for end-to-end speech translation",
            "venue": "Proceedings of the 16th International Conference on Spoken Language Translation, Hong Kong. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Sameer Bansal",
                "Herman Kamper",
                "Karen Livescu",
                "Adam Lopez",
                "Sharon Goldwater."
            ],
            "title": "Pre-training on high-resource speech recognition improves lowresource speech-to-text translation",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Beck",
                "Trevor Cohn",
                "Gholamreza Haffari."
            ],
            "title": "Neural speech translation using lattice transformations and graph networks",
            "venue": "Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13),",
            "year": 2019
        },
        {
            "authors": [
                "Alexandre B\u00e9rard",
                "Laurent Besacier",
                "Ali Can Kocabiyikoglu",
                "Olivier Pietquin."
            ],
            "title": "End-to-end automatic speech translation of audiobooks",
            "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), page 6224\u20136228.",
            "year": 2018
        },
        {
            "authors": [
                "Alexandre B\u00e9rard",
                "Olivier Pietquin",
                "Laurent Besacier",
                "Christophe Servan."
            ],
            "title": "Listen and Translate: A Proof of Concept for End-to-End Speech-toText Translation",
            "venue": "NIPS Workshop on end-to-end learning for speech and audio processing, Barcelona,",
            "year": 2016
        },
        {
            "authors": [
                "N. Bertoldi",
                "M. Federico."
            ],
            "title": "A new decoder for spoken language translation based on confusion networks",
            "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding, 2005., pages 86\u201391.",
            "year": 2005
        },
        {
            "authors": [
                "Sanyuan Chen",
                "Chengyi Wang",
                "Zhengyang Chen",
                "Yu Wu",
                "Shujie Liu",
                "Zhuo Chen",
                "Jinyu Li",
                "Naoyuki Kanda",
                "Takuya Yoshioka",
                "Xiong Xiao"
            ],
            "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
            "venue": "IEEE Journal of Se-",
            "year": 2022
        },
        {
            "authors": [
                "Qiao Cheng",
                "Meiyuan Fan",
                "Yaqian Han",
                "Jin Huang",
                "Yitao Duan."
            ],
            "title": "Breaking the data barrier: Towards robust speech translation via adversarial stability training",
            "venue": "Proceedings of the 16th International Conference on Spoken Language Translation, Hong",
            "year": 2019
        },
        {
            "authors": [
                "Matti Di Gangi",
                "Robert Enyedi",
                "Alessandra Brusadin",
                "Marcello Federico."
            ],
            "title": "Robust neural machine translation for clean and noisy speech transcripts",
            "venue": "Proceedings of the 16th International Conference on Spoken Language Translation, Hong",
            "year": 2019
        },
        {
            "authors": [
                "Mattia A. Di Gangi",
                "Roldano Cattoni",
                "Luisa Bentivogli",
                "Matteo Negri",
                "Marco Turchi."
            ],
            "title": "MuST-C: a Multilingual Speech Translation Corpus",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2019
        },
        {
            "authors": [
                "Long Duong",
                "Antonios Anastasopoulos",
                "David Chiang",
                "Steven Bird",
                "Trevor Cohn."
            ],
            "title": "An attentional model for speech translation without transcription",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Qingkai Fang",
                "Rong Ye",
                "Lei Li",
                "Yang Feng",
                "Mingxuan Wang."
            ],
            "title": "STEMM: Self-learning with speech-text manifold mixup for speech translation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Marco Gaido",
                "Mattia A. Di Gangi",
                "Matteo Negri",
                "Marco Turchi."
            ],
            "title": "End-to-end speech-translation with knowledge distillation: FBK@IWSLT2020",
            "venue": "Proceedings of the 17th International Conference on Spoken Language Translation, pages 80\u201388, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Mattia A. Di Gangi",
                "Matteo Negri",
                "Marco Turchi."
            ],
            "title": "Adapting Transformer to End-to-End Spoken Language Translation",
            "venue": "Proc. Interspeech 2019, pages 1133\u20131137.",
            "year": 2019
        },
        {
            "authors": [
                "Chi Han",
                "Mingxuan Wang",
                "Heng Ji",
                "Lei Li."
            ],
            "title": "Learning shared semantic space for speech-to-text translation",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2214\u20132225.",
            "year": 2021
        },
        {
            "authors": [
                "Hirofumi Inaguma",
                "Tatsuya Kawahara",
                "Shinji Watanabe."
            ],
            "title": "Source and target bidirectional knowledge distillation for end-to-end speech translation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "S Indurthi",
                "H Han",
                "NK Lakumarapu",
                "B Lee",
                "I Chung",
                "S Kim",
                "C Kim."
            ],
            "title": "Data efficient direct speech-to-text translation with modality agnostic meta-learning",
            "venue": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal",
            "year": 2020
        },
        {
            "authors": [
                "Sathish Indurthi",
                "Mohd Abbas Zaidi",
                "Nikhil Kumar Lakumarapu",
                "Beomseok Lee",
                "Hyojung Han",
                "Seokchan Ahn",
                "Sangha Kim",
                "Chanwoo Kim",
                "Inchul Hwang."
            ],
            "title": "Task aware multi-task learning for speech to text tasks",
            "venue": "ICASSP 2021 - 2021 IEEE",
            "year": 2021
        },
        {
            "authors": [
                "Ye Jia",
                "Melvin Johnson",
                "Wolfgang Macherey",
                "Ron J Weiss",
                "Yuan Cao",
                "Chung-Cheng Chiu",
                "Naveen Ari",
                "Stella Laurenzo",
                "Yonghui Wu."
            ],
            "title": "Leveraging weakly supervised data to improve end-to-end speech-to-text translation",
            "venue": "ICASSP 2019-2019",
            "year": 2019
        },
        {
            "authors": [
                "Takatomo Kano",
                "Sakriani Sakti",
                "Satoshi Nakamura."
            ],
            "title": "Structured-based curriculum learning for endto-end english-japanese speech translation",
            "venue": "Proc. Interspeech 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Philipp Koehn."
            ],
            "title": "Europarl: A parallel corpus for statistical machine translation",
            "venue": "Proceedings of machine translation summit x: papers, pages 79\u201386.",
            "year": 2005
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson."
            ],
            "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2018
        },
        {
            "authors": [
                "Tsz Kin Lam",
                "Shigehiko Schamoni",
                "Stefan Riezler."
            ],
            "title": "Cascaded models with cyclic feedback for direct speech translation",
            "venue": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7508\u20137512.",
            "year": 2021
        },
        {
            "authors": [
                "Hang Le",
                "Juan Pino",
                "Changhan Wang",
                "Jiatao Gu",
                "Didier Schwab",
                "Laurent Besacier."
            ],
            "title": "Dual-decoder transformer for joint automatic speech recognition and multilingual speech translation",
            "venue": "Proceedings of the 28th International Conference on Com-",
            "year": 2020
        },
        {
            "authors": [
                "Bei Li",
                "Chuanhao Lv",
                "Zefan Zhou",
                "Tao Zhou",
                "Tong Xiao",
                "Anxiang Ma",
                "JingBo Zhu."
            ],
            "title": "On vision features in multimodal machine translation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2022
        },
        {
            "authors": [
                "Wei Li",
                "Can Gao",
                "Guocheng Niu",
                "Xinyan Xiao",
                "Hao Liu",
                "Jiachen Liu",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "UNIMO: Towards unified-modal understanding and generation via cross-modal contrastive learning",
            "venue": "Proceedings of the 59th Annual Meeting of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Jianhua Lin."
            ],
            "title": "Divergence measures based on the shannon entropy",
            "venue": "IEEE Transactions on Information theory, 37(1):145\u2013151.",
            "year": 1991
        },
        {
            "authors": [
                "Pierre Lison",
                "J\u00f6rg Tiedemann."
            ],
            "title": "OpenSubtitles2016: Extracting large parallel corpora from movie and TV subtitles",
            "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\u201916), pages 923\u2013929, Portoro\u017e,",
            "year": 2016
        },
        {
            "authors": [
                "Yinhan Liu",
                "Jiatao Gu",
                "Naman Goyal",
                "Xian Li",
                "Sergey Edunov",
                "Marjan Ghazvininejad",
                "Mike Lewis",
                "Luke Zettlemoyer."
            ],
            "title": "Multilingual denoising pretraining for neural machine translation",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2020
        },
        {
            "authors": [
                "Yuchen Liu",
                "Hao Xiong",
                "Jiajun Zhang",
                "Zhongjun He",
                "Hua Wu",
                "Haifeng Wang",
                "Chengqing Zong."
            ],
            "title": "End-to-End Speech Translation with Knowledge Distillation",
            "venue": "Proc. Interspeech 2019, pages 1128\u2013 1132.",
            "year": 2019
        },
        {
            "authors": [
                "Michael McAuliffe",
                "Michaela Socolof",
                "Sarah Mihuc",
                "Michael Wagner",
                "Morgan Sonderegger."
            ],
            "title": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi",
            "venue": "Proc. Interspeech 2017, pages 498\u2013502.",
            "year": 2017
        },
        {
            "authors": [
                "Vassil Panayotov",
                "Guoguo Chen",
                "Daniel Povey",
                "Sanjeev Khudanpur."
            ],
            "title": "Librispeech: An asr corpus based on public domain audio books",
            "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5206\u20135210.",
            "year": 2015
        },
        {
            "authors": [
                "Daniel S. Park",
                "William Chan",
                "Yu Zhang",
                "Chung-Cheng Chiu",
                "Barret Zoph",
                "Ekin Dogus Cubuk",
                "Quoc V. Le."
            ],
            "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
            "venue": "Proc. Interspeech 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Stephan Peitz",
                "Simon Wiesler",
                "Markus Nu\u00dfbaum-Thom",
                "Hermann Ney."
            ],
            "title": "Spoken language translation using automatically transcribed text in training",
            "venue": "Proceedings of the 9th International Workshop on Spoken Language Translation: Papers, pages 276\u2013",
            "year": 2012
        },
        {
            "authors": [
                "Juan Pino",
                "Qiantong Xu",
                "Xutai Ma",
                "Mohammad Javad Dousti",
                "Yun Tang."
            ],
            "title": "Self-Training for Endto-End Speech Translation",
            "venue": "Proc. Interspeech 2020, pages 1476\u20131480.",
            "year": 2020
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013 191, Brussels, Belgium. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Elizabeth Salesky",
                "Matthias Sperber",
                "Alexander Waibel."
            ],
            "title": "Fluent translations from disfluent speech in end-to-end speech translation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2019
        },
        {
            "authors": [
                "Kihyuk Sohn."
            ],
            "title": "Improved deep metric learning with multi-class n-pair loss objective",
            "venue": "Advances in neural information processing systems, 29.",
            "year": 2016
        },
        {
            "authors": [
                "Matthias Sperber",
                "Graham Neubig",
                "Jan Niehues",
                "Alex Waibel."
            ],
            "title": "Neural lattice-to-sequence models for uncertain inputs",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1380\u20131389, Copenhagen,",
            "year": 2017
        },
        {
            "authors": [
                "Matthias Sperber",
                "Graham Neubig",
                "Ngoc-Quan Pham",
                "Alex Waibel."
            ],
            "title": "Self-attentional models for lattice inputs",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1185\u20131197, Florence, Italy. Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Yun Tang",
                "Hongyu Gong",
                "Ning Dong",
                "Changhan Wang",
                "Wei-Ning Hsu",
                "Jiatao Gu",
                "Alexei Baevski",
                "Xian Li",
                "Abdelrahman Mohamed",
                "Michael Auli",
                "Juan Pino."
            ],
            "title": "Unified speech-text pre-training for speech translation and recognition",
            "venue": "Proceedings",
            "year": 2022
        },
        {
            "authors": [
                "Yun Tang",
                "Juan Pino",
                "Xian Li",
                "Changhan Wang",
                "Dmitriy Genzel."
            ],
            "title": "Improving speech translation by understanding and learning from the auxiliary text translation task",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Yun Tang",
                "Juan Pino",
                "Changhan Wang",
                "Xutai Ma",
                "Dmitriy Genzel."
            ],
            "title": "A general multi-task learning framework to leverage text data for speech to text tasks",
            "venue": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Laura Cross Vila",
                "Carlos Escolano",
                "Jos\u00e9 A.R. Fonollosa",
                "Marta Ruiz Costa-juss\u00e0."
            ],
            "title": "End-toend speech translation with the transformer",
            "venue": "IberSPEECH Conference.",
            "year": 2018
        },
        {
            "authors": [
                "Hari Krishna Vydana",
                "Martin Karafi\u00e1t",
                "Katerina Zmolikova",
                "Luk\u00e1\u0161 Burget",
                "Honza \u010cernock\u00fd."
            ],
            "title": "Jointly trained transformers models for spoken language translation",
            "venue": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and",
            "year": 2021
        },
        {
            "authors": [
                "Chengyi Wang",
                "Yu Wu",
                "Shujie Liu",
                "Zhenglu Yang",
                "Ming Zhou."
            ],
            "title": "Bridging the gap between pretraining and fine-tuning for end-to-end speech translation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9161\u20139168.",
            "year": 2020
        },
        {
            "authors": [
                "Chengyi Wang",
                "Yu Wu",
                "Shujie Liu",
                "Ming Zhou",
                "Zhenglu Yang."
            ],
            "title": "Curriculum pre-training for end-to-end speech translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3728\u20133738, Online. Asso-",
            "year": 2020
        },
        {
            "authors": [
                "Ron J. Weiss",
                "Jan Chorowski",
                "Navdeep Jaitly",
                "Yonghui Wu",
                "Z. Chen"
            ],
            "title": "Sequence-to-sequence models can directly translate foreign speech",
            "year": 2017
        },
        {
            "authors": [
                "Chen Xu",
                "Bojie Hu",
                "Yanyang Li",
                "Yuhao Zhang",
                "Shen Huang",
                "Qi Ju",
                "Tong Xiao",
                "Jingbo Zhu."
            ],
            "title": "Stacked acoustic-and-textual encoding: Integrating the pre-trained models into speech translation encoders",
            "venue": "Proceedings of the 59th Annual Meet-",
            "year": 2021
        },
        {
            "authors": [
                "Rong Ye",
                "Mingxuan Wang",
                "Lei Li."
            ],
            "title": "End-toend speech translation via cross-modal progressive training",
            "venue": "Proc. Interspeech 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Rong Ye",
                "Mingxuan Wang",
                "Lei Li."
            ],
            "title": "Crossmodal contrastive learning for speech translation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Rong Ye",
                "Chengqi Zhao",
                "Tom Ko",
                "Chutong Meng",
                "Tao Wang",
                "Mingxuan Wang",
                "Jun Cao"
            ],
            "title": "Gigast: A 10,000-hour pseudo speech translation corpus",
            "venue": "In Proc. Interspeech",
            "year": 2023
        },
        {
            "authors": [
                "Pei Zhang",
                "Niyu Ge",
                "Boxing Chen",
                "Kai Fan."
            ],
            "title": "Lattice transformer for speech translation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6475\u2013 6484, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Jiawei Zhao",
                "Wei Luo",
                "Boxing Chen",
                "Andrew Gilman."
            ],
            "title": "Mutual-learning improves end-toend speech translation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3989\u20133994, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Renjie Zheng",
                "Junkun Chen",
                "Mingbo Ma",
                "Liang Huang."
            ],
            "title": "Fused acoustic and text encoding for multimodal bilingual pretraining and speech translation",
            "venue": "International Conference on Machine Learning, pages 12736\u201312746. PMLR.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Speech translation (ST) accepts speech signals as the input and outputs target translation. Speech translation can be broadly categorized into cascade system and end-to-end speech translation (E2E ST). Cascade system (Sperber et al., 2017; Zhang et al., 2019; Lam et al., 2021) usually combines automatic speech recognition (ASR) and machine translation (MT). The MT subsystem uses ASR transcripts as input, which provide clear expression but may contain errors stemming from ASR. While E2E ST (Tang et al., 2021a; Fang et al., 2022; Ye et al.,\n\u2217Corresponding author\n2022) can directly map speech signals to the target translation, thus avoiding the problem of error propagation.\nWe observe a noticeable modality distribution difference between speech and ASR transcript as shown in Figure 1a. Speech signals contain abundant information, such as paralinguistics and speaker characteristics, but they are harder to model and more susceptible to noise. On the other hand, ASR transcripts are clear but may contain errors. For example, as shown in Figure 1b, the speech signal conveys the message \"I\u2019m delight to be here\", but the speech signal contains numerous blank segments and background noise. E2E ST may encounter challenges in accurately extracting information directly from speech, particularly in the presence of noise, without compromising translation quality. Meanwhile, the ASR model mistakenly transcribes the speech as \"I\u2019d like to be here\". The MT subsystem may proceed with translation without awareness of errors in \"I\u2019d like\".\nInspired by these findings, we propose a model that fuses speech and ASR transcript (fused speechtext) as input to leverage their complementary strengths to improve speech translation. As shown in Figure 1b, our model supports three distinct modalities of input for translation: speech, text and fused speech-text.\nHowever, speech and text are disparate modalities, we observe three aspects of modality gap that impede their integration in a speech transla-\ntion model: 1. The speech representation is in continuous space, while the text representation is in discrete space. 2. When using speech and golden transcript as input, the model relies heavily on the golden transcript information and neglects the speech information. This behavior is because the golden transcript is more straightforward and contains precise semantic information, adequate for achieving high translation quality. The model will learn a shortcut to rely solely on the golden transcript. In contrast, speech representations obtained through the pre-trained model are harder to model, they include four types of information (Chen et al., 2022): content, semantics, paralinguistics and speaker characteristics. However, when using speech and ASR transcript as input, solely relying on textual information is insufficient to achieve good translation quality. We expect the model to incorporate more speech information. 3. Diverse modality inputs lead to distinct hidden states in the encoder and distinct distributions in the decoder.\nWe propose several methods to bridge the modality gap to better integrate speech and text. 1. We explore mapping continuous speech representation to a discrete space using a codebook to align with the text representation. 2. We adopt the prompt tags implicitly guide the model to utilize more speech information when the transcript is inaccurate. We further explore how to explicitly guide the model to fuse speech and text. Meanwhile, we apply Crossmodal Contrastive Learning (CCL, Sohn, 2016) to reduce the gap between the model semantic of speech and its corresponding transcript. 3. We adopt Cross-Attentive Regularization (CAR, Tang et al., 2021a) to align the states of the encoder and Cross-Modal Regularization (CMR) to align the distribution of the decoder.\nOur contributions are summarized as follows:\n\u2022 We propose a model that fuses speech and text to improve speech translation, which supports three distinct input modalities for translation: speech, text and fused speech-text.\n\u2022 To fuse speech and text as input and leverage their complementary strengths, we conduct a comprehensive analysis of the modality gap between speech and text. We propose targeted improvements to bridge the modality gap between speech and text.\n\u2022 Our experiments show that our model achieves an average 34.0 BLEU on MuST-\nC En\u2192De/Es/Fr (vs SOTA +1.1 BLEU) and achieves an average improvement of 3.2 BLEU over the pre-trained MT model on MuST-C."
        },
        {
            "heading": "2 Related Work",
            "text": "Cascade ST Cascade ST, achieved by concatenating ASR and MT components, has been extensively employed in commercial speech translation systems. However, cascade ST is vulnerable to challenges such as error propagation and high latency. To overcome the error propagation, ( Bertoldi and Federico, 2005; Beck et al., 2019; Sperber et al., 2019) proposed to feed the MT system with ASR data structures; ( Peitz et al., 2012; Cheng et al., 2019; Di Gangi et al., 2019a) proposed to make MT robust to ASR errors, for instance by training it on parallel data incorporating factual or emulated ASR errors. End-to-End ST To overcome the error propagation and high latency in the cascade ST systems, (B\u00e9rard et al., 2016; Duong et al., 2016) proposed an end-to-end architecture for speech translation, which has attracted extensive attention ( Vila et al., 2018; Salesky et al., 2019; Gangi et al., 2019; Inaguma et al., 2021; Zhao et al., 2021). However, it is difficult to train an end-to-end speech translation model directly, primarily due to the inherent variability and complexity of speech signals and the scarcity of high-quality speech-translation datasets. Some training methods like pretraining ( Weiss et al., 2017; B\u00e9rard et al., 2018; Bansal et al., 2019; Wang et al., 2020a; Tang et al., 2021b), multi-task learning( Le et al., 2020; Vydana et al., 2021; Ye et al., 2021; Tang et al., 2022), data augmentation ( Park et al., 2019; Jia et al., 2019; Bahar et al., 2019; Pino et al., 2020), meta-learning ( Indurthi et al., 2020), contrastive learning ( Li et al., 2021; Ye et al., 2022), knowledge distillation ( Liu et al., 2019; Tang et al., 2021a) and curriculum learning ( Kano et al., 2017; Wang et al., 2020b), are proved to be effective."
        },
        {
            "heading": "3 Methods",
            "text": ""
        },
        {
            "heading": "3.1 Problem Formulation",
            "text": "The speech translation corpus is usually comprised of triples that include speech, transcript and target translation, which can be denoted as D = (s, x, y). Here, s is an audio sequence, x is the corresponding transcript and y is the corresponding target translation. Our model supports three distinct input modal-\nities for translation: speech (ST: s \u2192 y), text (MT: x \u2192 y) and fused speech-text (FT: s+ x \u2192 y)."
        },
        {
            "heading": "3.2 Model Framework",
            "text": "As shown in Figure 2, our model consists of four sub-modules: Speech Encoder, Speech-Text Fusion Module, Transformer Encoder and Transformer Decoder. Speech Encoder The speech encoder(S-Enc) consists of Wav2vec2.0 (Baevski et al., 2020b) and two additional convolutional layers. The input is a raw waveform signal sampled at 16kHz, and Wav2vec2.0 is used to extract low-level speech representations from it. The two additional convolutional layers with stride 2 to shrink the speech length by a factor of 4. A greater degree of downsampling would have led to information loss, while a lesser degree of downsampling could have resulted in modal misalignment and compromised performance. Denote a = S-Enc(s) as the speech representation.\nTo reduce the number of parameters and facilitate knowledge transfer, we share the Transformer encoder and Transformer decoder for ST, MT and FT. Transformer Encoder and Transformer Decoder The Transformer encoder and Transformer decoder are composed of Ne transformer encoder layers and Nd transformer decoder layers, respectively, with the same configuration as the original implementation (Vaswani et al., 2017). We fisrt pre-train the model on external MT data and then optimize the whole model by minimizing the final loss. For the MT task, the input of the Transformer encoder is the embedding of transcript e = Emb(x). For the ST task, the input is the audio output representation of the speech encoder a = S-Enc(s). For the FT task, the input is the fused speechtext representation f (see details in Section 3.3). The Transformer encoder further extracts the highlevel semantic hidden representations and facilitates knowledge sharing across the three modalities. The Transformer decoder generates corresponding target translation for ST, MT and FT. Besides, we train our model with auxiliary ASR task to improve translation performance. The training losses of ST, MT, FT and ASR are as follows:\nLST = \u2212 \u2211 n logP (yn|sn) (1)\nLMT = \u2212 \u2211 n logP (yn|xn) (2)\nLFT = \u2212 \u2211 n logP (yn|sn, xn) (3)\nLASR = \u2212 \u2211 n logP (xn|sn) (4)\nLST , LMT , LFT and LASR are cross-entropy losses on <speech, target>, <transcript, target>, <speech, transcript, target> and <speech, transcript>, respectively."
        },
        {
            "heading": "3.3 Fusing Speech and Text",
            "text": "To leverage the complementary strengths between speech and text, we propose the Fuse-SpeechText(FuseST) method. We first introduce FuseST in this section and later show how to bridge the modality gap between speech and text.\nHere, we utilize an open-source ASR model to construct a dataset of <speech, ASR transcript, target> pair from the original dataset of <speech, golden transcript, target> pair. Given a speechtranscript-target pair (s, x, y), the transcript x could be an ASR transcript or golden transcript, and the fused speech-text representation f is defined as:\nf = Concat(P s, S-Enc(s), P t, P f , Emb(x)) (5) where P s and P t are prompt tags to identify whether the input modal is speech or text, and P f is a prompt tag (<golden>/<asr>) to indicate whether the transcript is manually annotated or generated through ASR system. The prompt tags serve as implicit guidelines for the model, prompting it to harness a higher degree of speech information when the transcript is inaccurate, while inducing a decreased reliance on speech information when the transcript is deemed accurate."
        },
        {
            "heading": "3.4 Align Speech and Text with FuseST",
            "text": "As we analyzed in Section 1, we observe three aspects of the modality gap between speech and text. We propose several methods to bridge the modality gap between speech and text. In addition to the implicit guidance mentioned above, we introduce three additional methods in this section: Crossmodal Contrastive Learning, Cross-Attentive Regularization and Cross-Modal Regularization. Cross-modal Contrastive Learning Given a positive example of speech-transcript (s, x) pair, we\ncan get speech representation a = S-Enc(s), transcript representation e = Emb(x). We randomly pick a set of B \u2212 1 transcripts { e\u2212i }B\u22121 i=1\nfrom the same batch as negative examples. For speech representation a and transcript representation e, we first average them in terms of the time dimension and apply the multi-class N-pair contrastive loss (Sohn, 2016):\nu = MeanPool(a) (6)\nv = MeanPool(e) (7)\nLCCL = \u2212 \u2211 s,x log exp(sim(u, v)/\u03c4)\u2211\nej\u2208A exp(sim(u, v(ej))/\u03c4) (8)\nwhere A = {e} \u222a { e\u2212i }B\u22121 i=1\n, \u03c4 is the temperature hyper-parameter and sim is the cosine similarity function. Cross-Attentive Regularization The crossattentive regularization (Tang et al., 2021a) (CAR) can increase the similarity among distinct modalities. The essence of CAR is to use a similarity matrix to project a tensor sequence onto a space of equivalent length as another tensor sequence, then compute L2 loss between the two sequences. Here, we utilize the CAR to compute losses separately between speech representation a and fused speech-text representation f and\nbetween text representation e and fused speech-text representation f . The CAR loss is defined as:\nLCAR = LCAR(a, f) + LCAR(e, f) (9)\nCross-Modal Regularization To bridge the modality gap in inference, we endeavor to optimize the congruity of the ultimate distributions of ST, MT and FT:\nP (y|s) = P (y|s, x) = P (y|x) (10)\nThe ST task is more difficult than the MT task since the speech signals are harder to model and more susceptible to noise. Previous works (Liu et al., 2019; Gaido et al., 2020; Tang et al., 2021a) utilize knowledge distillation to facilitate knowledge acquisition by an ST model from a welltrained MT model. However, in our work, the transcript may contain errors; the accuracy of the FT task is usually much higher than the corresponding ST and MT task. We designate the FT task as the teacher while ST and MT tasks as the student, minimizing the loss between the student and teacher outputs. The KD loss is defined as:\nLKD = LKD(a, f) + LKD(e, f) (11)\nFurthermore, we minimize the Jensen-Shannon Divergence (Lin, 1991) (JSD) loss between the three output distributions, which is:\nLJSD = \u2211 n (JSD {p(yn|an)||p(yn|, fn)}\n+JSD {p(yn|en)||p(yn|fn)}) (12)\nThe final training objective is as follows:\nL =\u03b1LST + \u03b1LMT + (1\u2212 \u03b1)LKD + LFT +LASR + LCCL + \u03b2LCAR + LJSD\n(13) where \u03b1 and \u03b2 are predefined hyper-parameters."
        },
        {
            "heading": "3.5 Inference",
            "text": "Our model supports three distinct modalities for translation: speech, text and fused speech-text. For FT task, given an audio sequence s, we use an open-source ASR model to get its corresponding transcript x, then fuse the audio sequence s and its corresponding transcript x as input and output target translation."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setups",
            "text": "ST Datasets We conduct experiments on MuSTC1 (Di Gangi et al., 2019b) and GigaST2 (Ye et al.). MuST-C is a multilingual speech translation dataset that contains translations from English to 8 languages. MuST-C contains several hundred hours of audio recordings from English TED Talks; we conduct experiments on MuST-C En\u2192De/Es/Fr. We use the dev set for development and the tst-COMMON set for test. GigaST is a large-scale pseudo speech translation dataset created by translating the text in GigaSpeech. We conduct experiments on GigaST En\u2192Zh, and test on in-house cgtn, zhiyuan and aiconf datasets. We utilize an open-source ASR model (whisper base.en 3) to construct a dataset of <speech, ASR transcript, target> pair from original ST dataset. The Word Error Rate (WER) of our constructed datasets are shown in the last column of Table 1. MT Datasets Our model allows us to use the external MT dataset for further training. We introduce external WMT datasets for En\u2192De/Es/Fr and in-house MT dataset for En\u2192Zh. The detailed statistics of all datasets are shown in Table 1.\n1We use v1.0 https://ict.fbk.eu/must-c/ 2https://st-benchmark.github.io/resources/\nGigaST 3https://huggingface.co/openai/whisper-base. en\nModel Configuration For the speech encoder, we use Wav2vec2.04 following the base configuration, which is only pre-trained on Librispeech (Panayotov et al., 2015) without any finetuning. Two layers of CNNs after the Wav2vec2.0 with kernel size 5, stride size 2, padding 2 and hidden dimension 1024. The transformer encoder and decoder follow the base configuration, with hidden size hd = 512, 8 attention heads and 2048 FFN hidden states. We use Ne = 6 transformer encoder layers and Nd = 6 transformer decoder layers. Experiment Details We first pre-train our model on the external MT dataset; the learning rate is 5e-4. We then optimize our model by minimizing the final loss; the learning rate is 6e-5. We use the raw 16kHZ speech as input and jointly tokenize the bilingual text using SentencePiece (Kudo and Richardson, 2018). We use an Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.98 and 20k warm-up updates. The dropout is set to 0.15 and the value of label smoothing is set to 0.1. For the training loss, we set weight of LST and LMT \u03b1 = 0.8, contrastive temperature \u03c4 = 0.02 and weight of LCAR \u03b2 = 0.02. We use sacreBLEU5 (Post, 2018) to evaluate case-sensitive detokenized BLEU."
        },
        {
            "heading": "4.2 Baseline Systems",
            "text": "We compare our method with cascade models and end-to-end baseline models including: Espnet (Inaguma et al., 2021), W2V2-Transformer (Fang et al., 2022), Ye et al., 2021, Xu et al., 2021, MTL (Tang et al., 2021b), FAT-ST (Zheng et al., 2021), JT-S-MT (Tang et al., 2021a), Chimera (Han et al., 2021), XSTNet (Ye et al., 2021), SATE (Xu et al., 2021), STEMM (Fang et al., 2022), TaskAware (Indurthi et al., 2021), STPT (Tang et al., 2022), ConST (Ye et al., 2022).\nBesides, we implement several methods for fusing speech and text modalities. The only difference\n4https://dl.fbaipublicfiles.com/fairseq/ wav2vec/wav2vec_small.pt\n5https://github.com/mjpost/sacrebleu, sacreBLEU signature: nrefs:1 | bs:1000 | seed:12345 | case:mixed | eff:no | tok:13a | smooth:exp | version:2.0.0\nbetween our approach and others is the specific method for fusing speech and text. SA-CTR: our implementation involved drawing inspiration from the image and text fusion techniques employed in Li et al.\u2019s (2022) to propose a method for fusing speech and text. We utilize the method from Baevski et al. (2020a) to map the speech representation e onto a discrete space using a codebook; then we concatenate discrete speech representation and text representation as ours. CodebookGumbel-Softmax: the Gumbel-Softmax quantization computes logits representing the codebook vectors; Codebook-K-means: K-means vector quantization computes the distance to all codeword vector and chooses the closest."
        },
        {
            "heading": "4.3 Main Results",
            "text": "Comparison with End-to-End Baselines As shown in Table 2, we compare our model with several strong end-to-end baselines. Many existing\nworks rely on additional auxiliary data for better performance, e.g. large-scale MT data and unlabeled audio data. In the table, we provide a summary of the auxiliary data employed by these baselines, with a \u2713 denoting its usage in the corresponding column. Our E2E FuseST-ST achieves comparable results with the previous best models. When fusing speech and ASR transcript as input, our FuseST-FT outperforms SOTA by an average 1.1 BLEU on MuST-C, demonstrating the superiority of our approach. Comparison with Cascade Baselines We compare our model with several strong cascade systems. W2V2-Transformer, Ye et al. (2021) and Xu et al. (2021) provided three strong cascade systems trained using MuST-C and external ASR and MT data. As shown in Table 2, our E2E FuseSTST achieves comparable results with these strong cascade models, while our FuseST-FT significantly outperforms these strong cascade models.\nComparison with MT Baselines We fisrt pre-train our model on external MT data and then jointly train on multiple tasks. Previous work has encountered catastrophic forgetting problems on MT task during joint training (Fang et al., 2022), which significantly degrades performance on MT tasks. We evaluate our model on the MT task and show the result in Table 3 and Table 4. Our model achieves significant improvement on the MT task instead of a decline in performance. Our model even outperforms pre-trained 24E6D (24 transformer encoder layers and 6 transformer decoder layers) MT on MuST-C. However, with the increase of training data, our model performs lower than pre-trained 24E6D MT when using ASR transcript as input (En\u2192Zh). Nevertheless, our model can still outperform pre-trained 24E6D MT when fusing speech and text as input on En\u2192Zh."
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "As shown in Equation 13, our training objective contains eight terms. In addition to the crossentropy objective LST ,LMT ,LFT , we investigate the effects of the other auxiliary training objectives. By gradually removing each loss, Table 5 shows the improvements brought by each auxiliary training objective."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Is FuseST better than other fusion methods?",
            "text": "We compare our model with different fusion methods, such as SA-CTR and STEMM. SA-CTR proposed a selective attention and gated fusion mechanism to fuse two different modalities; STEMM proposed the speech-text manifold mixup to mix up the representation sequences of different modalities. Our model achieves better results than theirs by utilizing a prompt-based approach."
        },
        {
            "heading": "5.2 Should speech representation be discrete or continuous?",
            "text": "As Section 1 mentions, speech input representation is in continuous space, while text input representation is in discrete space. We utilize the method from Baevski et al. (2020a) to map the speech representation onto a discrete space using a codebook. However, Codebook-Gumbel-Softmax and Codebook-K-means led to a decrease in BLEU score (shown in Table 4). Our conjecture here is that the existing speech pre-trained models extract continuous features that undergo a mapping process to a discrete space, resulting in the loss of audio information and a subsequent reduction in the BLEU score. Nonetheless, if the speech pretrained models can extract high-quality discrete features, it is plausible that such discrete features could enhance performance."
        },
        {
            "heading": "5.3 With the increasing advancements of speech pre-trained models, does the fusion of speech and text remain effective?",
            "text": "Here, we report the results of Wav2vec2.0, HuBERT Large6 and HuBERT Extra Large7, which are widely used in speech translation. As shown in Tabel 6, as the strength of speech pre-trained models increases, the performance of the models on ST and FT improves. Nevertheless, as speech pre-trained models undergo further advancements, the marginal gains in speech translation resulting from text fusion have shown a diminishing trend. This phenomenon can be ascribed to the progressive refinement of speech representations, in contrast to the relatively inferior quality of our textual representations (WER \u2248 20). By incorporating stronger textual representations, the enhancement in speech translation through text would become more pronounced."
        },
        {
            "heading": "5.4 How does our fusing strategy perform on different levels of ASR transcript quality?",
            "text": "We conduct an empirical study to examine the efficacy of our approach in enhancing BLEU score through the fusion of speech information under different WER present in the ASR transcript. As shown in Figure 3, when the word error rate of ASR transcript is minimal (0 \u2264 WER < 5), the fusion of speech information results in a slight decrease in BLEU score. This behavior is because the transcript is adequate for achieving high translation quality, and the fusion of speech information may introduce noise. As the WER increases, the ad-\n6https://dl.fbaipublicfiles.com/hubert/hubert_ large_ll60k.pt\n7https://dl.fbaipublicfiles.com/hubert/hubert_ xtralarge_ll60k.pt\nModels CASE 1 Ref. src: I\u2019m extraordinarily delight to be here. asr: I\u2019m extraordinary. I\u2019d like to be here. tgt: \u6211\u975e\u5e38\u9ad8\u5174\u6765\u5230\u8fd9\u91cc\u3002 FuseST-MT tgt \u6211\u975e\u540c\u5bfb\u5e38,\u6211\u60f3\u5728\u8fd9\u91cc\u3002 FuseST-ST tgt \u6211\u975e\u5e38\u9ad8\u5174\u6765\u5230\u8fd9\u91cc\u3002 FuseST-FT tgt \u6211\u975e\u5e38\u9ad8\u5174\u6765\u5230\u8fd9\u91cc\u3002\nCASE 2\nlations, and the blue strikethrough indicates missing translation.\nvantages of integrating speech information become more pronounced."
        },
        {
            "heading": "5.5 Is our model robust to different ASR errors?",
            "text": "In the GigaST En\u2192Zh experiment, we utilize an ASR model different from the one used to construct the training set to generate the ASR transcripts for the test (in-house cgtn, zhiyuan and aiconf). As shown in Table 4, using fused speech-text for translation still outperforms using ASR transcript. Our model demonstrates strong robustness to ASR errors under various distributions."
        },
        {
            "heading": "5.6 What is the comparative effectiveness between implicit guidance and explicit guidance in the fusion process?",
            "text": "In Section 3.3, we utilize prompt tags to implicitly guide the model to fuse speech and text. This section further explores how to explicitly guide the model to fuse speech and text. We propose an additional auxiliary task named Align-Mask: we first use the Montreal Forced Aligner(MFA, (McAuliffe et al., 2017)) toolkit to get word-level speech-transcript alignment pairs; then we random mask a consecutive sequence of 1 to 4 words in the text with a probability of 15%, we only preserve the speech segment corresponding to the masked text segment; then we predict the masked words. We aspire for the model to learn the correspondence between speech and text to fuse speech and text better. However, Align-Mask performs worse as shown in Table 4. Our hypothesis is that using MFA to align introduces errors, which affect the effectiveness of Align-Mask. In the future, we will further explore explicit guidance fusion without external tools."
        },
        {
            "heading": "6 Case Study",
            "text": "In this section, we present several cases generated by FuseST-MT, FuseST-ST and FuseST-FT. In the first case, the ASR model mistakenly transcribes \"delight\" as \"I\u2019d like\" due to the highly similar pronunciations of these two words. FuseST-MT fails to generate the correct translation as a result of errors present in the ASR transcript. Meanwhile, FuseST-ST produces omissions, as modeling direct speech to target translation proves to be more challenging. Notably, only FuseST-FT translates the sentence correctly, leveraging the complementary strengths of speech and text. In the second case, the speech signal contains numerous background noises; the ASR model mistakenly transcribes the \"they are\" as \"their\", FuseST-MT and FuseST-ST are mistranslated, and only FuseST-FT translates correctly."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we propose FuseST, a cross-modal model which supports three distinct input modalities for translation: speech, text and fused speechtext. We comprehensively analyze the modality gap between speech and text and, utilize multiple techniques to bridge the modality gap. We then fuse speech and text to improve speech translation. Experiments and analysis demonstrate the effectiveness of our proposed method.\nLimitations\nThis work improves speech translation by fusing speech and text, but the model is far from being achieved for industrialgrade implementations. Although the ChatGPT and Whisper models exhibit superior speech-to-text capabilities compared to our model, we maintain that fusing speech and text remains a viable approach in the era of largescale models. There are two significant limitations in this study that could be addressed in future research. First, our model still relies on an ASR system to transcribe speech into text, which does not address the issue of high latency in the cascade system. Second, our model needs labeled data for training, especially the <speech, transcript, target> pair. Speech data is exceptionally scarce, and obtaining speech data for many languages around the world is particularly challenging."
        }
    ],
    "title": "Improving Speech Translation by Fusing Speech and Text",
    "year": 2023
}