{
    "abstractText": "By modeling the interaction among instances and avoiding error propagation, Set Prediction Networks (SPNs) achieve state-of-the-art performance on the tasks of named entity recognition and relation triple extraction respectively. However, how to jointly extract entities and relation triples via SPNs remains an unexplored problem, where the main challenge is the maintenance of coherence between the predicted entity/relation sets during one-pass generation. In this work, we present Bipartite Set Prediction Network (BiSPN), a novel joint entity-relation extraction model that can efficiently generate entity set and relation set in parallel. To overcome the challenge of coherence, BiSPN is equipped with a novel bipartite consistency loss as well as an entity-relation linking loss during training. Experiments on three biomedical/clinical datasets and a generaldomain dataset show that BiSPN achieves new state of the art in knowledge-intensive scene and performs competitively in general-domain, while being more efficient than two-stage joint extraction methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuxin He"
        },
        {
            "affiliations": [],
            "name": "Buzhou Tang"
        }
    ],
    "id": "SP:f60d576c9e9471bed50b62d0b55b8f6ced094b1c",
    "references": [
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko."
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "Computer Vision \u2013 ECCV 2020, pages 213\u2013229, Cham. Springer International Pub-",
            "year": 2020
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Ziqing Yang."
            ],
            "title": "Pre-training with whole word masking for chinese bert",
            "venue": "IEEE Transactions on Audio, Speech and Language Processing.",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Kalpit Dixit",
                "Yaser Al-Onaizan."
            ],
            "title": "Span-level model for relation extraction",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5308\u20135314, Florence, Italy. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Harsha Gurulingappa",
                "Abdul Mateen Rajput",
                "Angus Roberts",
                "Juliane Fluck",
                "Martin Hofmann-Apitius",
                "Luca Toldo"
            ],
            "title": "Development of a benchmark corpus to support the automatic extraction of drugrelated adverse effects from medical case reports",
            "year": 2012
        },
        {
            "authors": [
                "Hasham Ul Haq",
                "Veysel Kocaman",
                "David Talby."
            ],
            "title": "Mining Adverse Drug Reactions from Unstructured Mediums at Scale, pages 361\u2013375",
            "venue": "Springer International Publishing, Cham.",
            "year": 2023
        },
        {
            "authors": [
                "Bin Ji",
                "Jie Yu",
                "Shasha Li",
                "Jun Ma",
                "Qingbo Wu",
                "Yusong Tan",
                "Huijun Liu."
            ],
            "title": "Span-based joint entity and relation extraction with attention-based spanspecific and contextual semantic representations",
            "venue": "Proceedings of the 28th International Conference on",
            "year": 2020
        },
        {
            "authors": [
                "Hrant Khachatrian",
                "Lilit Nersisyan",
                "Karen Hambardzumyan",
                "Tigran Galstyan",
                "Anna Hakobyan",
                "Arsen Arakelyan",
                "Andrey Rzhetsky",
                "Aram Galstyan"
            ],
            "title": "BioRelEx 1.0: Biological relation extraction benchmark",
            "venue": "In Proceedings of the 18th BioNLP",
            "year": 2019
        },
        {
            "authors": [
                "H.W. Kuhn."
            ],
            "title": "The hungarian method for the assignment problem",
            "venue": "Naval Research Logistics Quarterly, 2(1-2):83\u201397.",
            "year": 1955
        },
        {
            "authors": [
                "Tuan Lai",
                "Heng Ji",
                "ChengXiang Zhai",
                "Quan Hung Tran."
            ],
            "title": "Joint biomedical entity and relation extraction with knowledge-enhanced collective inference",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Xiaoya Li",
                "Fan Yin",
                "Zijun Sun",
                "Xiayu Li",
                "Arianna Yuan",
                "Duo Chai",
                "Mingxin Zhou",
                "Jiwei Li."
            ],
            "title": "Entityrelation extraction as multi-turn question answering",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1340\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "Cite arxiv:1711.05101Comment: Published as a conference paper at ICLR 2019.",
            "year": 2017
        },
        {
            "authors": [
                "Yaojie Lu",
                "Qing Liu",
                "Dai Dai",
                "Xinyan Xiao",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun",
                "Hua Wu."
            ],
            "title": "Unified structure generation for universal information extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "jani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "year": 2019
        },
        {
            "authors": [
                "Yongliang Shen",
                "Xiaobin Wang",
                "Zeqi Tan",
                "Guangwei Xu",
                "Pengjun Xie",
                "Fei Huang",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "Parallel instance query network for named entity recognition",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Jianlin Su"
            ],
            "title": "Efficient globalpointer",
            "year": 2022
        },
        {
            "authors": [
                "Dianbo Sui",
                "Yubo Chen",
                "Kang Liu",
                "Jun Zhao",
                "Xiangrong Zeng",
                "Shengping Liu."
            ],
            "title": "Joint Entity and Relation Extraction with Set Prediction Networks",
            "venue": "arXiv e-prints, page arXiv:2011.01675.",
            "year": 2020
        },
        {
            "authors": [
                "Zeqi Tan",
                "Yongliang Shen",
                "Xuming Hu",
                "Wenqi Zhang",
                "Xiaoxia Cheng",
                "Weiming Lu",
                "Yueting Zhuang."
            ],
            "title": "Query-based instance discrimination network for relational triple extraction",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Nat-",
            "year": 2022
        },
        {
            "authors": [
                "Zeqi Tan",
                "Yongliang Shen",
                "Shuai Zhang."
            ],
            "title": "A sequence-to-set network for nested named entity recognition",
            "venue": "Proceedings of the 30th International Joint Conference on Artificial Intelligence, IJCAI-21.",
            "year": 2021
        },
        {
            "authors": [
                "Wei Tang",
                "Benfeng Xu",
                "Yuyue Zhao",
                "Zhendong Mao",
                "Yifeng Liu",
                "Yong Liao",
                "Haiyong Xie."
            ],
            "title": "UniRel: Unified representation and interaction for joint relational triple extraction",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Nat-",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Proceedings of the 31st International",
            "year": 2017
        },
        {
            "authors": [
                "Christopher Walker",
                "Stephanie Strassel",
                "Kazuaki Maeda."
            ],
            "title": "The automatic content extraction (ace) program tasks, data, and evaluation",
            "venue": "Linguistic Data Consortium, page 57, Philadelphia.",
            "year": 2006
        },
        {
            "authors": [
                "Yijun Wang",
                "Changzhi Sun",
                "Yuanbin Wu",
                "Hao Zhou",
                "Lei Li",
                "Junchi Yan."
            ],
            "title": "UniRE: A unified label space for entity relation extraction",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Yucheng Wang",
                "Bowen Yu",
                "Yueyang Zhang",
                "Tingwen Liu",
                "Hongsong Zhu",
                "Limin Sun."
            ],
            "title": "TPLinker: Single-stage joint extraction of entities and relations through token pair linking",
            "venue": "Proceedings of the 28th International Conference on Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Zhiheng Yan",
                "Chong Zhang",
                "Jinlan Fu",
                "Qi Zhang",
                "Zhongyu Wei."
            ],
            "title": "A partition filter network for joint entity and relation extraction",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 185\u2013197, Online",
            "year": 2021
        },
        {
            "authors": [
                "Deming Ye",
                "Yankai Lin",
                "Peng Li",
                "Maosong Sun."
            ],
            "title": "Packed levitated marker for entity and relation extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland,",
            "year": 2022
        },
        {
            "authors": [
                "Xiangrong Zeng",
                "Daojian Zeng",
                "Shizhu He",
                "Kang Liu",
                "Jun Zhao."
            ],
            "title": "Extracting relational facts by an end-to-end neural model with copy mechanism",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2018
        },
        {
            "authors": [
                "Zexuan Zhong",
                "Danqi Chen."
            ],
            "title": "A frustratingly easy approach for entity and relation extraction",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Extracting entities and relation triples from text is a fundamental task of Information Extraction. There have been many efforts that decompose the problem into separate tasks, i.e. named entity recognition (NER) and relation triple extraction (RE), and solve them respectively. Among these efforts, Set Prediction Networks (SPNs) have demonstrated state-of-the-art performance on NER (Tan et al., 2021; Shen et al., 2022) and RE (Sui et al., 2020; Tan et al., 2022).\nTypically, SPNs leverage a set of learnable queries to model the interaction among instances (entities or relation triples) via attention mechanism and generate the set of instances naturally. The success of SPNs on NER and RE inspires us to explore\n*Corresponding Author.\nthe possibility of jointly solving the extraction of entities and relation triples with SPNs, which is a promising but unexplored direction.\nIn this paper, we propose Bipartite Set Prediction Network (BiSPN), a variant of SPNs, to generate target entity set and relation set in one pass. It can not only avoid the negative effect of cascade error but also enjoy the benefit of high inference speed. However, it is challenged by the difficulty to maintain the coherence between the generated entity set and relation set, due to its parallel design.\nAs illustrated in Figure 1, the head/tail entities of generated relation triples should be included in the generated entity set. The significance of this coherence is two-fold: 1) by requiring the generated entity set to contain the head/tail entities, the recall of the generated entities is more guaranteed; 2) by restricting the head/tail entities within the generated entity set, the precision and recall of the generated triples are more guaranteed.\nDespite that, it is difficult to maintain such consistency when generating the two sets in parallel, since all instance queries are assumed to be of equal status and their hidden representations are updated\nusing bidirectional attention without any further restriction.\nTo overcome this challenge, we come up with two novel solutions. The first one is a Bipartite Consistency Loss function. It works by looking for a reference entity from the generated entity set for each relational subject/object and forcing the subject/object to simulate the reference entity. Symmetrically, it also find a reference subject/object for each entity classified as involved in relation and force the entity to simulate the reference subject/object. Our second solution is an Entity-Relation Linking Loss function, which works in the hidden semantic space. By computing the linking scores between the projected representations of entity queries and relation queries, it encourages the model to learn the interaction between entity instances and relation triple instances.\nTo sum up, our main contributions include:\n\u2022 We present BiSPN, the first SPN-based joint entity and relation extraction model, which is able to generate target entity set and relation set in one pass.\n\u2022 To maintain the coherence between the generated entity set and relation set, two novel loss functions, i.e., Bipartite Consistency Loss and Entity-Relation Linking Loss are introduced.\n\u2022 BiSPN outperforms SOTA methods on three biomedical/clinical datasets, which is knowledge-intensive, and achieves competitive results on a general-domain benchmark. Besides, it infers much faster than two-stage joint extraction methods."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Joint Entity and Relation Extraction",
            "text": "The target of joint entity and relation extraction is to recognize all entities mentioned in a given text and identify all entity pairs involved in relation. Existing methods for joint entity and relation extraction fall into four categories: (1) span-based methods (Dixit and Al-Onaizan, 2019; Zhong and Chen, 2021; Ye et al., 2022) that enumerate spans and conduct span-level classification to extract entities, enumerate and classify span pairs to extract relation triples; (2) table filling-based methods (Wang et al., 2020, 2021; Yan et al., 2021) that fill out a table for each entity/relation type via token pair classification; (3) machine reading comprehension\n(MRC)-based methods (Li et al., 2019) that casts the task as a multi-turn question answering problem via manually designed templates; (4) autoregressive generation-based methods (Zeng et al., 2018; Lu et al., 2022) that reformulate the task as a sequence-to-sequence problem by linearizing target entities/relations into a pointer sequence or augmented natural language.\nAmong them, only the methods based on table filling can extract entities and relation triples in one stage, all other methods perform multi-step prediction, suffering from cascade errors and low inference speed. In this paper, we provide a new choice for one-stage joint entity and relation extraction, which is based on set prediction networks."
        },
        {
            "heading": "2.2 Set Prediction Networks",
            "text": "Set prediction networks are originally proposed for the task of object detection in computer vision (Carion et al., 2020). And they are successfully extended for information extraction by (Sui et al., 2020; Tan et al., 2021; Shen et al., 2022; Tan et al., 2022).\nGenerally, these methods employ a set of learnable queries as additional input, model the interaction among instances (entities/relations) via selfattention among the queries and one-way attention between the queries and the textual context. However, all these methods can only perform named entity recognition (Tan et al., 2021; Shen et al., 2022) or relation triple extraction (Sui et al., 2020; Tan et al., 2022), rather than jointly solve both of them."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Problem Formulation",
            "text": "Given an input sentence x = x1, x2, ..., xL, the aim of joint entity and relation extraction is to predict the set of entities {ei}Nei=1 and the set of relation triples {rj}Nrj=1 mentioned in the sentence. Here, L is the length of the input sentence, Ne and Nr are the numbers of target entities and target relation triples respectively. The i-th entity ei is denoted as (starti, endi, t e i ), where starti, endi are the start token index and end token index of the entity, tei \u2208 Te is the entity type label. The j-th relation triple rj is denoted as (ehj , t r j , e t j), where e h j and e t j are the head entity (starthj , end h j , t e,h j ) and tail entity (starttj , end t j , t e,t j ) of the relation triple, t r j \u2208 Tr is the relation type label. We additionally define a null label \u2205 for the set of entity types Te and the\nset of relation types Tr respectively, to indicate that no entity or no relation is recognized."
        },
        {
            "heading": "3.2 BiSPN",
            "text": "As illustrated in Figure 2, the proposed model mainly consists of a shared encoder, a shared decoder, an entity decoding module (in blue) and a relation decoding module (in red)."
        },
        {
            "heading": "3.2.1 Shared Encoder",
            "text": "The encoder of BiSPN is essentially a bidirectional pretrained language model (Devlin et al., 2019) with modified input style and attention design.\nWe first transform the input sentence x into input embeddings X \u2208 RL\u00d7d, and then concatenate X with a series of learnable entity queries Qe and relation queries Qr to form the model input X\u0303:\nX\u0303 = [X;Qe;Qr] \u2208 R(L+Me+Mr)\u00d7d (1)\nwhere d is the model dimension, Me and Mr are hyperparameters controlling the number of entity queries and the number of relation queries (Me \u226b Ne, Mr \u226b Nr).\nTo prevent the randomly initialized queries from negatively affecting the contextual token encodings,\nwe follow the work (Shen et al., 2022) to modify the bidirectional self-attention into one-way selfattention. Concretely, the upper right L\u00d7 (Me + Mr) sub-matrix of the attention mask is filled with negative infinity value so that the entity/relation queries become invisible to the token encodings, while the entity/relation queries can still attend to each other and the token encodings.\nAfter multiple one-way self-attention layers and feed-forward layers, the encoder outputs the contextual token encodings as well as the contextual entity/relation queries."
        },
        {
            "heading": "3.2.2 Shared Decoder",
            "text": "The shared decoder consists of N decoding blocks. Each decoding block includes an one-way selfattention layer (as described above), a bidirectional self-attention layer and feed-forward layers. The one-way self-attention layer here functions as the cross-attention layer of Transformer decoder (Vaswani et al., 2017), which aggregates textual context for decoding. (The main difference between one-way self-attention and cross-attention is that the contextual token encodings also get updated by one-way self-attention.) The bidirectional self-attention layer updates the entity/relation\nqueries via modeling the interaction among them. After shared decoding, the decoder outputs the updated token representations Hx, entity queries He and relation queries Hr."
        },
        {
            "heading": "3.2.3 Entity Decoding Module",
            "text": "The entity decoding module consists of an entityview projection layer, an entity decoder and an entity predictor.\nThe entity-view projection layer first linearly transforms the token encodings Hx into entityview:\nHxe = Linear(H x) (2)\nThe entity decoder, which includes multiple layers of cross-attention and bidirectional selfattention, receives the transformed token encodings Hxe as decoding context and the entity queries He as decoder input, and output the final representation of entity queries H\u0303e:\nH\u0303e = EntityDecoder(He|Hxe ) (3)\nThe entity predictor is responsible for predicting the boundary and entity type of each entity query.\nFor each entity query, it first fuse the query representation with the transformed token encodings, and then calculate the probability of each token in the sentence being the start/end token of the corresponding entity:\nS\u03b4i = Linear ( Relu(Linear(H\u0303ei ) + Linear(H x e )) ) (4)\nP \u03b4i = Softmax(S \u03b4 i ), \u03b4 \u2208 {start, end} (5)\nwhere S\u03b4i \u2208 RL is a vector of logits of each token being the start/end token of the entity associated with i-th entity query, P \u03b4i is the corresponding probability distribution.\nAn MLP-based classifier is leveraged to predict the type of the entity associated with the i-th entity query:\nP t e\ni = Softmax(MLP(H\u0303ei )) (6)\nDuring inference, the predicted boundary and entity type corresponding to the k-th entity query are calculated as:\nscorek(i, j) = P startk [i] + P end k [j] (7)\n( \u02c6startk, \u02c6endk) = argmax (i,j): 0<j\u2212i<L scorek(i, j) (8)\nt\u0302ek = argmaxP te k (9)\nNote that, the entity predictor will filter out the entity whose predicted type label is \u2205."
        },
        {
            "heading": "3.2.4 Relation Decoding Module",
            "text": "The relation decoding module consists of a relationview projection layer, a relation decoder, a head-tail predictor and a relation type predictor.\nThe relation-view projection layer and relation decoder work in the same manner as the entityview projection layer and entity decoder, except that the relation decoder splits relation queries into head/tail queries before decoding:\n[Hh;Ht] = Linear(Hr) (10)\nHxr = Linear(H x) (11)\nH\u0303h, H\u0303t, H\u0303r = RelationDecoder(Hh, Ht, Hr|Hxr ) (12)\nThe head-tail predictor then predicts the boundary and entity type of the head/tail entity associated with each relation queries. This process is similar to the entity prediction process (Equation 4-10). The only difference is that the entities queries becomes the head/tail queries H\u0303h/t and the token encodings is now in relation-view Hxr .\nThe relation type predictor classifies the category of i-th relation query according to H\u0303ri :\nP t r\ni = Softmax(MLP(H\u0303ri )) (13)"
        },
        {
            "heading": "3.3 Prediction Loss",
            "text": "To train the model, we should find the optimal assignment between the gold entity set and the generated entity set, as well as the optimal assignment between the gold relation set and the generated relation set, which are calculated in the same way as in (Tan et al., 2021; Shen et al., 2022) using the Hungarian algorithm (Kuhn, 1955).\nAfter the optimal assignments are obtained, we calculate the following prediction loss Lpred for each sample:\nLent = \u2212 Me\u2211 i=1 ( logP starti [start\u03c6(i)] + logP end i [end\u03c6(i)]\n+ logP t e i [t e \u03c6(i)] ) (14)\nLh/tent = \u2212 Mr\u2211 j=1 ( logP startj,h/t[start h/t \u03c3(j)] + logP end j,h/t[end h/t \u03c3(j)]\n+ logP t e j,h/t[t e,h/t \u03c3(j)]\n) (15)\nLrel = Lh/tent \u2212 Mr\u2211 j=1 logP t r j [t r \u03c3(j)] (16) Lpred = Lent + Lrel (17)\nwhere \u03c6(i) is the index of the gold entity assigned to the i-th generated entity, \u03c3(j) is the index of the\ngold relation triple assigned to the j-th generated relation triple, Lh/tent represents the loss of head/tail entity prediction."
        },
        {
            "heading": "3.4 Bipartite Consistency Loss",
            "text": "To calculate the bipartite consistency loss, we first find a reference entity from the generated entity set for each head/tail entity. A reference entity is defined as the entity most similar to the referring head/tail entity. Concretely, the similarity between ea, the a-th generated entity, and eh/tb , the head/tail entity of the b-th generated relation triple is measured in KL divergence between the start/end/type probability distributions of ea and eh/tb :\nsim(ea, eh/tb ) = \u2211\n\u03b4\u2208{start,end,te}\n\u2212DKL(P \u03b4a ||P \u03b4,h/t b )\nwhere DKL(P ||Q) is the KL divergence between target distribution P and approximate distribution Q; P \u03b4,h/tb means P \u03b4,h b when e h/t b is a head entity, otherwise P \u03b4,h/tb means P \u03b4,t b .\nWe want every head/tail entity to simulate its reference entity, which is equivalent to maximizing the similarity. Hence, the consistency loss in the relation \u2192 entity direction is computed as:\nLrel\u2192ent = \u2212 Mr\u2211 i=1 [ max j\u2208(1,Me) sim(ej , ehi )\n+ max j\u2208(1,Me)\nsim(ej , eti) ] (18)\nSymmetrically, we also find a reference head/tail entity for each generated entity that is classified as having relation. The classification is conducted by a binary classifier, which is trained with a binary cross-entropy loss function:\nphas-reli = sigmoid(MLP(H\u0303ei )) (19)\nLhas-rel = \u2212 1\nMe Me\u2211 i=1 [ yhas-reli logp has-rel i +\n(1\u2212 yhas-reli ) log(1\u2212 phas-reli ) ] (20)\nwhere yhas-reli = 1 only if the gold entity assigned to the i-th entity query is involved in relation.\nThe consistency loss in the entity \u2192 relation direction is then calculated as follows:\n\u02dcsim(eh/tb , ea) = \u2211\n\u03b4\u2208{start,end,te}\n\u2212DKL(P \u03b4,h/tb ||P \u03b4 a )\nLent\u2192rel = \u2212 \u2211 i\u2208\u2126 max j\u2208(1,Mr) \u02dcsim(eh/tj , ei) (21)\n\u2126 = {i | phas-reli \u2265 0.5, i \u2208 (1,Me)} (22)\nwhere \u2126 is the set of indices of the entities classified as involved in relation.\nWe sum up Lent\u2192rel, Lrel\u2192ent and Lhas-rel to get the overall bipartite consistency loss Lent\u2194rel."
        },
        {
            "heading": "3.5 Entity-Relation Linking Loss",
            "text": "While the bipartite consistency loss softly aligns the predicted distributions between the generated entity set and relation set during training, the entityrelation linking loss encourages BiSPN to model the interaction between entity queries and relation queries.\nTo this end, we first project the intermediate representations of entity queries and relation queries and then compute the linking scores between them via a Biaffine layer:\nH\u0304e = Linear(He) (23)\nH\u0304r = Linear(Hr) (24)\nSlink = Biaffine(H\u0304e, H\u0304r) \u2208 RMe\u00d7Mr (25)\nWith the linking scores, we calculate the following binary cross-entropy loss:\nP link = sigmoid(Slink) (26)\nLlink = \u2212 1\nMeMr Me\u2211 i=1 Mr\u2211 j=1 [ ylinki,j logP link i,j\n+ (1\u2212 ylinki,j ) log(1\u2212 P linki,j ) ] (27)\nwhere ylinki,j = 1 only if the gold entity assigned to the i-th entity query appears in the gold relation triple assigned to the j-th relation query."
        },
        {
            "heading": "4.1 Experiment Setups",
            "text": ""
        },
        {
            "heading": "4 Experiments",
            "text": "Datasets. We experiment on one general-domain dataset (ACE05) and three knowledge-intensive datasets (BioRelEx, ADE, Text2DT). ACE05 (Walker et al., 2006) includes a corpus of newswire, broadcast news, telephone conversations. BioRelEx (Khachatrian et al., 2019) contains biomedical literature about binding interactions between proteins and/or biomolecules. ADE (Gurulingappa et al., 2012) consists of medical reports describing drug-related adverse effects. Text2DT1 is originally an benchmark for medical Decision Trees extraction task in China Health Information Processing Conference 2022. And we only use its entity/relation annotation for experiments. See Table 1 for detailed statistics of the datasets.\nWe additionally experiment on the SciERC dataset, where we follow the same setting as in (Wang et al., 2021; Ye et al., 2022). See Appendix B for the results.\nEvaluation Metrics. Strict evaluation metrics are applied, where an entity is confirmed correct only if its boundary and entity type are correctly predicted; a relation triple is confirmed correct only if its relation type and head/tail entity are correctly predicted. For ACE05 and BioRelEx, we report the averaged Micro F1 scores over 3 random seeds. For ADE, we follows (Ji et al., 2020; Lai et al., 2021) to conduct 10-fold cross-validation and report the averaged Macro F1 scores. For Text2DT, we follows the top-1 system on the evaluation task to ensemble 5 models trained with different random seeds and report the Micro F1 scores.\n1http://www.cips-chip.org.cn/2022/eval3"
        },
        {
            "heading": "4.2 Implementation Details",
            "text": "We implement BiSPN with Pytorch (Paszke et al., 2019) and run experiments on NVIDIA Tesla V100 GPUs. For ACE05, we follow (Wang et al., 2021; Ye et al., 2022) to initialize the shared encoder with BERT-base (Devlin et al., 2019). For BioRelEx and ADE, we follow (Haq et al., 2023; Lai et al., 2021) to initialize the shared encoder with BioBERT-base. For Text2DT, we initialize the shared encoder with Chinese-bert-wwm-ext (Cui et al., 2021). The decoding modules are randomly initialized. Following (Shen et al., 2022), we freeze the encoder in the first 5 epochs and unfreeze it in the remaining epochs. The learning rate of decoding modules is set to be larger than the learning rate of the encoder. We adopt an AdamW optimizer (Loshchilov and Hutter, 2017) equipped with a linear warm-up scheduler to tune the model. See Appendix A for details of hyperparameter tuning."
        },
        {
            "heading": "4.3 Compared Baselines",
            "text": "We compare BiSPN with several SOTA methods listed as follows.\nKECI (Lai et al., 2021): A knowledge-enhanced two-stage extraction model based on span graphs.\nMADR (Haq et al., 2023): A pipeline of independent NER and RE models.\nPL-Marker (Ye et al., 2022): A span-based method that models the interrelation between spans by packing levitated markers in the encoder.\nPFN (Yan et al., 2021): A partition filter network that models two-way interaction between NER and RE subtasks.\nUniRE (Wang et al., 2021): A method based on table filling, featured with a unified label space for one-stage joint entity and relation extraction.\nTOP1: The top-1 system on the Text2DT evaluation task, which combines PFN (Yan et al., 2021) with Efficient GlobalPointer (Su, 2022).\nNote that, we do not compare with SPN (Sui et al., 2020), UniRel (Tang et al., 2022) and QIDN (Tan et al., 2022), since these methods can only extract relation triples and cannot recognize those entities uninvolved in relation."
        },
        {
            "heading": "4.4 Main Results",
            "text": "Table 2 summarizes the overall performance of BiSPN and compared baselines on ACE05, BioRelEx, ADE and Text2DT. In terms of entity extraction, BiSPN performs competitively with or slightly better than SOTA methods on ACE05, BioRelEx and ADE, while outperforming the SOTA method PL-Marker by 0.8 F1 on Text2DT.\nIn terms of relation extraction, BiSPN boosts SOTA performance by 0.7, 0.5 and 0.5 F1 on BioRelEx, ADE and Text2DT respectively, verifying the effectiveness of our method on knowledgeintensive scene. However, although BiSPN outperforms SOTA one-stage methods by 0.6 F1 on ACE05, it is no match for the two-stage method PL-Marker on this general-domain dataset. We will look into the reason behind this in Section 4.6.2."
        },
        {
            "heading": "4.5 Inference Efficiency",
            "text": "We compare the inference efficiency of KEIC, UniRE, PL-Marker, BiSPN on the BioRelEx and Text2DT datasets. For a fair comparison, the experiments are all conducted on a server with Intel(R) Xeon(R) E5-2698 CPUs and NVIDIA Tesla V100 GPUs. And we fix the batch size as 8 during evaluation. As shown in Table 3, BiSPN can process around 40\u223c60 sentences per second, which is 2 times faster than the SOTA two-stage method PL-Marker. Although UniRE, a SOTA one-stage method, is about 2.5 times faster than BiSPN, its performance of relation extraction is uncompetitive against BiSPN."
        },
        {
            "heading": "4.6 Analysis",
            "text": "4.6.1 Effects of Lent\u2194rel and Llink We conduct ablation study and qualitative visualization to analyze how the bipartite consistency loss Lent\u2194rel and entity-relation linking loss Llink work.\nThe results of ablation study are shown in Table 2. Without the bipartite consistency loss, the entity F1 scores drop by 0.5, 1.4, 0.7, 0.3 and the relation F1 scores drop by 1.5, 0.8, 0.2, 0.5 on ACE05, BioRelEx, ADE and Text2DT respectively. Without the entity-relation linking loss, the entity F1 scores decrease by 0.2, 0.1, 0.3, 0 and the relation F1 scores decrease by 0.4, 0.3, 0.2, 0.3 on the four datasets respectively. After removing the bipartite consistency loss and entity-relation linking loss together, the entity F1 scores decrease by 0.8, 1.8, 1.2, 0.9 and the relation F1 scores decrease by 2.1, 1.0, 0.7, 0.8 on the four datasets respectively.\nThree conclusions can be derived from these results: 1) both the bipartite consistency loss and\nentity-relation linking loss contribute to the overall performance of BiSPN; 2) the bipartite consistency loss is much more effective than the entity-relation linking loss; 3) the effects of the two losses are not orthogonal but still complementary in some degree.\nTo visualize the attention between entity queries and relation queries, we record the attention weight in the last bidirectional self-attention layer of shared decoder for a sample from BioRelEx. Since the original weight is bidirectional, we average the weight over two directions and conduct normalization to obtain the weight for visualization. As shown in Figure 3, without Lent\u2194rel and Llink, the interaction between entity queries and relation queries is chaotic and pointless. After applying Lent\u2194rel or Llink separately, the relation queries associated with gold relation triples tend to focus on the entity queries associated with gold entities. When applying the two loss functions together, the relation queries associated with gold relation triples further concentrate on the entity queries whose target entities are the same as their head/tail entities. This phenomenon coincides with the purpose of the two loss functions."
        },
        {
            "heading": "4.6.2 Influence of Knowledge Density",
            "text": "The performance of BiSPN on ACE05 is not so good as its performance on BioRelEx, ADE and Text2DT. We hypothesize it is the sparsity of relation triples that hinders the learning of BiSPN. As listed in Table 1, the average number of relations per sentence is 0.53 on ACE05. In contrast, the average numbers of relations per sentence are 1.61, 1.60 and 6.39 on the other three datasets.\nTo verify our hypothesis, we filter samples according to the number of relations they contain and experiment on different versions of the filtered dataset. As shown in Table 4, when the samples\nwithout relation are discarded, the performance gap between PL-Marker and BiSPN narrows from 1.0 to 0.4. When further discarding the samples with less than 2 relation triples, BiSPN even performs slightly better than PL-Marker. This reveals that the strength of BiSPN emerges in knowledge-intensive scene, which is reasonable, since BiSPN works by modeling interaction among knowledge instances."
        },
        {
            "heading": "4.7 Case Study",
            "text": "Figure 4 illustrates two test cases from ACE05 and BioRelEx respectively. In the first case, BiSPN without Lent\u2194rel and Llink fails to recognizes the WEA entity \u201cmore\u201d and the PART-WHOLE relation between \u201cit\u201d and \u201cmore\u201d, while BiSPN successfully extracts them by considering the context in entity-view and relation-view concurrently. Likewise, in the second case, BiSPN successfully recognizes the entity \u201cpromoter\u201d after Lent\u2194rel and Llink is applied. However, it still fails to recognize \u201cRNA polymerase II preinitiation complex\u201d, which is complicated and may require domain knowledge for recognition."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we present BiSPN, a novel joint entity relation extraction framework based on bipartite set prediction. It generates entity set and relation set in a distributed manner, so as to avoid error propagation. To maintain the coherency between the generated entity set and relation set, we come up with two novel loss designs, namely bipartite consistency loss and entity-relation linking loss. The first one pulls closer the predicted boundary/type distributions of entities and head/tail entities, while the second one enforces the interaction between entity queries and relation queries. Extensive experiments demonstrate the advantage of BiSPN in\nknowledge-intensive scene, as well as the effectiveness of the proposed bipartite consistency loss and entity-relation linking loss.\nLimitations\nAs mentioned in Section 4.6.2, the performance of our BiSPN framework can be hindered when the distribution of relation triples are overly sparse in a dataset. This suggests that BiSPN is a better choice for biomedical and clinical domains but not the general-domain, where knowledge sparsity is common.\nAnother limitation of BiSPN is its reliance on a fixed number of entity/relation queries. Although it is possible to set the number of queries larger in order to let BiSPN generalize to longer text input, the cost is additional memory and time consumption that grows quadratically. To effectively address this, future work can draw lessons from the field of dynamic neural network and explore dynamic selection of instance queries."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank the reviewers for their valuable suggestions. This study is partially supported by National Key R&D Program of China (2021ZD0113402 ), National Natural Science Foundation of China (62276082), Major Key Project of PCL (PCL2021A06), Shenzhen Soft Science Research Program Project (No.KX20220705152815035) and the Fundamental Research Fund for the Central Universities (HIT.DZJJ.2023117)."
        },
        {
            "heading": "A Hyperparameter Configuration",
            "text": "We tune the hyperparameters for each dataset by manually trying different values of each hyperparameter within a specific interval and choosing the\nvalue that results in the highest relation F1 on the development set (For ADE, the validation set of its first fold of data is employed as the development set). After trial, we find it optimal to set the numbers of shared decoder layers, entity decoder layers and relation decoder layers as 4, 1, 1 for all datasets. The trial intervals and final configuration of other hyperparameters are shown in Table 5."
        },
        {
            "heading": "B Experiment Results on the SciERC dataset",
            "text": "Here, we append the results of additional experiment on the SciERC dataset. As shown in Table 6, in terms of entity recognition, BiSPN underperforms SOTA two-stage method PL-Marker by Ent-F1, but still outperforms SOTA one-stage methods (PFN, UniRE) substantially. In terms of relation triple extraction, BiSPN establishes new SOTA (Rel-F1) on the SciERC dataset. In terms of inference speed, BiSPN is about faster than PLMarker, but slower than other one-stage methods. The results after ablating the consistency loss and the linking loss verify the effectiveness of them on the dataset."
        }
    ],
    "title": "BiSPN: Generating Entity Set and Relation Set Coherently in One Pass",
    "year": 2023
}