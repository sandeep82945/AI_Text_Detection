{
    "abstractText": "Our investigation into the Affective Reasoning in Conversation (ARC) task highlights the challenge of causal discrimination. Almost all existing models, including large language models (LLMs), excel at capturing semantic correlations within utterance embeddings but fall short in determining the specific causal relationships. To overcome this limitation, we propose the incorporation of i.i.d. noise terms into the conversation process, thereby constructing a structural causal model (SCM). It explores how distinct causal relationships of fitted embeddings can be discerned through independent conditions. To facilitate the implementation of deep learning, we introduce the cogn frameworks to handle unstructured conversation data, and employ an autoencoder architecture to regard the unobservable noise as learnable \u201cimplicit causes.\u201d Moreover, we curate a synthetic dataset that includes i.i.d. noise. Through comprehensive experiments, we validate the effectiveness and interpretability of our approach. Our code is available in https://github.com/Zodiark-ch/ mater-of-our-EMNLP2023-paper.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hang Chen"
        },
        {
            "affiliations": [],
            "name": "Xinyu Yang"
        },
        {
            "affiliations": [],
            "name": "Jing Luo"
        },
        {
            "affiliations": [],
            "name": "Wenjing Zhu"
        },
        {
            "affiliations": [],
            "name": "Du Xiao"
        }
    ],
    "id": "SP:7d99bf72371ad887ef145dce7d723a3a09ca6d35",
    "references": [
        {
            "authors": [
                "Raj Agrawal",
                "Chandler Squires",
                "Neha Prasad",
                "Caroline Uhler."
            ],
            "title": "The decamfounder: Non-linear causal discovery in the presence of hidden variables",
            "venue": "arXiv preprint arXiv:2102.07921.",
            "year": 2021
        },
        {
            "authors": [
                "Siddhant Arora",
                "Hayato Futami",
                "Emiru Tsunoo",
                "Brian Yan",
                "Shinji Watanabe."
            ],
            "title": "Joint modelling of spoken language understanding tasks with integrated dialog history",
            "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal",
            "year": 2023
        },
        {
            "authors": [
                "Yinan Bao",
                "Qianwen Ma",
                "Lingwei Wei",
                "Wei Zhou",
                "Songlin Hu."
            ],
            "title": "Multi-granularity semantic aware graph model for reducing position bias in emotion-cause pair extraction",
            "venue": "arXiv preprint arXiv:2205.02132.",
            "year": 2022
        },
        {
            "authors": [
                "Carlos Busso",
                "Murtaza Bulut",
                "Chi-Chun Lee",
                "Abe Kazemzadeh",
                "Emily Mower",
                "Samuel Kim",
                "Jeannette N Chang",
                "Sungbok Lee",
                "Shrikanth S Narayanan."
            ],
            "title": "Iemocap: Interactive emotional dyadic motion capture database",
            "venue": "Language resources",
            "year": 2008
        },
        {
            "authors": [
                "Hang Chen",
                "Xinyu Yang",
                "Chenguang Li"
            ],
            "title": "Learning a general clause-to-clause relationships for enhancing emotion-cause pair extraction",
            "year": 2023
        },
        {
            "authors": [
                "Sheng-Yeh Chen",
                "Chao-Chun Hsu",
                "Chuan-Chun Kuo",
                "Lun-Wei Ku"
            ],
            "title": "Emotionlines: An emotion corpus of multi-party conversations",
            "venue": "arXiv preprint arXiv:1802.08379",
            "year": 2018
        },
        {
            "authors": [
                "Lu Cheng",
                "Ruocheng Guo",
                "Raha Moraffah",
                "Paras Sheth",
                "Kasim Selcuk Candan",
                "Huan Liu."
            ],
            "title": "Evaluation methods and measures for causal learning algorithms",
            "venue": "IEEE Transactions on Artificial Intelligence.",
            "year": 2022
        },
        {
            "authors": [
                "Diego Colombo",
                "Marloes H Maathuis",
                "Markus Kalisch",
                "Thomas S Richardson."
            ],
            "title": "Learning highdimensional directed acyclic graphs with latent and selection variables",
            "venue": "The Annals of Statistics, pages 294\u2013321.",
            "year": 2012
        },
        {
            "authors": [
                "Zixiang Ding",
                "Rui Xia",
                "Jianfei Yu."
            ],
            "title": "ECPE-2D: Emotion-cause pair extraction based on joint twodimensional representation, interaction and prediction",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Shutong Feng",
                "Nurul Lubis",
                "Christian Geishauser",
                "Hsien-chin Lin",
                "Michael Heck",
                "Carel van Niekerk",
                "Milica"
            ],
            "title": "Emowoz: A large-scale corpus and labelling scheme for emotion recognition in task-oriented dialogue systems",
            "venue": "Gas\u030cic",
            "year": 2022
        },
        {
            "authors": [
                "Deepanway Ghosal",
                "Navonil Majumder",
                "Soujanya Poria",
                "Niyati Chhaya",
                "Alexander Gelbukh."
            ],
            "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods",
            "year": 2019
        },
        {
            "authors": [
                "Taichi Ishiwatari",
                "Yuki Yasuda",
                "Taro Miyazaki",
                "Jun Goto."
            ],
            "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural",
            "year": 2021
        },
        {
            "authors": [
                "James M Joyce."
            ],
            "title": "Kullback-leibler divergence",
            "venue": "International encyclopedia of statistical science, pages 720\u2013722. Springer.",
            "year": 2011
        },
        {
            "authors": [
                "Enkelejda Kasneci",
                "Kathrin Se\u00dfler",
                "Stefan K\u00fcchemann",
                "Maria Bannert",
                "Daryna Dementieva",
                "Frank Fischer",
                "Urs Gasser",
                "Georg Groh",
                "Stephan G\u00fcnnemann",
                "Eyke H\u00fcllermeier"
            ],
            "title": "Chatgpt for good? on opportunities and challenges of large language models",
            "year": 2023
        },
        {
            "authors": [
                "D.P. Kingma",
                "M. Welling."
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv.org.",
            "year": 2014
        },
        {
            "authors": [
                "Boris Knyazev",
                "Graham W Taylor",
                "Mohamed Amer."
            ],
            "title": "Understanding attention and generalization in graph neural networks",
            "venue": "Advances in neural information processing systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Meredyth Krych-Appelbaum",
                "Julie Banzon Law",
                "Dayna Jones",
                "Allyson Barnacz",
                "Amanda Johnson",
                "Julian Paul Keenan."
            ],
            "title": "i think i know what you mean\u201d: The role of theory of mind in collaborative communication",
            "venue": "Interaction Studies, 8(2):267\u2013280.",
            "year": 2007
        },
        {
            "authors": [
                "Yanran Li",
                "Hui Su",
                "Xiaoyu Shen",
                "Wenjie Li",
                "Ziqiang Cao",
                "Shuzi Niu."
            ],
            "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Zheng Lian",
                "Bin Liu",
                "Jianhua Tao."
            ],
            "title": "Decn: Dialogical emotion correction network for conversational emotion recognition",
            "venue": "Neurocomputing, 454:483\u2013 495.",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Navonil Majumder",
                "Soujanya Poria",
                "Devamanyu Hazarika",
                "Rada Mihalcea",
                "Alexander Gelbukh",
                "Erik Cambria."
            ],
            "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, vol-",
            "year": 2019
        },
        {
            "authors": [
                "Jinjie Ni"
            ],
            "title": "Attention mechanism optimization for sub-symbolic-based and neural-symbolic-based natural language processing",
            "year": 2023
        },
        {
            "authors": [
                "Ana Rita Nogueira",
                "Andrea Pugnana",
                "Salvatore Ruggieri",
                "Dino Pedreschi",
                "Jo\u00e3o Gama."
            ],
            "title": "Methods and tools for causal discovery and causal inference",
            "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 12(2):e1449.",
            "year": 2022
        },
        {
            "authors": [
                "Juan Miguel Ogarrio",
                "Peter Spirtes",
                "Joe Ramsey."
            ],
            "title": "A hybrid causal search algorithm for latent variable models",
            "venue": "Conference on probabilistic graphical models, pages 368\u2013379. PMLR.",
            "year": 2016
        },
        {
            "authors": [
                "Desmond C Ong",
                "Jamil Zaki",
                "Noah D Goodman."
            ],
            "title": "Affective cognition: Exploring lay theories of emotion",
            "venue": "Cognition, 143:141\u2013162.",
            "year": 2015
        },
        {
            "authors": [
                "Desmond C Ong",
                "Jamil Zaki",
                "Noah D Goodman."
            ],
            "title": "Computational models of emotion inference in theory of mind: A review and roadmap",
            "venue": "Topics in cognitive science, 11(2):338\u2013357.",
            "year": 2019
        },
        {
            "authors": [
                "Patr\u00edcia Pereira",
                "Helena Moniz",
                "Joao Paulo Carvalho."
            ],
            "title": "Deep emotion recognition in textual conversations: A survey",
            "venue": "arXiv preprint arXiv:2211.09172.",
            "year": 2023
        },
        {
            "authors": [
                "Soujanya Poria",
                "Devamanyu Hazarika",
                "Navonil Majumder",
                "Gautam Naik",
                "Erik Cambria",
                "Rada Mihalcea."
            ],
            "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
            "venue": "Proceedings of the 57th Annual Meeting of the As-",
            "year": 2019
        },
        {
            "authors": [
                "Soujanya Poria",
                "Navonil Majumder",
                "Devamanyu Hazarika",
                "Deepanway Ghosal",
                "Rishabh Bhardwaj",
                "Samson Yu Bai Jian",
                "Pengfei Hong",
                "Romila Ghosh",
                "Abhinaba Roy",
                "Niyati Chhaya"
            ],
            "title": "Recognizing emotion cause in conversations",
            "year": 2021
        },
        {
            "authors": [
                "Ruben Sanchez-Romero",
                "Joseph D Ramsey",
                "Kun Zhang",
                "Madelyn RK Glymour",
                "Biwei Huang",
                "Clark Glymour."
            ],
            "title": "Estimating feedforward and feedback effective connections from fmri time series: Assessments of statistical methods",
            "venue": "Network Neuroscience,",
            "year": 2019
        },
        {
            "authors": [
                "Weizhou Shen",
                "Junqing Chen",
                "Xiaojun Quan",
                "Zhixiang Xie."
            ],
            "title": "Dialogxl: All-in-one xlnet for multiparty conversation emotion recognition",
            "venue": "AAAI.",
            "year": 2022
        },
        {
            "authors": [
                "Weizhou Shen",
                "Siyue Wu",
                "Yunyi Yang",
                "Xiaojun Quan."
            ],
            "title": "Directed acyclic graph network for conversational emotion recognition",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Shohei Shimizu",
                "Kenneth Bollen."
            ],
            "title": "Bayesian estimation of causal direction in acyclic structural equation models with individual-specific confounder variables and non-gaussian distributions",
            "venue": "J. Mach. Learn. Res., 15(1):2629\u20132652.",
            "year": 2014
        },
        {
            "authors": [
                "Shohei Shimizu",
                "Patrik O Hoyer",
                "Aapo Hyv\u00e4rinen",
                "Antti Kerminen",
                "Michael Jordan."
            ],
            "title": "A linear nongaussian acyclic model for causal discovery",
            "venue": "Journal of Machine Learning Research, 7(10).",
            "year": 2006
        },
        {
            "authors": [
                "Keisuke Shirai",
                "Hirotaka Kameko",
                "Shinsuke Mori."
            ],
            "title": "Towards flow graph prediction of open-domain procedural texts",
            "venue": "arXiv preprint arXiv:2305.19497.",
            "year": 2023
        },
        {
            "authors": [
                "Francesc Sidera",
                "Georgina Perpi\u00f1\u00e0",
                "J\u00e8ssica Serrano",
                "Carles Rostan"
            ],
            "title": "Why is theory of mind important for referential communication",
            "venue": "Current Psychology,",
            "year": 2018
        },
        {
            "authors": [
                "Pater Spirtes",
                "Clark Glymour",
                "Richard Scheines",
                "Stuart Kauffman",
                "Valerio Aimale",
                "Frank Wimberly"
            ],
            "title": "Constructing bayesian network models of gene expression networks from microarray data",
            "year": 2000
        },
        {
            "authors": [
                "Chandler Squires",
                "Annie Yun",
                "Eshaan Nichani",
                "Raj Agrawal",
                "Caroline Uhler."
            ],
            "title": "Causal structure discovery between clusters of nodes induced by latent factors",
            "venue": "Conference on Causal Learning and Reasoning, pages 669\u2013687. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Palash Thakur",
                "Ronit Shahu",
                "Vikas Gupta."
            ],
            "title": "Audio and text-based emotion recognition system using deep learning",
            "venue": "Advances in Signal Processing, Embedded Systems and IoT: Proceedings of Seventh ICMEET-2022, pages 447\u2013459. Springer.",
            "year": 2023
        },
        {
            "authors": [
                "Veronika Thost",
                "Jie Chen."
            ],
            "title": "Directed acyclic graph neural networks",
            "venue": "arXiv preprint arXiv:2101.07965.",
            "year": 2021
        },
        {
            "authors": [
                "Hande Aka Uymaz",
                "Senem Kumova Metin."
            ],
            "title": "Vector based sentiment and emotion analysis from text: A survey",
            "venue": "Engineering Applications of Artificial Intelligence, 113:104922.",
            "year": 2022
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Lio",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "arXiv preprint arXiv:1710.10903.",
            "year": 2017
        },
        {
            "authors": [
                "Penghui Wei",
                "Jiahao Zhao",
                "Wenji Mao."
            ],
            "title": "Effective inter-clause modeling for end-to-end emotioncause pair extraction",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3171\u20133181, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Rui Xia",
                "Zixiang Ding."
            ],
            "title": "Emotion-cause pair extraction: A new task to emotion analysis in texts",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1003\u2013 1012, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Mao Ye",
                "Haitao Wang",
                "Zheqian Chen."
            ],
            "title": "Msmix: An interpolation-based text data augmentation method manifold swap mixup",
            "venue": "arXiv preprint arXiv:2305.19617.",
            "year": 2023
        },
        {
            "authors": [
                "Sayyed Zahiri",
                "Jinho D. Choi."
            ],
            "title": "Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks",
            "venue": "Proceedings of the AAAI Workshop on Affective Content Analysis, AFFCON\u201918, pages 44\u201351, New Orleans, LA.",
            "year": 2018
        },
        {
            "authors": [
                "Dong Zhang",
                "Liangqing Wu",
                "Changlong Sun",
                "Shoushan Li",
                "Qiaoming Zhu",
                "Guodong Zhou."
            ],
            "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
            "venue": "IJCAI, pages 5415\u20135421.",
            "year": 2019
        },
        {
            "authors": [
                "Shen"
            ],
            "title": "Statistics of 6 cogn skeletons. We detailed the hypotheses each cogn skeleton adopted and the original works",
            "year": 2021
        },
        {
            "authors": [
                "D \u03bei"
            ],
            "title": "Implementation Details In the word embedding, we adopt the affect-based pre-trained features1 proposed by Shen et al. (2021) for all baselines and models",
            "year": 2021
        },
        {
            "authors": [
                "Shen"
            ],
            "title": "2021) in the ERC task. Moreover, in the ECPE and ECSR, the learning rate is set to 3e-5, batch size is set to 32, and epoch is set to 60",
            "year": 2021
        },
        {
            "authors": [
                "Shen"
            ],
            "title": "2021) towards ERC, Xia and Ding (2019) towards ECPE, and Poria et al. (2021) towards ECSR. Specifically, we adopt the macro F1 score in ECPE and ECSR tasks, micro F1 score for DailyDialog",
            "venue": "For evaluation metrics,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Nowadays, numerous conversation recognition tasks (such as Emotion Recognition in Conversation (ERC) task (Pereira et al., 2023; Thakur et al., 2023), Intent Recognition (IR) task (Ye et al., 2023; Ni, 2023) and Dialogue Act Recognition (DAR) task (Arora et al., 2023)) have shown promising performance in specialized supervised and unsupervised methods. Considering the RoBERTa pretrained model (Liu et al., 2019) as the examples, \u201cMy eyelids are fighting\u201d and \u201cI want to sleep,\u201d which have similar semantics but different tokens can be well fitted within embeddings. (i.e., these two embeddings exhibit a strong resemblance via certain metrics such as cosine similarity.)\nHowever, when it comes to the relationship between two utterances, denoted as A and B, wherein\ntheir embeddings can be fitted, various possible relationships exist: A acts as the cause of B (A \u2192 B), A acts as the outcome of B (A \u2190 B), or more complex, A and B are both influenced by a common cause (A\u2190 C \u2192 B), and so on. Particularly in reasoning tasks (Uymaz and Metin, 2022; Feng et al., 2022), it is crucial for these methods to transcend the mere fitting of embeddings and possess the capacity to discriminate diverse causal relationships. (i.e., the ability of causal discrimination) (Bao et al., 2022; Shirai et al., 2023).\nTo specifically investigate the causal discrimination capability of existing methods in conversation, we narrow down our research to a particular task: Affective Reasoning in Conversation (ARC), which has included Emotion-Cause Pair Extraction (ECPE) Xia and Ding (2019) and Emotion-Cause Span Recognition (ECSR) Poria et al. (2021).\nWe begin with conducting tests to evaluate the causal discrimination of existing methods including the large language models (LLMs) (Kasneci et al., 2023). One typical evaluation involves the causal reversal test: for emotion-cause utterance pairs with true labels (A, B) representing a causal relationship of B \u2192 A, we scrutinize the predictions generated by the existing methods using both positive pairs (A, B) and negative pairs (B, A). The results reveal that all the examined methods performed similarly across the two sample types. As we are concerned, they lacked causal discriminability. (Details are shown in Section 2.3)\nIn order to discriminate different causal relationships between two similar embeddings, we construct the dialogue process as a Structural Causal Model (SCM). Many endeavors (Cheng et al., 2022; Nogueira et al., 2022) supporting that i.i.d. noise of SCM could facilitate the discrimination of causal relationships when fitting two variables. Under the presence of noise, each utterance is not only explicitly influenced by the other utterances but also implicitly influenced by the i.i.d. exoge-\nnous noise. Consequently, this framework ensures that two fitted embeddings result in diverse causal relationships, which are determined by corresponding independent conditions between the residual terms and embeddings. For simplicity, we refer to other utterances as explicit causes and exogenous noise as implicit causes.\nFurthermore, to enable the learnability of such causal discrimination within embeddings, we propose a common skeleton, named centering one graph node (cogn) skeleton for each utterance derived from some broadly accepted prior hypotheses. It can address the challenges arising from variable-length and unstructured dialogue samples. Subsequently, we develop an autoencoder architecture to learn the unobservable implicit causes. Specifically, we consider the implicit causes as latent variables and utilize a graph attention network (GAT) (Velic\u030ckovic\u0301 et al., 2017) to encode its representation. Additionally, the decoder leverages the inverse matrix of the causal strength, ensuring an accurate retrieval of the causal relationships.\nFinally, we conduct extensive experimental evaluations: 1) our approach significantly outperforms existing methods including prominent LLMs (GPT3.5 and GPT-4) in two affective reasoning tasks (ECPE and ECSR) and one emotion recognition task (ERC), demonstrating its effectiveness in affective reasoning. 2) our method exhibits a significant reduction in false predictions for negative samples across three causal discrimination scenarios. 3) we curate a synthetic dataset with implicit causes to visualize the latent variable in our implementation.\nOur contribution is four-fold:\n\u2022 We formulated the dialogue process as an SCM and analyzed the causal relationships represented by different independent conditions.\n\u2022 We devised the cogn skeleton to address the problems of variable-length and unstructured dialogue samples.\n\u2022 We adopted an autoencoder architecture to overcome the unobservability of implicit causes and make it learnable.\n\u2022 We constructed a synthetic dataset with implicit causes and conducted extensive evaluations of our proposed method."
        },
        {
            "heading": "2 Related Works and Challenges",
            "text": ""
        },
        {
            "heading": "2.1 Task Definition",
            "text": "For notational consistency, we use the following terminology. The target utterance Ut is the tth utterances of a conversation D = (U1, U2, U3, . . . , UN ) where N is the maximum number of utterances in this conversation and 0 < t \u2a7d N . The emotion label Emot denotes the emotion type of Ut. The emotion-cause pair (ECP) is a pair (Ut, Ui), where Ui is the ith utterance of this conversation. In the ECP, Ut represents the emotion utterance and Ui is the corresponding cause utterance. Moreover, the cause label Ct,i denotes the cause span type of the ECP (Ut, Ui).\nThus, in a given text, ERC is the task of identifying all Emot. Moreover, ECPE aims to extract a set of ECPs and ECSR aims to identify all Ct,i."
        },
        {
            "heading": "2.2 Affective Reasoning in Conversation",
            "text": "Chen et al. (2018) introduced the pioneering work on ERC due to the growing availability of public conversational data. Building upon this, Xia and Ding (2019) further advanced the field by proposing the ECPE that jointly identifies both emotions and their corresponding causes. Moreover, Poria et al. (2021) has extended ECPE into conversations and proposed a novel ECSR task, specifically designed to identify ECP spans within conversation contexts. More recently, increasing works have indicated the crucial role played by accurate inference models in facilitating complex reasoning within these tasks, such as the assumption about interlocutors (Zhang et al., 2019; Lian et al., 2021; Shen et al., 2021) and context (Ghosal et al., 2019; Shen et al., 2022; Chen et al., 2023)."
        },
        {
            "heading": "2.3 Challenge of Affective Reasoning",
            "text": "We examined the performance of a range of methods for addressing affective reasoning in conversations, including both unsupervised approaches (large language models (LLMs), BERT-based pretrained models) and supervised approaches (taskrelated approaches).\nOverall, all the methods demonstrated a lack of discriminability on two types of challenges:\n\u2022 Samples where emotional utterances and causal utterances are interchanged. For a dialogue instance, if the ECP is (U1, U2) (U2 is the cause of U1), the prediction results obtained by the existing methods tend to include both (U1, U2) and (U2, U1).\n\u2022 Samples with indirect connections. For example, if the ECPs in a conversation are (U1, U2) and (U2, U3), the prediction results obtained by the methods often include an additional pair (U1, U3).\nWe evaluated the performance of existing methods on these two challenges, and the detailed results are shown in Table 1. All evaluated methods extracted a nearly equal number of negative samples as positive samples. Considering their performance in broad research domains, both unsupervised and supervised methods could demonstrate a desirable fitting ability to capture the semantic similarity between two utterances. This often apparently results in satisfactory performance in most tasks. However, when it comes to more specific causal relationships within semantically similar sentences (such as reasoning tasks), they may not exhibit the same level of \u201cintelligence\u201d and output some \u201cpseudo-correlation\u201d.\nIn the area of causal discovery, Causal Markov and Faithfulness Assumptions (Spirtes et al., 2000; Colombo et al., 2012; Ogarrio et al., 2016), provide insights into capturing more specific causal relationships in the situation of the above challenges. Considering two similar variables: A and B that can be fitted, the independence tests enable the determination of specific causal relationships, such as \u201cA \u2192 B,\u201d \u201cB \u2192 A,\u201d or \u201cA \u2192 L \u2192 B\u201d. More recently, the Structural Causal Model (SCM) (Shimizu et al., 2006; Shimizu and Bollen, 2014; Sanchez-Romero et al., 2019) built upon the independent noise assumptions has emerged as an effective approach to the limitation of Markov equivalence classes in distinguishing causal relationships.\nThe noise terms (also called exogenous variables) for each variable, enables methods such as Independent Component Analysis (ICA) to identify more comprehensive causal relationships between the two fitted variables."
        },
        {
            "heading": "3 Methodology",
            "text": "In this section, we begin by outlining incorporating i.i.d. noise terms into a dialogue model to construct an SCM in Section 3.1, demonstrating independent residual allowing for the identification of more specific causal relations within pairs of fitted utterances. Next, to mitigate conflicts between SCM models and dialogue data, we designed cogn skeletons with six instantiations in Section 3.2. Finally, we propose a deep learning implementation to tackle the issue of noise being unknown in dialogue data in Section 3.3."
        },
        {
            "heading": "3.1 Structural Causal Model",
            "text": "In order to imbue causal discriminability into the fitting process of two relevant utterances, we algebraically construct the conversation model as a Structural Causal Model (SCM).\nDefinition 1: An SCM of a dialogue is a 3 tuple \u27e8U,E,F\u27e9, where U is the set of utterances U = {Ui}Ni=1, E is the set of exogenous noises E = {Ei}Ni=1 corresponding to each Ui, N is\nthe number of utterances. Note that each Ei is independent in the SCM. Structural equations F = {fi}Ni=1 are functions that determine U with Ui = fi(relUi) + Ei, where relUt denotes a set of utterances that point to the Ut.\nDefinition 1 establishes the construction of a novel computational model for dialogue process, as exemplified in Figure 1. In such a computational model, each utterance is endogenous and influenced by an independent exogenous variable. For simplicity, we refer to the variable U as the explicit causes and the variable E as the implicit causes. The independence of the implicit causes makes the residual terms meaningful during the fitting of any two utterances.\nDefinition 2: The relationship of two utterances X and Y in a dialogue is causal discriminable, from the independent conditions:\n\u2022 \u03a3X \u22a5\u22a5 Y,\u03a3Y \u22a5\u0338\u22a5 X \u21d2 Y \u2192 X\n\u2022 \u03a3X \u22a5\u0338\u22a5 Y,\u03a3Y \u22a5\u22a5 X \u21d2 X \u2192 Y\n\u2022 \u03a3X \u22a5\u0338\u22a5 Y,\u03a3Y \u22a5\u0338\u22a5 X \u21d2 L\u2192 X,L\u2192 Y\n\u2022 \u03a3X \u22a5\u22a5 Y,\u03a3Y \u22a5\u22a5 X \u21d2 X \u2192 L, Y \u2192 L\nwhere \u03a3 represents the residual terms in fitting process. (The proof is shown in Appendix A.)\nExample 1: A 4-utterance dialogue SCM D = {{U1, U2, U3, U4}, {E1, E2, E3, E4}, {a, b, c}} with the true relationships are U1 = E1, U2 = aU1 +E2, U3 = bU1 +E3, U4 = cU3 +E4. The fitting of U2 with U3 and U4 yield U2 = a bU3 + 0U4 + \u03a3U2 , while the fitting of U3 with U2 and U4 yield U3 = baU2 + 1 cU4 + \u03a3U3 . Additionally, the fitting of U4 with U2 and U3 lead to U4 = 0U2 + cU3 +\u03a3U4 .\nIn Example 1, it is observed that any two utterances can be fitted together as they are mutually dependent. However, causal discriminability can be employed to differentiate their distinct causal structures. For instance, the residual term \u03a3U3 is not independent of U4, while \u03a3U4 is independent of U3, indicating that U3 is a cause of U4. Furthermore, the residual term \u03a3U3 is not independent of \u03a3U2 , and \u03a3U2 is not independent of U3, implying the presence of common cause (U1) between U2 and U3."
        },
        {
            "heading": "3.2 Causal Skeleton Estimation",
            "text": "Establishing a skeleton is the first step in causal discovery, as different skeletons provide distinct\nlearning strategies for recovering the relationships between variables. However, utterances differ from the variables that causal discovery often uses. Specifically, each conversation has a different amount (N ) of utterances, and different inter-utterances relationships related to the context. Hence, it is intractable to build a general causal skeleton with fixed nodes and edges to describe all conversation samples.\nFortunately, several published GNN-based approaches (Shen et al., 2021; Ishiwatari et al., 2021; Ghosal et al., 2019; Lian et al., 2021; Zhang et al., 2019) in ERC have proposed and verified a common hypothesis to settle down this issue. The Hypotheses are elaborated on in Appendix B.\nFigure 2 showcases the design of six cogn skeletons, derived from the latest works that have employed one or more of these hypotheses. The statistic and specific algorithms are also shown in Appendix B. Note that we only conduct experiments for II-VI because our structure is hard to apply with the recurrent-based skeleton."
        },
        {
            "heading": "3.3 Approach Implementation",
            "text": "From a given causal skeleton, a linear SCM can be equivalently represented as:\nUt = \u2211 i\u2208relt \u03b1i,tUi + Et (1)\nwhere relt denotes a set of utterances that point to the Ut (7-th utterance) in Figure 2, Et represents the exogenous variable towards the variable Ut in\nSCM, as well as the implicit cause towards the utterance Ut in conversation. Furthermore, we denote the word embedding of U by H = h1, h2, . . . , hN , and the relationships between utterances in rows can also be written as: H = ATH + E, where Ai,t \u0338= 0 stands for a directed edge from Ui to Ut in SCM. Thus we can define the Graph G = (V, E) with adjacency matrix Ai,i = 0 for all i.\nHowever, in this equation, only H is known. The unknown variable A is typically the target of inference, while the unknown variable E represents exogenous variables that implicitly influence each utterance, such as the speaker\u2019s memory, experiences, or desires. (Krych-Appelbaum et al., 2007; Sidera et al., 2018) These factors are typically missing in existing conversation resources. Therefore, determining A based on completely unknown E is another problem we aim to address.\nHence, we treat AT as an autoregression matrix of the G, and then E can be yielded by an autoencoder model. The whole process reads:\nE = f((I \u2212AT )H) (2)\nH\u0302 = g((I \u2212AT )\u22121E) (3) where f(\u00b7) and g(\u00b7) represent the encoder and decoder neural networks respectively. Encoder aims to generate an implicit cause E, and Decoder devotes to yielding a causal representation H\u0302 . From Equation 1, causal representation H\u0302t reasons about the fusion relations of heterogeneous explicit causes \u2211 i\u2208relt Hi and implicit cause Et. The details of this process are shown in Figure 3. Encoder. We use the graph attention mechanism to learn the adjacency matrix A and construct a hierarchical GNN to instantiate the f(\u00b7).\n\u2113 = 1, 2, . . . , L\u2212 1 represents the layer of GNN. Thus, for each utterance at the \u2113-th layer, the A\u2113i,t computed by attention mechanism is a weighted combination of h\u2113t for each directly related utterance Ui(i \u2208 relt):\nA\u2113i,t = LeakyReLU(e\u2113i,t)\u2211\nj\u2208relt LeakyReLU(e \u2113 j,t)\n(4)\ne\u2113i,t = \u2212\u2192 h iW \u2113 i(row) + ( \u2212\u2192 h tW \u2113 t(col)) T (5)\nwhere W \u2113row \u2208 RN\u00d71 and W \u2113col \u2208 RN\u00d71 are the learnable parameters in the graph attention. Moreover, the GNN aggregates the information from the neighbor utterances as following:\nH\u2113+1 = eLU((I \u2212 (A\u2113)T )H\u2113W \u2113) (6)\nwhere W \u2113 stands for parameters in the corresponding layer. From the final layer of the evaluation process, by extracting AL\u22121 computed in Equation 4, the marginal or conditional \u201cdistribution\u201d of H is obtained, showing how to discover Causal Graph G from D. Besides, by extracting HL in Equation 6, we can obtain the independent embedding for the implicit causes E = MLP (HL).\nDecoder. We utilize the A and E computed from Encoder to generate the causal representation H\u0302 . With a fixed adjacency matrix A, the GNN aggregates the information of implicit causes from neighbor nodes as follows:\nE\u0302\u2113+1 = eLU((I \u2212 (AL)T )\u22121E\u2113M \u2113) (7)\nwhere M \u2113 is parameters in the corresponding layer. As the same architecture as the encoder, H\u0302 =\nMLP (EL). Additionally, the plug-in RNN is integrated with GNN to address the appetite of Hypothesis 6:\nE\u0302\u2113+1 = GRU \u2113(E\u0302\u2113, p\u2113) (8)\nwhere p\u2113 is the state of GRU model, with p computed by self-attention proposed by Thost and Chen (2021)."
        },
        {
            "heading": "3.4 Optimization",
            "text": "In our approach, H\u0302 and H acts identically under the linear SCM model. Similarly, H\u0302 should be aligned with H in emotion dimensions under the non-linear SCM model. In short, we adopt an auxiliary loss measuring the Kullback-Leibler (KL) divergence (Joyce, 2011) of H\u0302 and H mapped into the exact emotion dimensions. Moreover, implicit causes E is one of the crucial influence factors on H\u0302 , so that the loss aims to impose the constraint that H\u0302 is the embedding of our need to ensure generating correct E.\nLossKL = \u2211 t \u2211 e\u2208Emot pe(U\u0302t) log pe(U\u0302t) pe(Ut) (9)\nwhere e is any emotion type in Emot, pe denotes the probability labeled with emotion e. In the whole process of ARC tasks, we followed (Wei et al., 2020; Poria et al., 2021) to add several losses of ECPE and ECSR respectively.\nFurthermore, we would like to explain the difference between our approach and Variational AutoEncoder (VAE) (Kingma and Welling, 2014). The output of the encoder in VAE is q\u03d5(Z). With this estimation of q\u0302\u03d5(Z), we can measure the variation \u03be(q\u03d5(Z)) (also called \u2207\u03d5ELBO(q\u0302\u03d5(Z))) to obtain the approximation estimation of ELBO(q). In contrast, our output is E, a fixed matrix rather than a distribution. In other words, the VAE depends on the prior distribution over the latent variables, whereas our approach has a dependency on the consistency of H and H\u0302 , which is non-sampling and non-distributive."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we conduct extensive experiments to answer the 3 research questions:\nRQ1: How effective is our method in affective reasoning tasks?\nRQ2: How do we justify the causal discriminability of our method?\nRQ3: How do we gauge the difference between the latent variable E and designed implicit causes?"
        },
        {
            "heading": "4.1 Datasets, Implementation and Baselines",
            "text": "We use six real datasets for three affective reasoning tasks and one synthetic dataset for justifying E in our model. The statistics of them are shown in Table 2. Appendix C depicts the detailed introductions of each dataset.\nWe adopt the consistent benchmarks of the SOTA methods in three tasks, including the pretraining language model, hyper-parameters, t-tests, and metrics. The implementation details are shown in Appendix D.\nAccording to the hypotheses of these baselines, for each cogn skeleton, we choose one recent SOTA work: II: DialogXL (Shen et al., 2022). III: EGAT (Chen et al., 2023). IV: RGAT (Ishiwatari et al., 2021). V: DECN (Lian et al., 2021). VI: DAG-ERC (Shen et al., 2021)."
        },
        {
            "heading": "4.2 Overall Performance (RQ1)",
            "text": "Table 3 reports the results of ECPE and ECSR, with p <0.01 in the t-test, where the best improvement and best performance both concentrate on VI. With the visualization of Appendix F, we infer that the upper triangular adjacency matrix of DAG-ERC, not restricted by the backpropagation, benefits from Hypothesis 6. Moreover, II lags farthest behind in the ECPE while achieving the second best in the ECSR, showing that the reliance on a hypothesis is not equal in different tasks. Furthermore, without Hypotheses 1 and 6, III, IV, and V are far from the best performance since Hypothesis 1 has the maximum identifying space, and Hypothesis 6 supplies the highest number of information passing. Finally, it is worth noting that three skeleton-agnostic baselines and unsupervised methods perform poorly in the RECCON-IE dataset, indicating that our models have stronger representation learning capabilities as well as highlighting the continued research value of affective reasoning tasks.\nWe further conducted six sets of ablation experiments to study the effects of different modules. In Table 4, we summarized results under the following cases: replacing LossKL with BCE loss function\n(BCE); removing the LossKL (w/o LossKL); replacing Decoder module with a Linear layer (w/o Decoder); removing the RNN module (w/o Hypo 6); adding the edges from successors to predecessors (w/o Hypo 5); reducing the speaker types to one (w/o Hypo 4).\nAs shown in Table 4, BCEloss performs similarly to LossKL; thus, we empirically demonstrate that our auxiliary loss is essentially different from LossKL in VAE. The F1 score decreases heavily without auxiliary loss or decoder, these two are necessary ingredients for building complete processing to learn the causal representation via E. Besides, Hypotheses 4, 5, and 6 are all critical but removing Hypothesis 4 leads to the highlight degradation in 3 skeletons. This result corroborates the theory of Lian et al. (2021) and Shen et al. (2021), who state that speaker identity is the strong inductive bias in conversation. Finally, it is expected to see that skeleton with Hypotheses 4, 5, and 6 should be the closest to perfection while the DAG-ERC+Ours indeed achieves the SOTA.\nFurthermore, Appendix E reports the results of ERC task and sensitivity experiments to analyze how our model performs in different L and k."
        },
        {
            "heading": "4.3 Relationship analysis (RQ2)",
            "text": "We are also concerned about the causal discriminability for similar utterances. Table 5 demonstrates that in all three different causal models, none of the methods were able to distinguish between negative and positive samples. Because both negative and positive samples can be fit within these three causal models, solely from an embedding similarity perspective. However, our method significantly decreases the percentage of negative samples indicating the effectiveness of incorporating implicit cause noise to enhance causal discriminative ability.\nAdditionally, we show the adjacent matrices of our model and current SOTA methods in Appendix F. which indicates that our model can more freely explore the relationship between different utterances via adjacent matrices shifting rather than being limited to a fixed structure (e.g., attention module)."
        },
        {
            "heading": "4.4 Implicit Causes Study (RQ3)",
            "text": "The latent variable E is intended to represent the mentioned implicit causes. Therefore, the global distribution of the latent variable E should be approximately equal to the one of implicit causes. Although human evaluation labels are better for proving reasonable performance, it is intractable to annotate implicit causes due to their unobservability. We thus trained our model in a synthetic dataset given a set of fixed i.i.d. implicit causes to observe how the E is similar to the ground truth implicit causes distributions. Figure 4 (a-b) shows the projection of E and implicit causes, respectively, using t-SNE (Knyazev et al., 2019). We observe that E and implicit causes are both similarly clustered into three parts through the distribution properties. E is consistent with the implicit causes in the samples with or without noise indicating that E successfully learns the implicit causes.\nMoreover, in Appendix G, we first prove the approximate emotion consistency between utterance Ut and its implicit causes when Ut and Ui in the emotion-cause pair (Ut, Ui) do not belong to the same emotion category. Then, we demonstrate through the ERC task that by replacing H\u0302 with E, the emotion consistency provided by implicit causes is preserved."
        },
        {
            "heading": "4.5 Limitations",
            "text": "In our model, our method can distinguish between Ui \u2192 Uj and Ui \u2190 Uk \u2192 Uj . However, our method is unable to distinguish between Ui \u2192 Uj and Ui \u2190 L \u2192 Uj , where L represents a unobserved variable, called common causes or con-\nfounders. In Tables 3, 7, and 8, skeletons II, III, and IV generally lag far behind V and VI. This unsatisfactory performance of these skeletons indicates that excessive adding-edge leads to serious confounders.\nTherefore, we proposed a theoretical design for testifying the existing of latent confounders:\nConfounding between Non-adjacent Nodes: Consider two utterances Ui and Uj being nonadjacent utterances. Let Pa be the union of the parents of Ui and Uj : Pa = Ui \u222a Uj . If we perform an intervention on Pa (i.e., do(Pa = pa)), we thus have Ui \u22a5\u22a5 Uj if and only if there is a latent confounder L such that Ui \u2190 L\u2192 Uj .\nConfounding between Adjacent Nodes: Consider two utterances Ui and Uj being adjacent utterances: Ui \u2192 Uj . If there are no latent confounders, we have P (Uj |Ui) = P (Uj |do(Ui = ui)).\nIndeed, implementing intervention operations on conversation data poses a significant challenge. Therefore, in our new work, we have proposed general intervention writing: do(X) := Pa(X) = \u2205 where Pa(X) denotes the parent set. Moreover, the most significant obstacle to further research is the lack of a high-quality dataset with complete causal relationship labels. Hence, we have constructed a simulated dialogue dataset via GPT-4 and plan to make it open soon."
        },
        {
            "heading": "5 Conclusion",
            "text": "The results of testing prevalent approaches on the ARC task have demonstrated that almost all approaches are unable to determine the specific causal relationship that leads to the association of two well-fitted embeddings. In order to enhance the causal discrimination of existing methods, we constructed a SCM with i.i.d. noise terms, and analyzed the independent conditions that can identify the causal relationships between two fitted utterances. Moreover, we proposed the cogn framework to address the unstructured nature of conversation data, designed an autoencoder implementation to make implicit cause learnable, and created a synthetic dataset with noise labels for comprehensive experimental evaluation. While our method still has some limitations, such as confounders and the inability to scale to all methods, we hope that our theory, design, and model can provide valuable insights for the broader exploration of this problem to demonstrate that our work is de facto need for identifying causal relationships."
        },
        {
            "heading": "A Proof of Definition 2",
            "text": "Let X and Y be two variables in an SCM, with their respective noise terms denoted as EX and EY (where EX and EY are mutually independent). Let X\u0302 and Y\u0302 represent the fitted values of X and Y w.r.t. each other: X\u0302 = \u03bbY and Y\u0302 = 1\u03bbX . The residual terms between the fitted values and the true values are denoted as \u03a3X = X \u2212 X\u0302 and \u03a3Y = Y \u2212 Y\u0302 . The true strength of Y \u2192 X is k.\nHence, if the SCM only contains two variables writing:\nY = EY (10)\nX = kY + EX (11)\nThe residual terms could write:\n\u03a3X = X \u2212 \u03bb( 1\nk (X \u2212 EX)) (12)\n\u03a3Y = Y \u2212 1\n\u03bb (ky + EX) (13)\nThen, if the true causal relationship is from Y to X , \u03bb = k. \u03a3X does not contain the term of EY while \u03a3Y contains the term of EX . We could obtain the independence of residual terms writting:\n\u03a3X = \u03bbEX \u22a5\u22a5 Y (14)\n\u03a3Y = 1\n\u03bb EX \u22a5\u0338\u22a5 X (15)\nand vice versa. Therefore, we could obtain the independence condition:\n\u2022 \u03a3X \u22a5\u22a5 Y,\u03a3Y \u22a5\u0338\u22a5 X \u21d2 Y \u2192 X\n\u2022 \u03a3X \u22a5\u0338\u22a5 Y,\u03a3Y \u22a5\u22a5 X \u21d2 X \u2192 Y\nFurthermore, there may exist a set of independence: \u03a3X \u22a5\u0338\u22a5 Y , \u03a3Y \u22a5\u0338\u22a5 X . We would like to assume that there is a latent variable L, for this situation, constructing two relationships L\u2192 X and L\u2192 Y . Then we obtain: \u03a3L \u22a5\u0338\u22a5 X , \u03a3L \u22a5\u0338\u22a5 Y . By utilizing the transitivity of conditional independence, we can establish X \u22a5\u0338\u22a5 Y , and finally acheive the situation \u03a3X \u22a5\u0338\u22a5 Y , \u03a3Y \u22a5\u0338\u22a5 X . We likewise assume a latent variable L establishing X \u2192 L and Y \u2192 L for the opposite situation where \u03a3X \u22a5\u22a5 Y , \u03a3Y \u22a5\u22a5 X , and X , Y are two isolated variables in SCM. From the above independence conditions, we could obtain: \u03a3L \u22a5\u22a5 X , \u03a3L \u22a5\u22a5 Y . Due to the graph structure of SCM, we could obtain: \u03a3X \u22a5\u22a5 Y,\u03a3Y \u22a5\u22a5 X \u21d2 X \u22a5\u22a5 Y . Considering the residual terms, we finally obtain: X \u22a5\u0338\u22a5 \u03a3X and X \u22a5\u22a5 Y \u21d2 \u03a3X \u22a5\u22a5 Y and Y \u22a5\u0338\u22a5 \u03a3Y and Y \u22a5\u22a5 X \u21d2 \u03a3Y \u22a5\u22a5 X .\nHence, we could obtain additional two independence conditions:\n\u2022 \u03a3X \u22a5\u0338\u22a5 Y,\u03a3Y \u22a5\u0338\u22a5 X \u21d2 L\u2192 X,L\u2192 Y\n\u2022 \u03a3X \u22a5\u22a5 Y,\u03a3Y \u22a5\u22a5 X \u21d2 X \u2192 L, Y \u2192 L\nBased on the independence conditions of 2- variables SCM, we could extend it to the general SCM including more than 2 variables. Given any two variables in a SCM, we could testify to the independence condition and finally orientate via the whole SCM."
        },
        {
            "heading": "B Hypotheses and Algorithms for Skeletons",
            "text": "Hypothesis 0. \u2200Ui \u2208 D, it has the same causal skeleton as other utterances.\nBy regarding Hypothesis 0 as the prior knowledge, a common causal skeleton containing a target variable and a fixed number of related variables can reason about the relations between the target utterance and other considered utterances. We denote this skeleton of Ut by S(Ut). There are \u2200Ui, Uj \u2208 D, S(Ui) = S(Uj).\nAdditionally, there are some other empirical hypotheses from the above approaches. These hypotheses can be divided into two categories: one is about the \u201corder\u201d of utterances (Hypotheses 1, 2, 3), and the other is about intermingling dynamics among the interlocutors (Hypotheses 4, 5, 6).\nHypothesis 1. (Majumder et al., 2019) Under the sequential order, the target utterance receives information only from the previous utterance.\nCategory Hypothesis Original work I 1 Majumder et al. (2019) II 2 Velic\u030ckovic\u0301 et al. (2017) III 2,4 Chen et al. (2023) IV 3, 4 Ghosal et al. (2019) V 3(k = 1), 4, 5 Lian et al. (2021) VI 3, 4, 5, 6 Shen et al. (2021)\nTable 6: Statistics of 6 cogn skeletons. We detailed the hypotheses each cogn skeleton adopted and the original works from which we designed them.\nHypothesis 2. (Wei et al., 2020) Under the graph order, the target utterance receives information from all other utterances.\nHypothesis 3. (Ghosal et al., 2019) Under the local graph order, target utterance receives local information from k surround utterances.\nHypothesis 4. (Zhang et al., 2019) The influence between two utterances can be discriminated by whether the two utterances belong to the same speaker identity.\nHypothesis 5. (Lian et al., 2021) Target utterance only receives information from the predecessor utterances.\nHypothesis 6. (Shen et al., 2021) Between two utterances both related to the target utterance, there is also information passing, often dubbed as a partial order.\nA cogn skeleton is denoted by H = (V, E ,M). The V = U1, U2, U3, ..., UN represents a set of utterances in a conversation, and the edge (i, j,mi,j) \u2208 E denotes the influence from Ui to Uj , where mi,j \u2208 M is the type of the edge depending on whether Ui and Uj belong to one and the same speaker. ThusM = 0, 1, where 1 for that they are the same speaker and 0 for different. Then we denote the speaker type of Ui by a function p(Ui). At last, we show the process of building 6 cogn skeletons in Algorithms 1\u2212 6.\nAlgorithm 1: Buliding I cogn skeleton Input: D, p(\u00b7), k Output: H = (V, E)\n1 V \u2190 U1, U2, U3, ..., UN 2 E \u2190 \u2205 3 forall i \u2208 2, 3, . . . , N \u2212 1 do 4 E \u2190 E \u222a (i, i+ 1) 5 returnH = (V, E)\nFinally, in Figure 5, we show the adjacency matrix of each cogn skeleton by inputting a binary alternating conversation case with 6 utterances. But\nAlgorithm 2: Buliding II cogn skeleton Input: D, p(\u00b7), k Output: H = (V, E)\n1 V \u2190 U1, U2, U3, ..., UN 2 E \u2190 \u2205 3 forall i \u2208 2, 3, . . . , N do 4 forall j \u2208 2, 3, . . . , N do 5 if i! = j then 6 E \u2190 E \u222a (j, i) 7 else 8 Continue\n9 returnH = (V, E)\nAlgorithm 3: Buliding III cogn skeleton Input: D, p(\u00b7), k Output: H = (V, E ,M)\n1 V \u2190 U1, U2, U3, ..., UN 2 E \u2190 \u2205 3 M\u2190 0, 1 4 forall i \u2208 2, 3, . . . , N do 5 forall j \u2208 2, 3, . . . , N do 6 if p(Uj) = p(Ui) and i! = j then 7 E \u2190 E \u222a (j, i, 1) 8 else if p(Uj)! = p(Ui) and i! = j then 9 E \u2190 E \u222a (j, i, 0)"
        },
        {
            "heading": "10 else",
            "text": ""
        },
        {
            "heading": "11 Continue",
            "text": "12 returnH = (V, E ,M)\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0\n0 1 0 1\n0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0\n1 0 1 0\n0 1 0\n1 0\n1 0\n1 0\n0 1 0 0 1 0 1 0 1 0\n1 0 1 0\nI II III IV V VI"
        },
        {
            "heading": "10 else",
            "text": ""
        },
        {
            "heading": "11 Continue",
            "text": "Algorithm 5: Buliding V cogn skeleton Input: D, p(\u00b7), k Output: H = (V, E ,M)\n1 V \u2190 U1, U2, U3, ..., UN 2 E \u2190 \u2205 3 M\u2190 0, 1 4 forall i \u2208 2, 3, . . . , N do 5 \u03b3 \u2190 i\u2212 1 6 if p(U\u03b3) = p(Ui) then 7 E \u2190 E \u222a (\u03b3, i, 1) 8 else 9 E \u2190 E \u222a (\u03b3, i, 0)\n10 \u03b3 \u2190 \u03b3 \u2212 1 11 returnH = (V, E ,M)"
        },
        {
            "heading": "11 else",
            "text": "note that adjacency can not indicate all the differences among these skeletons, for example, Hypothesis 6 takes effect when the model learns the relationship based on the VI skeleton."
        },
        {
            "heading": "C Datasets",
            "text": "DailyDialog (Li et al., 2017): A Human-written dialogs dataset with 7 emotion labels (neutral, happiness, surprise, sadness, anger, disgust, and fear). We follow Shen et al. (2021) to regard utterance turns as speaker turns.\nMELD (Poria et al., 2019): A multimodel ERC dataset with 7 emotion labels as the same as DailyDialog.\nEmoryNLP (Zahiri and Choi, 2018): A TV show scripts dataset with 7 emotion labels (neutral, sad, mad, scared, powerful, peaceful, joyful).\nIEMOCAP (Busso et al., 2008): A multimodel ERC dataset with 9 emotion labels (neutral, happy, sad, angry, frustrated, excited, surprised, disappointed, and fear). However, models in ERC field\nare often evaluated on samples with first six emotions due to the too few samples of latter three emotions. 20 dialogues for validation set is following Shen et al. (2021).\nRECCON (Poria et al., 2021): The first dataset for emotion cause recognition of conversation including RECCON-DD and RECCON-IE (a subset emulating an out-of-distribution generalization test). RECCON-DD includes 5380 labeled ECPs and 5 cause spans (no-context, inter-personal, selfcontagion, hybrid, and latent).\nSynthetic dataset: We create a synthetic dataset by following the benchmark of the causal discovery field (Agrawal et al., 2021; Squires et al., 2022). To minimize sample bias, we did not randomly draw causal graphs as samples. Inversely, the number of samples in the synthetic dataset and the number of utterances and labels per sample are restricted to be consistent with RECCON. We use Causal Additive Models (CAMs), Specifically SCM structure for our datasets. As shown in Algorithm 7, first, we assume that each i.i.d. implicit causes E \u223c \u222550N (1, 1) if it is an emotion utterance in the original dataset, and E \u223c \u222550N (\u22121, 1) if it is not. Then, we update each utterance via speaker turns S: if there is an emotion-cause pair (Ui, Uj) \u2208 L, then Ui = \u03b1j,iUj + Ei (\u03b1j,i \u223c Unifrom([0.7, 1])), and for those pairs without emotion-cause label, \u03b1j,i \u223c Unifrom([0, 0.3]). Finally, we randomly select a noise \u03be \u223c Unifrom([\u22120.25, 0.25]) for each utterance Ui = Ui + \u03bei.\nD Implementation Details\nIn the word embedding, we adopt the affect-based pre-trained features1 proposed by Shen et al. (2021) for all baselines and models.\nAlthough there are different pre-trained models in these skeleton baselines, the SOTA work DAG-ERC and EGAT have investigated their performances in a consistent pre-trained model. Therefore, for a fair and direct comparison, we continue this benchmark using the pre-trained embedding published by DAG-ERC for three tasks.\nIn the hyper-parameters, we follow the setting of Shen et al. (2021) in the ERC task. Moreover, in the ECPE and ECSR, the learning rate is set to 3e-5, batch size is set to 32, and epoch is set to 60. Further in our approach, we set L to 1, and implicit cause size is set to 192, hidden size of GNN is set\n1https://drive.google.com/file/d/1R5K_ 2PlZ3p3RFQ1Ycgmo3TgxvYBzptQG/view?usp=sharing\nAlgorithm 7: Creating Non-noise Synthetic dataset Input: D, S, L Output: SCMD\n1 forall i \u2208 2, 3, . . . , S do 2 if Emotion(Ui) then 3 Ei \u223c \u222550N (1, 1) 4 else 5 Ei \u223c \u222550N (\u22121, 1) 6 Ui \u2190 Ei 7 forall i \u2208 1, 2, 3, . . . , S do 8 forall j \u2208 1, 2, . . . , i do 9 if (Ui, Uj) \u2208 L then\n10 Ui = \u03b1j,iUj + Ei(\u03b1j,i \u223c Unifrom([0.7, 1]))"
        },
        {
            "heading": "11 else",
            "text": "12 Ui = \u03b1j,iUj + Ei(\u03b1j,i \u223c\nUnifrom([0, 0.3]))\n13 SCMD \u2190 U1, U2, . . . , US return SCMD\nto 300, and dropout rate is 0.3. Meanwhile, because there is only one training dataset for ECPE and ECSR, we evaluated our method ten times with different data splits by following Chen et al. (2023) and then performed paired sample t-test on the experimental results.\nFinally, we adopted downstream task modules consistent with the SOTA baselines: Wei et al. (2020) in ECPE and ECSR, and Shen et al. (2021)for the ERC task.\nFor evaluation metrics, we follow Shen et al. (2021) towards ERC, Xia and Ding (2019) towards ECPE, and Poria et al. (2021) towards ECSR. Specifically, we adopt the macro F1 score in ECPE and ECSR tasks, micro F1 score for DailyDialog, and macro F1 score for the other three datasets in ERC task."
        },
        {
            "heading": "E Other Experiments in Affective Reasoning",
            "text": "In Table 7, our approach performs better than the corresponding baseline under all skeletons in four datasets. Hence, using a causal auto-encoder to find the implicit causes benefits this task. Besides, our approach improves significantly under skeletons II, III, and IV. From Figure 2, these three skeletons have more relevant nodes than others, so there are more redundant edges to be corrected\nby our approach, which is demonstrated again in Appendix E. In contrast, V and VI achieve the best results in MELD, EmoryNLP, and IEMOCAP datasets, which indicates that Hypothesis 5 is more probably a strong inductive bias that conversation enjoys.\nThen, we investigate how the number of layers and the variants of causal skeletons would affect the performance of our approach. So we further conducted several contrasts with k up to 5 and L up to 6, as shown in Figure 6. One observation is that the best performance occurs at either k = 1, 2, or 3, which indicates that k \u2a7e 4 offers no advantage and even leads to confounding. Moreover, L = 1 achieves the best performance under all k values. In other words, one layer is sufficient to yield the most effective implicit causes.\nF Visualization of Causal Graph\nIn the Figure 7 to 11, we showed the Visualization of the adjacency matrix (I \u2212 AT )\u22121. When the auxiliary loss LossKL achieves the lower bound, (I \u2212AT )\u22121 represents the relationship matrix between utterances and implicit causes.\nIn the ECPE task, we extracted 10 samples from test sets in different folds. To facilitate comparison and contrasting, we selected five 7-utterances cases and five 8-utterances cases. The IDs are as\nfollowing: 7-utterances cases: 110, 170, 224, 372, 500. 8-utterances cases: 62, 74, 104, 177, 584. To obtain the non-negative value, we adopted the T = sigmoid(\u00b7)\u2212 0.05 to process the original tensors (I \u2212 AT )\u22121 outputted from the encoder. We follow a common practice: set the threshold as 0.05 to delete some unimportant edges. And to highlight which implicit cause contributes the each utterance best, we adopted the sofmax(\u00b7) to process columns afterward and labeled the block with value > 0.\nIt is excepted that: (i) when skeletons construct overage edges, our model is able to degrade the influences of some negligible utterances by deleting the corresponding edges from their implicit causes. (ii) when skeletons construct insufficient edges, our model can add some edges to obtain more information."
        },
        {
            "heading": "G Proof of emotion consistency of implicit causes and utterances",
            "text": "We would like to explain why implicit causes and utterances are consistent in emotion from both theory and euqation, in the condition where emotional utterance and cause utterance possess different emotion types.\nWe define the implicit causes as the unobservable emotional desire and the utterances as the observable emotional expression. This definition is proposed in Ong et al. (2019, 2015), which also argues that emotional expression is affected by desires and event outcomes. Moreover, for emotion utterances that are not influenced by explicit cause factors, the source of their emotions should originate from implicit causes. The desire and the expression generally belong to the same emotion because the outcomes often have little effect on emotional expression. Our paper can also deduce this conclusion from the SCM (Equation 1). Considering there is a linear map f(\u00b7) from representation space to emotion space. Then we can obtain the\nfollowing:\nf((I \u2212A)U) = f(E) (16)\n(I \u2212A)f(U) = f(E) (17)\nf(U) = W T f(E) (18)\nNote that W = (I\u2212A) and Ai,i = 0. So in W , the value of the elements on the diagonal is constant at 1 and is a constant maximum of each column. Naturally, f(E) is an approximate estimate of f(U) especially Ut and Ui in the ECP (Ut, Ui) do not belong to the same emotion category, which is why we think implicit causes are reasonable when the F1 score of Table 6 is high.\nTherefore, we test the F1 scores in ERC task by replacing H\u0302 with E from a consensus that implicit causes should be aligned with utterances in the emotion types.\nIn Table 8, we reported the overall results of E in ERC task. Note that we only examine the sample of ECP with different emotion types. Among five skeletons and four datasets, almost all results achieve 90% scores of corresponding performances of H\u0302 , which indicates that E is practically aligned with H\u0302 in the affective dimension."
        }
    ],
    "title": "How to Enhance Causal Discrimination of Utterances: A Case on Affective Reasoning",
    "year": 2023
}