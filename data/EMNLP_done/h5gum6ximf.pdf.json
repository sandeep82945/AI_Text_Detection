{
    "abstractText": "This paper addresses the limitations of the common data annotation and training methods for objective single-label classification tasks. Typically, in such tasks annotators are only asked to provide a single label for each sample and annotator disagreement is discarded when a final hard label is decided through majority voting. We challenge this traditional approach, acknowledging that determining the appropriate label can be difficult due to the ambiguity and lack of context in the data samples. Rather than discarding the information from such ambiguous annotations, our soft label method makes use of them for training. Our findings indicate that additional annotator information, such as confidence, secondary label and disagreement, can be used to effectively generate soft labels. Training classifiers with these soft labels then leads to improved performance and calibration on the hard label test set.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ben Wu"
        },
        {
            "affiliations": [],
            "name": "Yue Li"
        },
        {
            "affiliations": [],
            "name": "Yida Mu"
        },
        {
            "affiliations": [],
            "name": "Carolina Scarton"
        },
        {
            "affiliations": [],
            "name": "Kalina Bontcheva"
        },
        {
            "affiliations": [],
            "name": "Xingyi Song"
        }
    ],
    "id": "SP:aa3d8356d847f192ad9fdef973feca140054de7c",
    "references": [
        {
            "authors": [
                "Joris Baan",
                "Wilker Aziz",
                "Barbara Plank",
                "Raquel Fernandez."
            ],
            "title": "Stop measuring calibration when humans disagree",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1892\u20131915, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Valerio Basile",
                "Michael Fell",
                "Tommaso Fornaciari",
                "Dirk Hovy",
                "Silviu Paun",
                "Barbara Plank",
                "Massimo Poesio",
                "Alexandra Uma."
            ],
            "title": "We need to consider disagreement in evaluation",
            "venue": "Proceedings of the 1st Workshop on Benchmarking: Past, Present and",
            "year": 2021
        },
        {
            "authors": [
                "Valerio Basile"
            ],
            "title": "It\u2019s the end of the gold standard as we know it",
            "venue": "on the impact of pre-aggregation on the evaluation of highly subjective tasks. In CEUR WORKSHOP PROCEEDINGS, volume 2776, pages 31\u201340. CEUR-WS.",
            "year": 2020
        },
        {
            "authors": [
                "Eyal Beigman",
                "Beata Beigman Klebanov."
            ],
            "title": "Learning with annotation noise",
            "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,",
            "year": 2009
        },
        {
            "authors": [
                "Katherine M Collins",
                "Umang Bhatt",
                "Adrian Weller."
            ],
            "title": "Eliciting and learning with soft labels from every annotator",
            "venue": "Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, volume 10, pages 40\u201352.",
            "year": 2022
        },
        {
            "authors": [
                "Aida Mostafazadeh Davani",
                "Mark D\u00edaz",
                "Vinodkumar Prabhakaran."
            ],
            "title": "Dealing with disagreements: Looking beyond the majority vote in subjective annotations",
            "venue": "Transactions of the Association for Computational Linguistics, 10:92\u2013110.",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Philip Dawid",
                "Allan M Skene."
            ],
            "title": "Maximum likelihood estimation of observer errorrates using the em algorithm",
            "venue": "Journal of the Royal Statistical Society: Series C (Applied Statistics), 28(1):20\u201328.",
            "year": 1979
        },
        {
            "authors": [
                "Anca Dumitrache",
                "Lora Aroyo",
                "Chris Welty."
            ],
            "title": "Crowdsourcing semantic label propagation in relation classification",
            "venue": "Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 16\u201321, Brussels, Belgium. Association for Computa-",
            "year": 2018
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Dirk Hovy",
                "Taylor Berg-Kirkpatrick",
                "Ashish Vaswani",
                "Eduard Hovy."
            ],
            "title": "Learning whom to trust with MACE",
            "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2013
        },
        {
            "authors": [
                "Emily Jamison",
                "Iryna Gurevych."
            ],
            "title": "Noise or additional information? leveraging crowdsource annotation item agreement for natural language tasks",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2015
        },
        {
            "authors": [
                "Sarah Lichtenstein",
                "Baruch Fischhoff"
            ],
            "title": "Do those who know more also know more about how much they know? Organizational behavior and human performance, 20(2):159\u2013183",
            "year": 1977
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Yida Mu",
                "Mali Jin",
                "Charlie Grimshaw",
                "Carolina Scarton",
                "Kalina Bontcheva",
                "Xingyi Song."
            ],
            "title": "Vaxxhesitancy: A dataset for studying hesitancy towards covid-19 vaccination on twitter",
            "venue": "arXiv preprint arXiv:2301.06660.",
            "year": 2023
        },
        {
            "authors": [
                "Martin M\u00fcller",
                "Marcel Salath\u00e9",
                "Per E. Kummervold."
            ],
            "title": "Covid-twitter-bert: A natural language processing model to analyse covid-19 content on twitter",
            "venue": "Frontiers in Artificial Intelligence, 6.",
            "year": 2023
        },
        {
            "authors": [
                "Mahdi Pakdaman Naeini",
                "Gregory Cooper",
                "Milos Hauskrecht."
            ],
            "title": "Obtaining well calibrated probabilities using bayesian binning",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 29.",
            "year": 2015
        },
        {
            "authors": [
                "Yixin Nie",
                "Xiang Zhou",
                "Mohit Bansal"
            ],
            "title": "What can we learn from collective human opinions on natural language inference data",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Rebecca J. Passonneau",
                "Bob Carpenter."
            ],
            "title": "The benefits of a model of annotation",
            "venue": "Transactions of the Association for Computational Linguistics, 2:311\u2013 326.",
            "year": 2014
        },
        {
            "authors": [
                "Andrew Ng"
            ],
            "title": "Cheap and fast \u2013 but is it good",
            "year": 2008
        },
        {
            "authors": [
                "Alexandra Uma",
                "Tommaso Fornaciari",
                "Dirk Hovy",
                "Silviu Paun",
                "Barbara Plank",
                "Massimo Poesio."
            ],
            "title": "A case for soft loss functions",
            "venue": "Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, volume 8, pages 173\u2013177.",
            "year": 2020
        },
        {
            "authors": [
                "Alexandra N Uma",
                "Tommaso Fornaciari",
                "Dirk Hovy",
                "Silviu Paun",
                "Barbara Plank",
                "Massimo Poesio."
            ],
            "title": "Learning from disagreement: A survey",
            "venue": "Journal of Artificial Intelligence Research, 72:1385\u20131470.",
            "year": 2021
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Reliable, human-annotated data is crucial for training and evaluation of classification models, with the quality of annotations directly impacting the models\u2019 classification performance. Traditionally, in order to ensure high quality annotated data, multiple annotators are asked to judge each individual data instance, and the final \u2018gold standard\u2019 hard label is determined by majority vote.\nHowever, this hard label approach tends to ignore valuable information from the annotation process, failing to capture the uncertainties and intricacies in real-world data (Uma et al., 2021). An emerging alternative approach that addresses these limitations is the use of soft labels through techniques such as Knowledge Distillation (Hinton et al., 2015), Label Smoothing (Szegedy et al., 2016), Confidence-based Labeling (Collins et al., 2022), and Annotation Aggregation (Uma et al., 2020). These soft label approaches demonstrate potential for improved robustness (Peterson et al.,\n2019), superior calibration, enhanced performance (Fornaciari et al., 2021) and even enable less than one-shot learning (Sucholutsky and Schonlau, 2021).\nThis paper\u2019s primary focus is on exploring effective ways for improving classification performance using soft labels. Our experimental findings indicate that confidence-based labelling significantly enhances model performance. Nevertheless, the interpretation of confidence scores can also profoundly influence model capability. Given the variability in confidence levels among different annotators (Lichtenstein and Fischhoff, 1977), aligning these disparate confidence levels emerges as the central research question of this paper.\nTo address this challenge, we propose a novel method for generating enhanced soft labels by leveraging annotator agreement to align confidence levels. Our contributions include:\n\u2022 We demonstrate how classification performance can be improved by using soft labels generated from annotator confidence and secondary labels. This presents a solution to the challenge of generating high-quality soft labels with limited annotator resources. \u2022 We propose a Bayesian approach to leveraging annotator agreement as a way of aligning individual annotators\u2019 confidence scores. \u2022 We introduce a novel dataset to facilitate research on the use of soft labels in Natural Language Processing.1"
        },
        {
            "heading": "2 Related Work",
            "text": "Current research typically interprets annotator disagreement in two primary ways, either by capturing diverse beliefs among annotators, or by assuming a single ground truth label exists despite disagreement (Rottger et al., 2022; Uma et al., 2021). This paper focuses on situations where the latter\n1Dataset can be found at: https://github.com/ GateNLP/dont-waste-single-annotation\nviewpoint is more applicable. Thus, we focus on traditional \u201chard\u201d evaluation metrics such as F1score which rely on a gold-label, despite the emergence of alternative, non-aggregated evaluation approaches (Basile et al., 2021; Baan et al., 2022; Basile et al., 2020). This is made possible because we evaluate on high-agreement test sets, where the \u2018true\u2019 label is fairly certain.\nAggregation of annotator disagreement generally falls into two categories: aggregating labels into a one-hot hard label (Dawid and Skene, 1979; Hovy et al., 2013; Jamison and Gurevych, 2015; Beigman and Beigman Klebanov, 2009), or modeling disagreement as a probability distribution with soft labels (Sheng et al., 2008; Uma et al., 2020; Peterson et al., 2019; Davani et al., 2022; Rodrigues and Pereira, 2018; Fornaciari et al., 2021).\nSimilar to Collins et al. (2022), our study explores how soft labels can be generated from a small pool of annotators, using additional information such as their self-reported confidence. This has benefits over traditional hard/soft label aggregation, which requires extensive annotator resources and/or reliance on potentially unreliable crowd-sourced annotators (Snow et al., 2008; Dumitrache et al., 2018; Poesio et al., 2019; Nie et al., 2020)."
        },
        {
            "heading": "3 Methodology",
            "text": "In order to generate soft labels, our methodology requires annotators to provide confidence scores. Figure 1 shows how each annotator provides both a primary class label and a confidence rating that represents their certainty. This ranges from 0 to 1, with 1 representing 100% confidence.2 In addition, annotators can also provide an optional \u2018secondary\u2019 class label. This represents their selection of the second most probable class. Thus, formally, the annotation of the text xi by an annotator am consists of a primary label lim, its confidence rating cim, and an optional secondary label l2im. We use yi to denote the text\u2019s true label.\nOverall, there are three steps to generating soft labels as shown in Figure 1:\n1. Annotator confidences are calibrated using our Bayesian method (Section 3.1) 2. Annotations are converted to soft labels (Section 3.2) 3. Annotator soft labels are merged into a final\n2In practice, annotators can provide this directly as a percentage or choose from a Likert-style numerical rating (e.g. 1-5) that is then converted to 0-1 scale.\nsoft label (Section 3.2)"
        },
        {
            "heading": "3.1 Bayesian Confidence Calibration",
            "text": "To calibrate confidence levels across annotators, we use annotator agreement as a proxy for reliability. For each annotator, we consider the level of agreement obtained (across all their annotations) when they have expressed a particular confidence score, and use this to re-weight their confidence. The process comprises two steps:\nFirst, we compute the probability of the primary label (lim) according to the confidence level (cim). This step is agnostic to the identity of the annotator.\nP (y\u0302i = lim|cim) = P (cim|lim)P (lim)\nP (cim) (1)\nWhere P (lim) is the prior, P (cim|lim) is the likelihood of the confidence score assigned to the primary label and P (cim) = P (cim|lim)P (lim) + P (cim|\u00aclim)P (\u00aclim) is the marginal probability. In this paper, we just make a simple assumption P (lim) = 1/C and P (\u00aclim) = (C\u2212 1)/C, where C is the total number of possible classes. We also assume that P (cim|yi) = cim and P (cim|\u00acyi) = 1\u2212 cim.\nIn the second step, using information about agreement, we compute the calibrated probability of the primary label (lim) given the specifc annotator (am).\nP (yi = lim|am) = P (am|lim)P (lim)\nP (am) (2)\nWhere our updated prior P (lim) = P (y\u0302i) calculated from Equation 1. P (am|lim) is the likelihood that an annotator assigns the label lim matching the true label, yi, and P (am) = P (am|yi)P (y\u0302i) + P (ai|\u00acyi)P (\u00acy\u0302i).\nWe calculate this likelihood based on annotator disagreement:\nP (am|yi) = Count(am \u2229 \u00acam, am, li) Count(am \u2229 \u00acam,\u00acam, li) (3)\nCount(am\u2229\u00acam, am, li) is the number of samples that involves annotator am and any other annotator(s) \u00acam, where both am and at least one other annotator provided the label li. Count(am \u2229 \u00acam,\u00acam, li) is the number of samples that involves annotator am and other annotator(s) \u00acam, where at least one annotator provided the label li.\nSimilarly, we calculate:\nP (am|\u00acyi) = Count(am \u222a \u00acam, am, li)\nCount(am \u2229 \u00acam,\u00acam,\u00acli) (4)\nCount(am\u222a\u00acam, am, li) is the number of samples that involves annotator am and other annotator(s) \u00acam where am annotated the sample as li but others did not. Count(am \u2229 \u00acam,\u00acam,\u00acli) is the number of samples that involves annotator am and other annotator \u00acam where \u00acam did not annotate the sample as label li.\nWe also employ fallback mechanisms to mitigate cases where annotator agreement cannot be reliably estimated. If the number of counts (< 3 in our experiments) is insufficient to calculate posterior probability, we fall back to using the prior confidence score."
        },
        {
            "heading": "3.2 Soft Label Conversion & Merging Annotations",
            "text": "Once we have calibrated confidence, P (yi|am), we assign this probability to the primary class li, and assign 1\u2212P (yi|am) to the secondary class l2i . Thus, in Figure 1, Annotator 1\u2019s soft label consists of 0.7 for their primary class and 0.3 for their secondary class. If a secondary class label is not provided by the annotator, we just uniformly distribute the confidence level to the other classes. In Annotator 2\u2019s case, this means 0.85 for their primary class and 0.05 for the three remaining classes.\nOnce we have generated soft labels for each annotator, we merge these into a final soft label by taking the mean of each class."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We experiment with two datasets: VaxxHesitancy (Mu et al., 2023) and the COVID-19 Disinformation Corpus (CDS) (Song et al., 2021). Both\ndatasets release the confidence scores which annotators provided alongside their class labels (annotators are denoted by a corresponding anonymous ID)."
        },
        {
            "heading": "4.1.1 COVID-19 Disinformation Corpus (CDS)",
            "text": "CDS (Song et al., 2021) includes 1,480 debunks of COVID-19-related disinformation from various countries. The debunks are classified into ten topic categories (e.g., public authority, conspiracies and prominent actors). The number of annotators per instance ranges from one to six. Each annotator has provided only one first-choice topic class and their confidence score for each annotated debunk (0 \u2264 cim \u2264 9)."
        },
        {
            "heading": "4.1.2 VaxxHesitancy",
            "text": "VaxxHesitancy (Mu et al., 2023) consists of 3,221 tweets annotated for stance towards the COVID19 vaccine. Each instance is categorised into provaccine, anti-vaccine, vaccine-hesitant, or irrelevant. The number of annotators per tweet ranges from one to three. Annotators provide a firstchoice stance category and a confidence score (1 \u2264 cim \u2264 5).\nVaxxHesitancy Additional Annotation As our aim is to investigate how additional information provided by annotators could impact classification performance, we also explore the integration of a secondary label for instances where annotators have expressed uncertainty about their primary label choice.\nAs none of the original datasets had such secondary labels, we undertake an additional round of data annotation, based on the original annotation guidelines. We introduce two new tasks in this data annotation round: 1) For all instances (train + test set) exhibiting low confidence (less than 4), we optionally request that annotators provide a \u2018second stance\u2019 label. We guide annotators to propose this if they believe it to be appropriate, even if it wasn\u2019t their primary choice. Consequently, we add 569 additional second-choice stances. 2) We assign a third annotator to all instances annotated by two annotators. As a result, we obtain a majority vote for the majority of annotated tweets. This majority vote can be employed for hard label training in subsequent experiments."
        },
        {
            "heading": "4.1.3 Dataset Split",
            "text": "We construct the test sets to contain instances where annotators reach agreement on every instance with high confidence scores. For VaxxHesitancy, we follow the original train-test split by including instances whose confidence scores are larger than three in the test set. For CDS, the test set has debunks that are labelled by more than one annotator with confidence scores larger than six (on their original 10 point scale). Given the limited size of this subset, data with only one annotation but very high confidence scores is also included in the CDS test set. Summary of the statistics is in the Appendix Table 2."
        },
        {
            "heading": "4.2 Baselines & Ablations",
            "text": "We compare our methods against a variety of hard/soft label aggregation strategies, which make use of annotator confidences/secondary labels to varying degrees. Hard label w/o confidence We employ majority voting for hard labels. In the absence of consensus, a class category is chosen at random. Hard label with confidence: For each xi, we aggregate ai to estimate a single hard label y\u0302i by giving different weights to lim according to the annotator confidence cim. Dawid-Skene Soft label:We utilise an enhanced Dawid-Skene model (Passonneau and Carpenter, 2014) as an alternative to majority voting, and use the model\u2019s confusion matrices to generate soft labels. This model only relies on class labels does not make use of additional information. Label Smoothing Soft Label: For each class, we use a mixture of its one-hot hard label vector\nand the uniform prior distribution over this class (Szegedy et al., 2016). Soft label w/o annotator confidence We explore generating soft labels using only annotator disagreement. In this approach, we assign 0.7 to the primary stance, 0.3 to the secondary stance label (if available), or evenly distribute the remaining probability among all other classes."
        },
        {
            "heading": "4.3 Experimental Setup",
            "text": "We conduct 5-fold cross-validation where each fold contains the entire training set, and 4/5 folds of the test set. We also investigate a second scenario in which we perform 5-fold cross-validation only on the test set. These two scenarios allow us to investigate the performance of soft labels when the level of annotator agreement differs.\nThese two scenarios are motivated by the fact that the train-test splits of our datasets contain an uneven distribution of samples: low annotator agreement samples were placed in the train set and high-agreement samples in the test set. This is necessary to enable evaluation against a gold-standard test set. However, for generating soft labels, we want to use a mixture of high-agreement and lowagreement annotations, so we include a portion of the original test set for training.\nWe perform experiments using Pytorch (Paszke et al., 2019) and the Transformers library from HuggingFace (Wolf et al., 2020). We fine-tune COVID-Twitter BERT V2, a BERT large uncased model that has been pre-trained on COVID-19 data (M\u00fcller et al., 2023). We fine-tune for 20 epochs with learning rate as 2e-5 (1 epoch warm-up followed by linear decay) and batch size as 8, with\nAdamW optimizer (Loshchilov and Hutter, 2017). We use cross-entropy loss. The model performance is evaluated with macro-F1 due to the imbalanced datasets, and we use expected calibration error (ECE) (Naeini et al., 2015) to measure model calibration.\nFor both the VaxxHesitancy and CDS datasets, we harmonise the range of confidence scores. In the case of the VaxxHesitancy dataset, this involves converting a confidence score of 5 to 1, 4 to 0.9, and so forth, down to 1 being converted to 0.6. Similarly, for the CDS dataset, a confidence score of 10 is converted to 1, 9 to 0.95, and so on, with 1 also becoming 0.6. 3"
        },
        {
            "heading": "5 Results",
            "text": "Soft labels improve classification performance across all of our scenarios. Table 1 presents the results on the VaxxHesitancy and CDS datasets. Soft labels surpass hard labels for both datasets, with and without confidence, as well as for the test-only and train + test set scenarios. In the case of test-set only, soft labels achieve 68.7 F1 Macro vs hard label\u2019s 66.9 (VaxxHesitancy) and 67.7 vs 65.2 (CDS). As previously mentioned, the test set comprises of only high-agreement samples, so this indicates that soft labels are beneficial for learning even when there is not a lot of disagreement between annotators and they are relatively certain.\nCombining confidence scores and secondary labels generates better soft labels. Using annotators\u2019 self-reported confidence scores helps to improve soft labels, as shown by the F1 and calibration improvements between soft labels in the \u2018with\u2019 and \u2018without confidence\u2019 settings (Table 1). Alternative approaches such as Dawid Skene are able to outperform soft labels when confidence scores are not available (71.7 vs 71.0 on VaxxHesitancy). However, once confidence information is introduced, soft labels significantly improves and outperforms alternatives, achieving 73.7 (from 71.0) on VaxxHesitancy and 71.1 (from 68.2 on CDS).\nFurthermore, for the VaxxHesitancy dataset, once secondary label information is included, classification performance is further improved from 73.7 to 74.5. This suggests that more consideration should be taken during the data annotation stage\n3We manually tested different confidence conversion scales and this conversion yields the best classification performance. See Appendix 3 for an alternative conversion strategy.\nto collect such information, as it can be greatly beneficial for the creation of effective soft labels.\nBayesian calibration outperforms other methods on the VaxxHesitancy dataset. By incorporating the full annotation information, i.e., confidence, secondary label, and annotator agreement, our Bayesian soft label method achieves a 75.2 F1 Macro score on the VaxxHesitancy dataset, improving upon 74.5 from the soft label stance 1 and 2. In addition, Bayesian soft label also has the best confidence alignment (ECE) score.\nHowever, on the CDS dataset, despite outperforming hard labels, our Bayesian method fails to improve upon soft labels. Its adjustments to soft labels results in a fall from 71.1 to 70.4. We believe this is due to the characteristics of the CDS dataset, which has 10 possible classes (vs the 4 of VaxxHesitancy), an increased range of possible confidences (1-9), as well as fewer overall samples. Because there is less annotator overlap, this greater range of options makes it more difficult to accurately estimate annotator reliability on a perconfidence score level. This reveals a direction in which our Bayes methodcould be improved, as it is currently reliant on sufficient overlap between an individual annotator and their peers."
        },
        {
            "heading": "6 Conclusion",
            "text": "We demonstrate the benefits of using soft labels over traditional hard labels in classification tasks. We propose a novel Bayesian method for annotation confidence calibration, and efficiently utilising all available annotation information, outperforming other methods for the VaxxHesitancy dataset. The performance improvements offered by soft labels suggests the importance of collecting additional information from annotators during the annotation process, with annotator confidence being particularly important."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work has been co-funded by the UK\u2019s innovation agency (Innovate UK) grant 10039055 (approved under the Horizon Europe Programme as vera.ai, EU grant agreement 101070093).4, the European Union under action number 2020-EU-IA-0282 and agreement number INEA/CEF/ICT/A2020/2381686 (EDMO Ire-\n4https://www.veraai.eu/\nland).5. Ben Wu is supported by an EPSRC Doctoral Training Partnership Grant and Yue Li is supported by a Sheffield\u2013China Scholarships Council PhD Scholarship.\nLimitations\nOur dataset construction introduces secondary labels that are not provided by the same annotators as those who created the original dataset, which may not accurately reflect the choices that the original annotators would have made.\nAs discussed in the Results section, the CDS dataset was more challenging due number of class labels and number of samples. This caused the Dawid Skene model to perform poorly. This issue may be alleviated using Laplace Smoothing, but we did not explore this due to time constraints.\nAnother important limitation of our Bayesian method is its assumption that an individual annotator\u2019s level of agreement with their peers is a good proxy for their reliability. This leaves it vulnerable to situations where there is high agreement between poor annotators.\nEven though our soft label method is effective, it is not compared against \u2018traditional\u2019 soft labels, which are constructed by aggregating many annotator labels per sample, since this would necessitate the large-scale annotation of the two datasets by many users, which we are trying to avoid as our goal is to reduce the amount of annotators and annotation effort required.\nFinally, we observed high variance across folds during cross-validation. We believe this was due to the small size of test set as well as the presence of \u2018hard-to-classify\u2019 samples in certain folds. These were samples where annotators relied on multimodal information to come to a decision (e.g. viewing a video embedded in the tweet). Our model is only provided with text, and so struggles on these samples.\nEthics Statement\nOur work has received ethical approval from the Ethics Committee of our university and complies with the research policies of Twitter and follows established protocols for the data annotation process. The human annotators were recruited and trained following our university\u2019s ethics protocols, including provision of an information sheet, a consent form, and the ability to withdraw from annotation\n5https://edmohub.ie\nat any time. For quality and ethics reasons, volunteer annotators were recruited from our university, rather than via Mechanical Turk."
        },
        {
            "heading": "A Appendix",
            "text": "Table 2 shows information about the composition of the two datasets used in our experiments.\nTable 3 shows the effect of changing the annotator confidence conversion scale from the one presented in the main section of this paper (9: 1.0 ... 1:0.6) to an alternative one (9: 1.0, 8: 0.9 ... 1: 0.1). By comparing between the two columns, we can see that this change leads to a drop of 1 F1 point for the resulting soft labels. This highlights the importance of selecting a good initial conversion scale."
        }
    ],
    "title": "Don\u2019t Waste a Single Annotation: Improving Single-Label Classifiers Through Soft Labels",
    "year": 2023
}