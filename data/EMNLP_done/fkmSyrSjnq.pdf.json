{
    "abstractText": "Dealing with multiple topics should be considered an important issue in dialogue summarization, because dialogues, unlike documents, are prone to topic drift. Thus, we propose a new dialogue summarization model that reflects dialogue topic distribution to consider all topics present in the dialogue. First, the distribution of dialogue topics is estimated by an effective topic discovery model. Then topic-informed prompt transfers estimated topic distribution information to the output of encoder and decoder vectors. Finally, the topic extractor estimates the summary topic distribution from the output context vector of decoder to distinguish its difference from the dialogue topic distribution. To consider the proportion of each topic distribution appeared in the dialogue, the extractor is trained to reduce the difference between the distributions of the dialogue and the summary. The experimental results on SAMSum and DialogSum show that our model outperforms state-of-the-art methods on ROUGE scores. The human evaluation results also show that our framework well generates comprehensive summaries.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jaeah You"
        },
        {
            "affiliations": [],
            "name": "Youngjoong Ko"
        }
    ],
    "id": "SP:64e1e801e0d0cda8683bc880b056ce4c3686250c",
    "references": [
        {
            "authors": [
                "David M Blei",
                "Andrew Y Ng",
                "Michael I Jordan."
            ],
            "title": "Latent dirichlet allocation",
            "venue": "Journal of machine Learning research, 3(Jan):993\u20131022.",
            "year": 2003
        },
        {
            "authors": [
                "Yulong Chen",
                "Yang Liu",
                "Liang Chen",
                "Yue Zhang."
            ],
            "title": "DialogSum: A real-life scenario dialogue summarization dataset",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5062\u20135074, Online. Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Yue Fang",
                "Hainan Zhang",
                "Hongshen Chen",
                "Zhuoye Ding",
                "Bo Long",
                "Yanyan Lan",
                "Yanquan Zhou."
            ],
            "title": "From spoken dialogue to formal summary: An utterance rewriting for dialogue summarization",
            "venue": "Proceedings of the 2022 Conference of the North",
            "year": 2022
        },
        {
            "authors": [
                "Xiachong Feng",
                "Xiaocheng Feng",
                "Bing Qin."
            ],
            "title": "A survey on dialogue summarization: Recent advances and new frontiers",
            "venue": "arXiv preprint arXiv:2107.03175.",
            "year": 2021
        },
        {
            "authors": [
                "Xiachong Feng",
                "Xiaocheng Feng",
                "Libo Qin",
                "Bing Qin",
                "Ting Liu."
            ],
            "title": "Language model as an annotator: Exploring dialogpt for dialogue summarization",
            "venue": "arXiv preprint arXiv:2105.12544.",
            "year": 2021
        },
        {
            "authors": [
                "Bogdan Gliwa",
                "Iwona Mochol",
                "Maciej Biesek",
                "Aleksander Wawer."
            ],
            "title": "Samsum corpus: A humanannotated dialogue dataset for abstractive summarization",
            "venue": "arXiv preprint arXiv:1911.12237.",
            "year": 2019
        },
        {
            "authors": [
                "Seungone Kim",
                "Se June Joo",
                "Hyungjoo Chae",
                "Chaehyeong Kim",
                "Seung-won Hwang",
                "Jinyoung Yeo."
            ],
            "title": "Mind the gap! injecting commonsense knowledge for abstractive dialogue summarization",
            "venue": "arXiv preprint arXiv:2209.00930.",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "year": 2019
        },
        {
            "authors": [
                "Changqun Li",
                "Linlin Wang",
                "Xin Lin",
                "Gerard de Melo",
                "Liang He."
            ],
            "title": "Curriculum prompt learning with self-training for abstractive dialogue summarization",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Junpeng Liu",
                "Yanyan Zou",
                "Hainan Zhang",
                "Hongshen Chen",
                "Zhuoye Ding",
                "Caixia Yuan",
                "Xiaojie Wang."
            ],
            "title": "Topic-aware contrastive learning for abstractive dialogue summarization",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Zhengyuan Liu",
                "Nancy Chen."
            ],
            "title": "Controllable neural dialogue summarization with personal named entity planning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 92\u2013106, Online and Punta Cana,",
            "year": 2021
        },
        {
            "authors": [
                "Yu Meng",
                "Yunyi Zhang",
                "Jiaxin Huang",
                "Yu Zhang",
                "Jiawei Han."
            ],
            "title": "Topic discovery via latent space clustering of pretrained language model representations",
            "venue": "The Web Conference.",
            "year": 2022
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Bowen Zhou",
                "Caglar Gulcehre",
                "Bing Xiang"
            ],
            "title": "Abstractive text summarization using sequence-to-sequence rnns and beyond",
            "venue": "arXiv preprint arXiv:1602.06023",
            "year": 2016
        },
        {
            "authors": [
                "Alexander M Rush",
                "Sumit Chopra",
                "Jason Weston."
            ],
            "title": "A neural attention model for abstractive sentence summarization",
            "venue": "arXiv preprint arXiv:1509.00685.",
            "year": 2015
        },
        {
            "authors": [
                "Xiangru Tang",
                "Arjun Nair",
                "Borui Wang",
                "Bingyao Wang",
                "Jai Desai",
                "Aaron Wade",
                "Haoran Li",
                "Asli Celikyilmaz",
                "Yashar Mehdad",
                "Dragomir Radev"
            ],
            "title": "CONFIT: Toward faithful dialogue summarization with linguistically-informed contrastive fine-tuning",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Yizhe Zhang",
                "Siqi Sun",
                "Michel Galley",
                "Yen-Chun Chen",
                "Chris Brockett",
                "Xiang Gao",
                "Jianfeng Gao",
                "Jingjing Liu",
                "Bill Dolan."
            ],
            "title": "Dialogpt: Large-scale generative pre-training for conversational response generation",
            "venue": "arXiv preprint arXiv:1911.00536.",
            "year": 2019
        },
        {
            "authors": [
                "Lulu Zhao",
                "Weiran Xu",
                "Jun Guo."
            ],
            "title": "Improving abstractive dialogue summarization with graph structures and topic words",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 437\u2013449.",
            "year": 2020
        },
        {
            "authors": [
                "Yicheng Zou",
                "Lujun Zhao",
                "Yangyang Kang",
                "Jun Lin",
                "Minlong Peng",
                "Zhuoren Jiang",
                "Changlong Sun",
                "Qi Zhang",
                "Xuanjing Huang",
                "Xiaozhong Liu"
            ],
            "title": "2021a. Topic-oriented spoken dialogue summarization for customer service with saliency-aware topic",
            "year": 2021
        },
        {
            "authors": [
                "Yicheng Zou",
                "Lujun Zhao",
                "Yangyang Kang",
                "Jun Lin",
                "Minlong Peng",
                "Zhuoren Jiang",
                "Changlong Sun",
                "Qi Zhang",
                "Xuanjing Huang",
                "Xiaozhong Liu"
            ],
            "title": "2021b. Topic-oriented spoken dialogue summarization for customer service with saliency-aware topic",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In general, text summarization aims to generate a summary by capturing the core meaning from an original document consistently written by one participant on a single topic, such as news, scientific publications, etc (Rush et al., 2015; Nallapati et al., 2016). On the other hand, since a dialogue consists of multi-speakers, the topic of the dialogue may be changed as a topic drift according to the speaker\u2019s intentions (Zhao et al., 2020; Feng et al., 2021a). Therefore, the dialogue summarization should take into account the distribution of multiple topics in a dialogue and reflect this distribution in generating the summary(Zou et al., 2021a).\n\u2217 Corresponding author\nFigure 1 shows an example of dialogue summary generation results from BART (Lewis et al., 2019) and TIDSum models in the SAMSum dataset (Gliwa et al., 2019); BART is a baseline model that has been widely used due to high performance in summary tasks. The content of the example dialogue in Figure 1 can be divided into three parts: 1 Tina and Steve are having pasta for dinner, 2 they will do the shopping together, and 3 they make an appointment to meet in the car park after Steve finishes work. That is, we can think that the dialogue contains three topics. However, BART did not capture the most important purpose of Tina and Steve\u2019s appointment to have pasta for dinner. They made an appointment to go shopping for ingredients for pasta in dinner, so this should be included in the summary. Therefore, we focus on generating a more comprehensive summary that captures all the topics in the dialogue, without missing an important topic.\nIn this paper, we propose a novel model, TopicInformed Dialogue Summarizer (TIDSum), that\ngenerates a summary by considering the distribution of topics existing in the dialogue. To estimate the distribution of multiple topics in a dialogue, we exploit the TopClus model to obtain the distribution for the input dialogue, which was developed for automatic topic discovery from text corpora (Meng et al., 2022). Moreover, a task-specific soft-prompt, namely the topic-informed prompt, is added to make the encoder context vector1 well capture the dialogue topic information in the decoding phase as well as the encoding phase. The topic-informed prompt is created by the latent embedding of the auto-encoder in the TopClus model. Since the latent embedding of dialogue from the TopClus model sufficiently contains the topic information of the dialogue, the topic-informed prompt can influence the context vector of other tokens in the encoder and decoder of summarizer through the encoder self-attention and decoder cross-attention processes. The output context vectors from the decoder are used for the topic extractor to estimate the topic distribution of the generated summary with it. Then the topic extractor provides an auxiliary loss function to reduce the difference between the dialogue topic distribution and the summary topic distribution in the training phase. This learning approach generates a better summary that well reflects the topic of the dialogue.\nIn the experiments, two daily dialogue summarization datasets, SAMSum and DialogSum, were used to evaluate our model. Compared to the previous model, the proposed model improved the SOTA performance by 1.19%p and 1.94%p in Rouge-1 for SAMSum and DialogSum, respectively."
        },
        {
            "heading": "2 Related Work",
            "text": "Recently, there has been increasing attention on neural summarization for dialogues. Current studies mainly apply transformer-based models (e.g., BART (Lewis et al., 2019)) to abstractly summarize dialogues. However, these models are pre-trained on generic text corpora and it is essential to finetune them in a specific way for dialog data. Many studies have investigated how to find topics in dialogues. Zhao et al. (2020) modeled the dialogue as an interactive graph according to the topic word information extracted from LDA (Blei et al., 2003). Feng et al. (2021b), which used DialoGPT (Zhang et al., 2019) as the annotator, performed three dia-\n1The last hidden states of the encoder that are used as the decoder input are called the encoder context vector.\nlogue annotation tasks, including keywords extraction, redundancy detection, and topic segmentation. Liu et al. (2021) proposed two topic-aware contrastive learning objectives. This method implicitly modeled the topic change and handled information scattering challenges for the dialogue summarization. Since summarizing dialogues is essential in customer services, Zou et al. (2021b) proposed a topic-augmented two-stage summarizer with a multi-role-specific topic modeling mechanism. Li et al. (2022) presented a novel curriculum-based prompt learning and applied a topic-aware prompt, from which we got the idea for a topic-informed prompt."
        },
        {
            "heading": "3 Topic-informed Summary Generation Framework",
            "text": "To perform the dialogue summarization effectively, it is necessary to identify the distribution of topics in the dialogue scattered across multiple utterances. Therefore, we propose a model to generate a topic-informed summary by reflecting the dialogue topic distribution to the summary topic distribution. The base architecture of TIDSum is a Transformerbased auto-regressive language model, BART."
        },
        {
            "heading": "3.1 Topic-Informed Prompt",
            "text": "In Figure 2-(1), we input a dialogue into TopClus to obtain the latent topic embedding and dialogue topic distribution. To be specific, the latent topic embedding lte is derived from the auto-encoder structure, TopClus, which ignores extraneous elements and contains only salient information from the input. Each topic tk is associated with the dialogue-topic distribution p(tk|lte), where k is the number of topics2. The distribution not only represents all the topical information present in the dialogue but also distinguishes between important and unimportant topics. As you can see in Figure 2-(1), the topic-informed prompt tip is created by concatenating two ltes to match the dimensions of BART. Since lte is a hidden state of the TopClus, an auto-encoder structure, it must be smaller than the input dimension of TopClus. The TopClus input is fixed at 768 dimensions because it origins from the CLS token of BERT that encodes the input dialogue. Thus, we set it to 512 dimensions to easily match the input dimensions of BART-large (1024 dims) by concatenating two\n2We set the number of topics to 5 for SAMSum and 7 for DialogSum, which show the highest performance.\nltes (1024 dims). Then, tip is located at the first token of the encoder input, preceding the dialogue D = {w1, w2, ..., wn} with n tokens. The final input form is X = {tip, w1, w2, ..., wn}. The topic information of the prompt tip infuses each token through a process known as encoder self-attention during training. In this manner, the encoder context vectors are topically enhanced within the encoder and their topic information is also propagated into the tokens of the decoder via encoder-decoder cross-attention."
        },
        {
            "heading": "3.2 Topic Extractor",
            "text": "In Figure 2-(2), the topic extractor, composed of MLP, serves to extract the distribution of the summary. Through encoder-decoder attention, the topic information sourced from tip is reflected in the decoder tokens. Therefore, we perform mean pooling on all tokens of the decoder to get the decoder topic-informed vector dti as follows:\ndti = 1\nM M\u2211 m=1 ym (1)\nwhere M is a length of the summary and Y = {y1, y2, ..., ym} is the corresponding summary of m tokens. The topic extractor estimates a summary topic distribution p(tk|dti) of k topics from dti. It is trained with a cross-entropy loss to reduce the difference between the dialogue topic distribution and the summary topic distribution. The topic distribution loss Ltop is formulated as follows:\nLtop = \u2212 K\u2211 k=1 p(tk|lte) log p(tk|dti) (2)\nwhere K is the number of topics. This ensures that the summary Y is generated to reflect the dialogue topics."
        },
        {
            "heading": "3.3 Topic-informed Summary Generation",
            "text": "The generation loss Lgen is typically defined as the negative log-likelihood of the target summary given the input dialogue.\nLgen = \u2212 M\u2211\nm=1\nlog p(ym|y1:m\u22121,X ) (3)\nThe final loss Lfinal is a weighted sum of the generation loss Lgen and the topic distribution loss Ltop: Lfinal = Lgen + \u03bbLtop (4) where \u03bb is a hyperparameter that controls the relative importance of these two losses. Experimentally, setting \u03bb to 0.75 showed the best performance. By minimizing the final loss Lfinal during training, the model is encouraged to generate summaries that are faithful to the input dialogue and well reflect the topic distribution of the dialogue."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "Two dialogue summarization datasets, SAMSum (Gliwa et al., 2019) and DialogSum (Chen et al., 2021), were used to verify the proposed model. SAMSum is an online chit-chat dataset from WhatsApp and WeChat. DialogSum is a real-life dialogue dataset containing diverse task-oriented scenarios and topics. It consists of a formal style of dialogue. Table 1 shows the additional details for each dataset."
        },
        {
            "heading": "4.2 Experimental Settings",
            "text": "We loaded the pre-trained \u201cfacebook/bart-large\u201d3 for initialization. The learning rates of SAMSum and DialogSum were set to 1e-5 and 3e-5, and the train batch size were 2 and 4, respectively. The training was conducted at Nvidia Quadro RTX 8000 48G. We employed Py-rouge package to evaluate the models following (Feng et al., 2021b; Liu and Chen, 2021)."
        },
        {
            "heading": "4.3 Comparison Models",
            "text": "TGDGA uses topic information and interactive graph structures. BART (DALL) performs three dialogue annotation tasks using PLM. ReWriteSum used the utterance rewriting mechanism to complete the omitted content. ConFiT is also trained via a novel contrastive fine-tuning. SICK++ summarized the dialogue in a way that utilizes\n3https://huggingface.co/facebook/bart-large\ncommensense knowledge. Li et al. (2022) applies a curriculum-based prompt learning. CtrlDiaSumm+Coref+DA generates controllable summaries with personal named entity."
        },
        {
            "heading": "4.4 Main Results",
            "text": "To evaluate our model, we employed the ROUGE scores, which are widely used in summarization tasks. In detail, the Rouge-1, Rouge-2 and RougeL variants, which consider unigram, bigram, and longest common subsequence overlap between generated and reference summaries, were utilized in our experiments (Lin, 2004).\nTable 2 provides a comparison of our model with previous approaches on SAMSum and DialogSum. As shown in Table 2, TIDSum achieved the-stateof-the-art performances on both datasets. TIDSum obtains relative improvements of 1.19%p on Rouge1, 1.71%p on Rouge-2 and 1.03%p on Rouge-L compared with the previous SOTA model in SAMSum, and 1.94%p, 0.85%p and 3.07%p in DialogSum. These results demonstrate the effectiveness of our technique for generating a summary by distinguishing and reflecting on the topics that appear in the dialogue.\nIn the ablation study, the topic extractor was found to have the most impact on performance, as it is responsible for creating a summary topic distribution in the framework that essentially reflects dialogue topics."
        },
        {
            "heading": "4.5 Human Evaluation",
            "text": "For qualitative measurement of the generated summaries, we conducted human evaluations on three metrics by just following Feng et al. (2021b). Informativeness evaluates how well the generated summaries capture more salient information. Conciseness measures how well the summary discards redundant information. Coverage measures how well the summary covers each part of the dialogue. We randomly sampled 60 dialogues with\ncorresponding generated summaries to evaluate the SAMSum dataset. We asked three expert evaluators to rate each metric on a scale of 1 to 5, with higher scores being better. The results are shown in Table 3.\nIn the case of informativeness, the golden summary has the highest value because it is a summary written by a person. However, TIDSum showed higher performance than BART. Conciseness is probably the shorter, the better, so it was slightly higher for TIDSum using only the topic extractor than for TIDSum infused with more topic information, but the performance was almost the same. Finally, coverage is a metric about whether a summary covers the entire content of the dialog, and TIDSum scored higher than the golden summary on this metric. This result shows that TIDSum effectively covers all the content of the dialogue."
        },
        {
            "heading": "5 Analysis",
            "text": "Comparison for Single-Topic vs Multi-Topic Dialogues\nWe herein attempt to verify that our model works better in multi-topic dialogues than in single-topic dialogues and it can generate comprehensive summaries well. The SAMSum test dataset was divided into single-topic and multi-topic ones. Dialogues with an entropy value of topic distribution less than 0.5 were regarded as single-topic ones. Eventually, the total dialogues are separated into 178 singletopic dialogues and 641 multi-topic dialogues. As a result, TIDSum showed larger improvement differences over baseline, BART-large in multi-topic dia-\nlogues (b) as shown in Figure 3. The result proves that TIDSum is effective for summarizing more multi-topic dialogues. In real-world scenarios, TIDSum can be applied not only to simple dialogues between two speakers, but also to multi-party dialogues, discussion summarization, etc. with more speakers and various topics. Figure 3 shows that the performance difference between TIDSum and BART-large is much larger in multi-topic than in single-topic. This suggests that it is applicable to dialogues with more diverse topics."
        },
        {
            "heading": "6 Conclusion",
            "text": "We propose TIDSum, a novel model for dialogue summarization. By reflecting the distribution of topics in the dialogue, TIDSum generates comprehensive summaries. We utilize the TopClus model to estimate topic distributions in the dialogue, and introduce a task-specific soft-prompt, the topic-informed prompt, to capture and infuse topic information through the encoding and decoding phases. The generated summaries were evaluated using SAMSum and DialogSum datasets, and our model outperformed previous approaches with a significant improvement in ROUGE scores and human evaluation results. Overall, TIDSum effectively captures and summarizes the details of each topic in the dialogue, resulting in high-quality summaries.\nLimitations\nThe proposed method needs to train the TopClus model with the dialogue data to get the topic distribution and latent topic embedding of the dialogue before fine-tuning the BART based summarization model. Since TopClus is an auto-encoder model with high dimensional layers, it takes a long time to train. With the obtained topic distribution and latent topic embedding, the BART based summarization model is trained and generates a summary in inference phase. This two-stage process is complicated and requires some time. Therefore, in order to simplify this process, our future work is to incorporate only the essential parts of TopClus into the main learning process."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF2020R1A2C2100362), in part by Institute for Infor-\nmation & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-00369, (Part 4) Development of AI Technology to support Expert Decision-making that can Explain the Reasons/Grounds for Judgment Results based on Expert Knowledge), in part by Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea Government (MSIT) (No. 2020-0-00368, A Neural-Symbolic Model for Knowledge Acquisition and Inference Techniques), and in part by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00421, AI Graduate School Support Program (Sungkyunkwan University))"
        }
    ],
    "title": "Topic-Informed Dialogue Summarization using Topic Distribution and Prompt-based Modeling",
    "year": 2023
}