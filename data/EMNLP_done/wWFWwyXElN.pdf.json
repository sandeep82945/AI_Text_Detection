{
    "abstractText": "This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR, using the synthesised data. We compare the performance of training with data generated in English and target languages, as well as translated Englishgenerated data, revealing the overall advantages of incorporating data generated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best case. Furthermore, we conduct a human evaluation by asking native speakers to assess the naturalness and logical coherence of the generated examples across different languages. The results of the evaluation indicate that LLMs such as ChatGPT and GPT-4 excel at producing natural and coherent text in most languages, however, they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset, whereas examples from GPT-4 exhibit competitive logical consistency. We release the generated data at https://github.com/mbzuai-nlp/Gen-X.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chenxi Whitehouse"
        },
        {
            "affiliations": [],
            "name": "Monojit Choudhury"
        },
        {
            "affiliations": [],
            "name": "Alham Fikri Aji"
        }
    ],
    "id": "SP:d7d63d445ccd1d0b57d0fbf81e094d393453d58d",
    "references": [
        {
            "authors": [
                "David Ifeoluwa Adelani",
                "Jade Abbott",
                "Graham Neubig",
                "Daniel D\u2019souza",
                "Julia Kreutzer",
                "Constantine Lignos",
                "Chester Palen-Michel",
                "Happy Buzaaba",
                "Shruti Rijhwani",
                "Sebastian Ruder"
            ],
            "title": "Masakhaner: Named entity recognition for african",
            "year": 2021
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Sebastian Ruder",
                "Dani Yogatama."
            ],
            "title": "On the cross-lingual transferability of monolingual representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623\u20134637, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Holger Schwenk."
            ],
            "title": "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
            "venue": "Transactions of the Association for Computational Linguistics, 7:597\u2013610.",
            "year": 2019
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Ruty Rinott",
                "Guillaume Lample",
                "Adina Williams",
                "Samuel Bowman",
                "Holger Schwenk",
                "Veselin Stoyanov."
            ],
            "title": "XNLI: Evaluating crosslingual sentence representations",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Nat-",
            "year": 2018
        },
        {
            "authors": [
                "Haixing Dai",
                "Zhengliang Liu",
                "Wenxiong Liao",
                "Xiaoke Huang",
                "Zihao Wu",
                "Lin Zhao",
                "Wei Liu",
                "Ninghao Liu",
                "Sheng Li",
                "Dajiang Zhu"
            ],
            "title": "Chataug: Leveraging chatgpt for text data augmentation",
            "venue": "arXiv preprint arXiv:2302.13007",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Kuan-Hao Huang",
                "Wasi Ahmad",
                "Nanyun Peng",
                "KaiWei Chang."
            ],
            "title": "Improving zero-shot cross-lingual transfer learning via robust training",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1684\u20131697, Online",
            "year": 2021
        },
        {
            "authors": [
                "Pratik Joshi",
                "Sebastin Santy",
                "Amar Budhiraja",
                "Kalika Bali",
                "Monojit Choudhury."
            ],
            "title": "The state and fate of linguistic diversity and inclusion in the NLP world",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Shanu Kumar",
                "Sandipan Dandapat",
                "Monojit Choudhury."
            ],
            "title": "diversity and uncertainty in moderation\u201d are the key to data selection for multilingual few-shot transfer",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1042\u20131055,",
            "year": 2022
        },
        {
            "authors": [
                "Anne Lauscher",
                "Vinit Ravishankar",
                "Ivan Vuli\u0107",
                "Goran Glava\u0161."
            ],
            "title": "From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Grandee Lee",
                "Xianghu Yue",
                "Haizhou Li."
            ],
            "title": "Linguistically Motivated Parallel Data Augmentation for Code-Switch Language Modeling",
            "venue": "Proc. Interspeech 2019, pages 3730\u20133734.",
            "year": 2019
        },
        {
            "authors": [
                "Hector Levesque",
                "Ernest Davis",
                "Leora Morgenstern."
            ],
            "title": "The winograd schema challenge",
            "venue": "Thirteenth international conference on the principles of knowledge representation and reasoning.",
            "year": 2012
        },
        {
            "authors": [
                "moyer",
                "Zornitsa Kozareva",
                "Mona Diab",
                "Veselin Stoyanov",
                "Xian Li"
            ],
            "title": "Few-shot learning with multilingual generative language models",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Haokun Liu",
                "William Huang",
                "Dhara Mungra",
                "Samuel R. Bowman."
            ],
            "title": "Precise task formalization matters in Winograd schema evaluations",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Nasrin Mostafazadeh",
                "Nathanael Chambers",
                "Xiaodong He",
                "Devi Parikh",
                "Dhruv Batra",
                "Lucy Vanderwende",
                "Pushmeet Kohli",
                "James Allen."
            ],
            "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
            "venue": "Proceedings of the 2016",
            "year": 2016
        },
        {
            "authors": [
                "banie",
                "Zaid Alyafeai",
                "Albert Webson",
                "Edward Raff",
                "Colin Raffel"
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
            "year": 2023
        },
        {
            "authors": [
                "Farhad Nooralahzadeh",
                "Giannis Bekoulis",
                "Johannes Bjerva",
                "Isabelle Augenstein."
            ],
            "title": "Zero-shot cross-lingual transfer with meta learning",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
            "year": 2020
        },
        {
            "authors": [
                "Edoardo Maria Ponti",
                "Goran Glava\u0161",
                "Olga Majewska",
                "Qianchu Liu",
                "Ivan Vuli\u0107",
                "Anna Korhonen."
            ],
            "title": "XCOPA: A multilingual dataset for causal commonsense reasoning",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Edoardo Maria Ponti",
                "Helen O\u2019Horan",
                "Yevgeni Berzak",
                "Ivan Vuli\u0107",
                "Roi Reichart",
                "Thierry Poibeau",
                "Ekaterina Shutova",
                "Anna Korhonen"
            ],
            "title": "Modeling language variation and universals: A survey on typological linguistics for natural language processing",
            "year": 2019
        },
        {
            "authors": [
                "Adithya Pratapa",
                "Gayatri Bhat",
                "Monojit Choudhury",
                "Sunayana Sitaram",
                "Sandipan Dandapat",
                "Kalika Bali."
            ],
            "title": "Language modeling for code-mixing: The role of linguistic theory based synthetic data",
            "venue": "Proceedings of the 56th Annual Meeting of the As-",
            "year": 2018
        },
        {
            "authors": [
                "Melissa Roemmele",
                "Cosmin Adrian Bejan",
                "Andrew S Gordon."
            ],
            "title": "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
            "venue": "AAAI spring symposium: logical formalizations of commonsense reasoning, pages 90\u201395.",
            "year": 2011
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Anirudh Srinivasan",
                "Gauri Kholkar",
                "Rahul Kejriwal",
                "Tanuja Ganu",
                "Sandipan Dandapat",
                "Sunayana Sitaram",
                "Balakrishnan Santhanam",
                "Somak Aditya",
                "Kalika Bali",
                "Monojit Choudhury"
            ],
            "title": "Litmus predictor: An ai assistant for building reliable, high-performing",
            "year": 2022
        },
        {
            "authors": [
                "Ishan Tarunesh",
                "Syamantak Kumar",
                "Preethi Jyothi."
            ],
            "title": "From machine translation to code-switching: Generating high-quality code-switched text",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Inter-",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Tikhonov",
                "Max Ryabinin"
            ],
            "title": "It\u2019s All in the Heads: Using Attention Heads as a Baseline",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "2023a. Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "2023b. Llama 2: Open foundation and fine-tuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi."
            ],
            "title": "Self-instruct: Aligning language models with self-generated instructions",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for",
            "year": 2023
        },
        {
            "authors": [
                "Chenxi Whitehouse",
                "Fenia Christopoulou",
                "Ignacio Iacobacci."
            ],
            "title": "EntityCS: Improving zero-shot cross-lingual transfer with entity-centric code switching",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6698\u20136714,",
            "year": 2022
        },
        {
            "authors": [
                "Genta Indra Winata",
                "Andrea Madotto",
                "Chien-Sheng Wu",
                "Pascale Fung."
            ],
            "title": "Code-switched language models using neural based synthetic data from parallel sentences",
            "venue": "Proceedings of the 23rd Conference on Computational Natural Language Learning",
            "year": 2019
        },
        {
            "authors": [
                "Zeqiu Wu",
                "Yushi Hu",
                "Weijia Shi",
                "Nouha Dziri",
                "Alane Suhr",
                "Prithviraj Ammanabrolu",
                "Noah A Smith",
                "Mari Ostendorf",
                "Hannaneh Hajishirzi."
            ],
            "title": "Fine-grained human feedback gives better rewards for language model training",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The success of NLP models greatly depends on the availability and quality of training data. This poses a significant challenge for multilingual NLP, as data for languages other than English is typically limited (Ponti et al., 2019; Joshi et al., 2020; Whitehouse et al., 2022). An approach to address the data scarcity challenge is through zero-shot crosslingual transfer or multitask training, in which a\n\u2217Work conducted while visiting MBZUAI.\nmodel is trained across data of diverse tasks and languages, exhibiting the capability to handle unseen tasks, particularly in larger models (Artetxe and Schwenk, 2019; Nooralahzadeh et al., 2020; Huang et al., 2021). However, when aiming for task-specific objectives, a smaller, fine-tuned model dedicated to that particular task often outperforms larger general-purpose, zero-shot models. In addition, a smaller task-specific model is more practical and cost-effective for training and deployment. Nevertheless, developing a powerful task-specific model becomes challenging in the absence of training data (Lauscher et al., 2020).\nConversely, recent powerful Large Language Models (LLMs) excel at handling general instructions and have shown promise in data generation tasks (Wang et al., 2023). In this work, we leverage LLMs to generate synthetic data for various multilingual commonsense reasoning tasks, XCOPA (Ponti et al., 2020), XWinograd (Tikhonov and Ryabinin, 2021), and XStoryCloze (Lin et al., 2022), where the training data is limited even for English (see Table 1). To augment the training data, we provide LLMs with instructions and examples from the original training data, prompting them to generate new and diverse examples. We explore the generation of synthetic data in English using different LLMs, including open-source models like Dolly-v21 and StableVicuna2, as well as ChatGPT and GPT-4. Although the weights and capabilities of the latter two models remain undisclosed, we explore them as they extend the capability of generating texts in languages beyond English.\nWe develop task-specific models by fine-tuning multilingual pre-trained language models, namely mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020), using the generated data. We then compare their performance against models trained on a limited set of human-created data in the\n1https://github.com/databrickslabs/dolly 2https://github.com/Stability-AI/StableLM\ntarget language whenever available, and otherwise through zero-shot transfer learning from manually created English training data. Our experiments demonstrate that training the models with relatively large synthetically generated datasets yields better performance than training with limited manuallycreated datasets. This finding empirically confirms the utility of synthetic data generated by LLMs for improving downstream task-specific models.\nWe expand the multilingual data synthesis using ChatGPT and GPT-4 on XCOPA and find that generating multilingual datasets generally surpasses the effectiveness of the zero-shot cross-lingual transfer. We further assess the quality of the generated dataset in different languages by asking native speakers to evaluate the naturalness and logical soundness of the generated dataset compared to the human-written examples. The annotation results reveal that while ChatGPT and GPT-4 successfully generate natural text in most languages, they struggle with generating understandable text in certain languages such as Tamil. Moreover, a noticeable gap is observed in terms of commonsense coherence when comparing ChatGPT-generated data to human-constructed data. On the other hand, GPT-4 significantly narrows this difference.\nTo summarise, our work has the following key contributions:\n\u2022 Augmenting three low-resource, multilingual commonsense reasoning datasets by leveraging and prompting four LLMs; \u2022 Fine-tuning smaller models, mBERT and XLMR, using the synthesised data and showcasing the practical value of the LLMgenerated data; \u2022 Performing an extensive analysis of the effects of various target languages in data generation and scaling, as well as a human evaluation of\nthe naturalness and logical coherence of the data generated in various languages; \u2022 Releasing the synthesised datasets for public use and reproducibility."
        },
        {
            "heading": "2 Related Work",
            "text": "Multilingual and Low-Resource NLP\nRecently, there has been increased attention on expanding NLP beyond English, including the development of multilingual models (Devlin et al., 2019; Conneau et al., 2020; Xue et al., 2021; Scao et al., 2022) as well as the creation of benchmarks to address multilingual challenges (Conneau et al., 2018; Artetxe et al., 2020; Adelani et al., 2021; Winata et al., 2023). Among the prevailing challenges faced across various languages, a common theme is the scarcity of available data.\nConsequently, when data is lacking, one approach is to employ zero-shot cross-lingual transfer. Studies conducted by Winata et al. (2023) have demonstrated the effectiveness of zero-shot crosslingual transfer for related languages. Additionally, Muennighoff et al. (2023) show that models finetuned only with English instruction data are capable of understanding multilingual instructions. In this work, we are tackling a similar scenario where the availability of data is limited.\nMultilingual Data Augmentation\nLauscher et al. (2020) show that few-shot can drastically increase the cross-lingual performance of small models, proving that multilingual data augmentation is an effective strategy. A series of works try to predict the cross-lingual accuracy of models through measurements and modelling (Xia et al., 2020), and study strategies for multilingual data augmentation, such as choosing the transfer languages (Lin et al., 2019), and predicting multilingual few-shot accuracy leading for optimal data augmentation approaches (Srinivasan et al., 2022).\nMany works focus on synthetic data augmentation for code-mixing, including utilising linguistic theories (Lee et al., 2019; Pratapa et al., 2018), machine translation models (Tarunesh et al., 2021), parallel corpus and Wikipedia (Winata et al., 2019; Whitehouse et al., 2022), and employing ChatGPT (Dai et al., 2023). Our work explores data augmentation on multilingual commonsense datasets with powerful instruction-tuned LLMs."
        },
        {
            "heading": "3 Dataset Augmentation",
            "text": "Our experiments use XCOPA, XWinograd, and XStoryCloze, which are selected due to (1) the limited availability of training data and (2) commonsense reasoning datasets present greater challenges for data synthesis. Table 1 summarises the statistics of the three datasets.\nXCOPA is a cross-lingual Choice of Plausible Alternatives dataset that translates and re-annotates the validation and test sets of English (EN) COPA (Roemmele et al., 2011) into 11 target languages (ET: Estonian, HT: Haitian Creole, ID: Indonesian, IT: Italian, QU: Quechua, SW: Swahili, TA: Tamil, TH: Thai, TR: Turkish, VI: Vietnamese, and ZH: Chinese).3 Each instance consists of a premise, a question (cuase/result), and two alternatives. The task is to predict the more plausible alternative.\nXWinograd expands the original English Winograd Schema Challenge (WSC) (Levesque et al., 2012) to five other languages (FR: French, JA: Japanese, PT: Portuguese, RU: Russian, and ZH),4 which consists of pronoun resolution problems aim-\n3https://huggingface.co/datasets/xcopa 4https://huggingface.co/datasets/Muennighoff/\nxwinograd\ning to evaluate the commonsense reasoning ability of a machine. Given a statement with two noun phrases and a pronoun, the challenge of WSC is to determine the referent of the pronoun, which can only be inferred from the context.\nXStoryCloze is collected by Lin et al. (2022), where the validation split of the original English StoryCloze dataset (Mostafazadeh et al., 2016) is translated into 10 other typologically diverse languages (RU, ZH, ES: Spanish, AR: Arabic, HI: Hindi, ID, TE: Telugu, SW, EU: Basque, and MY: Burmese). Each example consists of a foursentence commonsense story, a correct ending, as well as a wrong ending."
        },
        {
            "heading": "3.1 LLMs for Data Generation",
            "text": "Our preliminary experiments reveal that language models that are specifically fine-tuned on downstream NLP tasks, such as BLOOMZ (Scao et al., 2022) and Flan-T5 (Chung et al., 2022), struggle to follow the complex instructions. Conversely, more recent LLMs such as Dolly-v2, StableVicuna, ChatGPT, and GPT-4, which are designed to handle more intricate and general-purpose instructions, have demonstrated success in following our instructions for data generation. ChatGPT and GPT-4 also\nstand out with the capability of generating examples in non-English languages.\nWe explore synthetic data generation with the four aforementioned LLMs, balancing between open-access models and closed models (see \u00a75.1). Specifically, we use dolly-v2-12b,5 which is derived from EleutherAI\u2019s Pythia-12b (Biderman et al., 2023) and fine-tuned on a \u223c15K instructions generated by Databricks employees; and StableVicuna-13B, an RLHF (reinforcement learning from human feedback) fine-tuned Vicuna model on various conversational and instructional datasets - Vicuna is an open-source LLaMA model (Touvron et al., 2023a) fine-tuned on user-shared conversations collected from ShareGPT.6"
        },
        {
            "heading": "3.2 Instructions and Responses",
            "text": "We utilise LLMs to generate synthetic examples for all datasets by prompting them. We construct instructions using the descriptions from the dataset papers as a reference and provide LLMs with some examples, randomly sampled from the train (+validation) split of the original dataset, then ask LLMs to generate similar data points. We experiment with various instructions and evaluate the synthesised data on a smaller scale, update the instructions based on the errors, and then choose the best instruction to generate the final datasets.\nThe final instructions and responses are in Table 2. Our data generation process comprises the following key steps: (1) We establish the desired total number of examples to generate. This quantity can be determined by various factors such as budget constraints, a fixed ratio concerning the original dataset, etc. (2) We proceed to generate examples through the following iterative process: (a) To ensure diversity,7 we randomly sample a set of n examples from the training datasets. (b) We append these sampled examples to the instructions and prompt the model to generate an additional set of m new examples. (c) Afterwards, we perform post-processing and only add valid and unique examples to the generated set. Typically, the values of n and m are set to 5 to 10.\nWe focus on a fixed-budget scenario and first generate a total of 3-4K data points for each dataset with LLMs. LLMs tend to generate fewer samples than requested or inconsistent output in invalid for-\n5Model details are included in Appendix A. 6https://github.com/lm-sys/FastChat 7An analysis of the diversity of the generation as well as topic\ncoverage is included in Appendix B.\nmats. We report the success rate for different LLMs on the three datasets in Table 3, which indicates that GPT-4 has the most robustness.\nAmong the datasets, LLMs have the lowest generation success rate for XWinograd, which is more challenging. XWinograd requires both answers to be from the generated sentence, with only one pronoun being replaced. In addition, we observed pronoun inconsistency in the generated XWinograd data. Despite the requirement for interchangeable pronouns in the options, models frequently fail to comply. For example, \u201cThe dog bit the mailman because _ entered the yard.\u201d is generated by ChatGPT with the options \u2018The dog\u2018\u201d or \u201cthe mailman\u201d, however, \u201c_\u201d in the sentence cannot be replaced by the same pronoun for the given two options, hence it may make the task easier and the example is considered suboptimal. We keep those instances in the dataset and discuss further in \u00a76.1."
        },
        {
            "heading": "4 Experimental Setups",
            "text": "We first generate synthetic English examples for XCOPA, XWinograd, and XStoryCloze, with Dolly-v2, StableVicuna, ChatGPT, and GPT-4. The size of the final filtered synthesised data for the three datasets is 3.7k, 2K, and 1.7K, respectively. We then fine-tune mBERT, XLMR-base, and XLMR-large with the synthesised data and compare the zero-shot cross-lingual transfer performance across different languages, where we use the original validation set in target languages.\nFor XCOPA, we additionally experiment with generating data points directly in non-English languages, by providing examples in the target language and specifying the language desired for the generated data (see Table 2). However, since no examples for cause are included in TH and TR train/validation data (they do appear in the test split), we do not generate XCOPA for the two languages. We use ChatGPT and GPT-4 for multilingual synthetic data generation, as both Dolly-v2\nand StableVicuna exhibit limitations in effectively generating multilingual text. The size of the multilingual synthesised data is \u223c3.6K in each language.\nWe fine-tune models on all datasets as multiplechoice tasks8 by searching best learning rate from {5e\u22126, 10e\u22126}, and batch size from {8, 16, 32}. All the fine-tuning experiments are conducted on a single 40G A100. For generating data with Dollyv2 and StableVicuna, we use 2\u00d740G A100."
        },
        {
            "heading": "5 Results and Discussion",
            "text": "This section presents the main results of fine-tuned models on the three datasets and compares performance with generated data in different LLMs, languages, and scales."
        },
        {
            "heading": "5.1 General Result",
            "text": "Table 4 presents the average accuracy of fine-tuned mBERT, XLMR-Base, and XLMR-Large models across all languages on the three datasets. The models are trained using original data (ORI), different LLM-generated data (GEN), as well as a combination of both sources (O+G) in English.\nAcross different datasets, LLMs, and fine-tuned models, consistent improvements are observed when using both original and LLM-generated data. Among the models, Dolly-v2 performs the best on Xingorad when fine-tuned on mBERT, while\n8In our preliminary experiments, we find that formulating XWinograd as a binary text classification results poorly, in line with the observation from Liu et al. (2020) that the task formulation is essential to the performance of Winograd.\nGPT-4 achieves the highest accuracy in other settings. The most significant improvement is shown in XWinograd with XLMR-Base, where the addition of an extra 2k datapoints leads to an average accuracy enhancement of 12.8 compared to the baseline, across all four LLMs.\nWhen using only LLM-generated data, smaller models like mBERT and XLMR-Base generally outperform the baseline. However, with XLMRLarge, which achieves stronger baselines. e.g. >80 in XWinograd and XStoryCloze, the accuracy remains similar or even worse compared to using the original data. GPT-4-generated data demonstrates the best robustness but still experiences a decline in performance in XWinograd when the generated data size is similar to the original data. This highlights the challenges of generating data at a human-level quality."
        },
        {
            "heading": "5.2 Multilingual Data Generation",
            "text": "We investigate whether the synthetically generated multilingual dataset outperforms training solely in English. We choose the XCOPA dataset and explore two settings: synthetic multilingual data by asking LLMs to generate responses in the target languages directly and translating the Englishgenerated data to target languages with Google Translate API. We exclude Dolly-v2 and StableVicuna due to their limited effectiveness in generating non-English text. Although GPT-4 exhibits the most promising performance, it is significantly costlier compared to ChatGPT. Therefore, we also\nconsider using ChatGPT as a contrasting experiment under resource-constrained conditions.\nTable 5 shows the results for the languages that are available for all settings, excluding TR and TH (unavailable for LLM-generation, refer to \u00a74), and QU (not supported by the Google Translate API). We can see the impact of the generated data varies across different fine-tuned models and languages, aligning with the findings of Kumar et al. (2022). Training on GPT-4 synthesised data displays consistent improvement across all scenarios and languages, except the zero-shot cross-lingual result on HT with XLMR-Large.\nMore fluctuating results can be observed with ChatGPT-generated data. A comparison between GENEN + ORI and GENXX + ORI indicates that utilising data generated in target languages generally leads to improved performance with GPT-4 generated data, as well as in base models with ChatGPT-generated data. However, for XLMRLarge, employing ChatGPT-generated data in target languages mostly yields negative outcomes. In languages such as TA and VI, training on generated data in the target languages results in more performance degradation compared to zero-shot cross-lingual transfer. This suggests that ChatGPT\nperforms worse in those languages than XLMRLarge (Ahuja et al., 2023).\nTranslating the English dataset generally shows overall better results than training on the data generated directly in the target languages, with the exception of XLMR-Large with GPT-4. For SW, XLMR models fined-tuned with ChatGPT-generated data exhibit performance decline in most cases, even when the English-generated data benefits all other languages. This observation suggests that XLMR struggles with SW. In \u00a76.1 we select TA, SW, and the two best languages, ID and ZH, along with EN, for human evaluation.\nAdditionally, we conduct experiments adding Target Languages in Validation (TLV). This only results in minor variations in the performance, consistent with the findings of Ponti et al. (2020). We include the full results in Table 11 in Appendix D."
        },
        {
            "heading": "5.3 Dataset Scaling Up",
            "text": "We now investigate the impact of training on a larger scale of generated data on model performance. We focus on the XCOPA dataset and expand the generated data with ChatGPT (more budget-efficient) to 28.6k examples in English. We also compare the results of zero-shot cross-lingual\ntransfer with translating the English-generated data to target languages.\nThe results in Table 6 demonstrate the positive impact of scaling up the generated data on model performance. Particularly, XLMR-Large exhibits the most significant improvement.\nFurthermore, we conduct experiments on generating data with a fixed ratio of the original datasets and the results are included in Appendix C."
        },
        {
            "heading": "6 Human Evaluation",
            "text": "To better evaluate the quality of the generated datasets and compare them with the human-created data, we ask native speakers to annotate the multilingual data generated by ChatGPT and GPT-4.\nFor each dataset, we first select 50 generated examples in English, and then request two annotators to evaluate the examples in two categories: (1) Text Naturalness. The annotators are asked to choose one of the following options for each example: \u201cthe text sounds natural\u201d, \u201cthe text sounds awkward but understandable\u201d, or \u201cthe text is not understandable\u201d, and (2) Logic Soundness. This category focuses on the commonsense aspect of the examples. The annotators are required to select the most appropriate description from: \u201cthe correct option is (clearly) more plausible\u201d, \u201cboth options are equally plausible\u201d, \u201cboth options are implausible\u201d, or \u201cthe wrong option is actually more plausible\u201d. We only ask the annotators to evaluate the logic if the text is at least understandable.\nFor XWinograd, we introduce an additional evaluation criterion. Annotators are asked to determine whether the two noun phrases in the examples can be replaced by the same pronoun (refer to \u00a73.2). For XCOPA, we extend the annotations to nonEnglish languages, where we choose the two languages that demonstrate the most notable improvement, namely ZH and ID, as well as the two languages that exhibit the least improvement or regres-\nsion in performance with ChatGPT-generated data, namely TA and SW (see Table 5). In addition to the original examples and the generated examples in the target languages, we include 50 examples that are translated from the same English-generated examples (that were selected for annotation).\nTo ensure impartiality, all the examples are shuffled, and the annotators are not provided with information regarding the source of the examples (human-created, LLM-generated, or translated)."
        },
        {
            "heading": "6.1 Text Naturalness",
            "text": "Figure 1 presents the annotation results for XCOPA, averaged from two annotators for each language. For Text Naturalness, we can see that in EN, ID, ZH, and SW, both ChatGPT and GPT-4 achieved higher naturalness than the original dataset. This is particularly prominent in ID, revealing the fluency issue in the original ID data in XCOPA, which is also confirmed by a native speaker.\nIssue with Tamil In contrast, the performance of the TA dataset is surprisingly low, with a majority of examples classified as \"not understandable.\" Upon consulting language experts, we have identified several main issues in Tamil, including (1) the insertion of redundant words with the same meaning, such as \u201cI will retry to try it again\u201d (2) verb agreement errors, and (3) the presence of uncommon and out-of-context words.\nIt is worth noting that generating Tamil using GPT-4 is both slow and costly. We suspect that the tokenizer for Tamil, as well as similar languages like Telugu and Kannada, are poorly trained, resulting in unusable generation in those languages. While the low quality of the generated data could explain the significant decline in the performance of the XLMR-Large model when trained on ChatGPT-generated data in Tamil, intriguingly, models trained on Tamil data generated by GPT-4 show improvement over the baselines.\nTo further investigate this issue, we conduct an experiment where we fine-tune the models using only five examples from the TA examples generated by GPT-4 that are identified as natural and sound by the annotators. The improvement on mBERT under this setting is 50% of the total improvement seen with the entire 3.6K TA examples. For XLMR-base and XLMR-large, 15% and 3% of the total improvement can be observed, respectively. Considering that the estimated number of correct samples in\nthe 3.6k dataset is around 360, it is plausible that training solely on those examples could raise the accuracy level, or even surpass, what we observe for the entire dataset.9 An intriguing question that remains to be investigated in future research is why the remaining 3.2k incorrect or unnatural examples do not negatively impact the model\u2019s performance.\nThe translated text is typically less natural than the original and generated data (apart from ID due to issues in the original data). This result affirms that LLMs generally excel in generating fluent text for the languages it supports."
        },
        {
            "heading": "6.2 Logic Soundness",
            "text": "In terms of logic soundness, ChatGPT falls short compared to the original dataset. We further illustrate the categorised issues in the last column of the plots in Figure 1. We can see that for ChatGPT, the majority of the examples are labelled as \u201cboth options are equally plausible\u201d, only SW has more problematic examples with \u201cthe wrong option is actually more plausible\u201d. We suspect that this issue arises from the instruction provided (taken from the description of the original COPA dataset), which states that \u201cboth options could be plausible, but one is more plausible.\u201d In some cases, ChatGPT generates two choices that are excessively similar in terms of plausibility. On the other hand, GPT-4\n9We could not conduct this experiment as the entire dataset was not manually labelled.\ntends to generate options with more clear-cut differences in plausibility, mirroring the original data. We note that despite the description/instruction that both alternatives could happen, both the original dataset and the data synthesised by GPT-4 tend to present one plausible and one implausible option.\nFor English XWinograd and XstoryCloze, the majority of the examples in both original and generated examples are evaluated as natural and logically sound. For XWinograd, although more than 47 examples are evaluated to exhibit high text quality and follow commonsense logic, only 23 ChatGPTgenerated examples fulfil the requirement that both noun phrases should be interchangeable with the same pronoun. GPT-4 examples demonstrate better consistency, with 36 following this rule, whereas all original examples are found satisfactory."
        },
        {
            "heading": "7 Conclusions",
            "text": "This paper explores the effectiveness of utilising LLMs for data augmentation in cross-lingual datasets with limited training data. We specifically focus on commonsense reasoning tasks that are challenging for data synthesis. Our experiments including four LLMs for data generation on three datasets, showcase enhanced cross-lingual zeroshot transfer on smaller fine-tuned task-specific language models. However, the impact varies across different datasets and languages. Notably, larger models such as XLMR-Large, which have higher\nbaselines, demonstrate more difficulty in achieving performance improvements with LLM-generated data. Among the four LLMs, GPT-4-generated data exhibits mostly consistent superior performance.\nExpanding data generation directly in target languages also shows general improvements compared to cross-lingual zero-shot with the Englishgenerated data. Human evaluation of the synthesised multilingual dataset shows that the ChatGPT and GPT-4 generated data demonstrate high naturalness in most languages, even surpassing the original data. However, in certain languages like TA, both models fail to generate natural text. Additionally, when assessing the logical soundness of the dataset, examples synthesised by ChatGPT reveal notable inconsistencies regarding more plausible options compared to the original human-created data. In contrast, GPT-4 exhibits a performance on par with human-written data.\nIn conclusion, leveraging LLMs for data augmentation shows promise. However, the choice of LLM used for data generation significantly influences the quality of the resulting data, as well as its applicability to the language under consideration. In circumstances where a more advanced model such as GPT-4 cannot be accessed, other models can be utilised, though this might result in performance difficulties in certain non-English languages - a challenge that also exists for GPT-4 - and concerns regarding logical coherence. A compelling direction for future research could involve exploring the efficacy of more recent instruction-tuned or aligned open-source LLMs, such as LLaMA 2 (Touvron et al., 2023b) or T\u00dcLU (Wu et al., 2023), in enhancing data augmentation.\nLimitations\nWe have identified the following limitations in this work: (1)While LLMs, especially GPT-4, exhibit promising results in the context of multilingual commonsense data augmentation, they may encounter challenges when applied to extremely lowresource languages. (2) In order to achieve optimal performance, few-shot examples in the target language are still necessary for generating new examples. However, acquiring such examples may not always be feasible for all languages of interest. (3) The usage of closed models like GPT-4 is limited by licensing restrictions, and the results obtained from these models may not be reproducible. Nonetheless, the experiments conducted in this\nstudy demonstrate the potential benefits of leveraging LLMs for multilingual dataset augmentation.\nEthical Consideration\nSynthetic data generation with LLMs, especially multilingual data, should be approached with sensitivity and respect, as it reflects the linguistic, social, and cultural identity of a multilingual community. Since LLMs are trained on web data, they may encode biases perpetuating stereotypes, discrimination, or marginalisation of specific languages or communities. Therefore, collaboration with linguists, language experts, and community representatives is necessary to avoid the unintentional perpetuation of stereotypes and cultural insensitivity."
        },
        {
            "heading": "A Model Details",
            "text": "The open-source models used in the experiments are as follows:\n\u2022 mBERT: https://huggingface.co/ bert-base-multilingual-uncased\n\u2022 XLMR-base: https://huggingface.co/ xlm-roberta-base\n\u2022 XLMR-large: https://huggingface.co/ xlm-roberta-large\n\u2022 Dolly-v2: https://huggingface.co/ databricks/dolly-v2-12b\n\u2022 StableVinuca: https://huggingface.co/ CarperAI/stable-vicuna-13b-delta"
        },
        {
            "heading": "B Sentences and Event Diversity of ChatGPT-generated StoryCloze Data",
            "text": "As the StoryCloze dataset contains more sentences and has richer content, we follow the analysis of the ROC story and further compare the stylistic features in terms of sentence length, and the most frequent events10 generated by ChatGPT with the original data. This helps us to determine whether ChatGPT-generated data can capture the corpus distribution by randomly sampling n examples from the dataset in the instructions.\nIn Figure 2, we present the results of comparing the generated data points with the original 300 train set used as few-shot examples in the generation instructions. We can see that 23 of the 30 most frequent events in the original dataset can also be found in the 30 most frequent events of the ChatGPT-generated data. Regarding the sentence length, we observe that ChatGPT tends to\n10Here we follow Mostafazadeh et al. (2016) where an event is counted as any hyponym of \u201cevent\u201d or \u201cprocess\u201d in WordNet.\ngenerate longer sentences, especially for the ending sentences, whereas in the original dataset, they tend to be the shortest among all sentences."
        },
        {
            "heading": "C Fixed Ratio Data Augmentation",
            "text": "We experiment with generating data with a fixed ratio of the original datasets. Specifically, we compare training with the original English data (200 randomly selected examples) and augment it with different quantities of English examples generated by GPT-4, where we include original training instances in all cases.\nThe results in Table 7 showcase the performance on English test examples when fine-tuning mBERT and XLMR models with training data sizes that are 1\u00d7, 2\u00d7, 5\u00d7, and 10\u00d7 the size of the original dataset. We can see that performance consistently improves as we increase the amount of generated data except XStoryCloze, which has the highest baselines, echoing the previous findings. The relative performance gain is generally more pronounced when increasing the data from 2\u00d7 to 5\u00d7 for the other two datasets."
        },
        {
            "heading": "D Additional Results",
            "text": "This section includes the following additional results: Table 8, Table 9, and Table 10 show generated data in English with different LLMs on XCOPA, XWinograd, and XStoryCloze. Table 11 and Table 12 show the full result on XCOPA with ChatGPT and GPT-4."
        }
    ],
    "title": "LLM-powered Data Augmentation for Enhanced Crosslingual Performance",
    "year": 2023
}