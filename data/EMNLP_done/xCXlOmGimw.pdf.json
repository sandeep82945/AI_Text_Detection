{
    "abstractText": "Question generation (QG) from a given context can enhance comprehension, engagement, assessment, and overall efficacy in learning or conversational environments. Despite recent advancements in QG, the challenge of enhancing or measuring the diversity of generated questions often remains unaddressed. In this paper, we introduce a multi-question generation model (mQG), which is capable of generating multiple, diverse, and answerable questions by focusing on context and questions. To validate the answerability of the generated questions, we employ a SQuAD2.0 fine-tuned question answering model, classifying the questions as answerable or not. We train and evaluate mQG on the FairytaleQA dataset, a well-structured QA dataset based on storybooks, with narrative questions. We further apply a zero-shot adaptation on the TellMeWhy and SQuAD1.1 datasets. mQG shows promising results across various evaluation metrics, among strong baselines.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Hokeun Yoon"
        },
        {
            "affiliations": [],
            "name": "JinYeong Bak"
        }
    ],
    "id": "SP:2eb63acb5ca703f1d83f36154fa87cfdd5791f55",
    "references": [
        {
            "authors": [
                "Shuyang Cao",
                "Lu Wang."
            ],
            "title": "Controllable openended question generation with A new question type ontology",
            "venue": "CoRR, abs/2107.00152.",
            "year": 2021
        },
        {
            "authors": [
                "Boxing Chen",
                "Colin Cherry."
            ],
            "title": "A systematic comparison of smoothing techniques for sentencelevel BLEU",
            "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 362\u2013367, Baltimore, Maryland, USA. Association for Compu-",
            "year": 2014
        },
        {
            "authors": [
                "Yi Cheng",
                "Siyao Li",
                "Bang Liu",
                "Ruihui Zhao",
                "Sujian Li",
                "Chenghua Lin",
                "Yefeng Zheng."
            ],
            "title": "Guiding the growth: Difficulty-controllable question generation through step-by-step rewriting",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Dorottya Demszky",
                "Kelvin Guu",
                "Percy Liang."
            ],
            "title": "Transforming question answering datasets into natural language inference datasets",
            "venue": "CoRR, abs/1809.02922.",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Li Dong",
                "Nan Yang",
                "Wenhui Wang",
                "Furu Wei",
                "Xiaodong Liu",
                "Yu Wang",
                "Jianfeng Gao",
                "Ming Zhou",
                "Hsiao-Wuen Hon."
            ],
            "title": "Unified language model pre-training for natural language understanding and generation",
            "venue": "CoRR, abs/1905.03197.",
            "year": 2019
        },
        {
            "authors": [
                "Xinya Du",
                "Claire Cardie."
            ],
            "title": "Identifying where to focus in reading comprehension for neural question generation",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2067\u20132073, Copenhagen, Denmark. Asso-",
            "year": 2017
        },
        {
            "authors": [
                "Xinya Du",
                "Junru Shao",
                "Claire Cardie."
            ],
            "title": "Learning to ask: Neural question generation for reading comprehension",
            "venue": "Proceedings of the 55th Annual",
            "year": 2017
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898, Melbourne, Australia. Association",
            "year": 2018
        },
        {
            "authors": [
                "David J Francis",
                "Jack M Fletcher",
                "Hugh W Catts",
                "J Bruce Tomblin."
            ],
            "title": "Dimensions affecting the assessment of reading comprehension",
            "venue": "Children\u2019s Reading Comprehension and Assessment.",
            "year": 2005
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Deberta: Decoding-enhanced bert with disentangled attention",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Wenpeng Hu",
                "Bing Liu",
                "Rui Yan",
                "Dongyan Zhao",
                "Jinwen Ma"
            ],
            "title": "Topic-based question generation",
            "year": 2018
        },
        {
            "authors": [
                "Tanja Janssen",
                "Martine Braaksma",
                "Michel Couzijn."
            ],
            "title": "Self-questioning in the literature classroom: Effects on students\u2019 interpretation and appreciation of short stories",
            "venue": "L1-Educational Studies in Language and Literature, 9(1):91\u2013116.",
            "year": 2009
        },
        {
            "authors": [
                "Lauri Karttunen."
            ],
            "title": "Syntax and semantics of questions",
            "venue": "Linguistics and Philosophy, 1(1):3\u201344.",
            "year": 1977
        },
        {
            "authors": [
                "Young-Suk Grace Kim."
            ],
            "title": "Why the simple view of reading is not simplistic: Unpacking component skills of reading using a direct and indirect effect model of reading (dier)",
            "venue": "Scientific Studies of Reading, 21(4):310\u2013333.",
            "year": 2017
        },
        {
            "authors": [
                "Philippe Laban",
                "John Canny",
                "Marti A. Hearst."
            ],
            "title": "What\u2019s the latest? a question-driven news chatbot",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 380\u2013387, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Yash Kumar Lal",
                "Nathanael Chambers",
                "Raymond Mooney",
                "Niranjan Balasubramanian."
            ],
            "title": "TellMeWhy: A dataset for answering why-questions in narratives",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages",
            "year": 2021
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Nahid Mohseni Takaloo",
                "Mohammad Reza",
                "Ahmadi."
            ],
            "title": "The effect of learners\u2019 motivation on their reading comprehension skill: A literature review",
            "venue": "International Journal of Research in English Education, 2(3).",
            "year": 2017
        },
        {
            "authors": [
                "Nasrin Mostafazadeh",
                "Nathanael Chambers",
                "Xiaodong He",
                "Devi Parikh",
                "Dhruv Batra",
                "Lucy Vanderwende",
                "Pushmeet Kohli",
                "James Allen."
            ],
            "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
            "venue": "Proceedings of the 2016",
            "year": 2016
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Alison Paris",
                "Scott Paris."
            ],
            "title": "Assessing narrative comprehension in young children",
            "venue": "Reading Research Quarterly - READ RES QUART, 38:36\u201376.",
            "year": 2003
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Know what you don\u2019t know: Unanswerable questions for SQuAD",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784\u2013789,",
            "year": 2018
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Matthew Richardson",
                "Christopher J.C. Burges",
                "Erin Renshaw."
            ],
            "title": "MCTest: A challenge dataset for the open-domain machine comprehension of text",
            "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 193\u2013203,",
            "year": 2013
        },
        {
            "authors": [
                "Thomas Scialom",
                "Benjamin Piwowarski",
                "Jacopo Staiano"
            ],
            "title": "2019. Self-attention architectures",
            "year": 2019
        },
        {
            "authors": [
                "Thibault Sellam",
                "Dipanjan Das",
                "Ankur Parikh."
            ],
            "title": "BLEURT: Learning robust metrics for text generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881\u20137892, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Yixuan Su",
                "Tian Lan",
                "Yan Wang",
                "Dani Yogatama",
                "Lingpeng Kong",
                "Nigel Collier."
            ],
            "title": "A contrastive framework for neural text generation",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Md Arafat Sultan",
                "Shubham Chandel",
                "Ram\u00f3n Fernandez Astudillo",
                "Vittorio Castelli."
            ],
            "title": "On the importance of diversity in question generation for QA",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Siyuan Wang",
                "Zhongyu Wei",
                "Zhihao Fan",
                "Zengfeng Huang",
                "Weijian Sun",
                "Qi Zhang",
                "Xuanjing Huang."
            ],
            "title": "PathQG: Neural question generation from facts",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Zhen Wang",
                "Siwei Rao",
                "Jie Zhang",
                "Zhen Qin",
                "Guangjian Tian",
                "Jun Wang."
            ],
            "title": "Diversify question generation with continuous content selectors and question type modeling",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Sean Welleck",
                "Ilia Kulikov",
                "Stephen Roller",
                "Emily Dinan",
                "Kyunghyun Cho",
                "Jason Weston."
            ],
            "title": "Neural text generation with unlikelihood training",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Yu Yan",
                "Weizhen Qi",
                "Yeyun Gong",
                "Dayiheng Liu",
                "Nan Duan",
                "Jiusheng Chen",
                "Ruofei Zhang",
                "Ming Zhou."
            ],
            "title": "Prophetnet: Predicting future ngram for sequence-to-sequence pre-training",
            "venue": "CoRR, abs/2001.04063.",
            "year": 2020
        },
        {
            "authors": [
                "Bingsheng Yao",
                "Dakuo Wang",
                "Tongshuang Wu",
                "Zheng Zhang",
                "Toby Li",
                "Mo Yu",
                "Ying Xu."
            ],
            "title": "It is AI\u2019s turn to ask humans a question: Questionanswer pair generation for children\u2019s story books",
            "venue": "Proceedings of the 60th Annual Meeting of the",
            "year": 2022
        },
        {
            "authors": [
                "Xingdi Yuan",
                "Tong Wang",
                "\u00c7aglar G\u00fcl\u00e7ehre",
                "Alessandro Sordoni",
                "Philip Bachman",
                "Sandeep Subramanian",
                "Saizheng Zhang",
                "Adam Trischler."
            ],
            "title": "Machine comprehension by text-to-text neural question generation",
            "venue": "CoRR, abs/1705.02012.",
            "year": 2017
        },
        {
            "authors": [
                "Shiyue Zhang",
                "Mohit Bansal."
            ],
            "title": "Addressing semantic drift in question generation for semisupervised question answering",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi"
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "year": 2020
        },
        {
            "authors": [
                "Zhenjie Zhao",
                "Yufang Hou",
                "Dakuo Wang",
                "Mo Yu",
                "Chengzhong Liu",
                "Xiaojuan Ma."
            ],
            "title": "Educational question generation of children storybooks via question type distribution learning and event-centric summarization",
            "venue": "Proceedings of the 60th Annual",
            "year": 2022
        },
        {
            "authors": [
                "Qingyu Zhou",
                "Nan Yang",
                "Furu Wei",
                "Chuanqi Tan",
                "Hangbo Bao",
                "Ming Zhou."
            ],
            "title": "Neural question generation from text: A preliminary study",
            "venue": "CoRR, abs/1704.01792.",
            "year": 2017
        },
        {
            "authors": [
                "Yaoming Zhu",
                "Sidi Lu",
                "Lei Zheng",
                "Jiaxian Guo",
                "Weinan Zhang",
                "Jun Wang",
                "Yong Yu."
            ],
            "title": "Texygen: A benchmarking platform for text generation models",
            "venue": "The 41st International ACM SIGIR Conference on Research amp; Development in Information Re-",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Question generation (QG), focusing on the questions derived from specific text passages or documents, plays an integral role in a wide array of domains. It improves question answering (QA) systems (Sultan et al., 2020), enriches educational experiences (Yao et al., 2022), and enhances the engagement factor in chatbots (Laban et al., 2020). The effectiveness of QG tasks can be significantly improved by generating multiple questions, ensuring a broader, more comprehensive exploration of the content.\nThe importance of generating and evaluating multiple questions becomes evident when we examine the creation process of QA datasets (Richardson et al., 2013; Rajpurkar et al., 2016; Xu et al., 2022). Traditional QA dataset creation typically involves\n1Code: https://github.com/hkyoon95/mQG\ninstructing annotators to create a pre-determined number of questions for a given context. Recent QG research (Wang et al., 2020a; Yao et al., 2022), however, tends to rely on automatic evaluation of semantic similarity with golden questions, often overlooking the potential for diverse aspects of questions. When generating multiple questions, diversity is a crucial aspect to consider. The diversity of questions can span several dimensions, including varied aspects of the context, different answer types, and different phrasings for essentially the same question (Karttunen, 1977). This diversity allows for a more comprehensive exploration of the context. The diversity of questions can be broadly measured based on the type of answers they require; explicit questions with answers that can be explicitly found in the reading materials, and implicit questions with answers that require deductive reasoning. The crafting of multiple questions, bearing in mind both diversity and alignment with reading materials, poses a cognitively demanding and time-consuming task for humans.\nOne significant application of generating diverse and multiple questions is education. It has been observed that children can develop better reading comprehension skills at an early age by creating narrative questions themselves and being asked comprehension-related questions about storybooks (Francis et al., 2005; Janssen et al., 2009). Reading comprehension is an essential skill that requires learners to combine knowledge and reason about relations, entities, and events across a given context (Kim, 2017; Mohseni Takaloo and Ahmadi, 2017). Consequently, a system that can generate diverse and multiple narrative questions can serve as a valuable enhancement to educational resources, aiding in student engagement and promoting a deeper understanding of study materials.\nRecently, some researchers have attempted to generate multiple narrative questions. For educational applications, Yao et al. (2022) proposed to\ngenerate question-answer pairs with a three-step pipeline. As they use heuristic-generated answers to generate narrative questions most of their outcome is restricted to explicit questions. Also, Zhao et al. (2022) proposed to generate certain types of narrative questions and they tried to restrict the number of generated questions to a number of ground-truth questions, insisting that knowing question type distribution for each context is a subskill in education (Paris and Paris, 2003). We set these two approaches as our main baselines.\nTo address the above challenges, we introduce a multi-question generation model (mQG) that generates diverse and contextually relevant questions by referencing questions from the same context. mQG is trained with maximum question similarity loss LMQS , which is designed to make the representation of reference questions and the representation of a target question similar. Moreover, mQG employs a recursive generation framework, where previously generated questions are recursively fed back into the model as mQG is trained to output different questions from reference questions. Same as our two baselines, mQG is trained and evaluated on the FairytaleQA dataset, which focuses on narrative comprehension of storybooks. This dataset is designed to provide high-quality narrative QA pairs for students from kindergarten to eighth grade (ages 4 to 14), and labeled questions as explicit or implicit. We adopt Self-BLEU (Zhu et al., 2018) to evaluate the diversity of generated questions. Beyond diversity, to consider generated questions relevant to the context, we demonstrate the answerability evaluation model to assess whether the generated questions are answerable. We also evaluate on TellMeWhy (Lal et al., 2021) and SQuAD1.1 (Rajpurkar et al., 2016) datasets with zero-shot adaptation to further analyze the performance of mQG in different settings. Differing from previous approaches, mQG successfully generates a substantial number of diverse and answerable narrative questions.\nThe main contributions of this paper are summarized as follows.\n\u2022 We expand the scope of the question generation task by generating a comprehensive set of questions, regardless of our knowledge of the answers, and subsequently categorize them into answerable and non-answerable questions.\n\u2022 We introduce mQG, a novel question genera-\ntion model that is trained using the maximum question similarity loss LMQS and employs a recursive referencing process for generating a wide array of questions while preserving semantic correctness.\n\u2022 We introduce an answerability evaluation model capable of classifying questions as implicit, explicit, or unanswerable."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Question Generation",
            "text": "Based on given contents, question generation aims to generate natural language questions, where the generated questions are able to be addressed with the given contents. After neural approaches took over a large proportion in QG (Yuan et al., 2017; Zhou et al., 2017), QG can largely be separated by target answer aspect into answer-aware QG and answer-unaware QG. Answer-aware QG, as its name implies, provides an answer to a model and prompts it to generate questions based on those answers. On the other hand, answer-unaware QG mainly focuses on the context to formulate questions. The introduction of pre-trained Language Models (LMs) further accelerated advancements in QG, and many works have demonstrated significant improvement in the answer-aware QG task and presented promising possibilities for QG (Zhang and Bansal, 2019; Dong et al., 2019; Yan et al., 2020). This approach inherently favors explicit questions, which can be directly answered with the provided context. In answer-unaware QG, only a handful of studies have been conducted, primarily focusing on strategies such as sentence selection from a paragraph (Du and Cardie, 2017), employing transformer architectures with out-of-vocabulary methods (Scialom et al., 2019), and generating questions based on silver summaries (Zhao et al., 2022). In this paper, we utilize answer-unaware question generation, giving consideration to both the diversity and quality of explicit and implicit questions."
        },
        {
            "heading": "2.2 Diversity",
            "text": "In natural language generation (NLG), generating outputs that are not only correct but also diverse is essential. In the decoding aspect, diversity has been researched in areas such as top-k sampling (Fan et al., 2018), and nucleus sampling (Holtzman et al., 2020). These decoding methods tried to sample tokens from less likely vocabularies. Certain studies have focused on training models to\nyield more diverse outputs (Welleck et al., 2020; Yao et al., 2022), and on leveraging the combination of contrastive training and generation (Su et al., 2022). Recently, Sultan et al. (2020) evaluated the importance of diversity in QG, insisting that diverse and accurate questions yield better QA results. Additionally, some researchers explored diversity in QG based on relevant topic (Hu et al., 2018), content selectors with question type modeling (Wang et al., 2020b), control of question type (Cao and Wang, 2021), and difficulty level (Cheng et al., 2021). While these studies have addressed various aspects of diversity in QG, there is still considerable room for further research in this area. In this paper, we consider diversity a significant challenge in the question generation task and propose a model that can generate a wide range of answerable questions."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we formalize the multi-question generation task and introduce our mQG. We first formulate our task and then explain how our model\u2019s training process incorporates a maximum question similarity loss LMQS . Finally, we provide a detailed outline of our recursive generation framework."
        },
        {
            "heading": "3.1 Task Formulation",
            "text": "The QG task in this paper aims to generate each question using a given context, question type, and the history of questions generated from the same context with the same question type. We use seven\nwh-words (what, when, where, which, who, why, how) as question types. Mathematically, given the context C, question type QT , and history of generated questions Hi = (GQ1, GQ2, ..., GQi\u22121), this task can be defined as generating a question, G\u0302Q, where:\nG\u0302Q = argmaxGQi(Prob(GQi|QT,C,Hi)) (1)\nFor the training process, we extract wh-words from each question by applying part-of-speech tagging with the Spacy2 English Model. Due to the absence of a history of generated questions and an insufficient number of questions per context per question type in the FairytaleQA dataset, we utilize groundtruth questions that only share the context as the history of questions within the training process."
        },
        {
            "heading": "3.2 Diversity Enhanced Training",
            "text": "mQG is built upon BART (Lewis et al., 2020), which has demonstrated remarkable performance in various natural language processing tasks. The primary pre-training objective of BART is to reconstruct the masked input based on unmasked input. To further leverage the capabilities of the pre-trained BART, we introduce a maximum question similarity loss LMQS . This loss is designed to promote similar representations for different questions from the encoder and decoder.\nAs shown in Figure 1, the encoder takes in three inputs: the question type, which signifies the type of question to be generated; the context, which pro-\n2https://spacy.io/\nvides the necessary information for question generation; and ground-truth questions from the same context, serving as reference questions. These three inputs are concatenated, with a [SEP] token inserted between them. The encoder processes the input sequence and produces its corresponding representations. Subsequently, the decoder generates the representation for the target question. To calculate the maximum question similarity loss LMQS , we use mean pooling layers to convert question representations into sentence-level representations. The maximum question similarity loss LMQS is calculated between the sentence-level representation of the reference questions and the sentencelevel representation of a generated question. By encouraging the representation of different questions to be similar, we promote the generation of diverse questions that differ from reference questions.\nGiven a set of reference questions sentencelevel representation as Q = {Q1, ..., Qm} and a sentence-level representation of the target question as TQ, the maximum question similarity loss LMQS is computed as follows:\nLMQS = 1\nm m\u2211 i=1 max(0, 1\u2212 s(Qi, TQ)) (2)\nwhere s(Qi, TQ) is a cosine similarity calculation between representations. By optimizing the model parameters to maximize the sentence-level similarity between these different representations, we guide mQG to generate diverse questions within the range of semantic correctness. This is achieved by ensuring that all the representations, which are the ground truth questions, are semantically correct. In doing so, we maintain a balance between diversity and accuracy in the generated questions. The overall training objective L is defined as\nL = LCE + LMQS (3)\nLCE refers to the cross-entropy loss from a target question. As cross-entropy loss is calculated at the token level, the use of cross-entropy loss enhances mQG to generate syntactically correct questions."
        },
        {
            "heading": "3.3 Recursive Generation Framework",
            "text": "Figure 2 illustrates the generation process of mQG. First, the encoder takes question type, and context as input. The decoder then generates a question based on the information provided by the encoder.\nFor the subsequent generation steps, the previously generated questions are recursively fed back into the model. Specifically, the previous questions are concatenated with the same question type and context, separated by a [SEP] token. This concatenated sequence is then used as input for the next generation step. This recursive generation process continues until the desired number of questions per context per question type is achieved.\nThe use of this recursive generation process allows mQG to generate multiple questions while considering the previously generated questions. Following the training process of mQG, this generation process enables mQG to build upon its own previous outputs and generate different questions from previous outputs. We use beam search for the decoding method and return multiple sequences to exclude pre-generated questions. By leveraging a recursive framework, mQG demonstrates its capability to generate a variety of diverse questions that are contextually relevant and coherent."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "FairytaleQA (Xu et al., 2022). We train mQG with the FairytaleQA dataset, which is constructed for educational purposes. Each book is split into sections and annotators were instructed to create on average 2-3 narrative question-answer pairs per section. All question-answer pairs are annotated based on seven question types that capture narrative elements/relations. Questions are labeled as explicit or implicit questions based on whether or not the answer source can be directly found\nin the context. The original FairytaleQA dataset is constructed in a train/validation/test set with 232/23/23 books and 8,548/1,025/1,007 QA pairs. From the entire dataset, a small portion of questions (985 out of 10,580) spans multiple paragraphs. As mQG and baselines are fit for one paragraph we remove those questions. To cross-validate, we randomly shuffled the dataset and split it by books in train/validation/test set with roughly matching 80/10/10 (%)."
        },
        {
            "heading": "4.2 Baselines",
            "text": "In the experiments, we compare mQG with four baselines; an end-to-end model initialized with BART-large, and methods proposed in Su et al. (2022), Yao et al. (2022), Zhao et al. (2022) denoted as CB, QAG, and EQG. The last two baselines are designed for multiple question generation purposes.\nE2E. As the FairytaleQA dataset consists of multiple questions in one context, we concat all questions and train the BART-large model to generate questions based on each context. To match the number of generated questions, we set the maximal target length to 280 tokens which roughly matches the number of generated questions setting of mQG.\nCB (Contrastive Baseline). We construct this baseline following the framework in Su et al. (2022), which tackles the problem of diversity in open-ended text generation. This framework first trains the language model using contrastive loss and decodes it with a contrastive search method. Since the contrastive baseline is proven for diverse text generation we apply it to GPT2 (denoted as CB (GPT2)), and BART (denoted as CB (BART)) and set it as our baseline. During generation, the maximal target length is set to 280 tokens.\nQAG. This baseline follows a question-answer generation architecture by Yao et al. (2022). This architecture first generates answers based on a heuristic-based answer generation module, which generates multiple answers per context. With the generated answers, BART generates corresponding questions. And, to verify the quality of the generated questions, DistilBERT ranking module ranks each QA pair and chooses the top questions. As our task is to generate multiple questions, we\ndenote architecture without a ranking module as QAG and the top 10 questions per context chosen by the ranking module as QAG (top 10).\nEQG. EQG model (Zhao et al., 2022) generates questions based on silver summaries. Silver summary is a method proposed by Demszky et al. (2018), which inserts answers into the semantic parsed questions with a rule-based method. EQG consists of three steps: 1) generate question type distribution for each context with BERT; 2) generate silver summary with BART, using question type, question type ordering from a question type distribution module, and context; 3) generate question based on silver summary, question type, and question ordering with BART. Without a question type distribution module, EQG is able to generate multiple questions. Since our approach is to generate multiple questions we set the EQG baseline without question type distribution module."
        },
        {
            "heading": "4.3 Automatic Evaluation",
            "text": ""
        },
        {
            "heading": "4.3.1 Evaluation Metrics",
            "text": "In evaluating question generation, both the quality and diversity of the generated questions are critical components. Thus, we evaluate each aspect with separate automatic evaluation metrics. We use Rouge-L score (Lin, 2004), BERTScore (Zhang et al., 2020), and BLEURT (Sellam et al., 2020) to measure the quality of generated questions. Similar to Yao et al. (2022), for each ground-truth question, we find the highest semantic similarity score on generated questions from the same context than average overall semantic similarity scores. And, with multiple questions generated from the same context, we recognize the necessity to measure diversity automatically. For diversity measurement, we use Self-BLEU score (Zhu et al., 2018) which was introduced to evaluate just a variety of sentences. The Self-BLEU score, which uses each generated sentence as a hypothesis and others as references, is employed to evaluate the diversity of questions generated from the same context. A lower Self-BLEU score represents greater diversity. All metrics ranges are between 0 to 1 except Rouge-L score (0 to 100)."
        },
        {
            "heading": "4.3.2 Answerability Evaluation Model",
            "text": "In order to evaluate whether the generated questions correspond to the context, we leverage SQuAD2.0 dataset (Rajpurkar et al., 2018) to build\nan evaluation model. SQuAD2.0 is a questionanswering dataset with 100K answerable questions and 50K unanswerable questions. This dataset is used to enhance the evaluation model by classifying whether the questions are answerable or not. We use DeBERTa-base (He et al., 2021) as the backbone model.\nTo achieve our goal, we train the evaluation model on the QA task following implementation in Devlin et al. (2019). We construct two dense layers above the encoder; one for the answer start position and the other for the answer end position. And, as unanswerable questions and implicit questions do not have an answer span, for these questions [CLS] token is assigned as the answer start position and the answer end position. For implicit questions in the FairytaleQA dataset, we add a special token [IMP] and assign it as an answer start span and answer end span. First, we train the evaluation model with the SQuAD2.0 dataset on the QA task. For the second step, we train the evaluation model again with the FairytaleQA dataset. By utilizing a two-step training, the evaluation model is able to classify generated questions as explicit, implicit, or unanswerable. The number of answerable questions per section in Table 1 are based on classified results by the evaluation model. If the evaluation model classifies generated questions as implicit or explicit, then we count them as answerable. (Answerability evaluation model details are given in Appendix A.)"
        },
        {
            "heading": "4.3.3 Results",
            "text": "Table 1 presents evaluation results on the FairytaleQA test set. \u2018# Generated Questions Per Section\u2019 refers to the number of questions generated\nfor each section. In \u2018# Answerable Questions Per Section\u2019, as duplicate questions within the same context are not needed, we leave only one question from duplicate questions. Even though mQG is able to generate multiple questions within the maximum token length of BART, we roughly match the number of questions to QAG for fair comparison in Rouge-L F1, setting mQG to generate 4 questions per section per question type, totaling 28 questions per section. The same setting is applied to EQG, as EQG does not have limitations in generating multiple questions.\nGeneral baselines (E2E and CB) that generate multiple questions in one iteration show significant underperformance in the Rouge-L F1 score and in the number of generated questions, compared to strong baselines (QAG and EQG), and the mQG. This indicates that to generate multiple questions, a specific model is needed. Across all evaluation metrics, mQG consistently outperforms the baselines."
        },
        {
            "heading": "4.4 Human Evaluation",
            "text": "We evaluate the diversity and quality of generated questions on the FairytaleQA dataset with human judges. We hire five annotators, proficient in English as their first foreign language, to further evaluate the diversity and quality of the generated questions. We follow the human evaluation procedure described by Cao and Wang (2021) and compare mQG, with two robust baselines, EQG and QAG.\nQuestion Diversity. In the question diversity study, we randomly sample 5 books from the"
        },
        {
            "heading": "Architecture Type (%) Syntax (%) Content (%)",
            "text": "original test set; and for each book, we randomly sample 8 sections, totaling 40 sections. For each section, we randomly sample three questions as a question set from each model, and provide only the question sets for annotation. For each question set, the annotators rank the three models on a scale of 1 (highest) to 3 (lowest) based on three dimensions of diversity: type\u2013whether the three selected questions have different question types; syntax\u2013whether the three selected questions use different syntax; and content\u2013whether the three selected questions need to be addressed with diverse answers.\nAs shown in Table 2, on all dimensions, human annotators rate mQG as generating the most diverse questions compared to the other models, with each question requiring a different answer.\nQuestion Quality. In the question quality study, we again randomly sample 5 books from the original test set. For each book, we select a random sample of 8 sections. Each section contains four questions, each randomly sampled from three models and ground-truth, totaling 160 questions. Two dimensions are rated from 1 (worst) to 5 (best): appropriateness\u2013whether the question is semantically correct; answerability\u2013whether the question can be addressed by a given section.\nAs shown in Table 3, all models, when compared to the ground-truth, generate semantically correct questions. Given that mQG can generate a broad diversity of questions, these results confirm that mQG fulfills our goal of generating multiple questions while maintaining semantic correctness and relevance to the context."
        },
        {
            "heading": "4.5 Zero-shot Performance Evaluation",
            "text": "We conduct a zero-shot evaluation on two distinct datasets, to test mQG more in various real-world scenarios, where contexts and desired questions can differ. Zero-shot evaluation is essential for"
        },
        {
            "heading": "Architecture Appro. Ans.",
            "text": "assessing model performance as it illuminates the model\u2019s ability to generalize beyond the specific examples it was trained on."
        },
        {
            "heading": "4.5.1 Dataset",
            "text": "TellMeWhy (Lal et al., 2021). TellMeWhy dataset comprises free-form why-questions related to events in short sections. The dataset was created using template-based transformations to generate questions, with crowdsourcing to gather answers. Sections were sourced from ROCStories (Mostafazadeh et al., 2016), a similar domain to the training dataset (FairytaleQA). TellMeWhy contains a mixture of explicit and implicit questions. Approximately 28.82% of questions in the dataset are implicit. We evaluate with 1,134 sections and 10,689 questions from the test split.\nSQuAD1.1 (Rajpurkar et al., 2016). Squad1.1 dataset is a comprehensive benchmark that focuses on machine comprehension, question generation, and question answering tasks. It consists of a large collection of articles from Wikipedia, covering a wide range of topics, which is a different source from the training dataset (FairytaleQA). Each article is accompanied by a set of only explicit questions. We evaluate with 2,429 sections, and 12,010 questions from the SQuAD1.1 test split created by Du et al. (2017)."
        },
        {
            "heading": "4.5.2 Zero-shot Results",
            "text": "In zero-shot evaluation, we compare mQG with two strong baselines, EQG and QAG. Initially, we examine the performance on the Tellmewhy dataset in Table 4. Given that the TellMeWhy dataset only contains why-questions, we select why-questions from the generated questions for evaluation. mQG achieved the highest semantic similarity scores and outperformed baseline models in terms of the number of answerable questions and exhibited better\ndiversity. Zero-shot evaluation on the Tellmewhy dataset, which contains a mix of explicit and implicit questions, demonstrates the ability of mQG to generate different question styles based on answers effectively.\nTable 5 shows evaluation results on the SQuAD1.1 dataset. Even with an out-of-domain dataset, mQG still demonstrates notable performance. mQG outperforms in generating diverse questions and producing a greater number of answerable questions compared to other baselines. However, in the Rouge-L F1 score, mQG is slightly lower than QAG. This can be attributed to the exclusive focus of the SQuAD dataset on explicit questions, and the answer-aware question generation method used by QAG, which is renowned for its effectiveness in generating explicit questions. Yet, when employing embedding-based evaluation methods such as BERTScore and BLEURT, mQG outperforms the baseline models, particularly in the case of BLEURT. The fact that mQG still demonstrates decent performance on the SQuAD dataset, despite the limitation of the dataset to explicit questions and its status as an out-of-domain dataset, further emphasizes the effectiveness of mQG.\nThrough these two different settings, we see promising results of mQG. It shows the adaptability of mQG to diverse question styles and domains, further validating the robustness and utility of mQG."
        },
        {
            "heading": "5 Ablation Study",
            "text": ""
        },
        {
            "heading": "5.1 Setting of Question Number",
            "text": "Given that mQG can be set with the number of questions to generate, we conduct an experiment on various settings of question number per section per question type to generate. In Figure 3, the evaluation result is based on the original FairytaleQA test set. As the quantity of generated questions increases, the Rouge-L F1 score provides satisfactory results, though diversity decreases. This indicates\nthat a significant increase in the number of generated questions tends to produce similar questions with different phrasings. Setting the number of generated questions at 4 shows the optimal trade-off between the Rouge-L F1 and the Self-BLEU."
        },
        {
            "heading": "5.2 Analysis of Maximum Question Similarity Loss and Recursive Framework",
            "text": "As discussed in section 5.2, mQG aims to increase diversity within questions while maintaining semantic correctness. mQG w/o LMQS refers to the mQG model only trained with LCE . For mQG w/o LMQS and reference questions, we give only question type and context as input while training, and no recursive framework is used in inference. Table 6 shows that the mQG model with maximum question similarity loss LMQS and reference questions hugely increase diversity. Additionally, the number of answerable questions has also improved. This could be attributed to the fact that all ground-truth questions are answerable, and mQG maximizes the similarity between these questions and continually references the most probable question during inference. These results indicate that each framework of mQG effectively enhances the probability of generating a diverse set of possible questions."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we extend the scope of answerunaware question generation to generate multiple diverse questions. We propose a novel framework that applies a maximum question similarity loss during training to promote question diversity, followed by a recursive generation process for further refinement. Additionally, an evaluation model is introduced to verify the answerability of the generated questions. Recognizing the essential role of narrative questions in education, we train and evaluate mQG accordingly. Comprehensive experiments validate the efficacy of mQG across a variety\nof datasets, highlighting its potential utility in environments that demand diverse narrative questions."
        },
        {
            "heading": "Limitations",
            "text": "mQG framework utilizes a recursive feedback mechanism for generating questions during the inference stage. However, the quality of these generated questions remains uncertain. If the quality of previously generated questions is poor, this may adversely impact the quality of subsequent questions produced by mQG. Moreover, the quantity of questions that can be generated is limited by a maximum token threshold. Another limitation is the potential risk of misclassification by the evaluation model, which could lead to the categorization of unanswerable questions as answerable. Despite our efforts to mitigate this risk, the evaluation model is still at a level of uncertainty in accurately classifying the generated questions. Even with the fact that reliability scores can be low in NLP tasks, in the quality human evaluation, the reliability scores are relatively low. This can lead to uncertainty in the results."
        },
        {
            "heading": "Ethics Statement",
            "text": "The results are appropriately placed in the context of prior and existing research. All generation models are trained on the FairytaleQA dataset which is publicly available and has no ethical issues as annotated by educational experts. In the human evaluation process, we pay annotators more than the minimum wage."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank the anonymous reviewers for their helpful questions and comments. JinYeong Bak is the corresponding author. This work was partly supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea\ngovernment (MSIT) (No.2022-0-00680, Abductive inference framework using omni-data for understanding complex causal relations & No.20190-00421, AI Graduate School Support Program (Sungkyunkwan University)), and a grant from the National Research Foundation of Korea (NRF) [NRF-2021R1A4A3033128]."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "A Further Analysis on Evaluation Model",
            "text": ""
        },
        {
            "heading": "A.1 Preprocessing Dataset",
            "text": "To evaluate each cross-validation set with an answerability evaluation model, we train the evaluation model with different FairytaleQA trainsets. One is an originally constructed trainset and the others are randomly split by books. From the FairytaleQA dataset, some explicit questions were not able to be found in the section and some questions\nwith cross-annotated answers had different aspects of answers (explicit, implicit). We removed those questions and a number of total questions after preprocessing is described in Table 7."
        },
        {
            "heading": "A.2 Evaluation Model Postprocessing",
            "text": "In terms of post-processing, we take a similar approach by Devlin et al. (2019). Classified results yc of each question are formulated as:\nyc =  No Answer, if CLSse > ase + \u03c4 and CLSse > IMPse + \u03c4\nImplicit, else if IMPse > ase Explicit, otherwise.\n(4) CLSse denotes score of [CLS] token as answer start span and answer end span. IMPse denotes score of [IMP] token as answer start span and answer end span. ase denotes the best score of answer"
        },
        {
            "heading": "FairytaleQA",
            "text": "start span and answer end span without [CLS] and [IMP]. Additionally, if an answer end span indice is lower than an answer start span indice we classify it as no answer. Threshold \u03c4 is selected on the ground-truth set to maximize the performance. This threshold is set differently for each evaluation model. Figure 5 shows the answerable ratio percentage by different threshold settings. We also train three evaluation models with each train set for cross-validation in the main results. We select each threshold before a significant drop in the answerable ratio is observed. -12, -10, and -11 are each threshold for experiment1, experiment2, and experiment3."
        },
        {
            "heading": "A.3 Evaluation Model Results",
            "text": "We perform cross-validation to measure the performance of the main results in Table 1, and as a result, we train each evaluation model with each trainset. Since our goal is to classify questions as explicit, implicit, or unanswerable, we count explicit questions as accurate if at least one of the predicted answer tokens is found in the ground-truth answer. This is denoted as \"Accuracy\" in Table 8. The F1 measurement follows the implementation by Devlin et al. (2019). The evaluation model classifies explicit questions more accurately than implicit questions."
        },
        {
            "heading": "A.4 Classified Questions Analysis",
            "text": "We analyze the ratio of questions classified into different answer types by the answerability evaluation model. Even though the ground-truth questions do not contain unanswerable questions, the evaluation model classifies approximately 4.5% of the questions as unanswerable, as shown in Table 5. The problem of answer-aware question generation is well-known. QAG uses the answer as an input in the question generation process, and our results show that QAG is not fit for generating implicit questions, as only about 5.1% of questions are classified as implicit. The EQG baseline generates both explicit and implicit questions but only has a small number of total questions after removing duplicates. On the other hand, the mQG still has a large number of questions even after removing duplicates, totaling 8,820, with explicit and implicit questions roughly in a 3-to-1 ratio. These results show that the mQG generates both types of multiple questions better than other baselines."
        },
        {
            "heading": "B Diversity Exploration",
            "text": "For diversity evaluation, we calculate the SelfBLEU score among generated questions from the same context. Self-BLEU score is based on BLEU evaluation method (Papineni et al., 2002). The BLEU evaluation method has many criticisms for evaluating sentence-level corpus. If a higher-order n-gram precision goes to 0, the total BLEU score goes to 0. As an outcome, many variations applying the smoothing method for the BLEU score have shown (Chen and Cherry, 2014). We apply \u2019smoothing 1\u2019 described in Chen and Cherry (2014) since all the generated questions are sentence-level."
        },
        {
            "heading": "FairytaleQA",
            "text": "Examples of Self-BLEU scores are shown in table 10. When the Self-BLEU score goes up to 0.7830, almost all questions can be addressed by the same answers."
        },
        {
            "heading": "C Decoding Method and Model Selection",
            "text": "Moreover, in addition to the main results, we compare the performance of mQG between different backbone models and decoding methods. In Table 11, T5-based mQG exhibits the best Self-BLEU score but significantly lags behind BART-based mQG in terms of # Answerable Questions Per Section and Rouge-L score. This suggests that T5based mQG struggles to generate semantically correct questions. When comparing decoding methods, beam search outperforms nucleus sampling in all dimensions. This is due to the decoding process of mQG, which returns multiple sequences to exclude pre-generated questions. Beam search utilizes a tree search algorithm, whereas nucleus sampling does not. As a result, nucleus sampling tends to generate duplicate questions."
        },
        {
            "heading": "D Weighting Factor Impact on Performance",
            "text": "To determine how MQS loss affects training, we conduct experiments with the mQG model using different settings for the weighting factor \u03b2. The overall training objective L is defined as\nL = LCE + \u03b2 \u2217 LMQS (5)\nIn Table 12, Self-BLEU is calculated between questions that share context and question type. The optimal point of diversity is achieved when \u03b2 is set to 0.4. As \u03b2 increases, the Self-BLEU score decreases, while the number of answerable questions increases. This outcome aligns with our goal\nFairytaleQA"
        },
        {
            "heading": "FairytaleQA",
            "text": ""
        },
        {
            "heading": "TellMeWhy",
            "text": ""
        },
        {
            "heading": "SQuAD1.1",
            "text": "of implementing MQS loss to enhance diversity within the bounds of semantic correctness."
        },
        {
            "heading": "E Another Rouge-L Calculation",
            "text": "As mentioned in Section 4.3, we calculate the Rouge-L score only to find the highest score for each ground-truth question. This calculation method may lead to the one-to-many matching problem. To determine if the problem has occurred, we compare the results with another Rouge-L calculation Rouge-L (alt). This calculation excludes previously matched generated questions, allowing for only one-to-one matches. In Table 13, most Rouge-L (alt) results exhibit slightly lower scores in comparison to Rouge-L (ori), suggesting that\none-to-many problems have occurred, although the impact is relatively minor as the ground-truth questions are a unique set of questions. The significant difference in the TellMeWhy dataset can be attributed to the limited number of \u2019why\u2019 questions generated.\nF Implementation Details\nFor the mQG model, we use the MQS loss of the validation set as the selecting criteria. For the mQG models without MQS loss, we use MLE loss as the selecting criteria. Total training time was about 3 hours with 1 RTX A6000 GPU. We initialize the mQG model with pretrained BART-large, which has 406M parameters. Hyperparameters are follow: learning rate = 5e-6; batch size = 8; epoch = 15\nWe use RoBERTa-large model for BERTScore and BLEURT-20 model for BLEURT. For the evaluation model, we load SQuAD 2.0 finetuned DeBERTa-base model 3, which has 86M parameters, to further finetune. Total training time was about an hour with 1 RTX A6000 GPU. Hyperparameters are follow: learning rate = 5e-6; batch size = 16; epoch = 8"
        },
        {
            "heading": "G Examples of Generated Questions",
            "text": "Tables 14 and 15 show the generated examples of the mQG, EQG, QAG, and ground truth questions with the according section and classified results with the answerability evaluation model. Even with different settings for generating multiple questions, EQG still generated duplicate questions because it guided the model only with special tokens to generate multiple questions. QAG has generated different questions but with less diversity. In all questions, the evaluation model accurately classified the questions. Given the sufficient number of questions generated by each model, we selected four questions as representative examples. Given the sufficient number of questions generated by each model, we selected 4 questions as representative examples.\n3https://huggingface.co/deepset/ deberta-v3-base-squad2"
        }
    ],
    "title": "Diversity Enhanced Narrative Question Generation for StoryBooks",
    "year": 2023
}