{
    "abstractText": "Interlinear Morphological Glosses are annotations produced in the context of language documentation. Their goal is to identify morphs occurring in an L1 sentence and to explicit their function and meaning, with the further support of an associated translation in L2. We study here the task of automatic glossing, aiming to provide linguists with adequate tools to facilitate this process. Our formalisation of glossing uses a latent variable Conditional Random Field (CRF), which labels the L1 morphs while simultaneously aligning them to L2 words. In experiments with several under-resourced languages, we show that this approach is both effective and data-efficient and mitigates the problem of annotating unknown morphs. We also discuss various design choices regarding the alignment process and the selection of features. We finally demonstrate that it can benefit from multilingual (pre-)training, achieving results which outperform very strong baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shu Okabe"
        },
        {
            "affiliations": [],
            "name": "Fran\u00e7ois Yvon"
        }
    ],
    "id": "SP:2f9a4a463d685e57928e6f54d070afd3a7885819",
    "references": [
        {
            "authors": [
                "Ife Adebara",
                "AbdelRahim Elmadany",
                "Muhammad Abdul-Mageed",
                "Alcides Alcoba Inciarte"
            ],
            "title": "SERENGETI: Massively Multilingual Language Models for Africa",
            "year": 2023
        },
        {
            "authors": [
                "Cyril Allauzen",
                "Michael Riley",
                "Johan Schalkwyk",
                "Wojciech Skut",
                "Mehryar Mohri"
            ],
            "title": "Openfst: A general and efficient weighted finite-state transducer",
            "year": 2007
        },
        {
            "authors": [
                "Antonios Anastasopoulos",
                "Graham Neubig."
            ],
            "title": "Pushing the limits of low-resource morphological inflection",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Jason Baldridge",
                "Alexis Palmer."
            ],
            "title": "How well does active learning actually work? Time-based evaluation of cost-reduction strategies for language documentation",
            "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2009
        },
        {
            "authors": [
                "Diego Barriga Mart\u00ednez",
                "Victor Mijangos",
                "Ximena Gutierrez-Vasques."
            ],
            "title": "Automatic interlinear glossing for Otomi language",
            "venue": "Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas, pages",
            "year": 2021
        },
        {
            "authors": [
                "Emily M. Bender",
                "Joshua Crowgey",
                "Michael Wayne Goodman",
                "Fei Xia."
            ],
            "title": "Learning grammar specifications from IGT: A case study of chintang",
            "venue": "Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered",
            "year": 2014
        },
        {
            "authors": [
                "Taylor Berg-Kirkpatrick",
                "Alexandre Bouchard-C\u00f4t\u00e9",
                "John DeNero",
                "Dan Klein."
            ],
            "title": "Painless unsupervised learning with features",
            "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association",
            "year": 2010
        },
        {
            "authors": [
                "Balthazar Bickel",
                "Bernard. Comrie",
                "Martin Haspelmath."
            ],
            "title": "The Leipzig Glossing Rules: Conventions for interlinear morpheme-bymorpheme glosses",
            "venue": "Leipzig: Max Planck Institute for Evolutionary Anthropology, Department of",
            "year": 2008
        },
        {
            "authors": [
                "Phil Blunsom",
                "Trevor Cohn",
                "Miles Osborne."
            ],
            "title": "A discriminative latent variable model for statistical machine translation",
            "venue": "Proceedings of ACL-08: HLT, pages 200\u2013208, Columbus, Ohio. Association for Computational Linguistics.",
            "year": 2008
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Chris Dyer",
                "Jonathan H. Clark",
                "Alon Lavie",
                "Noah A. Smith."
            ],
            "title": "Unsupervised word alignment with arbitrary features",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2011
        },
        {
            "authors": [
                "David M. Eberhard",
                "Gary F. Simons",
                "Charles D. Fennig."
            ],
            "title": "Ethnologue: Languages of the World, 26th edition",
            "venue": "SIL International, Dallas, Texas.",
            "year": 2023
        },
        {
            "authors": [
                "Ryan Georgi."
            ],
            "title": "From Aari to Zulu : massively multilingual creation of language tools using interlinear glossed text",
            "venue": "Ph.D. thesis, University of Washington, Seattle, WA, USA.",
            "year": 2016
        },
        {
            "authors": [
                "Ryan Georgi",
                "Fei Xia",
                "William Lewis."
            ],
            "title": "Improving dependency parsing with interlinear glossed text and syntactic projection",
            "venue": "Proceedings of COLING 2012: Posters, pages 371\u2013380, Mumbai, India. The COLING 2012 Organizing Committee.",
            "year": 2012
        },
        {
            "authors": [
                "Ryan Georgi",
                "Fei Xia",
                "William D. Lewis."
            ],
            "title": "Enhanced and portable dependency projection algorithms using interlinear glossed text",
            "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
            "year": 2013
        },
        {
            "authors": [
                "Michael Ginn."
            ],
            "title": "SIGMORPHON 2023 shared task of interlinear glossing: Baseline model",
            "venue": "arXiv preprint arXiv:2303.14234.",
            "year": 2023
        },
        {
            "authors": [
                "Michael Ginn",
                "Sarah Moeller",
                "Alexis Palmer",
                "Anna Stacey",
                "Garrett Nicolai",
                "Mans Hulden",
                "Miikka Silfverberg."
            ],
            "title": "Findings of the SIGMORPHON 2023 shared task on interlinear glossing",
            "venue": "Proceedings of the 20th SIGMORPHON workshop on",
            "year": 2023
        },
        {
            "authors": [
                "Rebecca Hwa",
                "Philip Resnik",
                "Amy Weinberg",
                "Clara Cabezas",
                "Okan Kolak."
            ],
            "title": "Bootstrapping parsers via syntactic projection across parallel texts",
            "venue": "Nat. Lang. Eng., 11(3):311\u2013325.",
            "year": 2005
        },
        {
            "authors": [
                "Masoud Jalili Sabet",
                "Philipp Dufter",
                "Fran\u00e7ois Yvon",
                "Hinrich Sch\u00fctze."
            ],
            "title": "SimAlign: High quality word alignments without parallel training data using static and contextualized embeddings",
            "venue": "Findings of the Association for Computational Linguistics:",
            "year": 2020
        },
        {
            "authors": [
                "John D. Lafferty",
                "Andrew McCallum",
                "Fernando C.N. Pereira."
            ],
            "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
            "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML",
            "year": 2001
        },
        {
            "authors": [
                "Guillaume Lample",
                "Miguel Ballesteros",
                "Sandeep Subramanian",
                "Kazuya Kawakami",
                "Chris Dyer."
            ],
            "title": "Neural architectures for named entity recognition",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Thomas Lavergne",
                "Alexandre Allauzen",
                "Josep Maria Crego",
                "Fran\u00e7ois Yvon."
            ],
            "title": "From n-grambased to CRF-based translation models",
            "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 542\u2013553, Edinburgh, Scotland.",
            "year": 2011
        },
        {
            "authors": [
                "Thomas Lavergne",
                "Olivier Capp\u00e9",
                "Fran\u00e7ois Yvon."
            ],
            "title": "Practical very large scale CRFs",
            "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504\u2013513, Uppsala, Sweden. Association for Computational Linguistics.",
            "year": 2010
        },
        {
            "authors": [
                "Christian Lehmann."
            ],
            "title": "Interlinear morphemic glossing",
            "venue": "Morphologie. Ein internationales Handbuch zur Flexion und Wortbildung., volume 17 of Handb\u00fccher zur Sprach- und Kommunikationswissenschaft, pages 1834\u20131857. Berlin & New York: W.",
            "year": 2004
        },
        {
            "authors": [
                "William D. Lewis",
                "Fei Xia."
            ],
            "title": "Developing ODIN: A multilingual repository of annotated language data for hundreds of the world\u2019s languages",
            "venue": "Journal of Literary and Linguistic Computing (LLC), 25(3):309\u2013 319.",
            "year": 2010
        },
        {
            "authors": [
                "Angelina McMillan-Major."
            ],
            "title": "Automating gloss generation in interlinear glossed text",
            "venue": "Proceedings of the Society for Computation in Linguistics 2020, pages 355\u2013366, New York, New York. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Sarah Moeller",
                "Mans Hulden."
            ],
            "title": "Automatic glossing in a low-resource setting for language documentation",
            "venue": "Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages, pages 84\u201393, Santa Fe, New Mexico, USA. Associa-",
            "year": 2018
        },
        {
            "authors": [
                "Jan Niehues",
                "Stephan Vogel."
            ],
            "title": "Discriminative word alignment via alignment matrix modeling",
            "venue": "Proceedings of the Third Workshop on Statistical Machine Translation, pages 18\u201325, Columbus, Ohio. Association for Computational Linguistics.",
            "year": 2008
        },
        {
            "authors": [
                "Sebastian Nordhoff",
                "Thomas Kr\u00e4mer."
            ],
            "title": "IMTVault: Extracting and enriching low-resource language interlinear glossed text from grammatical descriptions and typological survey articles",
            "venue": "Proceedings of the 8th Workshop on Linked Data in Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Alexis Palmer",
                "Taesun Moon",
                "Jason Baldridge."
            ],
            "title": "Evaluating automation strategies in language documentation",
            "venue": "Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing, pages 36\u201344, Boulder, Colorado.",
            "year": 2009
        },
        {
            "authors": [
                "Fuchun Peng",
                "Fangfang Feng",
                "Andrew McCallum."
            ],
            "title": "Chinese segmentation and new word detection using conditional random fields",
            "venue": "COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, pages 562\u2013568, Geneva,",
            "year": 2004
        },
        {
            "authors": [
                "Slav Petrov",
                "Dan Klein."
            ],
            "title": "Discriminative loglinear grammars with latent variables",
            "venue": "Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc.",
            "year": 2007
        },
        {
            "authors": [
                "Martin Riedmiller",
                "Heinrich Braun."
            ],
            "title": "A direct adaptive method for faster backpropagation learning: the RPROP algorithm",
            "venue": "IEEE International Conference on Neural Networks, volume 1, pages 586\u2013591.",
            "year": 1993
        },
        {
            "authors": [
                "Tanja Samard\u017ei\u0107",
                "Robert Schikowski",
                "Sabine Stoll."
            ],
            "title": "Automatic interlinear glossing as two-level sequence classification",
            "venue": "Proceedings of the 9th SIGHUM Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",
            "year": 2015
        },
        {
            "authors": [
                "Christoph Schmidt",
                "Oscar Koller",
                "Hermann Ney",
                "Thomas Hoyoux",
                "Justus Piater."
            ],
            "title": "Using viseme recognition to improve a sign language translation system",
            "venue": "Proceedings of the 10th International Workshop on Spoken Language Translation:",
            "year": 2013
        },
        {
            "authors": [
                "Charles Sutton",
                "Andrew McCallum."
            ],
            "title": "An introduction to conditional random fields for relational learning",
            "venue": "Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning, chapter 4, pages 93\u2013127. MIT Press.",
            "year": 2007
        },
        {
            "authors": [
                "Oscar T\u00e4ckstr\u00f6m",
                "Dipanjan Das",
                "Slav Petrov",
                "Ryan McDonald",
                "Joakim Nivre."
            ],
            "title": "Token and type constraints for cross-lingual part-of-speech tagging",
            "venue": "Transactions of the Association for Computational Linguistics, 1:1\u201312.",
            "year": 2013
        },
        {
            "authors": [
                "Tim Vieira",
                "Ryan Cotterell",
                "Jason Eisner."
            ],
            "title": "Speed-accuracy tradeoffs in tagging with variableorder CRFs and structured sparsity",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1973\u20131978, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Xinyi Wang",
                "Sebastian Ruder",
                "Graham Neubig."
            ],
            "title": "Expanding pretrained models to thousands more languages via lexicon-based adaptation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2022
        },
        {
            "authors": [
                "Fei Xia",
                "William Lewis."
            ],
            "title": "Multilingual structural projection across interlinear text",
            "venue": "Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main",
            "year": 2007
        },
        {
            "authors": [
                "Fei Xia",
                "William Lewis",
                "Michael Wayne Goodman",
                "Joshua Crowgey",
                "Emily M. Bender."
            ],
            "title": "Enriching ODIN",
            "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914), pages 3151\u20133157, Reykjavik,",
            "year": 2014
        },
        {
            "authors": [
                "Xingyuan Zhao",
                "Satoru Ozaki",
                "Antonios Anastasopoulos",
                "Graham Neubig",
                "Lori Levin."
            ],
            "title": "Automatic interlinear glossing for under-resourced languages leveraging translations",
            "venue": "Proceedings of the 28th International Conference on Computational",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Interlinear Morphological Gloss (IMG) (Lehmann, 2004; Bickel et al., 2008) is an annotation layer aimed to explicit the meaning and function of each morpheme in some documentation (\u2018object\u2019) language L1, using a (meta)-language L2. In computational language documentation scenarios, L1 is typically a low-resource language under study, and L2 is a well-resourced language such as English.\nFigure 1 displays an example IMG: the source sentence t in L1 is overtly segmented into a sequence of morphemes (x), each of which is in one-to-one correspondence the corresponding gloss sequence y. Each unit in the gloss tier is either a grammatical description (OBL for the oblique marker in Figure 1) or a semantic tag (son in Figure 1), expressed by a lexeme in L2. An idiomatic free translation z in L2 is usually also provided. y and z help linguists unfamiliar with L1 to understand the morphological analysis in x.\nIn this paper, we study the task of automatically computing the gloss tier, assuming that the morphological analysis x and the free L2 translation z are available. As each morpheme has exactly one associated gloss,1 an obvious formalisation of the task that we mostly adopt views glossing as a sequence labelling task performed at the morpheme level. Yet, while grammatical glosses effectively constitute a finite set of labels, the diversity of lexical glosses is unbounded, meaning that our tagging model must accommodate an open vocabulary of labels. This issue proves to be the main challenge of this task, especially in small training data regimes.\nTo handle such cases, we assume that lexical glosses can be directly inferred from the translation tier, an assumption we share with (McMillanMajor, 2020; Zhao et al., 2020). In our model, we thus consider that the set of possible morpheme labels in any given sentence is the union of (i) all grammatical glosses, (ii) lemmas occurring in the target translation, and (iii) frequently-associated labels from the training data. This makes our model a hybrid between sequence tagging (because of (i) and (iii)) and unsupervised sequence alignment (because of (ii)), as illustrated in Figure 2. Our implementation relies on a variant of Conditional Random Fields (CRFs) (Lafferty et al., 2001; Sut-\n1The reality is slightly more complex, as illustrated by \u2018compound\u2019 glosses such as \u2018he.OBL\u2019 in Figure 1, associating two descriptions to the same morpheme. In this work, such compound labels are processed just as any other purely lexical gloss during feature extraction, training and inference. This allows us to only distinguish two types of labels.\nton and McCallum, 2007), which handles latent variables and offers the ability to locally restrict the set of possible labels. The choice of a CRF-based approach is motivated by its notable data-efficiency, while methods based on neural networks have difficulties handling very low resource settings\u2014this is again confirmed by results in \u00a75.1.\nIn this work, we generalise previous attempts to tackle this task with sequence tagging systems based on CRFs such as (Moeller and Hulden, 2018; McMillan-Major, 2020; Barriga Mart\u00ednez et al., 2021) and makes the following contributions: (a) we introduce (\u00a72) a principled and effective endto-end solution to the open vocabulary problem; (b) we design, implement and evaluate several variants of this solution (\u00a73), which obtain results that match that of the best-performing systems in the 2023 Shared Task on automatic glossing (\u00a75.1); (c) in experiments with several low-resource languages (\u00a74), we evaluate the benefits of an additional multilingual pre-training step, leveraging features that are useful cross-linguistically (\u00a75.5). Owing to the transparency of CRF features, we also provide an analysis of the most useful features (\u00a75.6) and discuss prospects for improving these techniques."
        },
        {
            "heading": "2 A hybrid tagging / alignment model",
            "text": ""
        },
        {
            "heading": "2.1 The tagging component",
            "text": "The core of our approach is a CRF model, the main properties of which are defined below. Assuming for now that the set of possible glosses is a closed set Y , our approach defines the conditional probability of a sequence y of T labels in Y given a sequence x of T morphemes as:\np\u03b8(y|x) = 1\nZ\u03b8(x) exp { K\u2211 k=1 \u03b8kGk(x,y) } , (1)\nwhere {Gk, k = 1 . . .K} are the feature functions with associated weights \u03b8 = [\u03b81 . . . \u03b8K ]T \u2208 RK , and Z\u03b8(x) is the partition function summing over all label sequences. For tractability, in linearchain CRFs, the feature function Gk only test local properties, meaning that each Gk decomposes as Gk(x,y) = \u2211 t gk(yt, yt\u22121, t,x) with gk() a local feature. Training is performed by maximising the regularised conditional log-likelihood on a set of fully labelled instances, where the regulariser is proportional to the \u21131 (|\u03b8|) or \u21132 (||\u03b8||2) norm of the parameter vector. Exact decoding of the most likely label sequence is achieved with Dynamic Programming (DP); furthermore, an adaptation of the forward-backward algorithm computes the posterior distribution of any yt conditioned on x.\nUsing CRFs for sequence labelling tasks has long been the best option in the pre-neural era, owing to (i) fast and data-efficient training procedures, even for medium-size label sets (e.g. hundreds of labels (Schmidt et al., 2013)) and higher-order label dependencies (Vieira et al., 2016), (ii) the ability to handle extremely large sets of interdependent features (Lavergne et al., 2010). They can also be used in combination with dense features computed by deep neural networks (Lample et al., 2016)."
        },
        {
            "heading": "2.2 Augmenting labels with translations",
            "text": "One of the challenges of automatic glossing is the need to introduce new lexical glosses in the course of the annotation process. This requires extending the basic CRF approach and incorporating a growing repertoire of lexical labels. We make the assumption [H] that these new labels can be extracted from the L2 translation (z in Figure 1).\nInformally, this means that the grammatical label set YG now needs to be augmented with L2 words in z or equivalently, with indices in [1 . . . |z|]. This raises two related questions: (a) how to exactly specify the set of labels Y in inference and training. (b) depending on answers to question (a), how to learn the model parameters?\nIn our model, we additionally consider an extra source of possible lexical labels, YL(x), which contains likely glosses for morphemes in x. There are several options to design YL(x): for instance, to include all the lexical glosses seen in training or to restrict to one or several glosses for each word\nxt. In our experiments (\u00a75), we select for each morpheme in x the most frequently associated gloss in the training corpus. Y thus decomposes into a global part YG and a sentence-dependent part YL(x) \u222a [1 . . . |z|]. Performing inference with this model yields values yt that either directly correspond to the desired gloss or correspond to an integer, in which case the (lexical) gloss at position t is zyt . Formally, the gloss labels are thus obtained as y\u0303t = \u03d5(yt), with \u03d5() the deterministic decoding function defined as \u2200y \u2208 YG \u222a YL(x) : \u03d5(y) = y and \u2200i \u2208 [1 . . . |z|] : \u03d5(i) = zi.\nTraining this hybrid model is more difficult than for regular CRFs, for lack of directly observing yt. We observe instead y\u0303t = \u03d5(yt): while the correspondence is non-ambiguous for grammatical glosses, there is an ambiguity when y\u0303t is present in both YL(x) and z, or when it occurs multiple times in z. We thus introduce a new, partially observed, variable ot which indicates the origin of gloss y\u0303t: ot = 0 when y\u0303t \u2208 YG \u222a YL(x) and ot > 0 when y\u0303t comes from L2 word zot . The full model is:\np\u03b8(y,o|x, z) = 1\nZ\u03b8(x, z) exp\n{ \u03b8TG(x,y, z,o) } .\nBy making the origin of the lexical label(s) explicit, we distinguish in \u00a73.3 between feature functions associated with word occurrences in z and those for word types in YL(x) (T\u00e4ckstr\u00f6m et al., 2013).\nLearning \u03b8 with (partially observed) variables is possible in CRFs and yields a non-convex optimisation problem (see e.g. (Blunsom et al., 2008)). In this case, the gradient of the objective is a difference of two expectations (Dyer et al., 2011, eq. (1)) and can be computed with forward-backward recursions. We, however, pursued another approach, which relies on an automatic word alignment a between lexical glosses and translation (\u00a73.1) to provide proxy information for o. Assuming at = 0 for grammatical glosses and unaligned lexical glosses and at > 0 otherwise, we can readily derive the supervision information ot needed in training, according to the heuristics detailed in Table 1, which depend on the values of yt and at.\nThree heuristic supervision schemes are in Table 1, which vary on how ambiguous label sources are handled: (S1) only considers dictionary entries, which makes the processing of unknown words impossible; (S2) only considers translations, possibly disregarding correct supervision from the dictio-\n2I.e., the correct label is always part of the search space.\nnary; (S3) assumes that the label originates from the translation only if an exact match is found."
        },
        {
            "heading": "3 Implementation choices",
            "text": ""
        },
        {
            "heading": "3.1 Aligning lexical glosses with target words",
            "text": "To align the lexical glosses with the L2 translation, we use SimAlign (Jalili Sabet et al., 2020), an unsupervised, multilingual word aligner which primarily computes source / target alignment links based on the similarity of the corresponding embeddings in some multilingual space. Note that our task is much simpler than word alignment in bitexts, as the lexical gloss and the translation are often in the same language (e.g. English or Spanish), meaning that similarities can be computed in a monolingual embedding space. We extract alignments from the similarity matrix with Match heuristic, as it gave the best results in preliminary experiments. Match views alignment as a maximal matching problem in the weighted bipartite graph containing all possible links between lexical glosses and L2 words. This ensures that all lexical morphemes are aligned with exactly one L2 word.3\nFigure 3 displays an alignment computed with the Match method. Most alignments are trivial and associate identical units (e.g. one/\u2018one\u2019) or morphologically related words (e.g. son/\u2018sons\u2019). Non-trivial (correct) links comprise (khan/\u2018kings\u2019),\n3Assuming there are fewer lexical glosses than L2 words.\nwhich is the best option as \u2018khan\u2019 does not occur in the translation. A less positive case is the (erroneous) alignment of be with \u2018for\u2019, which only exists because of the constraint of aligning every lexical gloss. Nevertheless, frequent lemmas such as \u2018be\u2019 will occur in multiple sentences, and their correct labels are often observed in other training sentences. We analyse these alignments in \u00a75.4."
        },
        {
            "heading": "3.2 Implementing the hybrid CRF model",
            "text": "Following e.g. (Dyer et al., 2011; Lavergne et al., 2011), our implementation of the CRF model4 heavily relies on weighted finite-state models and operations (Allauzen et al., 2007), which we use to represent the spaces of all possible and reference labellings on a per sentence basis, and to efficiently compute the expectations involved in the gradient, as well as to search for the optimal labellings and compute alignment and label posteriors.\nTraining is performed by optimising the penalised conditional log-likelihood with a variant of gradient descent (Rprop, (Riedmiller and Braun, 1993)), with \u21131 regularisation to perform feature selection, associated with parameter value 0.5 that was set during preliminary experiments and kept fixed for producing all the results below.\nGiven the size of our training data (\u00a74.1) and typical sentence lengths, training and decoding are very fast, even with hundreds of labels and millions of features. A full experiment for Lezgi takes about 20 minutes on a desktop machine; processing the larger Tsez dataset takes about 10 hours."
        },
        {
            "heading": "3.3 Observation and label features",
            "text": "Our implementation can handle a very large set of sparse feature functions gk(), testing arbitrary properties of the input L1 sequence in conjunction with either isolated labels (unigram features) or pairs of labels (bigram features). Regarding L1, we distinguish between orthographic features, which test various properties of the morpheme string (its content, prefix, suffix, CV structure and length), and positional features, which give information about the position of the morpheme in a word; all these features can also test the properties of the surrounding morphemes (within the same word or in its neighbours). Note that a number of these features abstract away the orthographic content, a property we exploit in our multilingual model.\n4Our code is available at: https://github.com/ shuokabe/gloss_lost.\nOn the label side, feature functions test the gloss value y and type b (GRAM or LEX); for labels aligned with an L2 word, we additionally collect its PoS p5 and its position l in z, which acts as a distortion feature.6 Such label features enable us to generalise alignment patterns for unknown L1 morphemes. More about features in Appendix A."
        },
        {
            "heading": "4 Experimental conditions",
            "text": ""
        },
        {
            "heading": "4.1 L1 languages",
            "text": "We consider five (out of seven) languages from the SIGMORPHON 2023 Shared Task on Interlinear Glossing (Ginn et al., 2023): Tsez (ddo), Gitksan (git), Lezgi (lez), Natugu (ntu; surprise language), and Uspanteko (usp; target translation in Spanish).7 Table 2 gives general statistics about the associated datasets; a brief presentation of these languages is in Appendix B and in (Ginn et al., 2023)."
        },
        {
            "heading": "4.2 Pre-processing L2 translations",
            "text": "L2 translations are lemmatised and PoS tagged with spaCy,8 using the en_core_web_sm and es_core_news_sm pipelines for English and Spanish respectively. All lemmas in the translation are lowercased except for proper nouns."
        },
        {
            "heading": "4.3 Multilingual corpus",
            "text": "We also explore multilingual pre-training by leveraging a recent IGT corpus, IMTVault (Nordhoff and Kr\u00e4mer, 2022), which contains IGT examples from various publications in Language Science Press. In an attempt to capture cross-linguistic patterns, we only keep sentences with a well-defined language code with at least 30 sentences, since the languages we study range from 31 to thousands of sentences, leaving us 173 languages. The dataset\n5For labels in YL(x), we associate the most frequently found PoS tag in the training dataset via alignment. As grammatical morphemes have no aligned target words, we use the generic label GRAM for all grammatical glosses.\n6Non-aligned units (i.e. glosses from YL(x) and YG) have dedicated positions (\u22121 and \u22122, respectively).\n7We did not run our models on the Nyangbo (nyb) dataset, which does not include a translation tier.\n8https://spacy.io/.\nis shuffled and then split into 30K training, 2K development, and 2K test datasets."
        },
        {
            "heading": "4.4 SimAlign settings",
            "text": "Since the glosses and the translation are in the same language, we use the embeddings from the English BERT (bert-base-uncased) (Devlin et al., 2019) when the L2 language is English and mBERT (\u2018bert-base-multilingual-uncased\u2019) for Spanish (for Uspanteko). We stress here that our model is compatible with several target languages, SimAlign being an off-the-shelf multilingual (neural) aligner.\nOur preliminary experiments showed that the embeddings from the 0-th layer yielded the best alignments, especially compared to the 8-th layer, which seems to work best in most alignment tasks. A plausible explanation is that contextualised embeddings are unnecessary here because lexical glosses do not constitute a standard English sentence (for instance, they do not contain stop words, and their word order reflects the L1 word order)."
        },
        {
            "heading": "4.5 Evaluation metrics",
            "text": "We use the official evaluation metrics from the Shared Task: morpheme accuracy, word accuracy, BLEU, and precision, recall, and F1-score computed separately for grammatical (Gram) and lexical (Lex) glosses. We report the results of our best system with all metrics in Appendix."
        },
        {
            "heading": "4.6 Baselines",
            "text": "Below, we consider three baseline approaches which handle glossing as a sequence-labelling task:\n\u2022 maj: a dictionary-based approach, which assigns the majority label (grammatical and lexical) seen in the training dataset to a source morpheme and fails for out-of-vocabulary morphemes; \u2022 CRF+maj: a hybrid model relying on a CRF to predict grammatical glosses and a unified lexical label (as in (Moeller and Hulden, 2018; Barriga Mart\u00ednez et al., 2021)). Known lexical morphemes are then assigned a lexical label according to the maj model; \u2022 BASE_ST: is the Transformer-based baseline developed by the SIGMORPHON Shared Task organisers and detailed in Ginn (2023).9\n9https://github.com/sigmorphon/2023glossingST/ tree/main/baseline."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Results",
            "text": "Table 3 reports the scores of the baselines from \u00a74.6, as well as the best results in the Shared Task on Automatic Glossing (BEST_ST)10 and the results of variants of our system on the official testsets. We only report below the word- and morpheme-level (overall) accuracy, which are the two official metrics of the Shared Task.11\nA first observation is that system (S3), which effectively combines the information available in a dictionary and obtained via alignment in an integrated fashion (see \u00a72.2) greatly outperforms (S1) (only dictionary) and (S2) (only alignment), obtaining the best performance among our three variants. (S3) is also consistently better than all the baselines, with larger gaps when few training sentences are available (e.g. Gitksan or Lezgi). In comparison, the BERT-based baseline suffers a large performance drop in very low-data settings, as also reported in (Ginn, 2023). Our CRF model also achieves competitive results compared to the best system submitted to the Shared Task, especially for the word-level scores. These scores confirm\n10https://github.com/sigmorphon/2023glossingST/ blob/main/results.md.\n11Full results of CRF (S3) in Appendix D. 12CRF+maj was not computed on Tsez because the number of features is too prohibitive. For our systems, CRF (S1) and CRF (S2) are the lower bounds of our results.\nthat decent to good accuracy numbers can be obtained based on some hundreds of training sentences. Note, however, that annotated datasets of that size are not so easily found: in the IMTVault (Nordhoff and Kr\u00e4mer, 2022), only 16 languages have more than 700 sentences, which is about the size of the Lezgi and the Natugu corpora."
        },
        {
            "heading": "5.2 Handling unknown morphemes",
            "text": "Leveraging the translation in glossing opens the way to better handle morphemes that were unseen in training. Table 4 displays some statistics about unknown morphemes in test datasets. For most languages, they are quite rare, representing solely around or below 10% of all lexical glosses in the test set, with Gitksan a notable outlier (Ginn et al., 2023). Among those unseen morphemes, a significant proportion (from a third in Tsez up to 70% in Gitksan) of the reference lexical gloss is not even present in the translation13 (cf. \u2018not in L2\u2019 in Table 4). Taking this into account nuances the seemingly low accuracy for unknown lexical morphemes: in Uspanteko, for instance, the system reaches an accuracy of 29.3 when the best achievable score is about 35. To have a more optimistic view of our prediction, we \u2018approximate\u2019 lexical glosses with lemmas from the translation, using automatic alignments (e.g., king instead of khan in Figure 3). By evaluating the unknown morphemes with their reachable labels14 (cf. \u2018align. accuracy\u2019 line), we get higher scores, such as 40.4 in Natugu."
        },
        {
            "heading": "5.3 Data efficiency",
            "text": "An important property of our approach seems to be its data efficiency. To better document this property, we report in Table 5 the morpheme-level accuracy obtained with increasingly large training data of\n13We studied the exact match with any lemmas in the translation; composed glosses are hence always considered as absent.\n14Whenever possible; if the lexical gloss has no automatic alignment, we keep the reference gloss.\nsize (50, 200, 700, 1,000, 2,000, full) in Tsez. With 200 examples already, our model does much better than the simple baseline maj and delivers usable outputs. (S3) also has a faster improvement rate than the baseline for small training datasets (e.g. almost +7 points between 200 and 700 sentences), while maj increases by only +4 points. The return of increasing the dataset size above 1,000 is, in comparison, much smaller. While the exact numbers are likely to vary depending on the language and the linguistic variety of the material collected on the field, they suggest that ML techniques could be used from the onset of the annotation process."
        },
        {
            "heading": "5.4 Analysis of automatic alignments",
            "text": "Our approach relies on automatic alignment computed with SimAlign to supervise the learning process. When using the Match method, (almost) all lexical glosses are aligned with a word in the translation (cf. footnote 3). We cannot evaluate the overall alignment quality, as reference alignments are unobserved. However, we measure in Table 6 the proportion of exact matches between the reference gloss and the lemma of the aligned word.\nOverall, around half of our alignments are trivial and hence sure, which is more or less in line with the proportions found by Georgi (2016), albeit lower due to the marked linguistic difference between L1 and L2 in our case. These seemingly low scores have to be nuanced by two facts. First, they are a lower bound to evaluate alignment quality since synonyms (such as khan/king) are counted as wrong alignments. In some cases, inflected forms are also used as a gloss (e.g., dijo/decir in Uspanteko). Second, the alignment-induced glosses are not used as is in our experiments: they supplement\nthe output label with their PoS tag and their position in the L2 sentence. This means that even non-exact matches can yield useful features.\nRemoving L2 stop words We carried out a complementary experiment where we filtered stop words in the L2 translation in Gitksan to remove a potential source of error in the alignments. The number of unaligned lexical glosses (\u00a73.1) thus increases, which generally means a reduced noise in the alignment for the Match method. Yet, using these better alignments and reduced label sets in training and inference yields mixed results: +1 point in word accuracy, \u22122 points in morpheme accuracy."
        },
        {
            "heading": "5.5 Multilingual pre-training",
            "text": "Cross-lingual transfer techniques via multilingual pre-training (Conneau et al., 2020) are the cornerstone of multilingual NLP and have been used in multiple contexts and tasks, including morphological analysis for low-resource languages (Anastasopoulos and Neubig, 2019). In this section, we apply the same idea to evaluate how well such techniques can help in our context. We train the model to predict the nature of the gloss (grammatical or lexical) with multilingual features (see Appendix A): for a given morpheme, its position in the word, its length in characters, its CV skeleton,15 and the number of morphemes in the word. Using IMTVault (\u00a74.3) for this task, the model reaches around 80 of accuracy.\nWe use these pre-trained weights to initialise the multilingual features in our (S3) system. To help feature selection, we notably reduce the value of \u21131 to 0.4. Pre-training results (+ IMT) are in Table 7. We note that pre-training has a negligible effect, except in some metrics, such as in Lezgi for the word level. We observed that the most important weights in the pre-trained model correspond\n15We identify consonants and vowels based on orthography.\nto delexicalised pattern features that are relevant in IMTVault but not present at all in our datasets.\nCross-lingual study Besides, since in our studied languages, Tsez and Lezgi belong to the same language family, we reiterate this methodology but with the Tsez dataset as a pre-training source to predict Lezgi glosses. This kinship is also explored through successful cross-lingual transfer in (Zhao et al., 2020). We obtain 83.3 and 87.1 for word and morpheme accuracy, which is very close to the performance with (and without) IMTVault despite containing fewer sentences.\nVery low resource scenarios A final experiment focuses on a very low training data scenario. Save the Gitksan corpus, the test languages already represent hundreds of annotated sentences. This contrasts with actual data conditions: in IMTVault, for instance, only 16 languages have equivalent or more sentences than Lezgi (the second-lowest language in terms of training sentences in our study; cf. \u00a75.1). We thus focus here on (simulated) very lowresource data settings by considering only 50 sentences16 selected from the training data.\nTable 8 reports the results obtained with this configuration. We observe that using the multilingual pre-training helps for all languages to a more noticeable extent than before. These three experiments confirm the potential of multilingual transfer for this task, which can help improve performance in very low-resource scenarios. Contrarily, when hundreds of sentences are available, pretraining delexicalised features proves ineffective, sometimes even detrimental."
        },
        {
            "heading": "5.6 Feature analysis",
            "text": "One positive point in using models based on CRFs relies on access to features and their correspond-\n16Filtering IMTVault with this threshold would lead to 124 languages kept.\ning learnt weights. We report in Appendix E ten features with the largest weight in Natugu.\nAmong the top 1% of active features in Natugu in terms of weight, we find two features testing the gloss type b with the morpheme length: (LEX, 6+) and (LEX, 5). These indicate that longer morphemes are likely to have a lexical label. Such an analysis can also be relevant to weights learnt through multilingual pre-training. For instance, (LEX, 5) is among the top 10% features on the IMTVault dataset, suggesting a cross-lingual tendency of longer morphemes being lexical."
        },
        {
            "heading": "6 Related work",
            "text": "Language documentation With the everpressing need to collect and annotate linguistic resources for endangered languages, the field of computational language documentation is quickly developing, as acknowledged by the ComputEL workshop.17 Regarding annotation tools, recent research has focused on all the steps of language documentation, from speech segmentation and transcription to automatic word and morpheme splitting to automatic interlinear glossing.\n(Xia and Lewis, 2007; Georgi et al., 2012, 2013) use, as we do, interlinear glosses to align morphs and translations, then to project syntactic parses from L2 back to L1, a technique pioneered by Hwa et al. (2005), or to extract grammatical patterns (Bender et al., 2014). Through these studies, a large multilingual database of IMGs was collected from linguistic papers, curated, and enriched with additional annotation layers (Lewis and Xia, 2010; Xia et al., 2014) for more than 1,400 languages. (Georgi, 2016) notably discusses alignment strategies in ODIN and trains a multilingual alignment model between the gloss and translation layers - in our work, we extend multilingual training to the full annotation process. Another massive multilingual source of glosses is IMTVault, described in (Nordhoff and Kr\u00e4mer, 2022), studied in \u00a75.5.\nAutomatic glossing Automatic glossing was first studied in (Palmer et al., 2009; Baldridge and Palmer, 2009), where active learning was used to incrementally update an underlying tagging system focusing mainly on grammatical morphemes (lexical items are tagged with their PoS). (Samard\u017eic\u0301 et al., 2015) added to this an extra layer aimed to annotate the missing lexical tags, yielding a system that resembles our CRF+maj baseline.\n17https://computel-workshop.org/.\nBoth (Moeller and Hulden, 2018) and (McMillan-Major, 2020) rely on CRFs, the latter study being closer to our approach as it tries to combine post-hoc the output of two CRF models operating respectively on the L1 and L2 tier, where our system introduces an integrated end-to-end architecture. Zhao et al. (2020) develop an architecture inspired by multi-source neural translation models, where one source is the L1 sequence, and the other the L2 translation. They experiment with Arapaho, Lezgi, and Tsez, while also applying some sort of cross-lingual transfer learning. The recent SIGMORPHON exercise (Ginn et al., 2023) is a first attempt to standardise benchmarks and task settings and shows that morpheme-level accuracy in the high 80s can be obtained for most languages considered.\nLatent variable CRFs models Extended CRF models were proposed and used in many studies, including latent variables to represent, e.g. hidden segmentation as in (Peng et al., 2004) or hidden syntactic labels (Petrov and Klein, 2007). Closer to our work, (Blunsom et al., 2008; Lavergne et al., 2011) use latent structures to train discriminative statistical machine translation systems. Other relevant work on unsupervised discriminative alignment are in (Berg-Kirkpatrick et al., 2010; Dyer et al., 2011), while Niehues and Vogel (2008) use a supervised symmetric bi-dimensional word alignment model."
        },
        {
            "heading": "7 Conclusion",
            "text": "This paper presented a hybrid CRF model for the automatic interlinear glossing task, which was specifically designed and tailored to work well in low data conditions and to effectively address issues due to out-of-vocabulary morphemes. We presented our main approach, which relies on analysing the translation tier, and discussed our main implementation choices. In experiments with five low-resource languages, we obtained accuracy scores that match or even outperform those of very strong baselines, confirming that accuracy values in the 80s or above could be obtained with a few hundred training examples. Using a large multilingual gloss database, we finally studied the possibility of performing cross-lingual transfer for this task.\nThere are various ways to continue this work and improve these results, such as removing the noise introduced via erroneous alignments links\u2014 either by marginalising over the \u2018origin\u2019 variable, by filtering unlikely alignments based on link poste-\nrior values, or by also trying to generate alignment links for function words (Georgi, 2016; McMillanMajor, 2020). Our initial experiments along these lines suggest that this may not be the most promising direction. We may also introduce powerful neural representations for L1 languages; while these were usually available for a restricted number of languages, recent works have shown that even lowresource languages could benefit from these techniques (Wang et al., 2022; Adebara et al., 2023).\nLimitations\nThe main limitation comes from the small set of languages (and corresponding language family) studied in this work. In general, texts annotated with Interlinear Morphological Gloss are scarcely available due to the time and expertise needed to annotate sentences with glosses. However, corpora such as IMTVault (Nordhoff and Kr\u00e4mer, 2022) or ODIN (Lewis and Xia, 2010) or languages such as Arapaho (39,501 training sentences in the Shared Task) pave the way for further experiments.\nMoreover, another shortcoming of our work stems from the fact that we do not use neural models in our work, while, for instance, the best submission to the Shared Task relies on such architectures. In this sense, we have yet to compare the scalability of our performance to larger data. Still, one of our main focuses was to tackle glossing in very low-resource situations for early steps of language documentation and to study how to handle previously unseen morphemes at the inference step."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the anonymous reviewers and metareviewer for their comments and suggestions. This work was partly funded by French ANR and German DFG under grant ANR-19-CE38-0015 (CLD 2025). The authors warmly thank Thomas Lavergne for his help and assistance regarding the configuration and exploitation of Lost."
        },
        {
            "heading": "A Model features",
            "text": "The input side x is the L1 morpheme sequence; from each xt, from which we also deduce the following features: its position t within the word coded as a numerical value (from 0 to n) for complex words, or as \u2018F\u2019 for free morphemes, its length l in characters, its 3-char prefix and suffix (d and e respectively), the number of morphemes in the word, and its CV skeleton dl (Consonant and Vowels) based on the orthography of the morpheme. Figure 4 displays an example of input and the associated features, while Table 9 illustrates the output label format according to the origin of the gloss.\nWith all these inputs to predict the output labels, we compute unigram and bigram feature functions, detailed in Table 13.\nBesides, we add features that test properties between the source and translation: a copy feature to\nhandle glosses that literally appear in the L2 sentence (namely, proper nouns) and a distortion feature which tests the difference in relative positions between the source morpheme and the (possible) lexical label, whenever at > 0."
        },
        {
            "heading": "B Brief language presentation",
            "text": "We describe below the studied languages.\n\u2022 Tsez (ddo) is a Nakh-Daghestanian language spoken in the Republic of Dagestan in Russia.\n\u2022 Gitksan (git) is a Tsimshian language spoken on the western coast of Canada (British Columbia).\n\u2022 Lezgi (lez) is a Nakh-Daghestanian language spoken in the Republic of Dagestan in Russia and in Azerbaijan.\n\u2022 Natugu (ntu) is an Austronesian language spoken in the Solomon Islands.\n\u2022 Uspanteko (usp) is a Mayan language spoken in Guatemala.\nAccording to Ethnologue (Eberhard et al., 2023), the two Nakh-Daghestanian languages have between 10K to 1M speakers, while the other three have fewer than 10K users."
        },
        {
            "heading": "C Number of active features",
            "text": "Table 10 presents the number of active features (in thousands) selected among all features (in mil-\nlions) for S3. We note here that thanks to the l1regularisation term, most feature weights are set to 0 and less than 1% of the features are actually retained."
        },
        {
            "heading": "D Full results",
            "text": "Table 11 displays the results of the S3 system with all metrics presented in \u00a74.5. Two scores of accuracy are computed for both morpheme and word levels: an overall (Ovr) value and a sentenceaveraged value (Avg)."
        },
        {
            "heading": "E Example of learnt features",
            "text": "Table 12 displays 10 features with the largest weight in the S3 system in Natugu. Here, we ignore trivial features for numbers or punctuation signs that also have high weight values. The ID of the feature refers to Table 13."
        }
    ],
    "title": "Towards Multilingual Interlinear Morphological Glossing",
    "year": 2023
}