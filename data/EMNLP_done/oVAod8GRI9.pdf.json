{
    "abstractText": "We are interested in image manipulation via natural language text \u2013 a task that is useful for multiple AI applications but requires complex reasoning over multi-modal spaces. We extend recently proposed Neuro Symbolic Concept Learning (NSCL) (Mao et al., 2019), which has been quite effective for the task of Visual Question Answering (VQA), for the task of image manipulation. Our system referred to as NEUROSIM can perform complex multi-hop reasoning over multi-object scenes and only requires weak supervision in the form of annotated data for VQA. NEUROSIM parses an instruction into a symbolic program, based on a Domain Specific Language (DSL) comprising of object attributes and manipulation operations, that guides its execution. We create a new dataset for the task, and extensive experiments demonstrate that NEUROSIM is highly competitive with or beats SOTA baselines that make use of supervised data for manipulation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Harman Singh"
        },
        {
            "affiliations": [],
            "name": "Poorva Garg"
        },
        {
            "affiliations": [],
            "name": "Mohit Gupta"
        },
        {
            "affiliations": [],
            "name": "Kevin Shah"
        },
        {
            "affiliations": [],
            "name": "Ashish Goswami"
        },
        {
            "affiliations": [],
            "name": "Satyam Modi"
        },
        {
            "affiliations": [],
            "name": "Arnab Kumar Mondal"
        },
        {
            "affiliations": [],
            "name": "Dinesh Khandelwal"
        },
        {
            "affiliations": [],
            "name": "Dinesh Garg"
        },
        {
            "affiliations": [],
            "name": "Parag Singla"
        }
    ],
    "id": "SP:e7a3d88e6cac3397e16bcd80bc39d7b622a7ce9b",
    "references": [
        {
            "authors": [
                "Jacob Andreas",
                "Marcus Rohrbach",
                "Trevor Darrell",
                "Dan Klein."
            ],
            "title": "Learning to compose neural networks for question answering",
            "venue": "Proceedings of NAACL-HLT, pages 1545\u20131554.",
            "year": 2016
        },
        {
            "authors": [
                "Tim Brooks",
                "Aleksander Holynski",
                "Alexei A. Efros."
            ],
            "title": "Instructpix2pix: Learning to follow image editing instructions",
            "venue": "Proceedings of CVPR, pages 18392\u201318402.",
            "year": 2023
        },
        {
            "authors": [
                "Shaowei Cai",
                "Kaile Su."
            ],
            "title": "Configuration checking with aspiration in local search for SAT",
            "venue": "Proceedings of AAAI, pages 434\u2013440.",
            "year": 2012
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "George Papandreou",
                "Iasonas Kokkinos",
                "Kevin Murphy",
                "Alan L Yuille."
            ],
            "title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",
            "venue": "IEEE transactions on pattern analysis",
            "year": 2017
        },
        {
            "authors": [
                "Lichang Chen",
                "Guosheng Lin",
                "Shijie Wang",
                "Qingyao Wu."
            ],
            "title": "Graph edit distance reward: Learning to edit scene graph",
            "venue": "Proceedings of ECCV, pages 539\u2013554.",
            "year": 2020
        },
        {
            "authors": [
                "Qifeng Chen",
                "Vladlen Koltun."
            ],
            "title": "Photographic image synthesis with cascaded refinement networks",
            "venue": "Proceedings of ICCV, pages 1520\u20131529.",
            "year": 2017
        },
        {
            "authors": [
                "Blender Online Community."
            ],
            "title": "Blender - a 3D modelling and rendering package",
            "venue": "Blender Foundation, Stichting Blender Foundation, Amsterdam.",
            "year": 2018
        },
        {
            "authors": [
                "Wang-Zhou Dai",
                "Qiu-Ling Xu",
                "Yang Yu",
                "Zhi-Hua Zhou."
            ],
            "title": "Bridging machine learning and logical reasoning by abductive learning",
            "venue": "Proceedings of NeurIPS, pages 2811\u20132822.",
            "year": 2019
        },
        {
            "authors": [
                "Brian Dolhansky",
                "Joanna Bitton",
                "Ben Pflaum",
                "Jikuo Lu",
                "Russ Howes",
                "Menglin Wang",
                "Cristian Canton Ferrer."
            ],
            "title": "The deepfake detection challenge (dfdc) dataset",
            "venue": "ArXiv preprint, abs/2006.07397.",
            "year": 2020
        },
        {
            "authors": [
                "Hao Dong",
                "Simiao Yu",
                "Chao Wu",
                "Yike Guo."
            ],
            "title": "Semantic image synthesis via adversarial learning",
            "venue": "Proceedings of ICCV, pages 5707\u20135715.",
            "year": 2017
        },
        {
            "authors": [
                "Honghua Dong",
                "Jiayuan Mao",
                "Tian Lin",
                "Chong Wang",
                "Lihong Li",
                "Denny Zhou."
            ],
            "title": "Neural logic machines",
            "venue": "Proceedings of ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Alaaeldin El-Nouby",
                "Shikhar Sharma",
                "Hannes Schulz",
                "R. Devon Hjelm",
                "Layla El Asri",
                "Samira Ebrahimi Kahou",
                "Yoshua Bengio",
                "Graham W. Taylor."
            ],
            "title": "Tell, draw, and repeat: Generating and modifying images based on continual linguistic instruction",
            "venue": "In",
            "year": 2019
        },
        {
            "authors": [
                "Kevin Ellis",
                "Lucas Morales",
                "Mathias Sabl\u00e9-Meyer",
                "Armando Solar-Lezama",
                "Josh Tenenbaum."
            ],
            "title": "Learning libraries of subroutines for neurally\u2013guided bayesian program induction",
            "venue": "Proceedings of NeurIPS.",
            "year": 2018
        },
        {
            "authors": [
                "Kevin Ellis",
                "Catherine Wong",
                "Maxwell Nye",
                "Mathias Sabl\u00e9-Meyer",
                "Lucas Morales",
                "Luke Hewitt",
                "Luc Cary",
                "Armando Solar-Lezama",
                "Joshua B Tenenbaum."
            ],
            "title": "Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Feng-Lei Fan",
                "Jinjun Xiong",
                "Mengzhou Li",
                "Ge Wang."
            ],
            "title": "On interpretability of artificial neural networks: A survey",
            "venue": "IEEE Transactions on Radiation and Plasma Medical Sciences, 5(6):741\u2013760.",
            "year": 2021
        },
        {
            "authors": [
                "Joseph L Fleiss",
                "Bruce Levin",
                "Myunghee Cho Paik."
            ],
            "title": "Statistical methods for rates and proportions",
            "venue": "john wiley & sons.",
            "year": 2013
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron C. Courville",
                "Yoshua Bengio."
            ],
            "title": "Generative adversarial nets",
            "venue": "Proceedings of NeurIPS, pages 2672\u20132680.",
            "year": 2014
        },
        {
            "authors": [
                "Chi Han",
                "Jiayuan Mao",
                "Chuang Gan",
                "Josh Tenenbaum",
                "Jiajun Wu."
            ],
            "title": "Visual concept-metaconcept learning",
            "venue": "Proceedings of NeurIPS, pages 5002\u2013 5013.",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of CVPR, pages 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter."
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Proceedings of NeurIPS, pages 6626\u20136637.",
            "year": 2017
        },
        {
            "authors": [
                "Ronghang Hu",
                "Jacob Andreas",
                "Marcus Rohrbach",
                "Trevor Darrell",
                "Kate Saenko."
            ],
            "title": "Learning to reason: End-to-end module networks for visual question answering",
            "venue": "Proceedings of ICCV.",
            "year": 2017
        },
        {
            "authors": [
                "Wentao Jiang",
                "Ning Xu",
                "Jiayun Wang",
                "Chen Gao",
                "Jing Shi",
                "Zhe Lin",
                "Si Liu."
            ],
            "title": "Language-guided global image editing via cross-modal cyclic mechanism",
            "venue": "Proceedings of ICCV, pages 2115\u20132124.",
            "year": 2021
        },
        {
            "authors": [
                "Justin Johnson",
                "Agrim Gupta",
                "Li Fei-Fei."
            ],
            "title": "Image generation from scene graphs",
            "venue": "Proceedings of CVPR, pages 1219\u20131228.",
            "year": 2018
        },
        {
            "authors": [
                "Justin Johnson",
                "Bharath Hariharan",
                "Laurens Van Der Maaten",
                "Judy Hoffman",
                "Li Fei-Fei",
                "C. Lawrence Zitnick",
                "Ross Girshick."
            ],
            "title": "Inferring and executing programs for visual reasoning",
            "venue": "Proceedings of ICCV, pages 3008\u20133017.",
            "year": 2017
        },
        {
            "authors": [
                "Justin Johnson",
                "Bharath Hariharan",
                "Laurens van der Maaten",
                "Li Fei-Fei",
                "C. Lawrence Zitnick",
                "Ross B. Girshick"
            ],
            "title": "CLEVR: A diagnostic dataset",
            "year": 2017
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "Proceedings of ICLR.",
            "year": 2015
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E. Hinton."
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Proceedings of NeurIPS, pages 1106\u20131114.",
            "year": 2012
        },
        {
            "authors": [
                "H.W. Kuhn."
            ],
            "title": "The hungarian method for the assignment problem",
            "venue": "Naval Research Logistics Quarterly, 2(1-2):83\u201397.",
            "year": 1955
        },
        {
            "authors": [
                "Bowen Li",
                "Xiaojuan Qi",
                "Thomas Lukasiewicz",
                "Philip H.S. Torr."
            ],
            "title": "Manigan: Text-guided image manipulation",
            "venue": "Proceedings of CVPR, pages 7877\u20137886. IEEE.",
            "year": 2020
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "Proceedings of ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Jiayuan Mao",
                "Chuang Gan",
                "Pushmeet Kohli",
                "Joshua B. Tenenbaum",
                "Jiajun Wu."
            ],
            "title": "The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",
            "venue": "Proceedings of ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Gary Marcus",
                "Ernest Davis",
                "Scott Aaronson."
            ],
            "title": "A very preliminary analysis of dall-e 2",
            "venue": "ArXiv preprint, abs/2204.13807.",
            "year": 2022
        },
        {
            "authors": [
                "Yisroel Mirsky",
                "Wenke Lee."
            ],
            "title": "The creation and detection of deepfakes: A survey",
            "venue": "ACM Computing Surveys (CSUR), 54(1):1\u201341.",
            "year": 2021
        },
        {
            "authors": [
                "Seonghyeon Nam",
                "Yunji Kim",
                "Seon Joo Kim."
            ],
            "title": "Text-adaptive generative adversarial networks: Manipulating images with natural language",
            "venue": "Proceedings of NeurIPS, pages 42\u201351.",
            "year": 2018
        },
        {
            "authors": [
                "Gaurav Parmar",
                "Richard Zhang",
                "Jun-Yan Zhu."
            ],
            "title": "On aliased resizing and surprising subtleties in gan evaluation",
            "venue": "Proceedings of CVPR, pages 11400\u2013 11410.",
            "year": 2022
        },
        {
            "authors": [
                "Duc Nghia Pham",
                "John Thornton",
                "Abdul Sattar."
            ],
            "title": "Building structure into local search for sat",
            "venue": "Proceedings of IJCAI, pages 2359\u20132364.",
            "year": 2007
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen."
            ],
            "title": "Hierarchical textconditional image generation with clip latents",
            "venue": "ArXiv preprint, abs/2204.06125.",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily Denton",
                "Seyed Kamyar Seyed Ghasemipour",
                "Burcu Karagol Ayan",
                "S Sara Mahdavi",
                "Rapha Gontijo Lopes"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language",
            "year": 2022
        },
        {
            "authors": [
                "Jing Shi",
                "Ning Xu",
                "Yihang Xu",
                "Trung Bui",
                "Franck Dernoncourt",
                "Chenliang Xu."
            ],
            "title": "Learning by planning: Language-guided global image editing",
            "venue": "Proceedings of CVPR, pages 13590\u201313599.",
            "year": 2021
        },
        {
            "authors": [
                "Cristian Vaccari",
                "Andrew Chadwick."
            ],
            "title": "Deepfakes and disinformation: Exploring the impact of synthetic political video on deception, uncertainty, and trust in news",
            "venue": "Social Media+ Society, 6(1).",
            "year": 2020
        },
        {
            "authors": [
                "Nam Vo",
                "Lu Jiang",
                "Chen Sun",
                "Kevin Murphy",
                "Li-Jia Li",
                "Li Fei-Fei",
                "James Hays."
            ],
            "title": "Composing text and image for image retrieval - an empirical odyssey",
            "venue": "Proceedings of CVPR, pages 6439\u20136448.",
            "year": 2019
        },
        {
            "authors": [
                "Ronald J Williams."
            ],
            "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
            "venue": "Machine learning, 8(3):229\u2013256.",
            "year": 1992
        },
        {
            "authors": [
                "Yonghui Wu",
                "Mike Schuster",
                "Zhifeng Chen",
                "Quoc V Le",
                "Mohammad Norouzi",
                "Wolfgang Macherey",
                "Maxim Krikun",
                "Yuan Cao",
                "Qin Gao",
                "Klaus Macherey"
            ],
            "title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine",
            "year": 2016
        },
        {
            "authors": [
                "Kexin Yi",
                "Jiajun Wu",
                "Chuang Gan",
                "Antonio Torralba",
                "Pushmeet Kohli",
                "Josh Tenenbaum."
            ],
            "title": "Neuralsymbolic VQA: disentangling reasoning from vision and language understanding",
            "venue": "Proceedings of NeurIPS, pages 1039\u20131050.",
            "year": 2018
        },
        {
            "authors": [
                "Dong Yu",
                "Li Deng."
            ],
            "title": "Automatic speech recognition, volume 1",
            "venue": "Springer.",
            "year": 2016
        },
        {
            "authors": [
                "Tianhao Zhang",
                "Hung-Yu Tseng",
                "Lu Jiang",
                "Weilong Yang",
                "Honglak Lee",
                "Irfan Essa."
            ],
            "title": "Text as neural operator: Image manipulation by text instruction",
            "venue": "Proceedings of ACM MM, pages 1893\u20131902.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The last decade has seen significant growth in the application of neural models to a variety of tasks including those in computer vision (Chen et al., 2017; Krizhevsky et al., 2012), NLP (Wu et al., 2016), robotics and speech (Yu and Deng, 2016). It has been observed that these models often lack interpretability (Fan et al., 2021), and may not always be well suited to handle complex reasoning tasks (Dai et al., 2019). On the other hand, classical AI systems can seamlessly perform complex reasoning in an interpretable manner due to their symbolic representation (Pham et al., 2007; Cai and Su, 2012). But these models often lack in their ability to handle low-level representations and be robust to noise. Neuro-Symbolic models (Dong\n\u2020 Equal Contribution, \u2217Equal Contribution. First four authors did this work while at IIT Delhi. Correspondence: harmansingh.iitd@gmail.com, parags@iitd.ac.in\net al., 2019; Mao et al., 2019; Han et al., 2019) overcome these limitations by combining the power of (purely) neural with (purely) symbolic representations. Studies (Andreas et al., 2016; Hu et al., 2017; Johnson et al., 2017a; Mao et al., 2019) have shown that neuro-symbolic models have several desirable properties such as modularity, interpretability, and improved generalizability.\nOur aim in this work is to build neuro-symbolic models for the task of weakly supervised manipulation of images comprising multiple objects, via complex multi-hop natural language instructions. Specifically, we are interested in weak supervision that only uses the data annotated for VQA tasks, avoiding the high cost of getting supervised annotations in the form of target manipulated images. Our key intuition here is that this task can be solved simply by querying the manipulated representation without ever explicitly looking at the target image. The prior work includes weakly supervised approaches (Nam et al., 2018; Li et al., 2020) that require textual descriptions of images during training and are limited to very simple scenes (or instructions). (See Section 2 for a survey).\nOur solution builds on Neuro-Symbolic Concept Learner (NSCL) proposed by (Mao et al., 2019) for solving VQA. We extend this work to incorporate the notion of manipulation operations such as change, add, and remove objects in a given image. As one of our main contributions, we design novel neural modules and a training strategy that just uses VQA annotations as weakly supervised data for the task of image manipulation. The neural modules are trained with the help of novel loss functions that measure the faithfulness of the manipulated scene and object representations by accessing a separate set of query networks, interchangeably referred to as quantization networks, trained just using VQA data. The manipulation takes place through interpretable programs created using primitive neural and symbolic operations from a Domain Specific\nLanguage (DSL). Separately, a network is trained to render the image from a scene graph representation using a combination of L1 and adversarial losses as done by (Johnson et al., 2018). The entire pipeline is trained without any intermediate supervision. We refer to our system as Neuro-Symbolic Image Manipulator (NEUROSIM). Figure 1 shows an example of I/O pair for our approach. Contributions of our work are as follows:\n1. We create NEUROSIM, the first neuro-symbolic, weakly supervised, and interpretable model for the task of text-guided image manipulation, that does not require output images for training. 2. We extend CLEVR (Johnson et al., 2017b), a benchmark dataset for VQA, to incorporate manipulation instructions and create a new dataset called as Complex Image Manipulation via Natural Language Instructions (CIM-NLI). We also create CIM-NLI-LARGE dataset to test zero-shot generalization. 3. We provide extensive quantitative experiments on newly created CIM-NLI, CIM-NLILARGE datasets along with qualitative experiments on Minecraft (Yi et al., 2018). Despite being weakly supervised, NEUROSIM is highly competitive to supervised SOTA approaches including a recently proposed diffusion based model (Brooks et al., 2023). NEUROSIM also performs well on instructions requiring multihop reasoning, all while being interpretable. We publicly release our code and data 1."
        },
        {
            "heading": "2 Related Work",
            "text": "Table 1 categorizes the related work across three broad dimensions - problem setting, task complexity, and approach. The problem setting comprises two sub-dimensions: i) supervision type - self, direct, or weak, ii) instruction format - text or UIbased. The task complexity comprises of following sub-dimensions: ii) scene complexity \u2013 single or multiple objects, ii) instruction complexity - zero or\n1https://github.com/dair-iitd/NeuroSIM\nmulti-hop instructions, iii) kinds of manipulations allowed - add, remove, or change. Finally, the approach consists of the following sub-dimensions: i) model \u2013 neural or neuro-symbolic and ii) whether a symbolic program is generated on the way or not. Dong et al. (2017), TAGAN (Nam et al., 2018), and ManiGAN (Li et al., 2020) are close to us in terms of the problem setting. These manipulate the source image using a GAN-based encoder-decoder architecture. Their weak supervision differs from ours \u2013 We need VQA annotation, they need captions or textual descriptions. The complexity of their natural language instructions is restricted to 0-hop. Most of their experimentation is limited to single (salient) object scenes.\nIn terms of task complexity, the closest to us are approaches such as TIM-GAN (Zhang et al., 2021), GeNeVA (El-Nouby et al., 2019), which build an encoder-decoder architecture and work with a latent representation of the image as well as the manipulation instruction. They require a large number of manipulated images as explicit annotations for training.\nIn terms of technique, the closest to our work are neuro-symbolic approaches for VQA such as NSVQA (Yi et al., 2018), NSCL (Mao et al., 2019), Neural Module Networks (Andreas et al., 2016) and its extensions (Hu et al., 2017; Johnson et al., 2017a). Clearly, while the modeling approach is similar and consists of constructing latent programs, the desired tasks are different in the two cases. Our work extends the NSCL approach for the task of automated image manipulation.\nJiang et al. (2021),Shi et al. (2021) deal with editing global features, such as brightness, contrast, etc., instead of object-level manipulations like in our case. Recent models such as InstructPix2Pix (Brooks et al., 2023), DALL-E (Ramesh et al., 2022) and Imagen (Saharia et al., 2022) on text-to-image generation using diffusion models are capable of editing images but require captions for input images; preliminary studies (Marcus et al., 2022) highlight their shortcomings in composi-\ntional reasoning and handling relations."
        },
        {
            "heading": "3 Neuro-Symbolic Image Manipulator",
            "text": ""
        },
        {
            "heading": "3.1 Motivation and Architecture Overview",
            "text": "The key motivation behind our approach comes from the following hypothesis: consider a learner L (e.g., a neural network or the student in Fig 2) with sufficient capacity trying to achieve the task of manipulation over Images I . Further, let each image be represented in terms of its properties, or properties of its constituents (e.g. objects like apple, leaf, tree, etc. in Fig 2), where each property comes from a finite set S e.g, attributes of objects in an image. Let the learner be provided with the prior knowledge (for e.g. through Question Answering as in Fig 2) about properties (e.g., color) and their possible values (e.g., red). Then, in order to learn the task of manipulation, it suffices to provide the learner with a query network, which given a manipulated image I\u0303 constructed by the learner via command C, can correctly answer questions (i.e. query) about the desired state of various properties of the constituents of the image I\u0303 . The query network can be internal to the learner (e.g., the student in Fig 2 can query himself for checking the color of apples in the manipulated image). The learner can query repeatedly until it learns to perform the manipulation task correctly. Note, the learner does not have access to the supervised data corresponding to triplets of the form (Is, C, If ), where Is is the starting image, C is the manipulation command, and If is the target manipulated image. Inspired by this, we set out to test this hypothesis by building a model capable of manipulating images, without target images as supervision.\nFigure 3 captures a high-level architecture of the proposed NEUROSIM pipeline. NEUROSIM allows manipulating images containing multiple objects, via complex natural language instructions. Similar to Mao et al. (2019), NEUROSIM assumes the availability of a domain-specific language (DSL) for parsing the instruction text T into an executable program P . NEUROSIM is capable of handling addition, removal, and change operations over image objects. It reasons over the image for locating where the manipulation needs to take place followed by carrying out the manipulation operation. The first three modules, namely i) visual representation network, ii) semantic parser, and iii) concept quantization network are suitably customized from the NSCL and trained as required for our purpose. In what follows, we describe the design and training mechanism of NEUROSIM."
        },
        {
            "heading": "3.2 Modules Inherited from NSCL",
            "text": "1] Visual Representation Network: Given input image I , this network converts it into a scene graph GI = (N,E). The nodes N of this scene graph are object embeddings and the edges E are embeddings capturing the relationship between pair of objects (nodes). Node embeddings are obtained by passing the bounding box of each object (along with the full image) through a ResNet-34 (He et al., 2016). Edge embeddings are obtained by concatenating the corresponding object embeddings. 2] Semantic Parsing Module: The input to this module is a manipulation instruction text T in natural language. Output is a symbolic program P generated by parsing the input text. The symbolic programs are made of operators, that are part of\nour DSL (Specified in Appendix Section A). 3] Concept Quantization Network: Any object in an image is defined by the set of visual attributes (A), and set of symbolic values (Sa) for each attribute a \u2208 A. E.g., attributes can be shape, size, etc. Different symbolic values allowed for an attribute are also known as concepts. E.g., Scolor = {red, blue, green, . . .}. Each visual attribute a \u2208 A is implemented via a separate neural network fa(\u00b7) which takes the object embedding as input and outputs the attribute value for the object in a continuous (not symbolic) space. Let fcolor : Rdobj \u2212\u2192 Rdattr represent a neural network for the color attribute and consider o \u2208 Rdobj as the object embedding. Then, vcolor = fcolor(o) \u2208 Rdattr is the embedding for the object o pertaining to the color attribute. Each symbolic concept s \u2208 Sa for a particular attribute a (e.g., different colors) is also assigned a respective embedding in the same continuous space Rdattr . Such an embedding is denoted by cs. These concept embeddings are initialized at random, and later on, fine-tuned during training. An attribute embedding (e.g. vcolor) can be compared with the embeddings of all the concepts (e.g., cred, cblue, etc.) using cosine similarity, for the purpose of concept quantization of objects. Training for VQA: As a first step, we train the above three modules via a curriculum learning process (Mao et al., 2019). The semantic parser is trained jointly with the concept quantization networks for generating programs for the question texts coming from the VQA dataset. The corresponding output programs are composed of primitive operations coming from the DSL (e.g. filter,\ncount, etc.) and do not include constructs related to manipulation operations. This trains the first three modules with high accuracy on the VQA task."
        },
        {
            "heading": "3.3 Novel Modules and Training NEUROSIM",
            "text": "NEUROSIM training starts with three sub-modules trained on the VQA task as described in Section 3.2. Next, we extend the original DSL to include three additional functional sub-modules within the semantic parsing module, namely add, remove, and change. Refer to appendix section A for details on the DSL. We now reset the semantic parsing module and train it again from scratch for generating programs corresponding to image manipulation instruction text T . Such a program is subsequently used by the downstream pipeline to reason over the scene graph GI and manipulate the image. In this step, the semantic parser is trained using an offpolicy program search based REINFORCE (Williams, 1992) algorithm. Unlike the training of semantic parser for the VQA task, in this step, we do not have any final answer like reward supervision for training. Hence, we resort to a weaker form of supervision. In particular, consider an input instruction text T and set of all possible manipulation program templates Pt from which one can create any actual program P that is executable over the scene graph of the input image. For a program P \u2208 Pt, our reward is positive if this program P selects any object (or part of the scene graph) to be sent to the manipulation networks (change/add/remove). See Appendix C for more details. Once the semantic parser is retrained, we clamp the first three modules and continue using them for the purpose of parsing\ninstructions and converting images into their scene graph representations. Scene graphs are manipulated using our novel module called manipulation network which is described next. 4] Manipulation Network: This is our key module responsible for carrying out the manipulation operations. We allow three kinds of manipulation operations \u2013 add, remove, and change. Each of these operations is a composition of a quasi-symbolic and symbolic operation. A symbolic operation corresponds to a function that performs the required structural changes (i.e. addition/deletion of a node or an edge) in the scene graph GI against a given instruction. A quasi-symbolic operation is a dedicated neural network that takes the relevant part of GI as input and computes new representations of nodes and edges that are compatible with the changes described in the parsed instruction. (a) Change Network: For each visual attribute a \u2208 A (e.g. shape, size, . . . ), we have a separate change neural network that takes the pair of (object embedding, embedding of the changed concept) as input and outputs the embedding of the changed object. This is the quasi-symbolic part of the change function, while the symbolic part is identity mapping. For e.g., let gcolor : Rdobj+dattr \u2212\u2192 Rdobj represent the neural network that changes the color of an object. Consider o \u2208 Rdobj as the object embedding and cred \u2208 Rdattr as the concept embedding for the red color, then o\u0303 = gcolor(o; cred) \u2208 Rdobj represents the changed object embedding, whose color would be red. After applying the change neural network, we obtain the changed representation of the object o\u0303 = ga(o; cs\u2217a), where s \u2217 a is the desired changed value for the attribute a. This network is trained using the following losses.\n\u2113a = \u2212 \u2211\n\u2200s\u2208Sa\nIs=s\u2217a log [p(ha (o\u0303) = s)] (1)\n\u2113a = \u2212 \u2211\n\u2200a\u2032\u2208A,a\u2032 \u0338=a \u2211 \u2200s\u2208Sa\u2032 p(ha\u2032(o) = s)\u2217 log[p(ha\u2032(o\u0303) = s)] (2)\nwhere, ha(x) gives the concept value of the attribute a (in symbolic form s \u2208 Sa) for the object x. The quantity p (ha(x) = s) denotes the probability that the concept value of the attribute a for the object x is equal to s and is given as follows p (ha(x) = s) = expdist(fa(x),cs)/ \u2211 s\u0303\u2208Sa exp\ndist(fa(x),cs\u0303) where, dist(a, b) = (a\u22a4b \u2212 t2)/t1 is the shifted and scaled cosine similarity, t1, t2 being constants. The first loss term \u2113a penalizes the model if\nthe (symbolic) value of the attribute a for the manipulated object is different from the desired value s\u2217a in terms of probabilities. The second term \u2113a, on the other hand, penalizes the model if the values of any of the other attributes a\u2032, deviate from their original values. Apart from these losses, we also include following additional losses.\n\u2113cycle = \u2225o\u2212 ga(o\u0303; cold)\u22252; (3) \u2113consistency = \u2225o\u2212 ga(o; cold)\u22252 (4)\n\u2113objGAN = \u2212 \u2211\no\u2032\u2208O [logD((o\u2032) + log ( 1\u2212D ( ga(o \u2032; c) )) ]\n(5)\nwhere cold is the original value of the attribute a of object o, before undergoing change. Intuitively the first loss term \u2113cycle says that, changing an object and then changing it back should result in the same object. The second loss term \u2113consistency intuitively means that changing an object o that has value cold for attribute a, into a new object with the same value cold, should not result in any change. These additional losses prevent the change network from changing attributes which are not explicitly taken care of in earlier losses (1) and (2). For e.g., rotation or location attributes of the objects that are not part of our DSL. We also impose an adversarial loss \u2113objGAN to ensure that the new object embedding o\u0303 is from the same distribution as real object embeddings. See Appendix C for more details. (b) Remove Network: This network takes the scene graph GI of the input image and removes the subgraph from GI that contains the nodes (and incident edges) corresponding to the object(s) that need to be removed, and returns a new scene graph G\nI\u0303 which is reduced in size. The quasi-symbolic function for the remove network is identity. (c) Add Network: For adding a new object into the scene, add network requires the symbolic values of different attributes, say {sa1 , sa2 , . . . , sak}, for the new object, e.g., {red, cylinder, . . .}. It also requires the spatial relation r (e.g. RightOf) of the new object with respect to an existing object in the scene. The add function first predicts the object (node) embedding o\u0303new for the object to be added, followed by predicting edge embeddings for new edges incident on the new node. New object embedding is obtained as follows: o\u0303new = gaddObj({csa1 , csa2 , \u00b7 \u00b7 \u00b7 , csak}, orel, cr) where, orel is the object embedding of an existing object, relative to which the new object\u2019s position r is specified. For each existing objects oi in the scene, an\nedge e\u0303new,i is predicted between the newly added object o\u0303new and existing object oi in following manner: e\u0303new,i = gaddEdge(o\u0303new, oi). Functions gaddObj(\u00b7) and gaddEdge(\u00b7) are quasi-symbolic operations. Symbolic operations in add network comprise adding the above node and the incident edges into the scene graph.\nThe add network is trained in a self-supervised manner. For this, we pick a training image and create its scene graph. Next, we randomly select an object o from this image and quantize its concepts, along with a relation with any other object oi in the same image. We then use our remove network to remove this object o from the scene. Finally, we use the quantized concepts and the relation that were gathered above and add this object o back into the scene graph using gaddObj(\u00b7) and gaddEdge(\u00b7). Let the embedding of the object after adding it back is o\u0303new. The training losses are as follows:\n\u2113concepts =\u2212 k\u2211\nj=1\nlog ( p(haj (o\u0303new) = saj ) ) (6)\n\u2113relation =\u2212 log(p(hr(o\u0303new, oi) = r)) (7) \u2113objSup = \u2225o\u2212 o\u0303new\u22252 (8)\n\u2113edgeSup = \u2211\ni\u2208O \u2225eold,i \u2212 e\u0303new,i\u22252 (9) \u2113edgeGAN = \u2212 \u2211 \u2200i\u2208O [logD({o; eold,i; oi})+\nlog(1\u2212D ({o\u0303new; e\u0303new,i; oi}))] (10)\nwhere saj is the required (symbolic) value of the attribute aj for the original object o, and r is the required relational concept. O is the set of the objects in the image, eold,i is the edge embedding for the edge between original object o and its neighboring object oi. Similarly, e\u0303new,i is the corresponding embedding of the same edge but after when we have (removed + added back) the original object. The loss terms \u2113concepts and \u2113relation ensure that the added object comprises desired values of attributes and relation, respectively. Since we had first removed and then added the object back, we already have the original edge and object representation, and hence we use them in loss terms given in equation 9. We use adversarial loss equation 10 for generating real (object, edge, object) triples and also a loss similar to equation 5 for generating real objects."
        },
        {
            "heading": "3.4 Image Rendering from Scene Graph",
            "text": "5] Rendering Network: Following Johnson et al. (2018), the scene graph for an image is first generated using the visual representation network,\nwhich is the processed by a GCN and passed through a mask regression network followed by a box regression network to generate a coarse 2- dimensional structure (scene layout). A Cascaded Refinement Network (Chen and Koltun, 2017) is then employed to generate an image from the scene layout. A min-max adversarial training procedure is used to generate realistic images, using a patchbased and object-based discriminator."
        },
        {
            "heading": "4 Experiments",
            "text": "Datasets: Among the existing datasets, CSS (Vo et al., 2019) contains simple 0-hop instructions and is primarily designed for the text-guided image retrieval task. Other datasets such as i-CLEVR (ElNouby et al., 2019) and CoDraw are designed for iterative image editing. i-CLEVR contains only \"add\" instructions and CoDraw doesn\u2019t contain multi-hop instructions. Hence we created our own multi-object multi-hop instruction based image manipulation dataset, referred to as CIM-NLI. This dataset was generated with the help of CLEVR toolkit (Johnson et al., 2017b). CIM-NLI consists of (Source image I , Instruction text T , Target image I\u0303\u2217) triplets. The dataset contains a total of 18K, 5K, 5K unique images and 54K, 14K, 14K instructions in the train, validation and test splits respectively. Refer to Appendix B for more details about the dataset generation and dataset splits. Baselines: We compare our model with purely supervised approaches such as TIM-GAN (Zhang et al., 2021), GeNeVA (El-Nouby et al., 2019) and InstructPix2Pix (Brooks et al., 2023). In order to make a fair and meaningful comparison between the two kinds (supervised and our, weaklysupervised) approaches, we carve out the following set-up. Assume the cost required to create one single annotated example for image manipulation task be \u03b1m while the corresponding cost for the VQA task be \u03b1v. Let \u03b1 = \u03b1m/\u03b1v. Let \u03b2m be the number of annotated examples required by a supervised baseline for reaching a performance level of \u03b7m on the image manipulation task. Similarly, let \u03b2v be the number of annotated VQA examples required to train NEUROSIM to reach the performance level of \u03b7v. Let \u03b2 = \u03b2m/\u03b2v. We are interested in figuring out the range of \u03b2 for which performance of our system (\u03b7v) is at least as good as the baseline (\u03b7m). Correspondingly we can compute the ratio of the labeling effort required, i.e., \u03b1 \u2217 \u03b2, to reach these performance levels.\nIf \u03b1 \u2217 \u03b2 > 1, our system achieves the same or better performance, with lower annotation cost. Weakly supervised models (Li et al., 2020; Nam et al., 2018) are designed for a problem setting different from ours \u2013 single salient object scenes, simple 0-hop instructions (Refer Section 2 for details). Further, they require paired images and their textual descriptions as annotations. We, therefore, do not compare with them in our experiments. See Appendix G, H for computational resources and hyperparameters respectively.\nEvaluation Metrics: For evaluation on image manipulation task, we use three metrics - i) FID, ii) Recall@k, and iii) Relational-similarity (rsim). FID (Heusel et al., 2017) measures the realism of the generated images. We use the implementation proposed in Parmar et al. (2022) to compute FID. Recall@k measures the semantic similarity of gold manipulated image I\u0303\u2217 and system produced manipulated image I\u0303 . For computing Recall@k, we follow Zhang et al. (2021), i.e. we use I\u0303 as a query and retrieve images from a corpus comprising the entire test set. rsim measures how many of the ground truth relations between the objects are present in the generated image. We follow (El-Nouby et al., 2019) to implement rsim metric that uses predictions from a trained object-detector (Faster-RCNN) to perform relation matching between the scene-graphs of ground-truth and generated images."
        },
        {
            "heading": "4.1 Performance with varying Dataset Size",
            "text": "Table 2 compares the performance of NEUROSIM other SoTA methods two level of \u03b2 0.054 and 0.54 representing use of 10% and 100% samples from CIM-NLI. Despite being weakly supervised, NEUROSIM performs significantly better than the baselines with just 10k data samples (especially TIM-GAN) and not too far from diffusion model\nbased IP2P in full data setting, using the R@1 performance metric. This clearly demonstrates the strength of our approach in learning to manipulate while only making use of VQA annotations. We hypothesize that, in most cases, NEUROSIM will be preferable since we expect the cost of annotating an output image for manipulation to be significantly higher than the cost of annotating a VQA example. To reach the performance of the NEUROSIM in a low data regime, TIM-GAN requires a larger number of expensive annotated examples (ref. Table 13 in Appendix). The FID metric shows similar trend across dataset sizes and across models. The FID scores for NEUROSIM could potentially be improved by jointly training VQA module along with image decoder and is a future direction.\nWe evaluate InstructPix2Pix (IP2P) (Brooks et al., 2023), a state-of-the-art pre-trained diffusion model for image editing, in a zero-shot manner on the CIM-NLI dataset. Considering its extensive pre-training, we expect IP2P to have learned the concepts present in the CIM-NLI dataset. In this setting IP2P achieves a FID score of 33.07 and R@1 score of 7.48 illustrating the limitations of\nlarge-scale models in effectively executing complex instruction-based editing tasks without full dataset fine-tuning. Table 2 contains the results obtained by IP2P after fine-tuning for 16k iterations on CIM-NLI dataset."
        },
        {
            "heading": "4.2 Performance versus Reasoning Hops",
            "text": "Table 3 (right) compares baselines with NEUROSIM for performance over instructions requiring zero-hop (ZH) versus multi-hop (1\u2212 3 hops) (MH) reasoning. Since there are no Add instructions with ZH, we exclude them from this experiment for the comparison to be meaningful. GeNeVA performs abysmally on both ZH as well as MH. We see a significant drop in the performance of both TIM-GAN and IP2P when going from ZH to MH instructions, both for training on 5.4K, as well as, 54K datapoints. In contrast, NEUROSIM trained on 10% data, sees a performance drop of only 1.5 points showing its robustness for complex reasoning tasks."
        },
        {
            "heading": "4.3 Zero-shot Generalization to Larger Scenes",
            "text": "We developed another dataset called CIM-NLILARGE, consisting of scenes having 10 \u2212 13 objects (See Appendix B for details). We study the combinatorial generalization ability of NEUROSIM and the baselines when the models are trained on CIM-NLI containing scenes with 3\u2212 8 objects only and evaluated on CIM-NLI-LARGE. Table 3 captures such a comparison. NEUROSIM does significantly better, i.e., 33 pts (R1) than TIMGAN and is competitive with IP2P when trained on 10% (5.4Kdata points) of CIM-NLI. We do see a drop in performance relative to baselines when they are trained on full (54K) data, but this is expected as effect of supervision takes over, and ours is a weakly supervised model. Nevertheless, this experiments demonstrates the effectiveness of our\nmodel for zero-shot generalization, despite being weakly sueprvised."
        },
        {
            "heading": "4.4 Qualitative Analysis and Interpretability",
            "text": "Figure 4 shows anecdotal examples for visually comparing NEUROSIM with baselines. Note, GeNeVA either performs the wrong operation on the image (row #1, 2, 3) or simply copies the input image to output without any modifications. TIMGAN often makes semantic errors which show its lack of reasoning (row #3) or make partial edits (row #1). IP2P also suffers from this where it edits incorrect object (row #1,2). Compared to baselines, NEUROSIM produces semantically more meaningful image manipulation. NEUROSIM can also easily recover occluded objects (row #4). For more results, see Appendix I, J. NEUROSIM produces interpretable output programs, showing the steps taken by the model to edit the images, which also helps in detecting errors (ref. Appendix L)."
        },
        {
            "heading": "4.5 Evaluating Manipulated Scene Graph",
            "text": "We strongly believe image rendering module of NEUROSIM pipeline and encoder modules used for computing Recall@k add some amount of inefficiencies resulting in lower R1 and R3 scores for us. Therefore, we decide to assess the quality of manipulated scene graph G\nI\u0303 .\nMethod R1 R3\nText-Only 0.2 0.4 Image-Only 34.1 83.6 Concat 39.5 86.9 TIRG 34.8 84.6 NEUROSIM 85.8 92.9\nTable 4: GI\u0303 Quality via image retrieval.\nFor this, we consider the text guided image retrieval task proposed by (Vo et al., 2019). In this task, an image from the database has to be retrieved which would be the closest match to the desired manipulated image. Therefore, we use our manipulated scene graph G\nI\u0303 as\nthe latent representation of the input instruction and image for image retrieval. We retrieve images from the database based on a novel graph edit distance between NEUROSIM generated G\nI\u0303 of the desired\nmanipulated images, and scene graphs of the images in the database. This distance is defined using the Hungarian algorithm (Kuhn, 1955) with a simple cost defined between any 2 nodes of the graph (ref. Appendix D for details). Table 4 captures the performance of NEUROSIM and other popular baselines for the image retrieval task. NEUROSIM significantly outperforms supervised learning baselines by a margin of \u223c 50% without using output image supervision, demonstrating that NEUROSIM\nmeaningfully edits the scene graph. Refer to Section 4.7 for human evaluation results and Appendix Section D-E, K, for more results including results on Minecraft dataset and ablations."
        },
        {
            "heading": "4.6 A Hybrid Approach using NEUROSIM",
            "text": "From Table 3, we observe that both TIM-GAN and IP2P suffer a significant drop in performance when moving from ZH to MH instructions, whereas NEUROSIM is fairly robust to this change. Further, we note that the manipulation instructions in our dataset are multi-hop in terms of reasoning, but once an object of interest is identified, the actual manipulation operation can be seen as single hop. We use this observation to design a hybrid supervised baseline that utilizes the superior reasoning capability of NEUROSIM and high quality editing and generation capabilities of IP2P.\nWe take the CIM-NLI test set and parse the textinstructions through our trained semantic-parser to obtain the object embeddings over which the manipulation operation is to be performed. We utilize our trained query networks to obtain the symbolic attributes such as color, shape, size and material of the identified object. Using these attributes we simplify a complex multi-hop instruction into a simple instruction with 0 or 1 hops using a simple template based approach (see Appendix Section N for details). These simplified instructions are fed to the fine-tuned IP2P model to generate the edited images. We refer to our hybrid approach as IP2PNS where NS refers to Neuro-Symbolic. Table 5 presents the results. We find that there is a clear advantage of using a hybrid neuro-symbolic model integrating NEUROSIM with IP2P. We see a significant gain on FID, recall, rsim when we use the hybrid approach, especially in the low resource setting (\u03b2 = 0.054). Compared to IP2P, the hybrid neuro-symbolic approach results in better FID, recall and rsim scores, except a small drop in R1 for \u03b2 = 0.54 setting. This opens up the possibility of further exploring such hybrid models in future for improved performance (in the supervised setting)."
        },
        {
            "heading": "4.7 Human Evaluation",
            "text": "For the human evaluation study, we presented 10 evaluators with a set of five images each, including: The input image, the ground-truth image and manipulated images generated by NEUROSIM 5.4K, TIM-GAN 54K, and IP2P 54K. Images generated by the candidate models were randomly shuffled to prevent any bias. Evaluators were asked two binary questions, each requiring a \u2019yes\u2019 (1) or \u2019no\u2019 (0) response, to assess the models: (Q1) Does the model perform the desired change mentioned in the input instruction?, (Q2) Does the model not introduce any undesired change elsewhere in the image? Refer to Appendix Section M for more details about exact questions and the human evaluation process.\nThe average scores from the evaluators across different questions can be found in Table 6. The study achieved a high average Fleiss\u2019 kappa score (Fleiss et al., 2013) of 0.646, indicating strong inter-evaluator agreement. Notably, NEUROSIM (5.4K) outperforms TIM-GAN and IP2P (54K) in Q1 suggesting its superior ability to do reasoning, and identify the relevant object as well as affect the desired change. In contrast, TIM-GAN and IP2P score significantly better in Q2, demonstrating their ability not to introduce unwanted changes elsewhere in the image, possibly due to better generation quality compared to NEUROSIM."
        },
        {
            "heading": "5 Conclusion",
            "text": "We present a neuro-symbolic, interpretable approach NEUROSIM to solve image manipulation task using weak supervision in the form of VQA annotations. Our approach can handle multi-object scenes with complex instructions requiring multihop reasoning, and solve the task without any output image supervision. We also curate a dataset of image manipulation and demonstrate the potential of our approach compared to supervised baselines. Future work includes understanding the nature of errors made by NEUROSIM, having a human in the loop to provide feedback to the system for correction, and experimenting with real image datasets."
        },
        {
            "heading": "6 Ethics Statement",
            "text": "All the datasets used in this paper were synthetically generated and do not contain any personally identifiable information or offensive content. The ideas and techniques proposed in this paper are useful in designing interpretable natural languageguided tools for image editing, computer-aided design, and video games. One of the possible adverse impacts of AI-based image manipulation is the creation of deepfakes (Vaccari and Chadwick, 2020) (using deep learning to create fake images). To counter deepfakes, several researchers (Dolhansky et al., 2020; Mirsky and Lee, 2021) have also looked into the problem of detecting real vs. fake images."
        },
        {
            "heading": "7 Limitations",
            "text": "A limitation of our approach is that when transferring to a new domain, having different visual concepts requires not only learning new visual concepts but also the DSL needs to be redefined. Automatic learning of DSL from data has been explored in some prior works (Ellis et al., 2021, 2018), and improving our model using these techniques are future work for us. We can also use more powerful graph decoders for image generation, for improved image quality, which would naturally result in stronger results on image manipulation."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank anonymous reviewers for their insightful suggestions that helped in greatly improving our paper. We also thank Rushil Gupta and other members of IIT Delhi DAIR group for their helpful comments and suggestions on this work. This work was supported by an IBM AI Horizons Network (AIHN) grant. We thank IIT Delhi HPC facility2, IBM cloud facility, and IBM Cognitive Computing Cluster (CCC) for computational resources. Ashish Goswami is a PhD student at Yardi-ScAI@IITDelhi and is supported by Yardi School of AI Publication Grant. Parag Singla was supported by the DARPA Explainable Artificial Intelligence (XAI) Program with number N66001-17-2-4032, IBM AI Horizon Networks (AIHN) grant and IBM SUR awards. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views\n2http://supercomputing.iitd.ac.in\nor official policies, either expressed or implied, of the funding agencies."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "A Domain Specific Language (DSL)",
            "text": "Table 7 captures the DSL used by our NEUROSIM pipeline. The first 5 constructs in this table are common with the DSL used in Mao et al. (2019). The last 3 operations (Change, Add, and Remove) were added by us to allow for the manipulation operations. Table 8 shows the type system used by the DSL in this work. The first 5 types are inherited from (Mao et al., 2019) while the last one is an extension of the type system for handling the inputs to the Add operator."
        },
        {
            "heading": "B Dataset Details",
            "text": "We use CLEVR dataset and CLEVR toolkit (code to generate the dataset). These are public and are under CC and BSD licenses respectively, and are used by many works, including ours, for research purposes. We now give details of the datasets we create, building upon CLEVR."
        },
        {
            "heading": "B.1 CIM-NLI Dataset",
            "text": "This dataset was generated with the help of CLEVR toolkit (Johnson et al., 2017b) by using following recipe.\n1. First, we create a source image I and the corresponding scene data by using Blender (Community, 2018) software.\n2. For each source image I created above, we generate multiple instruction texts T \u2019s using its scene data. These are generated using templates, similar to question templates proposed by (Johnson et al., 2017b).\n3. For each such (I, T ) pair, we attach a corresponding symbolic program P (not used by NEUROSIM though) as well as scene data for the corresponding changed image.\n4. Finally, for each (I, T ) pair, we generate the target gold image I\u0303\u2217 using Blender software and its scene data from the previous step.\nBelow are some of the important characteristics of the CIM-NLI dataset.\n\u2022 Each source image I comprises several objects and each object comprises four visual attributes - color, shape, size, and material.\n\u2022 Each instructions text T comprises one of the following three kinds of manipulation operations - add, remove, and change.\n\u2022 An add instruction specifies color, shape, size, and material of the object that needs to be added. It also specifies a direct (or indirect) relation with one or more existing objects (called reference object(s)). The number of relations that are required to traverse for nailing down the target object is referred to as # of reasoning hops and we have allowed instructions with up to 3-hops reasoning. We do not generate any 0-hop instruction for add due to ambiguity of where to place the object inside the scene.\n\u2022 A change instruction first specifies zero or more attributes to uniquely identify the object that needs to be changed. It may also specify a direct (or indirect) relation with one or more existing reference objects. Lastly, it specifies the target values of an attribute for the identified object which needs to be changed.\n\u2022 A remove instruction specifies zero or more attributes of the object(s) to be removed. Additionally, it may specify a direct (or indirect) relation with one or more existing reference objects.\nTable 9 captures the fine grained statistics about the CIM-NLI dataset. Specifically, it further splits each of the train, validation, and test set across the instruction types - add, remove, and change."
        },
        {
            "heading": "B.2 CIM-NLI-LARGE Dataset",
            "text": "We created another dataset called CIM-NLILARGE to test the generalization ability of NEUROSIM on images containing more number of objects than training images. CIM-NLI-LARGE tests the zero-shot transfer ability of both NEUROSIM and baselines on scenes containing more objects.\nEach image in CIM-NLI-LARGE dataset comprises of 10\u221213 objects as opposed to 3\u22128 objects in CIM-NLI dataset which was used to train NEUROSIM. The CIM-NLI-LARGE dataset consists of 1K unique input images. We have created 3 instructions for each image resulting in a total of 3K instructions. The number of add instructions is significantly less since there is very little free space available in the scene to add new objects. To create scenes with 12 and 13 objects, we made all objects as small size and the minimum distance between\nobjects was reduced so that all objects could fit in the scene. Table 10 captures the statistics about this dataset."
        },
        {
            "heading": "B.3 Multi-hop Instructions",
            "text": "In what follows, we have given examples of the instructions that require multi-hop reasoning to nail down the location/object to be manipulated in the image.\n\u2022 Remove the tiny green rubber ball. (0-hop)\n\u2022 There is a block right of the tiny green rubber ball, remove it. (1-hop)\n\u2022 Remove the shiny cube left of the block in front of the gray thing. (2-hop)\n\u2022 Remove the small thing that is left of the brown matte object behind the tiny cylinder that is behind the big yellow metal block. (3-hop)"
        },
        {
            "heading": "C Model Details",
            "text": ""
        },
        {
            "heading": "C.1 Semantic Parser",
            "text": ""
        },
        {
            "heading": "C.1.1 Details on Parsing",
            "text": "We begin by extending the type system of (Mao et al., 2019) and add ConceptSet because our add operation takes as input a set of concepts depicting attribute values of the new object being added (refer Table 8 for the details). Next, in a manner similar to (Mao et al., 2019), we use a rule based system for extracting concept words from the input text. We, however, add an extra rule for extracting ConceptSet from the input sentence. Rest of the semantic parsing methodology remains the same as given in (Mao et al., 2019), with the difference being that our training is weakly supervised (refer Section 3.3 of the main paper)."
        },
        {
            "heading": "C.1.2 Training",
            "text": "As explained in Section 3.3 of the main paper, for training with weaker form of supervision, we use an off-policy program search based REINFORCE (Williams, 1992) algorithm for calculating the exact gradient. For this, we define a set of all possible program templates Pt. For a given input instruc-\ntion text T , we create a set of all possible programs {PT } from Pt. For e.g. given a template {remove(relate(\u00b7, filter(\u00b7, scene())))}, this is filled in all possible ways, with concepts, conceptSet, attributes and relational concepts extracted from the input sentence to get programs for this particular template. All such programs created using all templates form the set PT . All PT are executed over the scene graph of the input image. A typical program structure in our work is of the form manip_op(reasoning()), where manip_op represents the manipulation operator, for example change, add, or remove; and reasoning() either selects objects for change or remove, or it selects a reference object for adding another object in relation to it. After a hyperparameter search for the reward (refer Section H of the appendix), we assign a reward of +8 if the reasoning() part of the program leads to an object being selected for change/remove instruction or a related object being selected for add instruction. If no such object is selected, we give a reward of +2. Reward values were decided on the basis of validation set accuracy. We find that with this training strategy, we achieve the validation set accuracy of 95.64%, where this accuracy is calculated based on whether a program lead to an object being selected or not. Note, this is a proxy to the actual accuracy. For finding the actual accuracy, we would need a validation set of (instruction, ground truth output program) pairs, but we do not use this supervised data for training or validation."
        },
        {
            "heading": "C.2 Manipulation Network",
            "text": "In what follows, we provide finer details of manipulation network components.\nChange Network: As described in Section 3.3 of the main paper, we have a change neural network for each attribute. For changing the current attribute value of a given object o, we use the following neural network: o\u0303 = ga(o; cs\u2217a), where s \u2217 a is the desired changed value for the attribute a. o\u0303 is the new representation of the object. We model ga(\u00b7) by a single layer neural network without having any non-linearity. The input dimension of this neural network is (256 + 64) because we concatenate the object representation o \u2208 R256 with the desired concept representation d \u2208 R64. We pass this concatenated vector through ga(\u00b7) to get the revised representation of the object: o\u0303 \u2208 R256.\nThe loss used to train the weights of the change network is a weighted sum of losses equation 1 to equation 5 given in the main paper. This leads to the overall loss function given below.\nLoverall_change = \u03bb1 \u2113a + \u03bb2 \u2113a + \u03bb3 \u2113cycle + \u03bb4 \u2113consistency + \u03bb5 \u2113objGAN\nwhere, \u2113objGAN above is the modified GAN loss (Goodfellow et al., 2014). Here \u03bb1 = 1, \u03bb2 = 1/((num_attrs\u22121)\u2217(num_concepts)), \u03bb3 = \u03bb4 = 10\n3, and \u03bb5 = 1/(num_objects). Here, (num_objects) is the number of objects in input image, (num_attrs) is the total number of attributes for each object, and (num_concepts) are the total number of concepts in the NSCL (Mao et al., 2019) framework.\nThe object discriminator is a neural network with input dimension 256 and a single 300 dimensional hidden layer with ReLU activation function. This discriminator is trained using standard GAN objective \u2113objGAN. See Fig 5a for an overview of the change operator\nRemove Network: The remove network is a symbolic operation as described in Section 3.3 of the main paper. That is, given an input set of objects, the remove operation deletes the subgraph of the scene graph that contains the nodes corresponding to removed objects and the edges incident on those nodes. See Fig 5c for an overview of the remove operator.\nAdd Network: The neural operation in the add operator comprises of predicting the object representation for the newly added object using a function gaddObj(\u00b7). This function is modeled as a single layer neural network without any activation. The input to this network is a concatenated vector [[csa1 , csa2 , \u00b7 \u00b7 \u00b7 , csak ], orel, cr], where [csa1 , csa2 , \u00b7 \u00b7 \u00b7 , csak ] represents the concatenation of all the concept vectors of the desired new objects. The vector orel is the representation of the object with whom the relation (i.e. position) of the new object has been specified and cr is the concept vector for that relationship. The input dimension of gaddObj(\u00b7) is (k \u2217 64 + 256 + 64) and the output dimension is 256. For predicting representation of newly added edges in the scene graph, we use edge predictor gaddEdge(\u00b7). The input to this edge predictor function is the concatenated representation of the objects which are linked by the edge. The input dimension of gaddEdge(\u00b7) is (256 + 256) and the output dimension is 256.\nThe loss used to train the add network weights is a weighted sum of losses equation 6 to equation 10 along with an object discriminator loss. The overall loss is given by the following expression.\nLoverall_add = \u03bb1\u2113concepts + \u03bb2\u2113relation + \u03bb3\u2113objSup + \u03bb4\u2113edgeSup + \u03bb5\u2113edgeGAN + \u03bb6\u2113objGAN\nwhere, \u2113objGAN and \u2113edgeGAN above denotes the modified GAN loss (Goodfellow et al., 2014). Here\n\u03bb1 = \u03bb2 = 1/(num_attrs), \u03bb3 = \u03bb4 = 103, \u03bb6 = 1/(num_objects).\nThe object discriminator is a neural network with input dimension as 256 and a single 300 dimensional hidden layer with ReLU activation function. This discriminator is trained using the standard GAN objective \u2113objGAN. Note, \u2113objGAN has 2 parts \u2013 i) the loss for the generated (fake) object embedding using the add network, and ii) the loss for the real objects (all the unchanged object embeddings of the image). The former is unscaled but the latter one is scaled by a factor of 1/(num_objects).\nThe edge discriminator is a neural network with input dimension as (256 \u2217 3) and a single 300 dimensional hidden layer with ReLU activation function. As input to this discriminator network, we pass the concatenation of the two objects and the edge connecting them. This discriminator is trained using the standard GAN objective \u2113edgeGAN. See Fig 5b for an overview of the add operator"
        },
        {
            "heading": "D Additional Results",
            "text": ""
        },
        {
            "heading": "D.1 Detailed Performance for Zero-Shot Generalization on Larger Scenes",
            "text": "Table 11 below is a detailed version of the Table 3 in the main paper. This table compares the performance of NEUROSIM with baseline methods TIM-GAN, GeNeVA and IP2P for the zero-shot generalization to larger scenes (with \u2265 10 objects), while the models were trained on images with 3\u22128 objects. Relative to the main paper\u2019s table 3, this table offers separate performance numbers for each of the add, remove and change instructions.\nD.2 Image Retrieval Task\nA task that is closely related to the image manipulation task is the task of Text Guided Image Retrieval, proposed by (Vo et al., 2019). Through this experiment, our is to demonstrate that NEUROSIM is highly effective in solving this task as well. In what follows, we provide details about this task, baselines, evaluation metric, how we adapted NEUROSIM for this task, and finally performance results in Table 12. This table is a detailed version of the Table 4 in the main paper.\nTask Definition: Given an Image I , a text instruction T , and a database of images D, the task is to retrieve an image from the database that is semantically as close to the ground truth manipulated image as possible.\nNote, for each such (I, T ) pair, some image from the database, say I\u0303 \u2208 D, is assumed to be the ideal image that should ideally be retrieved at rank-1. This, so called desired gold retrieval image might even be an image which is the ideal manipulated version of the original images I in terms of satisfying the instruction T perfectly. Or, image I\u0303 may not be such an ideal manipulated image but it still may be the image in whole corpus D that comes closest to the ideal manipulated image.\nIn practice, while measuring the performance of any such system for this task, the gold manipulated image for (I, T ) pair is typically inserted into the database D and such an image then serves as the desired gold retrieval image I\u0303 .\nBaselines: Our baselines includes popular supervised learning systems designed for this task. The first baseline is TIRG proposed by Vo et al. (2019) where they combine image and text to get a joint embedding and train their model in a supervised manner using embedding of the desired retrieved image as supervision. For completeness, we also include comparison with other baselines \u2013 Concat, Image-Only, and Text-Only \u2013 that were introduced by Vo et al. (2019).\nA recent model proposed by Chen et al. (2020) uses symbolic scene graphs (instead of embeddings) to retrieve images from the database. Motivated by this, we also retrieve images via the scene graph that is generated by the manipulation module of NEUROSIM. However, unlike Chen et al. (2020), the nodes and edges in our scene graph have associated vectors and make a novel use of them while retrieving. We do not compare our performance with (Chen et al., 2020) since its code is unavailable and we haven\u2019t been able to reproduce their numbers on datasets used in their paper. Moreover, (Chen et al., 2020) uses full supervision of the desired output image (which is converted to a symbolic scene graph), while we do not.\nEvaluation Metric: We use Recall@k (and report results for k = 1, 3) for evaluating the performance of text guided image retrieval algorithms which is standard in the literature.\nRetrieval using Scene Graphs: We use the scene graph generated by NEUROSIM as the latent representation to retrieve images from the database. We introduce a novel yet simple method to retrieve images using scene graph representation. For converting an image into the scene graph, we use the vi-\nsual representation network of NEUROSIM. Given the scene graph G for the input image I and the manipulation instruction text T , NEUROSIM converts the scene graph into the changed scene graph G\nI\u0303 , as described in Section C in Appendix. Now,\nwe use this graph G I\u0303 as a query to retrieve images from the database D. For retrieval, we use the novel graph edit distance (GED) between G I\u0303 and the scene graph representation of the database images. The scene graph for each database image is also obtained using the visual representation network of NEUROSIM. The graph edit distance is given below.\nGED(G I\u0303 , GD) =\n{ \u221e |N\nI\u0303 |\u0338= |N D\u0303 | min\u03c0\u2208\u03a0 \u2211\n\u2200i\u2208{1,\u00b7\u00b7\u00b7,|N I\u0303 |} c(ni, yi) otherwise.\nwhere, G I\u0303 = (N I\u0303 , V I\u0303 ) and GD = (ND, VD). ni and yi are the node embeddings of the query graph G\nI\u0303 and scene graph GD of an image from\nthe database. c(a, b) is the cosine similarities between embeddings a and b. This GED is much simpler than that defined in (Chen et al., 2020), since it does not need any hand designed cost for change, removal, or addition of nodes, or different attribute values. It can simply rely on the cosine similarities between node embeddings. We use the Hungarian algorithm (Kuhn, 1955) for calculating the optimal matching \u03c0 of the nodes, among all possible matching \u03a0. We use the negative of the cosine similarity scores between nodes to create the cost matrix for the Hungarian algorithm to process. This\nsimple yet highly effective approach (See Table 4 in the main paper and Table 12 in the appendix), can be improved by more sophisticated techniques that include distance between edge embeddings and including notion of subgraphs in the GED. We leave this as future work. This result shows that our manipulation network edits the scene graph in a desirable manner, as per the input instruction."
        },
        {
            "heading": "D.3 Detailed Multi-hop Reasoning Performance",
            "text": "Table 14 below provides a detailed split of the performance numbers reported in Table 3 of the main paper across i) number of hops (0\u2212 3 hops) and ii) type of instructions (add/remove/change). We observe that for change and remove instructions, NEUROSIM improves over TIM-GAN, GeNeVA and IP2P trained on 5.4K CIM-NLI data points by a significant margin (\u223c 20% on 3- hop change/remove instructions). However, NEUROSIM lags behind TIM-GAN when the entire CIM-NLI labeled data is used to train TIM-GAN. We also observe that all the models perform poorly on the add instructions, as compared to change and remove instructions."
        },
        {
            "heading": "D.4 Detailed Performance for Different Cost Ratios \u03b2",
            "text": "Table 2 in Section 4 of the main paper showed the performance of NEUROSIM compared with TIMGAN and GeNeVA for various values of \u03b2, where\n\u03b2 is the ratio of the number of annotated (with output image supervision) image manipulation examples required by the supervised baselines, to the number of annotated VQA examples required to train NEUROSIM. In Table 13, we show a detailed split of the performance, for the add, change, and remove operators, across the same values of \u03b2 as taken before.\nWe find that for the change operator, NEUROSIM performs better than TIM-GAN by a margin of \u223c 8% (considering Recall@1) for \u03b2 \u2264 0.1. For the remove operator, NEUROSIM performs better than TIM-GAN by a margin of \u223c 4% (considering Recall@1) for \u03b2 \u2264 0.2. Overall, NEUROSIM performs similar to TIM-GAN, for \u03b2 = 0.2, for remove and change operators. All models perform poorly on the add operator as compared to the change and remove operators. We find that having full output image supervision allows TIM-GAN to reconstruct (copy) the unchanged objects from the input to the output for all the operators. This results in a higher recall in general but its effect is most pronounced in the Recall@3. NEUROSIM, on the other hand, suffers from rendering errors which makes the overall recall score (especially Recall@3) lower. We believe that improving image rendering quality would significantly improve the performance of NEUROSIM and we leave this as future work.\nD.5 Results on Datasets from different domains"
        },
        {
            "heading": "D.5.1 Minecraft Dataset",
            "text": "Dataset Creation: We create a new dataset having (Image, instruction) by building over the Minecraft\ndataset used in (Yi et al., 2018). Specifically, we create zero and one hop remove instructions and one hop add instructions similar to the creation of CIM-NLI. This dataset contains scenes and objects from the Minecraft video game and is used in prior works for testing Neuro-Symbolic VQA systems like NSCL (Mao et al., 2019) and NS-VQA (Yi et al., 2018). The setting of the Minecraft worlds dataset is significantly different from CLEVR in terms of concepts and attributes of objects and visual appearance.\nExperiment: We use the above dataset for testing the addition and removal of objects using NeuroSIM (See Fig 6). We train NeuroSIM\u2019s decoder to generate images from scene graphs of the minecraft dataset. We assume access to a parser that gives us programs for an instruction. For removal, we use the same remove network as described above, while for addition, we assume access to the features of object to be added, which is added to the scene graph of the image and the decoder decodes the final image. See Figure 6 for a set of successful examples on the Minecraft dataset. We see that using our method, one can add and remove objects from the scene successfully, without using any output image as supervision during training. Though we have assumed the availability of a parser in the above set-up, training it jointly with other modules should be straightforward, and can be achieved using our general approach described in Section 3 of the main paper."
        },
        {
            "heading": "E End-to-end Training",
            "text": "The main objective of this work is to make use of weakly supervised VQA data for the image manipu-\nlation task without using output image supervision. But a natural extension of our work is to use output image supervision as well, to improve the performance of NEUROSIM. We devised an experiment to compare how much performance boost can be obtained by utilizing ground truth output (manipulated) images as the supervision for different modules of NEUROSIM. This experiment demonstrates the value of end-to-end training for NEUROSIM and how it can exploit the supervised data. We refer to this variant as NEUROSIM(e2e). We begin with a pre-trained NEUROSIM model trained with VQA annotations and then fine-tune it using supervised manipulation data. The detailed results are given in Table 15. This experiment demonstrates that with a small amount of supervised data, the performance of NEUROSIM can be significantly improved (e.g., more than 9 points increase for the change instruction with only 5.4K supervision examples)\nGiven the significant increase in performance of NEUROSIM when using supervised data, we also test it\u2019s generalization capability (Analogous to Section 4.2, 4.3), and quality of scene graph retrieval (Analogous to Section 4.5 ).\nFrom Table 16, we see that NEUROSIM(e2e) shows improved zero-shot generalization to larger scenes. Even when trained on just 5.4k CIM-NLI data, NEUROSIM(e2e) improves over TIM-GAN54k by 3.9 R@1 points. A 5.3 point improvement over TIM-GAN is observed when full CIM-NLI data is used.\nNext, we measure drop in performance with increasing reasoning hops. From Table 17, we see that NEUROSIM(e2e) achieves the lowest drop when compared to TIM-GAN. NEUROSIM(e2e) improves over weakly supervised NEUROSIM baseline by 6.6 R@1 points.\nFinally, we measure the quality of scene graphs via retrieval. From Table 18, we see that super-\nvised training significantly improves the scene graph quality, thus improving retrieval performance. Supervised training improves retrieval by 7.3 R@1 points over weakly supervised NEUROSIM baseline. These findings suggest that NEUROSIM(e2e) significantly outperforms other supervised approaches in almost all settings. One can fine-tune the image decoder and the visual representation network to further enhance the findings, which should greatly enhance the outcomes."
        },
        {
            "heading": "F LLMs as few-shot parser",
            "text": "We also tested the semantic parsing ability of Large Language Models (LLMs), specifically GPT-4 for our task. The task of semantic parsing is given\nmanipulation instruction text in natural language, generated the symbolic program by parsing the input text. To provide GPT-4 with context, we designed an extensive prompt that begins with our DSL followed by six different in-context examples representing various instruction types for few-shot learning. This prompt is then followed with the instruction text that we want to parse. We tested GPT-4 on a randomly sampled subset of our test dataset. For evaluation, we measured the accuracy of semantic parsing using an exact match between the generated symbolic program and the groundtruth symbolic program.\nThe detailed results are given in Table 19. Interestingly, we observed that GPT-4 performed poorly on Add instructions, achieving less than 10% of parsing accuracy. To address this, we prompted GPT-4 separately with additional few-shot examples for Add instructions, which led to the results displayed in the table. Even with the additional\nguidance, Add instructions remained significantly lower in accuracy compared to other instruction types. This analysis demonstrates that our reinforcement learning-based instruction parser outperforms GPT-4, at least on this dataset. It also highlights the need for more careful prompt engineering before LLMs like GPT-4 can be readily applied in our specific setting."
        },
        {
            "heading": "G Computational Resources",
            "text": "We trained all our models and baselines on 1 Nvidia Volta V100 GPU with 32GB memory and 512GB system RAM except IP2P which was trained on 8-A100 80 GB GPUs. Our image decoder training takes about 4 days of training time. Training of the VQA task takes 5 \u2212 7 days of training time and training the Manipulation networks take 4\u2212 5 hours of training time."
        },
        {
            "heading": "H Hyperparameters and Validation Accuracies",
            "text": ""
        },
        {
            "heading": "H.1 Training for VQA Task",
            "text": "The hyperparameters for the VQA task are kept same as default values coming from the prior work (Mao et al., 2019). We refer the readers to (Mao et al., 2019) for more details. We obtained a question answering accuracy of 99.3% after training on the VQA task."
        },
        {
            "heading": "H.2 Training Semantic Parser",
            "text": "The semantic parser is trained to parse instructions. Learning of this module happens using the REINFORCE algorithm as described in Section C of this appendix. During REINFORCE algorithm, we search for positive rewards from the set {7, 8, 10}, and negative rewards from the set {0, 2, 3}. We finally choose a positive reward of 8 and negative reward of 2. For making this decision, we first train the semantic parser for 20 epochs and then calculate its accuracy by running it on the quantized scenes from the validation set. For a particular output program, we say it is correct if it leads to an object being selected (see Section C of the appendix for more information) and this is how the accuracy of the semantic parser is calculated. This\naccuracy is a proxy for the real accuracy. An alternative is to use annotated ground truth programs for calculating accuracy and then selecting hyperparameters. However, we do not use ground truth programs. All other hyperparameters are kept the same as used by (Mao et al., 2019) to train the parser on VQA task. We obtain a validation accuracy of 95.64% after training the semantic parser for manipulation instructions."
        },
        {
            "heading": "H.3 Training Manipulation Networks",
            "text": "The architecture details of the manipulation network are present in Section C of this appendix. We use batch size of 32, learning rate of 10\u22123, and optimize using AdamW (Loshchilov and Hutter, 2019) with weight decay of 10\u22124. Rest of the hyperparameters are kept the same as used in (Mao et al., 2019). During training, at every 5th epochs, we calculate the manipulation accuracy by using the query networks that were trained while training the NEUROSIM on VQA data. This serves as a proxy to the validation accuracy.\n\u2022 For the change network training, we use the query accuracy of whether the attribute that was supposed to change for a particular object, has changed correctly or not. Also, whether any other attribute has changed or not.\n\u2022 For the add network training, we use the query accuracy of whether the attributes of the added object are correct or not. Also, whether the added object is in a correct relation with reference object or not.\nWe obtained a validation accuracy (based on querying) of 95.9% for the add network and an accuracy of 99.1% for the change network.\nH.4 Image Decoder Training The architecture of the image decoder is similar to (Johnson et al., 2018) but our input scene graph (having embeddings for nodes and edges) is directly processed by the graph neural network. We use a batch size of 16, learning rate of 10\u22125, and optimize using Adam (Kingma and Ba, 2015) optimizer. The rest of the hyperparameters are same as (Johnson et al., 2018). We train the image decoder for a fixed set of 1000K iterations."
        },
        {
            "heading": "I Qualitative Analysis",
            "text": "Figures 7, 8, 9 compare the images generated by NEUROSIM, TIM-GAN, and GeNeVA on add,\nchange and remove instructions respectively. NEUROSIM\u2019s advantage lies in semantic correctness of manipulated images. For example, see Figure 7 row #3,4; Figure 8 row #2; 9 all images. In these images, NEUROSIM was able to achieve semantically correct changes, while TIM-GAN, GeNeVA faced problems like blurry, smudged objects while adding them to the scene, removing incorrect objects from the scene, or not changing/partially changing the object to be changed. Images generated by TIMGAN are better in quality as compared to NEUROSIM. We believe the reason for this is that TIMGAN, being fully supervised, only changes a small portion of the image and has learned to copy a significant portion of the input image directly to the output. However, this doesn\u2019t ensure the semantic correctness of TIM-GAN\u2019s manipulation, as described above with examples where it makes errors. The images generated by NEUROSIM look slightly worse since the entire image is generated from object based embeddings in the scene graph. Improving neural image rendering from scene graphs can be a promising step to improve NEUROSIM."
        },
        {
            "heading": "J Errors",
            "text": "Figure 10 captures the images generated by our model where it has made errors. The kind of errors that NEUROSIM makes can be broadly classified into three categories.\n\u2022 [Rendering Errors] This set includes images generated by our model which are semantically correct but suffer from rendering errors. The common rendering errors include malformed cubes, partial cubes, change in position of objects, and different lighting.\n\u2022 [Logical Errors] This set includes images generated by our model which have logical errors. That is, manipulation instruction has been interpreted incorrectly and a different manipulation has been performed. This happens mainly due to an incorrect parse of the input instruction into the program, or manipulation network not trained to perfection. For example, change network changing attributes which were supposed to remain unchanged.\n\u2022 [VQA Errors] The query networks are not ideal and have errors after they are trained on the VQA task. This in turn causes errors in supervision (obtained from query networks) while training the\nmanipulation networks and leads to a less than optimally trained manipulation network. Also, during inference, object embeddings may not be perfect due to the imperfections in the visual representation network and that leads to incorrect rendering."
        },
        {
            "heading": "K Ablations",
            "text": "Table 20 shows the performance of NEUROSIM when certain loss terms are removed while learning of the networks. This depicts the importance of loss terms that we have considered. In particular we test the performance of the network by removing\nedge adversarial loss used by add network (row 2), object adversarial losses for both add and change networks (row 3, 5), self supervision losses used by add network (row 4), cyclic (row 6) and consistency (row 7) losses used by change network.\nL Interpretability of NEUROSIM\nNEUROSIM allows for interpretable image manipulation through programs which are generated as an intermediate representation of the input instruction. This is one of the major strengths of NEUROSIM, since it allows humans to detect where NEUROSIM failed. This is not possible with purely neural models, that behave as a black box. Knowing about the\nfailure cases of NEUROSIM also means that it can be selectively trained to improve certain parts of the network (for eg individually training on change instructions to improve the change command, if the model is performing poorly on change instructions). We now assess the correctness of intermediate programs using randomly selected qualitative examples present in Figure 11. Since no wrong program was obtained in the randomly selected set, we find 2 more data points manually, to show some wrong examples."
        },
        {
            "heading": "M Human Evaluation Details",
            "text": "See Table 21 for the questions (paraphrased) asked to the evaluators. Detailed instructions and an example of the questions provided to the evaluators can be found in Figure 12. A total of 10 evaluators, consisting of a mix of undergraduate and post-graduate students, were involved in the study. The same set of 30 random images were given to each evaluator. They were compensated at a rate three times the average hourly salary in the country of origin. Each evaluator was given upto 24 hours to complete the task."
        },
        {
            "heading": "N Simplifying Multi-Hop Instructions using NeuroSIM Modules",
            "text": "In this section, we provide details on our method of utilizing the trained semantic parser to convert the complex multi-hop instruction into a simplified 0 or 1 hop instruction. We generate three simplified\ntemplates one for each edit operation. 1. Change the [attribute] of [size] [color] [material] [shape] to [attribute\u2019] 2. Remove the [size] [color] [material] [shape] 3. Add a [size] [color] [material] [shape] to the [relation] of [shape\u2019]. Next, given a multi-hop instruction we parse it using our semantic parser which gives us the object\u2019s embedding on which either an operation is to be executed (in case of change and remove operations) or a new object has to be inserted in relation to it (in case of add operation). The trained query-networks predicts the symbolic values of the concepts in the placeholders. Example, if the MH instruction is \"Change the size of the big thing that is behind the metallic cylinder behind the purple object that is to the right of the big brown shiny object to tiny\" , we find the placeholder attributes to be operation=change, attribute=size, color=yellow, shape=cube, size=large, material=rubber, attribute\u2019=tiny. Hence the simplified instruction becomes, \"Change the size of the large yellow rubber cube to tiny\". Add and Remove instructions follow similarly."
        }
    ],
    "title": "Image Manipulation via Multi-Hop Instructions - A New Dataset and Weakly-Supervised Neuro-Symbolic Approach",
    "year": 2023
}