{
    "abstractText": "Emotion arcs capture how an individual (or a population) feels over time. They are widely used in industry and research; however, there is little work on evaluating the automatically generated arcs. This is because of the difficulty of establishing the true (gold) emotion arc. Our work, for the first time, systematically and quantitatively evaluates automatically generated emotion arcs. We also compare two common ways of generating emotion arcs: Machine-Learning (ML) models and Lexicon-Only (LexO) methods. By running experiments on 18 diverse datasets in 9 languages, we show that despite being markedly poor at instance level emotion classification, LexO methods are highly accurate at generating emotion arcs when aggregating information from hundreds of instances. We also show, through experiments on six indigenous African languages, as well as Arabic, and Spanish, that automatic translations of English emotion lexicons can be used to generate high-quality emotion arcs in less-resource languages. This opens up avenues for work on emotions in languages from around the world; which is crucial for commerce, public policy, and health research in service of speakers often left behind. Code and resources: https://github.com/dteodore/EmotionArcs",
    "authors": [
        {
            "affiliations": [],
            "name": "Daniela Teodorescu"
        },
        {
            "affiliations": [],
            "name": "Saif M. Mohammad"
        }
    ],
    "id": "SP:e7c94844c1fe8250f5c5bd69376d78865304dbfe",
    "references": [
        {
            "authors": [
                "Cecilia Ovesdotter Alm",
                "Richard Sproat."
            ],
            "title": "Emotional sequencing and development in fairy tales",
            "venue": "Proceedings of the First International Conference on Affective Computing and Intelligent Interaction, ACII\u201905, page 668\u2013674, Berlin, Heidelberg. Springer-",
            "year": 2005
        },
        {
            "authors": [
                "Atsushi Ashida",
                "Masataka Tokumaru",
                "Tomoko Kojiri."
            ],
            "title": "Characters\u2019 emotion design support system for writing novels based on story arcs of target readers",
            "venue": "Information and Technology in Education and Learning, 1:Reg\u2013p005.",
            "year": 2021
        },
        {
            "authors": [
                "Sriharsh Bhyravajjula",
                "Ujwal Narayan",
                "Manish Shrivastava."
            ],
            "title": "Marcus: An event-centric nlp pipeline that generates character arcs from narratives",
            "venue": "Text2Story@ECIR.",
            "year": 2022
        },
        {
            "authors": [
                "Eric Horvitz"
            ],
            "title": "Predicting depression via so",
            "year": 2013
        },
        {
            "authors": [
                "Kristina Toutanova"
            ],
            "title": "2019. BERT: Pre-training",
            "year": 2019
        },
        {
            "authors": [
                "Kacker",
                "Bing She"
            ],
            "title": "Revealing public opin",
            "year": 2021
        },
        {
            "authors": [
                "approach. Saif Mohammad"
            ],
            "title": "From once upon a time",
            "year": 2011
        },
        {
            "authors": [
                "Saif Mohammad."
            ],
            "title": "Best practices in the creation and use of emotion lexicons",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 1825\u20131836, Dubrovnik, Croatia. Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Saif M. Mohammad."
            ],
            "title": "Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 english words",
            "venue": "Proceedings of The Annual Conference of the Association for Computational Linguistics (ACL), Melbourne, Australia.",
            "year": 2018
        },
        {
            "authors": [
                "Saif M. Mohammad"
            ],
            "title": "Practical and ethical considerations in the effective use of emotion and sentiment lexicons",
            "year": 2020
        },
        {
            "authors": [
                "Saif M. Mohammad."
            ],
            "title": "Sentiment analysis: Automatically detecting valence, emotions, and other affectual states from text",
            "venue": "Herb Meiselman, editor, Emotion Measurement (Second Edition). Elsevier.",
            "year": 2021
        },
        {
            "authors": [
                "Saif M. Mohammad."
            ],
            "title": "Ethics sheet for automatic emotion recognition and sentiment analysis",
            "venue": "To Appear in Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Saif M. Mohammad",
                "Felipe Bravo-Marquez",
                "Mohammad Salameh",
                "Svetlana Kiritchenko."
            ],
            "title": "Semeval-2018 Task 1: Affect in tweets",
            "venue": "Proceedings of International Workshop on Semantic Evaluation (SemEval-2018), New Orleans, LA, USA.",
            "year": 2018
        },
        {
            "authors": [
                "Saif M. Mohammad",
                "Mohammad Salameh",
                "Svetlana Kiritchenko."
            ],
            "title": "How translation alters sentiment",
            "venue": "J. Artif. Int. Res., 55(1):95\u2013130.",
            "year": 2016
        },
        {
            "authors": [
                "Saif M. Mohammad",
                "Peter D. Turney."
            ],
            "title": "Crowdsourcing a word-emotion association lexicon",
            "venue": "Computational Intelligence, 29(3):436\u2013465.",
            "year": 2013
        },
        {
            "authors": [
                "Brazdil."
            ],
            "title": "Naijasenti: A nigerian twitter sentiment corpus for multilingual sentiment analysis",
            "venue": "Proceedings of the 13th Language Resources and Evaluation Conference, pages 590\u2013602, Marseille, France. European Language Resources Association.",
            "year": 2022
        },
        {
            "authors": [
                "Emily \u00d6hman."
            ],
            "title": "The validity of lexicon-based sentiment analysis in interdisciplinary research",
            "venue": "Proceedings of the Workshop on Natural Language Processing for Digital Humanities, pages 7\u201312, NIT Silchar, India. NLP Association of India (NLPAI).",
            "year": 2021
        },
        {
            "authors": [
                "Andrew J. Reagan",
                "Lewis Mitchell",
                "Dilan Kiley",
                "Christopher M. Danforth",
                "Peter S. Dodds."
            ],
            "title": "The emotional arcs of stories are dominated by six basic shapes",
            "venue": "EPJ Data Science, 5(1):1\u201312. Copyright EPJ Data Science is a copyright of Springer, 2016;",
            "year": 2016
        },
        {
            "authors": [
                "Sara Rosenthal",
                "Alan Ritter",
                "Preslav Nakov",
                "Veselin Stoyanov."
            ],
            "title": "SemEval-2014 task 9: Sentiment analysis in Twitter",
            "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 73\u201380, Dublin, Ireland. Association for",
            "year": 2014
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Swapna Somasundaran",
                "Xianyang Chen",
                "Michael Flor."
            ],
            "title": "Emotion arcs of student narratives",
            "venue": "Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events, pages 97\u2013107, Online. Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Charles Spearman."
            ],
            "title": "The proof and measurement of association between two things",
            "venue": "The American journal of psychology, 100(3/4):441\u2013471.",
            "year": 1987
        },
        {
            "authors": [
                "Daniela Teodorescu",
                "Tiffany Cheng",
                "Alona Fyshe",
                "Saif M. Mohammad."
            ],
            "title": "Language and mental health: Measures of emotion dynamics from text as linguistic biosocial markers",
            "venue": "The 2023 Conference on Empirical Methods in Natural Language Process-",
            "year": 2023
        },
        {
            "authors": [
                "Daniela Teodorescu",
                "Alona Fyshe",
                "Saif Mohammad."
            ],
            "title": "Utterance emotion dynamics in children\u2019s poems: Emotional changes across age",
            "venue": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis,",
            "year": 2023
        },
        {
            "authors": [
                "Krishnapriya Vishnubhotla",
                "Saif M. Mohammad."
            ],
            "title": "Tweet emotion dynamics: Emotion word usage in tweets from us and canada",
            "venue": "Proceedings of the Thirteenth International Conference on Language Resources and Evaluation (LREC 2022), Marseille,",
            "year": 2022
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le."
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in Neural Information Processing Systems, volume 32. Curran Asso-",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Commercial applications as well as research projects often benefit from accurately tracking the emotions associated with an entity over time. Public health researchers are interested in analyzing social media posts to better understand populationlevel well-being (Vishnubhotla and Mohammad, 2022), loneliness (Guntuku et al., 2019), depression (De Choudhury et al., 2013), etc. Government Policy makers benefit from tracking public opinion over time for developing effective interventions and laws. For example, tracking sentiment towards\n\u2217 Work done while at the University of Alberta and internship with the National Research Council Canada.\nhealth interventions such as mask mandates and vaccine policies (Hu et al., 2021). Researchers in Digital Humanities are interested in understanding basic components of stories such as plot structures, how emotions are associated with compelling characters, categorizing stories based on emotion changes (Reagan et al., 2016), etc. In all of these applications, the goal is to determine whether the degree of a chosen emotion has remained steady, increased, or decreased from one time step to the next. The time steps of consideration may be days, weeks, years, etc. This series of time step\u2013emotion value pairs, which can be represented as a timeseries graph, is often referred to as an emotion arc (Mohammad, 2011; Reagan et al., 2016). Automatic methods generate emotion arcs using:\n\u2022 The text of interest where the individual sentences (or instances) are temporally ordered; possibly through available timestamps indicating when the instances were posted/uttered: e.g., all the tweets relevant to (or mentioning) a government policy along with their metadata that includes the date and time of posting.\n\u2022 The emotion dimension/category of interest: e.g., anxiety, anger, valence, arousal, etc.\n\u2022 The time step granularity of interest (e.g., days, weeks, months etc.)\nThe arcs are generated through a two-step process:\n\u2022 Apply emotion labels to units of text. Two common approaches for labeling units of text are: (1) The Lexicon-Only (LexO) Method: to label words using emotion lexicons and (2) The Machine-Learning (ML) Method: to label whole sentences using supervised ML models (with or without using emotion lexicons).\n\u2022 Aggregate the information to compute time step\u2013emotion value scores; e.g., if using the lexicon approach: for each time step, compute the percentage of anger words in the target text pertaining to each time step, and if\nusing the ML approach: for each time step, compute the percentage of angry sentences for each time step. Despite their wide-spread use in industry and research, there is little work on evaluating the generated emotion arcs (in part due to the difficulty of establishing the true (gold) emotion arc). This is problematic because we should know how accurate the generated emotion arcs are before drawing inferences from them or deploying these systems in applications. Further, different methods of generating emotion arcs (LexO or ML) have different characteristics. For example, LexO methods are interpretable, accessible, compute-friendly, and do not require annotated data (especially for each domain of application) (Mohammad, 2023, 2021; \u00d6hman, 2021); whereas ML methods tend to consider context and longer-range dependencies, making them more accurate at labeling individual instances for emotions. (Table 5 in Appendix A depicts the pros and cons in more detail.) Thus, it is often assumed that ML methods are markedly more accurate than LexO methods for all tasks, including the creation of emotion arcs. However, this assumption may be false; and can only be tested for the task of generating emotion arcs when there is a mechanism to evaluate the arcs.\nA robust and simple setup for evaluating emotion arcs also allows one to test different design decisions in how the arcs are generated. For example, we know little about how best to aggregate information when using emotion lexicons; e.g., is it better to use coarse or fine-grained lexicons, should we ignore slightly emotional words, etc.\nWork on emotions has largely focused on English, excluding the voices, cultures, and perspectives of a majority of people from around the world. Such an exclusion is not only detrimental to those that are excluded, but also to the world as a whole. For example, western-centric science that only draws on data from western languages may falsely claim certain universals about language, emotions, and stories, that when tested on data from other cultures does not hold true anymore. Thus, working with more languages helps discover new paradigms that represent the world better.\nHowever, generating emotion arcs in most languages other than English is stymied by the lack of emotion-labeled resources. Using translated labeled texts with ML methods is problematic (Mohammad et al., 2016; Hogenboom et al., 2014). However, for a number of NLP tasks, systems\nhave benefited from using translations of lexical resources (from say English) into the target language. Thus, high-accuracy LexO methods for creating emotion arcs can open up avenues of work in various languages using (manual or automatic) translations of existing emotion lexicons.\nOur work, for the first time, systematically and quantitatively evaluates automatically generated emotion arcs. We also compare the ML and LexO methods for generating arcs. Note, that our aim is not to build more accurate emotion classification systems, but rather we apply existing methods to evaluate emotion arcs generated across domains and languages. We conducted experiments on 18 datasets in nine languages from diverse domains, including: tweets, SMS messages, reviews, etc. (Two English datasets from movie reviews, four English datasets from SemEval 2014, six datasets from SemEval 2018 (two each in English, Arabic, and Spanish), and six African languages datasets from SemEval 2023 (AfriSenti).)\nEmotions can be characterized by various dimensions and categories (e.g., valence, arousal, dominance vs. anger, fear, joy, sadness). In this work we explore the valence (or sentiment), and therefore generate valence (or sentiment) arcs.1 Our work sets the foundation for future work which can extend analyses to various emotion categories."
        },
        {
            "heading": "2 Related Work",
            "text": "Emotion arcs have commonly been created from literary works and social media content. Alm and Sproat (2005) were the first to automatically classify sentences from literary works for emotions using a machine learning paradigm. Mohammad (2011) was the first to create emotion arcs and analyse the flow of emotions across the narrative in various novels and books using emotion lexicons. Kim et al. (2017) built on this work by creating emotion arcs to determine emotion information for various genres using the NRC Emotion Lexicon (Mohammad and Turney, 2013).2 Reagan et al. (2016) and Del Vecchio et al. (2018) clustered emotion arcs and found evidence for six prototypical arc shapes in stories. Hipson and Mohammad (2021)\n1Terminology: The positive\u2013negative (or pleasure\u2013 displeasure) dimension has long been a focus of study, especially in psychology. They refer to this dimension as valence. However, early NLP work in the area made use of product and movie review datasets, and referred to the dimension as sentiment \u2014 a term that has stuck in subsequent NLP work.\n2http://saifmohammad.com/WebPages/ NRC-Emotion-Lexicon.htm\nanalysed emotion arcs for individual characters (instead of the whole narrative) in movie dialogues. Emotion arcs of literary works have been used to better understand plot and character development (Reagan et al., 2016; Kim et al., 2017; Hipson and Mohammad, 2021); and also for assisting writers develop and improve stories (Ashida et al., 2021; Somasundaran et al., 2020).\nRecently, Hipson and Mohammad (2021) introduced how the patterns with which emotions change over time \u2014 emotion dynamics \u2014 can be inferred from text. Vishnubhotla and Mohammad (2022) computed UEDs from tweets and analysed how they have changed over the years. Teodorescu et al. (2023b) studied emotional development in children\u2019s writing and found meaningful patterns across age. In the public health domain, Teodorescu et al. (2023a) created emotion arcs for tweeters who self-disclosed as having a mental health diagnosis (on Twitter). They found that certain emotion dynamics patterns differed significantly between tweeters with mental health conditions (e.g., attention deficit hyperactivity disorder, depression, bipolar) compared to the control group.\nHowever, there is surprisingly little work on arc evaluation. A key reason for this is that it is hard to determine the true emotion arc of a story from data annotation. One attempt to evaluate aspects of an emotion arc can be seen in Bhyravajjula et al. (2022). They asked one volunteer to read minisegments of a \u2018The Lord of the Rings\u2019 novel to determine whether the protagonist\u2019s circumstance undergoes a positive or negative shift. They then determined the extent to which the automatic method captured the same shifts. Here we propose a simpler alternative way to robustly evaluate automatically generated emotion arcs on a wide variety of domains and multiple languages."
        },
        {
            "heading": "3 Experimental Setup to Evaluate Automatically Generated Emotion Arcs",
            "text": "We begin by describing the evaluation setup. This is a key contribution of this work since no prior work systematically evaluates automatically generated emotion arcs. We construct gold emotion arcs from existing datasets where individual instances are manually annotated for valence (sentiment). Here, an instance could be a tweet, a sentence from a customer review, a sentence from a personal blog, etc. Depending on the dataset, manual annotations for an instance may represent the emotion of a\nspeaker, or sentiment towards a product or entity. For a pre-chosen bin size of say 100 instances per bin, we compute the gold emotion score by taking the average of the human-labeled emotion scores of the instances in that bin (in-line with the commerce and social media use cases discussed earlier). We move the window forward by one instance, compute the average in that bin, and so on. (Using larger window sizes does not impact conclusions.) We created text streams for the experiments in Section 4 by ordering instances from a dataset by increasing gold score before binning. This created arcs with a more consistent emotion change rate and we first explore results in this scenario. In Section 7, we look at the impact of more dynamic emotion changes (e.g., varying peak heights and widths) on performance.\nWe automatically generate emotion arcs using the LexO and ML methods discussed in the two sections ahead. We standardized all arcs (aka zscore normalization) so that the gold arcs are comparable to automatically generated arcs.3 Finally, we evaluate the closeness of automatically generated emotion arcs with gold arcs using two metrics: linear correlation and Root Mean Squared Error (RMSE). We use Spearman rank correlation (Spearman, 1987) (range: -1 to 1). High correlation implies greater fidelity: no matter what the overall shape of the gold emotion arc, when it goes up (or down), the predicted emotion arc also goes up (or down).4 RMSE (range: 0\u2013\u221e) is a measure of the error between the true and predicted values. It penalizes bigger errors more, and thus is sensitive to outliers. Scores closer to 0 indicate better predictions. In our experiments, the correlation and RMSE scores are determined for thousands of points between the predicted and gold arcs.5\nTable 1 shows key details of the English instancelabeled datasets. To determine whether using automatic translations of English lexicons into relatively less-resource languages is a viable option, we also experiment with instance-labeled datasets in Arabic (Ar), Spanish (Es), Amharic (Am), Hausa (Ha), Igbo (Ig), Kinyarwanda (Kr), Swahili (Sw), and Yoruba (Yo). Just as the English set, these contain original tweets (not translated) with emotion\n3Subtract the mean from the score and divide by the standard deviation (arcs then have zero mean and unit variance).\n4Different orderings of a given dataset produce different shapes; however, the Spearman correlation between the automatic and gold arcs for that dataset remains the same.\n5For example, in a dataset with 3K instances, when using a rolling window of bin size 300, the arc consists of 2700 points.\nlabels by native speakers. The datasets are of two kinds: those with categorical labels such as the SemEval 2014, which has -1 (negative), 0 (neutral), and 1 (positive), as well as those with continuous labels such as SemEval 2018 (V-Reg), which has real-valued sentiment intensity scores between 0 (lowest/no intensity) and 1 (highest intensity).\nIn all, we conducted experiments with 18 emotion-labeled datasets from nine languages, with labels that are either categorical or continuous, for valence. The results also establish key benchmarks; useful to practitioners for estimating the quality of the arcs under various settings."
        },
        {
            "heading": "4 LexO Arcs: Emotion Arcs Generated from Counting Emotion Words",
            "text": "Recall that in the Introduction we described how emotion arcs are automatically generated. Key parameters in that process include bin size, type of emotion lexicon used (e.g., categorical or continuous emotion scores), and how to handle terms in the text that are not in the lexicon (OOV terms).\nChoice of bin size depends on the application and available data. For example, if a company wants to know how the proportion of angry comments has changed from month to month in the last 10 years, then the bin size to use is month. If instead, they want to know how things changed on a day-by-day basis over the last 45 days, then the bin size to use is a day. Depending on the volume of relevant data and the bin size chosen, the bins may include a higher or lower number of instances. For our experiments we explored various bin sizes (1, 10, 30, 50, 100, 200, and 300).\nTable 2 shows the English emotion lexicon we used: The NRC VAD Lexicon.6 It has both categorical and real-valued versions, which is useful to study whether using fine-grained lexicons leads to\n6http://saifmohammad.com/WebPages/nrc-vad.html\nmarkedly better emotion arcs. The two OOV handling methods explored were: 1. Assign label NA (no score available) and disregard these words, and 2. Assign 0 score (neutral or not associated with emotion category), thereby, leading to a lower average score for the instance than if the word was disregarded completely.\nWe then evaluated how closely the arcs correspond to the gold valence arcs. In Appendix B, we show the impact of using emotion lexicons more selectively\u2014using only the entries that have an emotion association greater than some predetermined threshold. Results (Valence): Figure 1 shows the correlations and RMSE values between predicted valence arcs and the gold valence arcs. Bin Size: Overall, across datasets and regardless of the type of lexicon used and how OOV words are handled, increasing the bin size dramatically improves correlation with the gold arcs. In fact, with bin sizes as small as 50, many of the generated arcs have correlations above 0.9. With bin size 100 and above correlations approach high 0.90\u2019s. Similarly, RMSE starts at approximately 1.20 to 1.10 at bin size 1, and quickly drops as bin size increases. At bin size 300, RMSE approaches quite low scores (0.3\u20130.1).\nIf we use a bin size of 1, we only get the ups and downs of the emotions associated with individual words, which will likely be quite far off from the true emotion arc. As bin size increases (more data per bin), the predicted arc gets closer to the true arc probably because:\nIf bin x is expressing more of an emotion (higher emotion intensity) than bin y, then x has a higher average word\u2013emotion association score than y.\nCategorical vs. Real-Valued Lexicons: Using a realvalued lexicon obtains higher correlations across bin sizes, methods for processing OOV terms, and\ndatasets. The difference is marked for very small bin sizes (such as 1 and 10) but progressively smaller for higher bin sizes. Entries from realvalued lexicons carry more fine-grained emotion information, and it is likely that this extra information is especially helpful when arcs are determined from very little text (as in the case of small bins). OOV Terms: Results with the two OOV-handling methods were comparable (no clear winner). Discussion: For many social media applications, one has access to tens of thousands, if not millions of posts. There, it is not uncommon to have timesteps (bins) that include thousands of instances. Thus, it is remarkable that even with relatively small bin sizes of a few hundred, the simple lexicon approach is able to obtain very high correlations. Of course, the point is not that the lexicon approach is somehow special, but rather that aggregation of information can very quickly generate high quality arcs, even if the constituent individual emotion signals are somewhat weak."
        },
        {
            "heading": "5 ML Arcs: Emotion Arcs Generated from Counting ML-Labeled Sentences",
            "text": "This section explores how the accuracy of instancelevel (sentence- or tweet-level) emotion labeling impacts the quality of the generated emotion arcs. We approached this by creating an \u2018oracle\u2019 system, which has access to the gold instance emotion labels.\nThere are several metrics for evaluating sentiment analysis at the instance level such as accuracy, correlation, or F-score. However, we focus on accuracy as it is a simple intuitive metric. Inputs to the Oracle system are a dataset of text (for emotion labelling) and a level of accuracy (e.g., 90% accuracy) to perform instance-level emotion labelling at. Then, the system goes through each instance and predicts the correct emotion label with a probability corresponding to the pre-chosen accuracy. In the case where the system decides to assign an incorrect label, it chooses one of the possible incorrect labels at random.\nWe use the Oracle to generate emotion labels pertaining to various levels of accuracy, for the same dataset. We then generate emotion arcs just as described in the previous section (by taking the average of the scores for each instance in a bin), and evaluate the generated arcs just as before (by determining correlation with the gold arc). This Oracle System allows us to see how accurate an instance level emotion labelling approach needs to be to obtain various levels of quality when generating emotion arcs.\nFigure 2 shows the correlations of the valence arcs generated using the Oracle System with the gold valence arcs created from the SemEval 2018 VOC test set (that has 7 possible labels: -3 to 3).7 We observe that, as expected the Oracle Systems with instance-level accuracy greater than approximately the random baseline (14.3% for this dataset) obtain positive correlations; whereas those with less than 14% accuracy obtain negative correlations. As seen with the results of the previous section, correlations increase markedly with increase in bin size. Even with an instance-level accuracy of 60%, correlation approaches 1 at larger bin sizes. Overall, we again observe high quality emotion arcs with bin sizes of just a few hundred instances.\nTable 3 shows the correlations obtained on the same dataset when using various deep neural network and transformer-based systems. Observe that the recent systems obtain nearly perfect correlation\n7Figure 5 (Appendix) shows similar Oracle System results for other datasets.\nat bin sizes 200 and 300. However, for a given application scenario, these results can only be obtained when the machine learning system is able to train on sufficient domain-specific training data (which is often scarce), the computer power, and knowledge to work with these systems is accessible. For applications where simple, interpretable, low-cost, and low-carbon-footprint systems are desired, the lexicon-based systems described in the previous section, are often more suitable."
        },
        {
            "heading": "6 LexO Arcs: Emotion Arcs Generated from Translated Emotion Lexicons",
            "text": "Given the competitive performance of the LexiconOnly (LexO) method, and the many benefits of the LexO method such as their simplicity and interpretability, we now explore whether high-quality emotion arcs can be created for low-resourced languages using automatic translations of English emotion lexicons. Specifically, we make use of translations of the NRC lexicons from English into the language of interest and perform similar experiments as described in the previous section.8 We explore bin sizes of 400 and 500 as well, as we expect that instance-level accuracy will be lower in the translated-lexicon case.\nWe would like to note that automatic translations may not be available for many very low resource languages. Currently, Google Translate has functionality to translate across about 120 languages.\n8The NRC Emotion and VAD Lexicon packages come with translations into over 100 languages.\nThis may seem large (and it is indeed a massive improvement from just a decade back), but thousands of languages still remain on the other side of the digital divide. Thus, even requiring word translations is a significant limitation for many languages.\nWe generated and evaluated emotion arcs in six African languages: Amharic, Hausa, Igbo, Kinyarwanda, Swahili, and Yoruba (using the datasets shown in Table 4). These are some of the most widely spoken indigenous African languages.9 Table 6 in the Appendix presents details about each, including their language family and the primary regions where they are spoken. Note that these languages themselves are diverse covering three different language families: Afroasiatic, Bantu, and Niger-Congo. Swahili, is more influenced by the Indo-European languages than Hausa \u2014about 40% of its vocabulary is made up of Arabic loan words. We contrast results on these languages to Arabic, a commonly spoken language in Northern Africa which has somewhat more resources than indigenous African languages. Arabic still has much fewer NLP resources than English. We also contrast our results to Spanish, which has fewer resources than English; and is more similar to English than Arabic. (The Ar and Es valencelabeled datasets used are shown in Table 7 in the Appendix.) High fidelity of the predicted arcs with the gold arcs will unlock the potential for research in affect-related psychology, health science, and digital humanities for a vast number of languages."
        },
        {
            "heading": "6.1 Valence Arcs: Using Translated Lexicons",
            "text": "We ran a wide range of experiments in Arabic, Spanish, and the African languages with all the parameter variations discussed earlier. We found the same trends for the OOV handling and lexicon granularity parameters for these languages as for English. Therefore, for brevity, Figure 3 only shows the results with using real-valued lexicons (which performed better than categorical score lexicons)\n9https://www.pangea.global/blog/2018/07/19/ 10-most-popular-african-languages/\nand assign label NA to OOV words (which was comparable to the \u2018assign neutral score\u2019 method). Figure 3 shows the results for generating emotion arcs using translations of English lexicons into Arabic, Spanish, and the African languages for both categorically (e.g., V-OC and SemEval 2023) and continuously (e.g., V-Reg) labelled valence datasets. We also provide the English results on the corresponding datasets for easy comparison.\nFor Arabic and Spanish (V-OC and V-Reg datasets): Correlations start at 0.40\u20130.50 at the instance-level (bin size 1), and reach 0.98\u20130.99 from bin sizes 200 and onwards. Performance using translations of English lexicons does surprisingly well. At bin size 1 English does performs better by about 10 points, however this difference quickly dissipates at bin size 50 and onwards.\nFor the African language texts (the SemEval 2023 datasets): Correlations start at lower values at the instance-level, than for Arabic and Spanish. At bin size 1, correlations are about 0.1\u20130.2 with the exception of Hausa at 0.374. As seen previously, with increasing bin size we are able to gain substantial improvements in performance by aggregating information. With a bin size of 200 and onwards correlations reach mid 0.90\u2019s which is a large gain in performance compared to instance-level. Among the African languages, at bin size 1\u201350 arcs for Hausa are by far the best, followed by arcs for Amharic; then arcs for Igbo, Kinyarwanda, and Swahili perform similarly; followed by Yoruba. For bin sizes 100 and onwards, Hausa and Kinyarwanda become the best performing, followed by Igbo and Amharic performing similarly, followed by Yoruba. Discussion: Using translations of English lexicons allows us to create high-quality emotion arcs in African languages. The variation in performance across the African languages could be because of the variation in quality of automatic translations of the emotion lexicons across languages and different inter-annotator agreements for the instance-level emotion labels in the different datasets.\nAlthough the results for Arabic and the African languages are on different datasets, it is of interest to contrast the performance of Arabic to the African languages because Google\u2019s translations of words in the lexicon to Arabic is expected to be of higher quality than to the African languages (due to the relatively higher amounts of English\u2013Arabic parallel text). Overall, it is interesting to note that the translation method creates relatively better arcs when the target language is closer to the source language (e.g., English to Spanish). However, even for distant languages, performance can be improved by increasing bin size (e.g., some African languages). Of course, increasing bin size means lower granularity, but in many application scenarios, and for drawing broad trends, it makes sense to aggregate hundreds or thousands of instances in each bin, leading to more reliable inferences about emotion trends over time."
        },
        {
            "heading": "7 Arc Quality under Varying Emotion Amplitudes and Spikiness",
            "text": "Aspects of how emotions change across the bins also impacts the quality of generated arcs. For example, for a given surge or dip of emotion, if the change of emotion strength is too small in magnitude (small amplitude) or occurs for too short of a time period (high spikiness), then the automatic method may fail to register the surge/dip. We will refer to arcs with many such emotion changes as more dynamic than those with more gradual and longer-duration emotion changes.\nRecall that in Section 4 we used text streams ordered by increasing gold score. Now, to test the\nrobustness of the LexO method, when dealing with dynamic emotion changes, we created new text streams by reordering the tweets in the emotionlabeled datasets in more random and dramatic ways. We created these dynamic text streams by sampling tweets from the chosen dataset with replacement until 200 crests and 200 troughs of various amplitudes and spikiness were obtained.10 The gold arc is then standardized as before. We will refer to this new text stream generated from a dataset as [dataset_name]-dynamic, and the gold arcs created with this process as dynamic gold arcs.\nThe gold line in Figure 4 shows the beginning portion of the gold valence arc produced from the SemEval 2018 (V-Reg) dataset (bin size = 100). Observe that some peaks are rather small in amplitude whereas some others are greater than two standard deviations. Also, some emotion changes occur across a wider span of data (x-axis) whereas some others show large emotion change and back in a short span.\nWe then generated the predicted arc for this dataset using the LexO method, the NRC VAD lexicon, and by ignoring terms not found in the lexicon when determining bin scores. The green line in Figure 4 shows the beginning portion of the gold valence arc when using a rolling window with bin size 100. Observe that the green line traces the gold line quite closely. There are also occasions where it overshoots or fails to reach the gold arc. Figure 4 shows gold and predicted arcs for the Hausa dataset (bin size = 300). Observe that here\n10Sampling with replacement allowed reuse of the limited labeled data to produce a long wave.\nthe predicted arc still follows the gold arc closely (but as expected, not as closely as a.)\nIn experiments modifying bin size, the predicted arcs obtain a high correlation (above 0.9) with a bin size of 100 (and above), even for the dynamic text stream. However, these scores are (as expected) consistently lower than what were observed for the corresponding non-dynamic text stream. Note that the difference in scores is more pronounced for the smaller bin sizes than for larger ones. Therefore, in scenarios with smaller amounts of data per bin, judicious use of parameters described in Section 4 (e.g., treating OOVs as neutrals, using real-valued lexicons, etc.) can have marked influence on arc quality. Overall, though, these results show that the LexO method performs rather well even in the case of highly dynamic emotion changes."
        },
        {
            "heading": "8 Concluding Remarks",
            "text": "This work made contributions in two broad directions: First, we showed how methods for predicting emotion arcs can be evaluated, and used it to systematically and quantitatively evaluate both Lexicon-Only and Machine Learning methods. Second, we showed that using translations of English lexicons into the language of interest can generate high quality emotion arcs, even for languages that are very different from English.\nBoth ML and LexO methods produced highquality emotion arcs when using bin size 50 and above (e.g., with bin size 100: obtaining correlations with gold valence arcs exceeding 0.98). ML methods are able to obtain markedly higher correlations at very low bin sizes (<50). However,\nwith the abundance of textual data available from social media, for many applications where one is aggregating hundreds or thousands of instances in each bin, the gains of ML methods in terms of correlation scores are miniscule.\nWith the cross-lingual experiments, we created emotion arcs from texts in six indigenous African languages, as well as Arabic and Spanish. For all of them, emotion arcs obtained high correlations as the bin size (aggregation level) was set to at least a few hundred instances. Correlations between predicted and gold emotion arcs were in general higher when the target language was closer to English.\nIn the last part of the paper, we explored how depending on the data available and the desired granularity of emotion arcs interacts with performance for various arc amplitudes and arc spikiness. We showed that the LexO method performs rather well even in the case of highly dynamic emotion changes. However, for smaller bin sizes, the predicted arc may markedly overshoot or not fully capture the gold peak or trough.\nIn all, we conducted experiments with 18 sentiment-labeled datasets from 9 languages, Thus the conclusions drawn are rather robust. The results on individual datasets also establish key benchmarks; useful to practitioners for estimating the quality of the arcs under various settings. Finally, since this work shows that simple lexicon-only approaches produce accurate arcs, practitioners from all fields, those working in low-resource languages, those interested in interpretability, or those without the resources to deploy neural models, can easily generate high-quality emotion arcs for their data."
        },
        {
            "heading": "9 Limitations",
            "text": "It is challenging to determine the true emotion arc of a text stream or story through data annotation. This is because usually data annotation is performed on smaller pieces of text such as sentences or paragraphs. It is difficult for people to read a large body of text, say from a novel, and produce consistent annotations for how emotions have changed as the text has progressed from start to finish. In the context of tweets, taking the average emotion scores of individual sentences is a reasonable option (as we have done here); however, that may not capture true emotions when dealing with large pieces of text written by the same person, for example, emotion arcs of stories in a novel. We hope future work will determine how to create gold arcs in such cases, and how different such arcs are from the arcs created by averaging the scores of individual sentences.\nConstructing emotion lexicons from human annotations of emotion scores takes time and resources. Thus, these resources exist only for a handful of languages. As a feasible approach in the meanwhile, we uses automatic translations of emotion lexicons from English to a desired target language to generate emotion arcs in various languages. While this approach produces highly accurate emotion arcs, there are several considerations: different languages and cultures express emotions differently, there may be lexical gaps among languages, and characteristics and meaning can be lost in the translation. Therefore, one would expect that using human annotated lexicons would lead to higher emotion arc quality."
        },
        {
            "heading": "10 Ethics",
            "text": "Our research interest is to study emotions at an aggregate/group level. This has applications in determining public policy (e.g., pandemic-response initiatives) and commerce (understanding attitudes towards products). However, emotions are complex, private, and central to an individual\u2019s experience. Additionally, each individual expresses emotion differently through language, which results in large amounts of variation. Therefore, several ethical considerations should be accounted for when performing any textual analysis of emotions (Mohammad, 2022, 2020). The ones we would particularly like to highlight are listed below:\n\u2022 Our work on studying emotion word usage should not be construed as detecting how people feel; rather, we draw inferences on the emotions that are conveyed by users via the language that they use.\n\u2022 The language used in an utterance may convey information about the emotional state (or perceived emotional state) of the speaker, listener, or someone mentioned in the utterance. However, it is not sufficient for accurately determining any of their momentary emotional states. Deciphering the true momentary emotional state of an individual requires extralinguistic context and world knowledge. Even then, one can be easily mistaken.\n\u2022 The inferences we draw in this paper are based on aggregate trends across large populations. We do not draw conclusions about specific individuals or momentary emotional states."
        },
        {
            "heading": "Acknowledgments",
            "text": "Thank you to Krishnapriya Vishnubhotla for the Emotion Dynamics code-base, which set the ground work for computing instance average emotion scores for instances. This research was supported by NSERC, Digital Research Alliance of Canada (alliancecan.ca), Alberta Innovates, and DeepMind. This research project is funded by the Bavarian Research Institute for Digital Transformation (bidt), an institute of the Bavarian Academy of Sciences and Humanities. The author is responsible for the content of this publication."
        },
        {
            "heading": "A LexO vs. ML Methods for Emotion Labelling",
            "text": "In Table 5 we show the characteristics of the Lexicon-Only (LexO) and Machine Learning (ML) based methods for emotion labelling instances.\nB Impact of Selectively Using the Lexicon\nThe continuously labeled emotion lexicons include words that may be very mildly associated with an emotion category or dimension. It is possible that the very low emotion association entries may in fact mislead the system, resulting in poor emotion arcs. We therefore also investigated the quality of emotion arcs by generating them only from terms with an emotion association score greater than a pre-chosen threshold; thereby using the lexicon entries more selectively. We systematically varied the threshold to study what patterns of threshold lead to better arcs across the emotion test datasets.11\nOverall, we observed that valence benefits from including all terms, even lowly associated emotion words, as the optimal threshold across continuous and categorically datasets is 0 with a few notable exceptions (SemEval 2014 LiveJournal, SemEval 2014 tweets, and V-OC). Generally, including only terms with emotion scores above 0.33 to 0.5 improves the quality of emotion arcs.\nC Impact of the Quality of Instance-Level Emotion Labeling on Emotion Arcs\nFigure 5 shows the results for the Oracle System on the categorically labeled SemEval 2014 datasets. We observe similar patterns as discussed in the paper for the SemEval 2018 V-OC dataset. (Note that the instance-level random-guess baseline is dependent on the number of class labels; thus, the minimum Oracle System Accuracy at which positive correlations with gold arcs appear is different across the datasets.)\n11Note that our goal is not to determine the predictive power of the system on new unseen test data. For that one would have to determine thresholds from a development set, and apply the model on unseen test data."
        },
        {
            "heading": "D African Languages",
            "text": "Information on the African languages used in our study are shown in Table 6."
        },
        {
            "heading": "E Emotion Labelled Datasets",
            "text": "In Table 7, we shown the emotion labelled instances for Arabic, and Spanish languages."
        },
        {
            "heading": "F Code and Resources",
            "text": "The code and resources used are made freely available on the project homepage.12 The code allows for easy generation of high-quality emotion arcs for the provided text stream (especially useful for those outside of computer science).\n12https://github.com/dteodore/EmotionArcs"
        }
    ],
    "title": "Evaluating Emotion Arcs Across Languages: Bridging the Global Divide in Sentiment Analysis",
    "year": 2023
}