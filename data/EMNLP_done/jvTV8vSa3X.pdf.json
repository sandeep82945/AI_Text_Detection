{
    "abstractText": "3D human modeling has been widely used for engaging interaction in gaming, film, and animation. The customization of these characters is crucial for creativity and scalability, which highlights the importance of controllability. In this work, we introduce Text-guided 3D Human Generation (T3H), where a model is to generate a 3D human, guided by the fashion description. There are two goals: 1) the 3D human should render articulately, and 2) its outfit is controlled by the given text. To address this T3H task, we propose Compositional Cross-modal Human (CCH). CCH adopts cross-modal attention to fuse compositional human rendering with the extracted fashion semantics. Each human body part perceives relevant textual guidance as its visual patterns. We incorporate the human prior and semantic discrimination to enhance 3D geometry transformation and fine-grained consistency, enabling this to learn from 2D collections for data efficiency. We conduct evaluations on DeepFashion and SHHQ with diverse fashion attributes covering the shape, fabric, and color of upper and lower clothing. Extensive experiments demonstrate that CCH achieves superior results for T3H with high efficiency.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tsu-Jui Fu"
        },
        {
            "affiliations": [],
            "name": "Wenhan Xiong"
        },
        {
            "affiliations": [],
            "name": "Yixin Nie"
        },
        {
            "affiliations": [],
            "name": "Jingyu Liu"
        },
        {
            "affiliations": [],
            "name": "Barlas O\u011fuz"
        },
        {
            "affiliations": [],
            "name": "William Yang Wang"
        }
    ],
    "id": "SP:52ac2934fa4b525c904c22b5554aac3e8e3531b7",
    "references": [
        {
            "authors": [
                "Mykhaylo Andriluka",
                "Leonid Pishchulin",
                "Peter Gehler",
                "Bernt Schiele."
            ],
            "title": "2D Human Pose Estimation: New Benchmark and State of the Art Analysis",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2014
        },
        {
            "authors": [
                "Jonathan T. Barron",
                "Ben Mildenhall",
                "Matthew Tancik",
                "Peter Hedman",
                "Ricardo Martin-Brualla",
                "Pratul P. Srinivasan."
            ],
            "title": "Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields",
            "venue": "Conference on Computer Vision and Pat-",
            "year": 2022
        },
        {
            "authors": [
                "Federica Bogo",
                "Angjoo Kanazawa",
                "Christoph Lassner",
                "Peter Gehler",
                "Javier Romero",
                "Michael J. Black."
            ],
            "title": "Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image",
            "venue": "European Conference on Computer Vision (ECCV).",
            "year": 2016
        },
        {
            "authors": [
                "Zhe Cao",
                "Gines Hidalgo",
                "Tomas Simon",
                "Shih-En Wei",
                "Yaser Sheikh."
            ],
            "title": "OpenPose: Realtime MultiPerson 2D Pose Estimation using Part Affinity Fields",
            "venue": "Transactions on Pattern Analysis and Machine Intelligence (TPAMI).",
            "year": 2019
        },
        {
            "authors": [
                "Eric R. Chan",
                "Connor Z. Lin",
                "Matthew A. Chan",
                "Koki Nagano",
                "Boxiao Pan",
                "Shalini De Mello",
                "Orazio Gallo",
                "Leonidas Guibas",
                "Jonathan Tremblay",
                "Sameh Khamis",
                "Tero Karras",
                "Gordon Wetzstein"
            ],
            "title": "Efficient Geometry-aware 3D Generative Adversarial",
            "year": 2022
        },
        {
            "authors": [
                "Eric R. Chan",
                "Marco Monteiro",
                "Petr Kellnhofer",
                "Jiajun Wu",
                "Gordon Wetzstein."
            ],
            "title": "pi-GAN: Periodic Implicit Generative Adversarial Networks for 3DAware Image Synthesis",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2021
        },
        {
            "authors": [
                "Mingfei Chen",
                "Jianfeng Zhang",
                "Xiangyu Xu",
                "Lijuan Liu",
                "Yujun Cai",
                "Jiashi Feng",
                "Shuicheng Yan."
            ],
            "title": "Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural Human Rendering",
            "venue": "Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Zhiqin Chen",
                "Hao Zhang."
            ],
            "title": "Learning Implicit Fields for Generative Shape Modeling",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2019
        },
        {
            "authors": [
                "Alvaro Collet",
                "Ming Chuang",
                "Pat Sweeney",
                "Don Gillett",
                "Dennis Evseev",
                "David Calabrese",
                "Hugues Hoppe",
                "Adam Kirk",
                "Steve Sullivan."
            ],
            "title": "High-quality Streamable Free-viewpoint Video",
            "venue": "Transactions on Graphics (TOG).",
            "year": 2015
        },
        {
            "authors": [
                "Ming Ding",
                "Zhuoyi Yang",
                "Wenyi Hong",
                "Wendi Zheng",
                "Chang Zhou",
                "Da Yin",
                "Junyang Lin",
                "Xu Zou",
                "Zhou Shao",
                "Hongxia Yang",
                "Jie Tang."
            ],
            "title": "CogView: Mastering Text-to-Image Generation via Transformers",
            "venue": "Conference on Neural Information Processing",
            "year": 2021
        },
        {
            "authors": [
                "Alaaeldin El-Nouby",
                "Shikhar Sharma",
                "Hannes Schulz",
                "Devon Hjelm",
                "Layla El Asri",
                "Samira Ebrahimi Kahou",
                "Yoshua Bengio",
                "Graham W.Taylor."
            ],
            "title": "Tell, Draw, and Repeat: Generating and Modifying Images Based on Continual Linguistic Instruction",
            "venue": "Inter-",
            "year": 2019
        },
        {
            "authors": [
                "Patrick Esser",
                "Robin Rombach",
                "Bj\u00f6rn Ommer."
            ],
            "title": "Taming Transformers for High-Resolution Image Synthesis",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2021
        },
        {
            "authors": [
                "Weixi Feng",
                "Xuehai He",
                "Tsu-Jui Fu",
                "Varun Jampani",
                "Arjun Reddy Akula",
                "Pradyumna Narayana",
                "Sugato Basu",
                "Xin Eric Wang",
                "William Yang Wang."
            ],
            "title": "Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis",
            "venue": "In-",
            "year": 2023
        },
        {
            "authors": [
                "Jianglin Fu",
                "Shikai Li",
                "Yuming Jiang",
                "Kwan-Yee Lin",
                "Chen Qian",
                "Chen Change Loy",
                "Wayne Wu",
                "Ziwei Liu."
            ],
            "title": "StyleGAN-Human: A Data-Centric Odyssey of Human Generation",
            "venue": "European Conference on Computer Vision (ECCV).",
            "year": 2022
        },
        {
            "authors": [
                "Tsu-Jui Fu",
                "Xin Eric Wang",
                "Scott Grafton",
                "Miguel Eckstein",
                "William Yang Wang."
            ],
            "title": "SSCR: Iterative Language-Based Image Editing via SelfSupervised Counterfactual Reasoning",
            "venue": "Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Tsu-Jui Fu",
                "Xin Eric Wang",
                "Scott Grafton",
                "Miguel Eckstein",
                "William Yang Wang."
            ],
            "title": "M3L: Language-based Video Editing via Multi-Modal Multi-Level Transformer",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2022
        },
        {
            "authors": [
                "Tsu-Jui Fu",
                "Xin Eric Wang",
                "William Yang Wang."
            ],
            "title": "Language-Driven Artistic Style Transfer",
            "venue": "European Conference on Computer Vision (ECCV).",
            "year": 2022
        },
        {
            "authors": [
                "Tsu-Jui Fu",
                "Licheng Yu",
                "Ning Zhang",
                "Cheng-Yang Fu",
                "Jong-Chyi Su",
                "William Yang Wang",
                "Sean Bell."
            ],
            "title": "Tell Me What Happened: Unifying Textguided Video Completion via Multimodal Masked Video Generation",
            "venue": "Conference on Computer Vi-",
            "year": 2023
        },
        {
            "authors": [
                "Lin Gao",
                "Jie Yang",
                "Tong Wu",
                "Yu-Jie Yuan",
                "Hongbo Fu",
                "Yu-Kun Lai",
                "Hao Zhang"
            ],
            "title": "SDM-NET: Deep Generative Network for Structured Deformable Mesh",
            "year": 2019
        },
        {
            "authors": [
                "Xiangjun Gao",
                "Jiaolong Yang",
                "Jongyoo Kim",
                "Sida Peng",
                "Zicheng Liu",
                "Xin Tong."
            ],
            "title": "MPS-NeRF: Generalizable 3D Human Rendering from Multiview Images",
            "venue": "Transactions on Pattern Analysis and Machine Intelligence (TPAMI).",
            "year": 2022
        },
        {
            "authors": [
                "Thiago L. Gomes",
                "Thiago M. Coutinho",
                "Rafael Azevedo",
                "Renato Martins",
                "Erickson R. Nascimento."
            ],
            "title": "Creating and Reenacting Controllable 3D Humans with Differentiable Rendering",
            "venue": "Winter Conference on Applications of Computer Vision (WACV).",
            "year": 2022
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio."
            ],
            "title": "Generative Adversarial Networks",
            "venue": "Conference on Neural Information Processing Systems (NeurIPS).",
            "year": 2015
        },
        {
            "authors": [
                "Amos Gropp",
                "Lior Yariv",
                "Niv Haim",
                "Matan Atzmon",
                "Yaron Lipman."
            ],
            "title": "Implicit Geometric Regularization for Learning Shapes",
            "venue": "International Conference on Machine Learning (ICML).",
            "year": 2020
        },
        {
            "authors": [
                "Jiatao Gu",
                "Lingjie Liu",
                "Peng Wang",
                "Christian Theobalt."
            ],
            "title": "StyleNeRF: A Style-based 3DAware Generator for High-resolution Image Synthesis",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2022
        },
        {
            "authors": [
                "Dou",
                "Sean Fanellov",
                "Graham Fyffe",
                "Christoph Rhemannv",
                "Jonathan Taylor",
                "Paul Debevec",
                "Shahram Izadi."
            ],
            "title": "The Relightables: Volumetric Performance Capture of Humans with Realistic Relighting",
            "venue": "Transactions on Graphics (TOG).",
            "year": 2019
        },
        {
            "authors": [
                "Paul Henderson",
                "Vagia Tsiminaki",
                "Christoph H. Lampert."
            ],
            "title": "Leveraging 2D Data to Learn Textured 3D Mesh Generation",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2020
        },
        {
            "authors": [
                "Jack Hessel",
                "Ari Holtzman",
                "Maxwell Forbes",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "CLIPScore: A Reference-free Evaluation Metric for Image Captioning",
            "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2021
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter."
            ],
            "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium",
            "venue": "Conference on Neural Information Processing Systems (NeurIPS).",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel."
            ],
            "title": "Denoising Diffusion Probabilistic Models",
            "venue": "Conference on Neural Information Processing Systems (NeurIPS).",
            "year": 2020
        },
        {
            "authors": [
                "Fangzhou Hong",
                "Zhaoxi Chen",
                "Yushi Lan",
                "Liang Pan",
                "Ziwei Liu."
            ],
            "title": "EVA3D: Compositional 3D Human Generation from 2D Image Collections",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2023
        },
        {
            "authors": [
                "Fangzhou Hong",
                "Mingyuan Zhang",
                "Liang Pan",
                "Zhongang Cai",
                "Lei Yang",
                "Ziwei Liu."
            ],
            "title": "AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars",
            "venue": "Special Interest Group on Computer Graphics and Interactive Techniques",
            "year": 2022
        },
        {
            "authors": [
                "Gabriel Ilharco",
                "Mitchell Wortsman",
                "Ross Wightman",
                "Cade Gordon",
                "Nicholas Carlini",
                "Rohan Taori",
                "Achal Dave",
                "Vaishaal Shankar",
                "Hongseok Namkoong",
                "John Miller",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Ludwig Schmidt."
            ],
            "title": "OpenCLIP",
            "venue": "https://github.",
            "year": 2021
        },
        {
            "authors": [
                "Ajay Jain",
                "Ben Mildenhall",
                "Jonathan T. Barron",
                "Pieter Abbeel",
                "Ben Poole."
            ],
            "title": "Zero-Shot TextGuided Object Generation with Dream Fields",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2022
        },
        {
            "authors": [
                "Yuming Jiang",
                "Shuai Yang",
                "Haonan Qiu",
                "Wayne Wu",
                "Chen Change Loy",
                "Ziwei Liu."
            ],
            "title": "Text2Human: Text-Driven Controllable Human Image Generation",
            "venue": "Transactions on Graphics (TOG).",
            "year": 2022
        },
        {
            "authors": [
                "Nasir Mohammad Khalid",
                "Tianhao Xie",
                "Eugene Belilovsky",
                "Tiberiu Popa."
            ],
            "title": "CLIP-Mesh: Generating textured meshes from text using pretrained image-text models",
            "venue": "Special Interest Group on Computer Graphics and Interactive Techniques in",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2015
        },
        {
            "authors": [
                "Nikos Kolotouros",
                "Georgios Pavlakos",
                "Michael J. Black",
                "Kostas Daniilidis."
            ],
            "title": "Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop",
            "venue": "International Conference on Computer Vision (ICCV).",
            "year": 2019
        },
        {
            "authors": [
                "Pavel Korshunov",
                "Sebastien Marcel."
            ],
            "title": "DeepFakes: a New Threat to Face Recognition? Assessment and Detection",
            "venue": "arXiv:1812.08685.",
            "year": 2018
        },
        {
            "authors": [
                "Chun-Liang Li",
                "Manzil Zaheer",
                "Yang Zhang",
                "Barnabas Poczos",
                "Ruslan Salakhutdinov."
            ],
            "title": "Point Cloud GAN",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2018
        },
        {
            "authors": [
                "Jun Li",
                "Kai Xu",
                "Siddhartha Chaudhuri",
                "Ersin Yumer",
                "Hao Zhang",
                "Leonidas Guibas."
            ],
            "title": "GRASS: Generative Recursive Autoencoders for Shape Structures",
            "venue": "Transactions on Graphics (TOG).",
            "year": 2017
        },
        {
            "authors": [
                "Yitong Li",
                "Martin Renqiang Min",
                "Dinghan Shen",
                "David Carlson",
                "Lawrence Carin."
            ],
            "title": "Video Generation From Text",
            "venue": "AAAI Conference on Artificial Intelligence (AAAI).",
            "year": 2018
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Shi Qiu",
                "Xiaogang Wang",
                "Xiaoou Tang."
            ],
            "title": "DeepFashion: Powering Robust Clothes Recognition and Retrieval with Rich Annotations",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2016
        },
        {
            "authors": [
                "Stephen Lombardi",
                "Tomas Simon",
                "Gabriel Schwartz",
                "Michael Zollhoefer",
                "Yaser Sheikh",
                "Jason Saragih."
            ],
            "title": "Mixture of Volumetric Primitives for Efficient Neural Rendering",
            "venue": "Special Interest Group on Computer Graphics and Interactive Techniques",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Luo",
                "Tianqin Li",
                "Wen-Hao Zhang",
                "Tai Sing Lee."
            ],
            "title": "SurfGen: Adversarial 3D Shape Synthesis with Explicit Surface Discriminators",
            "venue": "International Conference on Computer Vision (ICCV).",
            "year": 2021
        },
        {
            "authors": [
                "Tanya Marwah",
                "Gaurav Mittal",
                "Vineeth N. Balasubramanian."
            ],
            "title": "Attentive Semantic Video Generation using Captions",
            "venue": "International Conference on Computer Vision (ICCV).",
            "year": 2017
        },
        {
            "authors": [
                "Lars Mescheder",
                "Andreas Geiger",
                "Sebastian Nowozin"
            ],
            "title": "Which Training Methods for GANs do actually Converge",
            "venue": "In International Conference on Machine Learning (ICML)",
            "year": 2018
        },
        {
            "authors": [
                "Gal Metzer",
                "Elad Richardson",
                "Or Patashnik",
                "Raja Giryes",
                "Daniel Cohen-Or."
            ],
            "title": "Latent-NeRF for ShapeGuided Generation of 3D Shapes and Textures",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2023
        },
        {
            "authors": [
                "Oscar Michel",
                "Roi Bar-On",
                "Richard Liu",
                "Sagie Benaim",
                "Rana Hanocka."
            ],
            "title": "Text2Mesh: Text-Driven Neural Stylization for Meshes",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2022
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P. Srinivasan",
                "Matthew Tancik",
                "Jonathan T. Barron",
                "Ravi Ramamoorthi",
                "Ren Ng."
            ],
            "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
            "venue": "European Conference on Computer Vision (ECCV).",
            "year": 2020
        },
        {
            "authors": [
                "Thomas Muller",
                "Alex Evans",
                "Christoph Schied",
                "Alexander Keller."
            ],
            "title": "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding",
            "venue": "Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH).",
            "year": 2022
        },
        {
            "authors": [
                "Gimin Nam",
                "Mariem Khlifi",
                "Andrew Rodriguez",
                "Alberto Tono",
                "Linqi Zhou",
                "Paul Guerrero."
            ],
            "title": "3D-LDM: Neural Implicit 3D Shape Generation with Latent Diffusion Models",
            "venue": "arXiv:2212.00842.",
            "year": 2022
        },
        {
            "authors": [
                "Charlie Nash",
                "Yaroslav Ganin",
                "Ali Eslami",
                "Peter W. Battaglia."
            ],
            "title": "PolyGen: An Autoregressive Generative Model of 3D Meshes",
            "venue": "International Conference on Machine Learning (ICML).",
            "year": 2020
        },
        {
            "authors": [
                "Atsuhiro Noguchi",
                "Xiao Sun",
                "Stephen Lin",
                "Tatsuya Harada."
            ],
            "title": "Unsupervised Learning of Efficient Geometry-Aware Neural Articulated Representations",
            "venue": "European Conference on Computer Vision (ECCV).",
            "year": 2022
        },
        {
            "authors": [
                "Roy Or-El",
                "Xuan Luo",
                "Mengyi Shan",
                "Eli Shechtman",
                "Jeong Joon Park",
                "Ira Kemelmacher-Shlizerman."
            ],
            "title": "StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2022
        },
        {
            "authors": [
                "Jeong Joon Park",
                "Peter Florence",
                "Julian Straub",
                "Richard Newcombe",
                "Steven Lovegrove."
            ],
            "title": "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2019
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Soumith Chintala",
                "Gregory Chanan",
                "Edward Yang",
                "Zachary DeVito",
                "Zeming Lin",
                "Alban Desmaison",
                "Luca Antiga",
                "Adam Lerer."
            ],
            "title": "Automatic differentiation in PyTorch",
            "venue": "International Conference on Learning Representations",
            "year": 2017
        },
        {
            "authors": [
                "Georgios Pavlakos",
                "Vasileios Choutas",
                "Nima Ghorbani",
                "Ahmed A Timo Bolkart",
                "A. Osman",
                "Dimitrios Tzionas",
                "Michael J. Black."
            ],
            "title": "Expressive Body Capture: 3D Hands, Face, and Body from a Single Image",
            "venue": "Conference on Computer Vision",
            "year": 2019
        },
        {
            "authors": [
                "Sida Peng",
                "Junting Dong",
                "Qianqian Wang",
                "Shangzhan Zhang",
                "Qing Shuai",
                "Xiaowei Zhou",
                "Hujun Bao."
            ],
            "title": "Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies",
            "venue": "International Conference on Computer Vision (ICCV).",
            "year": 2021
        },
        {
            "authors": [
                "Sida Peng",
                "Yuanqing Zhang",
                "Yinghao Xu",
                "Qianqian Wang",
                "Qing Shuai",
                "Hujun Bao",
                "Xiaowei Zhou."
            ],
            "title": "Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans",
            "venue": "Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Ben Poole",
                "Ajay Jain",
                "Jonathan T. Barron",
                "Ben Mildenhall."
            ],
            "title": "DreamFusion: Text-to-3D using 2D Diffusion",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning Transferable Visual Models",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen."
            ],
            "title": "Hierarchical TextConditional Image Generation with CLIP Latents",
            "venue": "arXiv:2204.06125.",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Sutskever."
            ],
            "title": "Zero-Shot Text-to-Image Generation",
            "venue": "International Conference on Machine Learning (ICML).",
            "year": 2021
        },
        {
            "authors": [
                "Ren\u00e9 Ranftl",
                "Katrin Lasinger",
                "David Hafner",
                "Konrad Schindler",
                "Vladlen Koltun."
            ],
            "title": "Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer",
            "venue": "Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2020
        },
        {
            "authors": [
                "Scott Reed",
                "Zeynep Akata",
                "Xinchen Yan",
                "Lajanugen Logeswaran",
                "Bernt Schiele",
                "Honglak Lee."
            ],
            "title": "Generative Adversarial Text to Image Synthesis",
            "venue": "International Conference on Machine Learning (ICML).",
            "year": 2016
        },
        {
            "authors": [
                "Elad Richardson",
                "Gal Metzer",
                "Yuval Alaluf",
                "Raja Giryes",
                "Daniel Cohen-Or."
            ],
            "title": "TEXTure: Text-Guided Texturing of 3D Shapes",
            "venue": "Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH).",
            "year": 2023
        },
        {
            "authors": [
                "Katja Schwarz",
                "Yiyi Liao",
                "Michael Niemeyer",
                "Andreas Geiger."
            ],
            "title": "GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis",
            "venue": "Conference on Neural Information Processing Systems (NeurIPS).",
            "year": 2021
        },
        {
            "authors": [
                "Junyoung Seo",
                "Wooseok Jang",
                "Min-Seop Kwak",
                "Jaehoon Ko",
                "Hyeonsu Kim",
                "Junho Kim",
                "Jin-Hwa Kim",
                "Jiyoung Lee",
                "Seungryong Kim."
            ],
            "title": "Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation",
            "venue": "arXiv:2303.07937.",
            "year": 2023
        },
        {
            "authors": [
                "Aliaksandra Shysheya",
                "Egor Zakharov",
                "Kara-Ali Aliev",
                "Renat Bashirov",
                "Egor Burkov",
                "Karim Iskakov",
                "Aleksei Ivakhnenko",
                "Yury Malkov",
                "Igor Pasechnik",
                "Dmitry Ulyanov",
                "Alexander Vakhitov",
                "Victor Lempitsky."
            ],
            "title": "Textured Neural Avatars",
            "venue": "Con-",
            "year": 2019
        },
        {
            "authors": [
                "Vincent Sitzmann",
                "Julien N.P. Martel",
                "Alexander W. Bergman",
                "David B. Lindell",
                "Gordon Wetzstein."
            ],
            "title": "Implicit Neural Representations with Periodic Activation Functions",
            "venue": "Conference on Neural Information Processing Systems (NeurIPS).",
            "year": 2020
        },
        {
            "authors": [
                "Ivan Skorokhodov",
                "Aliaksandr Siarohin",
                "Yinghao Xu",
                "Jian Ren",
                "Hsin-Ying Lee",
                "Peter Wonka",
                "Sergey Tulyakov."
            ],
            "title": "3D generation on ImageNet",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2023
        },
        {
            "authors": [
                "Shih-Yang Su",
                "Frank Yu",
                "Michael Zollhoefer",
                "Helge Rhodin."
            ],
            "title": "A-NeRF: Articulated Neural Radiance Fields for Learning Human Shape, Appearance, and Pose",
            "venue": "Conference on Neural Information Processing Systems (NeurIPS).",
            "year": 2021
        },
        {
            "authors": [
                "Zhuo Su",
                "Lan Xu",
                "Zerong Zheng",
                "Tao Yu",
                "Yebin Liu",
                "Lu Fang."
            ],
            "title": "RobustFusion: Human Volumetric Capture with Data-driven Visual Cues using a RGBD Camera",
            "venue": "European Conference on Computer Vision (ECCV).",
            "year": 2020
        },
        {
            "authors": [
                "Junshu Tang",
                "Tengfei Wang",
                "Bo Zhang",
                "Ting Zhang",
                "Ran Yi",
                "Lizhuang Ma",
                "Dong Chen."
            ],
            "title": "MakeIt-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior",
            "venue": "arXiv:2303.14184.",
            "year": 2023
        },
        {
            "authors": [
                "Maxim Tatarchenko",
                "Alexey Dosovitskiy",
                "Thomas Brox."
            ],
            "title": "Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs",
            "venue": "International Conference on Computer Vision (ICCV).",
            "year": 2017
        },
        {
            "authors": [
                "Can Wang",
                "Menglei Chai",
                "Mingming He",
                "Dongdong Chen",
                "Jing Liao."
            ],
            "title": "CLIP-NeRF: Textand-Image Driven Manipulation of Neural Radiance Fields",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2022
        },
        {
            "authors": [
                "Haochen Wang",
                "Xiaodan Du",
                "Jiahao Li",
                "Raymond A. Yeh",
                "Greg Shakhnarovich."
            ],
            "title": "Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation",
            "venue": "arXiv:2212.00774.",
            "year": 2022
        },
        {
            "authors": [
                "Jianfeng Wang",
                "Zhengyuan Yang",
                "Xiaowei Hu",
                "Linjie Li",
                "Kevin Lin",
                "Zhe Gan",
                "Zicheng Liu",
                "Ce Liu",
                "Lijuan Wang."
            ],
            "title": "GIT: A Generative Image-to-text Transformer for Vision and Language",
            "venue": "Transactions on Machine Learning Research (TMLR).",
            "year": 2022
        },
        {
            "authors": [
                "Chung-Yi Weng",
                "Brian Curless",
                "Pratul P. Srinivasan",
                "Jonathan T. Barron",
                "Ira KemelmacherShlizerman."
            ],
            "title": "HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video",
            "venue": "Conference on Computer Vision and Pattern",
            "year": 2022
        },
        {
            "authors": [
                "Tao Xu",
                "Pengchuan Zhang",
                "Qiuyuan Huang",
                "Han Zhang",
                "Zhe Gan",
                "Xiaolei Huang",
                "Xiaodong He."
            ],
            "title": "AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks",
            "venue": "Conference on Computer Vision and Pattern",
            "year": 2018
        },
        {
            "authors": [
                "Xiangyu Xu",
                "Chen Change Loy."
            ],
            "title": "3D Human Texture Estimation from a Single Image with Transformers",
            "venue": "International Conference on Computer Vision (ICCV).",
            "year": 2021
        },
        {
            "authors": [
                "Guandao Yang",
                "Xun Huang",
                "Zekun Hao",
                "Ming-Yu Liu",
                "Serge Belongie",
                "Bharath Hariharan."
            ],
            "title": "PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows",
            "venue": "International Conference on Computer Vision (ICCV).",
            "year": 2019
        },
        {
            "authors": [
                "Zhuoqian Yang",
                "Shikai Li",
                "Wayne Wu",
                "Bo Dai."
            ],
            "title": "3DHumanGAN: Towards Photo-Realistic 3D-Aware Human Image Generation",
            "venue": "arXiv:2212.07378.",
            "year": 2022
        },
        {
            "authors": [
                "Lior Yariv",
                "Jiatao Gu",
                "Yoni Kasten",
                "Yaron Lipman."
            ],
            "title": "Volume Rendering of Neural Implicit Surfaces",
            "venue": "Conference on Neural Information Processing Systems (NeurIPS).",
            "year": 2021
        },
        {
            "authors": [
                "Jae Shin Yoon",
                "Lingjie Liu",
                "Vladislav Golyanik",
                "Kripasindhu Sarkar",
                "Hyun Soo Park",
                "Christian Theobalt."
            ],
            "title": "Pose-Guided Human Animation from a Single Image in the Wild",
            "venue": "Conference on Computer Vision and Pattern Recognition (CVPR).",
            "year": 2021
        },
        {
            "authors": [
                "Jason Y. Zhang",
                "Gengshan Yang",
                "Shubham Tulsiani",
                "Deva Ramanan."
            ],
            "title": "NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in the Wild",
            "venue": "Conference on Neural Information Processing Systems (NeurIPS).",
            "year": 2021
        },
        {
            "authors": [
                "Mingyuan Zhang",
                "Zhongang Cai",
                "Liang Pan",
                "Fangzhou Hong",
                "Xinying Guo",
                "Lei Yang",
                "Ziwei Liu."
            ],
            "title": "MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model",
            "venue": "arXiv:2208.15001.",
            "year": 2022
        },
        {
            "authors": [
                "Xin-Yang Zheng",
                "Yang Liu",
                "Peng-Shuai Wang",
                "Xin Tong."
            ],
            "title": "SDF-StyleGAN: Implicit SDF-Based StyleGAN for 3D Shape Generation",
            "venue": "Computer Graphics Forum (SGP).",
            "year": 2022
        },
        {
            "authors": [
                "Zerong Zheng",
                "Tao Yu",
                "Yebin Liu",
                "Qionghai Dai."
            ],
            "title": "PaMIR: Parametric Model-Conditioned Implicit Representation for Image-based Human Reconstruction",
            "venue": "Transactions on Pattern Analysis and Machine Intelligence (TPAMI).",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Our world is inherently three-dimensional, where this nature highlights the importance of 3D applications in various fields, including architecture, product design, and scientific simulation. The capability of 3D content generation helps bridge the gap between physical and virtual domains, providing an engaging interaction within digital media. Furthermore, realistic 3D humans have vast practical value, especially in gaming, film, and animation. Despite enhancing the user experience, the customization of the character is crucial for creativity and scalability. Language is the most direct way of communication. If a system follows the description and establishes\nProject website: https://text-3dh.github.io\nthe 3D human model, it will significantly improve controllability and meet the considerable demand.\nWe thus introduce Text-guided 3D Human Generation (T3H) to generate a 3D human with the customized outfit, guided via the fashion description. Previous works (Kolotouros et al., 2019; Gao et al., 2022) depend on multi-view videos to learn 3D human modeling, but these data are difficult to obtain and are not language controllable. Text-to-3D (Jain et al., 2022; Poole et al., 2023) has shown attractive 3D generation results through the success of neural rendering (Mildenhall et al., 2020). However, these methods apply iterative inference optimization by external guidance, which is inefficient for usage.\nTo tackle these above issues, we propose Compositional Cross-modal Human (CCH) to learn T3H from 2D collections. CCH divides the human body into different parts and employs individual volume rendering, inspired by EVA3D (Hong et al., 2023).\nWe extract the fashion semantics from the description and adopt cross-modal attention to fuse body volumes with textual features, where each part can learn to perceive its correlated fashion patterns. To support various angles of view, CCH leverages the human prior (Bogo et al., 2016) to guide the geometry transformation for concrete human architecture. Then these compositional volumes can jointly render a 3D human with the desired fashion efficiently. The semantic discrimination further considers compositional distinguishment over each human part, which improves the fine-grained alignment with its description through adversarial training.\nWe perform experiments on DeepFashion (Liu et al., 2016; Jiang et al., 2022) and SHHQ (Fu et al., 2022a), which contain human images with diverse fashion descriptions. The patterns include various types of shapes (sleeveless, medium short, long, etc.), fabrics (denim, cotton, furry, etc.), and colors (floral, graphic, pure color, etc.) for the upper and lower clothing. To study the performance of T3H, we conduct a thorough evaluation from both visual and semantic aspects. We treat overall realism, geometry measure, and pose correctness to assess the quality of generated 3D humans. For the alignment with the assigned fashion, we apply text-visual relevance from CLIP and fine-grained accuracy by a trained fashion classifier.\nThe experiments indicate that language is necessary to make 3D human generation controllable. Our proposed CCH adopts cross-modal attention to fuse compositional neural rendering with textual fashion as 3D humans, and semantic discrimination further helps fine-grained consistency. In summary, our contributions are three-fold: \u2022 We introduce T3H to control 3D human genera-\ntion via fashion description, learning from collections of 2D images. \u2022 We propose CCH to extract fashion semantics in the text and fuse with 3D rendering in one shot, leading to an effective and efficient T3H. \u2022 Extensive experiments show that CCH exhibits sharp 3D humans with clear textual-related fashion patterns. We advocate that T3H can become a new field of vision-and-language research."
        },
        {
            "heading": "2 Related Work",
            "text": "Text-guided Visual Generation. Using humanunderstandable language to guide visual generation can enhance controllability and benefit creative visual design. Previous works built upon adversarial\ntraining (Goodfellow et al., 2015; Reed et al., 2016) to produce images (Xu et al., 2018; El-Nouby et al., 2019; Fu et al., 2020, 2022c) or videos (Marwah et al., 2017; Li et al., 2018b; Fu et al., 2022b) conditioned on given descriptions. With sequential modeling from Transformer, vector quantization (Esser et al., 2021) can generate high-quality visual content as discrete tokens (Ramesh et al., 2021; Ding et al., 2021; Fu et al., 2023). The denoising diffusion framework (Ho et al., 2020; Ramesh et al., 2022; Saharia et al., 2022; Feng et al., 2023) gains much attention as its diversity and scalability via large-scale text-visual pre-training. Beyond images and videos, 3D content creation is more challenging due to the increasing complexity of the depth dimension and spatial consistency. In this paper, we consider text-guided 3D human generation (T3H), which has vast applications in animated characters and virtual assistants.\n3D Generation. Different representations have been explored for 3D shapes, such as mesh (Gao et al., 2019; Nash et al., 2020; Henderson et al., 2020), voxel grid (Tatarchenko et al., 2017; Li et al., 2017), point cloud (Li et al., 2018a; Yang et al., 2019; Luo et al., 2021), and implicit field (Chen and Zhang, 2019; Park et al., 2019; Zheng et al., 2022). Neural Radiance Field (NeRF) (Mildenhall et al., 2020; Barron et al., 2022; Muller et al., 2022) has shown remarkable results in novel view synthesis (Schwarz et al., 2021; Chan et al., 2021; Skorokhodov et al., 2023) and 3D reconstruction (Yariv et al., 2021; Zhang et al., 2021). With the differentiable neural rendering, NeRF can be guided by various objectives. Text-to-3D draws appreciable attraction these days, which adopts external textvisual alignments (Jain et al., 2022; Khalid et al., 2022; Michel et al., 2022; Wang et al., 2022a; Hong et al., 2022) and pre-trained text-to-image (Wang et al., 2022b; Nam et al., 2022; Poole et al., 2023; Metzer et al., 2023; Tang et al., 2023; Seo et al., 2023). However, existing methods take numerous iterations to optimize a NeRF model, which is timeconsuming for practical usage. Our CCH learns to extract fashion semantics with NeRF rendering and incorporates the human prior for a concrete human body, achieving effective and efficient T3H.\n3D Human Representation. To reconstruct a 3D human, early works (Collet et al., 2015; Guo et al., 2019; Su et al., 2020) count on off-the-shelf tools to predict the camera depth. As mitigating the costly hardware requirement, they estimate a 3D human\ntexture (Xu and Loy, 2021; Gomes et al., 2022) via the UV mapping (Shysheya et al., 2019; Yoon et al., 2021). With the promising success of NeRF, recent works (Peng et al., 2021b,a; Su et al., 2021) adopt volume rendering for 3D humans from multi-view videos (Weng et al., 2022; Chen et al., 2022). Since the data are difficult to collect, the 3D-aware generation (Chan et al., 2022; Gu et al., 2022; Noguchi et al., 2022) learns 3D modeling from the collection of human images (Yang et al., 2022; Hong et al., 2023). In place of arbitrary outputs, we introduce the first controllable 3D human generation that also learns from a 2D collection, and the presented fashion patterns should align with the description."
        },
        {
            "heading": "3 Text-guided 3D Human Generation",
            "text": ""
        },
        {
            "heading": "3.1 Task Definition",
            "text": "We present text-guided 3D human generation (T3H) to create 3D humans via fashion descriptions. For data efficiency, a 2D collection D={V, T } is provided, where V is the human image, and T is its fashion description. Our goal is to learn the neural rendering that maps T into an articulate 3D human with the fashion patterns of V ."
        },
        {
            "heading": "3.2 Background",
            "text": "Neural Radiance Field (NeRF) (Mildenhall et al., 2020) defines implicit 3D as {c, \u03c3}=F (x, d). The query point x in the viewing direction d holds the emitted radiance c and the volume density \u03c3. To get the RGB value C(r) of certain rays r(t), volume rendering is calculated along a ray r from the near\nbound tn to the far bound tf: T (t) = exp(\u2212 \u222b t tn \u03c3(r(s))ds),\nC(r) = \u222b tf tn T (t)\u03c3(r(t))c(r(t), d)dt, (1)\nwhere T (t) stands for their accumulated transmittance. StyleSDF (Or-El et al., 2022) then replaces \u03c3 with single distance field (SDF) d(x) for a better surface, where \u03c3(x) = \u03b1\u22121sigmoid(\u2212d(x)\u03b1 ) and \u03b1 is a learnable scalar that controls the tightness of the density around the surface boundary.\nSMPL (Bogo et al., 2016) defines the human body as {\u03b2, \u03b8}, where \u03b2 \u2208 R10 and \u03b8 \u2208 R3\u00d723 control its shape and pose. We consider Linear Blend Skinning (LBS) as the transformation from the canonical into the observation space for the point x to\u2211K\nk=1 hkHk(\u03b8, J)x, where hk \u2208 R is the scalar of the blend weight and Hk \u2208 R4\u00d74 is the transformation matrix of the kth joint. Inverse LBS transforms the observation back to the canonical space as a similar equation but with an inverted H ."
        },
        {
            "heading": "3.3 Compositional Cross-modal Human",
            "text": "Following EVA3D (Hong et al., 2023), we split the human body into 16 parts. As shown in Fig. 2, each body part holds its own bounding box {obmin, obmax}. To leverage the human prior for a target pose \u03b8, we transform these pre-defined bounding boxes with SMPL\u2019s transformation matrices Hk. Ray r(t) is sampled for each pixel on the canvas. For a ray that intersects bounding boxes, we pick up its near and far bounds (tn and tf) and sample N points as follows: ti \u223c U [ tn + i\u22121 N (tf \u2212 tn), tn + i N (tf \u2212 tn) ] .\nWe then transform these sampled points back to the canonical space with inverse LBS. For shape generalization, we consider not only pose transformation but also blend shapes (BP (\u03b8) and BS(\u03b2)) (Zheng et al., 2021). N contains K nearest vertices v of the target SMPL mesh for the sample point ray r(ti):\ngk = 1\n||r(ti)\u2212 vk|| ,\nMk = ( K\u2211 k=1 gkHk )[ I BPk +B S k 0 I ] ,[\nxi 1 ] = \u2211 vk\u2208N gk\u2211 vk\u2208N gk (Mk) \u22121 [ r(ti) 1 ] , (2) where gk \u2208 R is the inverse weight of the vertex vk and Mk \u2208 R4\u00d74 is the transformation matrix. The xi can be used for further volume rendering.\nCross-modal Attention. During rendering, if the canonical point xi with the viewing direction di is inside the bth bounding box, it will be treated as:\nx\u0302bi = 2xi \u2212 (obmax + obmin)\nobmax \u2212 obmin ,\nf bi = Linear(x\u0302 b i , di), (3)\nwhere a linear mapping is applied to acquire preliminary features f b \u2208 R16\u00d78\u00d7128. To exhibit the desired fashion in the final rendering, we extract the word features by the text encoder as {wl \u2208 R512} from T . We then fuse the textual features with fki via cross-modal attention:\npl = exp(f bi W b wTl )\u2211L \u03b9=1 exp(f b i W b wT\u03b9 ) ,\nCA(f bi | {w}) = L\u2211 l=1 plwl, (4)\nwhere L is the length of T and W b is the learnable matrix. In this way, each point can learn to perceive relevant textual guidance for the bth human body part and depict corresponding fashion patterns.\nEach body part has its individual volume rendering F b, which consists of stacked multilayer perceptrons (MLPs) with the SIREN activation (Sitzmann et al., 2020). Since the point xi may fall into multiple boxes Bi, we follow EVA3D to apply the mixture function (Lombardi et al., 2021):\n{cbi , \u03c3bi} = F b(CA(xbi , di | {w})), ub = exp(\u2212m(x\u0302bi(x)n + x\u0302bi(y)n + x\u0302bi(z)n)),\n{ci, \u03c3i} = 1\u2211\nb\u2208B ub \u2211 b\u2208B ub{cbi , \u03c3bi}, (5)\nAlgorithm 1 Compositional Cross-modal Human 1: D: 2D collection of human images / fashion descriptions 2: 3: G, D: the generator / discriminator model for T3H 4: while TRAIN_CCH do 5: V , T \u2190 sampled human / description from D 6: {\u03b2, \u03b8} \u2190 estimated SMPL parameters of V 7: {xi} \u2190 canonical points via inverse LBS \u25b7 Eq. 2 8: fbi \u2190 rendering features inside the bth box \u25b7 Eq. 3 9: {wl} \u2190 extracted textual features of T\n10: CA\u2190 fusion via cross-modal attention \u25b7 Eq. 4 11: {ci, \u03c3i} \u2190 mixture radiance / density \u25b7 Eq. 5 12: R\u2190 final rendering human \u25b7 Eq. 1 13: 14: S \u2190 segmentation map of V 15: Q\u2190 fashion map between S and T \u25b7 Eq. 6 16: Ladv \u2190 adversarial loss from D \u25b7 Eq. 8 17: Loff, Leik \u2190 offset and derivation loss \u25b7 Eq. 9 18: Lall \u2190 overall training loss \u25b7 Eq. 10 19: Update G by minimizing Lall 20: Update D by maximizing Lall 21: end while\nwhere m and n are hyperparameters. With {ci, \u03c3i}, we adopt Eq. 1 to render the RGB value of ray r(t). Through all sampled rays r, we then have our final human rendering R, where the overall process can be simplified as R = G(\u03b2, \u03b8 | T ). To summarize, CCH leverages the human prior and adopts inverse LBS to acquire the canonical space for the target pose. The human body is divided into 16 parts, and each of them fuses its correlated fashion semantics via cross-modal attention. Finally, compositional bodies jointly render the target 3D human.\nSemantic Discrimination. With the SMPL prior, our CCH contains robust geometry transformation for humans and can learn from 2D images without actual 3D guidance. For a ground-truth {V, T }, we parse the 2D human image as the segmentation S (MMHuman3D, 2021), which provides the reliable body architecture. To obtain its fashion map Q, we apply cross-modal attention between S and T :\n{ei,j} = Conv(S),\nQi,j = L\u2211 l=1 exp(ei,jWw T l )\u2211L \u03b9=1 exp(ei,jWw T \u03b9 ) wl, (6)\nwhere e is the same dimension as f , W is the learnable attention matrix, and Q perceives which human body part should showcase what fashion patterns. We concatenate the rendered human R (or the ground-truth V) with Q and feed them into our discriminator D to perform binary classification:\nD(R | T ) = BC([Conv(R), Q]). (7) In consequence, D can provide alignments of both\nthe human pose and fashion semantics, which improves the fine-grained consistency of our CCH.\nLearning of CCH. We include the non-saturating loss with R1 regularization (Mescheder et al., 2018) for adversarial learning over the ground-truth {V}:\nU(u) = \u2212 log(1 + exp(\u2212u)), Ladv = U(G(\u03b2, \u03b8 | T ) | T ) (8)\n+ U(\u2212D(V | T )) + \u03bb|\u2207D(V | T )|2. Following EVA3D, we also append the minimum offset loss Loff to maintain a plausible human shape as the template mesh. Leik penalizes the derivation of delta SDFs to zero and makes the estimated SDF physically valid (Gropp et al., 2020):\nLoff = ||\u2206d(x)||22, Leik = ||\u2207(\u2206d(x))||22. (9)\nThe learning process of our CCH is also illustrated as Algo. 1, where the overall optimization can be:\nLall = Ladv + 1.5 \u00b7 Loff + 0.5 \u00b7 Leik, (10) min G max D Lall."
        },
        {
            "heading": "4 Experiemnts",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Datasets. We coduct experiments on DeepFashion (Jiang et al., 2022) and SHHQ (Fu et al., 2022a) for T3H. DeepFashion contains 12K human images with upper and lower clothing descriptions. Since there are no annotations in SHHQ, we first fine-tune GIT (Wang et al., 2022c) on DeepFashion and then label for 40K text-human pairs. We follow OpenPose (Cao et al., 2019) and SMPLify-X (Pavlakos et al., 2019) to estimate the human keypoints and its SMPL parameters. The resolution is resized into 512x256 in our experiments. Note that all faces in datasets are blurred prior to training, and the model is not able to generate human faces.\nEvaluation Metrics. We apply metrics from both visual and semantic prospects. Following EVA3D, we adopt Frechet Inception Distance (FID) (Heusel et al., 2017) and Depth (Ranftl et al., 2020) to cal-\nculate visual and geometry similarity, compared to the ground-truth image. We treat Percentage of Correct Keypoints (PCK@0.5) (Andriluka et al., 2014) as the correctness of the generated pose. To investigate the textual relevance of T3H results, we follow CLIP Score (CLIP-S) (Hessel et al., 2021) for the text-visual similarity. We fine-tune CLIP (Radford et al., 2021) on DeepFashion for a more accurate alignment in this specific domain. To have the finegrained evaluation, we train a fashion classifier on DeepFashion labels1 and assess Fashion Accuracy (FA) of the generated human.\nBaselines. As a new task, we consider the following methods as the compared baselines. \u2022 Latent-NeRF (Metzer et al., 2023) brings NeRF\nto the latent space and guides its generation by the given object and a text-to-image prior. \u2022 TEXTure (Richardson et al., 2023) paints a 3D object from different viewpoints via leveraging the pre-trained depth-to-image diffusion model. \u2022 CLIP-O is inspired by AvatarCLIP (Hong et al., 2022), which customizes a human avatar from the description with CLIP text-visual alignment. We apply the guided loss to optimize a pre-trained EVA3D (Hong et al., 2023) for faster inference. \u2022 Texformer (Xu and Loy, 2021) estimates the human texture from an image. Text2Human (Jiang et al., 2022) predicts the target human image, and we treat Texformer to further build its 3D model.\nFor a fair comparison, all baselines are re-trained on face-blurred datasets and cannot produce identifiable human faces.\nImplementation Detail. We divide a human body into 16 parts and deploy individual StyleSDF (OrEl et al., 2022) for each volume rendering, and two following MLPs then estimate SDF and RGB values. We adopt the same discriminator as StyleSDF over fashion maps to distinguish between fake rendered humans and real images. We sample N=28 points for each ray and set (m, n) to (4, 8) for mix-\n1There are six targets for FA, including the shape, fabric, and color of the upper and lower clothing. The fashion classifier has +95% accuracy, which provides a precise evaluation.\nture rendering. The text encoder is initialized from CLIP and subsequently trained with CCH. We treat Adam (Kingma and Ba, 2015) with a batch size of 1, where the learning rates are 2e-5 for G and 2e-4 for D. We apply visual augmentations by randomly panning, scaling, and rotating within small ranges. All trainings are done using PyTorch (Paszke et al., 2017) on 8 NVIDIA A100 GPUs for 1M iterations."
        },
        {
            "heading": "4.2 Quantitative Results",
            "text": "Table 1 shows the pose-guided T3H results on DeepFashion and SHHQ, where we feed the estimated human mesh as the input object into Latent-NeRF and TEXTure. Although Latent-NeRF can portray body shapes in multiple angles from its latent NeRF space, the rendering is clearly counterfeit (higher FID and Depth). For TEXTure, the human architecture is constructed well by the given mesh (higher PCK). However, the estimated texture is still spatially inconsistent and contains inevitable artifacts (still higher FID). From the semantic aspect, LatentNeRF and TEXTure borrow those trained diffusion models and depict the assigned appearance in the description (higher CLIP-S than CLIP-O). CLIP-O relies on EVA3D to produce feasible 3D humans (lower FID). While the external CLIP loss attempts to guide the fashion, the global alignment is insufficient to demonstrate detailed patterns (lower FA). Without those above drawbacks, our CCH learns to extract fashion semantics along with the compositional human generation, leading to comprehensive superiority across all metrics.\nA similar trend can be found on SHHQ. LatentNeRF and TEXTure exhibit related fashion patterns but are hard to present realistic humans (higher FID and Depth). CLIP-O produces a sharp human body\nwith the correct pose, but not the assigned fashion (lower CLIP-S and FA) by the inexplicit alignment from CLIP. Table 2 presents the pose-free results. With the guided 2D image, Texformer contains the assigned clothing in the text (higher FA than CLIPO). But the 3D reconstruction is unable to handle spatial rendering, resulting in low-quality humans (higher FID). With cross-modal attention and semantic discrimination, CCH exceeds baselines in both visual and textual relevance, making concrete human rendering with the corresponding fashion."
        },
        {
            "heading": "4.3 Ablation Study",
            "text": "We study each component effect of CCH in Table 3. Without the guided description, the model lacks the target fashion and results in a poor FA. This further highlights the importance of textual guidance for controllable human generation. When applying the traditional training (Reed et al., 2016), conditional GAN is insufficient to extract fashion semantics for effective T3H (not good enough CLIP-S). On the other hand, our cross-modal attention constructs a better fusion between fashion patterns and volume rendering, facilitating a significant improvement in depicting the desired human appearance. Moreover, semantic discrimination benefits fine-grained alignment and leads to comprehensive advancement.\nFine-tune CLIP-S as Evaluator. CLIP has shown promising text-visual alignment, which can calculate feature similarity between the generated human and the given text as CLIP-S (Hessel et al., 2021). Since our T3H is in a specific fashion domain, we consider the larger-scaled trained checkpoint from OpenCLIP (Ilharco et al., 2021) and fine-tune it as\na more precise evaluator. Table 4 presents text-tofashion retrieval results, where a higher recall leads to a better alignment. Whether the original CLIP or OpenCLIP, both result in poor performance and is insufficient for our evaluation. By perceiving DeepFashion, fine-tuning helps bring reliable alignment and is treated as the final evaluator.\nHuman Evaluation. Apart from automatic metrics, we conduct the human evaluation with aspects of 3D quality and fashion relevance. We randomly sample 75 T3H results and consider MTurk2 to rank over baselines and our CCH. To avoid the potential ranking bias, we hire 3 MTurkers for each example. Table 5 shows the mean ranking score (from 1 to 4, the higher is the better). CLIP-O and CCH are built upon EVA3D, which provides an articulate human body for superior 3D quality. Even if Latent-NeRF and TEXTure take pre-trained diffusion models to acquire visual guidance, CCH exhibits more corresponding fashion via cross-modal fusion. This performance trend is similar to our evaluation, which supports the usage of CLIP-S and FA as metrics.\nInference Efficiency. In addition to T3H quality, 2Amazon MTurk: https://www.mturk.com\nour CCH also contains a higher efficiency. Table 6 shows the inference time and GPU cost on a single NVIDIA TITAN RTX. All baselines take more than 100 seconds since they require multiple iterations to optimize the 3D model from an external alignment. In contrast, we extract fashion semantics and carry out T3H in one shot. Without updating the model, we save the most GPU memory. In summary, CCH surpasses baselines on both quality and efficiency, leading to an effective and practical T3H."
        },
        {
            "heading": "4.4 Qualitative Results.",
            "text": "We demonstrate the qualitative comparison of poseguided T3H in Fig. 3. Although Latent-NeRF can portray the 3D human based on the given mesh, it only presents inauthentic rendering. TEXTure generates concrete humans, but there are still obvious cracks and inconsistent textures from different angles of view. Moreover, both of them fail to capture \u201cthree-point\u201d, where the rendered lower clothing is\nincorrectly depicted as long pants. Because CLIP provides an overall but inexplicit alignment to the description, CLIP-O is limited and exhibits vague \u201cdenim\u201d or \u201clong-sleeved\u201d. This observation further indicates the flaw of CLIP in detailed fashion patterns, even if it has been fine-tuned on the target dataset. In contrast, our CCH adopts cross-modal attention with NeRF, contributing to high-quality T3H with fine-grained fashion controllability. Fig. 4 shows the pose-free results. Texformer relies on a 2D image to estimate its 3D texture. Despite containing the assigned fashion, it is still restricted by the capability of 3D reconstruction, resulting in a low-resolution rendering. By learning text-to-3D directly, CCH can produce textual-related humans from random poses with clear visual patterns.\nPose-control T3H. Since our CCH is generating 3D humans from given SMPL parameters, as illustrated in Fig. 6, we can further control T3H with a specific pose. Different fashion descriptions make a\nhuman body present diverse appearances; different poses then guide the character to express rich body language. This flexibility in controlling appearance and pose allows for better practical customization.\nAnimatable T3H. In addition to static poses, CCH can benefit from dynamic motions to achieve animatable T3H. Fig. 5 adopts MotionDiffuse (Zhang et al., 2022) to create the assigned action also from the text and apply it to our produced 3D models. In this way, we prompt them to \u201craise arms\u201d or \u201cwalk\u201d for favorable dynamic scenarios."
        },
        {
            "heading": "5 Conclusion",
            "text": "We present text-guided 3D human generation (T3H) to create a 3D human by a fashion description. To learn this from 2D collections, we introduce Compositional Cross-modal Human (CCH). With crossmodal attention, CCH fuses compositional human rendering and textual semantics to build a concrete body architecture with the corresponding fashion. Experiments across various fashion attributes show that CCH effectively carries out T3H with high efficiency. We believe T3H helps advance a new field toward vision-and-language research.\nEthics Discussion and Limitation. Our work enhances the controllability of 3D human generation. To prevent identity leakage, we blur out faces prior to training and avoid risks similar to DeepFake (Korshunov and Marcel, 2018). Because we depend on SMPL parameters, an inaccurate estimation causes\na distribution shift and quality degradation. For the datasets, they reveal narrow viewing angles, which results in visible artifacts of 3D consistency.\nAcknowledgements. We appreciate the anonymous reviewers for constructive feedback. The research presented in this work was funded by Meta AI. The views expressed are those of the authors and do not reflect the official policy or position of the funding agency."
        }
    ],
    "title": "Text-guided 3D Human Generation from 2D Collections",
    "year": 2023
}