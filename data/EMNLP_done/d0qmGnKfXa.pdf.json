{
    "abstractText": "Retrieval-enhanced methods have become a primary approach in fact verification (FV); it requires reasoning over multiple retrieved pieces of evidence to verify the integrity of a claim. To retrieve evidence, existing work often employs off-the-shelf retrieval models whose design is based on the probability ranking principle. We argue that, rather than relevance, for FV we need to focus on the utility that a claim verifier derives from the retrieved evidence. We introduce the feedback-based evidence retriever (FER) that optimizes the evidence retrieval process by incorporating feedback from the claim verifier. As a feedback signal we use the divergence in utility between how effectively the verifier utilizes the retrieved evidence and the ground-truth evidence to produce the final claim label. Empirical studies demonstrate the superiority of FER over prevailing baselines.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Hengran Zhang"
        },
        {
            "affiliations": [],
            "name": "Ruqing Zhang"
        },
        {
            "affiliations": [],
            "name": "\u2217Jiafeng Guo"
        },
        {
            "affiliations": [],
            "name": "Maarten de Rijke"
        },
        {
            "affiliations": [],
            "name": "Yixing Fan"
        },
        {
            "affiliations": [],
            "name": "Xueqi Cheng"
        }
    ],
    "id": "SP:3ad30071280cd77c49ca5a64f07dc91758c2c8ef",
    "references": [
        {
            "authors": [
                "Chenxin An",
                "Ming Zhong",
                "Zhichao Geng",
                "Jianqiang Yang",
                "Xipeng Qiu."
            ],
            "title": "Retrievalsum: A retrieval enhanced framework for abstractive summarization",
            "venue": "arXiv preprint arXiv:2109.07943.",
            "year": 2021
        },
        {
            "authors": [
                "Tuhin Chakrabarty",
                "Tariq Alhindi",
                "Smaranda Muresan."
            ],
            "title": "Robust document retrieval and individual evidence modeling for fact extraction and verification",
            "venue": "Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 127\u2013131.",
            "year": 2018
        },
        {
            "authors": [
                "Jiangui Chen",
                "Ruqing Zhang",
                "Jiafeng Guo",
                "Yixing Fan",
                "Xueqi Cheng."
            ],
            "title": "Gere: Generative evidence retrieval for fact verification",
            "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2022
        },
        {
            "authors": [
                "Jiangui Chen",
                "Ruqing Zhang",
                "Jiafeng Guo",
                "Yiqun Liu",
                "Yixing Fan",
                "Xueqi Cheng."
            ],
            "title": "Corpusbrain: Pre-train a generative retrieval model for knowledgeintensive language tasks",
            "venue": "Proceedings of the 31st ACM International Conference on Information &",
            "year": 2022
        },
        {
            "authors": [
                "Miaoxin Chen",
                "Zibo Lin",
                "Rongyi Sun",
                "Kai Ouyang",
                "HaiTao Zheng",
                "Rui Xie",
                "Wei Wu."
            ],
            "title": "Retrieval enhanced segment generation neural network for taskoriented dialogue systems",
            "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech",
            "year": 2022
        },
        {
            "authors": [
                "Anton Chernyavskiy",
                "Dmitry Ilvovsky."
            ],
            "title": "Extract and aggregate: A novel domain-independent approach to factual data verification",
            "venue": "Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER).",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Zhihao Fan",
                "Yeyun Gong",
                "Zhongyu Wei",
                "Siyuan Wang",
                "Yameng Huang",
                "Jian Jiao",
                "Xuanjing Huang",
                "Nan Duan",
                "Ruofei Zhang."
            ],
            "title": "An enhanced knowledge injection model for commonsense generation",
            "venue": "Proceedings of the 28th International Confer-",
            "year": 2020
        },
        {
            "authors": [
                "Matt Gardner",
                "Joel Grus",
                "Mark Neumann",
                "Oyvind Tafjord",
                "Pradeep Dasigi",
                "Nelson F. Liu",
                "Matthew Peters",
                "Michael Schmitz",
                "Luke Zettlemoyer."
            ],
            "title": "AllenNLP: A deep semantic natural language processing platform",
            "venue": "Proceedings of Workshop for",
            "year": 2018
        },
        {
            "authors": [
                "Andreas Hanselowski",
                "Hao Zhang",
                "Zile Li",
                "Daniil Sorokin",
                "Benjamin Schiller",
                "Claudia Schulz",
                "Iryna Gurevych."
            ],
            "title": "UKP-athene: Multi-sentence textual entailment for claim verification",
            "venue": "Proceedings of the First Workshop on Fact Extraction and",
            "year": 2018
        },
        {
            "authors": [
                "Shwai He",
                "Run-Ze Fan",
                "Liang Ding",
                "Li Shen",
                "Tianyi Zhou",
                "Dacheng Tao."
            ],
            "title": "Mera: Merging pretrained adapters for few-shot learning",
            "venue": "arXiv preprint arXiv:2308.15982.",
            "year": 2023
        },
        {
            "authors": [
                "Christopher Hidey",
                "Mona Diab."
            ],
            "title": "Team SWEEPer: Joint sentence extraction and fact checking with pointer networks",
            "venue": "Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 150\u2013155, Brussels, Belgium. Asso-",
            "year": 2018
        },
        {
            "authors": [
                "Xuming Hu",
                "Zhaochen Hong",
                "Zhijiang Guo",
                "Lijie Wen",
                "Philip S. Yu."
            ],
            "title": "Read it twice: Towards faithfully interpretable fact verification by revisiting evidence",
            "venue": "volume abs/2305.03507.",
            "year": 2023
        },
        {
            "authors": [
                "Kelvin Jiang",
                "Ronak Pradeep",
                "Jimmy Lin."
            ],
            "title": "Exploring listwise evidence reasoning with t5 for fact verification",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Diane Kelly",
                "Jaime Arguello",
                "Robert Capra."
            ],
            "title": "NSF workshop on task-based information search systems",
            "venue": "ACM SIGIR Forum, volume 47, pages 116\u2013 127. ACM New York, NY, USA.",
            "year": 2013
        },
        {
            "authors": [
                "Amrith Krishna",
                "Sebastian Riedel",
                "Andreas Vlachos."
            ],
            "title": "Proofver: Natural logic theorem proving for fact verification",
            "venue": "Transactions of the Association for Computational Linguistics, 10:1013\u20131030.",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Yu-An Liu",
                "Ruqing Zhang",
                "Jiafeng Guo",
                "Wei Chen",
                "Xueqi Cheng."
            ],
            "title": "On the robustness of generative retrieval models: An out-of-distribution perspective",
            "venue": "arXiv preprint arXiv:2306.12756.",
            "year": 2023
        },
        {
            "authors": [
                "Zhenghao Liu",
                "Chenyan Xiong",
                "Maosong Sun",
                "Zhiyuan Liu."
            ],
            "title": "Fine-grained fact verification with kernel graph attention network",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7342\u20137351, On-",
            "year": 2020
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.",
            "year": 2019
        },
        {
            "authors": [
                "Jing Ma",
                "Wei Gao",
                "Shafiq Joty",
                "Kam-Fai Wong."
            ],
            "title": "Sentence-level evidence embedding for claim verification with hierarchical attention networks",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2561\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Yixin Nie",
                "Haonan Chen",
                "Mohit Bansal."
            ],
            "title": "Combining fact extraction and verification with neural semantic matching networks",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6859\u20136866.",
            "year": 2019
        },
        {
            "authors": [
                "Yixin Nie",
                "Songhe Wang",
                "Mohit Bansal."
            ],
            "title": "Revealing the importance of semantic retrieval for machine reading at scale",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Md Rizwan Parvez",
                "Jianfeng Chi",
                "Wasi Uddin Ahmad",
                "Yuan Tian",
                "Kai-Wei Chang."
            ],
            "title": "Retrieval enhanced data augmentation for question answering on privacy policies",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for",
            "year": 2023
        },
        {
            "authors": [
                "Stephen E. Robertson."
            ],
            "title": "The probability ranking principle in IR",
            "venue": "Journal of Documentation, 33(4):294\u2013304.",
            "year": 1977
        },
        {
            "authors": [
                "Justyna Sarzynska-Wawer",
                "Aleksander Wawer",
                "Aleksandra Pawlak",
                "Julia Szymanowska",
                "Izabela Stefaniak",
                "Michal Jarkiewicz",
                "Lukasz Okruszek."
            ],
            "title": "Detecting formal thought disorder by deep contextualized word representations",
            "venue": "Psychiatry Research,",
            "year": 2021
        },
        {
            "authors": [
                "Rongzhen Ye"
            ],
            "title": "A dqn-based approach",
            "year": 2021
        },
        {
            "authors": [
                "Hamed Zamani",
                "Fernando Diaz",
                "Mostafa Dehghani",
                "Donald Metzler",
                "Michael Bendersky."
            ],
            "title": "Retrieval-enhanced machine learning",
            "venue": "SIGIR \u201922: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2022
        },
        {
            "authors": [
                "Jing Zhang",
                "Xiaokang Zhang",
                "Jifan Yu",
                "Jian Tang",
                "Jie Tang",
                "Cuiping Li",
                "Hong Chen."
            ],
            "title": "Subgraph retrieval enhanced model for multi-hop knowledge base question answering",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Chen Zhao",
                "Chenyan Xiong",
                "Corby Rosset",
                "Xia Song",
                "Paul Bennett",
                "Saurabh Tiwary."
            ],
            "title": "Transformer-XH: Multi-evidence reasoning with extra hop attention",
            "venue": "International conference on learning representations.",
            "year": 2020
        },
        {
            "authors": [
                "Wanjun Zhong",
                "Jingjing Xu",
                "Duyu Tang",
                "Zenan Xu",
                "Nan Duan",
                "Ming Zhou",
                "Jiahai Wang",
                "Jian Yin."
            ],
            "title": "Reasoning over semantic-level graph for fact checking",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Jie Zhou",
                "Xu Han",
                "Cheng Yang",
                "Zhiyuan Liu",
                "Lifeng Wang",
                "Changcheng Li",
                "Maosong Sun."
            ],
            "title": "GEAR: Graph-based evidence aggregating and reasoning for fact verification",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Compu-",
            "year": 2019
        },
        {
            "authors": [
                "Qingfu Zhu",
                "Lei Cui",
                "Weinan Zhang",
                "Furu Wei",
                "Ting Liu."
            ],
            "title": "Retrieval-enhanced adversarial training for neural response generation",
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July",
            "year": 2019
        },
        {
            "authors": [
                "2019a et al",
                "2019 Ma et al",
                "Nie"
            ],
            "title": "2019b) and feature-based (Hidey and Diab, 2018",
            "year": 2022
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "ranked sentences as evidence based on relevance ranking (Jiang et al., 2021",
            "venue": "Soleimani et al.,",
            "year": 2018
        },
        {
            "authors": [
                "2019a",
                "Krishna et al",
                "Nie"
            ],
            "title": "2019b), as well as graph neural network-based methods (Liu et al., 2020; Zhou et al., 2019) that frame the verification task as a graph reasoning",
            "year": 2019
        },
        {
            "authors": [
                "2022c",
                "Parvez"
            ],
            "title": "2023), summarization (An et al., 2021), and fact verification (Thorne",
            "year": 2018
        },
        {
            "authors": [
                "Hanselowski"
            ],
            "title": "On average, four documents are retrieved for each claim. Following the traditional entity linking pipeline approach, there are three steps for document retrieval. First, the claims are parsed using AllenNLP",
            "year": 2018
        },
        {
            "authors": [
                "\u2022 ColumbiaNLP (Chakrabarty"
            ],
            "title": "2018) initially utilizes TF-IDF to rank the candidate sentences and subsequently selects the top 5 sentences with the highest relevance. To mitigate the presence of noisy data, ELMo embeddings",
            "year": 2018
        },
        {
            "authors": [
                "Sarzynska-Wawer"
            ],
            "title": "2021) are employed to convert the claims and sentences into vectors. Subsequently, the top-3 sentences with the highest cosine similarity are extracted as the final retrieval results",
            "year": 2021
        },
        {
            "authors": [
                "\u2022 GEAR (Zhou"
            ],
            "title": "2019) enhances the UKPAthene model by introducing a threshold. Sentences with relevance scores higher than the threshold (set to 0.001) are filtered and considered as retrieval results",
            "year": 2019
        },
        {
            "authors": [
                "Ella Marija"
            ],
            "title": "Lani Yelich O\u2019Connor born 7 November 1996, better known by her stage name Lorde",
            "year": 1996
        },
        {
            "authors": [
                "Ella Marija"
            ],
            "title": "Lani Yelich-O\u2019Connor born",
            "year": 1996
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The risk of misinformation has increased the demand for fact-checking, i.e., automatically assessing the truthfulness of textual claims using trustworthy corpora, e.g., Wikipedia. Existing work on fact verification (FV) commonly adopts a retrievalenhanced verification framework: an evidence retriever is employed to query the background corpus for relevant sentences, to serve as evidence for the subsequent claim verifier. High-quality evidence is the foundation of claim verification. Currently, prevailing approaches to identifying high-quality evidence typically adopt off-the-shelf retrieval models from the information retrieval (IR) field for evidence retrieval (Wan et al., 2021; Jiang et al., 2021; Chen et al., 2022a; Liu et al., 2020). These models are usually based on the probability ranking principle (PRP) (Robertson, 1977), ranking sentences\n\u2217Research conducted when the author was at the University of Amsterdam.\n1Our code and data are available at https://github. com/ict-bigdatalab/FER\nbased on their likelihood of being relevant to the claim. However, the sentences retrieved in this way are assumed to be consumed by humans, and this may not align with a retrieval-enhanced verification framework (Zamani et al., 2022): the top-ranked sentences produced by existing retrieval models do not always align with the judgments made by the claim verifier regarding what counts as evidence.\nWe argue that when designing evidence retrieval models for FV, the notion of relevance should be reconceptualized as the utility that the verifier derives from consuming the evidence provided by the retrieval model, a viewpoint that aligns well with task-based perspectives on IR (Kelly et al., 2013). Hence, we assume that when training evidence retrievers, it is advantageous to obtain feedback from the claim verifier as a signal to optimize the retrieval process.\nTherefore, we propose a shift in emphasis from relevance to utility in evidence retrieval for FV. We introduce the feedback-based evidence retriever (FER) that incorporates a feedback mechanism from the verifier to enhance the retrieval process. FER leverages a coarse-to-fine strategy. Initially, it identifies a candidate set of relevant sentences to the given claim from the large-scale corpus. Subsequently, the evidence retriever is trained using feedback from the verifier, enabling a re-evaluation of evidence within the candidate set. Here, feedback is defined as the utility divergence observed when the verifier evaluates the sentences returned by the retriever, compared to when it consumes the ground-truth evidence for predicting the claim label. By measuring the utility criterion between the two scenarios, we can optimize the retriever specifically for claim verification.\nExperimental results on the large-scale Fact Extraction and VERification (FEVER) dataset (Thorne et al., 2018) demonstrate a 23.7% F1 performance gain over the SOTA baseline."
        },
        {
            "heading": "2 Background",
            "text": "For the FEVER task (Thorne et al., 2018), automatic FV systems need to determine a threeway veracity label, i.e., SUPPORTS, REFUTES or NOT ENOUGH INFO, for each human-generated claim based on a set of supporting evidence from Wikipedia. The FV system can be split into document retrieval, evidence retrieval, and claim verification phases. The first phase is to retrieve related documents from the corpus. The second phase aims to extract evidence from all sentences of the recalled documents. The final phase aggregates the information of the retrieved evidence to predict a claim label. In this work, our contributions are focused on the evidence retrieval step. The related work and more extensive discussions are included in Appendix A.1."
        },
        {
            "heading": "3 Our Approach",
            "text": "Based on the related documents (i.e., Wikipedia pages) obtained by a common document retrieval method with entity linking (Hanselowski et al., 2018; Liu et al., 2020), FER comprises two components: (i) coarse retrieval: using a PRP-based retrieval model to recall a set of candidate sentences to the claim from the related documents; and (ii) fine-grained retrieval: selecting evidential sentences from the candidate set based on the feedback from the claim verifier. The overall architecture of FER is illustrated in Figure 1."
        },
        {
            "heading": "3.1 Coarse retrieval",
            "text": "For a given claim, we first retrieve a set S of candidate sentences from the related documents based on the PRP. Following (Zhou et al., 2019; Liu et al., 2020), we employ the base version of BERT for coarse retrieval. We utilize the hidden state of the \u201c[CLS]\u201d token to represent the claim and sentence pair. To project the \u201c[CLS]\u201d hidden state to a ranking score, we employ a linear layer followed by a tanh activation function. Lastly, we optimize the BERT-based ranking model using a typical pairwise loss function. During inference, given a test claim, we take the top-K ranked sentences to form the candidate set S."
        },
        {
            "heading": "3.2 Fine-grained retrieval with feedback",
            "text": "Given the candidate set S, we train the fine-grained evidence retriever R\u03b8 with the feedback from the claim verifier. We leverage the base version of BERT to implement R\u03b8. The claim and all sentences in S are concatenated as a single input sequence, with a special token [CLS] added at the\nbeginning of each sentence to indicate its presence. The training objective L for R\u03b8 consists of two parts, i.e., an evidence classification Lcla and a utility divergence Luti, denoted as:\nL = \u03b1Lcla + \u03b2Luti,\nwhere \u03b1 and \u03b2 are coefficients. Evidence classification. We first leverage the ground-truth evidence to boost plausibility. Specifically, we input the concatenated sequence into BERT and leverage the hidden state of each [CLS] to classify the corresponding sentence through linear layers and ReLU activation functions. Here, g \u2208 {0, 1} is the sentence label where 1 or 0 represents whether a sentence is the ground-truth evidence or not. Then, the classification loss is defined as a measure of the difference between the predicted evidence and the ground-truth evidence via binary cross-entropy loss, i.e.,\nLcla = \u2212 \u2211 s\u2208S g log(p(s))+ (1\u2212 g) log(1\u2212 p(s)),\nwhere s is a sentence in the candidate set S and p(s) is the predicted probability of s by R\u03b8 as the ground-truth. During training, this loss function Lcla encourages the retriever to select evidence sentences that are deemed plausible, representing ground-truth evidence.\nUtility divergence based on the claim verifier. To further ensure that the retrieved evidence improves verification performance, we propose to get feedback from the claim verifier as a signal for optimizing the evidence retriever. Here, we leverage the utility that the verifier obtains by consuming the retrieved evidence as the feedback. Specifically, for each claim c, we can measure the divergence in utility between the verifier\u2019s judgements based on the ground-truth evidence and those based on the\nsentences provided by the retriever when predicting the claim label, i.e.,\nLuti = y\u2217D\u03d5(c, E\u2217)\u2212 y\u2217D\u03d5(c,R\u03b8(c, S)),\nwhere c is the given claim, E\u2217 denotes the groundtruth evidence and y\u2217 denotes a one-hot indicator vector of the ground-truth claim label. R\u03b8(c, S) represents the sentences selected by the evidence retriever, which takes the candidate set S and the claim c as input. D\u03d5(\u00b7) is the probability distribution predicted by the claim verifier. The details of loss gradient passed to the fine-grained retriever can be found in the Appendix A.2.\nHere, we also use BERT\u2019s base version as the verifier in FER. During training, the verifier performs classification on the concatenated input, i.e., [CLS]+ c+ [SEP]+E\u2217+ [SEP], and leverage the hidden state of [CLS] to classify c into three categories via a linear layer and softmax function. After training, the verifier is fixed and we directly compute the probability prediction vector D\u03d5(\u00b7) based on E\u2217 and R\u03b8(c, S), respectively. Note that we can leverage other advanced claim verifiers to provide feedback (Section 4.4). The loss function Luti encourages the retriever to prioritize the selection of sentences that are crucial for claim verification.\nAt inference time, given a test claim, we leverage the optimized retriever to retrieve evidence (Section 4.2). Then, we can directly input the retrieved evidence to several advanced verification models to verify the claim (Section 4.3)."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental settings",
            "text": "We conduct experiments on the FEVER benchmark dataset (Thorne et al., 2018), which com-\nprises 185,455 claims with 5,416,537 Wikipedia documents from the June 2017 Wikipedia dump. All claims are annotated as \u201cSUPPORTS\u201d, \u201cREFUTES\u201d or \u201cNOT ENOUGH INFO\u201d.\nFor evaluation, we leverage official evaluation metrics, i.e., Precision (P), Recall (R), and F1 for evidence retrieval, and the FEVER score and Label Accuracy (LA) for claim verification. We prioritize the F1 as our primary metric for evidence retrieval because it directly reflects the model performance in terms of retrieving precise evidence.\nThe candidate set size K is set to 25; both \u03b1 and \u03b2 are set to 1. All hyper-parameters are tuned on the development set. We include detailed descriptions of the implementation details, evaluation metrics, and baselines in Appendix A.3."
        },
        {
            "heading": "4.2 Results on evidence retrieval",
            "text": "We select several representative evidence retrieval models as our baselines and conduct experiments on both the development and test set following the common setup in FEVER. Based on the results presented in Table 1, we find that: (i) FER significantly outperforms the state-of-the-art methods in terms of P and F1, demonstrating its superiority in providing supporting evidence for claim verification based on utility. (ii) The superior precision achieved by FER comes at the expense of lower recall when compared to baselines. The reason might be that FER aims to retrieve more precise and concise sets of evidence for each claim. That is, FER results in a smaller amount of retrieved evidence compared to the baselines. Similarly, DQN, GERE and Stammbach also aim to identify precise evidence. However, their P and F1 scores are substantially lower than those for FER, indicating that\nconsidering the utility of evidence contributes to retrieving evidential sentences for FV. (iii) FER without Lcla outperforms FER without Luti in terms of P, indicating that feedback from the verifier plays a crucial role in aiding the retriever to identify precise evidence that is crucial for the verification process. FER without Lcla retrieves a smaller number of sentences than FER without Luti, i.e., its performance in terms of R and F1 is poorer. Some case studies can be found in Appendix A.4.2."
        },
        {
            "heading": "4.3 Results on claim verification",
            "text": "To better understand the effectiveness of evidence retrieved by FER, we choose several advanced claim verification models, and provide them with evidence retrieved by FER and evidence obtained from the original paper, respectively. The experiments are reported on the test set; see Table 2. Similar findings can be obtained on the development set; see Appendix A.4.1. All the claim verification models leveraging evidence retrieved by FER outperform their respective original versions based on PRP. By conducting further analyses, we find that the evidence retrieved based on the likelihood of being relevant to the claim in the original papers may not be always useful for the verification process or contain conflicting pieces. FER is able to select more precise evidence for claim verification, contributing to the verification outcome and leading to improved results."
        },
        {
            "heading": "4.4 Quality analysis",
            "text": "Feedback using different verifiers. Here, we explore the potential of utilizing off-the-shelf claim verifiers to offer feedback to R\u03b8 in our FER. From Table 2, KGAT (Liu et al., 2020) and GAT exhibit promising verification performance, making them suitable choices for providing feedback. BERT Pair cannot be employed for providing feedback to FER, since it produces multiple independent probabilities instead of a probability distribution encompassing the final claim labels. In future work, we will explore alternative feedback formulations to incorporate additional verifiers. The experiments are reported on the test set; see Table 3. Similar findings have been obtained on the development set; see Appendix A.4.1. Obtaining feedback from other verifiers could also show promising evidence retrieval performance; this result showcases the adaptability of the proposed FER method, which effectively adjusts to different verifiers. Besides, good retrievers and verification models can mutually benefit from each other\u2019s strengths."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we proposed FER (feedback-based evidence retrieval) for FV, a novel evidence retrieval framework that incorporates feedback from the claim verifier. Unlike traditional approaches\nbased on PRP, FER places an emphasis on the utility of evidence that contributes meaningfully to the verification process, going beyond mere relevance. Through the integration of feedback from the verifier, FER effectively identifies the evidence that is both relevant and useful for claim verification. The experimental results on the FEVER dataset demonstrated the effectiveness of FER."
        },
        {
            "heading": "Limitations",
            "text": "In this paper, we utilized the performance disparity between ground-truth evidence and retrieved evidence for claim verification as a form of feedback to train an evidence retrieval model. There are two primary limitations that should be acknowledged: (i) Currently, the retrieval and verification models are optimized independently, lacking conditional optimization or joint end-to-end optimization. Future research could explore approaches to optimize both the retrieval and verification models in a single objective function. In this manner, we expect to see improved overall performance. (ii) In this study, we focused solely on investigating the probability distribution generated by the claim verifier to compute the utility divergence. However, it is important to note that there exist multiple methods for quantifying the divergence in utility of retrieval results, e.g., the gradients of the verification loss. Additionally, it is worth considering that there may be various forms of feedback that can be incorporated. We hope that this research will inspire further exploration and attention in this area for future studies. (iii) Finally, we only demonstrated the effectiveness of the proposed FER method on a single dataset, the FEVER dataset, and the evidence retrieval process. We encourage future work aimed at the creation of further claim verification datasets and document retrieval process."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was funded by the National Natural Science Foundation of China (NSFC) under Grants No. 62006218 and 61902381, the China Scholarship Council under Grants No. 202104910234, the Youth Innovation Promotion Association CAS under Grants No. 2021100, the project under Grants No. JCKY2022130C039 and 2021QY1701, and the Lenovo-CAS Joint Lab Youth Scientist Project. This work was also (partially) funded by the Hybrid Intelligence Center, a 10-year program funded by the\nDutch Ministry of Education, Culture and Science through the Netherlands Organisation for Scientific Research, https://hybrid-intelligence-centre. nl, and project LESSEN with project number NWA.1389.20.183 of the research program NWA ORC 2020/21, which is (partly) financed by the Dutch Research Council (NWO).\nAll content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors. We would like to thank the reviewers for their valuable feedback and suggestions."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Related work",
            "text": "In this section, we briefly review two lines of related work, i.e., fact verification and retrievalenhanced machine learning."
        },
        {
            "heading": "A.1.1 Fact Verification",
            "text": "The real-world fact verification (FV) task focuses on verifying the accuracy of claims made by humans through the retrieval of evidential sentences from reliable corpora such as Wikipedia (Chen et al., 2022a). Current FV models typically adopt a three-step pipeline framework, which includes document retrieval, evidence retrieval, and claim verification.\nIn the document retrieval stage, existing works could be broadly categorized into three categories: entity linking (Hanselowski et al., 2018; Chernyavskiy and Ilvovsky, 2019; Zhao et al., 2020; Liu et al., 2020), keyword matching (Nie et al., 2019a; Ma et al., 2019; Nie et al., 2019b) and feature-based (Hidey and Diab, 2018; Yang et al., 2017; Chen et al., 2022a) methods.\nIn the evidence retrieval stage, existing FV works commonly utilize retrieval models that are built upon the probability ranking principle (PRP) (Robertson, 1977) in the field of information retrieval (IR). These models aim to select the topranked sentences as evidence based on relevance ranking (Jiang et al., 2021; Soleimani et al., 2020; Zhou et al., 2019; Liu et al., 2020; Hanselowski et al., 2018; Wan et al., 2021). Very recently, Chen et al. (2022a); Liu et al. (2023) proposed to retrieve evidence in a generative fashion by generating document titles and evidence sentence identifiers.\nIn the claim verification stage, considerable attention has been given to this step in many existing FV works. These mainly include implication-based inference methods that treat the verification task as an entailment task (Thorne et al., 2018; Nie et al., 2019a; Krishna et al., 2022; Nie et al., 2019b), as well as graph neural network-based methods (Liu et al., 2020; Zhou et al., 2019) that frame the verification task as a graph reasoning task. It is worth noting that the accuracy of this stage heavily relies on the retrieved evidence. Therefore, having a precise set of evidence can significantly impact the verification outcome and lead to improved results.\nA.1.2 Retrieval-enhanced machine learning As mentioned in Zamani et al. (2022), one approach to improve accuracy is by increasing the parameter\nsize of machine learning models. However, it is important to consider that this approach comes with the trade-off of increased costs in model design and training. Hence, the task of alleviating model memory through retrieval enhancement holds significant applications in various domains (Sharma et al., 2021; He et al., 2023; Chen et al., 2022b), e.g., commonsense generation (Wang et al., 2021; Fan et al., 2020), dialogue systems (Song et al., 2016; Zhang et al., 2022; Zhu et al., 2019; Chen et al., 2022c; Parvez et al., 2023), summarization (An et al., 2021), and fact verification (Thorne et al., 2018).\nRetrieval-enhanced machine learning methods could be mainly divided into two categories: (i) The first one is retrieval-only: the retrieval model acts as an auxiliary tool to provide query-related responses to the prediction model. Note that the majority of current fact verification models primarily belong to this category. (ii) The second one is retrieval with feedback: the retrieval model is further trained based on the feedback provided by the prediction model. Very recently, Hu et al. (2023) proposed to utilize the loss value of the prediction model as a signal for optimizing the retrieval model. However, this method may not be suitable for large datasets because it relies on all provided sentences to compute the loss. Additionally, without access to ground-truth evidence for the prediction model, accurately capturing the utility of retrieved sentences may be challenging."
        },
        {
            "heading": "A.2 The training process of the fine-grained retriever",
            "text": "In this section, we provide a detailed description of training process of the fine-grained retriever, given the K candidate sentences from the coarse retriever, unfolds as follows:\nThe fine-grained retriever selects sentences from the candidate set. \u2022 The claim and K candidate sentences [s1, s2, . . . , sK ] are concatenated with a special token [CLS] at the beginning of each sentence and then provided as input to the fine-grained retriever. \u2022 Subsequently, the hidden state of each [CLS] undergoes a transformation through a linear layer followed by a ReLU activation, yielding K sets of 2-dimensional vectors, for example, [[0.1, 0.9], [0.2, 0.8], ..., [0.7, 0.3]]. \u2022 Finally, Gumbel Softmax is applied to these K 2-dimensional classification vectors, producing a\nsequence-length K vector like [1, 1, ..., 0], where 1 indicates the predicted evidence sentence. The fine-grained retriever forwards the selected sentence information to the verifier. \u2022 The sequence-length K vector is adaptively ex-\ntended to match the lengths of the K candidate sentences, aligning with the respective sentence lengths. For instance, [len(s1) \u2217 1, len(s2) \u2217 1, . . . , len(sK) \u2217 0] represents the selected vectors, which serve as the input_mask for the BERT tokenizer. Here, len(s1) \u2217 1 signifies the repetition of 1 for the length of s1. \u2022 The selected vectors [len(s1) \u2217 1, len(s2) \u2217 1, . . . , len(sK)\u22170] from the retriever, along with the K discrete sentences [s1, s2, . . . , sK ], are provided as input_mask and input_ids, respectively, to the verifier using the BERT tokenizer. \u2022 For the ground-truth evidence, the input_ids corresponds to the true discrete evidence, while the input_mask are represented as the unit vector. \u2022 Subsequently, the utility divergence between the verifier\u2019s judgments based on ground-truth evidence and those derived from retriever-provided sentences is computed, all in the context of predicting the claim label. The verifier back-propagates the loss gradient to the fine-grained retriever. \u2022 Following the computation of the utility diver-\ngence, gradients are back-propagated to the selected vectors, e.g., [len(s1) \u2217 1, len(s2) \u2217 1, . . . , len(sK) \u2217 0]. \u2022 As the Gumbel Softmax function is differentiable, gradients permeate through the selected vectors, extending back to the retriever."
        },
        {
            "heading": "A.3 Reproducibility",
            "text": "In this section, we introduce our experimental details. The source code and trained models will be made publicly available upon publication to improve the reproducibility of results."
        },
        {
            "heading": "A.3.1 Experimental settings",
            "text": "For document retrieval, we adopt the entity linking approach (Hanselowski et al., 2018) to retrieve relevant documents, following the methodology of Hanselowski et al. (2018). On average, four documents are retrieved for each claim. Following the traditional entity linking pipeline approach, there are three steps for document retrieval. First, the claims are parsed using AllenNLP (Gardner et al., 2018) to extract multiple entities. Secondly, the\nMediaWiki API2 is utilized to search for page titles that correspond to the identified entities. Lastly, the retrieval results are filtered using the entity coverage limitation to select the appropriate documents.\nFor coarse retrieval, we follow the approach described in (Liu et al., 2020), where we utilize the base version of the BERT model (Devlin et al., 2019) from Hugging Face3 to implement our coarse-grained retrieval model. We use the \u201c[CLS]\u201d hidden state to represent claim and sentence pairs. Training samples are constructed by using both ground-truth evidence and non-ground-truth sentences from document retrieval. The pairwise training method is employed to train the retrieval ranking model. The max length of the input is set to 130. We use the Adam optimizer with a learning rate of 5e-5 and a warm-up proportion of 0.1.\nFor fine-grained retrieval, it contains the finegrained retrieval model R\u03b8 and the verifier. Specifically, R\u03b8 concatenates the claim and the retrieved candidate sentences and feeds them into the BERT model. The maximum length is set to 512, and the sentences are separated using the special token \u201c[CLS]\u201d. The hidden vector corresponding to the \u201c[CLS]\u201d token is then used for evidence classification. The learning rate for the AdamW (Loshchilov and Hutter, 2019) optimizer is set to 2e-5. In the verifier model, the input consists of the concatenated sequence of the claim and the ground-truth evidence. The batch size is set to 5, and the accumulate step is also set to 5. The learning rate for the AdamW optimizer is set to 3e-5. The hyperparameters of \u03b1 and \u03b2 are set to 1 and 1, respectively.\nFurthermore, all models are implemented using the PyTorch framework. For online evidence evaluation, only the initial five sentences of predicted evidence provided by the candidate system are utilized for scoring. In order to adhere to the specifications of the online evaluation, the baselines and our FER select the five sentences as the evidence."
        },
        {
            "heading": "A.3.2 Evaluation metric",
            "text": "For evaluation on the FEVER benchmark dataset4, we use the official evaluation metrics, i.e., Precision (P), Recall (R), and F1 for evidence retrieval; FEVER score and Label Accuracy (LA) for claim verification. Following the official evaluation (Thorne et al., 2018), we compute P@5, R@5 and\n2https://www.mediawiki.org/wiki/API:Main_pagel 3https://huggingface.co/ 4https://fever.ai/dataset/fever.html\nF1@5. FEVER5 evaluates accuracy based on the condition that the predicted evidence fully covers the ground-truth evidence. LA assesses the accuracy of the claim label prediction without taking into account the validity of the retrieved evidence.\nA.3.3 Evidence retrieval baselines In our work, we consider several advanced evidence retrieval baselines. \u2022 TF-IDF (Thorne et al., 2018) is a traditional\nsparse retrieval model that combines bigram hashing and TF-IDF matching to return relevant documents. \u2022 ColumbiaNLP (Chakrabarty et al., 2018) initially utilizes TF-IDF to rank the candidate sentences and subsequently selects the top 5 sentences with the highest relevance. To mitigate the presence of noisy data, ELMo embeddings (Sarzynska-Wawer et al., 2021) are employed to convert the claims and sentences into vectors. Subsequently, the top-3 sentences with the highest cosine similarity are extracted as the final retrieval results. \u2022 UKP-Athene (Hanselowski et al., 2018) introduces a sentence ranking model utilizing the Enhanced Sequential Inference Model (ESIM) (Hanselowski et al., 2018). This model takes a claim and a sentence as input, and the predicted ranking score is obtained by passing the last hidden layer of the ESIM through a single neuron. \u2022 GEAR (Zhou et al., 2019) enhances the UKPAthene model by introducing a threshold. Sentences with relevance scores higher than the threshold (set to 0.001) are filtered and considered as retrieval results. \u2022 Kernel Graph Attention Network (KGAT) (Liu et al., 2020) utilizes both ESIM and BERT (Devlin et al., 2019) to construct evidence retrieval models, which are trained in a pairwise manner. \u2022 DREAM (Zhong et al., 2020) employs the contextual representation models XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019) to assess the relevance of a claim to each candidate evidence. \u2022 NSMN (Nie et al., 2019a) employs a direct traversal approach, where all sentences are compared with the claim to calculate relevance scores. Sentences with relevance scores higher than the threshold (set to 0.5) are considered as evidence.\n5https://codalab.lisn.upsaclay.fr/ competitions/7308\n\u2022 Deep Q-learning Network (DQN) (Wan et al., 2021) utilizes the RoBERTa for evidence representation and applies the deep Q-learning network to select precise evidence. \u2022 GERE (Chen et al., 2022a) employs the new paradigm of generative retrieval to generate the relevant evidence identifiers. \u2022 Stammbach (Stammbach, 2021) employs tokenbased single-document candidate sentence classification to retrieve evidence, which uses RoBERTa (Liu et al., 2019) and BigBird (Zaheer et al., 2020) to encode candidate sentences. To ensure a fair comparison, we adopt the results based on RoBERTa as our baseline.\nA.3.4 Veracity prediction In our work, we also leverage several advanced claim verification models for verification based on the retrieved evidence by FER. \u2022 BERT-pair and BERT concat (Zhou et al.,\n2019) consider claim-evidence pairs individually, or stitch all evidence and the claim together to predict the claim label. \u2022 GEAR (Zhou et al., 2019) considers the influence between evidence using a graphical attention network and aggregates all evidence through the attention layer. \u2022 KGAT (Liu et al., 2020) introduces a graph attention network to measure the importance of evidence nodes and the propagation of evidence among them through node and edge kernels.\nA.4 Experimental results A.4.1 Performance on the development set. As shown in Table 4, we report the performance of different claim verification models on the development set using the evidence retrieved by FER. As shown in Table 5, we report the evidence retrieval performance of our method with feedback from different verifiers on the development set. As shown in Figure 3, we report the effect of the size K on the final evidence retrieval performance on the development set."
        },
        {
            "heading": "A.4.2 Case study",
            "text": "Table 6 presents two illustrative examples from the FEVER development set. We can observe that: (i) The sentences provided by the coarse retrieval method, even though they are ranked highly, do not always offer useful information for claim verification, specifically in terms of providing ground-truth evidence. For example, in the second instance, out of the top-5 ranked sentences, only two of them actually consist of ground-truth evidence. (ii) Through fine-grained retrieval, we can effectively identify ground-truth evidence that might have initially been ranked low, such as the 14-th sentence in the first instance and the 11-th sentence in the second instance. (iii) In the case of fine\u2013 grained retrieval, not all ground-truth evidence are successfully identified. For example, in the second instance, the 8-th sentence is not identified. However, as observed in the given example, only the 8-th sentence is not retrieved, which does not significantly affect the judgment of the claims. Nonetheless, the issue of the low recall rate necessitates further resolution in future research."
        }
    ],
    "title": "From Relevance to Utility: Evidence Retrieval with Feedback for Fact Verification",
    "year": 2023
}