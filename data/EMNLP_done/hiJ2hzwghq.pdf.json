{
    "abstractText": "Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BIASX, a framework that assists content moderators with free-text explanations of statements\u2019 implied social biases, and explore its effectiveness through a large-scale user study. We show that participants indeed benefit substantially from explanations for correctly moderating subtly (non-)toxic content. The quality of explanations is critical: imperfect machine-generated explanations (+2.4% on hard toxic examples) help less compared to expert-written human explanations (+7.2%). Our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Yiming Zhang"
        },
        {
            "affiliations": [],
            "name": "Sravani Nanduri"
        },
        {
            "affiliations": [],
            "name": "Liwei Jiang"
        },
        {
            "affiliations": [],
            "name": "Tongshuang Wu"
        },
        {
            "affiliations": [],
            "name": "Maarten Sap"
        }
    ],
    "id": "SP:30a1e641b20174e5f5221f51b63db28781966af2",
    "references": [
        {
            "authors": [
                "Gagan Bansal",
                "Tongshuang Wu",
                "Joyce Zhou",
                "Raymond Fok",
                "Besmira Nushi",
                "Ece Kamar",
                "Marco Tulio Ribeiro",
                "Daniel Weld."
            ],
            "title": "Does the whole exceed its parts? the effect of ai explanations on complementary team performance",
            "venue": "Proceedings",
            "year": 2021
        },
        {
            "authors": [
                "Luke Breitfeller",
                "Emily Ahn",
                "David Jurgens",
                "Yulia Tsvetkov."
            ],
            "title": "Finding Microaggressions in the Wild: A Case for Locating Elusive Phenomena in Social Media Posts",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "\u00c1ngel D\u00edaz",
                "Laura Hecht-Felella."
            ],
            "title": "Double Standards in Social Media Content Moderation",
            "venue": "Technical report, Brennan Center for Justice.",
            "year": 2021
        },
        {
            "authors": [
                "Emily Dinan",
                "Samuel Humeau",
                "Bharath Chintagunta",
                "Jason Weston."
            ],
            "title": "Build it break it fix it for dialogue safety: Robustness from adversarial human attack",
            "venue": "EMNLP-IJCNLP.",
            "year": 2019
        },
        {
            "authors": [
                "Lucas Dixon",
                "John Li",
                "Jeffrey Sorensen",
                "Nithum Thain",
                "Lucy Vasserman."
            ],
            "title": "Measuring and mitigating unintended bias in text classification",
            "venue": "AIES, AIES \u201918.",
            "year": 2018
        },
        {
            "authors": [
                "Tim Draws",
                "Alisa Rieger",
                "Oana Inel",
                "Ujwal Gadiraju",
                "Nava Tintarev."
            ],
            "title": "A checklist to combat cognitive biases in crowdsourcing",
            "venue": "AAAI Conference on Human Computation & Crowdsourcing.",
            "year": 2021
        },
        {
            "authors": [
                "Franz Faul",
                "Edgar Erdfelder",
                "Axel Buchner",
                "AlbertGeorg Lang"
            ],
            "title": "Statistical power analyses using G*Power 3.1: Tests for correlation and regression analyses",
            "venue": "Behavior Research Methods,",
            "year": 2009
        },
        {
            "authors": [
                "Saadia Gabriel",
                "Skyler Hallinan",
                "Maarten Sap",
                "Pemi Nguyen",
                "Franziska Roesner",
                "Eunsol Choi",
                "Yejin Choi."
            ],
            "title": "Misinfo reaction frames: Reasoning about readers\u2019 reactions to news headlines",
            "venue": "ACL.",
            "year": 2022
        },
        {
            "authors": [
                "Sam Gehman",
                "Suchin Gururangan",
                "Maarten Sap",
                "Yejin Choi",
                "Noah A Smith."
            ],
            "title": "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
            "venue": "Findings of EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Xiaochuang Han",
                "Yulia Tsvetkov."
            ],
            "title": "Fortifying toxic speech detectors against veiled toxicity",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen"
            ],
            "title": "DEBERTA: DECODINGENHANCED BERT WITH DISENTANGLED ATTENTION",
            "venue": "In International Conference on Learning Representations",
            "year": 2021
        },
        {
            "authors": [
                "William James",
                "Frederick Burkhardt",
                "Fredson Bowers",
                "Ignas K Skrupskelis."
            ],
            "title": "The principles of psychology, volume 1",
            "venue": "Macmillan London.",
            "year": 1890
        },
        {
            "authors": [
                "Jialun Aaron Jiang",
                "Peipei Nie",
                "Jed R. Brubaker",
                "Casey Fiesler."
            ],
            "title": "A Trade-off-centered Framework of Content Moderation",
            "venue": "ACM Transactions on Computer-Human Interaction, 30(1):3:1\u20133:34.",
            "year": 2023
        },
        {
            "authors": [
                "Vivian Lai",
                "Han Liu",
                "Chenhao Tan."
            ],
            "title": "Why is \u2019Chicago\u2019 Deceptive?\" towards building modeldriven tutorials for humans",
            "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, CHI \u201920, pages 1\u201313, New York, NY,",
            "year": 2020
        },
        {
            "authors": [
                "Vivian Lai",
                "Yiming Zhang",
                "Chacha Chen",
                "Q. Vera Liao",
                "Chenhao Tan"
            ],
            "title": "Selective Explanations: Leveraging Human Input to Align Explainable AI",
            "year": 2023
        },
        {
            "authors": [
                "Chaitanya Malaviya",
                "Sudeep Bhatia",
                "Mark Yatskar."
            ],
            "title": "Cascading biases: Investigating the effect of heuristic annotation strategies on data and models",
            "venue": "EMNLP.",
            "year": 2022
        },
        {
            "authors": [
                "Antonis Maronikolakis",
                "Axel Wisiorek",
                "Leah Nann",
                "Haris Jabbar",
                "Sahana Udupa",
                "Hinrich Schuetze"
            ],
            "title": "Listening to Affected Communities to Define Extreme Speech: Dataset and Experiments",
            "year": 2022
        },
        {
            "authors": [
                "Tim Miller."
            ],
            "title": "Explanation in artificial intelligence: Insights from the social sciences",
            "venue": "Artificial intelligence.",
            "year": 2019
        },
        {
            "authors": [
                "Moin Nadeem",
                "Anna Bethke",
                "Siva Reddy"
            ],
            "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
            "year": 2020
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "ArXiv, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Jiaxin Pei",
                "David Jurgens"
            ],
            "title": "When Do Annotator Demographics Matter? Measuring the Influence of Annotator Demographics with the POPQUORN Dataset",
            "year": 2023
        },
        {
            "authors": [
                "Bj\u00f6rn Ross",
                "Michael Rist",
                "Guillermo Carbonell",
                "Benjamin Cabrera",
                "Nils Kurowsky",
                "Michael Wojatzki"
            ],
            "title": "Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis",
            "year": 2016
        },
        {
            "authors": [
                "Paul Rottger",
                "Bertie Vidgen",
                "Dirk Hovy",
                "Janet Pierrehumbert."
            ],
            "title": "Two contrasting data annotation paradigms for subjective NLP tasks",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Maarten Sap",
                "Dallas Card",
                "Saadia Gabriel",
                "Yejin Choi",
                "Noah A Smith."
            ],
            "title": "The risk of racial bias in hate speech detection",
            "venue": "ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Maarten Sap",
                "Saadia Gabriel",
                "Lianhui Qin",
                "Dan Jurafsky",
                "Noah A Smith",
                "Yejin Choi."
            ],
            "title": "Social bias frames: Reasoning about social and power implications of language",
            "venue": "ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Maarten Sap",
                "Swabha Swayamdipta",
                "Laura Vianna",
                "Xuhui Zhou",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "Annotators with attitudes: How annotator beliefs and identities bias toxic language detection",
            "venue": "NAACL.",
            "year": 2022
        },
        {
            "authors": [
                "A. Tversky",
                "D. Kahneman."
            ],
            "title": "Judgment under Uncertainty: Heuristics and Biases",
            "venue": "Science (New York, N.Y.), 185(4157):1124\u20131131.",
            "year": 1974
        },
        {
            "authors": [
                "Helena Vasconcelos",
                "Matthew J\u00f6rke",
                "Madeleine GrundeMcLaughlin",
                "Tobias Gerstenberg",
                "Michael Bernstein",
                "Ranjay Krishna"
            ],
            "title": "Explanations Can Reduce Overreliance on AI Systems During DecisionMaking",
            "year": 2023
        },
        {
            "authors": [
                "Monnica T. Williams."
            ],
            "title": "Microaggressions: Clarification, evidence, and impact",
            "venue": "Perspectives on Psychological Science, 15(1):3\u201326.",
            "year": 2020
        },
        {
            "authors": [
                "Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-Art Natural Language Processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Xuhui Zhou",
                "Hao Zhu",
                "Akhila Yerukola",
                "Thomas Davidson",
                "Jena D. Hwang",
                "Swabha Swayamdipta",
                "Maarten Sap."
            ],
            "title": "Cobra frames: Contextual reasoning about effects and harms of offensive statements",
            "venue": "Findings of ACL.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Content moderation and online hate speech detection frameworks, which aim to flag prejudiced, hateful, or otherwise toxic content, often fall prey to spurious correlations and lexical biases when deployed (Draws et al., 2021). For example, content moderators often assume that statements without hateful or profane words are not prejudiced or toxic (such as the subtly sexist statement in Figure 1), without deeper reasoning about potentially biased implications (Breitfeller et al., 2019; Sap et al., 2022). In the meantime, they overflag benign posts such as those that contain expletives or in-group phrases used by minorities (Dixon et al., 2018; Sap et al., 2019). These biases present in content moderation can suppress harmless speech by and about minorities (Yasin, 2018) , and risk hindering equitable online experiences.\nA major cause of such biases in moderation is the use of mental shortcuts by moderators. Defined by\n1Our code is publicly available at https://github. com/Y0mingZhang/biasx.\nTversky and Kahneman (1974), mental shortcuts are heuristics people employ in decision-making when facing uncertainty or pressure. Exacerbated by an increasing time pressure to moderate content (Roberts, 2019), the use of these heuristics by annotators can cascade to poor dataset quality and biased models (Malaviya et al., 2022).\nTo mitigate such shortcuts, we introduce BIASX, a framework to enhance content moderators\u2019 decision making with free-text explanations of a potentially toxic statement\u2019s targeted group and subtle biased or prejudiced implication (Figure 1). We take inspiration from cognitive science\u2019s dual process theory (James et al., 1890), BIASX is meant to encourage more conscious reasoning about statements beyond what is written (\u201cthinking slow\u201d; Kahneman, 2011), to circumvent the mental shortcuts and cognitive heuristics resulting from automatic processing (\u201cthinking fast\u201d) that often lead to a drop in model and human performance alike (Malaviya et al., 2022).2 To this end, we ground BIASX in SOCIAL BIAS FRAMES (Sap et al., 2020), a linguistic framework that explicit spells out the biases and offensiveness implied\u2014 but not written\u2014in text.\n2Note, \u201cthinking slow\u201d refers a deeper and more thoughtful reasoning about statements and their implications, not necessarily slower in terms of reading or decision time.\nVia a large-scale (N > 450) user study, we evaluate the usefulness of BIASX, and explore three primary research questions: (1) When do free-text explanations help improve the content moderation quality, and how? (2) Is the explanation format in BIASX effective? and (3) How might the quality of the explanations affect their helpfulness? Our results show that BIASX indeed helps moderators better detect hard, subtly toxic instances, as reflected both in increased moderation performance and subjective feedback, demonstrating the promise of domain-specific free-text explanations.\nNotably, we also find that explanation quality matters: models sometimes miss the veiled biases that are present in text, making their explanations unhelpful or even counterproductive for users. Our findings serve as a proof of concept in showing the promise of free-text explanations in improving content moderation fairness, while highlighting the need for AI systems that are more capable of identifying and explaining subtle biases in text."
        },
        {
            "heading": "2 Explaining (Non-)Toxicity with BIASX",
            "text": "The goal of our work is to help content moderators reason through whether statements could be subtly or implicitly biased, prejudiced, or offensive \u2014 to help them explicitly flag microaggressions and social biases projected by statements which are often missed, and alleviate the over-moderation of deceivingly non-toxic statements (Dixon et al., 2018; Dinan et al., 2019; Sap et al., 2022). To do so, we propose BIASX, a framework for assisting content moderators with free-text explanations of implied social biases. There are two primary design desiderata in the design of BIASX:\nFree-text explanations. Identifying and explaining implicit biases in online social interactions is difficult, as the underlying stereotypes are rarely stated explicitly by definition; this is nonetheless important due to the risk of harm to individuals (Williams, 2020). Psychologists have argued that common types of explanation in literature, such as highlights and rationales (e.g., Lai et al., 2020; Vasconcelos et al., 2023) or classifier confidence scores (e.g., Bansal et al., 2021) are of limited utility to humans (Miller, 2019). This motivates the need for explanations that go beyond what is written. Inspired by Gabriel et al. (2022) who use AI-generated free-text explanations of an author\u2019s likely intent to help users identify misinformation in news headlines, we propose to focus on free-text\nexplanations of offensiveness, which has the potential of communicating rich information to humans.\nImplied Social Biases. To maximize its utility, we further design BIASX to optimize for content moderation, by grounding the explanation format in the established SOCIAL BIAS FRAMES (SBF; Sap et al., 2020) formalism. SBF distills biases and offensiveness that are implied in language, and its definition and demonstration of implied stereotype naturally allows us for explaining subtly toxic statements. Specifically, for toxic posts, BIASX explanations take the same format as SOCIAL BIAS FRAMES, which spells out both the targeted group and the implied stereotype, as shown in Figure 1.\nOn the other hand, moderators also need help to avoid blocking benign posts that are seemingly toxic (e.g., positive posts with expletives or statements denouncing biases). To accommodate this need, we extend SOCIAL BIAS FRAMES-style implications to provide explanations of why a post might be non-toxic. For a non-toxic statement, the explanation acknowledges the (potential) aggressiveness of the statement while noting the lack of toxicity: given the statement \u201cThis is fucking annoying because it keeps raining in my country\u201d, BIASX could provide an explanation \u201cUses profanity without prejudice or hate\u201d.3"
        },
        {
            "heading": "3 Experiment Design",
            "text": "To explore the following questions, we conduct a user study to measure the effectiveness of BIASX. Q.1 Does BIASX improve the content moderation\nquality, especially on challenging instances? Q.2 Do BIASX explanations enable moderators to\nthink carefully about moderation decisions? Q.3 Are higher quality explanation more effective?\nTo answer these questions, we design a crowdsourced user study that simulates a real content moderation environment: crowdworkers are asked to play the role of content moderators, and to judge the toxicity of a series of 30 online posts, potentially with explanations from BIASX. Our study incorporates examples of varying difficulties and different forms of explanations as detailed below."
        },
        {
            "heading": "3.1 Experiment Setup",
            "text": "Conditions. Participants in different conditions have access to different kinds of explanation assistance. To answer Q.1 and Q.2, we set two base-\n3A non-toxic statement by definition does not target any minority group, and we use \u201cN/A\u201d as a filler.\nline conditions: (1) NO-EXPL, where participants make decisions without seeing any explanations; (2) LIGHT-EXPL, where we provide only the targeted group as the explanation. This can be considered an ablation of BIASX with the detailed implied stereotype on toxic posts and justification on non-toxic posts removed, and helps us verify the effectiveness of our explanation format. Further, to answer Q.3, we add two BIASX conditions, with varying qualities of explanations following Bansal et al. (2021): (3) HUMAN-EXPL with high quality explanations manually written by experts, and (4) MODEL-EXPL with possibly imperfect machinegenerated explanations.\nData selection. To better tease out when BIASX could be effective, we examine both easy and hard examples; we consider an example hard if it is likely to be mislabeled by an annotator using simple heuristics, such as exclusively using swearwords as the indicator for toxicity.4 Specifically, we define the following three categories of examples: simple examples which are easily flagged as toxic or non-toxic, hard-toxic examples that could potentially be overlooked due to the subtlety of the offensiveness, and hard-non-toxic examples that could be over-flagged as toxic despite being nontoxic. We drew 30 posts (10 from each category) from the SBIC dataset (Sap et al., 2020), which contains short social media posts paired with annotations of toxicity and SBF-implied stereotypes. To identify hard examples, we follow Han and Tsvetkov (2020) and use a fine-tuned DeBERTa toxicity classifier (He et al., 2021) to find misclassified instances from the SBIC test set, which are likely harder than those correctly classified. Among these, two authors removed mislabeled examples, and selected 20 that they agreed were hard but could be unambiguously labeled. The full list of examples can be found in Table 3.\nExplanation generation. To generate explanations for MODEL-EXPL, we used GPT-3.5 (Ouyang et al., 2022) prompted to generate SBF-style explanations with 3 toxic and 3 non-toxic in-context examples from SBIC.5 For the HUMAN-EXPL condition, two authors wrote SBF-style explanations.\n4While the easy vs. hard dichotomy is fuzzy in practice, we nevertheless note that data selection has no bearing on the deployment of our framework.\n5We use text-davinci-003 as the explanation generation model. Further details can be found in Appendix A.1.\nModeration labels. Granularity is desirable in content moderation (D\u00edaz and Hecht-Felella, 2021). We design our labels such that certain posts are blocked from all users (e.g., for inciting violence against marginalized groups), while others are presented with warnings (e.g., for projecting a subtle stereotype). Loosely following the moderation options available to Reddit content moderators, we provide participants with four options: Allow, Lenient, Moderate, and Block. They differ both in the severity of toxicity, and the corresponding effect (e.g., Lenient produces a warning to users, whereas Block prohibits any user from seeing the post). Mirroring most content moderation settings, the goal is for participants to correctly identify the right label for each post, following a prescriptive paradigm of data labeling (Rottger et al., 2022). Appendix B shows the label definitions."
        },
        {
            "heading": "3.2 Study Procedure",
            "text": "A total of N=454 participants recruited from Amazon MTurk are randomly assigned to one of the four conditions, in which they provide labels for 30 selected examples. Upon completion, participants also complete a post-study survey which collects their demographics information and subjective feedback on the moderation task and provided explanations (if any). We report additional details on the user study in Appendix C."
        },
        {
            "heading": "4 Results and Discussion",
            "text": "We analyze the usefulness of BIASX, examining worker moderation accuracy (Figure 2a), efficiency (Figure 2b), and subjective feedback (Figure 3).\nBIASX improves moderation quality, especially on hard-toxic examples. Shown in Figure 2a, we find that HUMAN-EXPL leads to substantial gains in moderation accuracy over the NO-EXPL baseline on both hard-toxic (+7.2%) and hard-nontoxic examples (+7.7%), a result further reflected in a +4.7% improvement overall. This indicates that the free-text explanations of BIASX do encourage content moderators to think more thoroughly about the toxicity of posts beyond what is written.\nIllustrating this effect, we show an example of a hard-toxic statement against transgender people in Figure 4A. While the majority of moderators (60.3%) in the NO-EXPL condition failed to flag this post, BIASX assistance in both MODELEXPL (+20.5%) and HUMAN-EXPL (+18.4%) conditions substantially improved moderator perfor-\nmance. The subjective feedback from moderators further corroborates this observation that BIASX explanations have potential (Figure 3): the majority of moderators agreed or strongly agreed that the BIASX explanations made them more aware of subtle stereotypes (77.1% in MODEL-EXPL; 78.1% in HUMAN-EXPL).\nOur designed explanation format efficiently promotes more thorough decisions. While our explanation-assisted framework improves moderators\u2019 accuracy and awareness of implied biases, it can slow down their labeling compared to the control condition without any explanation, due to the extra text to read.\nWe can quantify and analyze this trade-off by examining a lighter-weight explanation type (LIGHTEXPL) with only the targeted group, reducing the amount of text workers process. In Figure 2b, we indeed see a sizable increase (4-5s) in labeling time for MODEL-EXPL and HUMAN-EXPL. Interestingly, LIGHT-EXPL shares a similar increase in labeling time (\u223c4s). As LIGHT-EXPL has brief explanations (1-2 words), this increase is unlikely to be due to reading, but rather points to additional mental processing. This extra mental processing is further evident from users\u2019 subjective evaluation in Figure 3: 56% participants agreed or strongly agreed that the task was mentally demanding in\nthe LIGHT-EXPL condition, compared to 41% in MODEL-EXPL and in HUMAN-EXPL. This result suggests that providing the targeted group exclusively could mislead moderators without improving accuracy or efficiency.\nSuch a tradeoff between speed and accuracy is often necessary to ensure fair and accurate moderation (Jiang et al., 2023), as moderators very likely resort to heuristics when they do not spend enough time on their decisions. Given the correlation between increased use of heuristics and decreased time spent on task in content moderation (Malaviya et al., 2022), the increase in task time with BIASX explanations further suggests the promotion of deliberate and thoughtful decision-making.\nExplanation quality matters. Compared to expert-written explanations, the effect of modelgenerated explanations on moderator performance is mixed. A key reason behind this mixed result is that model explanations can be wrong, and thus mislead moderators. In Table 1, we compare the correctness of explanations to the accuracy of participants.6 On the hard toxic set, only 60% of model explanations are accurate, which leads to\n6Binarizing instances with moderation labels Allow and Lenient as non-toxic, and Moderate and Block as toxic.\n56.4% worker accuracy, a -7.7% drop from the HUMAN-EXPL condition where workers always have access to correct explanations. Figure 4B shows an example where the model explains an implicitly toxic statement as harmless and misleads content moderators (39.8% in MODEL-EXPL vs. 55.4% in NO-EXPL).\nOn a positive note, expert-written explanations substantially improve moderator performance over baselines, highlighting the potential of our framework with higher quality explanations and serving as a proof-of-concept of BIASX, while motivating future work to explore methods to generate higherquality explanations.7"
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "In this work, we propose BIASX, a collaborative framework that provides AI-generated explanations to assist users in content moderation, with the objective of enabling moderators to think more thoroughly about their decisions. In an online user study, we find that when shown explanations of subtle biases beyond what is written, humans can better identify hard-toxic examples. The even greater gain in performance with expert-written explanations further highlights the potential of framing content moderation under the lens of human-AI collaborative decision making, while motivating future work to build AI systems more capable of identifying and explaining biases in text beyond what current state-of-the-art models can do.\nOur research highlights the importance of adding explanations to help with difficult examples (subtle biases) in free text. Subsequent studies could investigate various forms of free-text explanations and objectives, e.g., reasoning about intent (Gabriel et al., 2022) or distilling possible harms to the targeted groups (e.g., CobraFrames; Zhou et al., 2023).\n7GPT-4 (OpenAI, 2023) reports 80% accuracy on the hard toxic set, a 20% improvement from GPT-3.5. While still not perfect, these explanations have potential to make BIASX more effective.\nOur less significant result on hard-non-toxic examples also sounds a cautionary note, and shows the need for investigating more careful definitions and frameworks around non-toxic examples (e.g., by extending SOCIAL BIAS FRAMES), or exploring alternative designs for their explanations.\nGoing from proof-of-concept to practical usage, we note two additional nuances that deserve careful consideration. On the one hand, our study shows that while explanations have benefits, they come at the cost of a sizable increase in labeling time. We argue for these high-stakes tasks, the increase in labeling time and cost is justifiable to a degree (echoing our intent of encouraging moderators to \u201cthink slow\u201d). However, we do hope future work could look more into potential ways to improve performance while reducing time through, e.g., selectively introducing explanations on hard examples (Lai et al., 2023). This approach could aid in scaling our framework for everyday use, where the delicate balance between swift annotation and careful moderation is more prominent. On the other hand, our study follows a set of prescriptive moderation guidelines (Rottger et al., 2022), written based on the researchers\u2019 definitions of toxicity. While they are similar to actual platforms\u2019 terms of service and moderation rules, they may not reflect the norms of all online communities. Customized labeling might be essential to accommodate for platform needs. We are excited to see further explorations around and extensions of our framework."
        },
        {
            "heading": "6 Limitations, Ethical Considerations &",
            "text": "Broader Impact\nWhile our user study of toxic content moderation is limited to examples in English and to a UScentric perspective, hate speech is hardly a monolingual (Ross et al., 2016) or a monocultural (Maronikolakis et al., 2022) issue, and future work can investigate the extension of BIASX to languages and communities beyond English. In addition, our study uses a fixed sample of 30 curated examples. The main reason for using a small set of representative examples is that it enables us to conduct the user study with a large number of participants to demonstrate salient effects across groups of participants. Another reason for the fixed sampling is the difficulty of identifying high-quality examples and generating human explanations: toxicity labels and implication annotations in existing datasets are noisy. Additional research efforts into build-\ning higher-quality datasets in implicit hate speech could enable larger-scale explorations of modelassisted content moderation.\nPre-trained language models are rife with biases that are present in their training corpus (Gehman et al., 2020; Nadeem et al., 2020). This is in-part what motivates our work on a human-AI collaborative framework, as opposed to full delegation to a potentially biased AI model. That said, imperfect models can generate biased explanations, potentially confirming annotators\u2019 own biases. Although we show that generated explanations are empirically useful for annotators, future work should nevertheless investigate how to better debias these language models.\nJust as communities have diverging norms, annotators have diverse identities and beliefs, which can shift their individual perception of toxicity (Rottger et al., 2022). Similar to Sap et al. (2022) and Pei and Jurgens (2023), we find annotator performance varies greatly depending on their demographics, specifically political orientation. As shown in Figure 9 (Appendix), a more liberal participant achieves higher labeling accuracies on hard-toxic, hard-non-toxic and easy examples than a more conservative one. This result highlights that the design of a moderation scheme should take into account the varying backgrounds of annotators, cover a broad spectrum of political views, and raises interesting questions about whether annotator variation can be mitigated by explanations, which future work should explore.\nDue to the nature of our user study, we expose crowdworkers to toxic content that may cause harm (Roberts, 2019). To mitigate the potential risks, we display content warnings before the task, and our study was approved by the Institutional Review Board (IRB) at the researchers\u2019 institution. Finally, we ensure that study participants are paid fair wages (> $10/hr). See Appendix C for further information regarding the user study."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank workers on Amazon Mturk who participated in our online user study for making our research possible. We thank Karen Zhou, people from various paper clinics and anonymous reviewers for insightful feedback and fruitful discussions. This research was supported in part by Meta Fundamental AI Research Laboratories (FAIR) \u201cDynabench Data Collection and Benchmarking Plat-\nform\u201d award \u201cContExTox: Context-Aware and Explainable Toxicity Detection.\u201d"
        },
        {
            "heading": "A.1 Explanation Generation with LLMs",
            "text": "We use large language models (Ouyang et al., 2022) to generate free-text explanations. Given a statement s, we use a pattern F to encode offensiveness of the statement w[off], the light explanation egroup and the full explanation efull in the simple format below:\nF (s) = {s, [SEP], Offensive: w[off], [SEP],\nTargeted group: egroup, [SEP], Explanation: efull},\nwhere [SEP] is a newline character. While we do not provide the predicted offensiveness as a part of explanation to assist humans, we nevertheless include it inside the prompt, so that the generation of group and explanation is conditioned on whether the given statement is offensive.\nThe prompt consists of 6 examples (3 toxic and 3 non-toxic) from SBIC with manually written explanations. During evaluation, we feed the prompt to GPT-3.5 (Ouyang et al., 2022) and extract the targeted group and explanation from its completion. We greedily decode the offensiveness token w[off], and sample the targeted group egroup and explanation efull with a temperature of 0.3."
        },
        {
            "heading": "A.2 Hyperparameters for fine-tuning DeBERTa",
            "text": "We use HuggingFace (Wolf et al., 2020) to fine-tune a pre-trained deberta-v3-large model. The model achieves an F1 score of 87.5% on the SBIC test set. For the fine-tuning of the DeBERTa classifier, we used the hyperparameters reported in Table 2. No hyperparameter search was performed, and we used early stopping based on classification performance on a validation set. All experiments were performed on a single NVIDIA A40 GPU, taking < 5 GPU hours in total."
        },
        {
            "heading": "B Moderation Labels",
            "text": "\u2022 Allow: the post is not offensive or prejudiced and anyone should be able to view it. \u2022 Lenient: the post might be aggressive, angry, or use profanity/swear words, but is not offensive or\nprejudiced and does not use slurs in hateful ways.\n\u2022 Moderate: the post is offensive, prejudiced, stereotypical, or uses generalizations about minority demographic groups, but is not calling for violence or hate against them.\n\u2022 Block: the post qualifies as hate speech, or encourages violence towards minority individuals or groups and should be blocked, such that no users will be able to view this content at all."
        },
        {
            "heading": "C Human Evaluation",
            "text": "We obtained an Institutional Review Board (IRB) approval for our user study. Prior to the user study, we conduted a power analysis to determine the scale of the experiment. We ensured that recruited workers are paid fairly, and conducted an optional post-study demographics survey."
        },
        {
            "heading": "C.1 Power Analysis",
            "text": "We used G*Power (Faul et al., 2009) to conduct an a priori power analysis for one-way ANOVA. With the goal of having 80% power to detect a moderate effect size of 0.15 at a significance level of 0.05, we yield a target number of 492 participants."
        },
        {
            "heading": "C.2 MTurk Setup and Participant Compensation",
            "text": "Our study consists of a qualification stage and a task stage. During qualification, we deployed Human Intelligence Tasks (HITs) on Amazon Mechanical Turk (MTurk) in which workers go through 4 rounds of training to familiarize with the task and the user interface. Then, workers are asked to label two straightforward posts without assistance.\nIn both the qualification phase and the task phase, we use the following MTurk qualifications: HIT Approval Rate \u2265 98%, Number of HITs Approved \u2265 5000, and location is US. Among the 731 workers who participated in the qualification phase, 603 passed, and the workers were paid a median hourly wage of $10.23/h. Among the workers passing qualification, 490 participated in the task phase, in which they were further paid a median hourly wage of $14.4/h. After filtering out workers who failed the qualification questions during the task stage, our user study has 454 remaining participants."
        },
        {
            "heading": "C.3 Human Evaluation User Interface",
            "text": "We provide comprehensive instructions for users to complete the task, as demonstrated in Figure 6. Figure 7 shows the interface for one of 4 rounds of user training, and Figure 8 shows the labeling interface, both under the MODEL-EXPL condition."
        },
        {
            "heading": "C.4 Participant Demographics",
            "text": "In the post-study survey, we included a optional demographics survey. Among users who self-identified gender, 53.4% were male, 46.1% were female and 0.4% were non-binary. The majority of participants identified as White (79.9%), 6.5% as Black/African American, 6.0% as Asian/Asian American, 3.6% as Hispanic/Latinx, 3.1% as Mixed/Other, 0.4% as Native Hawaiian/Pacific Islander, 0.2% as Middle Eastern and 0.2% as South Asian/Indian American. Most participants were aged 25-50 (72.6%)."
        }
    ],
    "title": "BIASX: \u201cThinking Slow\u201d in Toxic Content Moderation with Explanations of Implied Social Biases",
    "year": 2023
}