{
    "abstractText": "Transformer-based models have achieved stateof-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tackling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.",
    "authors": [
        {
            "affiliations": [],
            "name": "Peng Lu"
        },
        {
            "affiliations": [],
            "name": "Suyuchen Wang"
        },
        {
            "affiliations": [],
            "name": "Mehdi Rezagholizadeh"
        },
        {
            "affiliations": [],
            "name": "Bang Liu"
        },
        {
            "affiliations": [],
            "name": "Ivan Kobyzev"
        }
    ],
    "id": "SP:2c0f6b4a00221cb228bcb71ca7bab312fa4c8514",
    "references": [
        {
            "authors": [
                "Lei Jimmy Ba",
                "Jamie Ryan Kiros",
                "Geoffrey E. Hinton."
            ],
            "title": "Layer normalization",
            "venue": "CoRR, abs/1607.06450.",
            "year": 2016
        },
        {
            "authors": [
                "David Bamman",
                "Noah A. Smith."
            ],
            "title": "New alignment methods for discriminative book summarization",
            "venue": "CoRR, abs/1305.1319.",
            "year": 2013
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "CoRR, abs/2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Ilias Chalkidis",
                "Xiang Dai",
                "Manos Fergadiotis",
                "Prodromos Malakasiotis",
                "Desmond Elliott."
            ],
            "title": "An exploration of hierarchical attention transformers for efficient long document classification",
            "venue": "CoRR, abs/2210.05529.",
            "year": 2022
        },
        {
            "authors": [
                "Ilias Chalkidis",
                "Manos Fergadiotis",
                "Prodromos Malakasiotis",
                "Ion Androutsopoulos."
            ],
            "title": "Large-scale multi-label text classification on EU legislation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6314\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Ilias Chalkidis",
                "Manos Fergadiotis",
                "Dimitrios Tsarapatsanis",
                "Nikolaos Aletras",
                "Ion Androutsopoulos",
                "Prodromos Malakasiotis."
            ],
            "title": "Paragraph-level rationale extraction through regularization: A case study on european court of human rights cases",
            "venue": "Proceed-",
            "year": 2021
        },
        {
            "authors": [
                "Thomas H. Cormen",
                "Charles E. Leiserson",
                "Ronald L. Rivest",
                "Clifford Stein."
            ],
            "title": "Introduction to Algorithms, 3rd Edition",
            "venue": "MIT Press.",
            "year": 2009
        },
        {
            "authors": [
                "Xiang Dai",
                "Ilias Chalkidis",
                "Sune Darkner",
                "Desmond Elliott."
            ],
            "title": "Revisiting transformer-based models for long document classification",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December",
            "year": 2022
        },
        {
            "authors": [
                "Tri Dao",
                "Daniel Y. Fu",
                "Khaled Kamal Saab",
                "Armin W. Thomas",
                "Atri Rudra",
                "Christopher R\u00e9."
            ],
            "title": "Hungry hungry hippos: Towards language modeling with state space models",
            "venue": "CoRR, abs/2212.14052.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Karan Goel",
                "Albert Gu",
                "Chris Donahue",
                "Christopher R\u00e9."
            ],
            "title": "It\u2019s raw! audio generation with state-space models",
            "venue": "International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Ma-",
            "year": 2022
        },
        {
            "authors": [
                "Albert Gu",
                "Karan Goel",
                "Christopher R\u00e9."
            ],
            "title": "Efficiently modeling long sequences with structured state spaces",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.",
            "year": 2022
        },
        {
            "authors": [
                "Albert Gu",
                "Isys Johnson",
                "Aman Timalsina",
                "Atri Rudra",
                "Christopher R\u00e9."
            ],
            "title": "How to train your HIPPO: state space models with generalized orthogonal basis projections",
            "venue": "The Eleventh International Conference on Learning Representations, ICLR 2023, Ki-",
            "year": 2023
        },
        {
            "authors": [
                "Ruining He",
                "Julian J. McAuley."
            ],
            "title": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
            "venue": "Proceedings of the 25th International Conference on World Wide Web, WWW 2016, Montreal, Canada,",
            "year": 2016
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy."
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015,",
            "year": 2015
        },
        {
            "authors": [
                "Johannes Kiesel",
                "Maria Mestre",
                "Rishabh Shukla",
                "Emmanuel Vincent",
                "Payam Adineh",
                "David P.A. Corney",
                "Benno Stein",
                "Martin Potthast."
            ],
            "title": "Semeval2019 task 4: Hyperpartisan news detection",
            "venue": "Proceedings of the 13th International Workshop on Se-",
            "year": 2019
        },
        {
            "authors": [
                "Ken Lang."
            ],
            "title": "Newsweeder: Learning to filter netnews",
            "venue": "Machine Learning, Proceedings of the Twelfth International Conference on Machine Learning, Tahoe City, California, USA, July 9-12, 1995, pages 331\u2013339. Morgan Kaufmann.",
            "year": 1995
        },
        {
            "authors": [
                "Raghavendra Pappagari",
                "Piotr Zelasko",
                "Jes\u00fas Villalba",
                "Yishay Carmiel",
                "Najim Dehak."
            ],
            "title": "Hierarchical transformers for long document classification",
            "venue": "IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2019, Singapore, Decem-",
            "year": 2019
        },
        {
            "authors": [
                "Hyunji Hayley Park",
                "Yogarshi Vyas",
                "Kashif Shah."
            ],
            "title": "Efficient classification of long documents using transformers",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2022, Dublin,",
            "year": 2022
        },
        {
            "authors": [
                "Ofir Press",
                "Noah A. Smith",
                "Mike Lewis."
            ],
            "title": "Train short, test long: Attention with linear biases enables input length extrapolation",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.",
            "year": 2022
        },
        {
            "authors": [
                "Anian Ruoss",
                "Gr\u00e9goire Del\u00e9tang",
                "Tim Genewein",
                "Jordi Grau-Moya",
                "R\u00f3bert Csord\u00e1s",
                "Mehdi Bennani",
                "Shane Legg",
                "Joel Veness."
            ],
            "title": "Randomized positional encodings boost length generalization of transformers",
            "venue": "CoRR, abs/2305.16843.",
            "year": 2023
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Samira Abnar",
                "Yikang Shen",
                "Dara Bahri",
                "Philip Pham",
                "Jinfeng Rao",
                "Liu Yang",
                "Sebastian Ruder",
                "Donald Metzler."
            ],
            "title": "Long range arena : A benchmark for efficient transformers",
            "venue": "9th International Conference on Learning Repre-",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Manzil Zaheer",
                "Guru Guruganesh",
                "Kumar Avinava Dubey",
                "Joshua Ainslie",
                "Chris Alberti",
                "Santiago Onta\u00f1\u00f3n",
                "Philip Pham",
                "Anirudh Ravula",
                "Qifan Wang",
                "Li Yang",
                "Amr Ahmed."
            ],
            "title": "Big bird: Transformers for longer sequences",
            "venue": "Advances in Neural",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Since the emergence of large-scale pre-trained language models such as BERT (Devlin et al., 2019) and GPT3 (Brown et al., 2020), these transformerbased models have become popular solutions for many text classification and generation tasks. However, their benefit is constrained to short-length inputs when the computation resource is limited because attention module requires quadratic computation time and space. More specifically, each token in a sequence of length N requires pairwise computation with all N tokens, which results in O(N2) complexity. Such limitation makes transformerbased models hard to process long sequential data efficiently. There are many works aiming to improve the performance on Long Document Classi-\n\u2217Research done during internship in Huawei Noah\u2019s Ark Lab (Montreal).\nfication for transformers (Dai et al., 2022). One of the common approaches is to simply truncate long texts to a pre-defined length, e.g. 512, which makes pre-trained models to be applicable for them. Some work demonstrated this technique is not sufficient for long documents due to the missing of important information (Dai et al., 2022).\nAnother sort of technique attempts to reduce the computation overhead of attention-based systems. This problem has several relevant solutions, e.g. Sparse Attention models (Beltagy et al., 2020) and Hierarchical Attention models (Chalkidis et al., 2022). One of the important sparse attention methods is Longformer, which leverages local and global attention to reduce the computational complexity of the models and increases the input length up to 4096 tokens. Another popular sparse attention method is BigBird (Zaheer et al., 2020): besides the global and local attention, it includes extra random attention modules to attend to a predefined number of random tokens. Apart from designing sparse attention mechanisms, Hierarchical Transformers (HAN) like ToBERT (Pappagari et al., 2019) propose to construct systems on top of the conventional transformer (Chalkidis et al., 2022). Basically, the long text is first split into several chunks less than a fixed number, e.g. 200. Next, every chunk is encoded by a vanilla transformer one by one to form a collection of chunk representations and then another transformer processes the sequence of chuck representations.\nApart from the computation complexity issue, some works pointed out that the length extrapolation ability of self-attention-based models is quite limited (Press et al., 2022; Ruoss et al., 2023), namely, the transformer models perform poorly during inference if the sequence length of test data is beyond the sequence length of training data. As an order-invariant encoding mechanism, the selfattention-based encoder heavily relies on the Position Embedding (PEs) to model input orders, how-\never, these works demonstrate that the failure of transformers on long sequence is due to the limited length generalization ability of these position embedding methods. This also encourages exploring alternative architectures for the challenge of long document classification problems.\nRecently, Gu et al. (2022) propose modeling sequential data with state-space models and show surprisingly good performance on a benchmark for comparing Transformers over long sequential data (Tay et al., 2021). However, this benchmark only consists of one single text classification task. It is still unclear about the efficacy and efficiency of state-space models for long document tasks compared to transformer-based models.\nIn this paper, we aim to fill this gap with a more comprehensive by conducting extensive experiments and analysis on six long document classification benchmarks. Besides, we develop an efficient long document system and show that the state-space-based models outperform self-attentionbased models (including sparse attention and HAN) and yield consistent performance across datasets with much higher efficiency and more robustness to input noise."
        },
        {
            "heading": "2 Background and Methodology",
            "text": ""
        },
        {
            "heading": "2.1 State Space Model",
            "text": "In this section, we briefly introduce a recent long dependency modeling method utilizing State-space models to encode sequential data (Gu et al., 2022). The state-space model is defined by the following equations, which map the 1-dimensional continuous input signal u(t) to an N-dimensional hidden state x(t), and this hidden state is then projected to a 1-dimensional output y(t):\nx\u2032(t) = Ax(t) +Bu(t),\ny(t) = Cx(t) +Du(t), (1)\nwhere A,B,C,D are trainable parameters. A discrete sequence, like text, can be regarded as discretized data sampled from a continuous signal with a step size \u2206, and the corresponding SSM in the recurrence manner is:\nhk = Ahk\u22121 +Bxk,\nyk = Chk +Dxk, A = (I \u2212\u2206/2 \u00b7A)\u22121(I +\u2206/2 \u00b7A), (2)\nwhere A is the discretized state matrix and B,C,D share the similar formulas. The SSM can\nalso be rewritten in a linear convolution manner facilitating the encoding speed.\nK = (CB,CAB, ...,CA L\u22121 B),\nyk = k\u2211 j=0 (CA j B) \u00b7 xk\u2212j , (3)\nwhere K \u2208 RL is defined as the SSM kernel. Given an input sequence x = {x1, . . . , xL} \u2208 RL, the corresponding outputs y = {y1, . . . , yL} \u2208 RL of the convolution y = K \u2217 x can be computed efficiently in O(L logL) time with the Fast Fourier Transform (FFT) (Cormen et al., 2009).\nThe computational complexity of the SSM algorithm is O(L logL) in convolution mode, which enables it to process significantly longer input sequences with the same hardware resources compared to the attention mechanism O(L2). This results in a substantial reduction in information loss from the input side. Next, instead of relying on a limited number of global attention mechanisms to capture long-term dependencies in sparse attention models, the SSM model is designed to model long sequences as dynamic systems. By utilizing a Hurwitz state matrix A (Goel et al., 2022; Gu et al., 2023), it preserves long-term dependency information in a high-dimensional state. Such states enable SSM models strong ability to capture long dependency information."
        },
        {
            "heading": "2.2 SSM-based Systems",
            "text": "In practice, recent SSM-based systems, e.g. S4 (Gu et al., 2022; Goel et al., 2022) are built up with a block as shown on the left of Figure 1. This block shares a similar structure as the self-attention module with pre-Layer Normalization (Ioffe and Szegedy, 2015; Ba et al., 2016) but replaces the multi-head attention with a discretized state-space model. The Structured State Space sequence model (S4) (Gu et al., 2022) utilizes this structure and leverages the Cauchy kernel method to simplify the kernel computation of SSM. Building on this idea, Hungry Hungry Hippo (H3) (Dao et al., 2022) extends the mechanism by including an additional gate and a short convolution obtained via a shift SSM to improve the language modeling ability of SSM-based systems."
        },
        {
            "heading": "2.3 State-Space-Pooler",
            "text": "To further improve the system efficiency, we propose a modification of the system architecture which imposes a progressive reduction of the input length for deeper layers. Figure 1 depicts our system architecture. The model progressively constructs the representation from the token level toward the final representation level. By inserting a max pooling layer between each SSM block, the model at each level automatically extracts the important information between nearby inputs and reduces the input length to half of the previous layer, which further accelerates the speed of training and inference. The final representation of the long se-\nquence level is computed with the average of the last layer and then is fed to a fully-connected dense layer with softmax or sigmoid function to output the prediction probability for multi-class or multilabel problems, respectively."
        },
        {
            "heading": "3 Experiments",
            "text": "In this section, we present our experimental setup and results for multiple datasets."
        },
        {
            "heading": "3.1 Long Document Understanding (LDU)",
            "text": "Dataset. We first evaluated the effectiveness and efficiency on six commonly-used long text classification datasets including Book (Bamman and Smith, 2013), ECtHR (Chalkidis et al., 2021), Hyperpartisan (Kiesel et al., 2019), 20News (Lang, 1995), EURLEX (Chalkidis et al., 2019) and Amazon product reviews (AMZ) (He and McAuley, 2016). For the AMZ dataset, we randomly sampled product reviews longer than 2048 words from the Book category. We report accuracy for binary and multi-class classification tasks (Hyperpartisan, 20News, and AMZ) and macro-F1 for the rest of the multi-label classification problems. The detailed dataset statistics are included in Appendix A.\nBaselines. We compare our methods with Transformer (Vaswani et al., 2017), Longformer and S4 with and without pre-training. More specifically, for models without pre-training, we compare 6-layer S4 and S4-pooler models (11m parameters) with 6-layer Transformer (70m parameters)\nand Longformer models (99m parameters). For models with pre-trained checkpoints1, we choose BERT-base with truncated input length and its two variants BERT-random and BERT-textrank (Park et al., 2022), we choose one sparse attention model: Longformer (Beltagy et al., 2020) and one hierarchical transformer model (HAN): ToBERT (Pappagari et al., 2019). The detailed model settings and hyperparameters are included in Appendix B.\nResults. Table 1 shows the results of different methods on six commonly used long text classification datasets. Among the models without pretraining, the Transformer models perform more poorly than all other models by a large margin (3% - 6%). S4 and S4-pooler perform significantly better than Transformer and Longformer models on all datasets except for 20news, which is probably because the average length of this dataset is relatively short. On average, S4 and S4-pooler outperform the Longformer by 3.4% and 3.1%, respectively. For models fine-tuned with pre-trained checkpoints, although the BERT models with truncated inputs improve a lot compared to transformers without pre-training, they still underperform other methods, which indicates the necessity of processing longer text for this task. Again, the H3 and H3-pooler models give better performance on average. Besides, the S4-pooler and H3-pooler models demonstrate comparable performance to the corresponding original systems with around 36% training time reduction. We discuss the training time in the analysis section."
        },
        {
            "heading": "4 Discussion",
            "text": "In this section, we investigate the training efficiency, the impact of different down-sampling structures and the robustness to input noise of SSMbased models."
        },
        {
            "heading": "4.1 Time Analysis",
            "text": "Figure 2 shows the training time of different models on a single Nvidia V100 GPU for one epoch 2. The maximum input length is set to 512 for transformer models and 4096 for Longformer, S4, and S4-pooler models. All models use the same batch size of 16 and layer number of 6. SSM-based models have a significant advantage in training effi-\n1Our experiments were conducted with Huggingface toolkit (https://github.com/huggingface/transformers).\n2The experiments of SSM models are adapted from (https://github.com/HazyResearch/safari).\nciency compared to transformer and longformer models. Even with an 8 times greater maximum input length, S4 and S4-pooler models only took 42% and 27% of the training time of transformer, respectively. They also only took around 8% and 13% of the longformer\u2019s training time. By introducing pooling layers, the training time of the S4-pooler is reduced to 64% of S4 models on average."
        },
        {
            "heading": "4.2 Ablation Analysis",
            "text": "The pooling layer down-samples the input gradually, hence reducing the computation cost. Moreover, it changes the resolution of the input sequence by choosing different window sizes on different layers. We conducted experiments to investigate the impact of the hierarchical structures of S4-pooler models. S4-pooler-x-y refers to placing the pooling layer on every x layer with window size y. The results are averaged over three random seeds. The performance of different tasks is influenced by the hierarchical structures. For example, enlarging the down-sampling ratio from 2 to 5 decreases the performance of AMZ, while having less impact on the\nECtHR task."
        },
        {
            "heading": "4.3 Robustness Analysis",
            "text": "Next, we analyze the robustness of SSMs to input noise. We conduct experiments on the ECtHR dataset under different input noise rates \u03c1. More specifically, we randomly mask out \u03c1 percent of inputs and train S4 and S4-pooler models with the noisy data. Figure 3 shows the results of two models. We can see the performance of S4 drops much faster than S4-pooler model with the increasing percentage of noise. This is reasonable because the S4-pooler system has max-pooling layers and imposes a stronger information extraction effect."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we conduct a comprehensive evaluation of the SSM-based models and show their superiority over self-attention-based models on long document classification tasks in terms of performance and training time. We further propose an efficient SSM-based system by imposing input length reduction for deeper layers and show our method performs on par with the original SSM models while greatly improving time efficiency.\nLimitations\nThe model we proposed only focuses on the classification of long documents, therefore, it may be extended to other NLP tasks, like long document Question Answering tasks. Furthermore, we concentrated on English datasets during the evaluation. In the future, we plan to extend it to more tasks and languages."
        },
        {
            "heading": "A Dataset",
            "text": "The dataset statistics is shown in Table 3. # class is the number of labels, the max, min, mean indicates the maximum, minimum and mean number of tokens with BERT tokenizer, respectively."
        },
        {
            "heading": "B Experimental setup details",
            "text": "We used Adam optimizer with default \u03b2 and we searched the best learning rate from 5e-5, 3e-5,\n1e-5 for one run of each baseline model and selected the best learning rate for the model. For the Transformer and Longformer trained from scratch, we used the BERT-base and Longformer-base and truncated them to be 6 layers without using the pretrained checkpoints. For all methods, we set the maximum epoch to 20 and selected the best model based on the performance metric on the dev set or the checkpoint of the last epoch. We reported the average results on the test set over three different seeds. For Experiments with S4, we followed the recommended setting (Gu et al., 2022). For H3 models, we used H3-125M which has a similar model scale as BERT-base and Longformer-base models."
        }
    ],
    "title": "Efficient Classification of Long Documents via State-Space Models",
    "year": 2023
}