{
    "abstractText": "Document-Level Relation Extraction aims at predicting relations between entities from multiple sentences. A common practice is to select multi-label classification thresholds to decide whether a relation exists between an entity pair. However, in the document-level task, most entity pairs do not express any relations, resulting in a highly imbalanced distribution between positive and negative classes. We argue that the imbalance problem affects threshold selection and may lead to incorrect \"no-relation\" predictions. In this paper, we propose to downweight the easy negatives by utilizing a distance between the classification threshold and the predicted score of each relation. Our novel Adaptive Hinge Balance Loss measures the difficulty of each relation class with the distance, putting more focus on hard, misclassified relations, i.e. the minority positive relations. Experiment results on Re-DocRED demonstrate the superiority of our approach over other balancing methods. Source codes are available at https://github.com/Jize-W/HingeABL.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jize Wang"
        },
        {
            "affiliations": [],
            "name": "Xinyi Le"
        },
        {
            "affiliations": [],
            "name": "Xiaodi Peng"
        },
        {
            "affiliations": [],
            "name": "Cailian Chen"
        }
    ],
    "id": "SP:c15d2e1be43da5fce67be52849afaab9cc84ed32",
    "references": [
        {
            "authors": [
                "Yoshua Bengio",
                "Aaron Courville",
                "Pascal Vincent."
            ],
            "title": "Representation learning: A review and new perspectives",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828.",
            "year": 2013
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Priya Goyal",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Pieter Noordhuis",
                "Lukasz Wesolowski",
                "Aapo Kyrola",
                "Andrew Tulloch",
                "Yangqing Jia",
                "Kaiming He."
            ],
            "title": "Accurate, large minibatch sgd: Training imagenet in 1 hour",
            "venue": "arXiv preprint arXiv:1706.02677.",
            "year": 2017
        },
        {
            "authors": [
                "Marti A. Hearst",
                "Susan T Dumais",
                "Edgar Osuna",
                "John Platt",
                "Bernhard Scholkopf."
            ],
            "title": "Support vector machines",
            "venue": "IEEE Intelligent Systems and their applications, 13(4):18\u201328.",
            "year": 1998
        },
        {
            "authors": [
                "Quzhe Huang",
                "Shibo Hao",
                "Yuan Ye",
                "Shengqi Zhu",
                "Yansong Feng",
                "Dongyan Zhao."
            ],
            "title": "Does recommend-revise produce reliable annotations? an analysis on missing instances in DocRED",
            "venue": "Proceedings of the 60th Annual Meeting of the Associa-",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Paulius Micikevicius",
                "Sharan Narang",
                "Jonah Alben",
                "Gregory Diamos",
                "Erich Elsen",
                "David Garcia",
                "Boris Ginsburg",
                "Michael Houston",
                "Oleksii Kuchaiev",
                "Ganesh Venkatesh"
            ],
            "title": "Mixed precision training. ICLR",
            "year": 2018
        },
        {
            "authors": [
                "Qingyu Tan",
                "Ruidan He",
                "Lidong Bing",
                "Hwee Tou Ng."
            ],
            "title": "Document-level relation extraction with adaptive focal loss and knowledge distillation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1672\u20131681, Dublin, Ire-",
            "year": 2022
        },
        {
            "authors": [
                "Qingyu Tan",
                "Lu Xu",
                "Lidong Bing",
                "Hwee Tou Ng",
                "Sharifah Mahani Aljunied."
            ],
            "title": "Revisiting docredaddressing the false negative problem in relation extraction",
            "venue": "Proceedings of the 2022 Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Ying Wei",
                "Qi Li."
            ],
            "title": "Sagdre: Sequence-aware graph-based document-level relation extraction with adaptive margin loss",
            "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD \u201922, page 2000\u20132008,",
            "year": 2022
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Yuan Yao",
                "Deming Ye",
                "Peng Li",
                "Xu Han",
                "Yankai Lin",
                "Zhenghao Liu",
                "Zhiyuan Liu",
                "Lixin Huang",
                "Jie Zhou",
                "Maosong Sun."
            ],
            "title": "DocRED: A large-scale document-level relation extraction dataset",
            "venue": "Proceedings of the 57th Annual Meeting of the Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Wenxuan Zhou",
                "Kevin Huang",
                "Tengyu Ma",
                "Jing Huang."
            ],
            "title": "Document-level relation extraction with adaptive thresholding and localized context pooling",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 14612\u201314620.",
            "year": 2021
        },
        {
            "authors": [
                "Liu Zhuang",
                "Lin Wayne",
                "Shi Ya",
                "Zhao Jun."
            ],
            "title": "A robustly optimized BERT pre-training approach with post-training",
            "venue": "Proceedings of the 20th Chinese National Conference on Computational Linguistics, pages 1218\u20131227, Huhhot, China. Chinese Informa-",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Document-Level Relation Extraction (RE) plays an important role in NLP applications such as knowledge graph construction. It aims at predicting relations between entities from multiple sentences. As illustrated in Figure 1a, an entity pair may have zero, one, or multiple relations, so document-level RE is a multi-label classification task. To solve this, a common practice is to adaptively select thresholds for multi-label classification (Zhou et al., 2021). For a correct prediction, the confidence scores of existent relations should be higher than the threshold, and conversely, those of non-existent relations should be lower.\nHowever, there is a significant imbalance problem between positive and negative classes in document-level RE. The number of entity pairs\n\u2217Corresponding author.\nText: Ross Patterson Alger (August 20, 1920 - January 16, 1992) was a politician in the Canadian province of Alberta, ... After the war, he received an MBA from the University of Toronto. He settled in Calgary and started a career in accounting ...\nSubject: University of Toronto Object: Canadian Relation: country, located in\n(a) A sample document in Re-DocRED dataset.\n11.16 8.59 9.39\n-5.38 -4.89 -4.34 -3.64\nthreshold existent relation non-existent relation\n(b) False negative prediction with correct label ranking.\nTH RE\nSH OL\nD\nco un\ntr y\nlo ca\nte d\nin\npe rfo\nrm er\nda te\nof bi\nrth\nda te\nof de\nat h\nca st\nme mb\ner\n3.37 5.13 7.72\n-1.3 -1.0 -1.22 -1.06\n(c) Correct prediction after utilizing adaptive hinge balance loss.\nFigure 1: Illustration on multi-label classification in document-level relation extraction. (a) There are two relations existing between University of Toronto and Canadian. (b) The entity pair in (a) is incorrectly predicted as \"no-relation\". Scores of existent relations (country, located in) are lower than the threshold (11.16), but significantly higher than all non-existent relations. (c) After utilizing adaptive hinge balance loss, the threshold is reduced to an appropriate value.\nincreases quadratically with the number of entities. Thus, compared with the sentence-level counterpart, there are far more entity pairs to be classified in document-level RE, and most entity pairs express no relation. For example, in the documentlevel RE dataset, Re-DocRED (Tan et al., 2022b), 94% of the entity pairs express no relation.\nThe issue of class imbalance may lead to more incorrect \"no-relation\" predictions. In our paper, we mainly consider the \"positive/negative imbalance\", rather than the imbalance between different\ntypes of \"positive relations\". The positive/negative imbalance tends to drag the threshold towards the large class, that is, the \u201cno-relation\u201d class. We discover that 78.2% of incorrect \"no-relation\" predictions have correct label ranking, but the confidence score is lower than the threshold, which is shown in Figure 1b . In other words, the model has enough confidence in the existent relations, but it makes a \"no-relation\" prediction due to the unnecessarily high threshold.\nBased on this intuitive finding, we aim to address this \u201cincorrect predictions with correct label ranking\u201d phenomenon to improve the accuracy. We believe that overtraining on well-classified nonexistent relations may lead to unnecessarily high thresholds. Therefore, we propose to adaptively select thresholds, and then down-weight the relations that are far from the decision boundary using Hinge Weighting. Our contributions are three-fold:\n\u2022 We design a general pipeline termed Separate Adaptive Thresholding, to adaptively select thresholds for multi-label classification.\n\u2022 We propose a novel Adaptive Hinge Balance Loss, tackling the imbalance problem of positive and negative classes in document-level RE.\n\u2022 Among all the existing balancing methods, our method achieves the highest F1 score on the common dataset Re-DocRED."
        },
        {
            "heading": "2 Preliminary",
            "text": "The task of document-level relation extraction is concerned with the prediction of relation types between subject and object entities in a given document. We will first introduce the formulation of this task, and then discuss the commonly used ATL method."
        },
        {
            "heading": "2.1 Problem Formulation",
            "text": "Given a document D that contains a set of entities {ei}ni=1, the task of document-level relation extraction is to predict the relation types between the entity pairs (es, eo)s,o\u2208{1,...,n},s \u0338=o, where es and eo represent the subject entity and the object entity, respectively. The set of relations is defined as R \u222a {NA}, where R is a set of pre-defined relations and NA stands for no relation between a pair of entities.\nWith the document D and an entity pair (es, eo) contained in it, we can get the representation of the\nsubject and object entity through:\n[zs, zo] = Rep(D, es, eo), (1)\nwhere zs and zo are the representation of the subject and object entity. Rep is a representation module.\nThe score of relation r is defined as sr, which can be computed via the subject and object entity representation using a bilinear classifier:\nsr = zTs Wrzo + br, (2)\nwhere Wr \u2208 Rd\u00d7d, br \u2208 R are model parameters."
        },
        {
            "heading": "2.2 Adaptive Thresholding Loss",
            "text": "Adaptive Thresholding Loss (ATL) (Zhou et al., 2021) is the most widely used loss function in transformer-based document-level relation extraction methods. It enables the model to choose multilabel classification thresholds, thereby achieving superior results when compared to the global threshold of BCE loss (Bengio et al., 2013).\nIn ATL, the labels of entity pair T = (es, eo) are divided into two subsets: positive classes PT and negative classes NT , where PT \u2286 R denotes the relations that exist between T , and NT \u2286 R denotes the relations that do not exist between the entities. ATL introduces an additional threshold class TH. If an entity pair is correctly classified, the scores of PT should be higher than TH while those of NT should be lower. ATL comprises of two parts:\nL1 = \u2212 \u2211 r\u2208PT log\n( exp(sr)\u2211\nr\u2032\u2208PT\u222a{TH} exp(sr\u2032)\n) ,\n(3)\nL2 = \u2212log\n( exp(sTH)\u2211\nr\u2032\u2208NT\u222a{TH} exp(sr\u2032)\n) , (4)\nLATL = L1 + L2. (5)"
        },
        {
            "heading": "2.3 An Empirical Analysis of ATL",
            "text": "A preliminary analysis is conducted to investigate the cause of classification error in ATL, as shown in Table 1. All false predictions can be categorized into three patterns: FP, FN_CRK, and FN_IRK, which are illustrated in Figure 2. In particular, FN_CRK is the most dominant source of errors, which accounts for 78.2% of all false negative predictions.\nWe notice that the number of relations in NT is significantly larger than that in PT , and therefore\nL2 has a much greater impact on the overall loss than L1(see equations (3) and (4)). Due to the dominance of L2, it can be rewritten as the following form:\nL2 = \u2212log\n( 1\n1 + \u2211\nr\u2032\u2208NT exp(sr\u2032 \u2212 sTH)\n) .\n(6)\nL2 \u2192 0 when sr\u2032 \u2212 sTH \u2192 \u2212\u221e, which means sTH \u226b sr\u2032 . This suggests ATL learns a threshold sTH well above the candidate score, which leads to an increase in the number of FN_CRK predictions."
        },
        {
            "heading": "3 Adaptive Hinge Balance Loss",
            "text": "Based on the analysis above, we aim to maximize the distance between the decision boundary sTH and the sample point sr, r \u2208 R while simultaneously down-weighting the classes distant from the boundary. To this end, we propose our Adaptive Hinge Balance Loss."
        },
        {
            "heading": "3.1 Separate Adaptive Thresholding",
            "text": "An ideal loss should maximize the distance from the decision boundary to the sample point. Moreover, in the loss formulation, each relation class should be independent of the others to enable individual weighing of each class. Therefore, we propose the Separate Adaptive Thresholding (SAT), which is formulated as:\nL = \u2212 \u2211 r\u2208R log(\u03c3(\u2212dr)), (7)\ndr = { sr \u2212 sTH r \u2208 PT sTH \u2212 sr r \u2208 NT\n(8)\nwhere \u03c3 is the sigmoid function, i.e. \u03c3(x) = 11+ex . We define sTH as the decision boundary. Then dr is the distance from the decision boundary to the score of relation r \u2208 R. dr > 0 if a relation is correctly classified. L \u2192 0 when dr \u2192 \u221e.\nThe loss pushes dr to be as large as possible. The score of each relation is compared with the threshold separately. Thus we can assign different weights to different relations."
        },
        {
            "heading": "3.2 Hinge Weighting",
            "text": "To down-weight the easy and well-classified relations, i.e. the majority negative relations, we propose Hinge Weighting inspired by hinge loss. (Hearst et al., 1998)\nOur Hinge Weighting is shown in Figure 3. It is formulated as:\nwr = max(0,m\u2212 dr), r \u2208 R, (9)\nwhere m is a constant. When the distance dr is larger than m, the relation is not penalized. Otherwise, it is penalized linearly with dr. Essentially, Hinge Weighting implies that we should avoid focusing on the relationship with large dr. 2m is the maximum margin between positive and negative classes, which is illustrated in Figure 4.\nNote that our weighting mechanism downweights the well classified samples to zero. Since\nthe majority of well classified classes are negative, HingeABL achieves the effect of down-weighting the majority negative relations."
        },
        {
            "heading": "3.3 Loss Definition",
            "text": "Combining separate adaptive thresholding and hinge weighting, we obtain the adaptive hinge balance loss (HingeABL):\nL = \u2212 \u2211 r\u2208R wr\u2211 r\u2032\u2208Rwr\u2032 log(\u03c3(\u2212dr)), (10)\nwhere \u03c3 is the sigmoid function and wr is formulated as Equation (9). The hinge weights are normalized among all relations. Our adaptive hinge balance loss simultaneously maximizes the distance between the decision boundary and the sample point and down-weights easy classes that are far from it. This helps prevent over-fitting on wellclassified relations."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Setup",
            "text": "We conduct experiments on Re-DocRED (Tan et al., 2022b), the largest and well-labeled dataset for document-level RE. We use F1 and Ign_F1 as the metrics. Ign_F1 is measured by removing the relations existing in the training set from the dev/test sets. More details about statistics and implementation are provided in Appendix A and B. Note that we use micro F1 here in order to maintain consistency with previous methods. However, macro F1 is more suitable to illustrate whether the proposed method can perform better on minority classes. Results evaluated under macro F1 are provided in Appendix C."
        },
        {
            "heading": "4.2 Results",
            "text": "Different balancing methods. To compare different balancing methods, we use ATLOP (Zhou et al., 2021) as the representation module and BERTbase (Devlin et al., 2019) as the encoder of it. We also compare our method with three other approaches: Balanced Softmax (Zhang et al., 2021), AML (Adaptive Margin Loss) (Wei and Li, 2022), and AFL (Adaptive Focal Loss) (Tan et al., 2022a).\nBoth AML and HingeABL are margin-based loss functions.\nTo illustrate the effectiveness of hinge weighting, we implement an alternative weighted loss called MeanSAT by weighting positive and negative classes of SAT by the inverse of their number. Its formulation is in Appendix D.\nThe results are shown in Table 3. HingeABL achieves the highest F1 and Ign_F1 of 75.15 and 73.84 among all balancing methods. We observe a substantial increase in performance by implementing two weighting methods on the SAT. Our experiments indicate that hinge weighting surpasses constant weighting with MeanSAT, which demonstrates the superiority of HingeABL.\nBesides, we compare the two margin-based loss functions, AML and HingeABL, through mathematical analysis, provided in Appendix E. We find that AML penalizes the misclassified samples linearly with the distance, while HingeABL penalizes the misclassified samples nonlinearly with the distance. The nonlinear function is strictly convex, which benefits optimization. Different document-level RE models. To test the generality of our approach, we select three commonly used transformer-based methods for document-level relation extraction and replace their loss functions with our adaptive hinge balance loss. Among the three original base methods, ATLOP employs ATL loss, DocuNet employs Balanced Softmax loss, and KD-DocRE employs AFL loss. Both Balanced Softmax and AFL are the improvements of ATL. All methods use RoBERTalarge (Zhuang et al., 2021) as their encoder. Table 2 shows the results, all of which demonstrate consis-\ntent performance gains with the use of HingeABL. This affirms the generalizability of our approach. Note that HingeABL\u2019s improvement seems to be less significant when the base method is more powerful. This is a natural result because better base methods employ better loss functions. Replacing a better loss function with HingeABL results in a smaller improvement. Prediction statistics. To verify whether our model solves the problem of high thresholds, we count the number of prediction patterns from Figure 2 and present the results in Table 4. Our analysis reveals that the proportion of FN and FN_CRK has decreased, indicating that the issue has been resolved. An example of prediction results before and after applying HingeABL is shown in Figure 1b and 1c. While one might assume that lowering the threshold would lead to more false positive predictions, we observe that the total proportion of false predictions actually decreases. This suggests that HingeABL achieves a good balance in its threshold selection."
        },
        {
            "heading": "5 Conclusion",
            "text": "We propose a novel Adaptive Hinge Balance Loss for document-level relation extraction to tackle the imbalance problem of positive and negative classes. Experimental results show our approach outperforms existing methods. Since our loss is modelindependent, it has potential applicability to other multi-label classification scenarios.\nLimitations\nCompared with classifying an entity pair known with relation, accurately determining whether a relation exists between an entity pair is a more challenging task. Despite attempts to improve accuracy through better thresholding methods, the results are still far from ideal."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the National Key Research and Development Program of China (2021YFB1716000) and the National Natural Science Foundation of China (No.62176152)."
        },
        {
            "heading": "A Re-DocRED Statistics",
            "text": "Re-DocRED is a more reliable benchmark in document-level relation extraction. It is a revised version of DocRED (Yao et al., 2019), whose annotations are pointed out to be incomplete by recent works ((Huang et al., 2022; Tan et al., 2022a)). ReDocRED contains 96 relations. Each document has an average of 391 entity pairs, among which 94% contains no relation. The detailed statistics are shown in Table 5 and Table 6.\nB Implementation Details\nAll experiments are implemented based on Hugging Face\u2019s Transformers (Wolf et al., 2020). In the experiment of comparing different balancing methods, we use BERTbase (Devlin et al., 2019) as the encoder of ATLOP. In the experiment of comparing different RE models, we use RoBERTalarge (Devlin et al., 2019) as the encoder of these RE models, for the sake of comparison on the benchmark.\nWe select the margin of HingeABL as m = 5 when conducting experiments. We use mixedprecision training (Micikevicius et al., 2018) based on the PyTorch amp library1. The models are optimized with AdamW (Loshchilov and Hutter, 2019) with a linear warmup (Goyal et al., 2017) for the first 6% steps followed by a linear decay to 0. The learning rate is 5e-5 for models with BERT as the encoder and 3e-5 for models with RoBERTa as the encoder. The train batch size is 4 and the test batch size is 8. We train 30 epochs for each model. For each experiment, we run 5 different seeds (1, 5, 42, 66, 233) and report the average score. All models are trained with 1 Tesla A800 GPU."
        },
        {
            "heading": "C Comparison results under macro F1",
            "text": "The comparison results among different balancing methods under macro F1 and macro Ign_F1 are shown in Table 7. Our proposed HingeABL still achieves the highest score under macro F1."
        },
        {
            "heading": "D Formulation of MeanSAT",
            "text": "MeanSAT weights positive and negative classes of SAT by the inverse of their number. It is formulated\n1https://pytorch.org/docs/stable/amp.html\nas:\nL = \u2212 1 Np \u2211 r\u2208P log(\u03c3(\u2212dr))\u2212 1 Nn \u2211 r\u2208N log(\u03c3(\u2212dr)),\n(11)\nwhere Np and Nn are the number of positive and negative classes for the entity pair."
        },
        {
            "heading": "E Mathematical analysis of Adaptive Margin Loss and HingeABL",
            "text": "In addition to experiments, we also compare the two margin-based losses, Adaptive Margin Loss (AML) and HingeABL, from a mathematical analysis perspective.\nAnalysis 1: For a sample that is not well classified, the Adaptive Margin Loss is a linear function with respect to the distance.\nThe Adaptive Margin Loss is defined as:\nL = \u2211 r\u2208R max(0,m\u2212 dr). (12)\nFor class r, Lr = max(0,m\u2212 dr). For a well-classified sample, dr \u2265 m, Lr = 0. For a sample that is not well classified, dr < m, Lr = m\u2212 dr. In the second condition, we denote cr = \u2212dr > \u2212m. It measures the distance between a sample that is not well classified to the decision boundary. Note that we call cr \"distance\" here, but it is not necessarily greater than zero. The smaller cr is, the better the sample is classified. This means we should give a larger punishment to a larger cr. Then we have:\nLr = m+ cr, (13) \u2202Lr \u2202cr = 1. (14)\nThis means Adaptive Margin Loss penalizes the samples that are not well classified linearly with\nthe distance. (Note: A sample that is not well classified means cr = \u2212dr > \u2212m. A sample that is misclassified means cr = \u2212dr > 0.)\nAnalysis 2: For a sample that is not well classified, HingeABL is a strictly convex function with respect to the distance.\nHingeABL is defined as: L = \u2212 \u2211 r\u2208R wr\u2211 r\u2032\u2208R wr\u2032 log(\u03c3(\u2212dr)) (15)\n= \u2212 \u2211 r\u2208R max(0,m\u2212 dr)\u2211 r\u2032\u2208R max(0,m\u2212 dr\u2032) log (\n1\n1 + e\u2212dr\n) .\n(16) The denominator \u2211\nr\u2032\u2208R max(0,m\u2212dr\u2032) is a normalization factor, which we discard for ease of analysis.\nFor class r, if a sample is well classified, dr \u2265 m, Lr = 0.\nIf a sample is not well classified, dr < m, Lr = \u2212(m\u2212 dr)log ( 1\n1 + e\u2212dr\n) (17)\n= \u2212(m+ cr)log ( 1\n1 + ecr\n) , (18)\n\u2202Lr \u2202cr\n= \u2212log ( 1\n1 + ecr\n) +\nm+ cr e\u2212cr + 1 , (19)\n\u22022Lr \u2202c2r = ecr 1 + ecr + e\u2212cr + 1 + e\u2212cr (m+ cr) (e\u2212cr + 1)2 > 0.\n(20)\nThis means HingeABL penalizes the samples that are not well classified nonlinearly with the distance. The nonlinear function is strictly convex.\nAnalysis 3: Comparision between the Adaptive Margin Loss and HingeABL.\n1. Similarities. Both the Adaptive Margin Loss and HingeABL are margin-based loss functions. They do not punish a prediction if it is correct and \"good enough\" (rather than \"perfect\"), which is a form of regularization to prevent overfitting.\n2. Differences. For the wrong prediction part, they both give a penalty according to the distance cr. The Adaptive Margin Loss gives a linear penalty, while HingeABL gives a strictly convex penalty. Compared to a linear penalty, a strictly convex penalty has mainly two advantages: 1. When cr is larger, HingeABL gives a larger penalty than the Adaptive Margin Loss. 2. Compared to linear functions, the nature of strictly convex functions makes the optimization more stable and more likely to converge to a globally optimal solution."
        }
    ],
    "title": "Adaptive Hinge Balance Loss for Document-Level Relation Extraction",
    "year": 2023
}