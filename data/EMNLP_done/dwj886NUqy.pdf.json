{
    "abstractText": "The abstracts of scientific papers typically contain both premises (e.g., background and observations) and conclusions. Although conclusion sentences are highlighted in structured abstracts, in non-structured abstracts the concluding information is not explicitly marked, which makes the automatic segmentation of conclusions from scientific abstracts a challenging task. In this work, we explore Normalized Mutual Information (NMI) as a means for abstract segmentation. We consider each abstract as a recurrent cycle of sentences and place two segmentation boundaries by greedily optimizing the NMI score between the two segments, assuming that conclusions are strongly semantically linked with preceding premises. On nonstructured abstracts, our proposed unsupervised approach GreedyCAS achieves the best performance across all evaluation metrics; on structured abstracts, GreedyCAS outperforms all baseline methods measured by Pk. The strong correlation of NMI to our evaluation metrics reveals the effectiveness of NMI for abstract segmentation.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Yingqiang Gao"
        },
        {
            "affiliations": [],
            "name": "Jessica Lam"
        },
        {
            "affiliations": [],
            "name": "Nianlong Gu"
        },
        {
            "affiliations": [],
            "name": "Richard H.R. Hahnloser"
        }
    ],
    "id": "SP:99a939e8919b9f8765405900a37ba627cc39d08e",
    "references": [
        {
            "authors": [
                "Titipat Achakulvisut",
                "Chandra Bhagavatula",
                "Daniel Acuna",
                "Konrad Kording."
            ],
            "title": "Claim extraction in biomedical publications using deep discourse model and transfer learning",
            "venue": "arXiv preprint arXiv:1907.00962.",
            "year": 2019
        },
        {
            "authors": [
                "Alexander A Alemi",
                "Paul Ginsparg."
            ],
            "title": "Text segmentation based on semantic word embeddings",
            "venue": "arXiv preprint arXiv:1503.05543.",
            "year": 2015
        },
        {
            "authors": [
                "Dennis Aumiller",
                "Satya Almasian",
                "Sebastian Lackner",
                "Michael Gertz."
            ],
            "title": "Structural text segmentation of legal documents",
            "venue": "Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law, pages 2\u201311.",
            "year": 2021
        },
        {
            "authors": [
                "Pinkesh Badjatiya",
                "Litton J Kurisinkel",
                "Manish Gupta",
                "Vasudeva Varma."
            ],
            "title": "Attention-based neural text segmentation",
            "venue": "European Conference on Information Retrieval, pages 180\u2013193. Springer.",
            "year": 2018
        },
        {
            "authors": [
                "Zahra Bahadoran",
                "Parvin Mirmiran",
                "Khosrow Kashfi",
                "Asghar Ghasemi."
            ],
            "title": "The principles of biomedical scientific writing: abstract and keywords",
            "venue": "International Journal of Endocrinology and Metabolism, 18(1).",
            "year": 2020
        },
        {
            "authors": [
                "Soumya Banerjee",
                "Debarshi Kumar Sanyal",
                "Samiran Chattopadhyay",
                "Plaban Kumar Bhowmick",
                "Partha Pratim Das."
            ],
            "title": "Segmenting scientific abstracts into discourse categories: A deep learningbased approach for sparse labeled data",
            "venue": "Proceed-",
            "year": 2020
        },
        {
            "authors": [
                "Joe Barrow",
                "Rajiv Jain",
                "Vlad Morariu",
                "Varun Manjunatha",
                "Douglas W Oard",
                "Philip Resnik."
            ],
            "title": "A joint model for document segmentation and segment labeling",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Mostafa Bayomi",
                "S\u00e9amus Lawless."
            ],
            "title": "C-hts: A concept-based hierarchical text segmentation approach",
            "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).",
            "year": 2018
        },
        {
            "authors": [
                "Doug Beeferman",
                "Adam Berger",
                "John Lafferty."
            ],
            "title": "Statistical models for text segmentation",
            "venue": "Machine learning, 34(1):177\u2013210.",
            "year": 1999
        },
        {
            "authors": [
                "Sangwoo Cho",
                "Kaiqiang Song",
                "Xiaoyang Wang",
                "Fei Liu",
                "Dong Yu."
            ],
            "title": "Toward unifying text segmentation and long document summarization",
            "venue": "arXiv preprint arXiv:2210.16422.",
            "year": 2022
        },
        {
            "authors": [
                "Freddy YY Choi."
            ],
            "title": "Advances in domain independent linear text segmentation",
            "venue": "1st Meeting of the North American Chapter of the Association for Computational Linguistics.",
            "year": 2000
        },
        {
            "authors": [
                "Franck Dernoncourt",
                "Ji Young Lee."
            ],
            "title": "PubMed 200k RCT: a dataset for sequential sentence classification in medical abstracts",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Aris Fergadis",
                "Dimitris Pappas",
                "Antonia Karamolegkou",
                "Harris Papageorgiou."
            ],
            "title": "Argumentation mining in scientific literature for sustainable development",
            "venue": "Proceedings of the 8th Workshop on Argument Mining, pages 100\u2013111.",
            "year": 2021
        },
        {
            "authors": [
                "Pavlina Fragkou",
                "Vassilios Petridis",
                "Ath Kehagias."
            ],
            "title": "A dynamic programming algorithm for linear text segmentation",
            "venue": "Journal of Intelligent Information Systems, 23(2):179\u2013197.",
            "year": 2004
        },
        {
            "authors": [
                "Goran Glava\u0161",
                "Federico Nanni",
                "Simone Paolo Ponzetto."
            ],
            "title": "Unsupervised text segmentation using semantic relatedness graphs",
            "venue": "Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics, pages 125\u2013130.",
            "year": 2016
        },
        {
            "authors": [
                "Amir Hazem",
                "B\u00e9atrice Daille",
                "Dominique Stutzmann",
                "Christopher Kermorvant",
                "Louis Chevalier."
            ],
            "title": "Hierarchical text segmentation for medieval manuscripts",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Marti A Hearst."
            ],
            "title": "Multi-paragraph segmentation expository text",
            "venue": "32nd Annual Meeting of the Association for Computational Linguistics, pages 9\u201316.",
            "year": 1994
        },
        {
            "authors": [
                "Marti A Hearst."
            ],
            "title": "Text tiling: Segmenting text into multi-paragraph subtopic passages",
            "venue": "Computational linguistics, 23(1):33\u201364.",
            "year": 1997
        },
        {
            "authors": [
                "Xinyu Hua",
                "Zhe Hu",
                "Lu Wang."
            ],
            "title": "Argument generation with retrieval, planning, and realization",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2661\u2013 2672.",
            "year": 2019
        },
        {
            "authors": [
                "AP Huovila",
                "Astrid M Eder",
                "Stephen D Fuller."
            ],
            "title": "Hepatitis b surface antigen assembles in a post-er, pre-golgi compartment",
            "venue": "The Journal of cell biology, 118(6):1305\u20131320.",
            "year": 1992
        },
        {
            "authors": [
                "Omri Koshorek",
                "Adir Cohen",
                "Noam Mor",
                "Michael Rotman",
                "Jonathan Berant."
            ],
            "title": "Text segmentation as a supervised learning task",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2018
        },
        {
            "authors": [
                "Tarald O Kv\u00e5lseth."
            ],
            "title": "On normalized mutual information: measure derivations and properties",
            "venue": "Entropy, 19(11):631.",
            "year": 2017
        },
        {
            "authors": [
                "Bla\u017eenka D Letini\u0107",
                "Marinela Contreras",
                "Yael DahanMoss",
                "Ingrid Linnekugel",
                "Jos\u00e9 de la Fuente",
                "Lizette L Koekemoer."
            ],
            "title": "Additional evidence on the efficacy of different akirin vaccines assessed on anopheles arabiensis (diptera: Culicidae)",
            "venue": "Parasites",
            "year": 2021
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Kelvin Lo",
                "Yuan Jin",
                "Weicong Tan",
                "Ming Liu",
                "Lan Du",
                "Wray Buntine."
            ],
            "title": "Transformer over pretrained transformer for neural text segmentation with enhanced topic coherence",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Michal Lukasik",
                "Boris Dadachev",
                "Kishore Papineni",
                "Gon\u00e7alo Sim\u00f5es."
            ],
            "title": "Text segmentation by cross segment attention",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4707\u20134716.",
            "year": 2020
        },
        {
            "authors": [
                "PK Nair",
                "Vimala D Nair."
            ],
            "title": "Organization of a research paper: The imrad format",
            "venue": "Scientific writing and communication in agriculture and natural resources, pages 13\u201325. Springer.",
            "year": 2014
        },
        {
            "authors": [
                "Hiroki Nakayama",
                "Takahiro Kubo",
                "Junya Kamura",
                "Yasufumi Taniguchi",
                "Xu Liang."
            ],
            "title": "doccano: Text annotation tool for human",
            "venue": "Software available from https://github.com/doccano/doccano.",
            "year": 2018
        },
        {
            "authors": [
                "Vishakh Padmakumar",
                "He He."
            ],
            "title": "Unsupervised extractive summarization using pointwise mutual information",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2505\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Lev Pevzner",
                "Marti A Hearst."
            ],
            "title": "A critique and improvement of an evaluation metric for text segmentation",
            "venue": "Computational Linguistics, 28(1):19\u2013",
            "year": 2002
        },
        {
            "authors": [
                "Ben Poole",
                "Sherjil Ozair",
                "Aaron Van Den Oord",
                "Alex Alemi",
                "George Tucker"
            ],
            "title": "On variational bounds of mutual information",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Anna M Ripple",
                "James G Mork",
                "John M Rozier",
                "Lou S Knecht."
            ],
            "title": "Structured abstracts in medline: Twenty-five years later",
            "venue": "National Library of Medicine.",
            "year": 2012
        },
        {
            "authors": [
                "Tim Sainburg",
                "Brad Theilman",
                "Marvin Thielk",
                "Timothy Q Gentner."
            ],
            "title": "Parallels in the sequential organization of birdsong and human speech",
            "venue": "Nature communications, 10(1):1\u201311.",
            "year": 2019
        },
        {
            "authors": [
                "Benjamin Schiller",
                "Johannes Daxenberger",
                "Iryna Gurevych."
            ],
            "title": "Aspect-controlled neural argument generation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Te-Wei Shieh",
                "Yung-Sung Chuang",
                "ShangYu Su",
                "Yun-Nung Chen."
            ],
            "title": "Towards understanding of medical randomized controlled trials by conclusion generation",
            "venue": "Proceedings of the Tenth International Workshop on Health Text Mining and",
            "year": 2019
        },
        {
            "authors": [
                "Alessandro Solbiati",
                "Kevin Heffernan",
                "Georgios Damaskinos",
                "Shivani Poddar",
                "Shubham Modi",
                "Jacques Cali."
            ],
            "title": "Unsupervised topic segmentation of meetings with bert embeddings",
            "venue": "arXiv preprint arXiv:2106.12978.",
            "year": 2021
        },
        {
            "authors": [
                "Swapna Somasundaran"
            ],
            "title": "Two-level transformer and auxiliary coherence modeling for improved text segmentation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7797\u20137804.",
            "year": 2020
        },
        {
            "authors": [
                "Shufang Sun",
                "Danhua Lin",
                "Don Operario."
            ],
            "title": "Interest in covid-19 vaccine trials participation among young adults in china: Willingness, reasons for hesitancy, and demographic and psychosocial determinants",
            "venue": "Preventive Medicine Reports, 22:101350.",
            "year": 2021
        },
        {
            "authors": [
                "Jerry Sheehan",
                "Zhihong Shen",
                "Brandon Stilson",
                "Alex D. Wade",
                "Kuansan Wang",
                "Nancy Xin Ru Wang",
                "Christopher Wilhelm",
                "Boya Xie",
                "Douglas M. Raymond",
                "Daniel S. Weld",
                "Oren Etzioni",
                "Sebastian Kohlmeier"
            ],
            "title": "CORD-19: The COVID-19 open",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "CAj = {si \u2208 A | \u03b1Aj \u2264 i \u2264 \u03beAj }, PAj = {si \u2208 A | si /\u2208 CAj }.\nAssociated with a corpus A = {Ai}i\u2208N1:k of k abstracts, there is a set of G = {GAi}i\u2208N1:k of \u220fk i=1mAi possible segmentations. Searching for the best ensemble of segmentations of A\nwithin G may involve exhaustive enumeration over\u220fk i=1mAi segmentations, which is impossible under limited computational costs. Therefore, in this work, we concentrate on greedily approaching a reasonably good segmentation that is a tight lower bound of the actual global optimum."
        },
        {
            "heading": "3.1 Cyclic Abstract Segmentation",
            "text": "To reduce the search space, we make two assumptions: firstly, that conclusion sentences are located at the end of abstracts; and secondly, that each abstract contains at most three conclusion sentences. This results in m = 6 possible segmentations per abstract. The segmentations of an example abstract with n = 7 sentences are depicted in Table 1.\nBecause scientific abstracts typically end with conclusion sentences, we expect the stitching point of our cyclic abstracts to form a boundary. In other words, we read out the segmentation of interest from the first segment boundary \u03b1Aj ; the second segment boundary \u03beAj we expect to coincide with the abstract end. Thus, by optimizing the second segment boundary \u03beAj , we perform a sanity check that the unsupervised segmentation method is capable of detecting the abstract end, which is a natural boundary of the abstract."
        },
        {
            "heading": "3.2 Normalized Mutual Information",
            "text": "Our next step is to choose an optimization objective for the greedy search. Inspired by work on text summarization (Padmakumar and He, 2021) and birdsong analysis (Sainburg et al., 2019), we explore mutual information as the optimization objective.\nMutual information I(X;Y ) is a measure of the absolute reduction in information uncertainty (in bits) for a random variable X after observing another correlated random variable Y . Our proposed greedy approach is based on the assumption that the uncertainty of the conclusion is minimized after the premise is observed, i.e. that the segmentation maximizes mutual information.\nWe denote C = {CAiji }i\u2208N1:k as a possible conclusion ensemble spanned by all conclusion segments from the k abstracts, and P = {PAiji }i\u2208N1:k as one possible premise ensemble obtained in the same way. Note that the segmentation ji can be different for each abstract Ai.\nThe task can now be formulated as follows: given a corpus of abstracts A, determine the premise P and conclusion C ensembles that maximize the mutual information I(P;C).\nMore formally, we compute I(P;C) as follows:\nI(P;C) =\u2211 Ai\u2208A \u2211 wp\u2208P\nAi ji\n\u2211 wc\u2208C\nAi ji\np(wp;wc) log p(wp;wc)\np(wp)p(wc) ,\nwhere wp and wc are unigram tokens in the i-th premise segment PAiji and the i-th conclusion segment CAiji , respectively. p(wp;wc) indicates the joint probability of the premise word wp appearing in the premise segment PAiji and the conclusion word wc appearing in the conclusion segment CAiji . p(wp) and p(wc) denote marginal probabilities. Making use of language modeling statistics, we compute the marginal probabilities as follows:\np(wp) = c(wp,P)\u2211 w\u2032p c(w\u2032p,P) p(wc) = c(wc,C)\u2211 w\u2032c c(w\u2032c,C) ,\nwhere c(w,P) denotes the number of occurrences of w within the tokenized premise segments in P and w\u2032p is a token from the premise segment of any abstract. The terms c(w,C) and w\u2032c are defined analogously.\nThe joint probability is then computed as\np(wp;wc) =\n\u2211k i=1 c ( wp, P Ai ji ) c ( wc, C Ai ji )\u2211 (w\u2032p,w \u2032 c) c(w\u2032p,P)c(w\u2032c,C) ,\nBecause mutual information is an unbounded measure that increases with the size of A, it is not directly comparable across different P and C ensembles (Poole et al., 2019). We therefore normalize I(P;C) by mapping it onto the interval [0, 1] and use Normalized Mutual Information (NMI) as the final optimization objective.\nTaken from Kv\u00e5lseth (2017), we compute NMI as follows:\nNMI(P;C) = I(P;C) Ua\nwhere Ua denotes the non-decreasing theoretical upper bound of I(P;C) and is parametrized by the a-order arithmetic mean\nUa = ( UP + UC\n2\n)1/a .\nHere, we have UP = \u2212 \u2211 wp p(wp) log p(wp) = H(P)\nand UC = \u2212 \u2211 wc p(wc) log p(wc) = H(C)\nessentially being the entropy of the premise ensemble and the conclusion ensemble, respectively. For the least upper bound (a = \u2212\u221e), we have\nU\u2212\u221e = lim a\u2192\u2212\u221e Ua = min{UP,UC}\nIn this work, we use U\u2212\u221e to normalize I(P;C) to ensure that the maximal attainable NMI value is 1. This brings us the benefit of comparable NMI scores for different corpus sizes k."
        },
        {
            "heading": "3.3 Greedy Cyclic Abstract Segmentation",
            "text": "We now introduce our GreedyCAS approach to segment abstracts of scientific papers. GreedyCAS performs a search, where we first explore the best segmentation of one particular abstract that maximizes NMI(P;C), then iterate over all abstracts to perform the same maximization.\nAlgorithm 1 describes the basic segmentation approach GreedyCAS-base. Given the input abstract corpus A, the algorithm greedily searches for the segmentation that leads to the maximal NMI(P;C). The output is the optimized segmentation G\u2217.\nAlgorithm 2 illustrates the advanced approach GreedyCAS-NN, where we first split the abstract corpus A into a series of chunks (denoted as Achunk) in size of c; then, for each seed abstract Asji sampled from the current chunk A\nchunk, we perform embedding-based nearest neighbour (NN) search within the chunk to construct the batch (denoted as Abatchs ) comprising the b most semantically relevant abstracts for Asji , by computing the cosine similarity using their abstract embeddings:\n\u2200A \u2208 Achunk,\nsim(Asji , A) = e(Asji) \u00b7 e(A)\n||e(Asji)|| \u00b7 ||e(A)|| Abatchs = { A \u2208 Achunk : rank ( sim(Asji , A) ) \u2a7d b } We use a pre-trained Sentence-BERT model2\n(Reimers and Gurevych, 2019) to acquire the ab2We use the sentence-transformers encoder (pre-trained model all-MiniLM-L6-v2 with model size 80 MB), Apache-2.0 License, available at github.com/UKPLab/ sentence-transformers\nAlgorithm 1: GreedyCAS-base: unsupervised cyclic abstract segmentation\nInput: abstract corpus A = {Ai}i\u2208N1:k Output: optimized segmentation G = {GAi}i\u2208N1:k\n1 Ares \u2190 A; 2 while Ares \u0338= \u2205 do 3 O\u2217A \u2190 0; \u25b7 optimization objective. 4 G\u2217 \u2190 \u2205; 5 Ai \u2190 sample(A); 6 Ares \u2190 Ares\\Ai; 7 {gAij } 6 j=1 \u2190 configure(Ai); 8 GAi \u2190 {gAij } 6 j=1;\n9 foreach epoch do 10 GA res \u2190 \u2205; 11 foreach Ar \u2208 Ares do 12 {gArj } 6 j=1 \u2190 configure(Ar); 13 gArj \u2190 sample({g Ar j } 6 j=1); 14 GAr \u2190 {gArj }; 15 GA res \u2190 GA res \u222aGAr ; 16 end 17 foreach gAij \u2208 G\nAi do 18 gAij = (P Ai j , C Ai j ); 19 foreach GAr \u2208 GA res\ndo 20 gArj \u2190 G\nAr ; 21 gArj = (P Ar j , C Ar j ); 22 end 23 P = PAij \u222a {P Ar j }Ar\u2208Ares ; 24 C = CAij \u222a {C Ar j }Ar\u2208Ares ; 25 OA \u2190 compute-NMI(P;C); 26 if OA > O\u2217A then 27 O\u2217A \u2190 OA; 28 G\u2217 \u2190 GA res \u222aGAi ; 29 end 30 else 31 continue; 32 end 33 end 34 end 35 end 36 return G\u2217;\nstract embeddings. Finally, the same greedy strategy as described in GreedyCAS-base is applied to find the best segmentation for each abstract of the batch. To fully utilize the power of parallel computing, we use multi-threading3 to optimize NMI."
        },
        {
            "heading": "4 Dataset",
            "text": "Since we calculate NMI scores using lexical cooccurrences of words, we constructed a corpus of related abstracts based on the COVID-19 Open Research Dataset (CORD-19) released by Wang et al. (2020). This dataset is a massive collection of sci-\n3We use the Python MultiThreading library https:// docs.python.org/3/library/threading.html\nAlgorithm 2: GreedyCAS-NN: unsupervised cyclic abstract segmentation with nearest neighbor search\nInput: abstract corpus A = {Ai}i\u2208N1:k , chunk size c, batch size b Output: optimized segmentation G\u2217 = {GAi}i\u2208N1:k\n1 G\u2217 \u2190 \u2205; 2 {Achunki }i\u2208N1:k/c \u2190 truncate(A, c); 3 Achunk \u2190 {Achunki }i\u2208N1:k/c ; 4 while len(G\u2217) \u0338= k do 5 foreach Achunki \u2208 Achunk do 6 Gchunk\u2217 \u2190 \u2205; 7 {Asji}j\u2208N1:c/b \u2190 sample(A chunk i ); 8 Asi \u2190 {Asji}j\u2208N1:c/b ; \u25b7 seed abstracts 9 foreach Asji \u2208 A s i do 10 {Asjim}m\u2208N1:b \u2190 NN-search(A s ji ; b); 11 Abatchs \u2190 {Asjim}m\u2208N1:b ; 12 Gbatch\u2217 \u2190 GreedyCAS-base(Abatchs ); 13 Gchunk\u2217 \u2190 Gchunk\u2217 \u222aGbatch\u2217 ; 14 end 15 G\u2217 \u2190 G\u2217 \u222aGchunk\u2217 ; 16 end 17 end 18 return G\u2217;\nentific papers on SARS-CoV-2 coronavirus-related research published since March 2020. These papers share higher lexical commonality than biomedical papers in general due to the focused research interest in COVID-19.\nWe worked with abstracts whose sentences have been categorized into BACKGROUND, METHODS, RESULTS, and CONCLUSION discourse categories. We trusted the categories of these structured abstracts from the CORD-19 corpus since scientific papers are peer-reviewed and multi-round revised. We automatically aggregated the dataset CASauto from 697 structured scientific abstracts whose paper titles contained the keyword vaccine. Inspired by Shieh et al. (2019), we took sentences in BACKGROUND, METHODS, RESULTS categories as premises, and sentences in the CONCLUSION category as conclusions.\nIn addition, we manually constructed a dataset CAS-human of 196 non-structured abstracts from CORD-19, using the keyword antigen to find target abstracts. We then asked four human annotators to label the conclusion sentences within those abstracts. All human annotators were not instructed about the potential positions of conclusion sentences in scientific abstracts. By doing this, we avoided biasing them. To facilitate the annotation process and reduce the annotators\u2019 workload, we\nused the interactive data labeling platform Doccano4 (Nakayama et al., 2018) for constructing the CAS-human dataset.\nTable 2 shows the overall statistics of our proposed datasets for scientific abstract segmentation. During data preprocessing, we intentionally removed stop words, numbers, and punctuations (except \u201c.\u201d, which is essential for the sentence tokenizer5 we used) in the abstracts. We also lowercased all tokens in both datasets for increased computational efficiency.\nFigure 3 shows the positions of the conclusion sentences within the abstracts in the CAS-human dataset as labelled by the human annotators. Similarly, as shown in previous works (Fergadis et al., 2021; Achakulvisut et al., 2019), in 95% of our annotated non-structured abstracts, the positions of conclusion sentences were consistent with our prior assumption (they were among the last 3 sentences of the abstract)."
        },
        {
            "heading": "5 Evaluation",
            "text": ""
        },
        {
            "heading": "5.1 Metrics",
            "text": "To evaluate the segmentation results, we use both set similarity and textual relevance as metrics. We test the performance of our approaches on both automatically (CAS-auto) and manually (CAShuman) aggregated data. To evaluate the segmentation boundaries, we use text segmentation metrics Pk (Beeferman et al., 1999) and WindowDiff (WD, Pevzner and Hearst (2002)). Then, we use ROUGE score (Lin, 2004) to measure the textual relevance between the segmented and ground-truth conclusion sentences. We compute the arithmetic mean of ROUGE-1, ROUGE-2, and ROUGE-Lsum f-\n4MIT License, available at https://github.com/ doccano/doccano\n5We used the sentence-splitter by Philipp Koehn and Josh Schroeder, GNU Lesser General Public License, available at https://github.com/mediacloud/ sentence-splitter\nmeasures. Finally, we use Jaccard index to measure the similarity between the set of segmented conclusion sentences and the set of ground-truth conclusion sentences. Lower Pk and WD scores indicate better segmentation results, whereas higher ROUGE and Jaccard indexes represent better segmentation results."
        },
        {
            "heading": "5.2 Baselines",
            "text": "We present three unsupervised baseline methods for abstract segmentation as baselines. To ensure the comparability of the results, we manually added an additional segmentation boundary at the end of the abstract for any approach that provides only one boundary.\nRandom To test our prior knowledge of the position of conclusions sentences, we set up two random baselines: a) Random-base: following the idea initially proposed by Beeferman et al. (1999), we place segmentation boundaries after two randomly selected sentences; b) Random-plus: we segment an abstract by randomly selecting one from the six possible segmentations described in chapter 3.1.\nTextTiling6 As proposed in Hearst (1997) and serving as the classic text segmentation approach, TextTiling utilizes lexical information to detect topic changes within a given text. In our case, Text-\n6We use the HarvestText implementation for the English language, MIT License, available at https://github. com/blmoistawinde/HarvestText\nTiling places a segmentation boundary between sentences.\nSBERT-sim Inspired by Solbiati et al. (2021), we use the same Sentence-BERT encoder as GreedyCAS-NN to segment abstracts using sentence semantics. Each abstract is split into two segments such that the cosine similarity of their Sentence-BERT embeddings is maximized."
        },
        {
            "heading": "6 Results and Discussion",
            "text": ""
        },
        {
            "heading": "6.1 Non-structured Abstracts",
            "text": "First, we test our supervised GreedyCAS approaches against the baselines on the CAS-human dataset of non-structured abstracts. We list the experiment results in Table 3. For the GreedyCAS approaches, we report the empirical performance with the best batch size. The best batch size hints at how many closely related abstracts together can bring benefits to the estimation of word probabilities, and essentially, the final segmentation results. Intuitively, as the batch size grows, the relatedness among the abstracts drops because it is not possible to get a large number of abstracts that study precisely the same research question. For the Random baselines, we report the best segmentation results from 11 random trials.\nWe see that the GreedyCAS-NN achieves the best performance. The results suggest that this approach works well on non-structured abstracts. Thus, NMI is able to capture the conclusionrelevant information at both the abstracts\u2019 beginning and end."
        },
        {
            "heading": "6.2 Structured Abstracts",
            "text": "Next, we test GreedyCAS approaches against the baselines on the CAS-auto dataset of structured\nabstracts. The results are shown in Table 4.\nFirst, we found that compared to the Randombase baseline, Random-plus improves the results across different measures by large margins.\nWe then observe that the SBERT-sim baseline, which segments abstracts based on the cosine similarity between the premise and conclusion segments, achieves the best performance on three out of four metrics by a large margin. Our best model GreedyCAS-NN only achieves the leading performance measured by Pk, while achieving lower performance measured by other metrics. In Table 5 and 6 in appendix B, we show two abstracts that were wrongly segmented by GreedyCAS-NN: the first abstract has one additional sentence from the BACKGROUND category, whereas the second abstract has one additional sentence from the RESULTS category. These sentences were misattributed by GreedyCAS to the conclusion segment because they increase the NMI score; however, due to the complexity of NMI, understanding the exact reasons why NMI increases is non-trivial."
        },
        {
            "heading": "6.3 Analysis",
            "text": "In Figure 4, we compute the correlation coefficients between NMI scores and each evaluation metric. We plot the NMI scores and the evaluation metrics w.r.t the batch size (ranging from 2 to 12).\nFigure 4 shows that NMI scores are strongly negatively correlated with the text segmentation metrics Pk and WindowDiff, but are strongly positively correlated with the set similarity metric Jaccard index and the lexical metric ROUGE.\nIn Figure 5, rather than studying the complex scenario where an entire sentence gets re-attributed, we showcase the impact on NMI when just one word is moved from the premise segment to the conclusion segment. We studied whether placing segmentation boundaries between words can provide similar NMI(P;C) scores compared to placing\nthem within sentences. To do this, we randomly pick one abstract from the CAS-human dataset and calculate the changes in NMI(P;C) due to the relocation of the segmentation boundary caused by one word at a time moved to the right.\nWe see in Figure 5 that the slope at word positions near sentence boundary is not always steeper than at word positions within the sentences, which indicates that segmenting abstracts by putting seg-\nmentation boundaries after complete sentences might not be the optimal choice when optimizing with NMI."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this explorative work, we propose an unsupervised approach, GreedyCAS, for automatically segmenting scientific abstracts into conclusions and premises. We introduce the cyclic abstract segmentation pipeline, which can be applied to structured and non-structured abstracts. Our approach leverages the lexical information between words that co-occur in the conclusion and premise segments and finds the best segmentation of a set of abstracts using NMI as an optimization objective. Our empirical results show that NMI is an effective indicator for the segmentation results of scientific abstracts."
        },
        {
            "heading": "8 Limitations",
            "text": "In this work, we explored the use of normalized mutual information as an optimization objective for scientific abstract segmentation. The main limitations of our work are listed below:\n\u2022 The input abstracts of GreedyCAS need to be on similar research topics; otherwise, their shared vocabulary is limited and the word probabilities in the computation of NMI cannot be estimated well.\n\u2022 GreedyCAS has a high time and space complexity because it involves searching for the best segmentation and enumeration over all possible word pairs at each iteration. As a result, GreedyCAS takes a long time to execute and using larger batch sizes is a challenge with limited computational resources.\n\u2022 We did not try to assess the reliability of wordpair probability estimation. Presumably, the more abstracts are considered, the better the probability estimates of frequent word pairs, but the more infrequent outlier pairs creep in with biased probability estimates. Thus, it seems not obvious that more abstracts will necessarily yield better probability and mutual information estimates.\nIn the future, we will analytically study how to increase the efficiency and the applicability of GreedyCAS by considering other ways of estimating the word probabilities."
        },
        {
            "heading": "9 Acknowledgements",
            "text": "We acknowledge the support from Swiss National Science Foundation NCCR Evolving Language, Agreement No.51NF40_180888. We also thank the anonymous reviewers for their constructive comments and feedback."
        },
        {
            "heading": "A Exhaustive Search",
            "text": "We tested GreedyCAS on five well-structured abstracts (in total 7,776 configurations) from the CAS-auto dataset, where we use different a-orders to normalize the MI. To do this, we brute-forcely calculated NMI values for all configurations. Figure 6 shows the progressive change of the running maximum NMI in the exhaustive search, where the global maximum was reached at around 5,100.th configuration. For all three cases, GreedyCAS managed to reach the same global maximum. However, due to the exponential increase of exhaustive search cost w.r.t number of abstracts, testing GreedyCAS over a larger amount of abstracts was difficult.\nWe further tested GreedyCAS on five abstracts with different numbers of trials. Each trial involved different random segmentation initializations. Figure 7 shows the distributions of the number of iterations under different numbers of trials that GreedyCAS required to achieve maximal NMI. We see that GreedyCAS is able to find the maximal NMI within 60 iterations during all trials."
        },
        {
            "heading": "B Dataset Example",
            "text": "In the following tables, sentences in the premise segment are highlighted in blue, whereas sentences in the conclusion segment are highlighted in red.\nTitle: Additional evidence on the efficacy of different Akirin vaccines assessed on Anopheles arabiensis (Diptera: Culicidae) (Letinic\u0301 et al., 2021)\nTitle: Interest in COVID-19 vaccine trials participation among young adults in China: Willingness, reasons for hesitancy, and demographic and psychosocial determinants (Sun et al., 2021)\nTable 7 shows an example abstract that contains a word pair, whose contribution to the overall NMI\nscore is the greatest.\nC Impact on NMI When Moving One Word\nLet wp \u2208 P be any premise word, wc \u2208 C be any conclusion word, Ai a fixed abstract, and gAij one possible segmentation (we use the index j to represent the segmentation). By moving one arbitrary word w from the premise segment PAij to the conclusion segment C Ai j , I(P;C) changes. We investigate the major terms in the equation of mutual information. We aim to find those word pairs that predominantly contribute to I(P;C) so that the computation can be simplified, which essentially will allow the algorithm to run on a larger batch size.\nFirst, we examine what happens to the marginal probabilities p(wp) and p(wc):\np(wp) = c(wp,P)\u2212 I[wp = w]\u2211\nw\u2032p c(w\u2032p,P)\u2212 1\np(wc) = c(wc,C) + I[wc = w]\u2211\nw\u2032c c(w\u2032c,C) + 1\nHere I denotes an indicator function and c a counter function. The indicator function takes the value 1 if the condition in the bracket is fulfilled, otherwise, it takes the value 0. For the marginal probabilities, we have the following cases:\n\u2022 If wp = w, then p(wp) decreases; if wp \u0338= w, then p(wp) increases.\n\u2022 If wc = w, then p(wc) increases; if wc \u0338= w, then p(wc) decreases.\nThen, we examine what happens to p(wp;wc), the main term in computation of I(P;C)\np(wp;wc) =\n\u2211 j \u0338=i c(wp, P Aj )c(wc, C Aj ) + ( c(wp, P Ai)\u2212 I[wp = w] ) ( c(wc, C Ai) + I[wc = w] )\u2211 (w\u2032p,w \u2032 c) ( c(w\u2032p,P)\u2212 I[w\u2032p = w] ) (c(w\u2032c,C) + I[w\u2032c = w])\n=\nconstant\ufe37 \ufe38\ufe38 \ufe37\u2211 j \u0338=i c(wp, P Aj )c(wc, C Aj )+ \u03b1\ufe37 \ufe38\ufe38 \ufe37( c(wp, P Ai)\u2212 I[wp = w] ) \u03b2\ufe37 \ufe38\ufe38 \ufe37( c(wc, C Ai) + I[wc = w] ) \u2211\n(w\u2032p,w \u2032 c) c(w\u2032p,P)c(w\u2032c,C)\ufe38 \ufe37\ufe37 \ufe38 constant\n+ \u2211\n(w\u2032p,w \u2032 c)\n{ c(w\u2032p,P)I[w\u2032c = w]\u2212 c(w\u2032c,C)I[w\u2032p = w]\u2212 I[w\u2032p = w]I[w\u2032c = w] } \ufe38 \ufe37\ufe37 \ufe38\n\u03b3\n= a+ \u03b1\u03b2\nb+ \u03b3 ,\nhere a and b are the constant terms within the fraction, since moving w in Ai will not affect other abstracts. For the joint probability, we have the following cases:\n\u2022 If wp \u0338= w and wc \u0338= w, p(wp;wc) remains unchanged.\n\u2022 If wp = w and wc \u0338= w, then\np(wp;wc) = a+ (\u03b1\u2212 1)\u03b2 b\u2212 c(wc,C) .\n\u2022 If wp \u0338= w and wc = w, then\np(wp;wc) = a+ \u03b1(\u03b2 + 1)\nb+ c(wp,P) .\n\u2022 If wp = w and wc = w, then\np(wp;wc) = a+ (\u03b1\u2212 1)(\u03b2 + 1)\nb+ c(wp,P)\u2212 c(wc,C)\u2212 1 .\nTill this point, we found it very difficult to predict how p(wp;wc) will change when moving w. The reasons are:\n\u2022 For the case of wp = w and wc = w, we cannot tell a priori whether c(w,P)\u2212 c(w,C) is positive, i.e. whether w appears more frequently within premise segments or conclusion segments. This leads to the uncertainty of determining the change in the sign of mutual information.\n\u2022 The normalizing factor of NMI, which is essentially the minimum between H(P) and H(C), cannot be determined after moving w.\nAlso, because for any research domain, it is nearly impossible to get a large number of papers that study exactly the same research question (e.g., it\u2019s not possible to get thousands of papers that study the effectiveness of COVID-19 vaccines, due to limited number of clinical trials that have been done so far), therefore, further increasing the batch size is not feasible.\nDue to the above reasons, we only computationally studied how NMI would change when moving one word from the premise segment to the conclusion segment (see Figure 5)."
        }
    ],
    "title": "GreedyCAS: Unsupervised Scientific Abstract Segmentation with Normalized Mutual Information",
    "year": 2023
}