{
    "abstractText": "Modern systems for multi-hop question answering (QA) typically break questions into a sequence of reasoning steps, termed chain-ofthought (CoT), before arriving at a final answer. Often, multiple chains are sampled and aggregated through a voting mechanism over the final answers, but the intermediate steps themselves are discarded. While such approaches improve performance, they do not consider the relations between intermediate steps across chains and do not provide a unified explanation for the predicted answer. We introduce MultiChain Reasoning (MCR), an approach which prompts large language models to meta-reason over multiple chains of thought, rather than aggregate their answers. MCR examines different reasoning chains, mixes information between them and selects the most relevant facts in generating an explanation and predicting the answer. MCR outperforms strong baselines on 7 multi-hop QA datasets. Moreover, our analysis reveals that MCR explanations exhibit high quality, enabling humans to verify its answers.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ori Yoran"
        },
        {
            "affiliations": [],
            "name": "Tomer Wolfson"
        },
        {
            "affiliations": [],
            "name": "Ben Bogin"
        },
        {
            "affiliations": [],
            "name": "Uri Katz"
        },
        {
            "affiliations": [],
            "name": "Daniel Deutch"
        },
        {
            "affiliations": [],
            "name": "Jonathan Berant"
        }
    ],
    "id": "SP:4326f1de033500c10c8630f6756318f2155623c4",
    "references": [
        {
            "authors": [
                "Daniel Adiwardana",
                "Minh-Thang Luong",
                "David R. So",
                "Jamie Hall",
                "Noah Fiedel",
                "Romal Thoppilan",
                "Zi Yang",
                "Apoorv Kulshreshtha",
                "Gaurav Nemade",
                "Yifeng Lu",
                "Quoc V. Le"
            ],
            "title": "Towards a human-like opendomain chatbot",
            "year": 2020
        },
        {
            "authors": [
                "Rami Aly",
                "Zhijiang Guo",
                "M. Schlichtkrull",
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Oana Cocarascu",
                "Arpit Mittal."
            ],
            "title": "Feverous: Fact extraction and verification over unstructured and structured information",
            "venue": "ArXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Jack Clark",
                "Christopher Berner",
                "Sam McCandlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei."
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Process-",
            "year": 2020
        },
        {
            "authors": [
                "Xinyun Chen",
                "Maxwell Lin",
                "Nathanael Sch\u00e4rli",
                "Denny Zhou"
            ],
            "title": "Teaching large language models to self-debug",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel."
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "ArXiv, abs/2204.02311.",
            "year": 2022
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Mark Chen",
                "Heewoo Jun",
                "Lukasz Kaiser",
                "Matthias Plappert",
                "Jerry Tworek",
                "Jacob Hilton",
                "Reiichiro Nakano"
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "ArXiv preprint,",
            "year": 2021
        },
        {
            "authors": [
                "Antonia Creswell",
                "Murray Shanahan",
                "Irina Higgins."
            ],
            "title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "venue": "ArXiv, abs/2205.09712.",
            "year": 2022
        },
        {
            "authors": [
                "Daniel De Freitas",
                "Minh-Thang Luong",
                "David R. So",
                "Jamie Hall",
                "Noah Fiedel",
                "Romal Thoppilan",
                "Zi Yang",
                "Apoorv Kulshreshtha",
                "Gaurav Nemade",
                "Yifeng Lu",
                "Quoc V. Le."
            ],
            "title": "Towards a human-like opendomain chatbot",
            "venue": "ArXiv, abs/2001.09977.",
            "year": 2020
        },
        {
            "authors": [
                "Yao Fu",
                "Hao-Chun Peng",
                "Ashish Sabharwal",
                "Peter Clark",
                "Tushar Khot."
            ],
            "title": "Complexity-based prompting for multi-step reasoning",
            "venue": "ArXiv, abs/2210.00720.",
            "year": 2022
        },
        {
            "authors": [
                "Luyu Gao",
                "Zhuyun Dai",
                "Panupong Pasupat",
                "Anthony Chen",
                "Arun Tejasvi Chaganty",
                "Yicheng Fan",
                "Vincent Y. Zhao",
                "Ni Lao",
                "Hongrae Lee",
                "Da-Cheng Juan",
                "Kelvin Guu"
            ],
            "title": "Rarr: Researching and revising what language models say, using language",
            "year": 2022
        },
        {
            "authors": [
                "Mor Geva",
                "Daniel Khashabi",
                "Elad Segal",
                "Tushar Khot",
                "Dan Roth",
                "Jonathan Berant."
            ],
            "title": "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
            "venue": "Transactions of the Association for Computational Linguistics, 9:346\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Hangfeng He",
                "Hongming Zhang",
                "Dan Roth"
            ],
            "title": "Rethinking with retrieval: Faithful large language model inference",
            "year": 2022
        },
        {
            "authors": [
                "Jie Huang",
                "Kevin Chen-Chuan Chang"
            ],
            "title": "Towards reasoning in large language models: A survey",
            "year": 2022
        },
        {
            "authors": [
                "Alon Jacovi",
                "Yoav Goldberg"
            ],
            "title": "Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Benjamin Mann",
                "Sam McCandlish",
                "Christopher Olah",
                "Jared Kaplan."
            ],
            "title": "Language models (mostly) know what they know",
            "venue": "ArXiv, abs/2207.05221.",
            "year": 2022
        },
        {
            "authors": [
                "Ashwin Kalyan",
                "Abhinav Kumar",
                "Arjun Chandrasekaran",
                "Ashish Sabharwal",
                "Peter Clark."
            ],
            "title": "How much coffee was consumed during EMNLP 2019? fermi problems: A new reasoning challenge for AI",
            "venue": "Proceedings of the 2021 Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "O. Khattab",
                "Keshav Santhanam",
                "Xiang Lisa Li",
                "David Leo Wright Hall",
                "Percy Liang",
                "Christopher Potts",
                "Matei A. Zaharia."
            ],
            "title": "Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive nlp",
            "venue": "ArXiv, abs/2212.14024.",
            "year": 2022
        },
        {
            "authors": [
                "Omar Khattab",
                "Matei Zaharia."
            ],
            "title": "Colbert: Efficient and effective passage search via contextualized late interaction over BERT",
            "venue": "Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval,",
            "year": 2020
        },
        {
            "authors": [
                "Tushar Khot",
                "Harsh Trivedi",
                "Matthew Finlayson",
                "Yao Fu",
                "Kyle Richardson",
                "Peter Clark",
                "Ashish Sabharwal"
            ],
            "title": "Decomposed prompting: A modular approach for solving complex tasks",
            "year": 2023
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "ICML 2022 Workshop on Knowledge Retrieval and Language Models.",
            "year": 2022
        },
        {
            "authors": [
                "Angeliki Lazaridou",
                "Elena Gribovskaya",
                "Wojciech Jan Stokowiec",
                "Nikolai Grigorev"
            ],
            "title": "Internetaugmented language models through few-shot prompting for open-domain question answering",
            "year": 2023
        },
        {
            "authors": [
                "Yifei Li",
                "Zeqi Lin",
                "Shizhuo Zhang",
                "Qiang Fu",
                "Bei Chen",
                "Jian-Guang Lou",
                "Weizhu Chen."
            ],
            "title": "On the advance of making language models better reasoners",
            "venue": "ArXiv, abs/2206.02336.",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Pan Lu",
                "Liang Qiu",
                "Wenhao Yu",
                "Sean Welleck",
                "Kai-Wei Chang"
            ],
            "title": "A survey of deep learning for mathematical reasoning",
            "year": 2022
        },
        {
            "authors": [
                "bakhsh",
                "Peter Clark"
            ],
            "title": "Self-refine: Iterative refinement with self-feedback",
            "year": 2023
        },
        {
            "authors": [
                "Maxwell Nye",
                "Anders Johan Andreassen",
                "Guy Gur-Ari",
                "Henryk Michalewski",
                "Jacob Austin",
                "David Bieber",
                "David Dohan",
                "Aitor Lewkowycz",
                "Maarten Bosma",
                "David Luan",
                "Charles Sutton",
                "Augustus Odena"
            ],
            "title": "Show your work: Scratchpads for intermediate",
            "year": 2022
        },
        {
            "authors": [
                "Bhargavi Paranjape",
                "Scott M. Lundberg",
                "Sameer Singh",
                "Hanna Hajishirzi",
                "Luke Zettlemoyer",
                "Marco Tulio Ribeiro."
            ],
            "title": "Art: Automatic multistep reasoning and tool-use for large language models",
            "venue": "ArXiv, abs/2303.09014.",
            "year": 2023
        },
        {
            "authors": [
                "Debjit Paul",
                "Mete Ismayilzada",
                "Maxime Peyrard",
                "Beatriz Borges",
                "Antoine Bosselut",
                "Robert West",
                "Boi Faltings"
            ],
            "title": "Refiner: Reasoning feedback on intermediate representations",
            "year": 2023
        },
        {
            "authors": [
                "Ofir Press",
                "Muru Zhang",
                "Sewon Min",
                "Ludwig Schmidt",
                "Noah A. Smith",
                "Mike Lewis."
            ],
            "title": "Measuring and narrowing the compositionality gap in language models",
            "venue": "ArXiv, abs/2210.03350.",
            "year": 2022
        },
        {
            "authors": [
                "Danish Pruthi",
                "Rachit Bansal",
                "Bhuwan Dhingra",
                "Livio Baldini Soares",
                "Michael Collins",
                "Zachary C. Lipton",
                "Graham Neubig",
                "William W. Cohen"
            ],
            "title": "Evaluating explanations: How much do explanations from the teacher aid students",
            "year": 2022
        },
        {
            "authors": [
                "Shuofei Qiao",
                "Yixin Ou",
                "Ningyu Zhang",
                "Xiang Chen",
                "Yunzhi Yao",
                "Shumin Deng",
                "Chuanqi Tan",
                "Fei Huang",
                "Huajun Chen"
            ],
            "title": "Reasoning with language model prompting: A survey",
            "year": 2022
        },
        {
            "authors": [
                "Ori Ram",
                "Yoav Levine",
                "Itay Dalmedigos",
                "Dor Muhlgay",
                "Amnon Shashua",
                "Kevin Leyton-Brown",
                "Yoav Shoham."
            ],
            "title": "In-context retrieval-augmented language models",
            "venue": "Transactions of the Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Keshav Santhanam",
                "Omar Khattab",
                "Jon Saad-Falcon",
                "Christopher Potts",
                "Matei Zaharia."
            ],
            "title": "ColBERTv2: Effective and efficient retrieval via lightweight late interaction",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of",
            "year": 2022
        },
        {
            "authors": [
                "Noah Shinn",
                "Federico Cassano",
                "Edward Berman",
                "Ashwin Gopinath",
                "Karthik Narasimhan",
                "Shunyu Yao"
            ],
            "title": "Reflexion: Language agents with verbal reinforcement learning",
            "year": 2023
        },
        {
            "authors": [
                "Zhiqing Sun",
                "Xuezhi Wang",
                "Yi Tay",
                "Yiming Yang",
                "Denny Zhou."
            ],
            "title": "Recitation-augmented language models",
            "venue": "ICLR.",
            "year": 2023
        },
        {
            "authors": [
                "Oyvind Tafjord",
                "Matt Gardner",
                "Kevin Lin",
                "Peter Clark."
            ],
            "title": "QuaRTz: An open-domain dataset of qualitative relationship questions",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Oyvind Tafjord",
                "Bhavana Dalvi Mishra",
                "Peter Clark"
            ],
            "title": "Entailer: Answering questions with faithful and truthful chains of reasoning",
            "year": 2022
        },
        {
            "authors": [
                "Alon Talmor",
                "Jonathan Berant."
            ],
            "title": "The web as a knowledge-base for answering complex questions",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2018
        },
        {
            "authors": [
                "Alon Talmor",
                "Ori Yoran",
                "Amnon Catav",
                "Dan Lahav",
                "Yizhong Wang",
                "Akari Asai",
                "Gabriel Ilharco",
                "Hannaneh Hajishirzi",
                "Jonathan Berant."
            ],
            "title": "Multimodalqa: complex question answering over text, tables and images",
            "venue": "9th International Conference",
            "year": 2021
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Harsh Trivedi",
                "Niranjan Balasubramanian",
                "Tushar Khot",
                "Ashish Sabharwal"
            ],
            "title": "2022a. Interleaving retrieval with chain-of-thought reasoning for knowledgeintensive multi-step questions",
            "year": 2022
        },
        {
            "authors": [
                "Harsh Trivedi",
                "Niranjan Balasubramanian",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "MuSiQue: Multihop questions via single-hop question composition",
            "venue": "Transactions of the Association for Computational Linguistics, 10:539\u2013554.",
            "year": 2022
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V Le",
                "Ed H. Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "The Eleventh International Conference on Learning",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Huai hsin Chi",
                "F. Xia",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Johannes Welbl",
                "Pontus Stenetorp",
                "Sebastian Riedel."
            ],
            "title": "Constructing datasets for multi-hop reading comprehension across documents",
            "venue": "Transactions of the Association for Computational Linguistics, 6:287\u2013 302.",
            "year": 2018
        },
        {
            "authors": [
                "Tomer Wolfson",
                "Mor Geva",
                "Ankit Gupta",
                "Matt Gardner",
                "Yoav Goldberg",
                "Daniel Deutch",
                "Jonathan Berant."
            ],
            "title": "Break it down: A question understanding benchmark",
            "venue": "Transactions of the Association for Computational Linguistics, 8:183\u2013198.",
            "year": 2020
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D. Manning."
            ],
            "title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
            "venue": "Proceedings of the 2018 Conference on Empiri-",
            "year": 2018
        },
        {
            "authors": [
                "Shunyu Yao",
                "Dian Yu",
                "Jeffrey Zhao",
                "Izhak Shafran",
                "Thomas L. Griffiths",
                "Yuan Cao",
                "Karthik Narasimhan"
            ],
            "title": "Tree of thoughts: Deliberate problem solving with large language models",
            "year": 2023
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik Narasimhan",
                "Yuan Cao."
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "venue": "ArXiv preprint, abs/2210.03629.",
            "year": 2022
        },
        {
            "authors": [
                "Ori Yoran",
                "Tomer Wolfson",
                "Ori Ram",
                "Jonathan Berant"
            ],
            "title": "Making retrieval-augmented language models robust to irrelevant context",
            "year": 2023
        },
        {
            "authors": [
                "Eric Zelikman",
                "Yuhuai Wu",
                "Jesse Mu",
                "Noah Goodman."
            ],
            "title": "STar: Bootstrapping reasoning with reasoning",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Scharli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Huai hsin Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "John",
                "Count"
            ],
            "title": "Palatine of Neumarkt: John (Johann von Pfalz-Neumarkt; 1383 \u2013 14 March 1443) was the Count Palatine of Neumarkt from 1410 to his death",
            "venue": "The son of Rupert III of the Palatinate, he married Catherine of Pomerania in 1407",
            "year": 1443
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In chain-of-thought (CoT) prompting, a large language model (Brown et al., 2020; Chowdhery et al., 2022; Kadavath et al., 2022; Touvron et al., 2023) is prompted to generate its answer following a stepby-step explanation (Wei et al., 2022; Nye et al., 2022). CoT prompting has been shown to dramatically improve performance on reasoning-heavy tasks (Kojima et al., 2022; Zhou et al., 2022). Furthermore, Wang et al. (2023) showed that sampling multiple chains of thought and returning their majority output further improves accuracy, a method which they term self-consistency (SC).\nWhile SC leads to performance gains, it also has several shortcomings. First, when the space of possible outputs is large (Kalyan et al., 2021), each reasoning chain may lead to a different output, in\n*Both authors contributed equally to this work.\nwhich case no significant majority will be formed. Second, focusing exclusively on the final output discards relevant information that is present in the intermediate reasoning steps. Consider answering the question \u201cDid Brad Peyton need to know about seismology?\u201d (Fig. 1). Reasoning chain #1 leads to an incorrect answer (\u201cNo\u201d), but its steps provide useful information. For example, the intermediate question, and following answer, on \u201cWhat is seismology?\u201d constitute an important fact that is absent from the other two chains. Last, using SC jointly with chain-of-thought prompting reduces interpretability, as there is no single reasoning chain that can be considered as an explanation.\nIn this work, we propose Multi-Chain Reasoning (MCR), where we prompt a large language model"
        },
        {
            "heading": "E1: Standing 309.6 metres (1,016 feet) high, The Shard \u2026",
            "text": ""
        },
        {
            "heading": "E2: Ants range in size from 0.75 \u2026",
            "text": ""
        },
        {
            "heading": "Q1: How high is the shard?",
            "text": ""
        },
        {
            "heading": "A1: The shard is 1016 feet in height.",
            "text": ""
        },
        {
            "heading": "Q2: How long is the average ant?",
            "text": "(LLM) to meta-reason across multiple reasoning chains and produce a final answer, alongside an explanation. Unlike prior work, sampled reasoning chains are used not for their predictions (as in SC) but as a means to collect pieces of evidence from multiple chains. Fig. 1 illustrates MCR compared to SC. While both methods rely on sampling multiple reasoning chains, SC returns the majority answer, \u201cNo\u201d (grey box, bottom right). By contrast, MCR concatenates the intermediate steps from each chain (blue boxes, top left) into a unified context, which is passed, along with the original question, to a meta-reasoner model. The metareasoner is a separate LLM, prompted to metareason on multiple reasoning chains and produce a final answer along with an explanation (pink box, bottom left). By reasoning on multiple reasoning chains, MCR is able to mitigate the aforementioned drawbacks \u2013 it combines facts from multiple chains to produce the correct final answer, with an explanation of the answer\u2019s validity.\nMCR has three main components (\u00a73). To generate reasoning chains we use two components, a decomposition model and a retriever which jointly generate the chain (Fig. 2), similar to prior work (Press et al., 2022; Trivedi et al., 2022a). These chains are then concatenated into a unified multichain context which is fed to the aforementioned\nmeta-reasoner. Fig. 1 highlights the ability of the meta-reasoner to combine facts from different reasoning chains (intermediate answers in pink). The output explanation combines facts from each of the three chains: (1) \u201cSeismology is the study of earthquakes\u201d; (2) \u201cSan Andreas is a film...\u201d; (3)\n\u201cBrad Peyton is a film director, writer...\u201d. SC (in grey) errs due to only using the answers, while the meta-reasoner reads entire reasoning chains, and is able to correctly answer the question.\nWe evaluate MCR on a wide range of challenging multi-hop question answering (QA) datasets, in an open-domain setting. The datasets can be categorized into two types of tasks: implicit reasoning tasks, where reasoning steps are implicit given the question text and need to be inferred using a strategy (Tafjord et al., 2019; Geva et al., 2021; Kalyan et al., 2021); explicit reasoning tasks, where a single reasoning strategy exists and can be directly inferred given the language of the question (Yang et al., 2018; Welbl et al., 2018; Press et al., 2022; Aly et al., 2021). As our baselines, we compare MCR to SC, as well as to variants of SelfAsk (Press et al., 2022) and CoT augmented with retrieval, following Trivedi et al. (2022a). Our results show MCR consistently outperforms all other baselines, in particular, beating SC by up to 5.7%, while using the same reasoning chains (\u00a74).\nWe analyze the qualities of MCR in \u00a75, by manually scoring its generated explanations and estimating their accuracy. Our analysis shows that MCR generates high quality explanations for over 82% of examples, while fewer than 3% are unhelpful. To conclude, our main contributions are:\n\u2022 We introduce the MCR method for metareasoning on multiple chains-of-thought.\n\u2022 We show that MCR outperforms all baselines, including self-consistency, on all 7 multi-hop open-domain QA benchmarks.\n\u2022 We analyze MCR for its explanation quality and its multi-chain reasoning capabilities.\nOur data and codebase are publicly available.1"
        },
        {
            "heading": "2 Background",
            "text": "Recently, there has been a surge of interest in answering multi-hop questions through few-shot prompting of LLMs (Wei et al., 2022; Nye et al., 2022; Yao et al., 2022). The majority of these works follow a common standard: First, given a question, plan a step-by-step reasoning chain to derive the answer and solve all intermediate steps, aided by a retriever to minimize model hallucination (Khot et al., 2023; Press et al., 2022; Yao et al., 2022; Lazaridou et al., 2023; Trivedi et al., 2022a; Khattab et al., 2022). Then, incorporate multiple reasoning chains with answers to derive the final answer (Wang et al., 2023; Li et al., 2022). In our work, we follow this template and focus on the latter part. However, our meta-reasoning approach differs from prior work by reasoning on multiple reasoning chains. Namely, we use multiple chains to collect relevant evidence for question answering."
        },
        {
            "heading": "3 Method",
            "text": "We present a method for answering questions by meta-reasoning on multiple reasoning chains. Our focus is on open-domain QA, where the input is a question q, and the evidence to answer it is found in one or more sentences in a corpus C. When answering q requires multiple reasoning steps, it can be expressed by a reasoning chain, denoted by r. The reasoning chain is a list of one or more intermediate question-evidence-answer triples (qi, ei, ai). Evidence ei \u2208 C is a sentence that is relevant to answering the intermediate question qi.\nFig. 2 describes our approach when answering \u201cHow many ants would fit into The Shard?\u201d. First,\n1https://github.com/oriyor/reasoning-on-cots\nwe use a prompted LLM to generate multiple reasoning chains, r(1), ..., r(k) (steps 1-2). Each r(j)\nis generated by interleaving generated intermediate questions with retrieved contexts (\u00a73.1). Our main contribution is step 3: We introduce a second LLM that is prompted to meta-reason on multiple reasoning chains, collecting evidence facts as its explanation and generating the final answer (\u00a73.2)."
        },
        {
            "heading": "3.1 Generating Reasoning Chains",
            "text": "Given a question q, we generate its reasoning chain using: (1) a decomposition model, and (2) a retriever component. Our reasoning chain generation process is largely based on prior work (Press et al., 2022; Trivedi et al., 2022a), discussed in \u00a72. Fig. 3 describes the interleaving of decomposition and retrieval. At each step, the decomposition model generates an intermediate question qi, based on the original question q and the previous reasoning steps. Then, the retriever uses qi to retrieve relevant evidence ei \u2208 C. We feed ei and qi to the decomposition model (along with the previous steps) to generate intermediate answer ai. During answer generation, we prepend intermediate evidence sentences to the beginning of the chain rather than interleaving them, as it improves the accuracy for all baselines. For decomposition prompts, see \u00a7D."
        },
        {
            "heading": "3.2 Reasoning over Reasoning Chains",
            "text": "The meta-reasoner module is the core contribution of MCR. Instead of sampling multiple chains for their predicted answers (Wang et al., 2023), we utilize them for context generation. This context\nis fed to a prompted LLM to read the generated chains and reason over them to return the answer.\nIn \u00a73.1, we defined a reasoning chain as a list of (qi, ei, ai) triples. We first sample multiple chains and use all of their intermediate questionanswer pairs (qi, ai) as our multi-chain context (a variant using question-evidence pairs (qi, ei) is described in \u00a7B.4). Fig. 2 presents the multi-chain context of the three sampled chains (lower pink box). Next, the multi-chain context and the original question are input to the meta-reasoner. This model is an LLM, few-shot prompted for QA over a multi-chain context. Fig. 4 presents one exemplar from the meta-reasoner prompt for the FEVEROUS dataset (full prompts in \u00a7D). We instruct the LLM to \u201canswer the question step-by-step\u201d given its multi-chain context, where each line describes a (qi, ai) pair from one of the sampled chains. Next, we append the question and a step-by-step reasoning chain followed by the final answer. This last chain serves as the explanation for solving the question. The meta-reasoner is prompted with 6-10 exemplars, based on the dataset (\u00a74.1).\nProviding the meta-reasoner with multiple chains allows it to combine and aggregate facts across chains. Moreover, the model needs to extract the most relevant facts in the chains to serve as its explanation. This enables MCR to be both more accurate and more interpretable than past multi-chain approaches (as we analyze in \u00a75)."
        },
        {
            "heading": "4 Experiments",
            "text": "We compare MCR to existing methods on 7 multihop QA benchmarks. These cover a wide range of reasoning skills, including commonsense, composition, comparison and fact verification. MCR consistently outperforms existing approaches on all benchmarks, when experimenting with two different LLMs and retrievers. Our setting is described in \u00a74.1 and we discuss our main results in \u00a74.2."
        },
        {
            "heading": "4.1 Experimental Setting",
            "text": ""
        },
        {
            "heading": "4.1.1 Datasets",
            "text": "As our focus is on multi-hop questions (in an opendomain setting), all datasets require multiple reasoning steps. Following prior work (Khattab et al., 2022; Trivedi et al., 2022a) and to limit the cost of model API calls, we evaluate on 500-1000 random examples from the development set of each\ndataset.2 We also evaluate on the official test sets of STRATEGYQA and FERMI, as they target implicit reasoning, have multiple valid strategies, and their test set evaluation cost is reasonable. For all datasets, we make sure that no evaluation questions appear in any of our prompts. Tab. 1 has example questions from each dataset. Our multi-hop QA benchmarks can be categorized based on their required reasoning skills:\n\u2022 Implicit Reasoning: Questions that entail implicit reasoning steps (Geva et al., 2021). The reasoning steps for solving it cannot be explicitly derived from the language of the question and require commonsense or arithmetic reasoning. Such questions may have multiple valid reasoning chains. We evaluate on: STRATEGYQA (Geva et al., 2021), FERMI (Kalyan et al., 2021) and QUARTZ (Tafjord et al., 2019).\n\u2022 Explicit Reasoning: Multi-hop questions where the reasoning steps are explicitly expressed in the language of the question (composition, comparison). These include HOTPOTQA (Yang et al., 2018), 2WIKIMQA (Welbl et al., 2018) and BAMBOOGLE (Press et al., 2022). We also evaluate on FEVEROUS (Aly et al., 2021), a fact verification dataset where claims require verifying multiple facts, and evidence may be either in sentences, tables or both.\nFor evaluation, we use F1 to compare predicted and gold answers for all explicit reasoning datasets\n2We use the entire development set for QUARTZ and BAMBOOGLE, since they include less than 500 examples. For FERMI we use all 286 \u201cReal Fermi Problems\u201d in its train and development sets. Exact numbers are listed in Tab. 2."
        },
        {
            "heading": "Dataset Example",
            "text": "STRATEGYQA (implicit) Can Arnold Schwarzenegger deadlift an adult Black rhinoceros?\nFERMI (implicit)\nHow many high fives has LeBron James given/received?\nQUARTZ (implicit)\nJeff drained his rice field in the wintertime. The field likely will produce __ crops when he uses it. A. more B. less\nHOTPOTQA (explicit)\nWhat city did the musician whose debut album shares its title with the 1959 Alfred Hitchcock film hail from?\n2WIKIMQA (explicit) Where was the place of death of Isabella of Bourbon\u2019s father?\nBAMBOOGLE (explicit) What is the maximum airspeed (in km/h) of the third fastest bird?\nFEVEROUS (explicit)\nIs it true that Robert Broderip lived in London all his life and wrote a considerable quantity of music during the earlier part of the nineteenth century?\nTable 1: The multi-hop QA datasets in our experiments.\nand exact-match for the binary-choice datasets. In FERMI, we use the official order-of-magnitude evaluation by Kalyan et al. (2021). We provide additional technical details on evaluation in \u00a7A."
        },
        {
            "heading": "4.1.2 Models",
            "text": "Our main models and baselines are all retrievalaugmented instances of code-davinci-002, prompted with in-context learning exemplars (Brown et al., 2020). In \u00a74.3, we include additional experiments with the open-source Vicuna-13B (Chiang et al., 2023) LLM. Prompt exemplars are formatted as described in \u00a73.2. The number of exemplars varies from 6-12 between datasets. Decomposition prompt exemplars are based on random examples from the train and development sets, coupled with their gold reasoning chain. For the meta-reasoner exemplars, we use reasoning chains sampled from the decomposition model as the multi-chain context. We ensure that the answer can be inferred using the sampled chains and add an explanation before the final answer, as shown in Fig. 4. For the binary-choice datasets, STRATEGYQA, QUARTZ, and FEVEROUS, the prompt contains an equal number of exemplars from each label. For additional details regarding the full prompts, length statistics and robustness to a different choice of prompts, please refer to \u00a7D.\nMeta-Reasoner We experiment with two variants of the meta-reasoner to measure the effect of\nreasoning on more than a single chain. \u2022 MCR: The meta-reasoner is given five rea-\nsoning chains as its multi-chain context (\u00a73.2). We decode one chain with greedy decoding, and sample another four reasoning chains with temperature t = 0.7.3 This enables the metareasoner to review different pieces of evidence when answering the full question (\u00a75).\n\u2022 SCR: Single-Chain Reasoning (SCR) serves as an ablation for the effect of the multi-chain context. In SCR, the meta-reasoner is given the same prompt as MCR aside from having only the greedy-decoded chain in its context. This disentangles the effect of using multiple chains from the effect of having an LLM that is separate from the decomposition model to generate the final answer.\nBaselines We evaluate the following baselines: \u2022 SA: Self-Ask (Press et al., 2022) returns the\nanswer of a single reasoning chain, that was generated with greedy decoding.\n\u2022 SC: Self-Consistency serves as a baseline which incorporates multiple reasoning chains (Wang et al., 2023). It returns the majority answer based on multiple chains sampled from the decomposition model. We experiment with variants of 3, 5 and 15 sampled chains (SC@3, SC@5 and SC@15), in line with prior work (Wang et al., 2023; Khattab et al., 2022; Sun et al., 2023). As in MCR, we use the chain generated with greedy decoding along with additional chains sampled with t = 0.7.\nRetrieval Similar to Press et al. (2022); Lazaridou et al. (2023); Paranjape et al. (2023), our models and baselines use a retriever based on Google Search, via the SerpAPI service.4 However, we also include results using an open-source retriever (Khattab and Zaharia, 2020) in \u00a74.3. As most of our datasets contain evidence from Wikipedia (\u00a74.1.1), we consider it as our retrieval corpus. Therefore, we format search queries as \u201cen.wikipedia.org qi\u201d, with the Wikipedia domain preceding the intermediate question. We return the top-1 evidence retrieved by Google. Retrieved evidence may be either sentences or parsed lists. Following Trivedi et al. (2022a), we also retrieve evidence for the original question q. Last, all retrieved evidence sentences are prepended to the decomposition (\u00a73.1).\n3Like Wang et al. (2023), we observe that greedy-decoded chains have higher accuracy compared to the other chains.\n4https://serpapi.com/\nAdditional implementation details about our retrieval and MCR are described in \u00a7B.1 and \u00a7B.2."
        },
        {
            "heading": "4.2 Main Results",
            "text": "Next, we report our evaluation results. Overall, MCR outperforms our baselines on all 7 datasets.\nMCR Performance Tab. 2 presents the results for all 7 multi-hop datasets (evaluation described in \u00a74.1.1). We evaluate both SC@5 and MCR using five reasoning chains. In addition, we list an oracle score which uses the best answer out of all five chains. MCR outperforms all baselines on all of the benchmarks, beating SC@5 on STRATEGYQA (+1.4%), FERMI (+0.6%), QUARTZ (+4.0%), HOTPOTQA (+5.7%), 2WIKIMQA (+2.5%), BAMBOOGLE (+1.5%) and FEVEROUS (+1.5%).\nAdding Reasoning Chains We measure the gains of MCR and SC when adding reasoning chains. As extending MCR is bounded by context length,5 we follow a straightforward approach and perform self-consistency on three MCR runs. We compare this model, MCR+SC@3, which used 15 reasoning chains (5 for each MCR run), to SC@15. Tab. 3 shows that MCR+SC@3 consistently outperforms SC@15. Furthermore, though MCR uses only 5 reasoning chains, it beats SC@15 on all datasets, save STRATEGYQA. Fig. 5 plots, for each dataset, the effect that adding more reasoning chains has on meta-reasoning performance. It presents the results with 1 chain (SCR), 5 chains (MCR) and 15 reasoning chains (MCR+SC@3).\nTest Set Results We evaluate our models on the official test sets of STRATEGYQA6 and FERMI, which include 490 and 558 examples respectively. The results in Tab. 4 show that on STRATEGYQA MCR consistently beats SC, when using the same\n5code-davinci-002 context is capped at 8,001 tokens. 6https://leaderboard.allenai.org/strategyqa\nnumber of reasoning chains. In FERMI, both methods perform similarly.\nRecent Approaches Previously, we established the advantages of meta-reasoning over multiple reasoning chains. While an apples-to-apples comparison with other recent approaches is impossible due to fundamental differences in the experimental setup (see \u00a7B.3), it serves as a rough measuring stick for the robustness of MCR across different tasks. In \u00a7B.3, Tab. 8 we compare MCR to five recent CoT-based approaches for multi-hop QA. MCR performance is comparable with the best results on all datasets (shared between these works), showcasing its robustness."
        },
        {
            "heading": "4.3 Open-source Models",
            "text": "To further examine MCR\u2019s performance (\u00a74.2) and for better reproducibility, we experiment with an additional open-source retriever and LLM. As our retriever, we use ColBERTv2 (Santhanam et al., 2022) over the 2018 Wikipedia dump from Karpukhin et al. (2020). In addition to code-davinci-002, we experiment with Vicuna-13B (Chiang et al., 2023), a 13-billion parameters model shown to outperform LLMs like LLaMA and Alpaca (Touvron et al., 2023; Taori et al., 2023). We use the same prompts as in code-davinci-002, trimmed to fit a 2,048 tokens context length.\nWe report the full results of the open-source ColBERTv2 retriever with code-davinci-002 and Vicuna-13B in Tab. 5. In addition, we provide results of open-source models when reasoning over 15 reasoning chains in Tab. 6. For code-davinci-002, substituting Google Search with ColBERTv2 exhibits the same trend as in Tab. 2, albeit a slight decrease in performance. MCR outperforms all other baselines, beating SC@5 on STRATEGYQA (+2.3%), FERMI (+3.4%), QUARTZ (+3.9%), HOTPOTQA (+3.5%), 2WIKIMQA (+1.2%), BAMBOOGLE (+3.6%) and FEVEROUS (+1.4%). Unsurprisingly, results sharply decrease when evaluating the smaller Vicuna-13B with ColBERTv2. The comparison between MCR and SCR suggests that reasoning over multiple chains is a challenge for the weaker Vicuna-13B model. For example, it generates open-ended answers such as \u201cUnknown\u201d or \u201cIt depends\u201d for over 24% of the questions in STRATEGYQA. This suggests that meta-reasoning over multiple chains has greater gains (compared to SCR) when both the decomposition model and meta-reasoner are larger LLMs.\nHowever, even on Vicuna-13B, MCR still outperforms all baselines on 5 datasets and beats SC@5 on all 7 of them: STRATEGYQA (+0.5%), FERMI (+4.6%), QUARTZ (+3.6%), HOTPOTQA (+6.5%), 2WIKIMQA (+0.3%), BAMBOOGLE\n(+3.0%) and FEVEROUS (+1.3%). When evaluating with 15 reasoning chains, in Tab. 6, MCR+SC@3 continually beats SC@15."
        },
        {
            "heading": "5 Analysis",
            "text": "Next, we measure the importance of incorporating multiple reasoning chains in MCR and qualitatively assess its output."
        },
        {
            "heading": "When are Multiple Chains Helpful? In \u00a74.2",
            "text": "we observed that MCR consistently outperforms single-chain reasoning (SCR). We wish to prove that this advantage lies in cases where the metareasoner uses additional chains. To this end, we sort examples based on the similarity of their greedydecoded chain to the MCR explanation (details in \u00a7C.1). Lower similarity indicates less reliance of MCR on the greedy chain. Fig. 6 presents an example where the MCR explanation (pink box) includes relevant facts from a chain other than the greedy one (additional examples in \u00a7C.2). Results in Fig. 7 empirically demonstrate that on STRATEGYQA, MCR gains over SCR are highest when MCR explanations are less similar to the greedy chain. We observe this trend in all datasets (\u00a7C.1), serving as further evidence for MCR\u2019s strengths.\nCombining Reasoning Chains In addition to choosing between reasoning chains, an interesting property of the meta-reasoner is that it can combine facts from different chains. We estimate the prevalence of this phenomenon on the implicit datasets, STRATEGYQA and FERMI, which are more challenging. Given an example, we automati-\ncally check if its meta-reasoner explanation is the result of combining chains. We examine if one of the output sentences appears in exactly one chain, while another sentence is absent from that chain and is part of a different chain. We consider sentences as similar if their ROUGE-1 precision is above 0.8, and distinct if it is below 0.2. Overall, in 20% of STRATEGYQA examples and 25% of FERMI, the MCR explanation results from combining reasoning chains. From a manual analysis of 50 such examples for each dataset, we observe that these multi-chain explanations are better than any individual reasoning chain in 10% of cases\n(see examples in \u00a7C.2, Fig. 10). For the remaining 90%, the reasoning expressed in the resulting combination is a paraphrase of an individual chain.\nExplanation Quality The meta-reasoner is prompted to generate an explanation alongside the final answer (\u00a73.2). Inspired by past work (Pruthi et al., 2022), we test the quality of the MCR explanations. Four of the authors manually reviewed 600 random examples, 100 per dataset (sans FEVEROUS \u00a7B.2) and scored their meta-reasoner explanations. Each explanation is scored as either 1 (irrelevant), 2 (partially relevant) or 3 (highly relevant), based on its relevance to answering the question. We find the explanation is highly relevant in 82% of the cases (87% excluding FERMI, which is the most challenging), and is irrelevant in less than 3%.\nNext, we evaluate the faithfulness of explanations (Jacovi and Goldberg, 2020), namely, whether a person provided only with the question and MCR explanation would answer the same as the model. Our focus was on examples with quality explanations (score 3), since they are answerable given the explanation. We answered each question based on\nthe model\u2019s explanation. In 90% of cases (95% excluding FERMI), the MCR predictions matched our own, highlighting the faithfulness of its explanations. We attribute part of the gap between human and MCR predictions to implicit reasoning tasks, where humans lead by five points, on average. For the full results, see \u00a7C.3.\nError Analysis We manually analyzed 700 errors by MCR (100 per dataset). We consider the following categories: Valid predictions where the generated answer is accurate or the original question is ambiguous; Decomposition errors where no chain has the necessary reasoning steps to answer the question; Retrieval errors where the retrieved contexts were irrelevant, leading the model to hallucinate; Explanation errors where MCR generates a wrong explanation while a correct one is present in the multi-chain context; Answer errors are when the MCR explanation is correct, but the answer is not; Contradicting facts are cases where MCR errs due to contrasting statements appearing in the multi-chain context.\nTab. 7 lists the prevalence of the error categories per dataset. In four datasets, over 20% of errors appear to be valid predictions, labeled as incorrect due to ambiguous questions, outdated answers or dataset errors. Decomposition is a challenge in the implicit datasets, STRATEGYQA and FERMI, with more than 24% of errors. Comparing errors on different reasoning datasets (excluding valid examples): Explanation and Answer errors are 50% on implicit reasoning datasets compared to 23% on explicit reasoning ones; Retrieval errors are more prevalent in explicit reasoning tasks with 66% of errors being due to Retrieval or Contradicting facts, compared to 30% in implicit datasets. Additional technical details on our analysis are in \u00a7C.4."
        },
        {
            "heading": "6 Related Work",
            "text": "For a thorough survey on LLM reasoning see Lu et al. (2022); Huang and Chang (2022); Qiao et al. (2022). A slew of recent works have focused on eliciting multi-step reasoning in LLMs, including scratchpads (Nye et al., 2022), chain-of-thought prompting (Wei et al., 2022; Zhou et al., 2022), learned verifiers (Cobbe et al., 2021), selectioninference (Creswell et al., 2022) and bootstrapping (Zelikman et al., 2022).\nSelf-consistency (Wang et al., 2023; Fu et al., 2022) selects the majority answer across multiple chains, outperforming learned verifiers and \u201csample-and-rank\u201d approaches (Adiwardana et al., 2020; Freitas et al., 2020). Li et al. (2022) further improve SC by increasing chains\u2019 diversity and introducing a trained verifier. Tafjord et al. (2022) over-samples chains and verifies them using a natural language inference model on intermediate steps, while He et al. (2022) re-rank chains based on intermediate retrieved evidence. In addition, metareasoning is closely tied to self-reflection in LLMs, which is becoming increasingly important in using the LLM to review multiple strategies (Yao et al., 2023; Shinn et al., 2023; Madaan et al., 2023).\nRecent works proposed revising LLM-generated texts by using retrieved sentences (Gao et al., 2022) or model-generated feedback (Madaan et al., 2023; Chen et al., 2023; Paul et al., 2023). MCR similarly reviews LLM-generated reasoning chains however, its focus is meta-reasoning on multiple chains.\nSignificant QA research has been dedicated to reasoning over multiple facts retrieved from an underlying corpus. Such tasks include multi-step questions that require explicit reasoning (Talmor and Berant, 2018; Welbl et al., 2018; Wolfson et al., 2020; Trivedi et al., 2022b), implicit reasoning (Geva et al., 2021) and multi-modal capabilities (Talmor et al., 2021).\nRecent works also target retrieval-augmented LLMs, prompted to solve open-domain questions (Lazaridou et al., 2023; Khattab et al., 2022; Trivedi et al., 2022a; Ram et al., 2023; Yoran et al., 2023)."
        },
        {
            "heading": "7 Conclusion",
            "text": "This work introduces MCR for meta-reasoning over multiple reasoning chains. We evaluate MCR on 7 datasets for multi-hop QA that require both implicit and explicit reasoning in an open-domain setting and show that it outperforms previous approaches on all evaluation benchmarks."
        },
        {
            "heading": "8 Limitations",
            "text": "In this work we introduce a meta-reasoner model to reason over multiple reasoning chains. While we opt for a prompted LLM as our meta-reasoner, we do not experiment with a fine-tuned meta-reasoning model. For the meta-reasoner context, we experiment with variants which include either generated QA pairs or retrieved evidence sentences. We leave further improvements to the meta-reasoner context as future work. Due to the inference costs of current state-of-the-art LLMs we evaluate on the code-davinci-002 model, similar to prior work (Trivedi et al., 2022a; Wang et al., 2023). However, to improve the reproducibility of our work we also provide results with an open-source LLM (Chiang et al., 2023) and retriever (Khattab and Zaharia, 2020)."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank Harsh Trivedi, Ofir Press, Mor Geva, Peter Clark and Ashish Sabharwal for their feedback and insightful comments. We thank SerpAPI for their support by granting us an academic discount. This research was partially supported by the Yandex Initiative for Machine Learning and the European Research Council (ERC) under the European Union Horizons 2020 research and innovation programme (grant ERC DELPHI 802800). This work was completed in partial fulfillment of the Ph.D. of Ori Yoran and the Ph.D. of Tomer Wolfson."
        },
        {
            "heading": "A Evaluation",
            "text": ""
        },
        {
            "heading": "A.1 Generating Unknown as the Answer",
            "text": "As we prompt LLMs to generate answers, a potential outcome is for the model to abstain from answering the question, by generating Unknown as its answer. Additional cases are when the model generates an end-of-sequence token without any final answer. In the binary-choice datasets, STRATEGYQA, QUARTZ and FEVEROUS, we assign a score of 0.5 to such examples, thereby simulating a random guess. When submitting predictions to the STRATEGYQA test set, we identify cases of model abstains or null predictions beforehand. For these examples, we assign a label of either Yes or No at random. In datasets with open-ended answers, we assign a score of 0 when the predicted answer is either Unknown or null. To make Self-Ask a stronger baseline, when the greedy decoded chain has a null answer, we randomly choose a prediction from one of the other chains. For SC, we do not consider predictions from chains where answers are Unknown or null."
        },
        {
            "heading": "A.2 FERMI",
            "text": "The FERMI dataset requires approximating numeric answers for open-ended questions. Example questions are shown in Tab. 1 and Fig. 2. When providing a FERMI question to our models and baselines we also add the gold answers measure units (e.g. meters, cubes, litres, etc.). While this additional\ninput helps the model, we note that we provide it to all our baselines for a fair comparison with MCR. Nevertheless, even when given the gold units, predicting the final answers to FERMI problems remains highly challenging."
        },
        {
            "heading": "B Models",
            "text": ""
        },
        {
            "heading": "B.1 Retrieval",
            "text": "For our retrieval, we use the Google Search Engine, via SerpAPI, and return the top-1 retrieved result as an evidence snippet. Snippets can include answerboxes and tables.7 We prepend the page title to the beginning of the snippet, as shown in Fig. 8.\nB.2 Implementation Details\nWe describe the design choices made in our MCR model, such as preforming retrieval on the original question and a variant of the meta-reasoner prompt for FEVEROUS. Due to cost limitations, we evaluate our design choices at a smaller scale and avoid running an exhaustive grid search.\nRetrieving the Original Question We follow past work (Trivedi et al., 2022a) by incorporating retrieved evidence for the original question in addition to evidence retrieved for the intermediate steps (\u00a73.1). This has a positive or negligible effect on most datasets however, it dramatically decreases the results of all models on the FERMI task. Results drop for SA (38.3\u00b10.7 to 34.7\u00b10.5), SC (38.3\u00b10.8 to 34.4\u00b10.3), SCR (38.1\u00b10.8 to 34.4\u00b10.8) and MCR (38.9\u00b10.8 to\n7https://serpapi.com/organic-results\n37.0\u00b10.7). Therefore, our models are run without original question retrieval when evaluated on FERMI. Interestingly, while all models perform roughly the same without original question retrieval, MCR appears better by 2 points when evidence for the original question is used. We hypothesize that it might be due to MCR being somewhat more robust to the addition of irrelevant evidence.\nFEVEROUS Meta-Reasoner Prompt As described in \u00a73.2, the meta-reasoner generates an explanation which precedes the final answer. FEVEROUS is distinct from all other datasets as it require verification of multiple facts in order to verify or disprove a complex statement. When a statement is false, we list one or more of its false intermediate facts along with its correction. For example, in Fig. 4 we list that Robert Broderip lived in Bristol, not London. When prompting the meta-reasoner to list both true and false intermediate facts, we observed a decrease in performance for both MCR (69.4\u00b11.0 to 66.4\u00b10.7) and SCR (65.1\u00b10.4 to 62.9\u00b10.3). We hypothesize that repeating multiple true facts excessively prompts the model to predict the label \u201cYes\u201d in cases where most (but not all) of the intermediate facts are correct."
        },
        {
            "heading": "B.3 Empirical Comparison to Recent Approaches",
            "text": "In Tab. 8, we compare MCR to recent CoT-based approaches for multi-hop reasoning. An apples-toapples comparison is not possible, as these methods do not evaluate on all 7 of our datasets and use varying samples of 500-1,000 dev examples for their evaluation. Moreover, different methods use different retrieval corpora, hyperparameters, prompts and LLMs. Nevertheless, we argue that a direct comparison serves as a measuring stick for MCR\u2019s robustness across multiple datasets, compared to\nsimilar solutions.\nEvaluation differences include the retrieval corpora, as both IR-CoT and DSP use the official Wikipedia dump provided with the HOTPOTQA dataset (Yang et al., 2018). Our retrieved evidence are from an updated version of Wikipedia, via Google Search. Since certain facts may change over time, this could potentially explain the high percentage of MCR predictions labeled as valid in our error analysis (\u00a75).\nWe emphasize that our focus is on highlighting the potential of reasoning on reasoning chains. MCR is a method aimed at improving models which generate reasoning chains. Compared to SC, we observe that MCR further boosts the underlying SA model. While task-specific improvements are possible, they are orthogonal to our work."
        },
        {
            "heading": "B.4 Reasoning on Retrieved Evidence",
            "text": "The meta-reasoner answers questions given a multichain context of question-answer (qi, ai) pairs, extracted from multiple reasoning chains (\u00a73.2). We experiment with an alternative multi-chain context, comprised of questions and retrieved evidence (qi, ei) (\u00a73.1). This setting resembles past work (Trivedi et al., 2022a) however, our sentences are intermediate evidence from multiple reasoning chains, not just the greedy-decoded chain. We compare these variants, MCR-EV and SCR-EV, to MCR and SCR that reason on QA pairs. Tab. 9 shows that meta-reasoning on retrieved evidence is less effective. The gap is more evident in implicit reasoning tasks, perhaps due to retrieved evidence being less relevant on average. Example prompts for MCR-EV and SCR-EV are listed in \u00a7D."
        },
        {
            "heading": "C Analysis",
            "text": ""
        },
        {
            "heading": "C.1 When are Multiple Chains Helpful?",
            "text": "In \u00a75, we have shown that the advantage of MCR over SCR lies in examples where the metareasoner uses chains other than the one generated through greedy decoding. In Fig. 9 we provide the results for all other datasets, in addition to the STRATEGYQA results in Fig. 7. The similar trend among all datasets is that in examples with lower similarity to the greedy chain, MCR gains over SCR are higher.\nThe similarity between the meta-reasoner explanation and the greedy decoded reasoning chain is defined as follows: We calculate the ROUGE-1precision (Lin, 2004) between the explanation and the chain. Low, Medium, and High are based on thresholds of 13 , 2 3 , and 1 respectively, with the Identical category indicating an exact match."
        },
        {
            "heading": "C.2 Combining Reasoning Chains",
            "text": "Fig. 10 provides additional examples for combining facts between multiple reasoning chains."
        },
        {
            "heading": "C.3 Explanation Quality Analysis",
            "text": "We provide additional details on the annotation for the scoring meta-reasoner explanations. The annotation was performed by 4 graduate students that are authors of this paper. The annotators were presented with a question and an explanation, and asked to perform two tasks: (a) score the explanation for its quality and (b) answer the question based on the meta-reasoner explanation. We provide the full instructions shown to the annotators in Fig. 11 and the full results in Tab. 10."
        },
        {
            "heading": "C.4 Error Analysis",
            "text": "We provide additional details regarding our error analysis (\u00a75). In less than 5%, we encountered grammatically bad questions which we were unable to comprehend and were therefor discarded from our analysis. For example the HOTPOTQA\nquestion: \u201cWhat does the goddess associated with the goddess Frigg consists of what tales?\u201d\nThe input to our meta-reasoner model is a context comprised of (qi, ai) pairs, generated by the decomposition model. As the decomposition model is an LLM that is conditioned on retrieved evidence (and prior decomposition steps) it may hallucinate false intermediate answers. In cases of such hallucinations we distinguish between two error types, based on the relevant component. First, Retrieval errors are cases where no relevant information was retrieved, leading to the decomposition model hallucinating an incorrect ai, passed on to the metareasoner\u2019s context. Second, we treat cases where relevant evidence was retrieved, but the decomposition model ignored it and hallucinated an incorrect ai as Decomposition errors.\nErrors stemming from Contradicting Facts, are cases where the meta-reasoner context contains two contradicting facts, one accurate while the other was hallucinated by the decomposition model. For example, Fig. 12 displays an example where the context has contradicting facts on who was the father of Eliezer Ben-Yehuda. When the metareasoner has contradicting facts, it is expected to select the correct fact, based on the knowledge encoded in its parameters. Addressing such errors in future work could rely on refining generated text with methods such as RARR (Gao et al., 2022).\nAs our error classes mainly match the MCR components, this error breakdown could potentially help to guide future improvements."
        },
        {
            "heading": "D Prompts",
            "text": ""
        },
        {
            "heading": "D.1 Prompt Details",
            "text": "We provide example prompts for our models for one explicit dataset (2WIKIMQA, decomposition: Fig. 13, MCR/SCR: Fig. 15, MCR-EV/SCR-EV: Fig. 17) and one implicit dataset (STRATEGYQA, decomposition: Fig. 14, MCR/SCR: Fig. 16, MCR-EV/SCR-EV:Fig. 18). All of our prompts\nwill be released along with our codebase. We use random examples and spend minimal effort on prompt engineering. The number of exemplars varies slightly between dataset and model, with the exact numbers listed in Tab. 11."
        },
        {
            "heading": "D.2 Prompt Statistics",
            "text": "In Tab. 12 we provide statistics of the sequence lengths for all of our models, which include all the decomposition prompts, output decomposition sequences, retrieved evidence and the meta-reasoning prompts. The statistics are for our decomposition model (used by all of our baselines), as well as for the meta-reasoning prompts (used by SCR and MCR). Note that generating a single reasoning chain requires multiple LLM calls, one for each decomposition step. Therefore, a single decomposition generation is generally longer than applying one additional meta-reasoning step.\nResults are averaged over multiple runs, corresponding to the results in Tab. 2. Sequence lengths in Tab. 12 correspond to the number of tokens provided by the code-davinci-002 tokenizer."
        },
        {
            "heading": "D.3 Robustness to Choice of Prompt",
            "text": "We empirically measure our method\u2019s sensitivity to the prompt of choice. To this end, we randomly sampled new exemplars for both our decomposition and meta-reasoning prompts for STRATE-\nGYQA and HOTPOTQA. When using different random exemplars, we observe that MCR still outperforms all baselines. Even though decomposition performance (SA) is more affected by the set of exemplars, the performance trend remains the same, with MCR being on top. Tab. 13 lists the experiment results, evaluated on 500 examples from each dataset. We also provide the original prompt results in parenthesis (averaged over 3 runs).\nGiven a question and a context, answer the question stepby-step. If you are unsure, answer Unknown."
        },
        {
            "heading": "Context:",
            "text": "Who is the father of modern Hebrew? The father of modern Hebrew is Eliezer Ben-Yehuda. Who is the father of Eliezer Ben-Yehuda? The father of Eliezer Ben-Yehuda is Abraham. ... Who is the father of modern Hebrew? The father of modern Hebrew is Eliezer Ben-Yehuda. Who is the father of Eliezer Ben-Yehuda? Eliezer BenYehuda\u2019s father is Yehuda Leib.\nQuestion: Who is the father of the father of modern Hebrew?\nAnswer: The father of modern Hebrew is Eliezer BenYehuda. The father of Eliezer Ben-Yehuda is Abraham. So the answer is: Abraham.\nGold answer is: Yehuda Leib\nFigure 12: Example a Contradicting Facts error. When generating the explanation, the meta-reasoner has to rely on knowledge encoded in its parameters to decide between multiple contradicting facts in its context on who was the father of Eliezer Ben-Yehuda.\nGiven the following question, answer it by providing follow up questions and intermediate answers. If no follow up questions are necessary, answer the question directly. You are also provided with the most relevant google snippet for each intermediate question.\n# Context1: Xawery Z\u0307u\u0142awski: Polish-Russian War (Wojna polsko-ruska) is a 2009 Polish film directed by Xawery Z\u0307u\u0142awski based on the novel Polish-Russian War under the white-red flag by Dorota Mas\u0142owska. So the answer is Xawery Z\u0307u\u0142awski. Context2: Xawery Z\u0307u\u0142awski: Xawery Z\u0307u\u0142awski ; National Film School in \u0141\u00f3dz\u0301 \u00b7 1995\u2013present \u00b7 Maria Strzelecka \u00b7 2. Question: Who is the mother of the director of film Polish-Russian War (Film)? Are follow up questions needed here: Yes. Follow up: Who is the director of the film Polish-Russian War (Film)? Intermediate answer: The director of the film Polish-Russian War is Xawery Z\u0307u\u0142awski. Follow up: Who is the mother of Xawery Z\u0307u\u0142awski? Intermediate answer: The mother of Xawery Z\u0307u\u0142awski is Ma\u0142gorzata Braunek. So the final answer is: Rick Scott Ma\u0142gorzata Braunek. ... # Context1: John, Count Palatine of Neumarkt: John (Johann von Pfalz-Neumarkt; 1383 \u2013 14 March 1443) was the Count Palatine of Neumarkt from 1410 to his death. The son of Rupert III of the Palatinate, he married Catherine of Pomerania in 1407. Context2: John, Count Palatine of Neumarkt: John (Johann von Pfalz-Neumarkt; 1383 \u2013 14 March 1443) was the Count Palatine of Neumarkt from 1410 to his death. The son of Rupert III of the Palatinate, he married Catherine of Pomerania in 1407. Question: Who is Catherine Of Pomerania, Countess Palatine Of Neumarkt\u2019s father-in-law? Are follow up questions needed here: Yes. Follow up: Who is the husband of Catherine of Pomerania, Countess Palatine of Neumarkt? Intermediate answer: The husband of Catherine of Pomerania, Countess Palatine of Neumarkt is John, Count Palatine of Neumarkt. Follow up: Who is the father of John, Count Palatine of Neumarkt? Intermediate answer: The father of John, Count Palatine of Neumarkt is Rupert III of the Palatinate. So the final answer is: Rupert III of the Palatinate. # Context1: Crimen a las tres: Crimen a las tres is a 1935 Argentine crime film directed and written by Luis Saslavsky. Crimen a las tres. Directed by, Luis Saslavsky. Context2: Elio Petri: The Working Class Goes to Heaven (Italian: La classe operaia va in paradiso), released in the US as Lulu the Tool, is a 1971 political drama film directed by Elio Petri. So the answer is Elio Petri. Context3: March 20, 1995: Luis Saslavsky (April 21, 1903 \u2013 March 20, 1995) was an Argentine film director, screenwriter and film producer, and one of the influential directors in the Cinema of Argentina of the classic era. So the answer is March 20, 1995. Context4: Elio Petri: Final years. In 1981, Petri visited Geneva to direct Arthur Millers\u0301 new play The American Clock, with Marcello Mastroianni playing the lead role. Petri died of cancer on 10 November 1982. He was 53 years old. Question: Which film has the director died first, Crimen A Las Tres or The Working Class Goes To Heaven? Are follow up questions needed here: Yes. Follow up: Who is the director of Crimen a las tres? Intermediate answer: The director of Crimen a las tres is Luis Saslavsky. Follow up: Who is the director of The Working Class Goes to Heaven? Intermediate answer: The director of The Working Class Goes to Heaven is Elio Petri. Follow up: When did Luis Saslavsky die? Intermediate answer: Luis Saslavsky died on March 20, 1995. Follow up: When did Elio Petri die? Intermediate answer: Elio Petri died on 10 November 1982. So the final answer is: The Working Class Goes to Heaven. #\nFigure 13: Instruction and exemplars for the 2WIKIMQA decomposition prompt.\nGiven the following question, answer it by providing follow up questions and intermediate answers. For each follow up question, you are given a context which is the top returned google snippet for the question from Wikipedia. If no follow up questions are necessary, answer the question directly.\n# Context1: Frost: Frost is a thin layer of ice on a solid surface, which forms from water vapor in an above-freezing atmosphere coming in contact with a solid surface whose ... Context2: Graduation: Graduation is the awarding of a diploma to a student by an educational institution. It may also refer to the ceremony that is associated with it. Context3: Winter: Winter ; Astronomical season, 22 December \u2013 21 March ; Meteorological season, 1 December \u2013 28/29 February ; Solar (Celtic) season, 1 November \u2013 31 January. Question: Is it common to see frost during some college commencements? Are follow up questions needed here: Yes. Follow up: What seasons can you expect to see frost? Intermediate answer: Frost is common during the winter. Follow up: When is college commencement? Intermediate answer: College commencement ceremonies often happen during the months of December, May, June. Follow up: Do any of the months December, May, June occur during the Winter? Intermediate answer: December is in the winter. So the final answer is: Yes. ... # Context1: Last rites: The last rites, also known as the Commendation of the Dying, are the last prayers and ministrations given to an individual of Christian faith, when possible, shortly before death. They may be administered to those awaiting execution, mortally injured, or terminally ill. Context2: Richard Dawkins: Dawkins is an outspoken atheist and a supporter of various atheist, secular, and humanist organisations, including Humanists UK and the Brights movement. Dawkins suggests that atheists should be proud, not apologetic, stressing that atheism is evidence of a healthy, independent mind. Context3: Prayer in the Catholic Church: In the Catholic Church, prayer is \"the raising of ones\u0301 mind and heart to God or the requesting of good things from God.\" It is an act of the moral virtue ... Question: Would Richard Dawkins hypothetically refuse an offering of the Last rites? Are follow up questions needed here: Yes. Follow up: What are the last Rites? Intermediate answer: The Last rites, in Catholicism, are the last prayers and ministrations given to an individual of the faith, when possible, shortly before death. Follow up: What are Richard Dawkins religious beliefs? Intermediate answer: Richard Dawkins is known as an outspoken atheist, well known for his criticism of creationism and intelligent design. Follow up: Would an atheist participate in Catholics prayers? Intermediate answer: It is unlikely that an atheist would participate in Catholics prayers. So the final answer is: Yes. # Context1: number 1: Hydrogen is the chemical element with the symbol H and atomic number 1. Hydrogen is the lightest element. So the answer is number 1. Context2: Spice Girls - Simple English Wikipedia, the free encyclopedia: The group has five members. Each member uses a nickname initially given to them: Melanie Chisholm (\"Sporty Spice\"), Emma Bunton (\"Baby Spice\"), Melanie Brown (\"Scary Spice\"), Victoria Beckham (n\u00e9e Adams) (\"Posh Spice\"), and Geri Halliwell (\"Ginger Spice\") . Context3: Square number: In mathematics, a square number or perfect square is an integer that is the square of an integer; in other words, it is the product of some integer with ... Question: Hydrogen\u2019s atomic number squared exceeds number of Spice Girls? Are follow up questions needed here: Yes. Follow up: What is the atomic number of hydrogen? Intermediate answer: Hydrogen has an atomic number of 1. Follow up: How many people are in the Spice Girls band? Intermediate answer: The Spice Girls has 5 members. Follow up: Is the square of 1 greater than 5? Intermediate answer: The square of 1 is 1 which is less than 5. So the final answer is: No. #\nFigure 14: Instruction and exemplars for the STRATEGYQA decomposition prompt.\nGiven a question and a context, answer the question and explain why. If you are unsure, answer Unknown.\n# Context: Who is the wife of Douglas Leiterman? The wife of Douglas Leiterman is Beryl Fox. Where was Beryl Fox born? Beryl Fox was born in Winnipeg, Manitoba. When and where was Mary born? Mary was born in c. 18 BC or September 8 (21), 16 BC Herodian Kingdom of Judea. Where was Beryl Fox born? Beryl Fox was born in Winnipeg, Manitoba. Who is the wife of Douglas Leiterman? The wife of Douglas Leiterman is Mary. Who is the wife of Douglas Leiterman? The wife of Douglas Leiterman is Beryl Fox.\nQuestion: Where was the wife of Douglas Leiterman born?\nAnswer: The wife of Douglas Leiterman is Beryl Fox. Beryl Fox was born in Winnipeg, Manitoba. So the answer is: Winnipeg, Manitoba. ... # Context: Who is Beatrice of Aragon\u2019s father? The father of Beatrice of Aragon is King Ferdinand I of Naples. Who is the father of Rupert III, Elector Palatine? The father of Rupert III, Elector Palatine is Rupert II, Elector Palatine. Who is the husband of Catherine of Pomerania? The husband of Catherine of Pomerania is John II, Count Palatine of Neumarkt. Who is Catherine Of Pomerania, Countess Palatine Of Neumarkt\u2019s husband? The husband of Catherine Of Pomerania, Countess Palatine Of Neumarkt is John I, Count Palatine of Neumarkt. Who is the father of John II, Count of Holstein-Rendsburg? The father of John II, Count of Holstein-Rendsburg is Henry II, Count of Holstein-Rendsburg. Who is Catherine Of Pomerania, Countess Palatine Of Neumarkt\u2019s husband? The husband of Catherine Of Pomerania, Countess Palatine Of Neumarkt is John II, Count of Holstein-Rendsburg. Who is the father of John I, Count Palatine of Neumarkt? The father of John I, Count Palatine of Neumarkt is Rupert III, Elector Palatine. Who are the parents of Rupert III, Elector Palatine? The parents of Rupert III, Elector Palatine are Rupert II, Elector Palatine and Beatrice of Aragon. Who is the father of John II, Count Palatine of Neumarkt? The father of John II, Count Palatine of Neumarkt is Rupert III, Elector Palatine.\nQuestion: Who is Catherine Of Pomerania, Countess Palatine Of Neumarkt\u2019s father-in-law?\nAnswer: The husband of Catherine Of Pomerania, Countess Palatine Of Neumarkt is John I, Count Palatine of Neumarkt. The father of John I, Count Palatine of Neumarkt is Rupert III, Elector Palatine. So the answer is: Rupert III, Elector Palatine. # Context: When did Elio Petri die? Elio Petri died on 10 November 1982. Who is the director of The Working Class Goes to Heaven? The director of The Working Class Goes to Heaven is Elio Petri. Who is the director of Crimen A Las Tres? The director of Crimen A Las Tres is Luis Saslavsky. Who is the director of Crimen A Las Tres? The director of Crimen A Las Tres is Luis Saslavsky. When did Luis Saslavsky die? Luis Saslavsky died on March 20, 1995. Who is the director of Crimen A Las Tres? The director of Crimen A Las Tres is Luis Saslavsky. When did Elio Petri die? Elio Petri died on 10 November 1982. When did Luis Saslavsky die? Luis Saslavsky died on March 20, 1995. When did Luis Saslavsky die? Luis Saslavsky died on March 20, 1995. When did Elio Petri die? Elio Petri died on 10 November 1982. Who is the director of The Working Class Goes to Heaven? The director of The Working Class Goes to Heaven is Elio Petri. Who is the director of The Working Class Goes to Heaven? The director of The Working Class Goes to Heaven is Elio Petri.\nQuestion: Which film has the director died first, Crimen A Las Tres or The Working Class Goes To Heaven?\nAnswer: The director of Crimen A Las Tres is Luis Saslavsky. The director of The Working Class Goes to Heaven is Elio Petri. Luis Saslavsky died on March 20, 1995. Elio Petri died on 10 November 1982. So the answer is: The Working Class Goes To Heaven. #\nFigure 15: Instruction and exemplars for the 2WIKIMQA meta-reasoner prompt.\nGiven a question and a context, provide a Yes or No answer and explain why. If you are unsure, answer Unknown.\n# Context: What type of animal is a jellyfish? A jellyfish is a type of invertebrate. Do jellyfish have arteries? No, jellyfish do not have arteries. They have a type of nervous system called a nerve net. What is atherosclerosis? Atherosclerosis is a disease where plaque builds up in the arteries. Do jellyfish have arteries? Jellyfish do not have a circulatory system and therefore do not have arteries. So jellyfish don\u2019t have atherosclerosis because they don\u2019t have arteries? Yes, that is correct. What is atherosclerosis? Atherosclerosis is a type of heart disease. Is an invertebrate susceptible to atherosclerosis? No, invertebrates are not susceptible to atherosclerosis. What is atherosclerosis? Atherosclerosis is a disease in which plaque builds up on the walls of the arteries.\nQuestion: Is a jellyfish safe from atherosclerosis?\nAnswer: Jellyfish do not have a circulatory system and therefore do not have arteries. Atherosclerosis is a disease in which plaque builds up on the walls of the arteries. So the answer is: Yes. # ... Context: What is tonsure? Tonsure is the practice of cutting or shaving the hair from the scalp, as a sign of religious devotion or humility. What is Christopher Hitchens\u2019 religious affiliation? Christopher Hitchens was an atheist. What is tonsure? Tonsure is a practice of cutting or shaving the hair from the scalp. Would Christopher Hitchens be likely to do something as a sign of religious devotion? No. Would Christopher Hitchens be religious? Christopher Hitchens was an atheist. Is an atheist likely to engage in religious practices? No, an atheist is not likely to engage in religious practices. What is tonsure? Tonsure is the practice of cutting or shaving the hair from the scalp, as a sign of religious devotion or humility. What is Christopher Hitchens\u2019 religious affiliation? Christopher Hitchens was an atheist. Do atheists usually engage in tonsure? No, atheists do not usually engage in tonsure as it is a religious practice.\nQuestion: Would Christopher Hitchens be very unlikely to engage in tonsure?\nAnswer: Tonsure is the practice of cutting or shaving the hair from the scalp, as a sign of religious devotion or humility. Christopher Hitchens was an atheist. An atheist is not likely to engage in religious practices. So the answer is: Yes. # Context: What is the evidence that Neanderthals used arithmetic? There is no direct evidence that Neanderthals used arithmetic, but indirect evidence suggests that they may have had some understanding of basic mathematical concepts. What is the earliest evidence of arithmetic? The earliest evidence of arithmetic dates back to the ancient Sumerians, who lived in what is now modern-day Iraq. Did Neanderthals live before the ancient Sumerians? Yes. What is arithmetic? Arithmetic is the branch of mathematics that deals with the properties and manipulation of numbers. Is there any reason to believe that Neanderthals could not have used arithmetic? There is no reason to believe that Neanderthals could not have used arithmetic. Do we have any evidence that Neanderthals could manipulate numbers? We have no evidence that Neanderthals could manipulate numbers. What is arithmetic? Arithmetic is the branch of mathematics that deals with the properties and manipulation of numbers. What is arithmetic? Arithmetic is the branch of mathematics that deals with the addition, subtraction, multiplication, and division of whole numbers and with the manipulation of fractions and decimals.\nQuestion: Did Neanderthals use arithmetic?\nAnswer: There is no evidence that Neanderthals used arithmetic. The earliest evidence of arithmetic dates back to the ancient Sumerians who lived after the Neanderthals. So the answer is: No. #\nFigure 16: Instruction and exemplars for the STRATEGYQA meta-reasoner prompt.\nGiven a question and a context, answer the question and explain why. If you are unsure, answer Unknown.\n# Context: Who is the wife of Douglas Leiterman? | Douglas Leiterman (1927 \u2013 19 December 2012) was a Canadian television producer.: [[D\u0301ouglas Leiterman]\u0301, [S\u0301pouse,\u0301 B\u0301eryl Fox]\u0301] Where was Beryl Fox born? | Winnipeg, Manitoba: Fox was born in 1931 in Winnipeg, Manitoba. So the answer is Winnipeg, Manitoba. Who is Douglas Leiterman\u2019s wife? | Douglas Leiterman (1927 \u2013 19 December 2012) was a Canadian television producer.: [[D\u0301ouglas Leiterman]\u0301, [K\u0301nown for,\u0301 T\u0301his Hour Has Seven Days]\u0301, [S\u0301pouse,\u0301 B\u0301eryl Fox]\u0301] Where was Beryl Fox born? | Winnipeg, Manitoba: Fox was born in 1931 in Winnipeg, Manitoba. So the answer is Winnipeg, Manitoba. Who is the wife of Douglas Leiterman? | Douglas Leiterman (1927 \u2013 19 December 2012) was a Canadian television producer.: [[D\u0301ouglas Leiterman]\u0301, [S\u0301pouse,\u0301 B\u0301eryl Fox]\u0301] When and where was Mary born? | Mary was born in c. 18 BC or September 8 (21), 16 BC Herodian Kingdom of Judea.\nQuestion: Where was the wife of Douglas Leiterman born?\nAnswer: The wife of Douglas Leiterman is Beryl Fox. Beryl Fox was born in Winnipeg, Manitoba. So the answer is: Winnipeg, Manitoba. ... # Context: Who is the director of Crimen A Las Tres? | Crimen a las tres: Crimen a las tres is a 1935 Argentine crime film directed and written by Luis Saslavsky. Who is the director of The Working Class Goes to Heaven? | The Working Class Goes to Heaven: The Working Class Goes to Heaven (Italian: La classe operaia va in paradiso), released in the US as Lulu the Tool, is a 1971 political drama film directed by Elio Petri. When did Luis Saslavsky die? | Luis Saslavsky: Luis Saslavsky (April 21, 1903 \u2013 March 20, 1995) was an Argentine film director, screenwriter and film producer, and one of the influential directors in the Cinema of Argentina of the classic era. When did Elio Petri die? | Elio Petri: Petri died of cancer on 10 November 1982. He was 53 years old. Who is the director of Crimen A Las Tres? | Crimen a las tres: Crimen a las tres is a 1935 Argentine crime film directed and written by Luis Saslavsky. Who is the director of The Working Class Goes to Heaven? | The Working Class Goes to Heaven: The Working Class Goes to Heaven (Italian: La classe operaia va in paradiso), released in the US as Lulu the Tool, is a 1971 political drama film directed by Elio Petri. When did Luis Saslavsky die? | Luis Saslavsky: Luis Saslavsky (April 21, 1903 \u2013 March 20, 1995) was an Argentine film director, screenwriter and film producer, and one of the influential directors in the Cinema of Argentina of the classic era. When did Elio Petri die? | Elio Petri: Petri died of cancer on 10 November 1982. He was 53 years old. Who is the director of Crimen A Las Tres? | Crimen a las tres: Crimen a las tres is a 1935 Argentine crime film directed and written by Luis Saslavsky. When did Luis Saslavsky die? | Luis Saslavsky: Luis Saslavsky (April 21, 1903 \u2013 March 20, 1995) was an Argentine film director, screenwriter and film producer, and one of the influential directors in the Cinema of Argentina of the classic era. Who is the director of The Working Class Goes to Heaven? | The Working Class Goes to Heaven: The Working Class Goes to Heaven (Italian: La classe operaia va in paradiso), released in the US as Lulu the Tool, is a 1971 political drama film directed by Elio Petri. When did Elio Petri die? | Elio Petri: Petri died of cancer on 10 November 1982. He was 53 years old.\nQuestion: Which film has the director died first, Crimen A Las Tres or The Working Class Goes To Heaven?\nAnswer: The director of Crimen A Las Tres is Luis Saslavsky. The director of The Working Class Goes to Heaven is Elio Petri. Luis Saslavsky died on March 20, 1995. Elio Petri died on 10 November 1982. So the answer is: The Working Class Goes To Heaven. #\nFigure 17: Instruction and exemplars for the 2WIKIMQA meta-reasoner prompt for MCR-EV and SCR-EV reasoning over retrieved evidence.\nGiven a question and a context, answer the question step-by-step. If you are unsure, answer Unknown.\n# Context: What is atherosclerosis? | Atherosclerosis: Atherosclerosis is a pattern of the disease arteriosclerosis in which the wall of the artery develops abnormalities, called lesions. These lesions may lead to narrowing due to the buildup of atheromatous plaque. At onset there are usually no symptoms, but if they develop, symptoms generally begin around middle age. What type of animal is a jellyfish? | Jellyfish - Simple English Wikipedia, the free encyclopedia: Jellyfish are animals of the phylum Cnidaria. They are a monophyletic clade, the Medusozoa. Most of them live in the oceans, in salt water, where they eat small sea animals like plankton and little fish, and float in the sea. Is an invertebrate susceptible to atherosclerosis? | Atherosclerosis: Atherosclerosis is a pattern of the disease arteriosclerosis in which the wall of the artery develops abnormalities, called lesions. What is atherosclerosis? | Atherosclerosis: Atherosclerosis is a pattern of the disease arteriosclerosis in which the wall of the artery develops abnormalities, called lesions. These lesions may lead to narrowing due to the buildup of atheromatous plaque. At onset there are usually no symptoms, but if they develop, symptoms generally begin around middle age. Do jellyfish have arteries? | Jellyfish: Jellyfish are mainly free-swimming marine animals with umbrella-shaped bells and trailing tentacles, although a few are anchored to the seabed by stalks rather What is atherosclerosis? | Atherosclerosis: Atherosclerosis is a pattern of the disease arteriosclerosis in which the wall of the artery develops abnormalities, called lesions. These lesions may lead to narrowing due to the buildup of atheromatous plaque. At onset there are usually no symptoms, but if they develop, symptoms generally begin around middle age. Do jellyfish have arteries? | Jellyfish: Jellyfish are mainly free-swimming marine animals with umbrella-shaped bells and trailing tentacles, although a few are anchored to the seabed by stalks rather So jellyfish don\u2019t have atherosclerosis because they don\u2019t have arteries? | Jellyfish: A free-swimming marine coelenterate that is the sexually reproducing form of a hydrozoan or scyphozoan and has a nearly transparent saucer-shaped body and\nQuestion:\nIs a jellyfish safe from atherosclerosis? Answer: Jellyfish do not have a circulatory system and therefore do not have arteries. Atherosclerosis is a disease in which plaque builds up on the walls of the arteries. So the answer is: Yes. ... # Context: What is arithmetic? | Arithmetic: Arithmetic is an elementary part of mathematics that consists of the study of the properties of the traditional operations on numbers\u2014addition, subtraction, multiplication, division, exponentiation, and extraction of roots. What is the evidence that Neanderthals used arithmetic? | Neanderthal: In 2012, British-American geneticist Graham Coop hypothesised that they instead found evidence of a different archaic human species interbreeding with modern Is there any reason to believe that Neanderthals could not have used arithmetic? | Neanderthal: A large part of the controversy stems from the vagueness of the term \"species\", as it is generally used to distinguish two genetically isolated populations, but What is arithmetic? | Arithmetic: Arithmetic is an elementary part of mathematics that consists of the study of the properties of the traditional operations on numbers\u2014addition, subtraction, multiplication, division, exponentiation, and extraction of roots. Do we have any evidence that Neanderthals could manipulate numbers? | Neanderthal: Neanderthals also written as Neandertals, are an extinct species or subspecies of archaic humans who lived in Eurasia until about 40,000 years ago. What is arithmetic? | Neanderthal: Neanderthals also written as Neandertals, are an extinct species or subspecies of archaic humans who lived in Eurasia until about 40,000 years ago. What is the earliest evidence of arithmetic? | Mathematics: It is in Babylonian mathematics that elementary arithmetic (addition, subtraction, multiplication, and division) first appear in the archaeological record. The Babylonians also possessed a place-value system and used a sexagesimal numeral system which is still in use today for measuring angles and time. Did Neanderthals live before the ancient Babylonians? | Neanderthal: Neanderthals also written as Neandertals, are an extinct species or subspecies of archaic humans who lived in Eurasia until about 40,000 years ago. Pre- and early Neanderthals, living before the Eemian interglacial\nQuestion: Did Neanderthals use arithmetic?\nAnswer: There is no evidence that Neanderthals used arithmetic. The earliest evidence of arithmetic dates back to the ancient Babylonians who lived after the Neanderthals. So the answer is: No. #\nFigure 18: Instruction and exemplars for the STRATEGYQA prompt for MCR-EV and SCR-EV reasoning over retrieved evidence."
        }
    ],
    "title": "Answering Questions by Meta-Reasoning over Multiple Chains of Thought",
    "year": 2023
}