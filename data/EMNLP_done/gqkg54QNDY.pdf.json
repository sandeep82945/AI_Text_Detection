{
    "abstractText": "English and Chinese, known as resource-rich languages, have witnessed the strong development of transformer-based language models for natural language processing tasks. Although Vietnam has approximately 100M people speaking Vietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA, performed well on general Vietnamese NLP tasks, including POS tagging and named entity recognition. These pre-trained language models are still limited to Vietnamese social media tasks. In this paper, we present the first monolingual pre-trained language model for Vietnamese social media texts, ViSoBERT, which is pre-trained on a large-scale corpus of high-quality and diverse Vietnamese social media texts using XLM-R architecture. Moreover, we explored our pre-trained model on five important natural language downstream tasks on Vietnamese social media texts: emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection. Our experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses the previous state-of-the-art models on multiple Vietnamese social media tasks. Our ViSoBERT model is available only for research purposes. Disclaimer: This paper contains actual comments on social networks that might be construed as abusive, offensive, or obscene.",
    "authors": [
        {
            "affiliations": [],
            "name": "Quoc-Nam Nguyen"
        },
        {
            "affiliations": [],
            "name": "Thang Chau Phan"
        },
        {
            "affiliations": [],
            "name": "Duc-Vu Nguyen"
        },
        {
            "affiliations": [],
            "name": "Kiet Van Nguyen"
        }
    ],
    "id": "SP:76da53434c8593e505395ad00a001cfb8c8c782a",
    "references": [
        {
            "authors": [
                "Francesco Barbieri",
                "Luis Espinosa Anke",
                "Jose Camacho-Collados."
            ],
            "title": "XLM-T: Multilingual language models in Twitter for sentiment analysis and beyond",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages",
            "year": 2022
        },
        {
            "authors": [
                "Iz Beltagy",
                "Kyle Lo",
                "Arman Cohan."
            ],
            "title": "SciBERT: A Pretrained Language Model for Scientific Text",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Ilias Chalkidis",
                "Manos Fergadiotis",
                "Prodromos Malakasiotis",
                "Nikolaos Aletras",
                "Ion Androutsopoulos."
            ],
            "title": "LEGAL-BERT: The Muppets straight out of Law School",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2898\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised Cross-lingual Representation Learning at Scale",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Alexandra DeLucia",
                "Shijie Wu",
                "Aaron Mueller",
                "Carlos Aguirre",
                "Philip Resnik",
                "Mark Dredze."
            ],
            "title": "Bernice: A Multilingual Pre-trained Encoder for Twitter",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Co Van Dinh",
                "Son T. Luu",
                "Anh Gia-Tuan Nguyen."
            ],
            "title": "Detecting Spam Reviews on Vietnamese ECommerce Websites",
            "venue": "Intelligent Information and Database Systems, pages 595\u2013607. Springer International Publishing.",
            "year": 2022
        },
        {
            "authors": [
                "Vong Anh Ho",
                "Duong Huynh-Cong Nguyen",
                "Danh Hoang Nguyen",
                "Linh Thi-Van Pham",
                "Duc-Vu Nguyen",
                "Kiet Van Nguyen",
                "Ngan Luu-Thuy Nguyen."
            ],
            "title": "Emotion Recognition for Vietnamese Social Media Text",
            "venue": "Computational Linguistics:",
            "year": 2020
        },
        {
            "authors": [
                "Phu Gia Hoang",
                "Canh Luu",
                "Khanh Tran",
                "Kiet Nguyen",
                "Ngan Nguyen."
            ],
            "title": "ViHOS: Hate Speech Spans Detection for Vietnamese",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 652\u2013669.",
            "year": 2023
        },
        {
            "authors": [
                "Yibo Hu",
                "MohammadSaleh Hosseini",
                "Erick Skorupa Parolin",
                "Javier Osorio",
                "Latifur Khan",
                "Patrick Brandt",
                "Vito D\u2019Orazio"
            ],
            "title": "ConfliBERT: A Pre-trained Language Model for Political Conflict and Violence",
            "venue": "In Proceedings of the 2022 Conference",
            "year": 2022
        },
        {
            "authors": [
                "Fajri Koto",
                "Jey Han Lau",
                "Timothy Baldwin."
            ],
            "title": "IndoBERTweet: A pretrained language model for Indonesian Twitter with effective domain-specific vocabulary initialization",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson."
            ],
            "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2018
        },
        {
            "authors": [
                "Guillaume Lample",
                "Alexis Conneau."
            ],
            "title": "Crosslingual Language Model Pretraining",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS).",
            "year": 2019
        },
        {
            "authors": [
                "Phuong Le-Hong."
            ],
            "title": "Diacritics generation and application in hate speech detection on Vietnamese social networks",
            "venue": "Knowledge-Based Systems, 233:107504.",
            "year": 2021
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang."
            ],
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "venue": "Bioinformatics, 36(4):1234\u20131240.",
            "year": 2019
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled Weight Decay Regularization",
            "venue": "Proceedings of the International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Son T. Luu",
                "Kiet Van Nguyen",
                "Ngan Luu-Thuy Nguyen."
            ],
            "title": "A Large-Scale Dataset for Hate Speech Detection on Vietnamese Social Media Texts",
            "venue": "Advances and Trends in Artificial Intelligence.",
            "year": 2021
        },
        {
            "authors": [
                "Nguyen Minh",
                "Vu Hoang Tran",
                "Vu Hoang",
                "Huy Duc Ta",
                "Trung Huu Bui",
                "Steven Quoc Hung Truong."
            ],
            "title": "ViHealthBERT: Pre-trained language models for Vietnamese in health text mining",
            "venue": "Proceedings of the Thirteenth Language Resources and Evalua-",
            "year": 2022
        },
        {
            "authors": [
                "B. Ngo."
            ],
            "title": "Vietnamese: An Essential Grammar",
            "venue": "Essential grammar. Routledge, Taylor & Francis Group.",
            "year": 2020
        },
        {
            "authors": [
                "Dat Quoc Nguyen",
                "Anh Tuan Nguyen."
            ],
            "title": "PhoBERT: Pre-trained language models for Vietnamese",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1037\u20131042, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Dat Quoc Nguyen",
                "Thanh Vu",
                "Anh Tuan Nguyen."
            ],
            "title": "BERTweet: A pre-trained language model for English tweets",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 9\u201314, On-",
            "year": 2020
        },
        {
            "authors": [
                "Huyen TM Nguyen",
                "Hung V Nguyen",
                "Quyen T Ngo",
                "Luong X Vu",
                "Vu Mai Tran",
                "Bach X Ngo",
                "Cuong A Le."
            ],
            "title": "VLSP Shared Task: Sentiment Analysis",
            "venue": "Journal of Computer Science and Cybernetics, 34(4):295\u2013310.",
            "year": 2018
        },
        {
            "authors": [
                "Khang Phuoc-Quy Nguyen",
                "Kiet Van Nguyen."
            ],
            "title": "Exploiting Vietnamese social media characteristics for textual emotion recognition in Vietnamese",
            "venue": "2020 International Conference on Asian Language Processing (IALP), pages 276\u2013281. IEEE.",
            "year": 2020
        },
        {
            "authors": [
                "Linh The Nguyen",
                "Dat Quoc Nguyen."
            ],
            "title": "PhoNLP: A joint multi-task learning model for Vietnamese part-of-speech tagging, named entity recognition and dependency parsing",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Luan Nguyen",
                "Kiet Nguyen",
                "Ngan Nguyen."
            ],
            "title": "SMTCE: A social media text classification evaluation benchmark and BERTology models for Vietnamese",
            "venue": "Proceedings of the 36th Pacific Asia Conference on Language, Information and Computation, pages 282\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Juan Manuel P\u00e9rez",
                "Dami\u00e1n Ariel Furman",
                "Laura Alonso Alemany",
                "Franco M. Luque."
            ],
            "title": "RoBERTuito: a pre-trained language model for social media text in Spanish",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Confer-",
            "year": 2022
        },
        {
            "authors": [
                "Long Phan",
                "Hieu Tran",
                "Hieu Nguyen",
                "Trieu H. Trinh."
            ],
            "title": "ViT5: Pretrained text-to-text transformer for Vietnamese language generation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Khanh Quoc Tran",
                "An Trong Nguyen",
                "Phu Gia Hoang",
                "Canh Duc Luu",
                "Trong-Hop Do",
                "Kiet Van Nguyen."
            ],
            "title": "Vietnamese hate and offensive detection using PhoBERT-CNN and social media streaming data",
            "venue": "Neural Computing and Applications,",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Laila Rasmy",
                "Yang Xiang",
                "Ziqian Xie",
                "Cui Tao",
                "Degui Zhi."
            ],
            "title": "Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction",
            "venue": "NPJ digital medicine, 4(1):86.",
            "year": 2021
        },
        {
            "authors": [
                "Cong Dao Tran",
                "Nhut Huy Pham",
                "Anh-Tuan Nguyen",
                "Truong Son Hy",
                "Tu Vu."
            ],
            "title": "ViDeBERTa: A powerful pre-trained language model for Vietnamese",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 1041\u20131048.",
            "year": 2023
        },
        {
            "authors": [
                "Nguyen Luong Tran",
                "Duong Minh Le",
                "Dat Quoc Nguyen."
            ],
            "title": "BARTpho: Pre-trained Sequence-toSequence Models for Vietnamese",
            "venue": "Proceedings of the 23rd Annual Conference of the International Speech Communication Association.",
            "year": 2022
        },
        {
            "authors": [
                "Thi Oanh Tran",
                "Phuong Le Hong"
            ],
            "title": "Improving sequence tagging for Vietnamese text using transformer-based neural models",
            "venue": "In Proceedings of the 34th Pacific Asia conference on language, information and computation,",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Alexander Wettig",
                "Tianyu Gao",
                "Zexuan Zhong",
                "Danqi Chen"
            ],
            "title": "Should You Mask 15% in Masked Language Modeling",
            "venue": "In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics,",
            "year": 2023
        },
        {
            "authors": [
                "Xinyang Zhang",
                "Yury Malkov",
                "Omar Florez",
                "Serim Park",
                "Brian McWilliams",
                "Jiawei Han",
                "Ahmed ElKishky."
            ],
            "title": "TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations",
            "venue": "arXiv preprint arXiv:2209.07562.",
            "year": 2022
        },
        {
            "authors": [
                "Jos\u00e9 \u00c1ngel Gonz\u00e1lez",
                "Llu\u00eds-F. Hurtado",
                "Ferran Pla."
            ],
            "title": "TWilBert: Pre-trained deep bidirectional transformers for Spanish Twitter",
            "venue": "Neurocomputing, 426:58\u201369.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Disclaimer: This paper contains actual comments on social networks that might be construed as abusive, offensive, or obscene."
        },
        {
            "heading": "1 Introduction",
            "text": "Language models based on transformer architecture (Vaswani et al., 2017) pre-trained on largescale datasets have brought about a paradigm shift in natural language processing (NLP), reshaping how we analyze, understand, and generate text. In\n*Equal contribution. 4 https://huggingface.co/uitnlp/visobert\nparticular, BERT (Devlin et al., 2019) and its variants (Liu et al., 2019; Conneau et al., 2020) have achieved state-of-the-art performance on a wide range of downstream NLP tasks, including but not limited to text classification, sentiment analysis, question answering, and machine translation. English is moving for the rapid development of language models across specific domains such as medical (Lee et al., 2019; Rasmy et al., 2021), scientific (Beltagy et al., 2019), legal (Chalkidis et al., 2020), political conflict and violence (Hu et al., 2022), and especially social media (Nguyen et al., 2020; DeLucia et al., 2022; P\u00e9rez et al., 2022; Zhang et al., 2022).\nVietnamese is the eighth largest language used over the internet, with around 85 million users across the world5. Despite a large amount of Vietnamese data available over the Internet, the advancement of NLP research in Vietnamese is still slow-moving. This can be attributed to several factors, to name a few: the scattered nature of available datasets, limited documentation, and minimal community engagement. Moreover, most existing pre-trained models for Vietnamese were primarily trained on large-scale corpora sourced from general texts (Tran et al., 2020; Nguyen and Tuan Nguyen, 2020; Tran et al., 2023). While these sources provide broad language coverage, they may not fully represent the sociolinguistic phenomena in Vietnamese social media texts. Social media texts often exhibit different linguistic patterns, informal language usage, non-standard vocabulary, lacking diacritics and emoticons that are not prevalent in formal written texts. The limitations of using language models pre-trained on general corpora become apparent when processing Vietnamese social media texts. The models can struggle to accurately un-\n5 https://www.internetworldstats.com/stats3.htm\nderstand and interpret the informal language, using emoji, teencode, and diacritics used in social media discussions. This can lead to suboptimal performance in Vietnamese social media tasks, including emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection.\nWe present ViSoBERT, a pre-trained language model designed explicitly for Vietnamese social media texts to address these challenges. ViSoBERT is based on the transformer architecture and trained on a large-scale dataset of Vietnamese posts and comments extracted from well-known social media networks, including Facebook, Tiktok, and Youtube. Our model outperforms existing pretrained models on various downstream tasks, including emotion recognition, hate speech detection, sentiment analysis, spam reviews detection, and hate speech spans detection, demonstrating its effectiveness in capturing the unique characteristics of Vietnamese social media texts. Our contributions are summarized as follows.\n\u2022 We presented ViSoBERT, the first PLM based on the XLM-R architecture and pre-training procedure for Vietnamese social media text processing. ViSoBERT is available publicly for research purposes in Vietnamese social media mining. ViSoBERT can be a strong baseline for Vietnamese social media text processing tasks and their applications.\n\u2022 ViSoBERT produces SOTA performances on multiple Vietnamese downstream social media tasks, thus illustrating the effectiveness of our PLM on Vietnamese social media texts.\n\u2022 To understand our pre-trained language model deeply, we analyze experimental results on the masking rate, examining social media characteristics, including emojis, teencode, and diacritics, and implementing feature-based extraction for task-specific models."
        },
        {
            "heading": "2 Fundamental of Pre-trained Language Models for Social Media Texts",
            "text": "Pre-trained Language Models (PLMs) based on transformers (Vaswani et al., 2017) have become a crucial element in cutting-edge NLP tasks, including text classification and natural language generation. Since then, language models based on transformers related to our study have been reviewed, including PLMs for Vietnamese social media texts."
        },
        {
            "heading": "2.1 Pre-trained Language Models for Vietnamese",
            "text": "Several PLMs have recently been developed for processing Vietnamese texts. These models have varied in their architectures, training data, and evaluation metrics. PhoBERT, developed by Nguyen and Tuan Nguyen (2020), is the first general pretrained language model (PLM) created for the Vietnamese language. The model employs the same architecture as BERT (Devlin et al., 2019) and the same pre-training technique as RoBERTa (Liu et al., 2019) to ensure robust and reliable performance. PhoBERT was trained on a 20GB wordlevel Vietnamese Wikipedia corpus, which produces SOTA performances on a range of downstream tasks of POS tagging, dependency parsing, NER, and NLI.\nFollowing the success of PhoBERT, viBERT (Tran et al., 2020) and vELECTRA (Tran et al., 2020), both monolingual pre-trained language models based on the BERT and ELECTRA architectures, were introduced. They were trained on substantial datasets, with ViBERT using a 10GB corpus and vELECTRA utilizing an even larger 60GB collection of uncompressed Vietnamese text. viBERT4news6 was published by NlpHUST, a Vietnamese version of BERT trained on more than 20 GB of news datasets. For Vietnamese text summarization, BARTpho (Tran et al., 2022) is presented as the first large-scale monolingual seq2seq models pre-trained for Vietnamese, based on the seq2seq denoising autoencoder BART. Moreover, ViT5 (Phan et al., 2022) follows the encoderdecoder architecture proposed by Vaswani et al. (2017) and the T5 framework proposed by Raffel et al. (2020). Many language models are designed for general use, while the availability of strong baseline models for domain-specific applications remains limited. Since then, Minh et al. (2022) introduced ViHealthBERT, the first domain-specific PLM for Vietnamese healthcare."
        },
        {
            "heading": "2.2 Pre-trained Language Models for Social Media Texts",
            "text": "Multiple PLMs were introduced for social media for multilingual and monolinguals. BERTweet (Nguyen et al., 2020) was presented as the first public large-scale PLM for English Tweets. BERTweet has the same architecture as BERTBase (Devlin et al., 2019) and is trained using the RoBERTa pre-\n6 https://github.com/bino282/bert4news\ntraining procedure (Liu et al., 2019). Koto et al. (2021) proposed IndoBERTweet, the first largescale pre-trained model for Indonesian Twitter. IndoBERTweet is trained by extending a monolingually trained Indonesian BERT model with an additive domain-specific vocabulary. RoBERTuito, presented in P\u00e9rez et al. (2022), is a robust transformer model trained on 500 million Spanish tweets. RoBERTuito excels in various language contexts, including multilingual and codeswitching scenarios, such as Spanish and English. TWilBert (\u00c1ngel Gonz\u00e1lez et al., 2021) is proposed as a specialization of BERT architecture both for the Spanish language and the Twitter domain to address text classification tasks in Spanish Twitter.\nBernice, introduced by DeLucia et al. (2022), is the first multilingual pre-trained encoder designed exclusively for Twitter data. This model uses a customized tokenizer trained solely on Twitter data and incorporates a larger volume of Twitter data (2.5B tweets) than most BERT-style models. Zhang et al. (2022) introduced TwHIN-BERT, a multilingual language model trained on 7 billion Twitter tweets in more than 100 different languages. It is designed to handle short, noisy, user-generated text effectively. Previously, (Barbieri et al., 2022) extended the training of the XLM-R (Conneau et al., 2020) checkpoint using a data set comprising 198 million multilingual tweets. As a result, XLM-T is adapted to the Twitter domain and was not exclusively trained on data from within that domain."
        },
        {
            "heading": "3 ViSoBERT",
            "text": "This section presents the architecture, pre-training data, and our custom tokenizer on Vietnamese social media texts for ViSoBERT."
        },
        {
            "heading": "3.1 Pre-training Data",
            "text": "We crawled textual data from Vietnamese public social networks such as Facebook7, Tiktok8, and YouTube9 which are the three most well known social networks in Vietnam, with 52.65, 49.86, and 63.00 million users10, respectively, in early 2023.\nTo effectively gather data from these platforms, we harnessed the capabilities of specialized tools provided by each platform.\n7 https://www.facebook.com/ 8 https://www.tiktok.com/ 9 https://www.youtube.com/\n10 https://datareportal.com/reports/\ndigital-2023-vietnam\n1. Facebook: We crawled comments from Vietnamese-verified pages by Facebook posts via the Facebook Graph API11 between January 2016 and December 2022.\n2. TikTok: We collected comments from Vietnamese-verified channels by TikTok through TikTok Research API12 between January 2020 and December 2022.\n3. YouTube: We scrapped comments from Vietnam-verified channels\u2019 videos by YouTube via YouTube Data API13 between January 2016 and December 2022.\nPre-processing Data: Pre-processing is vital for models consuming social media data, which is massively noisy, and has user handles (@username), hashtags, emojis, misspellings, hyperlinks, and other noncanonical texts. We perform the following steps to clean the dataset: removing noncanonical texts, removing comments including links, removing excessively repeated spam and meaningless comments, removing comments including only user handles (@username), and keeping emojis in training data.\nAs a result, our pretraining data after crawling and preprocessing contains 1GB of uncompressed text. Our pretraining data is available only for research purposes."
        },
        {
            "heading": "3.2 Model Architecture",
            "text": "Transformers (Vaswani et al., 2017) have significantly advanced NLP research using trained models in recent years. Although language models (Nguyen and Tuan Nguyen, 2020; Nguyen and Nguyen, 2021) have also proven effective on a range of Vietnamese NLP tasks, their results on Vietnamese social media tasks (Nguyen et al., 2022) need to be significantly improved. To address this issue, taking into account successful hyperparameters from XLM-R (Conneau et al., 2020), we proposed ViSoBERT, a transformerbased model in the style of XLM-R architecture with 768 hidden units, 12 self-attention layers, and 12 attention heads, and used a masked language objective (the same as Conneau et al. (2020)).\n11 https://developers.facebook.com/ 12 https://developers.tiktok.com/products/\nresearch-api/ 13 https://developers.google.com/youtube/v3"
        },
        {
            "heading": "3.3 The Vietnamese Social Media Tokenizer",
            "text": "To the best of our knowledge, ViSoBERT is the first PLM with a custom tokenizer for Vietnamese social media texts. Bernice (DeLucia et al., 2022) was the first multilingual model trained from scratch on Twitter14 data with a custom tokenizer; however, Bernice\u2019s tokenizer doesn\u2019t handle Vietnamese social media text effectively. Moreover, existing Vietnamese pre-trained models\u2019 tokenizers perform poorly on social media text because of different domain data training. Therefore, we developed the first custom tokenizer for Vietnamese social media texts.\nOwing to the ability to handle raw texts of SentencePiece (Kudo and Richardson, 2018) without any loss compared to Byte-Pair Encoding (Conneau et al., 2020), we built a custom tokenizer on Vietnamese social media by SentencePiece on the whole training dataset. A model has better coverage of data than another when fewer subwords are needed to represent the text, and the subwords are longer (DeLucia et al., 2022). Figure 2 (in Appendix A) displays the mean token length for each considered model and group of tasks. ViSoBERT achieves the shortest representations for all Vietnamese social media downstream tasks compared to other PLMs.\nEmojis and teencode are essential to the \u201clanguage\u201d on Vietnamese social media platforms. Our custom tokenizer\u2019s capability to decode emojis and teencode ensure that their semantic meaning and contextual significance are accurately captured and incorporated into the language representation, thus enhancing the overall quality and comprehensiveness of text analysis and understanding.\nTo assess the tokenized ability of Vietnamese social media textual data, we conducted an analysis of several data samples. Table 1 shows several actual social comments and their tokenizations with the tokenizers of the two pre-trained language models, ViSoBERT and PhoBERT, the best strong baseline. The results show that our custom tokenizer performed better compared to others."
        },
        {
            "heading": "4 Experiments and Results",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "We accumulate gradients over one step to simulate a batch size of 128. When pretraining from scratch, we train the model for 1.2M steps in 12\n14 https://twitter.com/\nepochs. We trained our model for about three days on 2\u00d7RTX4090 GPUs (24GB). Each sentence is tokenized and masked dynamically with a probability equal to 30% (which is extensively experimented on Section 5.1 to explore the optimal value). Further details on hyperparameters and training can be found in Table 6 of Appendix B.\nDownstream tasks. To evaluate ViSoBERT, we used five Vietnamese social media datasets available for research purposes, as summarized in Table 2. The downstream tasks include emotion recognition (UIT-VSMEC) (Ho et al., 2020), hate speech detection (UIT-ViHSD) (Luu et al., 2021), sentiment analysis (SA-VLSP2016) (Nguyen et al., 2018), spam reviews detection (ViSpamReviews) (Dinh et al., 2022), and hate speech spans detection (UIT-ViHOS) (Hoang et al., 2023).\nFine-tuning. We conducted empirical finetuning for all pre-trained language models using the simpletransformers15. Our fine-tuning process followed standard procedures, most of which are outlined in (Devlin et al., 2019). For all tasks mentioned above, we use a batch size of 40, a maximum token length of 128, a learning rate of 2e-5, and AdamW optimizer (Loshchilov and Hutter, 2019) with an epsilon of 1e-8. We executed a 10- epoch training process and evaluated downstream tasks using the best-performing model from those epochs. Furthermore, none of the pre-processing techniques is applied in all datasets to evaluate our PLM\u2019s ability to handle raw texts.\nBaseline models. To establish the main baseline models, we utilized several well-known PLMs, including monolingual and multilingual, to support Vietnamese NLP social media tasks. The details of each model are shown in Table 3.\n\u2022 Monolingual language models: viBERT (Tran et al., 2020) and vELECTRA (Tran et al., 2020) are PLMs for Vietnamese based on BERT and ELECTRA architecture, respectively. PhoBERT, which is based on BERT architecture and RoBERTa pre-training procedure, (Nguyen and Tuan Nguyen, 2020) is the first large-scale monolingual language model pre-trained for Vietnamese; PhoBERT obtains state-of-the-art performances on a range of Vietnamese NLP tasks.\n\u2022 Multilingual language models: Additionally, we incorporated two multilingual PLMs,\n15 https://simpletransformers.ai/ (ver 0.63.11)\nmBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020), which were previously shown to have competitive performances to monolingual Vietnamese models. XLM-R, a cross-lingual PLM introduced by Conneau et al. (2020), has been trained in 100 languages, among them Vietnamese, utilizing a vast 2.5TB Clean CommonCrawl dataset. XLM-R presents notable improvements in various downstream tasks, surpassing the performance of previously released multilingual models such as mBERT (Devlin et al., 2019) and XLM (Lample and Conneau, 2019).\n\u2022 Multilingual social media language models: To ensure a fair comparison with our PLM, we integrated multiple multilingual social media PLMs, including XLM-T (Barbieri et al., 2022), TwHIN-BERT (Zhang et al., 2022), and Bernice (DeLucia et al., 2022)."
        },
        {
            "heading": "4.2 Main Results",
            "text": "Table 4 shows ViSoBERT\u2019s scores with the previous highest reported results on other PLMs using the same experimental setup. It is clear that our ViSoBERT produces new SOTA performance results for multiple Vietnamese downstream social media tasks without any pre-processing technique.\nEmotion Recognition Task: PhoBERT and TwHIN-BERT archive the previous SOTA performances on monolingual and multilingual models, respectively. ViSoBERT obtains 68.10%, 68.37%,\nand 65.88% of Acc, WF1, and MF1, respectively, significantly higher than these PhoBERT and TwHIN-BERT models.\nHate Speech Detection Task: ViSoBERT achieves significant improvements over previous state-of-the-art models, PhoBERT and TwHINBERT, with scores of 88.51%, 88.31%, and 68.77% in Acc, WF1, and MF1, respectively. Notably, these achievements are made despite the presence of bias within the dataset16.\nSentiment Analysis Task: XLM-R archived SOTA performance on three evaluation metrics. However, there is no significant increase in performance on this downstream task, for 0.45%, 0.46%, and 0.46% higher on Acc, WF1, and MF1 compared to our pre-trained language model, PhoBERTLarge. The SA-VLSP2016 dataset domain is technical article reviews, including TinhTe17 and VnExpress18, which are often used as Vietnamese standard data. The reviews or comments in these newspapers are in proper form. While most of the dataset is sourced from articles, it also includes data from Facebook19, a Vietnamese social media platform that accounts for only 12.21% of the dataset. Therefore, the dataset does not fully capture Vietnamese social media platforms\u2019 diverse characteristics and infor-\n16UIT-HSD is massively imbalanced, included 19,886; 1,606; and 2,556 of CLEAN, OFFENSIVE, and HATE class.\n17 https://tinhte.vn/ 18 https://vnexpress.net/ 19 https://www.facebook.com/\nmal language. However, ViSoBERT still surpassed other baselines by obtaining 1.31%/0.91% Acc, 1.39%/0.92% WF1, and 1.53%/0.92% MF1 compared to PhoBERT/TwHIN-BERT.\nSpam Reviews Detection Task: ViSoBERT performed better than the top two baseline models, PhoBERT and TwHIN-BERT. Specifically, it achieved 0.8%, 0.9%, and 2.18% higher scores in accuracy (Acc), weighted F1 (WF1), and micro F1 (MF1) compared to PhoBERT. When compared to TwHIN-BERT, ViSoBERT outperformed it with 0.52%, 0.50%, and 1.78% higher scores in Acc, WF1, and MF1, respectively.\nHate Speech Spans Detection Task20: Our pre-trained ViSoBERT boosted the results up to 91.62%, 91.57%, and 86.80% on Acc, WF1, and MF1, respectively. While the difference is insignificant, ViSoBERT indicates an outstanding ability to capture Vietnamese social media information compared to other PLMs (see Section 5.3).\nMultilingual social media PLMs: The results show that ViSoBERT consistently outperforms\n20For the Hate Speech Spans Detection task, we evaluate the total of spans on each comment rather than spans of each word in Hoang et al. (2023) to retain the context of each comment.\nXLM-T and Bernice in five Vietnamese social media tasks. It\u2019s worth noting that XLM-T, TwHINBERT, and Bernice were all exclusively trained on data from the Twitter platform. However, this approach has limitations when applied to the Vietnamese context. The training data from this source may not capture the intricate linguistic and contextual nuances prevalent in Vietnamese social media because Twitter is not widely used in Vietnam."
        },
        {
            "heading": "5 Result Analysis and Discussion",
            "text": "In this section, we consider the improvement of our PLM more compared to powerful others, including PhoBERT and TwHIN-BERT, in terms of different aspects. Firstly, we investigate the effects of masking rate on our pre-trained model performance (see Section 5.1). Additionally, we examine the influence of social media characteristics on the model\u2019s ability to process and understand the language used in these social contexts (see Section 5.2). Lastly, we employed feature-based extraction techniques on task-specific models to verify the potential of leveraging social media textual data to enhance word representations (see Section 5.3)."
        },
        {
            "heading": "5.1 Impact of Masking Rate on Vietnamese Social Media PLM",
            "text": "For the first time presenting the Masked Language Model, Devlin et al. (2019) consciously utilized a random masking rate of 15%. The authors believed masking too many tokens could lead to losing crucial contextual information required to decode them accurately. Additionally, the authors felt that masking too few tokens would harm the training process and make it less effective. However, according to Wettig et al. (2023), 15% is not universally optimal for model and training data.\nWe experiment with masking rates ranging from 10% to 50% and evaluate the model\u2019s performance on five downstream Vietnamese social media tasks. Figure 1 illustrates the results obtained from our experiments with six different masking rates. Interestingly, our pre-trained ViSoBERT achieved the highest performance when using a masking rate of 30%. This suggests a delicate balance between the amount of contextual information retained and the efficiency of the training process, and an optimal masking rate can be found within this range.\nHowever, the optimal masking rate also depends on the specific task. For instance, in the hate speech detection task, we found that a masking rate of 50% yielded the best results, surpassing other masking rate values. This implies that the optimal masking rate may vary depending on the nature and requirements of different tasks.\nConsidering the overall performance across multiple tasks, we determined that a masking rate of 30% produced the optimal balance for our pre-trained ViSoBERT model. Consequently, we adopted this masking rate for ViSoBERT, ensuring efficient and effective utilization of contextual information during training."
        },
        {
            "heading": "5.2 Impact of Vietnamese Social Media Characteristics",
            "text": "Emojis, teencode, and diacritics are essential features of social media, especially Vietnamese social media. The ability of the tokenizer to decode emojis and the ability of the model to understand the context of teencode and diacritics are crucial. Hence, to evaluate the performance of ViSoBERT on social media characteristics, comprehensive experiments were conducted among several strong PLMs: PM4ViSMT, PhoBERT, and TwHIN-BERT.\nImpact of Emoji on PLMs: We conducted two experimental procedures to comprehensively investigate the importance of emojis, including converting emojis to general text and removing emojis.\nTable 5 shows our detailed setting and experimental results on downstream tasks and pre-trained models. The results indicate a moderate reduction in performance across all downstream tasks when emojis are removed or converted to text in our pre-trained ViSoBERT model. Our pre-trained decreases 0.62% Acc, 0.55% WF1, and 0.78% MF1 on Average for downstream tasks while converting emojis to text. In addition, an average reduction of 1.33% Acc, 1.32% WF1, and 1.42% MF1 can be seen in our pre-trained model while removing all emojis in each comment. This is because when emojis are converted to text, the context of the comment is preserved, while removing all emojis results in the loss of that context.\nThis trend is also observed in the TwHIN-BERT model, specifically designed for social media processing. However, TwHIN-BERT slightly improves emotion recognition and spam reviews detection tasks compared to its competitors when operating on raw texts. Nevertheless, this improvement is marginal and insignificant, as indicated by the small increments of 0.61%, 0.13%, and 0.21% in Acc, WF1, and MF1 on the emotion recognition task, respectively, and 0.08% Acc, 0.05% WF1, and 0.04% MF1 on spam reviews detection task. One potential reason for this phenomenon is that TwHIN-BERT and ViSoBERT are PLMs trained on emojis datasets. Consequently, these models can comprehend the contextual meaning conveyed by emojis. This finding underscores the importance of emojis in social media texts.\nIn contrast, there is a general trend of improved performance across a range of downstream tasks when removing or converting emojis to text on\nModel Emotion Regconition Hate Speech Detection Sentiment Analysis Spam Reviews Detection Hate Speech Spans DetectionAcc WF1 MF1 Acc WF1 MF1 Acc WF1 MF1 Acc WF1 MF1 Acc WF1 MF1 Converting emojis to text PhoBERTLarge 66.08 66.15 63.35 87.43 87.22 65.32 76.73 76.48 76.48 90.35 90.11 77.02 92.16 91.98 86.72 \u2206 \u2191 1.37 \u2191 1.49 \u2191 0.80 \u2191 0.11 \u2191 0.24 \u2191 0.18 \u2193 0.21 \u2193 0.12 \u2193 0.12 \u2191 0.23 \u2191 0.08 \u2191 0.14 \u2191 0.72 \u2191 0.52 \u2191 0.16 TwHIN-BERTLarge 64.82 64.42 61.33 86.03 85.52 63.52 75.42 75.95 75.95 90.55 90.47 77.32 92.21 92.01 86.84 \u2206 \u2191 0.61 \u2191 0.13 \u2191 0.21 \u2193 1.20 \u2193 1.26 \u2193 1.71 \u2193 1.50 \u2193 0.88 \u2193 0.88 \u2191 0.08 \u2191 0.05 \u2191 0.04 \u2191 0.76 \u2191 0.54 \u2191 0.19 ViSoBERT [\u2663] 67.53 67.93 65.42 87.82 87.88 67.25 76.95 76.85 76.85 90.22 90.18 78.25 92.42 92.11 87.01 \u2206 \u2193 0.57 \u2193 0.44 \u2193 0.46 \u2193 0.69 \u2193 0.41 \u2193 1.49 \u2193 0.88 \u2193 0.90 \u2193 0.90 \u2193 0.77 \u2193 0.74 \u2193 0.81 \u2191 0.80 \u2191 0.54 \u2191 0.21 Removing emojis PhoBERTLarge 65.21 65.14 62.81 87.25 86.72 64.85 76.72 76.48 76.48 90.21 90.09 77.02 91.53 91.51 86.62 \u2206 \u2191 0.50 \u2191 0.48 \u2191 0.26 \u2193 0.07 \u2193 0.26 \u2193 0.29 \u2191 0.20 \u2191 0.12 \u2191 0.12 \u2191 0.09 \u2191 0.06 \u2191 0.10 \u2191 0.09 \u2191 0.05 \u2191 0.09 TwHIN-BERTLarge 62.03 62.14 59.25 86.98 86.32 64.22 75.00 75.11 75.11 89.83 89.75 76.85 91.32 91.33 86.42 \u2206 \u2193 2.18 \u2193 1.15 \u2193 1.87 \u2193 0.25 \u2193 0.46 \u2193 1.01 \u2193 1.92 \u2193 1.72 \u2193 1.72 \u2193 0.64 \u2193 0.67 \u2193 0.43 \u2193 0.13 \u2193 0.14 \u2193 0.23 ViSoBERT [\u2666] 66.52 67.02 64.55 87.32 87.12 66.98 76.25 75.98 75.98 89.72 89.69 77.95 91.58 91.53 86.72 \u2206 \u2193 1.58 \u2193 1.35 \u2193 1.33 \u2193 1.19 \u2193 1.19 \u2193 1.79 \u2193 1.58 \u2193 1.77 \u2193 1.77 \u2193 1.27 \u2193 1.23 \u2193 1.11 \u2193 0.04 \u2193 0.04 \u2193 0.08 ViSoBERT [\u2660] 68.10 68.37 65.88 88.51 88.31 68.77 77.83 77.75 77.75 90.99 90.92 79.06 91.62 91.57 86.80\nTable 5: Performances of pre-trained models on downstream Vietnamese social media tasks by applying two emojis pre-processing techniques. [\u2663], [\u2666], and [\u2660] denoted our pre-trained language model ViSoBERT converting emoji to text, removing emojis and without applying any pre-processing techniques, respectively. \u2206 denoted the increase (\u2191) and the decrease (\u2193) in performances of the PLMs compared to their competitors without applying any pre-processing techniques.\nPhoBERT, the Vietnamese SOTA pre-trained language model. PhoBERT is a PLM on a general text (Vietnamese Wikipedia) dataset containing no emojis; therefore, when PhoBERT encounters an emoji, it treats it as an unknown token (see Table 1 Appendix B). Therefore, while applying emoji preprocessing techniques, including converting emoijs to text and removing emojis, PhoBERT produces better performances compared to raw text.\nOur pre-trained model ViSoBERT on raw texts outperformed PhoBERT and TwHIN-BERT even when applying two pre-processing emojis techniques. This claims our pre-trained model\u2019s ability to handle Vietnamese social media raw texts.\nImpact of Teencode on PLMs: Due to informal and casual communication, social media texts often lead to common linguistic errors, such as misspellings and teencode. For example, the phrase \u201c\u0103ng k\u01a1mmmmm\u201d should be \u201c\u0103n c\u01a1m\u201d (\u201cEat rice\u201d in English), and \u201cko\u201d should be \u201ckh\u00f4ng\u201d (\u201cNo\u201d in English). To address this challenge, Nguyen and Van Nguyen (2020) presented several rules to standardize social media texts. Building upon the previous work, Quoc Tran et al. (2023) proposed a strict and efficient pre-processing technique to clean comments on Vietnamese social media.\nTable 7 (in Appendix C) shows the results with and without standardizing teencode on social media texts. There is an uptrend across PhoBERT, TwHIN-BERT, and ViSoBERT while applying standardized pre-processing techniques. ViSoBERT, with standardized pre-processing techniques, outperforms almost downstream tasks but spam reviews detection. The possible reason is that\nthe ViSpamReviews dataset contains samples in which users use the word with duplicated characters to improve the comment length while standardizing teencodes leads to misunderstanding.\nExperimental results strongly suggest that the improvement achieved by applying complex preprocessing techniques to pre-trained models in the context of Vietnamese social media text is relatively insignificant. Despite the considerable time and effort invested in designing and implementing these techniques, the actual gains in PLMs performance are not substantial and unstable.\nImpact of Vietnamese Diacritics on PLMs: Vietnamese words are created from 29 letters, including seven letters using four diacritics (\u0103, \u00e2-\u00ea-\u00f4, \u01a1-\u01b0, and \u0111) and five diacritics used to designate tone (as in \u00e0, \u00e1, \u1ea3, \u00e3, and \u1ea1) (Ngo, 2020). These diacritics create meaningful words by combining syllables (Le-Hong, 2021). For instance, the syllable \u201cngu\u201d can be combined with five different diacritic marks, resulting in five distinct syllables: \u201cng\u00fa\u201d, \u201cng\u00f9\u201d, \u201cng\u1ee5\u201d, \u201cng\u1ee7\u201d, and \u201cng\u0169\u201d. Each of these syllables functions as a standalone word.\nHowever, social media text does not always adhere to proper writing conventions. Due to various reasons, many users write text without diacritic marks when commenting on social media platforms. Consequently, effectively handling diacritics in Vietnamese social media becomes a critical challenge. To evaluate the PLMs\u2019 capability to address this challenge, we experimented by removing all diacritic marks from the datasets of five downstream tasks. This experiment aimed to assess the model\u2019s performance in processing text without\ndiacritics and determine its ability to understand Vietnamese social media content in such cases.\nTable 8 (in Appendix C) presents the results of the two best baselines compared to our pre-trained diacritics experiments. The experimental results reveal that the performance of all pre-trained models, including ours, exhibited a significant decrease when dealing with social media comments lacking diacritics. This decline in performance can be attributed to the loss of contextual information caused by the removal of diacritics. The lower the percentage of diacritic removal in each comment, the more significant the performance improvement in all PLMs. However, our ViSoBERT demonstrated a relatively minor reduction in performance across all downstream tasks. This suggests that our model possesses a certain level of robustness and adaptability in comprehending and analyzing Vietnamese social media content without diacritics. We attribute this to the efficiency of the in-domain pre-training data of ViSoBERT.\nIn contrast, PhoBERT and TwHIN-BERT experienced a substantial drop in performance across the benchmark datasets. These PLMs struggled to cope with the absence of diacritics in Vietnamese social media comments. The main reason is that the tokenizer of PhoBERT can not encode nondiacritics comments due to not including those in pre-training data. Several tokenized examples of the three best PLMs are presented in Table 10 (in Appendix F). Thus, the significant decrease in its performance highlights the challenge of handling diacritics on Vietnamese social media. While handling diacritics remains challenging, ViSoBERT demonstrates promising performance, suggesting the potential for specialized language models tailored for Vietnamese social media analysis."
        },
        {
            "heading": "5.3 Impact of Feature-based Extraction to Task-Specific Models",
            "text": "In task-specific models, the contextualized word embeddings from PLMs are typically employed as input features. We aim to assess the quality of contextualized word embeddings generated by PhoBERT, TwHIN-BERT, and ViSoBERT to verify whether social media data can enhance word representation. These contextualized word embeddings are applied as embedding features to BiLSTM, and BiGRU is randomly initialized before the classification layer. We append a linear prediction layer to the last transformer layer of each PLM regard-\ning the first subword of each word token, which is similar to Devlin et al. (2019).\nOur experiment results (see Table 9 in Appendix C) demonstrate that the word embeddings generated by our pre-trained language model ViSoBERT outperform other pre-trained embeddings when utilized with BiLSTM and BiGRU for all downstream tasks. The experimental results indicate the significant impact of leveraging social media text data for enriching word embeddings. Furthermore, this finding underscores the effectiveness of our model in capturing the linguistic characteristics prevalent in Vietnamese social media texts.\nFigure 3 (in Appendix D) presents the performances of the PLMs as input features to BiLSTM and BiGRU on the dev set per epoch in terms of MF1. The results demonstrate that ViSoBERT reaches its peak MF1 score in only 1 to 3 epochs, whereas other PLMs typically require an average of 8 to 10 epochs to achieve on-par performance. This suggests that ViSoBERT has a superior capability to extract Vietnamese social media information compared to other models."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "We presented ViSoBERT, a novel large-scale monolingual pre-trained language model on Vietnamese social media texts. We illustrated that ViSoBERT with fewer parameters outperforms recent strong pre-trained language models such as viBERT, vELECTRA, PhoBERT, XLM-R, XLMT, TwHIN-BERT, and Bernice and achieves stateof-the-art performances for multiple downstream Vietnamese social media tasks, including emotion recognition, hate speech detection, spam reviews detection, and hate speech spans detection. We conducted extensive analyses to demonstrate the efficiency of ViSoBERT on various Vietnamese social media characteristics, including emojis, teencodes, and diacritics. Furthermore, our pre-trained language model ViSoBERT also shows the potential of leveraging Vietnamese social media text to enhance word representations compared to other PLMs. We hope the widespread use of our opensource ViSoBERT pre-trained language model will advance current NLP social media tasks and applications for Vietnamese. Other low-resource languages can adopt how to create PLMs for enhancing their current NLP social media tasks and relevant applications.\nLimitations\nWhile we have demonstrated that ViSoBERT can perform state-of-the-art on a range of NLP social media tasks for Vietnamese, we think additional analyses and experiments are necessary to fully comprehend what aspects of ViSoBERT were responsible for its success and what understanding of Vietnamese social media texts ViSoBERT captures. We leave these additional investigations to future research. Moreover, future work aims to explore a broader range of Vietnamese social media downstream tasks that this paper may not cover. Also, we chose to train the base-size transformer model instead of the Large variant because base models are more accessible due to their lower computational requirements. For PhoBERT, XLM-R, and TwHIN-BERT, we implemented two versions Base and Large for all Vietnamese social media downstream tasks. However, it is not a fair comparison due to their significantly larger model configurations. Moreover, regular updates and expansions of the pre-training data are essential to keep up with the rapid evolution of social media. This allows the pre-trained model to adapt effectively to the dynamic linguistic patterns and trends in Vietnamese social media.\nEthics Statement\nThe authors introduce ViSoBERT, a pre-trained language model for investigating social language phenomena in social media in Vietnamese. ViSoBERT is based on pre-training an existing pretrained language model (i.e., XLM-R), which lessens the influence of its construction on the environment. ViSoBERT makes use of a large-scale corpus of posts and comments from social communities that have been found to express harassment, bullying, incitement of violence, hate, offense, and abuse, as defined by the content policies of social media platforms, including Facebook, YouTube, and TikTok."
        },
        {
            "heading": "Acknowledgement",
            "text": "This research was supported by The VNUHCMUniversity of Information Technology\u2019s Scientific Research Support Fund. We thank the anonymous EMNLP reviewers for their time and helpful suggestions that improved the quality of the paper."
        },
        {
            "heading": "A Tokenizations of the PLMs on Social Comments",
            "text": "We conducted an analysis of average token length by tasks of Pre-trained Language Models to provide insights into how different PLMs perform regarding token length across various Vietnamese social media downstream tasks. Figure 2 shows the average token length by Vietnamese social media downstream tasks of baseline PLMs and ours."
        },
        {
            "heading": "B Experimental Settings",
            "text": "Following the hyperparameters in Table 6, we train our pre-trained language model ViSoBERT for Vietnamese social media texts."
        },
        {
            "heading": "C PLMs with Pre-processing Techniques",
            "text": "For an in-depth understanding of the impact of social media texts on PLMs, we conducted an analysis of the test results on various processing aspects. Table 7 presents performances of the pre-trained language models on downstream Vietnamese social media tasks by applying word standardizing pre-processing techniques, while Table 8 presents performances of the pre-trained language models on downstream Vietnamese social media tasks by removing diacritics in all datasets.\nTo emphasize the essentials of diacritics, we conducted an analysis on several data samples by removing 100%, 75%, 50%, and 25% diacritics of total words that included diacritics in each comment of five downstream tasks. Table 8 presents performances of the pre-trained language models on downstream Vietnamese social media tasks by removing diacritics in all datasets."
        },
        {
            "heading": "D PLM-based Features for BiLSTM and BiGRU",
            "text": "We conduct experiments with various models, including BiLSTM and BiGRU, to understand better the word embedding feature extracted from the pre-trained language models. Table 9 shows performances of the pre-trained language model as input features to BiLSTM and BiGRU on downstream Vietnamese social media tasks.\nWe implemented various PLMs when used as input features in combination with BiLSTM and BiGRU models to verify the ability to extract Vietnamese social media texts. The evaluation is conducted on the dev set, and the performance is measured per epoch for downstream tasks. Table 9 shows performances of the PLMs as input features to BiLSTM and BiGRU on the dev set per epoch."
        },
        {
            "heading": "E Updating New Spans of Hate Speech Span Detection Samples with Pre-processing Techniques",
            "text": "Due to performing pre-processing techniques, the span positions on the data samples can be changed. Therefore, we present Algorithm 1, which shows how to update new span positions of samples applied with pre-processing techniques in the Hate Speech Spans Detection task (UIT-ViHOS dataset). This algorithm takes as input a comment and its spans and returns the pre-processed comment and its span along with pre-processing techniques.\nAlgorithm 1 Updating new spans of samples applied with pre-processing techniques in Hate Speech Spans Detection task (UIT-ViHOS dataset).\n1: procedure ALGORITHM(comment, label, delete) 2: assert len(comment) == len(label) 3: new_comment \u2190 [], new_label \u2190 [] 4: for i \u2190 0 to len(comment) do 5: check \u2190 0 6: if comment[i] in emoji_to_word.keys() then 7: if delete then 8: continue 9: for j \u2190 0 to len(emoji_to_word[comment[i]].split(\u2018 \u2019)) do\n10: if label[i] == \u2018B-T\u2019 then 11: if check == 0 then 12: check \u2190 check + 1, new_label.append(label[i]) 13: else 14: new_label.append(\u2018I-T\u2019) 15: else 16: new_label.append(label[i]) 17: new_comment.append(emoji_to_word[comment[i]].split(\u2018 \u2019)[j]) 18: else 19: new_sentence.append(comment[i]) 20: new_label.append(label[i]) 21: assert len(new_comment) == len(new_label) 22: return new_comment, new_label"
        },
        {
            "heading": "F Tokenizations of the PLMs on Removing Diacritics Social Comments",
            "text": "We analyze several data samples to see the tokenized ability of Vietnamese social media textual data while removing diacritics on comments. Table 10 shows several non-diacritics Vietnamese social comments and their tokenizations with the tokenizers of the three best pre-trained language models, ViSoBERT (ours), PhoBERT, and TwHIN-BERT."
        }
    ],
    "title": "ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing",
    "year": 2023
}