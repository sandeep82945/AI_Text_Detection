{
    "abstractText": "The growing number of multimodal online discussions necessitates automatic summarization to save time and reduce content overload. However, existing summarization datasets are not suitable for this purpose, as they either do not cover discussions, multiple modalities, or both. To this end, we present MREDDITSUM, the first multimodal discussion summarization dataset. It consists of 3,033 discussion threads where a post solicits advice regarding an issue described with an image and text, and respective comments express diverse opinions. We annotate each thread with a human-written summary that captures both the essential information from the text, as well as the details available only in the image. Experiments show that popular summarization models\u2014GPT-3.5, BART, and T5\u2014consistently improve in performance when visual information is incorporated. We also introduce a novel method, cluster-based multi-stage summarization, that outperforms existing baselines and serves as a competitive baseline for future work.",
    "authors": [
        {
            "affiliations": [],
            "name": "Keighley Overbay"
        },
        {
            "affiliations": [],
            "name": "Jaewoo Ahn"
        },
        {
            "affiliations": [],
            "name": "Fatemeh Pesaran zadeh"
        },
        {
            "affiliations": [],
            "name": "Joonsuk Park"
        },
        {
            "affiliations": [],
            "name": "Gunhee Kim"
        }
    ],
    "id": "SP:e8aee22852393fbde46b4aa17ef70c746f2864ac",
    "references": [
        {
            "authors": [
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Stanislaw Antol",
                "Margaret Mitchell",
                "C. Lawrence Zitnick",
                "Devi Parikh",
                "Dhruv Batra."
            ],
            "title": "Vqa: Visual question answering",
            "venue": "Int. J. Comput. Vision, 123(1):4\u201331.",
            "year": 2017
        },
        {
            "authors": [
                "Jaewoo Ahn",
                "Yeda Song",
                "Sangdoo Yun",
                "Gunhee Kim."
            ],
            "title": "MPCHAT: Towards multimodal personagrounded conversation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3354\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Adithya Bhaskar",
                "Alexander R. Fabbri",
                "Greg Durrett."
            ],
            "title": "Zero-shot opinion summarization with gpt-3",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023. Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Sumit Bhatia",
                "Prakhar Biyani",
                "Prasenjit Mitra"
            ],
            "title": "Summarizing online forum discussions \u2013 can dialog 3https://www.redditinc.com/policies",
            "year": 2014
        },
        {
            "authors": [
                "Xinlei Chen",
                "Hao Fang",
                "Tsung-Yi Lin",
                "Ramakrishna Vedantam",
                "Saurabh Gupta",
                "Piotr Dollar",
                "C. Lawrence Zitnick"
            ],
            "title": "Microsoft coco captions: Data collection and evaluation",
            "year": 2015
        },
        {
            "authors": [
                "Karan Desai",
                "Gaurav Kaul",
                "Zubin Aysola",
                "Justin Johnson."
            ],
            "title": "Redcaps: Web-curated image-text data created by the people, for the people",
            "venue": "Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Fabbri",
                "Faiaz Rahman",
                "Imad Rizvi",
                "Borui Wang",
                "Haoran Li",
                "Yashar Mehdad",
                "Dragomir Radev."
            ],
            "title": "ConvoSumm: Conversation summarization benchmark and improved abstractive summarization with argument mining",
            "venue": "Proceedings",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Fabbri",
                "Xiaojian Wu",
                "Srini Iyer",
                "Haoran Li",
                "Mona Diab."
            ],
            "title": "AnswerSumm: A manuallycurated dataset and pipeline for answer summarization",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Com-",
            "year": 2022
        },
        {
            "authors": [
                "Tanya Goyal",
                "Junyi Jessy Li",
                "Greg Durrett"
            ],
            "title": "News summarization and evaluation in the era of gpt-3",
            "year": 2022
        },
        {
            "authors": [
                "Mandy Guo",
                "Joshua Ainslie",
                "David Uthus",
                "Santiago Ontanon",
                "Jianmo Ni",
                "Yun-Hsuan Sung",
                "Yinfei Yang."
            ],
            "title": "LongT5: Efficient text-to-text transformer for long sequences",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 724\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Kilem Gwet."
            ],
            "title": "Computing inter-rater reliability and its variance in the presence of high agreement",
            "venue": "The British journal of mathematical and statistical psychology, 61:29\u201348.",
            "year": 2008
        },
        {
            "authors": [
                "Jack Hessel",
                "Ari Holtzman",
                "Maxwell Forbes",
                "Ronan Le Bras",
                "Yejin Choi."
            ],
            "title": "CLIPScore: A reference-free evaluation metric for image captioning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi"
            ],
            "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi."
            ],
            "title": "BLIP: Bootstrapping language-image pretraining for unified vision-language understanding and generation",
            "venue": "Proceedings of the 39th International Conference on Machine Learning, volume 162",
            "year": 2022
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Pengchuan Zhang",
                "Haotian Zhang",
                "Jianwei Yang",
                "Chunyuan Li",
                "Yiwu Zhong",
                "Lijuan Wang",
                "Lu Yuan",
                "Lei Zhang",
                "Jenq-Neng Hwang",
                "KaiWei Chang",
                "Jianfeng Gao."
            ],
            "title": "Grounded language-image pre-training",
            "venue": "Proceedings of the",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Nayu Liu",
                "Xian Sun",
                "Hongfeng Yu",
                "Wenkai Zhang",
                "Guangluan Xu."
            ],
            "title": "Multistage fusion with forget gate for multimodal summarization in open-domain videos",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Bowen Zhou",
                "Cicero dos Santos",
                "\u00c7a\u011flar Gul\u00e7ehre",
                "Bing Xiang."
            ],
            "title": "Abstractive text summarization using sequence-to-sequence RNNs and beyond",
            "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural Lan-",
            "year": 2016
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Shruti Palaskar",
                "Jind\u0159ich Libovick\u00fd",
                "Spandana Gella",
                "Florian Metze."
            ],
            "title": "Multimodal abstractive summarization for how2 videos",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6587\u20136596, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Jielin Qiu",
                "Jiacheng Zhu",
                "William Han",
                "Aditesh Kumar",
                "Karthik Mittal",
                "Claire Jin",
                "Zhengyuan Yang",
                "Linjie Li",
                "Jianfeng Wang",
                "Bo Li",
                "Ding Zhao",
                "Lijuan Wang"
            ],
            "title": "Multisum: A dataset for multimodal summarization and thumbnail generation of videos",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Ramon Sanabria",
                "Ozan Caglayan",
                "Shruti Palaskar",
                "Desmond Elliott",
                "Lo\u00efc Barrault",
                "Lucia Specia",
                "Florian Metze."
            ],
            "title": "How2: a large-scale dataset for multimodal language understanding",
            "venue": "Proceedings of the Workshop on Visually Grounded Interaction",
            "year": 2018
        },
        {
            "authors": [
                "Ka Wong",
                "Praveen Paritosh",
                "Lora Aroyo."
            ],
            "title": "Cross-replication reliability - an empirical approach to interpreting inter-rater reliability",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Nahathai Wongpakaran",
                "Tinakon Wongpakaran",
                "Danny Wedding",
                "Kilem Gwet."
            ],
            "title": "A comparison of cohen\u2019s kappa and gwet\u2019s ac1 when calculating inter-rater reliability coefficients: A study conducted with personality disorder samples",
            "venue": "BMC Medical",
            "year": 2013
        },
        {
            "authors": [
                "Tiezheng Yu",
                "Wenliang Dai",
                "Zihan Liu",
                "Pascale Fung."
            ],
            "title": "Vision guided generative pre-trained language models for multimodal abstractive summarization",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Shiyue Zhang",
                "Asli Celikyilmaz",
                "Jianfeng Gao",
                "Mohit Bansal."
            ],
            "title": "EmailSum: Abstractive email thread summarization",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi"
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "In International Conference on Learning Representations",
            "year": 2020
        },
        {
            "authors": [
                "Junnan Zhu",
                "Haoran Li",
                "Tianshang Liu",
                "Yu Zhou",
                "Jiajun Zhang",
                "Chengqing Zong."
            ],
            "title": "MSMO: Multimodal summarization with multimodal output",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With the increased popularity of online discussion forums like Reddit, discussion threads that consist of a post and various numbers of comments, have quickly accumulated. It thus becomes overwhelming for users to sift through the threads to find the information they seek, which in turn has led to the development of automated means for text-only discussion summarization (Bhatia et al., 2014; Fabbri et al., 2021, 2022).\nHowever, discussion threads are often multimodal, containing visuals in addition to text. This added modality cannot be ignored, as it plays a key role in the respective discussions. For example, in Table 1, the image of the couch is essential for discussing which coffee table would go well with it.\n*Equal contribution.\nYet, multimodal summarization has so far been limited to news and instructional domains (Zhu et al., 2018; Sanabria et al., 2018; Palaskar et al., 2019; Liu et al., 2020) that are not easily transferable to online discussions surrounding images.\nTo fill the gap, we tackle multimodal discussion summarization. In particular, we consider Reddit discussion threads in which the post solicits advice regarding an issue described with an image and text, and commenters offer opinions, as opposed to simple reactions or jokes. Here, the goal is to generate an abstractive summary faithfully capturing the information from the post\u2014both image and text\u2014and comments. This task is especially challenging because along with the need to effectively process the multimodal input, a quality summary\nmust provide good coverage of commenters\u2019 varying perspectives and opinions without redundancy.\nTo facilitate research in this direction, we contribute the Multimodal Reddit Summarization (MREDDITSUM) dataset, consisting of 3,033 Reddit discussion threads containing posts (text and image) and comments (text-only), each accompanied by a human-written summary, as shown in Table 1. We carefully select subreddits with discussions surrounding an image, and collect summaries that not only summarize the text, but also make reference to relevant information present only in the image. See Appendix C for more examples.\nWe also propose cluster-based multi-stage summarization (CMS), a novel method to summarize multimodal discussions. It processes discussions in three stages: (i) comments are first clustered by similarity, (ii) each cluster is summarized in a sentence, and (iii) the cluster-summaries are summarized. Experiments show that CMS consistently outperforms popular large language models (LLMs) for summarization\u2014GPT-3.5 (Brown et al., 2020), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020). Also, incorporating image information, either as a dense vector or in text caption, consistently boosts the performance.\nOur main contributions are as follows:\n\u2022 We present MREDDITSUM, the first multimodal discussion summarization dataset with human-written summaries with essential information from both the text and the image. Table 2 presents comprehensive comparison with existing summarization datasets.\n\u2022 We propose cluster-based multi-stage summarization (CMS), a novel method to summarize multimodal discussions outperforming competitive baselines like GPT-3.5, BART and T5, as well as their vision-guided variations."
        },
        {
            "heading": "2 Related Work",
            "text": "Discussion Thread Summarization. Despite the prevalence of discussion threads online, it has been understudied for summarization. This is likely due to the fact that it can be a difficult target for extractive summarization, though there has been small extractive summarization dataset (Bhatia et al., 2014).\nMore recently, several abstractive summarization datasets have been proposed. ConvoSumm (Fabbri et al., 2021) presented a dataset of 2000 summarized forum threads, 500 from each of 4\ndifferent domains including NYT articles, Reddit, StackExchange, and Email threads. AnswerSumm (Fabbri et al., 2022) is another dataset consisting of 4,631 question-answering discussion threads sourced from StackExchange. AnswerSumm shares the most similarities with our dataset, as they also summarize multi-speaker threads, and their annotation pipeline shares some similarities with ours. They also cluster the comments and summarize these groups before going through a final summary editing process, similar to our pipeline. The key differences between this dataset and ours is that AnswerSumm is only text-based with no images and operates in a different domain, as they are all question-answering threads from StackExchange. In contrast, our dataset includes both images and text, and focuses on Reddit threads where the images play a key role. Additionally, in our annotation pipeline we summarize the original post and image as well, which to our knowledge has not been done in any other forum summarization dataset. This is useful because often the posts alone have unclear intent that may require context derived from the image or forum domain itself.\nOther related summarization datasets include multi-turn datasets such as SamSUM (Gliwa et al., 2019), consisting of chat-like dialogues and humanannotated summaries, and EmailSum (Zhang et al., 2021), containing work-related emails and both long and short reference summaries.\nOverall, though there is a small variety of existing discussion thread summarization datasets, they are all currently only text-based and none of these tackle both original post and thread summarization.\nMultimodal Summarization. Though other multimodal research areas such as VQA (Agrawal et al., 2017) and text-image pretraining (Radford et al., 2021; Li et al., 2022a, 2023) have been gaining attention in recent years, there only exist a small handful of works that address multimodal summarization, which aims to generate a summary that includes salient information from inputs with multiple modalities. For example, MSMO (Zhu et al., 2018; Qiu et al., 2023), Multimodal Summarization with Multimodal Outputs, takes inputs of various modalities and outputs both a text-based summary and a representative image.\nHowever, our task aims to generate a unimodal output\u2014that is, a purely textual summary. This is similar to the multimodal summarization done on the How2 Dataset (Sanabria et al., 2018; Palaskar\net al., 2019), where a textual transcript of the video along with the video frames are generated into a text summary. (Yu et al., 2021) reported that incorporating the additional modality of the video frames into their summarization models showed improvement compared to text-only based models. Though this multimodal summarization task is the most similar to ours, there are some key differences. The How2 dataset uses short video captions as pseudo-summaries, instead of detailed humanannotated summaries like we curate for MREDDITSUM. Additionally, our text is a rich multi-speaker discussion, rather than a transcript of audio. Finally, MREDDITSUM\u2019s threads are specifically selected to include images where their information is necessarily included in the summary, whereas there is no such assurance for How2\u2019s videos."
        },
        {
            "heading": "3 The MREDDITSUM Dataset",
            "text": ""
        },
        {
            "heading": "3.1 Data Preparation",
            "text": "To construct a meaningful multimodal discussion summarization dataset, we imposed three major criteria when selecting Reddit threads.\nCriterion 1. The discussion thread needs to contain an image. Since Reddit does not allow images embedded in comments, this means that the post needs to contain an image.\nCriterion 2. The discussion needs to be centered around an image in such a way that the information only available from the image plays a key role in the discussion. In some threads, the image does not provide any information, e.g. it is a favorite character of the original poster. In such cases, simply\nsummarizing the text is sufficient, and a multimodal model is unnecessary.\nCriterion 3. The discussion needs to be meaningfully summarizable. Many Reddit threads that include images are meant to incite reactions from other users, or to be shared in a jocular manner. On the other hand, some threads consist of the post clearly asking for advice or opinions, thereby eliciting diverse responses from a number of commenters. Summarizing these opinions along with the advice would be helpful for readers to understand the gist of the threads.\nSources. Given the aforementioned criteria, we identify 9 subreddits, e.g. r/fashionadvice (See Appendix A for the complete list), which consist primarily of image-based posts where the original poster is soliciting advice or opinions about either clothing or interior design. We collect all threads from these subreddits with over 5 comments from years 2015-2022. Collection is done with RedCaps (Desai et al., 2021) modified to collect all comments from each thread. We additionally follow similar preprocessing steps (Ahn et al., 2023), removing all posts that contain NSFW content or images with faces. We also remove any comments with NSFW content, and comments posted by bots. All responses to these removed comments are also removed. We replace URL\u2019s with the \u201c[URL]\" token."
        },
        {
            "heading": "3.2 Annotation",
            "text": "We then annotate the data after selecting qualified workers from Amazon Mechanical Turk (MTurk). We limit our workers to those from English-\nspeaking countries with a HIT approval rate over 98%, with greater than 5000 HITs approved. For all tasks, workers are required to pass a qualification task where the results are manually checked for quality. Any workers who are found to submit lowquality work have their qualification revoked. In total, a pool of 40 annotators were qualified to perform annotation tasks. As each task was performed separately, it was possible for multiple annotators to contribute to a single summary. so Additional detail on the annotation interface and instructions are found in Appendix B. The annotation is conducted in a 3-step annotation pipeline as follows.\nStep 1: Original Post Summarization. In the first step, we present workers with the original post along with the image from that post. We ask the annotators to summarize in a single sentence the intent of the original poster, as well as the most relevant details from the image. We use this method because a post that simply reads \"blue or black?\" may only be comprehensible when paired with the image of blue and black heels next to a blue dress, and a true text-only summary should be comprehensible without the image. Our summary may then read \"The original poster asked if blue or black heels would match better with a strapless, kneelength blue dress,\" eliminating the need of the image to comprehend the question. In this way, all information necessary to understand the question should be self-contained within the summary.\nStep 2: Comment Cluster Summarization. We first cluster the comments to identify groups of comments that share a similar opinion. We follow the method described in AnswerSUMM (Fabbri et al., 2022) to allow for clusters of varying sizes and number. We use a RoBERTa-based model fine-tuned for semantic similarity to get sentence embeddings of the top-level comments from each thread. We then use agglomerative clustering with average linkage, cosine distance, and a maximum distance of 0.5 to generate clusters of comments.\nWe then rank the comment clusters according to their size and Reddit score. We take the sum of all Reddit scores of the top-level comments in a single cluster as a saliency score of the cluster. We select the top 5 clusters with the highest saliency scores and use these for annotation. We do this to limit the size of the summary and to help remove irrelevant comments, while encouraging larger clusters of comments with a similar sentiment.\nWe then present these groups of comments to an-\nnotators along with the original post and image, and ask them to summarize within one or two sentences the main opinions present in each group of comments. We encourage the annotators to reference objects or details from the image when necessary. For consistency, we instruct the annotators to refer to the commenters as \"Commenters\" as opposed to people, users, or other words.\nStep 3: Summary Synthesis. Finally, we concatenate the original post summary as well as the comment cluster summaries, in descending order of their saliency-scores. We then present these summaries once more to annotators and ask them to edit them for fluency and readability. We encourage annotators to reduce repetitive wording, add connectives between sentences, and to rearrange sentences so that related topics are next to each other and the overall summary reads as more natural. We also ensure all summaries are written entirely in the past-tense."
        },
        {
            "heading": "3.3 Dataset Analyses",
            "text": "Statistics. The resulting dataset contains a total of 3,033 posts and summaries. We split these into a train, test, and validation set of sizes 2729, 152, and 152, respectively. We present further statistics in Table 2, where we compare with similar summarization datasets from a few different domains. The average summary length for MREDDITSUM is longer than other datasets; however, this is not surprising given the nature of summarizing varying opinions, of which there could be many. Additionally, we describe the structure-level statistics in Table 3; note that while the average length of the Original Post summary is longer than the document, this is due to the additional image description and context. For the full thread, the summary is 13.2% as long as the input on average, which is comparable to SamSUM\u2019s 19% and How2\u2019s 11.3%.\nSummary Quality. In order to ensure the quality summary, we additionally performed an exper-\niment to rate the annotated summaries out of 3 metrics. Following a closely related work, SamSUM (Gliwa et al., 2019), we randomly selected 100 thread-summary pairs and had 2 independent judges from MTurk grade them on a quality scale of -1, 0, or 1, where -1 means a summary is irrelevant or does not make sense, 0 means a summary extracts only part of the relevant information or makes some mistakes, and 1 means a summary that is understandable and delivers a brief overview of the relevant information from the thread. We asked annotators to score summaries on overall quality, fluency, and faithfulness, similar to our human annotation found in Sec 5.2. We found the scores were highly positive, with average scores of 0.95, 0.96, 0.83 for overall quality, fluency, and faithfulness respectively. Additionally, we found Gwet\u2019s AC1 agreement scores of .91, 0.89, and 0.53, corresponding to high, high, and moderate agreement, respectively. Note that we used the Gwet\u2019s AC1 score for interannotator agreement as it performs well despite class imbalances where agreement is high. (Gwet, 2008; Wongpakaran et al., 2013; Wong et al., 2021)\nAbstractiveness. Extractive-Oracle ROUGE scores in Table 4 show that our dataset is similar in abstractiveness to other multi-turn datasets, and much more abstractive than DailyMail. Though scores are not available for MSMO, it is expected that the scores would be similar to DailyMail.\nRelatedness between Text and Images. We also calculate the CLIPScore (Hessel et al., 2021), a metric that measures the correlation between text and an image, to determine how grounded our summaries are to the images from each thread. Our summaries have an average CLIPScore of 74.62, the post summaries alone achieve 74.89, and the comment cluster summaries alone score 68.34. These suggest our summaries, especially the post summaries, are well-correlated with the images."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Task Description",
            "text": "We consider the multimodal summarization task, where the input includes all original text and the image and the output is a text-only summary that describes both the document and image. The text includes the post and comments, and the goal is to accurately summarize both the original poster\u2019s intent and commenters\u2019 opinions. For this task, we format the text input as the following: \"Orig-\ninal Post: Original Post\", with \"Image: Image Caption.\" appended for models that include image captions. We then additionally append the comments in the form \"User 1: Comment 1. User 2: Comment 2. ...\", where each username has been anonymized. Comments are listed in the order that they are scraped from Reddit in. The target output is the result of our final summary."
        },
        {
            "heading": "4.2 Evaluation Metrics",
            "text": "Following the standard metric for summarization evaluation, we use the ROUGE (Lin, 2004) and BertScore (Zhang* et al., 2020). ROUGE1 measures the salience of model-generated summaries by comparing n-grams in the generated summary and gold summary. We consider three variants: ROUGE-1/2 (R1/2) measure the unigram/bigram overlap, and ROUGE-L (RL) determines the longest common subsequence between summaries. BertScore2 computes a soft token similarity using contextual embeddings output from BERT, instead of word matches. We use the default RoBERTa-large model and rescale with baseline."
        },
        {
            "heading": "4.3 Baseline Models",
            "text": "We consider four text-only baseline models: GPT3.5 (zero-shot), BART, T5, and LongT5 (finetuned), as well as their extensions to make use of image information, either as image captions or embeddings.\nExtractive Baselines (Lead-1, Lead-Comment, Ext-Oracle). Lead-1 uses the first sentence from the document as the summary, and Lead-Comment uses the leading top 5 comments from the thread. Ext-Oracle extracts passages from the document to achieve the maximum possible ROUGE score, and\n1https://github.com/pltrdy/rouge 2https://github.com/Tiiiger/bert_score\nthus is the highest possible performance from an extractive model.\nText-only Baselines (GPT-3.5, BART, T5, LongT5). GPT-3.5 (Ouyang et al., 2022) is an LLM that has shown excellent zero-shot performance in summarization tasks (Goyal et al., 2022; Bhaskar et al., 2023). We use the largest model, text-davinci-003, through the OpenAI API, with the prompt \"Summarize what the original post was asking about and the general opinions of the commenters.\", which is determined empirically to perform well and closely mimic the instructions given to annotators. We also evaluate three finetuned models, BART-base (Lewis et al., 2020), and T5-base (Raffel et al., 2020), which are highperforming LLMs with good summarization abilities, as well as LongT5-base (Guo et al., 2022) which is an extension of T5 that is capable of handling longer input sequences. We pretrain them on the CNN/DailyMail (Nallapati et al., 2016) summarization dataset before fine-tuning it for our task.\nExtensions with Image Captioning (GPT3.5ImgCap, BART-ImgCap, T5-ImgCap, LongT5ImgCap). We extend the text-only baselines to incorporate visual information through the use of an image caption, denoted as GPT3.5-ImgCap, BARTImgCap, T5-ImgCap, and LongT5-ImgCap respectively. They take advantage of powerful LLMs without large amounts of multimodal training to understand visual features. For image captions, we use the BLIP2 model (Li et al., 2023) trained on COCO image captions (Chen et al., 2015) and generate multiple image captions for each image using nucleus sampling. Since a more detailed and grounded image caption that describes concrete objects is best for this task, we use a image-grounding model, GLIP (Li et al., 2022b), to score each caption by grounding it with the image, and calculate how many image-text grounded pairs are above a threshold of 0.7. We then select the image caption with the highest score and append the caption to the input after the original post. We then fine-tune BART-ImgCap, T5-ImgCap, and LongT5-ImgCap as described above; for GPT3.5-ImgCap, we use the caption-appended prompt.\nExtensions with Vision-Guidance (VG-BART, VG-T5). Vision-Guided BART and T5 are presented in (Yu et al., 2021) for multimodal summarization. They include additional visual layers that receive video embeddings as input, and show stateof-the-art performance in multimodal summariza-\nCluster Summarization\nComment Clustering\nCluster Sum Summarization\nOP: Built in and we\u2019re not allowed to paint it..\nCommenters suggested adding a lot of plants to the shelves.\nCommenters suggested adding peel-and-stick wallpaper to the shelf.\nUser 1: Fill it up with plants User 2: Yes. Turn it into a big plant stand\u2026 User 3: Another vote for plants and bobbles. User 6: You could put removable wallpaper or contact paper up on the back wall...\nUser 1: Fill it up with plants User 2: Yes. Turn it into a big plant stand\u2026\nUser 6: You could put Removable wallpaper Or contact paper up... User 30: Definitely look into contact paper ...\n...\nThe OP asked how they can make a built-in wooden pantry cabinet look less overwhelming. Most commenters suggested adding a lot of plants to the shelves and offered different ways to redecorate the cabinet. Others suggested adding peeland-stick wallpaper to the shelf. \u2026\nOP: Built in and we\u2019re not allowed to paint it..\nOP: Built in and we\u2019re not allowed to paint it..\nComments\nI OP ComClus1 ComClusN OP I\u2026\nI OP ClusSum1 ClusSumN\u2026\nFinal Summary\nFigure 1: An illustration of Cluster-based Multi-stage Summarization (CMS): (1) comments are first clustered by similarity, (2) each cluster is summarized in a sentence, and (3) the cluster-summaries are summarized. (OP: the original post / I: the post image / ComClusk: the k-th comment cluster / N: the number of comment clusters / ClusSumk: the generated summary of the k-th comment cluster)\ntion for the How2 dataset. We modify the original models by instead using 768-D ViT-base (Dosovitskiy et al., 2021) image embeddings as input, as they have shown excellent performance as an image backbone. We use cross-modal dot product attention with a forget-gate and image transformer, as this version performed best in our experiments. We use the same T5-base and BART-base pretrained on CNN/DM to initialize the encoder and decoder. For VG-BART, we pretrain the visual layers using the COCO image captions before fine-tuning on our dataset; VG-T5 shows no performance increase from visual pretraining, so we initialize its layers from scratch."
        },
        {
            "heading": "4.4 Cluster-based Multi-stage Summarization",
            "text": "One challenge in summarizing discussions is that they can be very long. To confirm that this is causing an issue, we conduct a preliminary experiment on the fine-tuned BART model by comparing the results of two different test subsets: the long subset with more than 22 turns and the short subset with\nless than or equal to 22 turns. The performance on the long subset is noticeably worse than that on the short subset, lower by 4.95 ROUGE-1 and 6.1 BertScore.\nTo effectively handle this challenge, we present a novel method named cluster-based multi-stage summarization (CMS), consisting of three stages (See Figure 1):\n1. Comment Clustering. Similar comments are clustered using RoBERTa sentence embedding and agglomerative clustering.\n2. Cluster Summarization. Each cluster is summarized in about a sentence using an LLM with image captioning, or a vision-guided LLM, such as VG-BART or VG-T5.\n3. Cluster-summary Summarization. The cluster summaries are concatenated and further reduced into a coherent summary using a separate model, which is either an LLM with image captioning or a vision-guided LLM."
        },
        {
            "heading": "4.5 Implementation Details",
            "text": "The fine-tuned models are trained for 50 epochs on a single Titan RTX GPU for the BART and T5 models. We use a batch size of 4, and following (Yu et al., 2021; Raffel et al., 2020; Lewis\net al., 2020), we use learning rates 3e-5 to finetune the pre-trained parts of model weights, and 1.5e-4 to train the newly added visual layers in VG-BART and VG-T5. The decoding process uses beam-search with a size of 5. The average training time for BART, T5, BART-Cap, and T5-Cap is approximately 5 hours; the average training time for VG-BART and VG-T5 is about 8 hours, with the additional visual layers adding about 100 million extra parameters. We use the same training epochs, batch size, learning rates, and beam-search size for cluster-based multi-stage summarization. All results shown are an average of two runs."
        },
        {
            "heading": "5 Results and Analysis",
            "text": "Table 5 shows the results of all models evaluated across the test set. We see that our model, Clusterbased Multi-stage Summarization (CMS), outperformed baseline models for all metrics across both T5 and BART-based models. We believe this is due to our models\u2019 ability to better handle the long length of input threads, even outperforming LongT5 models; see \u00a7 D.1 for more detailed analysis. In general across all model types, models that contain image information through an image caption outperform those that only have access to textinformation. This supports that our dataset requires multimodal understanding in order to perform well on the summarization task. To confirm this, we additionally computed the CLIPScore between the image and the first sentence fo the generated summaries, which corresponds to the post summary and is where most of the image information is found. The results in Table 6 support that our methods incorporate more image information compared to a non-visual baseline. Vision-Guided models using text embeddings showed mixed results, with a marginal or no improvement over text-only models; we believe this to be due to a limitation of these models to effectively incorporate image information. Though they show strong performance on the How2 summarization task (Yu et al., 2021), mRedditSum has longer input and summary length, images, and fewer documents, likely contributing to the performance differences. Additionally, we note that T5 models show the best performance, followed by BART models and GPT3.5 models. For GPT3.5 models, we note that the low scores are likely due to inconsistencies in summary format, length, and detail, due to the zero-shot setting, but still receive relatively reasonable BertScore scores.\nWe provide further analyses on the effect of input length and subreddit category on performance in \u00a7 D."
        },
        {
            "heading": "5.1 Qualitative Analysis",
            "text": "In addition to our automatic evaluation, we check the test results manually for qualitative analysis. Several results can be found in Table 7. The primary advantage of our method, CMS, is that it has a greater coverage of relevant opinions compared to the baseline models. It is better able to filter out irrelevant or strange comments, while keeping the important opinions and including ones that are presented late in the thread.\nWe also find that all models, even those incorporating image information, are still prone to hallucinations of what is in the image. These include incorrect descriptions of object color and style, as well as describing objects that are not present in the image at all. Though our multimodal models are generally better at incorporating visual details than text-only models, their power to interpret the image seems still limited; we believe this to be due to potential undertraining of the text-vision fusion layers in the VG models, and the limitations of image caption models.\nThus, while our CMS model can overcome one weakness of the baseline multimodal summarization models, we still believe there to be significant room for improvement in the field of multimodal models, and hope that MREDDITSUM can help facilitate such research."
        },
        {
            "heading": "5.2 Human Evaluation",
            "text": "We additionally perform human evaluation studies via MTurk to compare the summaries generated from CMS-T5-ImgCap (ours) versus the baseline T5-ImgCap model. Based on similar works such as (Zhang et al., 2021), we use three metrics to measure the summary quality: fluency, faithfulness, and overall quality. Fluency measures which is\nmore naturally written, faithfulness measures how truthful the summary is to the document, and the overall quality represents general user preference. We randomly sample 25 datapoints from the test set and receive 3 annotations per sample. We note that this limited number of datapoints is due to the fact that this evaluation task is highly challenging for human annotators, given that the input, including the original post, threads, and image, is long and complex.\nFigure 2 shows the majority vote results that our summaries are overall more preferable in terms of fluency and overall quality, with similar performance for faithfulness. We believe this to be an indicator of our model\u2019s better ability to effectively summarize the thread, offering more fluent and higher-quality summaries. The similar faithfulness scores are likely due to our method sharing the same base model and image caption as the baseline, T5-ImgCap, granting a similar ability to incorporate correct image and text information."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "Online discussions are increasingly becoming multimodal, yet there are not sufficient resources for summarizing them. To this end, we presented MREDDITSUM, the first multimodal discussion summarization dataset containing 3,033 discussion threads and images with human-written summaries. Threads were carefully chosen so that the images play a key role in the respective threads, and summaries were written to capture this. Experiments showed that summarization models making use of visual information consistently outperform those that do not. Additionally, we introduced Cluster-based Multi-stage Summarization, which accounted for the structure of discussion thread\ndata and outperformed baseline methods. We hope this dataset will help to facilitate active research on multimodal discussion summarization.\nLimitations\nAs with any dataset, there are some limitations to MREDDITSUM. Though it is of comparable size to many other summarization datasets, the relatively small size of the dataset makes it hard to utilize without significant pretraining, thus limiting the use of the dataset to those with access to large-scale pretraining datasets or pretrained models.\nMREDDITSUM only includes Reddit threads with single images, as opposed to multiple images or videos. There is thus still room for improvement for multimodal summarization to additionally consider these threads.\nFurthermore, our dataset considers only Englishlanguage threads from a single forum, Reddit, and a limited number of subreddits. There thus may be some additional bias due to the relatively small domain and raw nature of the dataset.\nFor our cluster-based multi-stage summarization method, one limitation is the need to train an extra model in addition to the base summarization model. As a result, our method incurs some computational overhead. However, it is worth noting that both the training and inference processes can\nbe accommodated within a single Titan RTX GPU.\nEthics Statement\nAs we propose a novel multimodal dataset, there are ethical considerations about the use of the data.\nPrivacy. All data are sourced from Reddit, which is publicly available. Following Desai et al. (2021); Ahn et al. (2023), additional measures have been taken to address privacy considerations. This includes the exclusion of images or discussions with clear identifying information, such as names or faces. Additionally, posts that are removed by their authors from Reddit also render the image unavailable for our dataset, as we only provide the links to the images. Thus, any users who are concerned about their post being in the dataset may easily remove it from the dataset by deleting it from Reddit.\nBias. As all data are sourced from real discussions on a public forum, there may be biases within the discussions due to the demographics of the Reddit users. Though we use a NSFW filter to remove inappropriate words, and look at each datapoint by hand to further filter out any harmful or inappropriate images or discussions, it is possible a few may still be present in the dataset. Less obvious bias such as stereotyping based on gender, etc. may also still be present in the dataset.\nIntended Use. The MREDDITSUM dataset is intended to be used for research purposes only, and the use is subject to the the Reddit User Agreement, Privacy Policy, and Content Policy3.\nAnnotator Compensation. We ensured that our annotators were paid a fair wage of approximately USD $16/hour, which is greater than the minimum wage in all countries that we recruited annotators from: The United States, Canada, Australia, New Zealand, and Great Britain. The time to complete each task was determined by running multiple trials with researchers, and the payment per task was calculated from this time. The cost per datapoint was approximately $3.50, with some longer datapoints costing more to compensate for the extra annotation time."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was financially supported by SNUNAVER Hyperscale AI Center, as well as the Institute of Information & Communications Technology Planning & Evaluation (IITP) grants funded by the Korea government (MSIT) (No. 2019-0-01082, SW StarLab and NO. 2021-0-01343, Artificial Intelligence Graduate School Program (Seoul National University)), and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2023R1A2C2005573). Joonsuk Park and Gunhee Kim are the corresponding authors."
        },
        {
            "heading": "A Subreddits Used",
            "text": "We list all subreddits used for data collection, along with the number of threads from each present in the final dataset in Table 8."
        },
        {
            "heading": "B Annotation Interface",
            "text": "We listed a total of 3 tasks on Amazon Mechanical Turk for our data pipeline. We informed all annotators that this data would be used to help in summarizing Reddit threads, asked them to agree with the Reddit Terms of Use before participating, and notified them that participating in the HIT constituted acceptance of these terms of use.\nWe provided annotators with detailed instructions for the task and several acceptable and unacceptable examples to help them perform the task. In Figure 6, we show the instructions provided for Task 1; similar instructions were used in the other two tasks. Additionally, we show the annotation interface used for Tasks 2 and 3 in Figure 7 and Figure 8."
        },
        {
            "heading": "C Additional Sample Data",
            "text": "We show a few additional data points from the MREDDITSUM dataset in Table 9 and Table 10. Table 9 shows a datapoint from the fashion category, whereas Table 10 shows a datapoint from the interior design category."
        },
        {
            "heading": "D Further Analyses",
            "text": "D.1 Summarization based on the Length of Input Threads\nTo better understand whether CMS effectively handles long inputs, we run a further analysis using BART-based models (see Figure 3). As the number of comments increases, the R1 score consistently\ndecreases. This indicates that summarization indeed becomes more challenging when the input length is longer. However, the performance gap between the baseline models (i.e., BART, BARTImgCap) and the CMS-BART-ImgCap generally increases as the number of comments grows, supporting the idea that CMS better handles longer threads. As our model generates cluster summaries in stage 1, it reduces the average input length by 82.8%, and thus achieving better performance even on relatively challenging long inputs. We also provide results from T5-based models in Figure 4, showing similar trends; the gap between the baseline models and the CMS-T5-ImgCap is large when the number of comments falls within the range of [15,20) and [20,25).\nD.2 Summarization per Subreddit\nWe further explore the summarization across 9 different subreddits, as shown in Figure 5.\nThe results reveal that subreddits within the \u2018Interior\u2019 category (i.e., the left three subreddits in Figure 5) exhibit lower ROUGE scores in compar-\nison to subreddits within the \u2018Clothes\u2019 category (i.e., the right six subreddits in Figure 5). This discrepancy can be attributed to the difference in the input lengths across each subreddit. Given that the average input length of examples from the \u2018Interior\u2019 category exceeds that of examples from the \u2018Clothes\u2019 category, it is more difficult for our model to summarize the former. Additionally, we can also explain this gap by comparing the difference between domains. Specifically, while the model can easily comprehend clothing images by focusing on only salient objects, comprehending interior images is more challenging as it necessitates a broader range of information (e.g., wall color, spatial relationship between furniture, etc). Consequently, summarizing examples from the \u2018Interior\u2019 category proves to be more challenging for the models than summarizing examples from the \u2018Clothes\u2019 category."
        }
    ],
    "title": "MREDDITSUM: A Multimodal Abstractive Summarization Dataset of Reddit Threads with Images",
    "year": 2023
}