{
    "abstractText": "Most research on multimodal open-domain dialogue agents has focused on pretraining and multi-task learning using additional rich datasets beyond a given target dataset. However, methods for exploiting these additional datasets can be quite limited in real-world settings, creating a need for more efficient methods for constructing agents based solely on the target dataset. To address these issues, we present a new learning strategy called visionlanguage warm-up tasks for multimodal dialogue models (VLAW-MDM). This strategy does not require the use of large pretraining or multi-task datasets but rather relies solely on learning from target data. Moreover, our proposed approach automatically generates captions for images and incorporates them into the model\u2019s input to improve the contextualization of visual information. Using this novel approach, we empirically demonstrate that our learning strategy is effective for limited data and relatively small models. The result show that our method achieved comparable and in some cases superior performance compared to existing state-of-the-art models on various evaluation metrics. The code is available at https: //github.com/BeneciaLee/VLAW-MDM",
    "authors": [
        {
            "affiliations": [],
            "name": "Jaewook Lee"
        },
        {
            "affiliations": [],
            "name": "Seongsik Park"
        },
        {
            "affiliations": [],
            "name": "Seong-heum Park"
        },
        {
            "affiliations": [],
            "name": "Hongjin Kim"
        },
        {
            "affiliations": [],
            "name": "Harksoo Kim"
        }
    ],
    "id": "SP:3ababaa0ba30ef3cd5ad09245566d8dbbc9e2428",
    "references": [
        {
            "authors": [
                "Daniel Adiwardana",
                "Minh-Thang Luong",
                "David R So",
                "Jamie Hall",
                "Noah Fiedel",
                "Romal Thoppilan",
                "Zi Yang",
                "Apoorv Kulshreshtha",
                "Gaurav Nemade",
                "Yifeng Lu"
            ],
            "title": "Towards a human-like open-domain chatbot",
            "venue": "arXiv preprint arXiv:2001.09977",
            "year": 2020
        },
        {
            "authors": [
                "Peter Anderson",
                "Xiaodong He",
                "Chris Buehler",
                "Damien Teney",
                "Mark Johnson",
                "Stephen Gould",
                "Lei Zhang"
            ],
            "title": "Bottom-up and top-down attention for image",
            "year": 2018
        },
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "Vqa: Visual question answering",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433.",
            "year": 2015
        },
        {
            "authors": [
                "Xinlei Chen",
                "Hao Fang",
                "Tsung-Yi Lin",
                "Ramakrishna Vedantam",
                "Saurabh Gupta",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick."
            ],
            "title": "Microsoft coco captions: Data collection and evaluation server",
            "venue": "arXiv preprint arXiv:1504.00325.",
            "year": 2015
        },
        {
            "authors": [
                "Yen-Chun Chen",
                "Linjie Li",
                "Licheng Yu",
                "Ahmed El Kholy",
                "Faisal Ahmed",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu."
            ],
            "title": "Uniter: Universal image-text representation learning",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, Au-",
            "year": 2020
        },
        {
            "authors": [
                "Abhishek Das",
                "Satwik Kottur",
                "Khushi Gupta",
                "Avi Singh",
                "Deshraj Yadav",
                "Jose MF Moura",
                "Devi Parikh",
                "Dhruv Batra."
            ],
            "title": "Visual dialog",
            "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1080\u20131089. IEEE Computer",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Hao Cheng",
                "Hao Fang",
                "Saurabh Gupta",
                "Li Deng",
                "Xiaodong He",
                "Geoffrey Zweig",
                "Margaret Mitchell."
            ],
            "title": "Language models for image captioning: The quirks and what works",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association",
            "year": 2015
        },
        {
            "authors": [
                "Jeffrey Donahue",
                "Lisa Anne Hendricks",
                "Sergio Guadarrama",
                "Marcus Rohrbach",
                "Subhashini Venugopalan",
                "Kate Saenko",
                "Trevor Darrell."
            ],
            "title": "Long-term recurrent convolutional networks for visual recognition and description",
            "venue": "Proceedings of the IEEE",
            "year": 2015
        },
        {
            "authors": [
                "Hao Fang",
                "Saurabh Gupta",
                "Forrest Iandola",
                "Rupesh Srivastava",
                "Li Deng",
                "Piotr Dollar",
                "Jianfeng Gao",
                "Xiaodong He",
                "Margaret Mitchell",
                "John Platt"
            ],
            "title": "From captions to visual concepts and back",
            "year": 2015
        },
        {
            "authors": [
                "Yuheng Hu",
                "Lydia Manikonda",
                "Subbarao Kambhampati."
            ],
            "title": "What we instagram: A first analysis of instagram photo content and user types",
            "venue": "Proceedings of the international AAAI conference on web and social media, volume 8, pages 595\u2013598.",
            "year": 2014
        },
        {
            "authors": [
                "Da Ju",
                "Kurt Shuster",
                "Y-Lan Boureau",
                "Jason Weston."
            ],
            "title": "All-in-one image-grounded conversational agents",
            "venue": "arXiv preprint arXiv:1912.12394.",
            "year": 2019
        },
        {
            "authors": [
                "Douwe Kiela",
                "Suvrat Bhooshan",
                "Hamed Firooz",
                "Ethan Perez",
                "Davide Testuggine."
            ],
            "title": "Supervised multimodal bitransformers for classifying images and text",
            "venue": "arXiv preprint arXiv:1909.02950.",
            "year": 2019
        },
        {
            "authors": [
                "Hung Le",
                "Steven CH Hoi."
            ],
            "title": "Video-grounded dialogues with pretrained generation language models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5842\u20135848.",
            "year": 2020
        },
        {
            "authors": [
                "Gen Li",
                "Nan Duan",
                "Yuejian Fang",
                "Ming Gong",
                "Daxin Jiang."
            ],
            "title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pretraining",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11336\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi."
            ],
            "title": "Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597.",
            "year": 2023
        },
        {
            "authors": [
                "Margaret Li",
                "Stephen Roller",
                "Ilia Kulikov",
                "Sean Welleck",
                "Y-Lan Boureau",
                "Kyunghyun Cho",
                "Jason Weston."
            ],
            "title": "Don\u2019t say that! making inconsistent dialogue unlikely with unlikelihood training",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "Yan Ling",
                "Jianfei Yu",
                "Rui Xia."
            ],
            "title": "Visionlanguage pre-training for multimodal aspect-based sentiment analysis",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2149\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Mateusz Malinowski",
                "Mario Fritz."
            ],
            "title": "A multiworld approach to question answering about realworld scenes based on uncertain input",
            "venue": "Advances in neural information processing systems, 27.",
            "year": 2014
        },
        {
            "authors": [
                "Nasrin Mostafazadeh",
                "Chris Brockett",
                "William B Dolan",
                "Michel Galley",
                "Jianfeng Gao",
                "Georgios Spithourakis",
                "Lucy Vanderwende."
            ],
            "title": "Image-grounded conversations: Multimodal context for natural question and response generation",
            "venue": "Proceedings of",
            "year": 2017
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Arijit Ray",
                "Gordon Christie",
                "Mohit Bansal",
                "Dhruv Batra",
                "Devi Parikh."
            ],
            "title": "Question relevance in vqa: Identifying non-visual and false-premise questions",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 919\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Stephen Roller",
                "Emily Dinan",
                "Naman Goyal",
                "Da Ju",
                "Mary Williamson",
                "Yinhan Liu",
                "Jing Xu",
                "Myle Ott",
                "Eric Michael Smith",
                "Y-Lan Boureau"
            ],
            "title": "Recipes for building an open-domain chatbot",
            "venue": "In Proceedings of the 16th Conference of the European",
            "year": 2021
        },
        {
            "authors": [
                "Sheng Shen",
                "Liunian Harold Li",
                "Hao Tan",
                "Mohit Bansal",
                "Anna Rohrbach",
                "Kai-Wei Chang",
                "Zhewei Yao",
                "Kurt Keutzer"
            ],
            "title": "How much can clip benefit vision-and-language tasks? arXiv preprint arXiv:2107.06383",
            "year": 2021
        },
        {
            "authors": [
                "Kurt Shuster",
                "Samuel Humeau",
                "Antoine Bordes",
                "Jason Weston."
            ],
            "title": "Image-chat: Engaging grounded conversations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2414\u20132429.",
            "year": 2020
        },
        {
            "authors": [
                "Kurt Shuster",
                "Samuel Humeau",
                "Hexiang Hu",
                "Antoine Bordes",
                "Jason Weston."
            ],
            "title": "Engaging image captioning via personality",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12516\u201312526.",
            "year": 2019
        },
        {
            "authors": [
                "Kurt Shuster",
                "Da Ju",
                "Stephen Roller",
                "Emily Dinan",
                "YLan Boureau",
                "Jason Weston."
            ],
            "title": "The dialogue dodecathlon: Open-domain knowledge and image grounded conversational agents",
            "venue": "Proceedings of the 58th Annual Meeting of the Association",
            "year": 2020
        },
        {
            "authors": [
                "Kurt Shuster",
                "Eric Michael Smith",
                "Da Ju",
                "Jason Weston."
            ],
            "title": "Multi-modal open-domain dialogue",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4863\u20134883.",
            "year": 2021
        },
        {
            "authors": [
                "Leslie N Smith",
                "Nicholay Topin."
            ],
            "title": "Superconvergence: Very fast training of neural networks using large learning rates",
            "venue": "Artificial intelligence and machine learning for multi-domain operations applications, volume 11006, pages 369\u2013386. SPIE.",
            "year": 2019
        },
        {
            "authors": [
                "Bart Thomee",
                "David A Shamma",
                "Gerald Friedland",
                "Benjamin Elizalde",
                "Karl Ni",
                "Douglas Poland",
                "Damian Borth",
                "Li-Jia Li."
            ],
            "title": "Yfcc100m: The new data in multimedia research",
            "venue": "Communications of the ACM, 59(2):64\u201373.",
            "year": 2016
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Alexander Toshev",
                "Samy Bengio",
                "Dumitru Erhan."
            ],
            "title": "Show and tell: A neural image caption generator",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3156\u20133164.",
            "year": 2015
        },
        {
            "authors": [
                "Te-Lin Wu",
                "Alex Spangher",
                "Pegah Alipoormolabashi",
                "Marjorie Freedman",
                "Ralph Weischedel",
                "Nanyun Peng."
            ],
            "title": "Understanding multimodal procedural knowledge by sequencing multimodal instructional manuals",
            "venue": "Proceedings of the 60th Annual Meet-",
            "year": 2022
        },
        {
            "authors": [
                "Yiran Xing",
                "Zai Shi",
                "Zhao Meng",
                "Gerhard Lakemeyer",
                "Yunpu Ma",
                "Roger Wattenhofer."
            ],
            "title": "Km-bart: Knowledge enhanced multimodal bart for visual commonsense generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Kelvin Xu",
                "Jimmy Ba",
                "Ryan Kiros",
                "Kyunghyun Cho",
                "Aaron Courville",
                "Ruslan Salakhudinov",
                "Rich Zemel",
                "Yoshua Bengio."
            ],
            "title": "Show, attend and tell: Neural image caption generation with visual attention",
            "venue": "International conference on machine learning, pages",
            "year": 2015
        },
        {
            "authors": [
                "Hang Yan",
                "Junqi Dai",
                "Tuo Ji",
                "Xipeng Qiu",
                "Zheng Zhang."
            ],
            "title": "A unified generative framework for aspect-based sentiment analysis",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Peter Young",
                "Alice Lai",
                "Micah Hodosh",
                "Julia Hockenmaier."
            ],
            "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
            "venue": "Transactions of the Association for Computational Linguistics, 2:67\u201378.",
            "year": 2014
        },
        {
            "authors": [
                "Fei Yu",
                "Jiji Tang",
                "Weichong Yin",
                "Yu Sun",
                "Hao Tian",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "Ernie-vil: Knowledge enhanced vision-language representations through scene graphs",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 3208\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Dong Zhang",
                "Suzhong Wei",
                "Shoushan Li",
                "Hanqian Wu",
                "Qiaoming Zhu",
                "Guodong Zhou."
            ],
            "title": "Multimodal graph fusion for named entity recognition with targeted visual guidance",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 35,",
            "year": 2021
        },
        {
            "authors": [
                "Dodeca(Shuster"
            ],
            "title": "2020b): The multi-task learning approach was used to train multiple tasks at once, and for this purpose, the dodecaDialogue dataset was built. The dodecaDialogue dataset consists of 12 tasks, and the model",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Developing artificial intelligence (AI) that can converse naturally with humans is a primary goal of AI research. In particular, open-domain conversational agents that are not restricted to a specific domain have attracted considerable attention. Many studies have adopted a pretraining approach using large datasets to improve the performance of these opendomain conversational agents(Adiwardana et al., 2020; Roller et al., 2021).\nRecent works have focused on multimodal opendomain conversational agents that consider visual information in addition to textual information for dialog generation. This approach utilizes visual information to help understand the context of a\nconversation, which is more consistent with how humans communicate. This approach has been shown to be effective in generating conversations that users find more engaging(Hu et al., 2014). To build such multimodal open-domain conversational agents, most previous studies have considered multi-task learning or have utilized pretrained models using large-scale data beyond the target data(Shuster et al., 2020b, 2021). This allows models trained with only text information to accept visual information. However, the requirement of collecting additional datasets is restrictive, and pretraining with additional datasets is inefficient in terms of time and resources. In this study, we propose a method to align text and images using only the target data to address these issues. We experimentally evaluated the effectiveness of the proposed method with limited data or smaller models.\nThe proposed method utilizes only target data to align images and texts. By automatically generating captions for the images and adding them as input to the model, our approach enables a text-based pre-\ntrained model to process image information more effectively. This differs notably from existing multimodal open-domain models that only receive image and utterance information. We propose visionlanguage warm-up tasks for multimodal dialogue models (VLAW-MDM) to effectively integrate information from images, captions, and context data. To construct the framework, we incorporated four warm-up tasks based on existing multimodal pretraining models(Chen et al., 2020; Yu et al., 2021; Wu et al., 2022). These tasks include generation captioning (GCP), image swapping (ISP), masked region modeling (MRM), and masked language modeling (MLM). They can be applied using only target data without any additional data required. These warm-up tasks enable the model to learn the associations between images and utterances.\nThe construction of this framework was inspired by the generative framework used by Ling et al. (2022), which we reformulate into a multimodal architecture suitable for the purposes of this research. We apply our warm-up framework to the popular sequence-to-sequence models BART and BlenderBot. For BlenderBot, we used a smaller model (400M) than that (2.7B) adopted in the multi-modal blenderBot (MMB)(Shuster et al., 2021). This allowed us to explore the performance of our proposed framework on smaller versions of the model and to validate the framework against a relatively large model with additional training data.\nWe used the Image-Chat dataset(Shuster et al., 2020a) to evaluate the effectiveness of our proposed framework. The data were structured as shown in Table 1. Image-Chat comprises conversations organized into a series of turns with utterances based on the speakers\u2019 styles. We used Image-Chat for warmup tasks first, and then performed fine-tuning to evaluate the effectiveness of our proposed method in a constrained learning environment.\nThe main contributions of this study are summarized as follows:\n\u2022 We propose a framework for vision-language warm-up tasks in multimodal dialogue models, called VLAW-MDM, and describe the process of warming up the model using only data from the target task. We experimentally evaluated the performance of this framework as described above.\n\u2022 We introduce four different warm-up tasks (MLM, ISP, MRM, and GCD) and experimen-\ntally evaluated how they each affected the performance of the model. Our results show that the best performance was achieved when all four warm-up tasks were utilized together.\n\u2022 We analyzed how automatically generating and utilizing captions affected the performance of the model. Our results showed that our proposed framework incorporating caption information was effective for training a multimodal open-domain dialogue model.\n\u2022 We also evaluated the warm-up tasks in the absence of caption information. The results show that the proposed method is effective even in environments where captions are not available or are difficult to create."
        },
        {
            "heading": "2 Related Work",
            "text": "Vision and Language Tasks. The integration of language and vision is a topic of active research that traditionally includes tasks such as image captioning and visual question answering (VQA)(Devlin et al., 2015; Xu et al., 2015; Fang et al., 2015; Donahue et al., 2015; Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014). Image captioning tasks focus on generating appropriate descriptions of a given image. Major datasets include COCO Captions (Chen et al., 2015) and Flickr30k (Young et al., 2014). These datasets of images covering various topics provide an ideal benchmark for assessing model\u2019s ability to understand complex content in an image and express it in natural language. Sequence-to-sequence structures are the most common method to process these datasets (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018).\nThe VQA task (Antol et al., 2015) requires image recognition and factual verification of text content. It evaluates the ability of a model to generate accurate answers to questions related to a given image. As a natural extension of this work, a method has been proposed to generate questions that can capture a user\u2019s attention based on a given image. However, these methods involve the limitation that the conversation usually ends in a single turn. To address this, visual dialog(Das et al., 2017) extends this with a continuous question-answer scenario.\nHowever, this approach does not provide a direct way to evaluate whether users experience conversations as interesting and engaging. To address this, multimodal multi-turn dialog datasets\n(Mostafazadeh et al., 2017; Shuster et al., 2020a) have been proposed. In particular, Image-Chat (Shuster et al., 2020a) supports multi-turn dialog based on a single image. Each utterance contains style information to allow a model to learn different styles of conversation. Hence, Image-Chat is an ideal dataset for training multimodal open-domain agents and enables more engaging dialog generation.\nMultimodal Representation Learning. Utilizing the weights of existing models trained on a single modality and fusing them together is a common strategy for multimodal learning(Kiela et al., 2019; Le and Hoi, 2020; Chen et al., 2020; Yu et al., 2021; Zhang et al., 2021). Many studies have adopted this approach to reuse models pretrained on a single modality for multimodal representation learning.\nTo process images and text together, Shuster et al. (2020b) uses multi-task training, in which a model learns by bundling multiple tasks that are related to a given target task. For this purpose, Shuster et al. (2020b) includes 12 subtask sets, which allows it to perform multiple tasks on large datasets.\nIn some cases, multimodal representations have been learned from multimodal datasets(Li et al.,\n2020a; Chen et al., 2020). However, multimodal pretraining approach has generally not been performed with data in the target domain. One potential solution is to employ domain-adaptive pretraining by using data related to the domain of the target data(Shuster et al., 2021). This method enables a model to adapt more effectively to a specific domain. However, domain-adaptive pretraining also utilizes data related to the target domain and does not provide a pretraining methodology for specific target data. As a solution, Ling et al. (2022) proposed a task-specific vision-language pretraining framework for multimodal aspect-based sentiment analysis, which realizes target data-specific pretraining on multiple tasks instead of a single task."
        },
        {
            "heading": "3 Methodology",
            "text": "In the present work, we adopt BlenderBot as a backbone model. The overall architecture is illustrated in Figure 1. In this section, we describe the operation of the entire framework. First, we describe how we process the images used as input to the model. In particular, we discuss our approach to extract features from an image and generate captions. Then, we describe how the encoder is extended to handle multimodal inputs and how the decoder\ngenerates utterances from the information received from the encoder. Finally, we describe the warm-up tasks that comprise the framework."
        },
        {
            "heading": "3.1 Image Encoder",
            "text": "Our proposed method uses pretrained models to extract visual features from images. Previous studies(Shuster et al., 2021; Ling et al., 2022) have mainly used Faster R-CNN(Anderson et al., 2018). Recently, patch-based models have shown better performance in image encoding(Shen et al., 2021; Wu et al., 2022). Based on these findings, we adopt a patch-based method based on CLIP in the proposed approach(Radford et al., 2021). In image encoding, a single image is divided into nine patches used as input to the model to obtain visual features. We denote the visual features as R = {r1, . . . , r9}, where ri \u2208 R512 is the visual feature of the i-th patch. The obtained visual features do not have the same number of dimensions as the textual representation, so an additional linear transformation layer is used to put the visual features along with the textual representation as input to the multimodal encoder. This linear transformation layer projects the visual features of each patch to a d-dimensional vector denoted as V \u2208 Rd\u00d79."
        },
        {
            "heading": "3.2 Caption Generation",
            "text": "Image captions are textual descriptions of objects, situations, and other information provided by an image. We used captions as a bridge between images and text. Because a caption is a textual representation of the information in an image, we assume that aligning the image with the utterance text is beneficial. However, because there are no separate captions for images in the existing dataset, we use an image captioning model (Li et al., 2023) to generate captions. The generated captions provide a description of the image and are used as input to the multimodal encoder along with the image and utterance."
        },
        {
            "heading": "3.3 Multi-Modal Architecture",
            "text": "Encoder. The encoder receives different kinds of modality information. To separate the modality information, we add a segment embedding that separates the image from the text. We also add special tokens such as \u27e8img\u27e9, \u27e8/img\u27e9 before and after the extracted image features following Xing et al. (2021). As shown in Figure 1, the images are entered in the order they appear first in the modality information. The image feature is followed by\nthe caption created earlier. There is no special token for the caption; rather a \u27e8sep\u27e9 token is simply added to the end of the caption. The caption is followed by the style and utterance. An additional special token such as \u27e8sty\u27e9 is appended at the end of styles to distinguish them from utterances. Styles are followed by utterances, and the difference between the warm-up task and fine-tuning phases becomes relevant here. In the warm-up task phase, the styles and corresponding utterances are input to the encoder together. However, in the fine-tuning phase, the style is not followed by an utterance because the model needs to predict an utterance for the style. Therefore, in the warm-up task, styles and utterances are combined and followed by an \u27e8eos\u27e9 token. However, during the fine-tuning stage, the \u27e8eos\u27e9 token is appended immediately after the \u27e8sty\u27e9 special token representing the style.\nDecoder. As shown in Figure 1, all warm-up tasks are processed through a single decoder. To distinguish between warm-up tasks, we add a special token at the beginning of the decoder\u2019s input, following a prior work (Yan et al., 2021; Ling et al., 2022). The input of the decoder starts with \u27e8bos\u27e9, followed by \u27e8gcp\u27e9, \u27e8isp\u27e9, \u27e8mrm\u27e9, and \u27e8mlm\u27e9, depending on the warm-up task. The input format is followed by the label values according to the warm-up task."
        },
        {
            "heading": "3.4 Warm-up Tasks",
            "text": "In this study, we introduce VLAW-MDM to efficiently integrate multimodal information. This is a warm-up task that strengthens the connections between images and text before the fine-tuning phase to improve the model\u2019s ability to handle complex multimodal information more effectively. During this warm-up task, the model is trained to understand and strengthen the relationship between images and text by utilizing data on the target task. This improves the model\u2019s ability to process multimodal input, which in turn improves its performance by utilizing only data on the target task without any additional data for pretraining. These enhanced connections between images and text play an important role in improving performance on the target task during the fine-tuning phase, allowing for more effective utilization of multimodal data at no additional cost.\nGeneration Captioning (GCP). The GCP task replaces all captions with masking tokens and restores them to the original captions. In the GCP\ntask, the model interprets the context of the image based on other multimodal information such as image or utterance data without any caption information and generates caption accordingly. This helps the decoder not only analyze and understand information from each modality independently but also acquire the ability to comprehensively understand and appropriately integrate information from other modalities such as images and utterances.\nThe target sequence for the GCP task is Y = [\u27e8gcd\u27e9 , c1, . . . , cN , \u27e8eos\u27e9], where c represents caption tokens and N is the number of caption tokens. Traditional training methods such as maximum likelihood estimation (MLE) have a problem in that they mainly generate high-frequency responses that exist in the dataset. Therefore, to control these highfrequency responses, we adopt unlikelihood training (Roller et al., 2021; Li et al., 2020b; Welleck et al.). The formula for MLE is as follows:\nLMLE = \u2212 |Y |\u2211 t=1 logp\u03b8(yt|X\u0303, y<t), (1)\nwhere yt \u2208 Y and yt is a caption token or special token for Y . Let y<t be the tokens before the tth utterance of yt. The X\u0303 means that the caption input to the encoder is masked, where the rest of the multimodal information except for the caption token is in its normal form. The formula for the unlikelihood loss function is as follows:\nLUL = \u2212 |Y |\u2211 t=1 \u2211 yc\u2208Ct log(1\u2212 p\u03b8(yc|X\u0303, y<t)), (2)\nThe negative candidates Ct are the set of tokens that we do not want to generate at each time step. This is controlled by assigning a penalty if the token generated by the model belongs to Ct. Likelihood is used to increase the probability of the next fired token, yt, while unlikelihood is used to decrease the probability of yc. The final loss value for GCP is as follows:\nLGCP = LMLE + \u03b1LUL (3)\n\u03b1 is the weighting representing how much to reflect LUL.\nImage-Swapping(ISP). The ISP task serves to train the model\u2019s ability to determine whether an image is the original or an altered image. The main process of an ISP task is as follows. An image is replaced with another image in the batch with a\ncertain probability. Images are then fed into the encoder along with caption and dialog. The encoder processes this multimodal information and passes the results to the decoder. Based on this information, the decoder determines whether the image is an original or an altered image. The results are expressed as \"positive\" or \"negative.\" \"positive\" means that the decoder recognizes the original image, whereas \"negative\" means that the image has been altered.\nThe target sequence for the ISP action is Y = [\u27e8bos\u27e9 , \u27e8isp\u27e9 , S, \u27e8eos\u27e9], where S indicates \"positive\" or \"negative.\" The loss function for the ISP task is as follows:\nLISP = \u2212EX\u223cD,I=X\u222aX\u0303 |Y |\u2211 t=1 log p\u03b8(yt|I, y<t), (4)\nyt \u2208 Y , which refers to a text token or special token in Y . Let y<t be the tokens before the t-th utterance of yt.\nIn the loss function, we consider two cases: X and X\u0303 . X is an instance from a data distribution D that retains the original image, caption, and utterance. In contrast, X\u0303 represents a case where only the image changes while the caption and utterance remain the same. I denotes the combined input information for these cases, encompassing both X and X\u0303 .\nMasked Region Modeling(MRM). We adopted the MRM method used by Xing et al. (2021); Ling et al. (2022). We masked random positions in the patches. The masked image is passed to a multimodal encoder, and the decoder estimates the masked part(Ling et al., 2022). For the MRM task, the inputs to the decoder are \u27e8feat\u27e9 and \u27e8zero\u27e9. Here, the \u27e8feat\u27e9 token is used for the unmasked normal image region and the \u27e8zero\u27e9 token is used for the masked part, which feeds the value at position \u27e8zero\u27e9 into the MLP layer. The MLP layer is trained to match the output representation to the original image representation. The final loss value of MRM is as follows:\nLMRM = \u2212EX\u223cD\u03a3Rr=1DKL(q(vr)\u2225p(vr)), (5)\nThe representation predicted by the model is p(vr) and the actual input image representation is q(vr). The model was trained by minimizing the KL divergence, where R is the number of masked regions.\nMasked Language Modeling(MLM). For MLM, we masked the tokens in an utterance at\na certain rate. In this case, masking was not performed for the entire turn but rather only for a certain percentage in each turn(Devlin et al., 2018).\nThe target sequence for the MLM task is Y = [\u27e8mlm\u27e9, ds11 , . . . , d s1 N , \u27e8sty\u27e9, d u1 1 , . . . , d u1 M , \u27e8eos\u27e9]. The sequence consists of the style of the utterance and the turns that represent the utterance. It begins with a start token \u27e8bos\u27e9 and \u27e8mlm\u27e9, followed by N tokens ds indicating the style, and ending with the token \u27e8sty\u27e9 token to indicate the end of the style. This is followed by M tokens du representing the utterances in that turn, with the turns separated by \u27e8sep\u27e9 tokens. In this example, we indicate the end of the turn through the \u27e8eos\u27e9 token directly, without a separate \u27e8sep\u27e9 token, because it indicates that we sampled for Turn 1.\nTo calculate the MLM loss value, the loss value is calculated as in the GCP operation, and the final loss value is as follows:\nLMLM = LMLE + \u03b1LUL (6)\nFull Warm-up Task Loss. The final objective function is as follows:\nL = \u03bb1LGCP + \u03bb2LISP + \u03bb3LMRM +\u03bb4LMLM (7)\nThe \u03bb values given above are adjustable hyperparameter. For this experiment, all \u03bb values were fixed to 1."
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Evaluation Metrics. We used the F1, BLEU-4, and ROUGE-L metrics to evaluate the performance of the proposed model.\nFine-tuning the Dataset. We used the ImageChat dataset to verify the effectiveness of the proposed framework. The data consisted of an image, style attributes of two speakers (A and B), and a dialogue between the two speakers. It also included a set of 215 possible style attributes from Shuster et al. (2019), which are categorized as positive, neutral, and negative. The style attributes are used to define the speakers\u2019 personalities in the conversation. The images in the dataset were selected from the YFCC100M dataset (Thomee et al., 2016). Some statistics on the data included in the Image-Chat dataset are as shown in Table 2.\nThe utterance generation for the first turn using Image-Chat is shown in Figure 2. The data input to the encoder in the first turn were the image, caption, and style of the first turn. The encoder processes these inputs and passes the information to the decoder. The decoder generates an utterance for the first turn based on the information from the encoder. In the second turn, the encoder receives an image along with a caption and a style from the first turn, and utterance of the first turn, and a style from the second turn. The decoder generates the second round of utterance from these inputs, and the third round proceeds in the same manner."
        },
        {
            "heading": "4.2 Main Results",
            "text": "Impact of Each Warm-up Task. Table 3 shows the contribution of each warm-up task to performance. We compared the performance of models trained with and without warm-up tasks.\nTo validate the scalability of the warm-up task, we applied it to two different sequence-to-sequence models, including BART and BlenderBot. First, in terms of BlenderBot\u2019s performance, an improvement may be observed in all measures except the BLEU-4 score at Turn 3 when the MLM task was introduced compared to no warm-up task. This was likely due to the word prediction ability learned through the MLM helping with utterance generation. Next, when we added the ISP task alongside MLM, we observed additional performance\ngains on all scales except for the BLEU-4 score on Turn 1. ISP determines the appropriateness of a given image based on its caption and dialog. This allows the model to learn to align the image with the dialog, which is likely the reason for the performance improvement with the addition of ISP. Third, when we added the MRM task to MLM and ISP, the results showed a further increase in performance. MRM helps the model understand the image features given in the form of patches. This is important for multimodal open-domain agents that perform conversations based on images and seems to have helped with utterance generation. Finally, the highest performance was achieved when the GCD task was added to MLM, ISP, and MRM. The GCD task generates captions based on a given conversation and image. Through the GCD task, the model learns the caption and its relationship to a given conversation and image. This process allows the model to quickly learn the relationship between dialog, image, and caption. In particular, the information provided by the image during utterance generation appears to help the language model recognize and generate the correct utterance.\nThese experimental results show that each warmup task contributed to improving the performance\nof the model, and the best performance was achieved when all the warm-up tasks were combined. This demonstrates the effectiveness of the framework proposed in this study.\nTable 3 shows that applying a warm-up task improved the performance of both BART and BlenderBot. Compared to BlenderBot, BART is a smaller model. These results demonstrate that the proposed method is effective even for small models, as initially assumed.\nImpact of Caption. Table 4 compares the performance of the models tested with and without caption. In Table 4, row 1 shows the performance of the backbone model without caption and row 3 gives the performance of the backbone model with caption. Comparing the two, it may be observed that simply providing caption helped the backbone model generate utterances. This was likely the case because the caption effectively acted as a bridge between the image and the utterance. Of note, the improvement on Turn 3 was smaller than that on Turns 1 and 2. When generating an utterance for Turn 3, the input was an utterance from Turns 1 and 2. As with the caption, the utterances in Turns 1 and 2 provide information about the image in the form of text, which made it easier for the language\nmodel to understand the image and generate an utterance. This was most likely the reason that the two performances were similar.\nThe results with and without the warm-up task when no captions were provided are shown in rows 1 and 2. We excluded the GCD from the warmup task because they were not captioned. The results show that the warm-up task without captions helped improve performance. These results show that the proposed method is effective even in environments where captions are not available or cannot be automatically generated.\nThe performance on the warm-up task when captions were provided is shown in rows 3 and 4. Because captions were provided, we ran all the warm-up tasks, including GCD. The results showed that applying all warm-up tasks significantly improved performance. In particular, the performance for Turn 3 on row 4 shows that we achieved a sufficiently high performance improvement compared with the other methods. This suggests that warming up the model with GCD is more effective than naively entering textual information (captions and turn-by-turn utterances) for images."
        },
        {
            "heading": "4.3 Experimental results compared to baseline",
            "text": "To evaluate the performance of the proposed method, we conducted a comparative analysis of Image-Chat with various existing models used in the experiments. The results of the comparison are listed in Table 5. Most of the compared models use additional datasets other than the data of the target task to perform pretraining to align text and image information, or apply various multi-task techniques. This differs from our model, which utilizes only data on the target task. See Appendix A for a description of models compared.\nTable 5 shows that BlenderBot with our framework exhibited performance comparable to that of state-of-the-art methods. In particular, the highest performance was achieved in terms of F1 score. The proposed model is smaller than the BlenderBot model used by MMB. Nevertheless, our model outperformed the F1 score of MMB, which was previously the highest-scoring model, and also performed better in terms of BLEU-4 score. Consequently, these results show that our framework is able to incorporate image information into a model pretrained using only existing text data. They also demonstrate that our framework can be effectively applied to small models and can further improve\nutterance generation by utilizing additional caption information."
        },
        {
            "heading": "5 Conclusion",
            "text": "We have proposed VLAW-MDM as a methodology for training multimodal open-domain agents using only target data to obviate the need for large amounts of data for pretraining or multiple tasks. The experimental results have shown that even with limited data, a model pretrained from a single modality can effectively process multimodal information. Furthermore, our proposed approach outperformed existing models in terms of F1 score on the Image-Chat dataset and outperformed MMB in terms of F1 and BLEU-4 scores despite its smaller size. In future work, we plan to explore extensions to this framework.\nLimitations\nWhile the methodology presented in this study provides meaningful results, it also involves a number of limitations. To demonstrate the performance of the proposed framework, we only used ImageChat, which is characterized by combining image and style information to perform multi-turn conversations. Therefore, differences in style and dialog format may have affected the findings of this study.\nBecause our model utilizes image captions as an additional input, it is highly dependent on the accuracy of the generated captions. Captions play an important role in the learning and performance of the model because they serve as a textual representation of the image. However, errors in the caption generation process or captions that do not\naccurately reflect the key content of an image can affect the model\u2019s ability to generate utterances.\nFinally, we quantitatively evaluated the performance of the proposed model. However, although this quantitative evaluation is useful for measuring overall performance, it involves some limitations in capturing qualitative aspects such as the user experience. These qualitative factors such as user satisfaction, convenience, and understanding are useful to more accurately evaluate the actual performance of a learning model. They also play an important role in improving models based on user feedback.\nIn this study, we have focused on a quantitative evaluation to clearly demonstrate the performance of our proposed methodology. However, we acknowledge that this does not comprehensively cover the qualitative factors. In future work, we plan to perform a qualitative evaluation based on user feedback to further evaluate the performance of the model and user satisfaction."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by Institute of Information & Communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (RS-2023-00216011, Development of artificial complex intelligence for conceptually understanding and inferring like human). And this research was supported by the MSIT(Ministry of Science and ICT), Korea, under the ITRC(Information Technology Research Center) support program(IITP-2021-2016-0-00465) supervised by the IITP(Institute for Information & Communications Technology Planning & Evaluation). And this work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) under the metaverse support program to nurture the best talents (IITP-2023RS-2023-00256615) grant funded by the Korea government(MSIT)."
        },
        {
            "heading": "A Compare with existing models",
            "text": "DialoGPT(Zhang et al.): Additional social media data was used to perform conversational neural\nresponse generation from the GPT-2 model. This model can only take textual information as input.\nDodeca(Shuster et al., 2020b): The multi-task learning approach was used to train multiple tasks at once, and for this purpose, the dodecaDialogue dataset was built. The dodecaDialogue dataset consists of 12 tasks, and the model was trained on these tasks. For image feature extraction, the ResNeXTIG-3.5B model (Mahajan et al., 2018) was used.\n2AMMC(Ju et al., 2019): The model was constructed by combining ResNeXt-IG-3.B with Faster R-CNN image feature extraction. 2AMMC is utilized as a search model that references various transformers to blend ResNeXt-IG-3.5B and Faster R-CNN image features.\nBlenderBot(Roller et al., 2021): BlenderBot is a 2.7B-sized model with a sequence-to-sequence structure. It was also pretrained on a large dataset. It only takes a single modal representation, text, as input.\nMulti-Modal BlenderBot(Shuster et al., 2021): This is a multimodal extension of the BlenderBot model. MMB used BlenderBot\u2019s 2.7B model and domain-adaptive training to allow a model trained on a single modality to receive multimodal information.\nB Implementation Details.\nBlenderBot was used as the backbone model. Unlike MBB, we did not use the 2.7B BlenderBot model, but rather adopted a smaller 400M BlenderBot model. The default hyperparameter values of the model were used without modification for comparison with MBB. The warm-up task was trained for 20 epochs, batch size was set to 16, and number of accumulation steps was set to 126. The model trained with the warm-up task is fine-tuned for the target task, Image-Chat. For fine-tuning, set epoch to 10, batch size to 32, and accumulation step to 8. For BART, the warm-up task runs for 10 epochs, the batch size is set to 32, and the accumulation step is set to 2. For fine-tuning, we set the epoch to 7 and the batch size to 64. We used AdamW(Loshchilov and Hutter, 2017) as the optimizer and additionally OneCycleLR(Smith and Topin, 2019). All implementations for this experiment were done via Pytorch."
        }
    ],
    "title": "A Framework for Vision-Language Warm-up Tasks in Multimodal Dialogue Models",
    "year": 2023
}