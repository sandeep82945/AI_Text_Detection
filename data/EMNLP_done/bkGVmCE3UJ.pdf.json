{
    "abstractText": "Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills. However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that do exist (i) ignore the remarkable ability of humans to generalize, (ii) focus only on English, and (iii) investigate syntax or semantics and overlook other capabilities that lie at the heart of human language, like morphology. Here, we close these gaps by conducting the first rigorous analysis of the morphological capabilities of ChatGPT in four typologically varied languages (specifically, English, German, Tamil, and Turkish). We apply a version of Berko\u2019s (1958) wug test to ChatGPT, using novel, uncontaminated datasets for the four examined languages. We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results\u2014through the lens of morphology\u2014cast a new light on the linguistic capabilities of ChatGPT, suggesting that claims of human-like language skills are premature and misleading.",
    "authors": [
        {
            "affiliations": [],
            "name": "Leonie Weissweiler"
        },
        {
            "affiliations": [],
            "name": "Valentin Hofmann"
        },
        {
            "affiliations": [],
            "name": "Anjali Kantharuban"
        },
        {
            "affiliations": [],
            "name": "Anna Cai"
        },
        {
            "affiliations": [],
            "name": "Ritam Dutt"
        },
        {
            "affiliations": [],
            "name": "Amey Hengle"
        },
        {
            "affiliations": [],
            "name": "Anubha Kabra"
        },
        {
            "affiliations": [],
            "name": "Atharva Kulkarni"
        },
        {
            "affiliations": [],
            "name": "Abhishek Vijayakumar"
        },
        {
            "affiliations": [],
            "name": "Haofei Yu"
        },
        {
            "affiliations": [],
            "name": "Hinrich Sch\u00fctze"
        },
        {
            "affiliations": [],
            "name": "Kemal Oflazer"
        },
        {
            "affiliations": [],
            "name": "David R. Mortensen"
        }
    ],
    "id": "SP:53bb398eaeabbc1259a44200a94a491537c58c4b",
    "references": [
        {
            "authors": [
                "S Agesthialingom."
            ],
            "title": "A note on Tamil verbs",
            "venue": "Anthropological Linguistics, pages 121\u2013125.",
            "year": 1971
        },
        {
            "authors": [
                "Jaimeen Ahn",
                "Hwaran Lee",
                "Jinhwa Kim",
                "Alice Oh."
            ],
            "title": "Why knowledge distillation amplifies gender bias and how to mitigate from the perspective of DistilBERT",
            "venue": "Proceedings of the 4th Workshop on Gender Bias in Natural Language Process-",
            "year": 2022
        },
        {
            "authors": [
                "Adam Albright",
                "Bruce Hayes."
            ],
            "title": "Modeling english past tense intuitions with minimal generalization",
            "venue": "Proceedings of the ACL-02 Workshop on Morphological and Phonological Learning - Volume 6, MPL \u201902, page 58\u201369, USA. Association for Com-",
            "year": 2002
        },
        {
            "authors": [
                "Adam Albright",
                "Bruce Hayes."
            ],
            "title": "Rules vs",
            "venue": "analogy in English past tenses: A computational/experimental study. Cognition, 90(2):119\u2013 161.",
            "year": 2003
        },
        {
            "authors": [
                "Albert Henry Arden."
            ],
            "title": "A progressive grammar of common Tamil",
            "venue": "Society for Promoting Christian Knowledge.",
            "year": 1891
        },
        {
            "authors": [
                "Association. Jean Berko"
            ],
            "title": "The child\u2019s learning of English mor",
            "year": 1958
        },
        {
            "authors": [
                "Wei",
                "KathyMeier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel."
            ],
            "title": "PaLM: Scaling Language Modeling with Pathways",
            "venue": "Arxiv, 2204.02311.",
            "year": 2022
        },
        {
            "authors": [
                "Elizabeth Clark",
                "Tal August",
                "Sofia Serrano",
                "Nikita Haduong",
                "Suchin Gururangan",
                "Noah A. Smith."
            ],
            "title": "All that\u2019s \u2018human\u2019 is not gold: Evaluating human evaluation of generated text",
            "venue": "Proceedings of the 59th Annual Meeting of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Ryan Cotterell",
                "Christo Kirov",
                "John Sylak-Glassman",
                "David Yarowsky",
                "Jason Eisner",
                "Mans Hulden."
            ],
            "title": "The SIGMORPHON 2016 shared Task\u2014 Morphological reinflection",
            "venue": "Proceedings of the 14th SIGMORPHON Workshop on Computational",
            "year": 2016
        },
        {
            "authors": [
                "Ryan Cotterell",
                "Arun Kumar",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Morphological segmentation inside-out",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2325\u20132330, Austin, Texas. Association for Compu-",
            "year": 2016
        },
        {
            "authors": [
                "Ryan Cotterell",
                "Thomas M\u00fcller",
                "Alexander Fraser",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Labeled morphological segmentation with semi-Markov models",
            "venue": "Proceedings of the Nineteenth Conference on Computational Natural Language Learning, pages 164\u2013174, Bei-",
            "year": 2015
        },
        {
            "authors": [
                "Ryan Cotterell",
                "TimVieira",
                "Hinrich Sch\u00fctze."
            ],
            "title": "A joint model of orthography andmorphological segmentation",
            "venue": "InProceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2016
        },
        {
            "authors": [
                "Ryan Cotterell",
                "Ekaterina Vylomova",
                "Huda Khayrallah",
                "Christo Kirov",
                "David Yarowsky."
            ],
            "title": "Paradigm completion for derivational morphology",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2017
        },
        {
            "authors": [
                "Antje Dammel",
                "Oliver Schallert."
            ],
            "title": "Morphological variation: Theoretical and empirical perspectives",
            "venue": "John Benjamins, Amsterdam.",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Deutsch",
                "John Hewitt",
                "Dan Roth."
            ],
            "title": "A distributional and orthographic aggregation model for English derivational morphology",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Robert M. Dixon."
            ],
            "title": "Ergativity",
            "venue": "Cambridge University Press, Cambridge, UK.",
            "year": 1994
        },
        {
            "authors": [
                "Daniel Edmiston."
            ],
            "title": "A systematic analysis of morphological content in BERT models for multiple languages",
            "venue": "Arxiv, abs/2004.03032.",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Erdmann",
                "Micha Elsner",
                "Shijie Wu",
                "Ryan Cotterell",
                "Nizar Habash."
            ],
            "title": "The paradigm discovery problem",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7778\u20137790, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Erdmann",
                "Tom Kenter",
                "Markus Becker",
                "Christian Schallhart."
            ],
            "title": "Frugal paradigm completion",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8248\u20138273, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "YoavGoldberg."
            ],
            "title": "Assessing BERT\u2019s syntactic abilities",
            "venue": "Arxiv, 1901.05287.",
            "year": 2019
        },
        {
            "authors": [
                "Omer Goldman",
                "David Guriel",
                "Reut Tsarfaty."
            ],
            "title": "Un)solving morphological inflection: Lemma overlap artificially inflates models\u2019 performance",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short",
            "year": 2022
        },
        {
            "authors": [
                "Coleman Haley."
            ],
            "title": "This is a BERT",
            "venue": "now there are several of them. can they generalize to novel words? In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 333\u2013341, Online. Association for Com-",
            "year": 2020
        },
        {
            "authors": [
                "Harald Hammarstr\u00f6m",
                "Lars Borin."
            ],
            "title": "Unsupervised learning of morphology",
            "venue": "Computational Linguistics, 37(2):309\u2013350.",
            "year": 2011
        },
        {
            "authors": [
                "Martin Haspelmath",
                "Andrea Sims."
            ],
            "title": "Understanding Morphology",
            "venue": "Routledge, Oxford (UK).",
            "year": 2010
        },
        {
            "authors": [
                "Amr Hendy",
                "Mohamed Abdelrehim",
                "Amr Sharaf",
                "Vikas Raunak",
                "Mohamed Gabr",
                "Hitokazu Matsushita",
                "Young Jin Kim",
                "Mohamed Afify",
                "Hany Hassan Awadalla"
            ],
            "title": "How good are GPT models at machine translation? a comprehensive evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Charles F. Hockett."
            ],
            "title": "TwoModels of Grammatical Description",
            "venue": "WORD, 10(2-3):210\u2013234.",
            "year": 1954
        },
        {
            "authors": [
                "Valentin Hofmann",
                "Janet Pierrehumbert",
                "Hinrich Sch\u00fctze."
            ],
            "title": "DagoBERT: Generating derivational morphology with a pretrained language model",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Valentin Hofmann",
                "Janet Pierrehumbert",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Predicting the growth of morphological families from social and linguistic factors",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7273\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Valentin Hofmann",
                "Janet Pierrehumbert",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Superbizarre is not superb: Derivational morphology improves BERT\u2019s interpretation of complex words",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Valentin Hofmann",
                "Hinrich Schuetze",
                "Janet Pierrehumbert."
            ],
            "title": "An embarrassingly simple method to mitigate undesirable properties of pretrained language model tokenizers",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Valentin Hofmann",
                "Hinrich Sch\u00fctze",
                "Janet Pierrehumbert."
            ],
            "title": "A graph auto-encoder model of derivational morphology",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1127\u20131138, Online. Asso-",
            "year": 2020
        },
        {
            "authors": [
                "Alon Jacovi",
                "Avi Caciularu",
                "Omer Goldman",
                "Yoav Goldberg."
            ],
            "title": "Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks",
            "venue": "arXiv preprint arXiv:2305.10160.",
            "year": 2023
        },
        {
            "authors": [
                "Wenxiang Jiao",
                "WenxuanWang",
                "JT Huang",
                "XingWang",
                "ZP Tu."
            ],
            "title": "Is ChatGPT a good translator? Yes with GPT-4 as the engine",
            "venue": "arXiv, 2301.08745.",
            "year": 2023
        },
        {
            "authors": [
                "Huiming Jin",
                "Liwei Cai",
                "Yihui Peng",
                "Chen Xia",
                "Arya McCarthy",
                "Katharina Kann."
            ],
            "title": "Unsupervised morphological paradigm completion",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6696\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Zijia Jin",
                "Xingyu Zhang",
                "Mo Yu",
                "Lifu Huang."
            ],
            "title": "Probing script knowledge from pre-trained models",
            "venue": "Proceedings of the Workshop on Unimodal and Multimodal Induction of Linguistic Structures (UMIoS), pages 87\u201393, Abu Dhabi, United Arab Emi-",
            "year": 2022
        },
        {
            "authors": [
                "Katharina Kann",
                "Ryan Cotterell",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Neural morphological analysis: Encodingdecoding canonical segments",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 961\u2013967, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Katharina Kann",
                "Ryan Cotterell",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Neural multi-source morphological reinflection",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages",
            "year": 2017
        },
        {
            "authors": [
                "Katharina Kann",
                "Hinrich Sch\u00fctze."
            ],
            "title": "MED: The LMU system for the SIGMORPHON 2016 shared task on morphological reinflection",
            "venue": "Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Mor-",
            "year": 2016
        },
        {
            "authors": [
                "Christo Kirov",
                "Ryan Cotterell."
            ],
            "title": "Recurrent neural networks in linguistic theory: Revisiting pinker and prince (1988) and the past tense debate",
            "venue": "Transactions of the Association for Computational Linguistics, 6:651\u2013665.",
            "year": 2018
        },
        {
            "authors": [
                "Klaus-Michael K\u00f6pcke."
            ],
            "title": "Schemas in German plural formation",
            "venue": "Lingua, 74(4):303\u2013335.",
            "year": 1988
        },
        {
            "authors": [
                "Leigh Lisker."
            ],
            "title": "Tamil verb classification",
            "venue": "Journal of the American Oriental Society, 71(2):111\u2013114.",
            "year": 1951
        },
        {
            "authors": [
                "Ling Liu",
                "Mans Hulden."
            ],
            "title": "Leveraging principal parts for morphological inflection",
            "venue": "Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 153\u2013161, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Ling Liu",
                "Lingshuang Jack Mao."
            ],
            "title": "Morphological reinflection with conditional random fields and unsupervised features",
            "venue": "Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology,",
            "year": 2016
        },
        {
            "authors": [
                "Xiaomeng Ma",
                "Lingyu Gao."
            ],
            "title": "How do we get there? Evaluating transformer neural networks as cognitive models for English past tense inflection",
            "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Inbal Magar",
                "Roy Schwartz."
            ],
            "title": "Data contamination: From memorization to exploitation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 157\u2013165, Dublin, Ireland. Associa-",
            "year": 2022
        },
        {
            "authors": [
                "Gary F Marcus",
                "Ursula Brinkmann",
                "Harald Clahsen",
                "Richard Wiese",
                "Steven Pinker."
            ],
            "title": "German inflection: The exception that proves the rule",
            "venue": "Cognitive psychology, 29(3):189\u2013256.",
            "year": 1995
        },
        {
            "authors": [
                "Rowan Hall Maudslay",
                "Ryan Cotterell."
            ],
            "title": "Do syntactic probes probe syntax? experiments with jabberwocky probing",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Kate McCurdy",
                "Sharon Goldwater",
                "Adam Lopez."
            ],
            "title": "Inflecting when there\u2019s no majority: Limitations of encoder-decoder neural networks as cognitive models for German plurals",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "David R.Mortensen",
                "Siddharth Dalmia",
                "Patrick Littell."
            ],
            "title": "Epitran: Precision G2P for many languages",
            "venue": "InProceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Paris, France. European Language Re-",
            "year": 2018
        },
        {
            "authors": [
                "Kemal Oflazer."
            ],
            "title": "Two-level description of Turkish morphology",
            "venue": "Literary and Linguistic Computing, 9(2):137\u2013148.",
            "year": 1994
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "ChatGPT",
            "venue": "Large language model.",
            "year": 2023
        },
        {
            "authors": [
                "BenAmbridge",
                "Ekaterina Vylomova."
            ],
            "title": "SIGMORPHON 2021 shared task on morphological reinflection: Generalization across languages",
            "venue": "Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology,",
            "year": 2021
        },
        {
            "authors": [
                "Steven Pinker",
                "Alan Prince."
            ],
            "title": "On language and connectionism: Analysis of a parallel distributed processing model of language acquisition",
            "venue": "Cognition, 28(1-2):73\u2013193.",
            "year": 1988
        },
        {
            "authors": [
                "Ingo Plag."
            ],
            "title": "Morphological productivity: Structural constraints in English derivation",
            "venue": "De Gruyter, Berlin.",
            "year": 1999
        },
        {
            "authors": [
                "Kim Plunkett",
                "Patrick Juola."
            ],
            "title": "A connectionist model of English past tense and plural morphology",
            "venue": "Cognitive Science, 23(4):463\u2013490.",
            "year": 1999
        },
        {
            "authors": [
                "David E Rumelhart",
                "James L McClelland."
            ],
            "title": "On learning the past tenses of English verbs",
            "venue": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 2, pages 216\u2013271. MIT Press, Cambridge, MA.",
            "year": 1986
        },
        {
            "authors": [
                "David E. Rumelhart",
                "James L. McClelland."
            ],
            "title": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations",
            "venue": "The MIT Press.",
            "year": 1986
        },
        {
            "authors": [
                "Harold F Schiffman",
                "Vasu Renganathan."
            ],
            "title": "An English dictionary of the Tamil verb",
            "venue": "Linguistic Data Consortium.",
            "year": 2009
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Miikka Silfverberg",
                "AdamWiemerslage",
                "Ling Liu",
                "Lingshuang Jack Mao."
            ],
            "title": "Data augmentation for morphological reinflection",
            "venue": "Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection, pages 90\u201399, Van-",
            "year": 2017
        },
        {
            "authors": [
                "Radu Soricut",
                "Franz Och."
            ],
            "title": "Unsupervised morphology induction using word embeddings",
            "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2015
        },
        {
            "authors": [
                "Michael T Ullman",
                "Suzanne Corkin",
                "Marie Coppola",
                "Gregory Hickok",
                "John H Growdon",
                "Walter J Koroshetz",
                "Steven Pinker"
            ],
            "title": "A neural dissociation within language: Evidence that the mental dictionary",
            "year": 1997
        },
        {
            "authors": [
                "Ekaterina Vylomova",
                "Ryan Cotterell",
                "Timothy Baldwin",
                "Trevor Cohn."
            ],
            "title": "Context-aware prediction of derivational word-forms",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2,",
            "year": 2017
        },
        {
            "authors": [
                "Silfverberg",
                "Mans Hulden."
            ],
            "title": "SIGMORPHON 2020 shared task 0: Typologically diverse morphological inflection",
            "venue": "Proceedings of the 17th SIGMORPHON Workshop on Computational",
            "year": 2020
        },
        {
            "authors": [
                "Longyue Wang",
                "Chenyang Lyu",
                "Tianbo Ji",
                "Zhirui Zhang",
                "Dian Yu",
                "Shuming Shi",
                "Zhaopeng Tu."
            ],
            "title": "Document-level machine translation with large language models",
            "venue": "arXiv, 2304.02210.",
            "year": 2023
        },
        {
            "authors": [
                "Leonie Weissweiler",
                "Valentin Hofmann",
                "Masoud Jalili Sabet",
                "Hinrich Schuetze."
            ],
            "title": "CaMEL: Case Marker Extraction without Labels",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2022
        },
        {
            "authors": [
                "Colin Wilson",
                "Jane S.Y. Li."
            ],
            "title": "Were we there already? Applying minimal generalization to the SIGMORPHON-UniMorph shared task on cognitively plausible morphological inflection",
            "venue": "Proceedings of the 18th SIGMORPHON Workshop on",
            "year": 2021
        },
        {
            "authors": [
                "ShijieWu",
                "Ryan Cotterell",
                "andMansHulden"
            ],
            "title": "Applying the transformer to character-level transduction",
            "venue": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Hongzhi Xu",
                "Mitchell Marcus",
                "Charles Yang",
                "Lyle Ungar."
            ],
            "title": "Unsupervised morphology learning with statistical paradigms",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 44\u201354, Santa Fe, New Mexico,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Do large language models (LLMs) possess humanlike linguistic capabilities? With the advent of the latest generation of LLMs such as GPT-4 (OpenAI, 2023b), LLaMA (Touvron et al., 2023), and PaLM (Chowdhery et al., 2022), there appears to be growing evidence for answering this question with yes (Bubeck et al., 2023): LLMs are capable of generating text that crowdworkers cannot distinguish from human-generated text (Clark et al., 2021) and excel at linguistic probing tasks such as predicting grammaticality, detecting the subject and tense of\n*Equal contribution. \u2020Authors sorted alphabetically.\nclauses, and identifying the grammatical number of subjects and objects (Jin et al., 2022). Despite these encouraging results, the existing body of work has so far examined a relatively limited part of the full spectrum of phenomena that are known to characterize human language, with a heavy focus on syntax and semantics. One area that has been neglected in particular is morphology, i.e., the capacity to create words according to systematic patterns of covariation in form and meaning (Haspelmath and Sims, 2010). This gap in the LLM literature is noteworthy given that morphology has been a hallmark of research on computational approaches to language since the very beginnings of neural language processing in the 1980s (Rumelhart and McClelland, 1986b\u037e Plunkett and Juola, 1999\u037e Albright and Hayes, 2002, 2003\u037e Goldberg, 2019). In this study, we present the first systematic analysis of the morphological capabilities of LLMs, fo-\ncusing on ChatGPT (OpenAI, 2023a) as the most prominent and most widely-used LLM. Specifically, we investigate ChatGPT\u2019s morphological capabilities using the wug test (Berko, 1958), an experimental paradigm in which a participant is asked to provide an inflected or derived form of a nonce word. An example for our evaluation setup is given in Figure 1. Our experiments cover a broad range of morphological constructions and four typologically diverse languages: English, German, Tamil, and Turkish. We find that ChatGPT falls short not only of human performance but also of various supervised baselines. In sum, our contributions are as follows:\n\u2022 We conduct the first systematic analysis into the morphological capabilities of LLMs.\n\u2022 Our study covers a diverse set of morphological constructions/languages and introduces datasets for future research in the area.1\n\u2022 We show that ChatGPT has not achieved human parity\u2014or even state-of-the-art performance\u2014 on our nonce-word inflection/reinflection tasks but performs about as well as some older supervised models. We furthermore find evidence for the existence of a real word bias in ChatGPT that is the more pronounced the more data ChatGPT has seen for a given language."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Computational Morphology",
            "text": "Linguists divide morphology into inflection and derivation (Haspelmath and Sims, 2010). While inflection accounts for the different word forms of a lexeme, e.g., listen, listens, and listened, derivation accounts for the different lexemes of a word family, e.g., listen, listener, and listenable. Both inflection and derivation have been addressed in computational linguistics and natural language processing (NLP), albeit with a heavy focus on inflection. One line of work, which is conceptually similar to wug testing, has sought to generate inflected forms, given a stem and a morphological tag (Cotterell et al., 2017a, 2018\u037e Vylomova et al., 2020\u037e Goldman et al., 2022), using systems ranging from weighted finite state transducers and GRU/LSTM encoder-decoder models\n1We release our dataset along with our code at https:// github.com/dmort27/chatgpts-wugs, carefully following the guidelines laid out by Jacovi et al. (2023).\n(with soft attention or hard monotonic attention) to various transformer models. A special subtype of this task is morphological reinflection, where the input can be a form that is itself inflected (Cotterell et al., 2016a\u037e Kann and Sch\u00fctze, 2016\u037e Kann et al., 2017\u037e Silfverberg et al., 2017\u037e Pimentel et al., 2021). Other typical tasks in computational research on inflection are morphological segmentation (Cotterell et al., 2015, 2016b,c\u037e Kann et al., 2016), unsupervised morphology induction (Hammarstr\u00f6m and Borin, 2011\u037e Soricut and Och, 2015\u037e Xu et al., 2018\u037e Weissweiler et al., 2022), and morphological paradigm completion (Erdmann et al., 2020a,b\u037e Jin et al., 2020). There has also been some interest in the modeling of derivation (Cotterell et al., 2017b\u037e Vylomova et al., 2017\u037e Deutsch et al., 2018\u037e Hofmann et al., 2020b,c). More recently, there have been a few studies examining the morphological capabilities of language models (Edmiston, 2020\u037e Hofmann et al., 2020a), but they focus on smaller language models such as BERT (Devlin et al., 2019). By contrast, we examine ChatGPT, a model whose parameter count is three orders of magnitude larger, and we analyze its zero-, one-, and few-shot capabilities, an approach fully neglected by prior work."
        },
        {
            "heading": "2.2 Multilingual Capabilities of LLMs",
            "text": "Recent studies have extensively examined the evaluation of LLMs in multilingual settings. Some of these studies have specifically investigated the extent to which LLMs can be used for traditional multilingual NLP tasks such as machine translation (Bawden et al., 2022\u037e Hendy et al., 2023\u037e Jiao et al., 2023\u037eWang et al., 2023). Brown et al. (2023) demonstrate that LLMs perform well across multiple languages even with minimal task-specific training, highlighting their transferability and generalization in multilingual understanding."
        },
        {
            "heading": "2.3 LLM Performance on Unseen Data",
            "text": "The fact that LLMs have been pretrained on massive amounts of data means that they have seen and potentially memorized a substantial amount of the items of data used in typical evaluation setups (Magar and Schwartz, 2022). There have been a few attempts in NLP to specifically control for previous exposure (Haley, 2020\u037e Hofmann et al., 2020a\u037e Maudslay and Cotterell, 2021). We follow this idea by generating datasets of novel and uncontaminated nonce words, thus ensuring that the words have not been seen by ChatGPT before."
        },
        {
            "heading": "3 Data and Morphological Constructions",
            "text": "In this paper, we examine ChatGPT\u2019s morphological behavior on a typologically diverse set of languages: English, German, Tamil, and Turkish. While English and German belong to the same language family, German has a more fusional morphological system than English. Turkish is chosen since it is a non-Indo-European language with a fully agglutinative morphology. Tamil is chosen since it is a Dravidian language exhibiting an agglutinative morphology with fusional elements. Thus, in terms of the classical triangle of fusional, isolating, and agglutinative morphologies (Dixon, 1994), the languages cover four different points: almost fully isolating (English), intermediate between isolating and fusional (German), intermediate between fusional and agglutinative (Tamil), and fully agglutinative (Turkish). Furthermore, the chosen languages also cover different points in the spectrum from low-resource to high-resource, enabling us to form hypotheses about the impact of the amount of language-specific training data on the morphological capabilities of an LLM. Statistics for the amount of data in train, dev, and test for the baselines, as well as the number of wug test words, are given in Table 1. We report the accuracy of one annotator at a time against the judgments of all other annotators in Table 2."
        },
        {
            "heading": "3.1 English",
            "text": "The English past tense has a long and storied history in computational studies of morphology (Rumelhart and McClelland, 1986a\u037e Pinker and Prince, 1988\u037e Ullman et al., 1997\u037e Plunkett and Juola, 1999\u037e Albright and Hayes, 2002, 2003\u037e Kirov and Cotterell, 2018\u037e Ma and Gao, 2022). English displays a handful of conjugation classes as well as frequent morphographemic alternations\u2014 consonant doubling and e-deletion, for example\u2014 affecting past forms of verbs. To create the English data, 50 two- to five-letter irregular verbs (defined as verbs that do not form the past tense simply by adding -ed) were sampled from the UniMorph 4.0 dataset (Batsuren et al., 2022). These items were each perturbed by one or two letters (substituting phonetically similar sounds) producing a word not included in UniMorph. These verbs were then annotated by 28 volunteer annotators. Participants were asked to provide the past tense of the nonce word and given an example (wug\u2192 wugged) and the frame \u201cThey\n{nonce_word} all the time. In fact, they just yesterday.\u201d This yielded mappings between a lemma and a ranked list of inflected verbs, e.g., veed\u2192 [veeded, ved, vode]. Themodal annotation was always a regularly inflected form (-ed with appropriate allomorphic variation), but other inflectional classes were attested."
        },
        {
            "heading": "3.2 German",
            "text": "The German plural of nouns is a morphological phenomenon intensely studied in linguistics and the cognitive sciences due to the general complexity of the alternation between the eight different operations that can be used to express it. German pluralization is particularly notable due to the fact that none of the possible operations express it in a majority of cases (McCurdy et al., 2020). In fact, the most frequent German plural noun suffix -en has been argued not to be the default (i.e., the suffix that applies to novel nouns)\u2014an honor that goes to -s (Marcus et al., 1995). To create the dataset of novel German nonce nouns, we drew upon Unipseudo.2 We generated 200 nonce words with a length between four and seven characters (50 nonce words per character length), using German nouns as input to the algorithm. We then had one German native speaker unrelated to the study (i) generate articles (der, die, or das) for each of the nonce words, and (ii) generate a plural based on the nonce words and the previ-\n2http://www.lexique.org/shiny/unipseudo/\nously selected articles. We manually filtered out words whose plural is blocked by existing German lexemes, resulting in a final set of 174 nonce nouns. These nouns were then annotated by 21 volunteer annotators. Participants were asked to provide the plural of the nonce word and were given an example (Wug \u2192 Wugs) and the frame \u201cHier ist ein/e {nonce_word}. Jetzt sind es zwei .\u201d Similarly to English, this yielded mappings between a lemma and a ranked list of inflected nouns."
        },
        {
            "heading": "3.3 Tamil",
            "text": "Tamil is a Dravidian language primarily spoken in regions of South India and Sri Lanka. It is an agglutinative languange in which verbs are conjugated for tense, transitivity, person, number, and (in some cases) gender. For the most part, affixes display allomorphy only due to phonological conditioning and are otherwise invariant across verbs, as is the casewith the person/number/gender (PNG) affix (Arden, 1891, 71). This is not the case, however, for tense markers. Among linguists working on Tamil, it is not completely agreed upon how many verb classes there are in the language, with some proposing up to 13 and others as few as three (Lisker, 1951\u037e Agesthialingom, 1971). In the spoken form of Tamil, there are points where verbs are part of completely different classes than their literary counterpart, so in this study we focus exclusively on the written form (Schiffman and Renganathan, 2009). To simplify the analysis, we utilize a modification of Graul\u2019s classification seen in The English Dictionary of the Tamil Verb, where there are seven primary classes (Schiffman and Renganathan, 2009). The tense most impacted by these verb classes is the past tense, with each class having a unique form, while the present and future only demonstrate three forms across the classes. As such, we focus on the past tense and designate the same transitivity (intransitive) and PNG (third person singular masculine) affix across all experiments. In examining this, we gain information about the ways LLMs handle morphologically complex languages with inflectional classes defined in both phonological and morphological terms. This contrasts with English, where inflection is not agglutinative, and Turkish, where morphology is agglutinative but where there are no inflectional classes. To create a dataset for training the baseline models and generating samples for the few-shot\nprompts, 86 common Tamil verbs were sampled and conjugated with every possible combination of tense and PNG suffixes. These conjugations were generated automatically and then validated by two native speakers for accuracy. Unlike in the nonce word case, there was 100% agreement between speakers. The nonce words were generated by combining syllables from real verb roots and checking against a Tamil dictionary to assure the words created were not real. Nonce verbs were created to be between two and six letters long to best match the distribution of real Tamil verbs. In order to get the \u201ccorrect\u201d past tense for these verbs, five native Tamil speakers were asked to provide past tense forms (e.g.,\u0ba8\u0bbf\u0b9f\u0bc1 ni\u0288u\u2192 [\u0ba8\u0bbf\u0b9f\u0bc1\u0ba4\u0bcd\u0ba4\u0bbe\u0ba9\u0bcd ni\u0288ut\u032a\u02d0a\u02d0n, \u0ba8\u0bbf\u0b9f\u0bcd\u0b9f\u0bbe\u0ba9\u0bcd ni\u0288\u02d0a\u02d0n, \u0ba8\u0bc0\u0b9f\u0bbf\u0ba9\u0bbe\u0ba9\u0bcd ni\u02d0\u0288ina\u02d0n]). The mode of these responses was taken to be the gold form, with the level of agreement amongst speakers recorded for later analysis. The comparatively lower inter-annotator agreement can be explained by the lack of historical and linguistic context given to the annotators, since a large part of classification is historical."
        },
        {
            "heading": "3.4 Turkish",
            "text": "Turkish is an agglutinative language where words consist of multiple morphemes attached to a root. Surface realizations of morphemes are influenced by deterministic morphophonological processes like vowel harmony, consonant assimilation, and elision. Unlike many other languages, Turkish has complex word form morphotactics, particu-\nlarly when multiple derivations are present. To simplify the task and reduce the number of feature combinations, we utilized four datasets with different levels of complexity and a limited number of inflectional features. In most cases, the context provides an inflected form with one set of features, and the model must predict the form with the requested set of features. The first three tasks are reinflection tasks, demanding proficiency in both morphotactics and morphographemics. The fourth task is a straightforward inflection task (see Table 3). Each task consists of up to five shot examples for real roots and 10 test examples with nonce roots. Stimuli and gold annotations were produced by our (single) Turkish annotator."
        },
        {
            "heading": "4 Methodology",
            "text": "We compare the outputs of ChatGPT under a variety of prompting regimens and a substantial set of supervised baselines (both neural and non-neural) to human annotations of the data described in Appendix 3. Results are evaluated using accuracy at k (acc@k), i.e., a model\u2019s response is regarded as correct if it is in line with any of the top k human responses. This evaluation method takes into account inter-speaker morphological variability, which is more wide-spread than previously thought (Dammel and Schallert, 2019)."
        },
        {
            "heading": "4.1 Baselines",
            "text": "We investigate the efficacy of several baselines for the task of morphological inflection. The chosen baselines encompass both statistical and neural architectures that have shown impressive performance on the morphological generalization task in recent years. We evaluate their performance on the SIGMORPHON 2023 task as well as on our constructed wug test set. The baselines have complementary strengths (see Section 5)."
        },
        {
            "heading": "4.1.1 Training Data",
            "text": "We used the train/dev/test splits of the SIGMORPHON 2023 Inflection Shared Task3 for English and German. The choice of the train/dev/test splits was motivated by the fact that there was no overlap of lemmata between the individual splits, thus mimicking a wug-like setting. The Turkish training data for baselines was generated directly using a Turkish morphological ana3https://github.com/sigmorphon/ 2023InflectionST\nlyzer/generator (Oflazer, 1994), because the aforementioned SIGMORPHON 2023 dataset did not have a sufficient number of examples for most of the feature combinations. The morphological generator was set up to generate only Turkish word forms that corresponded to the selected inflectional morpheme combinations we selected, for all applicable roots. For testing, we expected the baseline systems to generate the word forms with the selected inflectional feature combinations, but for 10 nonce roots. The nonce roots were chosen so that they would force the inflected forms to orthogonally adhere to surface morphographemic constraints and rules such as various types of vowel harmony, consonant elision, or assimilation at morpheme boundaries. Similarly, for Tamil, we split the data into train and dev sets. Since we have a limited amount of Tamil data, we kept the split ratio at around 4:1 between train and dev sets. We report the results of all baselines in Table 4. Baselines generally perform as expected, validating our usage of them. It should be noted that MinGen and AED are evaluated in IPA/feature space and may therefore be at a disadvantage compared to baselines operating directly in orthography. The training data was converted from orthography into IPA using Epitran (Mortensen et al., 2018)."
        },
        {
            "heading": "4.1.2 Affix Rule Learner (ARL)",
            "text": "As a baseline for the 2020 and 2021 SIGMORPHON shared tasks, a simple non-neural system (Liu and Mao, 2016) was implemented that uses edit distance to \u201cdiscover prefix and suffix rules in training data.\u201d4 At test time, the system modifies a lemma by applying the longest matching suffix rule and most frequently applied prefix rule for a given morphosyntactic description."
        },
        {
            "heading": "4.1.3 Minimal Generalization Learner (MinGen)",
            "text": "Wilson and Li (2021) proposed a minimal generalization model based on a simplified form of Albright and Hayes (2002) to learn morphological rules. First, base rules that describe the changes needed to convert a lemma to an inflected form are generated from training data. The rules are further generalized by comparing phonological features of the rule contexts. The rules are then scored by a confidence metric based on their accuracy and\n4https://github.com/sigmorphon/2021Task0/tree/ main/baselines\nscope. At test time, the rule with the highest score among the applicable rules is used."
        },
        {
            "heading": "4.1.4 Feature Invariant Transformer (FIT)",
            "text": "Wu et al. (2021) proposed a simple technique employing a character-level transformer for featureguided transduction that was used as a baseline for the 2021 SIGMORPHON shared task.5 This is a generative model capable of performing characterlevel decoding to generate target inflections. In comparison to a vanilla transformer model, positional counts are used only for characters and not for features. The model also incorporates unique tokens to mark whether a given token is a feature."
        },
        {
            "heading": "4.1.5 Principle Parts for Inflection (PPI)",
            "text": "We apply the approach of Liu and Hulden (2020), which recasts the task of morphological inflection as a \u201cparadigm cell filling problem.\u201d This leverages a lexeme\u2019s principal parts\u2014the minimum subset of paradigm slots needed to generate the other slots in its paradigm. Specifically, for low-resource scenarios, the principal parts of a paradigm identify additional slots that are crucial in generating the target-inflected lemma."
        },
        {
            "heading": "4.1.6 Analogical Encoder-Decoder (AED)",
            "text": "Following up on Albright and Hayes (2003) and Kirov and Cotterell (2018), Calderone et al. (2021) proposed a recurrent neural network encoder-decoder architecture augmented with precompiled analogical patterns for generating morphological inflections of nonce words. This model leverages the UniMorph Tags and fine alternation pattern (FAP) associated with each lemma in relation to its inflection form. FAPs analyze the positioning of word forms within the system to identify recurrent patterns representing conventional linguistic elements.\n5https://github.com/sigmorphon/2021Task0/tree/ main/baselines"
        },
        {
            "heading": "4.2 Prompting",
            "text": "We employ three distinct prompting styles, namely zero-, one-, and few-shot, to interact with the language model. We start with a simple instruction in each language, for example:\n\u201cFill in the blank with the correct past tense of the word \u2018wug\u2019. Give your response in one word. They wug all the time. In fact, they just yesterday.\u201d\nFor Tamil, the instruction portion of the prompt is omitted because of ChatGPT\u2019s unreliable performance when given instructions in that language. We select one examplewith real words for eachmajor inflection class of the phenomenon in question. We then perform multiple runs: 10 for the zeroshot scenario, one for every shot for the one-shot scenario, and 10 for the few-shot scenario, with a new random permutation of all examples each time. We query gpt-3.5-turbo-0613, select the first word of the response, and filter by removing non-word characters. We evaluate by computing the accuracy for each of the runs, averaged over all queried nonce words, and compute the mean and standard deviation across all runs. We employ acc@k as our evaluation metric, setting k = 5 for our main evaluation. We provide results for k = 1 and k = 3 in Appendix A.4. The k gold forms are the k responses most frequently generated by humans. Since only one Turkish response is possible (the morphology is deterministic), k is always 1 for this language. We then perform an additional experiment for comparison in which we remove the context around the nonce word and only give the instructions as well as the last line. We call this the short prompt and the original described above the long prompt. We provide instances of long and short prompt in Appendix A.5."
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 Overall Performance",
            "text": "For acc@5, the performance of ChatGPT never exceeded that of the strongest baselines (ARL, AED, and PPI) regardless of the prompting regime, as shown in Table 5. However, it beats certain older baselines such as MinGen (the minimum generalization learner). ChatGPT performed best when it was explicitly prompted to complete an analogy with a single example (i.e., short 1-shot), as can be seen in Figure 2. We observe that similar trends hold for acc@1 and acc@3 (see Appendix A.4), but the gap between the strongest baselines and ChatGPT decreases with k.\nEnglish ChatGPT\u2019s performance on Englishwas uniformly worse than both the average annotator (87.64%) and the strongest baselines. acc@1 falls below 60% in the 0-shot condition but is markedly better when shots are supplied. Short prompts, which require the model to complete a simple analogy, resulted in better performance than long prompts. In all conditions, authentic Englishwords that did not occur in the reference annotations appeared as outputs when the nonce word and the authentic word were orthographically similar (see the discussion in Section 6.4).\nGerman The best German result was 88.94% (short 1-shot), which beat all of the baselines except for ARL and FIT. The other results are similarly strong in contrast to the other languages. The impact of k is not noticeable here. This, in combination with the fact that the human performance on acc@5 was 88%, indicates that the task is perfectly performed by ChatGPT. It has reached the upper bound given by the inherent subjectivity of the task (reflected in the human variability) and the impact of k is, therefore, not measurable. This is further solidified by the very small impact of the long vs. short prompts.\nTamil Tamil performance of ChatGPT was significantly worse than the provided baselines, even in the few-shot conditions. For the few-shot case, there was marginally better performance when using short prompts, but this did not apply to the 0- or 1-shot case (in which no accurate outputs were generated). Across the board, the performance on Tamil was markedly worse than performance on English and German. However, considering that the average annotator had only 43.85% accuracy\nagainst the judgments of the other annotators, the few-shot accuracy is quite reasonable.\nTurkish The prompting performance for the Turkish inflection task is worse than for English and German, especially in the long prompt case. For this task, the morphotactics is trivial but the selection of the allomorph depends on stem vowels, stem-final consonants, whether there is a consonant cluster ending the stem, and whether the stem is monosyllabic or not. ChatGPT gets better results with the short prompt through an analogical example. For the three reinflection tasks, ChatGPT gets mixed results that are overall worse than for the inflection task (see Table 6)."
        },
        {
            "heading": "6 Analysis",
            "text": ""
        },
        {
            "heading": "6.1 The Nature of the Task",
            "text": "The inherent complexity of the inflection tasks for the various languages (and the reinflection task for Turkish) varies greatly. English and Turkish are the simplest: the top-ranked form can always be obtained by adding a single suffix and applying a few morphographemic alternations. German annotations show no dominant pattern and assign nonce words to morphological classes according to complex criteria. However, German performance is clearly better, suggesting that factors other than inherent complexity play a role in ChatGPT\u2019s ability to generalize morphological patterns."
        },
        {
            "heading": "6.2 Impact of Tokenization",
            "text": "There is mounting evidence that the morphologically suboptimal nature of many tokenizers may limit the morphological capabilities of LLMs (Bostrom andDurrett, 2020\u037e Hofmann et al., 2021). ChatGPT\u2019s tokenization, i.e., byte-pair encoding\n(Sennrich et al., 2016), has been shown to be particularly problematic (Bostrom and Durrett, 2020\u037e Hofmann et al., 2022). To examine the impact of tokenization, we measured the number of tokens into which the nonce words are split for the individual languages and computed the accuracy as a function of the number of tokens. Our hypothesis was that longer token sequences are less optimal, potentially leading to worse performance. However, using two-sided t-tests, we did not find a significant difference between nonce words with different token lengths. We interpret this as indicating that tokenization plays a less pronounced role for ChatGPT."
        },
        {
            "heading": "6.3 Impact of k",
            "text": "We observe that the gap between the baselines and our results increases with k (see Table 5, Appendix A.4), suggesting that ChatGPT tends to generate either a top-ranked form or an implausible inflection while the baselines tend to produce plausible inflections which are less frequent in the human annotations. ChatGPT\u2019s penchant for implausible inflectionsmay be a result of its real word bias (see Section 6.4 below)."
        },
        {
            "heading": "6.4 Real Word Bias",
            "text": "In English and German\u2014and to a lesser extent in Turkish\u2014many of the forms generated by ChatGPT belong to a different lexeme than the nonce word and thus do not constitute inflections in any\nstrict linguistic sense (see Section 2.1). Crucially, the stem of the generated form is always a real word (i.e., a word that exists in the respective language). Examples of this phenomenon include, for English: did as the past tense of dedo, blushed as the past tense of blus, fried as the past tense of fride\u037e and for German: Ozeane (\u2018oceans\u2019) as the plural of Ozeak, Institute (\u2018institutes\u2019) as the plural of Instite, Sklaven (\u2018slaves\u2019) as the plural of Schlave. It is important to notice that in all these cases, (i) the generated form has the correct morphological properties\u2014e.g., the English forms did, blushed, fried are indeed past tense forms\u2014but the stem is a real word rather than the nonce word, and (ii) the stem that is generated in lieu of the nonce word is a frequently occurring word in the respective language and has a certain (sometimes strong) orthographic similarity to the nonce word. We denote this tendency real word bias.\nThe concept of real word bias allows us to make a hypothesis about the way in which ChatGPT addresses morphological tasks. We think ChatGPT is not applying morphological rules to a stem, which would be in line with item-and-process accounts of morphology (Hockett, 1954). Rather, it seems to linguistically decode the point in its representational space defined by the semantic constraints in the prompt. In cases where this point (and its immediate neighborhood) is unoccupied, it generates a form based on the nonce word, but in cases where there is a form of a real word close to the point (e.g., because of superficial orthographic similarity), it generates this form instead. The fact that the real word bias is strongest for German and English (the two high-resource languages) suggests that the representational space is more dense for these two languages, increasing the probability that there is a real word close to the point that the model is trying\nto decode based on the prompt."
        },
        {
            "heading": "6.5 Morphological Productivity",
            "text": "The productivity of a morpheme is traditionally defined as its propensity to be used in novel combinations (Plag, 1999\u037e Bauer, 2001\u037e Haspelmath and Sims, 2010). Crucially, morphemes with the same meaning can differ in their productivity\u2014for example, for English deadjectival nominalizing suffixes, -ness (e.g., robustness) is generallymore productive than -ity (e.g, equality), which in turn is more productive than the fully non-productive -th (e.g., warmth). We are interested to see whether there is any difference in the productivity of morphological patterns exhibited by ChatGPT compared to the human sample. We focus on German as it has the most complex pattern of competing morphemes, and we examine the few-shot results as they show the best performance overall. We start by comparing the distribution over alternative plural morphemes generated by ChatGPT with the human responses. As shown in Figure 3, there are several morphemes that are used by ChatGPT similarly to humans (e.g., the null morpheme). Cases of overgeneralization, where ChatGPT systematically generalizes the usage of a particular suffix to contexts where the suffix is not used by humans, are mainly limited to two plural morphemes: -en (77 generations for gold morpheme -e) and -s (79 generations for gold morpheme -e). Interestingly, these two plural morphemes are the two most productive plural morphemes in German (K\u00f6pcke, 1988). This indicates two important points: (i) ChatGPT is sensitive to the productivity of morphemes, i.e., it has acquired the ability to model how productive certain morphemes are\nas a result of pretraining\u037e (ii) it does not identically mirror the behavior of humans, but rather amplifies the productivity of certain morphemes. The finding that the most productive morphemes (for humans) are becoming more productive for ChatGPTwhile the least productivemorphemes (for humans) are becoming less productive for ChatGPT bears some theoretical resemblance to discussions about bias amplification (Ahn et al., 2022)."
        },
        {
            "heading": "7 Future Directions",
            "text": "Morphological patterns are only one kind of generalization that can be investigated through a wuglike experimental paradigm. The form-meaning relationships encoded in language and multimodal models, including constructional and iconic pairings, can be investigated through prompting with nonce stimuli, leading to new insights regarding the generalizations they capture.\nLimitations\nOur research was conducted with a single model (gpt-3.5-turbo-0613), so it is not certain that our results will generalize to other versions of GPT-3 or to GPT-4, let alone other LLMs. Although we went to great lengths to develop prompts that would maximize ChatGPT\u2019s performance on the tasks, it is not possible to state definitively that another strategy would not produce better performance. While the languages were typologically varied, it is not clear whether the results observed in the current study are generally robust or are coincidental properties of the small set of languages and datasets under investigation. Furthermore, comparing the languages to one another is problematic because it was not possible to control other variables while varying the language. For example, the English and Tamil tasks involve verbal inflection while the German and Turkish tasks involve nominal inflection. Finally, the number of annotators for Tamil was very small and inter-annotator agreement was very low, meaning that the results of the Tamil experiments must be approached with special caution (but see our discussion about morphological variation in Section 3).\nEthics\nLLMs are already impacting the world\u2019s people in significant ways, for good and ill. Understanding their limitations, particularly with regard to nonhegemonic language communities, is an ethical im-\nperative. This study highlights one specific way in which an LLM should not be treated as a surrogate human, thus motivating additional research on language modeling for structurally diverse and low-resource languages."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was funded by the European Research Council (#740516). The second author was also supported by the German Academic Scholarship Foundation. We thank the reviewers for their extremely helpful comments."
        },
        {
            "heading": "A Appendices",
            "text": "A.1 Morphological Tags In Table 7, we provide details about themorphological tags that are comprised by the train, dev, test, and wug test sets for the four languages. The tags for English (eng), German (deu), and Tamil (tam) are defined in accordance with the description in UniMorph 4.0 dataset. For Turkish (tur),the tags are defined in Section 3.\nA.2 Hyperparameter Tuning For all baselines, we follow the hyperparameter settings from the publicly available code repositories. The only exception is AED, where the number of epochs was increased from 40 to 200.\nA.3 Qualtrics Details Our study leveraged Qualtrics, a robust and comprehensive survey software tool that facilitates the\ndesign of intricate online surveys.6 We initiated the survey by presenting an introduction that detailed the concept of a wug test and the associated information for the survey. This introductory passage served to inform participants of the nature and intent of the research study, and it also provided examples to further facilitate their understanding of our task requirements. Our data collection phase consisted of two parts: the English wug test and the German wug test. Upon consenting to participate, respondents were guided through a series of thoughtfully designed prompts related to the wug test. These prompts encouraged them to provide suitable responses based on their understanding of the task. For the English wug test, we employed the following exemplary prompt: \u201cFill in the blank with the correct past tense of the word \u2018wug\u2019. There is no predetermined correct answer. We encourage you to rely on your linguistic intuition. If you believe there are multiple possible responses, simply note the form that seems most accurate to you. For instance, \u2018They wug all the time. In fact, they __ just yesterday!\u2019\u201d. Such prompts stimulated the participants to produce responses that were entirely their own, drawing on the provided information. For the German wug test, we translated the task instructions and prompts into German, ensuring easy comprehension for native German speakers. In total, the English wug test incorporated 50 unique words for participants to respond to, while the German version consisted of 174 unique words. We received 28 responses for the English wug test and 21 responses for the German wug test.\nA.4 Other Values of k Table 8 presents results for k = 1 and k = 3. Results for k = 5 are given in Section 5.\nA.5 Prompts We leveraged the following prompts for the individual languages:\n\u2022 English:\n\u2013 Long: \u201cFill in the blank with the correct past tense of the verb X. Answer with one word. They X all the time. In fact, they _ just yesterday! _ :\u201d\n\u2013 Short: \u201cForm the correct past tense of the verb X. Answer with one word. X :\u201d\n6https://www.qualtrics.com/\n\u2022 German:\n\u2013 Long: \u201cF\u00fclle die L\u00fccke mit dem korrekten Plural des Nomens X aus. Antworte mit einemWort. Hier ist ein X. Jetzt sind es zwei _! _:\u201d\n\u2013 Short: \u201cBilde den korrekten Plural des Nomens X. Antworte mit einemWort. X :\u201d\n\u2022 Tamil:\n\u2013 Long: \u201c\u0bc7\u0ba8\u0bb1\u0bcd\u0bb1\u0bc1 \u0b85\u0bb5\u0bb0\u0bbf\u0b9f\u0bae\u0bcd, \"\u0ba8\u0bc0 X\" \u0b8e\u0ba9\u0bcd\u0bc7\u0bb1\u0ba9\u0bcd. \u0b85\u0bc8\u0ba4\u0b95\u0bcd \u0bc7\u0b95\u0b9f\u0bcd\u0b9f\u0bc1 \u0b85\u0bb5\u0ba9\u0bcd \u0bc7\u0baa\u0bbe\u0baf\u0bcd _. _:\u201d\n\u2013 Short: \u201cX :\u201d\n\u2022 Turkish:\n\u2013 Long: \u201cBo\u015fluklar\u0131 X ile verilen eylemin birinci tekil \u015fah\u0131s ge\u00e7mi\u015f zaman formlar\u0131 ile doldurun. Ben her zaman X. Ama d\u00fcn _. _:\u201d\n\u2013 Short: \u201cTek bir s\u00f6zc\u00fck ile farazi X eyleminin birinci tekil \u015fah\u0131s ge\u00e7mi\u015f zaman hali nas\u0131l olur? X :\u201d"
        }
    ],
    "title": "Counting the Bugs in ChatGPT\u2019s Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model",
    "year": 2023
}