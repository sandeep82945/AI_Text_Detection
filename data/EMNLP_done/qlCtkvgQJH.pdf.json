{
    "abstractText": "Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive chain-of-thought reasoning ability. Recent work on self-instruction tuning, such as Alpaca, has focused on enhancing the general proficiency of models. These instructions enable the model to achieve performance comparable to GPT-3.5 on general tasks like open-domain text generation and paraphrasing. However, they fall short of helping the model handle complex reasoning tasks. To bridge the gap, this paper presents LogiCoT, a new instruction-tuning dataset for Logical Chain-of-Thought reasoning with GPT-4. We elaborate on the process of harvesting instructions for prompting GPT-4 to generate chainof-thought rationales. LogiCoT serves as an instruction set for teaching models of logical reasoning and elicits general reasoning skills.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hanmeng Liu"
        },
        {
            "affiliations": [],
            "name": "Zhiyang Teng"
        },
        {
            "affiliations": [],
            "name": "Leyang Cui"
        },
        {
            "affiliations": [],
            "name": "Chaoli Zhang"
        },
        {
            "affiliations": [],
            "name": "Qiji Zhou"
        },
        {
            "affiliations": [],
            "name": "Yue Zhang"
        }
    ],
    "id": "SP:8c09544a58541ad2d87414bd5a59df77024c3859",
    "references": [
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Bhavana Dalvi",
                "Peter Alexander Jansen",
                "Oyvind Tafjord",
                "Zhengnan Xie",
                "Hannah Smith",
                "Leighanna Pipatanangkura",
                "Peter Clark."
            ],
            "title": "Explaining answers with entailment trees",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2021
        },
        {
            "authors": [
                "Yao Fu",
                "Hao Peng",
                "Litu Ou",
                "Ashish Sabharwal",
                "Tushar Khot."
            ],
            "title": "Specializing smaller language models towards multi-step reasoning",
            "venue": "arXiv preprint arXiv:2301.12726.",
            "year": 2023
        },
        {
            "authors": [
                "Luyu Gao",
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Pengfei Liu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig."
            ],
            "title": "Pal: Program-aided language models",
            "venue": "arXiv preprint arXiv:2211.10435.",
            "year": 2022
        },
        {
            "authors": [
                "Tao Yu",
                "Rui Zhang",
                "Shafiq Joty",
                "Alexander R. Fabbri",
                "Wojciech Kryscinski",
                "Xi Victoria Lin",
                "Caiming Xiong",
                "Dragomir Radev"
            ],
            "title": "Folio: Natural language reasoning with first-order logic",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Saurav Kadavath",
                "Akul Arora",
                "Steven Basart",
                "Eric Tang",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring mathematical problem solving with the math dataset",
            "venue": "arXiv preprint arXiv:2103.03874.",
            "year": 2021
        },
        {
            "authors": [
                "Cheng-Yu Hsieh",
                "Chun-Liang Li",
                "Chih-Kuan Yeh",
                "Hootan Nakhost",
                "Yasuhisa Fujii",
                "Alexander Ratner",
                "Ranjay Krishna",
                "Chen-Yu Lee",
                "Tomas Pfister"
            ],
            "title": "Distilling step-by-step! outperforming larger language models with less training data and smaller",
            "year": 2023
        },
        {
            "authors": [
                "Jie Huang",
                "Kevin Chen-Chuan Chang."
            ],
            "title": "Towards reasoning in large language models: A survey",
            "venue": "arXiv preprint arXiv:2212.10403.",
            "year": 2022
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "arXiv preprint arXiv:2205.11916.",
            "year": 2022
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Jack Hessel",
                "Youngjae Yu",
                "Xiang Ren",
                "Kai-Wei Chang",
                "Yejin Choi."
            ],
            "title": "Symbolic chain-of-thought distillation: Small models can also \u201cthink\u201d step-by-step",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational",
            "year": 2023
        },
        {
            "authors": [
                "Tao Li",
                "Vivek Srikumar."
            ],
            "title": "Augmenting neural networks with first-order logic",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 292\u2013302, Florence, Italy. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Hanmeng Liu",
                "Ruoxi Ning",
                "Zhiyang Teng",
                "Jian Liu",
                "Qiji Zhou",
                "Yue Zhang"
            ],
            "title": "Evaluating the logical reasoning ability of chatgpt and gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "Jian Liu",
                "Leyang Cui",
                "Hanmeng Liu",
                "Dandan Huang",
                "Yile Wang",
                "Yue Zhang."
            ],
            "title": "Logiqa: A challenge dataset for machine reading comprehension with logical reasoning",
            "venue": "CoRR.",
            "year": 2020
        },
        {
            "authors": [
                "Qian Liu",
                "Fan Zhou",
                "Zhengbao Jiang",
                "Longxu Dou",
                "Min Lin."
            ],
            "title": "From zero to hero: Examining the power of symbolic tasks in instruction tuning",
            "venue": "arXiv preprint arXiv:2304.07995.",
            "year": 2023
        },
        {
            "authors": [
                "Shayne Longpre",
                "Le Hou",
                "Tu Vu",
                "Albert Webson",
                "Hyung Won Chung",
                "Yi Tay",
                "Denny Zhou",
                "Quoc V. Le",
                "Barret Zoph",
                "Jason Wei",
                "Adam Roberts"
            ],
            "title": "The flan collection: Designing data and methods for effective instruction tuning",
            "year": 2023
        },
        {
            "authors": [
                "Bill MacCartney",
                "Christopher D. Manning."
            ],
            "title": "Natural logic for textual inference",
            "venue": "Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.",
            "year": 2007
        },
        {
            "authors": [
                "Lucie Charlotte Magister",
                "Jonathan Mallinson",
                "Jakub Adamek",
                "Eric Malmi",
                "Aliaksei Severyn"
            ],
            "title": "Teaching small language models to reason",
            "year": 2023
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Hannaneh Hajishirzi."
            ],
            "title": "Cross-task generalization via natural language crowdsourcing instructions",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Santiago Ontanon",
                "Joshua Ainslie",
                "Vaclav Cvicek",
                "Zachary Fisher"
            ],
            "title": "Logicinference: A new dataset for teaching logical inference to seq2seq models",
            "year": 2022
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Baolin Peng",
                "Chunyuan Li",
                "Pengcheng He",
                "Michel Galley",
                "Jianfeng Gao"
            ],
            "title": "Instruction tuning with gpt-4",
            "year": 2023
        },
        {
            "authors": [
                "drea Santilli",
                "Thibault Fevry",
                "Jason Alan Fries",
                "Ryan Teehan",
                "Teven Le Scao",
                "Stella Biderman",
                "Leo Gao",
                "Thomas Wolf",
                "Alexander M Rush"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "In International Conference on Learning",
            "year": 2022
        },
        {
            "authors": [
                "Tian-Xiang Sun",
                "Xiang-Yang Liu",
                "Xi-Peng Qiu",
                "Xuan-Jing Huang."
            ],
            "title": "Paradigm shift in natural language processing",
            "venue": "Machine Intelligence Research, 19(3):169\u2013183.",
            "year": 2022
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proceedings of the",
            "year": 2018
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki."
            ],
            "title": "GPT-J6B: A 6 Billion Parameter Autoregressive Language Model",
            "venue": "https://github.com/kingoflolz/ mesh-transformer-jax.",
            "year": 2021
        },
        {
            "authors": [
                "Peifeng Wang",
                "Aaron Chan",
                "Filip Ilievski",
                "Muhao Chen",
                "Xiang Ren"
            ],
            "title": "Pinto: Faithful language reasoning using prompt-generated rationales",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le"
            ],
            "title": "Finetuned language models are zero-shot learners",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Weihao Yu",
                "Zihang Jiang",
                "Yanfei Dong",
                "Jiashi Feng."
            ],
            "title": "Reclor: A reading comprehension dataset requiring logical reasoning",
            "venue": "Proc. of ICLR.",
            "year": 2020
        },
        {
            "authors": [
                "Eric Zelikman",
                "Yuhuai Wu",
                "Jesse Mu",
                "Noah D. Goodman"
            ],
            "title": "2022. Star: Bootstrapping reasoning with reasoning",
            "year": 2022
        },
        {
            "authors": [
                "Ruiqi Zhong",
                "Kristy Lee",
                "Zheng Zhang",
                "Dan Klein"
            ],
            "title": "Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections",
            "year": 2021
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba"
            ],
            "title": "Large language models are human-level prompt engineers",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Instruction tuning Large Language Models (LLMs) has become a popular paradigm for Natural Language Processing (NLP) in recent years (Ouyang et al., 2022; Sun et al., 2022). A prominent line of research is the development of OpenAI\u2019s ChatGPT and GPT-4 (OpenAI, 2023). LLMs demonstrate multi-step chain-of-thought (CoT) reasoning ability with proper prompting (Kojima et al., 2022; Huang and Chang, 2022). CoT instruction tuning has drawn attention for its potential to encourage complex, step-by-step reasoning. For example, Wei et al. (2023) and Kojima et al. (2022) have demonstrated the ability of LLMs to generate a coherent sequence of reasoning steps leading to the final answer through CoT prompting. Moreover, ChatGPT and GPT-4 have shown remarkable zero-shot complex reasoning abilities on several logical reasoning datasets (Liu et al., 2023a).\nYet, developing such proprietary models as GPT4 and ChatGPT often necessitates intensive data\n\u2217Corresponding Authors\nand instruction engineering, a process that has, thus far, been largely kept private. Recent research endeavours have begun to explore the distillation of the instruction data using self-instruct techniques (Wang et al., 2022; Peng et al., 2023), where GPT-3 or GPT-4 are used to generate instruction-following examples. This technique represents a promising avenue for reducing the human labour involved in instruction tuning, offering a more economical way to produce community models trained with instructional data. A paradigmatic example is the pipeline by Wang et al. (2022) for cultivating instruction data, where initial instructions are authored by humans, and LLMs are then used to extend this instruction data. This pipeline has been used to produce multiple open-sourced, instruction-tuned\nmodels, such as Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023). The instruction tuning data share a common pool of instructions and fixed task templates. However, the scope of these instructions is limited and does not encapsulate diverse, complex reasoning scenarios such as multi-step logical reasoning.\nLogical reasoning is crucial in human cognition, embodying the ability to infer conclusions based on a structured progression of premises. The dearth of such abilities in community models (Liu et al., 2023a) presents a significant gap, inhibiting the development of open LLMs with strong chain reasoning. While GPT-4 has demonstrated its ability to produce high-quality CoT reasoning output, the potential of generating CoT instruction tuning data using this model remains largely unexplored. The need to cover more diverse and complex reasoning scenarios, particularly multi-step logical reasoning, represents a significant gap in the current instruction-tuning landscape.\nWe aim to address this gap by scaling up the instruction set (Chung et al., 2022), paving the way for more nuanced and sophisticated instructiontuned models. To this end, we introduce LogiCoT, a chain-of-thought (CoT) instruction-tuning dataset designed explicitly for logical reasoning. Our approach involves repurposing existing logical reasoning datasets, constructing logical reasoning CoT instructions from these resources, and leveraging the capabilities of GPT-4 to generate high-quality outputs. The resulting instruction data features both symbolic reasoning and multi-step CoT reasoning, providing a comprehensive and nuanced resource for enhancing the logical reasoning abilities of AI models.\nUsing LogiCoT, we fine-tune a LLaMA-7b (Touvron et al., 2023) model with an instruction-tuning scheme, resulting in a small-sized instruction-tuned open model. Results on both logical reasoning benchmarks and general human-centric benchmarks indicate remarkable performance elevations compared to state-of-the-art instruction-tuned models, showing the promise of leveraging complex CoT instruction data in the instruction-tuning process of LLMs.\nOur work is in line with recent research indicating that smaller language models can achieve competitive multi-step reasoning abilities when specialized on targeted CoT tasks. Examples of these tasks include the execution of SQL commands, mathe-\nmatical CoT reasoning, and generating code snippets (Gao et al., 2022; Liu et al., 2023b; Fu et al., 2023). By applying a similar specialization approach to the broader and more complex domain of logical reasoning, we aim to bring these capabilities into the mainstream, furthering the development of AI systems with advanced logical reasoning skills.\nWe release both the training data and model weights."
        },
        {
            "heading": "2 Related Work",
            "text": "Instruction tuning LLMs. Instruction-tuning of Large Language Models (LLMs) has become a thriving research area in Natural Language Processing (NLP), aiming to enable zero-shot generalization on unseen tasks (Zhong et al., 2021; Ouyang et al., 2022; Wei et al., 2022). This involves finetuning LLMs to perform diverse tasks by following a set of instructions, making the task source an essential component of instruction tuning (Longpre et al., 2023). Most existing instruction tuning methods rely heavily on human-crowdsourced tasks or model-generated tasks. Human-crowdsourced tasks originate from a variety of sources, such as T0 (Sanh et al., 2022), FLAN (Wei et al., 2022), and NaturalInstructions (Mishra et al., 2022). These tasks, although high-quality, rely on substantial human effort and are often limited in quantity. In contrast, model-generated tasks involve leveraging a powerful language model, such as GPT-3 and GPT-4, to generate a diverse set of instructions, task inputs, and task outputs based on a seed set (Wang et al., 2022; Peng et al., 2023). We seek to leverage GPT-4\u2019s chain-of-thought reasoning capabilities in instruction tuning. By introducing the LogiCoT dataset and incorporating symbolic tasks, we aim to advance the quality and scalability of instruction-following data and thereby enhance the overall performance of instruction-tuned LLMs.\nPINTO (Wang et al., 2023) offers a two-fold approach to elucidate the reasoning process of large language models (LMs). The core innovation lies in its prompt-based learning combined with counterfactual regularization to ensure faithfulness in the reasoning over generated rationales. While PINTO excels in achieving superior performance across in-distribution and out-of-distribution test sets and ensuring that rationales are faithful, its primary motivation is different from LogiCoT. Whereas PINTO emphasizes rationale transparency and faithfulness, LogiCoT is geared towards using\nlogic-related chain-of-thought data to achieve advanced logical reasoning skills.\nTeaching small language models to reason is another research direction (Magister et al., 2023; Li et al., 2023; Hsieh et al., 2023) that underscores the significance of knowledge distillation in porting reasoning capabilities from mammoth models to their relatively compact counterparts. Central to this approach is the fine-tuning of smaller models on the chain-of-thought rationales spawned by larger models. Such a strategy has demonstrated marked improvements in various reasoning datasets. Yet, this is where LogiCoT diverges. Our approach uniquely employs logical reasoning rationales and capitalizes on the generative prowess of GPT-4.\nIn essence, while the broader arena is populated with innovative methodologies aiming to amplify the reasoning prowess of LMs, LogiCoT distinguishes itself by synergizing logical reasoning with the generative might of GPT-4, setting a precedent in logical reasoning tasks.\nChain-of-thought rationales. Large Language Models (LLMs) can conduct complex reasoning tasks by generating intermediate reasoning steps through a process called chain-of-thought (CoT) prompting (Wei et al., 2023). Zero-Shot-CoT prompting uses a simple instruction (like \u201cLet\u2019s think step by step\u201d) to elicit step-by-step reasoning before answering a question. LLMs have exhibited reasonable zero-shot reasoning capabilities, generating outputs that inherently reflect CoT reasoning (Zhou et al., 2023). This notion inspired researchers to use self-generated rationales for demonstrations. In particular, Zelikman et al. (2022) demonstrated the practicality of using LLMs to generate rationales. They prompted GPT-J (Wang and Komatsuzaki, 2021) to generate rationales and then selected the ones leading to the correct answer. We adopt this method for data collection using GPT-4. Our approach, however, focuses on complex logical reasoning scenarios using questions with annotated answers.\nLogical reasoning. Logical reasoning is a key aspect of human cognition and a critical capability for AI systems. Researchers have been exploring various approaches to achieve this goal, including rule-based methods and symbolic systems (MacCartney and Manning, 2007), fine-tuning large language models (Wang et al., 2018), and combining both neural and symbolic approaches (Li and Sriku-\nmar, 2019). Logical reasoning tasks can require multi-step, complex reasoning, which makes them an ideal target for CoT instruction tuning. To our knowledge, we are the first to consider this method for logical reasoning, making use of rich reasoning chain data finetuning LLMs, increasing their task performance."
        },
        {
            "heading": "3 Dataset",
            "text": "The data construction of LogiCoT is a multistage process that uses GPT-4 as a teaching assistant, which is illustrated in Figure 1. We first establish a foundation by choosing suitable seeding data with gold output and optional CoT reasoning chains. With seeding data in place, we proceed to formulate instructions. The process involves translating the intended tasks into clear, unambiguous prompts that elicit GPT-4\u2019s capacity for logical reasoning rationales. We then combine the seeding data and the corresponding instructions and feed them into GPT-4 to generate responses. GPT-4\u2019s output is guided by the gold label and the reasoning chain. We use both the gold output and GPT-4 response as our instruction data. Figure 1 illustrates this process."
        },
        {
            "heading": "3.1 Seminal Data Selection",
            "text": "Selecting the seminal data for CoT instruction tuning of logical reasoning models involves choosing high-quality datasets that adequately cover the range of skills required for logical reasoning. The datasets should present challenges representing real-world logical reasoning tasks and be designed to support CoT instruction tuning. Below are the seminal instruction data we choose:\nLOGICINFERENCE (Ontanon et al., 2022) is a synthetically generated sequence-to-sequence dataset teaching models to perform logical inference using propositional logic and a subset of firstorder logic. Figure 2 in Appendix A shows an example. The input is a question, varying in types of problems ranging from language-to-logic translation to multi-step inference chains. The output provides the answer, including the reasoning chain to generate it. The output, in some cases, even provides the name of the inference rule used in each step.\nEntailmentBank (Dalvi et al., 2021) is an opendomain question answering data with rationales as an entailment tree. It uses multiple-choice questions from grade school science. The entailment\ntrees are constructed with human labour. Figure 3 in Appendix A shows an example. EntailmentBank provides a unique combination of open-domain question answering and logical reasoning in a format that closely aligns with our instruction tuning objectives. The structure of its rationales, presented as entailment trees, gives our model the opportunity to learn from and adapt to complex logical relationships in a controlled environment.\nFOLIO (Han et al., 2022) is an open-domain, logically complex and diverse dataset equipped with first-order logic (FOL) annotations. Figure 4 in Appendix A shows an example. What sets FOLIO apart is the parallel FOL annotations for each premise and conclusion, which are automatically verified by an FOL inference engine. This aspect provides a clear, precise standard for logical reasoning. In addition, the humanannotated nature of the dataset ensures high-quality data input. This dataset can be easily converted into a sequence-to-sequence structure, serving as instruction-following data for symbolic logic reasoning.\nReClor (Yu et al., 2020) and LogiQA (Liu et al., 2020) are datasets derived from verbal reasoning examinations, demanding various types of logical reasoning for answering multi-choice questions. Figure 5 in Appendix A shows an example from the ReClor data. These datasets are especially valuable in that they represent realistic human reasoning processes. Further, the real-world nature of the questions in these tests, which often require a mix of common sense and logical reasoning, ensures that the model is trained to tackle problems with varying degrees of complexity. We use the training set of the two datasets, keeping the test set out of the instruction tuning data. Specifically, we use the Chinese version of the LogiQA dataset.\nThese seminal datasets for CoT instruction tuning in GPT-4 offer a balanced, comprehensive, and challenging training environment. This approach ensures that the model gains exposure to a broad range of logical reasoning tasks, thus enhancing its ability to effectively handle similar tasks in realworld applications."
        },
        {
            "heading": "3.2 Instruction Types",
            "text": "We consider different types of instructions for instructing language models in various aspects of logical reasoning. Each type is designed to engage the model with logical inference tasks at differ-\nent levels of abstraction and complexity, with both natural language and symbolic language. To our knowledge, no similar instruction types exist in other instruction-following data.\nWe classify the instruction types into general inference (Section 3.2.1) and multi-choice reading comprehension (Section 3.2.2) tasks."
        },
        {
            "heading": "3.2.1 General Inference Task",
            "text": "This category includes instruction types that demand general reasoning and inferential skills, often involving an understanding of logical structures and principles. The model may need to perform operations such as translating natural language to formal logic, predicting possible inferences from given premises, or tracing inference chains. These tasks are designed to enhance the model\u2019s ability to think critically and logically, without relying too heavily on specific contexts or domain knowledge.\nTable 7 in Appendix B shows the instruction types for general inference. An example is offered to illustrate each instruction type.\nLanguage to Logic: This instruction involves translation from natural language into a more formal logical notation. It presents a foundational task of understanding and interpreting logical statements expressed in natural language and converting them into a formalized logical representation.\nOne-Step Inference: In this case, the model is presented with a set of premises and tasked with predicting all the potential inferences that can be derived from them in a single step. This type of instruction encourages the model to exercise deductive reasoning based on the provided premises. The premises and inferences can be in natural language or symbolic language. The latter encourages precise and abstract reasoning, while the former context simulates real-world language use scenarios.\nInference Chains: This instruction type takes logical reasoning a step further by requiring the model to establish whether a potential inference can be proven from a set of premises. The model must then provide the chain of reasoning leading to the answer. This type encourages deeper logical reasoning and the ability to construct logical arguments. The examples are either crafted in symbolic language or natural language."
        },
        {
            "heading": "3.2.2 Reading Comprehension Tasks",
            "text": "Machine reading comprehension (MRC) is the goto task for testing LLMs\u2019 reasoning ability, where a\nmodel is given a passage and a question and asked to find the answer. This category involves tasks that require a deep understanding of a given text, often demanding that the model identifies, extracts, or infers information from the text. The model might be asked to resolve a situation described in the text, to pinpoint a flaw in an argument presented, or to identify information that would either strengthen or weaken an argument.\nTable 8 in Appendix B shows the instruction types and running examples for logical reading comprehension.\nThe distinctiveness of these instruction types lies in their combination of logical reasoning tasks with natural language processing, providing a robust framework for training language models in logic-infused language understanding and generation. This comprehensive approach is unique to this data generation scheme and offers an innovative pathway for improving the logical reasoning capabilities of large language models."
        },
        {
            "heading": "3.3 Data Collection",
            "text": "We are granted early access to GPT-4 API, which provides a unique opportunity to leverage the advanced capabilities of this model for generating high-quality rationales. By using the API, we can pass the logical reasoning tasks derived from our seminal datasets (LogiQA, ReClor, LOGICINFERENCE, and FOLIO) to GPT-4 and collect the model\u2019s responses. The pseudo-code in Appendix C exemplifies the process for GPT-4 rationales collection.\nThe general inference task is converted from LOGICINFERENCE, EntailmentBank, and FOLIO. The three datasets are particularly valuable in this context as they offer data instances accompanied by precise reasoning chains. These reasoning processes, whether derived from rules or written by humans, serve as concrete examples for GPT-4 to learn from. Golden CoT output datasets provide GPT-4 with invaluable references, optimizing generation quality.\nThe machine reading comprehension task is derived from LogiQA and ReClor. These two datasets are not sequence-to-sequence; they do not offer step-by-step reasoning processes. However, GPT-4 scores well on these two datasets (Liu et al., 2023a) without in-context examples, GPT-4\u2019s commendable performance on these datasets assures quality generation."
        },
        {
            "heading": "3.4 Data Statistics and Quality Assessment",
            "text": "We collected 68,983 data instances in total. We illustrate their distribution in Table 1. Table 9 in Appendix D shows the root verb-noun pairs of our instruction set. As can be seen from the figure, the instructions are reasoning-centered. Figure 6 in Appendix D shows the root verb-noun pairs of GPT-4 responses, which cover a wide range of scenarios both in everyday language and in symbolic language.\nTogether, these tasks cover various logical reasoning abilities. We release our LogiCoT instruction tuning dataset at https://huggingface.co/datasets/csitfun/LogiCoT to facilitate future research.\nHuman Evaluation of Dataset Quality We conducted a comprehensive human evaluation of our generated reasoning chains. A random subset of 200 reasoning chains was selected. These chains were evaluated by 3 domain professional annotators using four key metrics: Relevance (Does the chain directly relate to the question? ), Coherence (Is the chain logically consistent? ), Completeness (Does it offer a full explanation for the reasoning?) , and Faithfulness ( Is the reasoning factual and not fabricating details? ). Each reasoning chain was rated on a scale from 1 (poor) to 5 (excellent) for each metric.\nThe reasoning chains achieved an average score of 4.9 for Relevance, 4.7 for Coherence, 4.5 for Completeness, and 4.5 for Faithfulness. The inter-annotator agreement, measured using Cohen\u2019s Kappa, was 0.87, indicating strong agreement among the annotators.\nThe human evaluation underscores the quality and reliability of the reasoning chains generated by our model. While the scores were high for relevance, we acknowledge room for improvement in faithfulness, and future work will aim to refine the generation process to address this."
        },
        {
            "heading": "4 Experiments",
            "text": "We use LogiCoT to fine-tune an open LLM and test the resulting model on logical reasoning and general human-centric benchmarks."
        },
        {
            "heading": "4.1 Models",
            "text": "LLaMA (Touvron et al., 2023) is an open-sourced LLM developed by Meta. We adopt LLaMA-7b (Touvron et al., 2023) as the base model for instruction tuning.\nWe shuffle the collected data to ensure the base model encounters a broad range of data characteristics throughout the instruction tuning phase. This approach bolsters the model\u2019s potential to generalize its learning to a variety of unseen situations. Following (Taori et al., 2023), we adopted their running scripts and set the learning rate to 2e-5, and the batch size to 4. We trained 2 epochs. Our experimental setup incorporated two A100 GPUs, running the instruction tuning process over 2 epochs. We use the Microsoft deepspeed library 1 to accelerate the training process. The training takes 4 days. We release our instruction-tuned LLaMA model as \u201cLLaMA-7b-logicot\u201d.\nTo compare our model performance with other open-sourced instruction-tuned LLMs, we choose two top models from the Open LLM Leaderboard 2, namely LLaMA-30b-supercot and Falcon-40binstruct. The former is a LLaMA-30b model tuned on the SuperCOT dataset, 3 which is also a CoT dataset. The latter is an instruction-tuned Falcon40b model (Almazrouei et al., 2023) tuned on a mix of datasets. 4"
        },
        {
            "heading": "4.2 Benchmarks",
            "text": "Logical reasoning The purpose of this exercise is to directly measure how effectively our model has incorporated the logical reasoning skills learned during the instruction tuning phase. This evaluation helps us understand the strength of our model in the domain it was explicitly tuned for.\n1https://github.com/microsoft/DeepSpeed 2https://huggingface.co/spaces/HuggingFaceH4/\nopen_llm_leaderboard 3https://huggingface.co/datasets/kaiokendev/ SuperCOT-dataset 4https://huggingface.co/tiiuae/ falcon-40b-instruct\nFor the assessment of our model\u2019s logical reasoning capabilities, we select the LogiEval benchmark (Liu et al., 2023a) as our primary testing suite, which is expressly designed with an instructprompting style to rigorously test the logical reasoning abilities of large language models (LLMs). Each dataset instance has integrated instructions which ensures the LLM comprehends the desired format and response requirements. Table 3 illustrates the 8 datasets and their task formats in the LogiEval benchmark.\nThe LogiEval benchmark consists of two types of tasks. Firstly, it encompasses multi-choice reading comprehension tasks that examine the model\u2019s ability to interpret, analyze, and make decisions based on given texts. Secondly, it includes natural language inference tasks, presenting an opportunity for the model to demonstrate its understanding of logic and coherence in language constructs. This combination of tasks within LogiEval provides extensive coverage of logical reasoning tasks, making it an ideal choice for our testing purposes.\nGeneral Tasks We further go beyond the confines of logical reasoning tasks. We probe the model\u2019s capabilities on general human-centric language model benchmarks. This broad-based testing strategy is aimed at gauging the generalizability of our model. It enables us to measure the performance of our model on tasks that span a wider array of human language processing tasks, beyond its specific training focus.\nTo conduct a comprehensive evaluation of our model, we employ the Massive Multitask Language\nUnderstanding (MMLU) benchmark (Hendrycks et al., 2021). This benchmark assesses a large language model\u2019s capabilities across a diverse range of domains, spanning from foundational knowledge sectors such as mathematics and history, to more specialized fields like law and ethics. We remain consistent with the protocols from previous evaluations.\nOur evaluation approach emphasizes exact matching. We instruct LLMs to generate the precise answer in accordance with the given prompt. The first token of the output is deemed as the final answer, which tests the LLM\u2019s proficiency in following instructions. The instruction prompts for the two tasks are detailed in Appendix E."
        },
        {
            "heading": "4.3 Results",
            "text": "The results on the LogiEval benchmark are shown in Table 2. With the varies of different data sizes, we represent the data size of each incorporated dataset in the second row. We report the performance of LLaMA-7b base model as one of the baselines.\nCompared to the LLaMA-7b base model, LLaMA-7b-logicot surpasses the base model on all datasets. Specifically, LLaMA-7b-logicot performs much better on the multi-choice reading comprehension task, except for the AR-LSAT data, where all four models give a relatively low performance. Compared to the two leading open-sourced models, our model gives the best performance except for the AR-LSAT data, where our model yields 16.96%\naccuracy, the second best compared to 17.98% accuracy of the LLaMA-30b-supercot model. Notice that the AR-LSAT dataset is a 5-way classification task, in contrast to our instruction tuning data\u2019s 4-way multi-choice reading comprehension.\nThe results on the MMLU benchmark are shown in Table 4. Overall, the average accuracy of LLaMA-7b-logicot is 43.3%, compared to the 31.8% accuracy of the LLaMA-7b base model. The improvement is salient given that our model has not been specifically tuned on the tested subjects. In detail, our model surpasses LLaMA-7b-base on every subject. The best improvements are seen in business, computer science, economics, etc., with over 10 points accuracy boost."
        },
        {
            "heading": "4.4 Discussion",
            "text": ""
        },
        {
            "heading": "4.4.1 Compare to ChatGPT and GPT-4",
            "text": "As our model is tuned on instruction data distilled from GPT-4, we therefore compare their performance on the LogiEval benchmark with our model. The results are shown in Tabel 5. LLaMA-7blogicot performs on par with ChatGPT on datasets like LogiQA 2.0 and AR-LSAT. Particularly, on ReClor, LogiQA OOD, and TaxiNLI, our model outperforms ChatGPT by a small margin. Given that LLaMA-7b-logicot is a small model with only 7 billion parameters, the results are quite inspiring. However, on the Chinese version of LogiQA, and on ConTRoL, the performance of our model lags ChatGPT by 20 points. This performance discrepancy suggests that our model may exhibit weaknesses in handling Chinese corpus and passagelevel texts.\nGPT-4 outperforms LLaMA-7b-logicot on each dataset, giving the best performance on the LogiEval benchmark except for the ConTRoL dataset, where the accuracy is lower than ChatGPT by 2 points. The results show that there is still a gap between our model and the best proprietary model."
        },
        {
            "heading": "4.4.2 Ablation Study",
            "text": "To better understand the specific contributions of various reasoning types employed in our instruction-tuning process, we conduct an ablation study. This involves evaluating model performance by selectively removing one reasoning type at a time and observing the resultant change in the model\u2019s ability to handle logical reasoning tasks.\nThe ablation study uses the instruction-tuned LLaMA-7b model. For each reasoning type, we\ntrain the model without that specific type while retaining the others. This is performed iteratively for all reasoning types.\nWe report the overall average score of LogiEval and MMLU, respectively, to provide a comprehensive understanding of the respective impacts of these reasoning types. The results are shown in Table 6. We elaborate on the results of ablating \u201cmulti-choice\u201d in Table 10 of Appendix F to give audiences a clear view.\nLanguage to Logic: Excluding this type led to a decline in performance on the LogiEval dataset. This underscores its significance in mapping linguistic constructs to logical expressions, which is foundational for reasoning tasks.\nOne-step Inference: The drop in accuracy suggests that even simple inferences play a vital role, especially in tasks where direct conclusions are drawn from given premises.\nInference Chain: The model\u2019s performance drop on both datasets highlights the importance of chained logical deductions. Tasks that require multi-step reasoning particularly benefit from this type.\nMulti-choice: Removing multi-choice reasoning impacted performance on MMLU more than on LogiEval, emphasizing its role in tasks where choosing among alternatives based on logical grounds is essential.\nThis ablation study reaffirms the unique contributions of each reasoning type to the model\u2019s performance. While all reasoning types contribute to enhancing the model\u2019s logical understanding, their impacts vary based on the nature of tasks and the datasets used. Future research can delve deeper into optimizing instruction-tuning processes based on specific reasoning type requirements of datasets."
        },
        {
            "heading": "4.4.3 Case Study",
            "text": "To further understand the reasoning ability of LLaMA-7b-logicot, we provide case study examples in this section. Rather than generating a single output token, we ask the model to generate both the answer and rationales by prompting it with \"Answer and reason: \".\nFigure 7 in Appendix G shows an example of LLaMA-7b-logicot solving the MRC task correctly. In this case, our model is asked to find the option that strengthens the given argument. The key idea in this example is that to save costs, the company needs to abandon a rule that leads to higher costs. Our model successfully identifies option A as proof of the costly rule. It also generates a coherent and convincing explanation.\nFigure 8 in Appendix G shows an example of LLaMA-7b-logicot solving the MRC task incorrectly. The example is a difficult analytical question involving complex logical relations and numerical calculations. The reasoning steps generated by our model are sensible in (1), (2), and (3). However, it came up with new constraints, as in (4), (5), (6), and (11). Moreover, the model does not examine all the options to decide the correct answer. Our model made a series of mistakes in this example which led to the wrong conclusion."
        },
        {
            "heading": "5 Conclusion",
            "text": "We collected LogiCoT, a set of CoT instructiontuning data using GPT-4 through the lens of logical reasoning tasks. With 70K training instances, LogiCoT offers diverse challenges in logical reasoning. Using LogiCoT, we instruction-tuned an open-sourced LLaMA-7b model and the experiments on our model demonstrate competitive logical reasoning and general inference abilities. To our knowledge, we are the first to construct an instruction-tuning dataset with rich and diverse logical reasoning steps, showing its potential to enhance a generative LLM. Future work includes the integration of our dataset to enhance dialogueoriented LLMs such as Alpaca."
        },
        {
            "heading": "Acknowledgement",
            "text": "We thank all the anonymous reviewers for their constructive suggestions. This publication has emanated from research conducted with the financial support of the Pioneer and \u201cLeading Goose\u201d R&D Program of Zhejiang under Grant Number 2022SDXHDX0003. Zhiyang Teng is partially supported by CAAI-Huawei MindSpore Open Fund (CAAIXSJLJJ-2021-046A). Yue Zhang and Zhiyang Teng are the corresponding authors."
        },
        {
            "heading": "A Datasets Examples",
            "text": "We illustrate data examples mentioned in Section 3 here."
        },
        {
            "heading": "B The Instruction Types and Examples for the MRC Task",
            "text": "The instructions below further cultivate the model\u2019s critical thinking and argument analysis skills.\nIdentify the Necessary Claim: This instruction tasks the model to pinpoint the claim that must be true or is required for an argument to work. It is essentially training the model to identify essential assumptions or premises in an argument, thus honing its ability to understand argument structures.\nStrengthen an Argument: Under this instruction, the model must identify information that would strengthen an argument. It requires the\nmodel to not just understand the argument, but also anticipate what additional information could make the argument more convincing. This helps the model to improve its capability to enhance log-\nical arguments. Weaken an Argument: This type is the opposite of Strengthen an Argument mentioned above. Here, the model is tasked with identifying information that would weaken an argument. This helps the model develop a nuanced understanding of argument structures and cultivate the ability to critique and dismantle arguments.\nResolve a Situation: This instruction requires the model to identify information that would explain or resolve a situation. This is about identifying missing information or finding potential solutions to a problem, further expanding the model\u2019s problem-solving capabilities.\nIdentify a Flaw in Arguments Reasoning: In this type, the model must identify a flaw in an argument\u2019s reasoning. This instruction cultivates the model\u2019s critical thinking skills, as it needs to scrutinize the argument and pinpoint any logical fallacies or inconsistencies.\nBy incorporating these instruction types, the data generation scheme is broadened to more complex logical reasoning tasks, particularly in the realm of argumentation and critical thinking, thereby enhancing the language model\u2019s ability to engage with more sophisticated and nuanced logical reasoning tasks.\nC Pseudo-Code for GPT-4 Generation\n1: prompt_with_cot{ \u2018instruction\u2019: instruction, \u2018input\u2019: input, \u2018output\u2019: output, } 2: prompt_without_cot{\n\u2018instruction\u2019: instruction, \u2018input\u2019: input, }\n3: output = openai.ChatCompletion.create( model = \"gpt-4\", messages = [{\u2018role\u2019: \u2018system\u2019, \u2018content\u2019:\ninstruction}, {\u2018role\u2019: \u2018user\u2019, \u2018content\u2019: input}, {\u2018role\u2019: \u2018assistant\u2019, \u2018content\u2019: cot}]\n)"
        },
        {
            "heading": "D Data Statistics",
            "text": ""
        },
        {
            "heading": "E Prompting Instruction-Tuned Models",
            "text": "LogiEval is an instruction-prompting style benchmark where each dataset instance has integrated instructions. This ensures the LLM comprehends the desired format and response requirements.\nBelow is the LogiEval instruction for the reading comprehension task:\nInstructions: You will be presented with a passage and a question about that passage. There are four options to be chosen from, you need to choose the only correct option to answer that question. If the first option is right, you generate the answer \u2019A\u2019, if the second option is right, you generate the answer \u2019B\u2019, if the third option is right, you generate the answer \u2019C\u2019, if the fourth option is right, you generate the answer \u2019D\u2019, if the fifth option is right, you generate the answer \u2019E\u2019. Read the question and options thoroughly and select the correct answer from the four answer labels. Read the passage thoroughly to ensure you know what the passage entails.\nAnd the instructions for the NLI task: Instructions: You will be presented with a set of facts and rules as premises, and a hypothesis about it. You need to decide whether the hypothesis is entailed by the premise by choosing one of the following answers: \u2019Yes\u2019: The hypothesis follows logically from the information contained in the premise. \u2019No\u2019: The hypothesis is logically false from the information contained in the premise. \u2019Neutral\u2019: It is not possible to determine whether the hypothesis is true or false without further information. Read the passage of information thoroughly and select the correct answer from the three answer labels. Read the premise thoroughly to ensure you know what the premise entails.\nIn the case of the MMLU dataset, we remain consistent with the protocols from previous evaluations. The prompt, \"The following are multiple choice questions (with answers) about subject. question Answer:\", provides both context and a clear response expectation."
        },
        {
            "heading": "F Ablation",
            "text": "G Case Study Example"
        }
    ],
    "title": "LogiCoT: Logical Chain-of-Thought Instruction Tuning",
    "year": 2023
}