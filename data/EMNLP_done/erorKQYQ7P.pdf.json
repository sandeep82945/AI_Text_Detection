{
    "abstractText": "Pre-trained Language Models are widely used in many important real-world applications. However, recent studies show that these models can encode social biases from large pre-training corpora and even amplify biases in downstream applications. To address this challenge, we propose CoPT, an efficient and effective debiaswhile-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks. Our experiments conducted on three extrinsic bias benchmarks demonstrate the effectiveness of CoPT on bias mitigation during the prompt tuning process and its adaptability to existing upstream debiased language models. These findings indicate the strength of CoPT and provide promising avenues for further enhancement in bias mitigation on downstream tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiangjue Dong"
        },
        {
            "affiliations": [],
            "name": "Ziwei Zhu"
        },
        {
            "affiliations": [],
            "name": "Zhuoer Wang"
        },
        {
            "affiliations": [],
            "name": "Maria Teleki"
        },
        {
            "affiliations": [],
            "name": "James Caverlee"
        },
        {
            "affiliations": [],
            "name": "George Mason"
        }
    ],
    "id": "SP:94028e838073ea650ecc337ce6bf380cb6bda287",
    "references": [
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing",
            "year": 2020
        },
        {
            "authors": [
                "Yang Trista Cao",
                "Yada Pruksachatkun",
                "Kai-Wei Chang",
                "Rahul Gupta",
                "Varun Kumar",
                "Jwala Dhamala",
                "Aram Galstyan"
            ],
            "title": "On the intrinsic and extrinsic",
            "year": 2022
        },
        {
            "authors": [
                "Pengyu Cheng",
                "Weituo Hao",
                "Siyang Yuan",
                "Shijing Si",
                "Lawrence Carin."
            ],
            "title": "FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Maria De-Arteaga",
                "Alexey Romanov",
                "Hanna Wallach",
                "Jennifer Chayes",
                "Christian Borgs",
                "Alexandra Chouldechova",
                "Sahin Geyik",
                "Krishnaram Kenthapadi",
                "Adam Tauman Kalai"
            ],
            "title": "Bias in bios: A case study of semantic representation bias in a high",
            "year": 2019
        },
        {
            "authors": [
                "Sunipa Dev",
                "Tao Li",
                "Jeff M Phillips",
                "Vivek Srikumar."
            ],
            "title": "On measuring and mitigating biased inferences of word embeddings",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7659\u20137666.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Xiangjue Dong",
                "Yun He",
                "Ziwei Zhu",
                "James Caverlee."
            ],
            "title": "PromptAttack: Probing dialogue state trackers with adversarial prompts",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 10651\u201310666, Toronto, Canada. Associ-",
            "year": 2023
        },
        {
            "authors": [
                "Xiangjue Dong",
                "Jiaying Lu",
                "Jianling Wang",
                "James Caverlee."
            ],
            "title": "Closed-book question generation via contrastive learning",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 3150\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Seraphina Goldfarb-Tarrant",
                "Rebecca Marchant",
                "Ricardo Mu\u00f1oz S\u00e1nchez",
                "Mugdha Pandya",
                "Adam Lopez."
            ],
            "title": "Intrinsic bias metrics do not correlate with application bias",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Yue Guo",
                "Yi Yang",
                "Ahmed Abbasi."
            ],
            "title": "Autodebias: Debiasing masked language models with automated biased prompts",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Xudong Han",
                "Timothy Baldwin",
                "Trevor Cohn."
            ],
            "title": "Diverse adversaries for mitigating bias in training",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2760\u20132765, Online.",
            "year": 2021
        },
        {
            "authors": [
                "Jacqueline He",
                "Mengzhou Xia",
                "Christiane Fellbaum",
                "Danqi Chen."
            ],
            "title": "MABEL: Attenuating gender bias using textual entailment data",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9681\u20139702, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Masahiro Kaneko",
                "Danushka Bollegala."
            ],
            "title": "Debiasing pre-trained contextualised embeddings",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1256\u20131266, Online.",
            "year": 2021
        },
        {
            "authors": [
                "Masahiro Kaneko",
                "Danushka Bollegala",
                "Naoaki Okazaki."
            ],
            "title": "Debiasing isn\u2019t enough! \u2013 on the effectiveness of debiasing MLMs and their social biases in downstream tasks",
            "venue": "Proceedings of the 29th International Conference on Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Anne Lauscher",
                "Tobias Lueken",
                "Goran Glava\u0161."
            ],
            "title": "Sustainable modular debiasing of language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4782\u20134797, Punta Cana, Dominican Republic. Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Klas Leino",
                "Matt Fredrikson",
                "Emily Black",
                "Shayak Sen",
                "Anupam Datta."
            ],
            "title": "Feature-Wise Bias Amplification",
            "venue": "International Conference on Learning Representations (ICLR).",
            "year": 2019
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Domini-",
            "year": 2021
        },
        {
            "authors": [
                "Lagunas",
                "Alexander Rush",
                "Thomas Wolf."
            ],
            "title": "Datasets: A community library for natural language processing",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 175\u2013184, Online",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Yingji Li",
                "Mengnan Du",
                "Xin Wang",
                "Ying Wang."
            ],
            "title": "Prompt tuning pushes farther, contrastive learning pulls closer: A two-stage approach to mitigate social biases",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguis-",
            "year": 2023
        },
        {
            "authors": [
                "Paul Pu Liang",
                "Irene Mengze Li",
                "Emily Zheng",
                "Yao Chong Lim",
                "Ruslan Salakhutdinov",
                "LouisPhilippe Morency."
            ],
            "title": "Towards debiasing sentence representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Weng Tam",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Xiao Liu",
                "Yanan Zheng",
                "Zhengxiao Du",
                "Ming Ding",
                "Yujie Qian",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "GPT understands, too",
            "venue": "arXiv preprint arXiv:2103.10385.",
            "year": 2021
        },
        {
            "authors": [
                "Kaiji Lu",
                "Piotr Mardziel",
                "Fangjing Wu",
                "Preetam Amancharla",
                "Anupam Datta."
            ],
            "title": "Gender bias in neural natural language processing",
            "venue": "Logic, Language, and Security: Essays Dedicated to Andre Scedrov on the Occasion of His 65th Birthday, pages 189\u2013202.",
            "year": 2020
        },
        {
            "authors": [
                "Nicholas Meade",
                "Elinor Poole-Dayan",
                "Siva Reddy."
            ],
            "title": "An empirical survey of the effectiveness of debiasing techniques for pre-trained language models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Moin Nadeem",
                "Anna Bethke",
                "Siva Reddy."
            ],
            "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Nikita Nangia",
                "Clara Vania",
                "Rasika Bhalerao",
                "Samuel R. Bowman."
            ],
            "title": "CrowS-pairs: A challenge dataset for measuring social biases in masked language models",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Shauli Ravfogel",
                "Yanai Elazar",
                "Hila Gonen",
                "Michael Twiton",
                "Yoav Goldberg."
            ],
            "title": "Null it out: Guarding protected attributes by iterative nullspace projection",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Alexey Romanov",
                "Maria De-Arteaga",
                "Hanna Wallach",
                "Jennifer Chayes",
                "Christian Borgs",
                "Alexandra Chouldechova",
                "Sahin Geyik",
                "Krishnaram Kenthapadi",
                "Anna Rumshisky",
                "Adam Kalai"
            ],
            "title": "What\u2019s in a name? Reducing bias in bios without access",
            "year": 2019
        },
        {
            "authors": [
                "Rachel Rudinger",
                "Jason Naradowsky",
                "Brian Leonard",
                "Benjamin Van Durme."
            ],
            "title": "Gender bias in coreference resolution",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2018
        },
        {
            "authors": [
                "Timo Schick",
                "Sahana Udupa",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in NLP",
            "venue": "Transactions of the Association for Computational Linguistics, 9:1408\u2013 1424.",
            "year": 2021
        },
        {
            "authors": [
                "Rachael Tatman."
            ],
            "title": "Gender and dialect bias in YouTube\u2019s automatic captions",
            "venue": "Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, pages 53\u201359, Valencia, Spain. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Yibo Wang",
                "Congying Xia",
                "Guan Wang",
                "Philip S. Yu."
            ],
            "title": "Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing",
            "venue": "2022 IEEE International Conference on Big Data (Big Data), pages 1383\u20131388.",
            "year": 2022
        },
        {
            "authors": [
                "Kellie Webster",
                "Xuezhi Wang",
                "Ian Tenney",
                "Alex Beutel",
                "Emily Pitler",
                "Ellie Pavlick",
                "Jilin Chen",
                "Ed Chi",
                "Slav Petrov."
            ],
            "title": "Measuring and reducing gendered correlations in pre-trained models",
            "venue": "arXiv preprint arXiv:2010.06032.",
            "year": 2020
        },
        {
            "authors": [
                "Zhongbin Xie",
                "Thomas Lukasiewicz."
            ],
            "title": "An empirical analysis of parameter-efficient methods for debiasing pre-trained language models",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Ke Yang",
                "Charles Yu",
                "Yi Fung",
                "Manling Li",
                "Heng Ji."
            ],
            "title": "ADEPT: A DEbiasing PrompT Framework",
            "venue": "arXiv preprint arXiv:2211.05414.",
            "year": 2022
        },
        {
            "authors": [
                "Jieyu Zhao",
                "Tianlu Wang",
                "Mark Yatskar",
                "Vicente Ordonez",
                "Kai-Wei Chang."
            ],
            "title": "Men also like shopping: Reducing gender bias amplification using corpus-level constraints",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Lan-",
            "year": 2017
        },
        {
            "authors": [
                "Jieyu Zhao",
                "Tianlu Wang",
                "Mark Yatskar",
                "Vicente Ordonez",
                "Kai-Wei Chang."
            ],
            "title": "Gender bias in coreference resolution: Evaluation and debiasing methods",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Ran Zmigrod",
                "Sabrina J. Mielke",
                "Hanna Wallach",
                "Ryan Cotterell."
            ],
            "title": "Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Pre-trained language models (PLMs) are widely used in many real-world applications, demonstrating remarkable performance (Devlin et al., 2019; Brown et al., 2020). However, it has been demonstrated that PLMs encode unfair social biases in their parameters based on their pre-training step over large-scale text corpora (May et al., 2019). Furthermore, these biases \u2013 for example, based on gender, race, or religion \u2013 can easily propagate to the downstream tasks that use these PLMs (Kaneko and Bollegala, 2021). For example, \u201cShe is a nurse\u201d can have a higher conditional likelihood than \u201cHe is a nurse\u201d in the language modeling task, and \u201cnurse\u201d can have higher coreference scores to \u201cshe\u201d than \u201che\u201d in the coreference resolution task (Lu et al., 2020). Considering that NLP applications like machine translation systems, resume filtering systems, dialogue systems, and speech recognition (Tatman, 2017) are widely used by millions of users globally, it is crucial to mitigate the social biases present in PLMs and strive for models that\nwill not propagate discriminatory predictions or offensive outputs towards specific groups before being deployed.\nMuch prior effort has focused primarily on debiasing the representations learned during the pretraining process, e.g., through projection (Dev et al., 2020; Liang et al., 2020; Ravfogel et al., 2020; Kaneko and Bollegala, 2021), further pre-training on unbiased external corpora (Webster et al., 2020; Lauscher et al., 2021; He et al., 2022), or finetuning to debias (Cheng et al., 2021; Guo et al., 2022). The effectiveness of such debiasing efforts is typically measured on intrinsic benchmarks like SEAT (Sentence Encoding Association Test) which computes the association between demographic terms (e.g., woman, man) and stereotype terms (e.g., science, art). An unbiased model should display no difference in the similarity between the representations of these terms (May et al., 2019).\nWhile these existing approaches help reduce social biases under intrinsic measures, these debiasthen-finetune methods are based on the hypothesis that if an upstream model is unbiased, it will also preserve its fairness effects on downstream tasks during the fine-tuning process. However, recent research investigating the relationship between intrinsic and extrinsic benchmarks (which evaluate fairness in downstream applications) finds these two benchmarks correlate weakly (Kaneko et al., 2022). Furthermore, they observe that models, even after being debiased, tend to re-acquire or even amplify biases (e.g., instance-related biases and labelrelated biases) during the fine-tuning process on downstream tasks (Zhao et al., 2017; Leino et al., 2019). Thus, this mismatch leads to our motivating research question \u2013 How can we develop an efficient and effective method to mitigate bias on downstream tasks?\nTo answer the aforementioned question, we propose Co2PT, a debias-while-prompt tuning approach through Counterfactual Contrastive Prompt\nTuning. In this method, we first freeze all parameters of the PLM and add tunable continuous prompts for every layer. Unlike the previous debias-then-finetune methods that require expensive re-training of the original PLM and risk knowledge forgetting, this deep prompt tuning framework saves computational and memory resources while preserving the original pre-trained knowledge and language modeling ability (Li and Liang, 2021; Liu et al., 2022). To ensure that a fair system generates unbiased results regardless of the demographic terms used, we construct counterfactual pairs directly from the training data, eliminating the need for external corpora that heavily depend on their quality for debiasing. Specifically, we replace demographic terms associated with either the dominant or minoritized group in the training data with terms representing the opposite group. Then, we integrate the ability to mitigate bias into the prompt parameters through a contrastive objective between counterfactual pairs while maintaining the parameters of PLMs frozen. Co2PT can be integrated into existing debiased models to help them mitigate biases on downstream tasks and offer flexibility in addressing different kinds of bias. These advantages establish Co2PT as an efficient and effective method for mitigating bias in downstream tasks.\nIn conclusion, the proposed Co2PT mitigates bias on downstream tasks through prompt tuning, making the following contributions:\n\u2022 Co2PT achieves time and memory efficiency without requiring access to an external corpus or retraining the entire model.\n\u2022 Over three extrinsic bias benchmarks, we show that Co2PT effectively mitigates bias amplified during the prompt tuning process on downstream tasks.\n\u2022 Furthermore, Co2PT can be extended to existing debiased language models, effectively bridging the gap between debiased upstream models and downstream tasks."
        },
        {
            "heading": "2 Related Work",
            "text": "Several approaches have been proposed for debiasing pre-trained language models such as projectionbased methods (Dev et al., 2020; Liang et al., 2020; Ravfogel et al., 2020; Kaneko and Bollegala, 2021), post-hoc text generation techniques (Schick et al., 2021), adversarial methods (Han et al., 2021), finetuning on biased prompts (Guo et al., 2022), with contrastive objective (Cheng et al., 2021) or with augmented data (Zhao et al., 2018), additional pretraining methods on re-balanced corpus through counterfactual data augmentation (Webster et al., 2020; Lauscher et al., 2021; Meade et al., 2022) or with a contrastive objective on gender-balanced entailment pairs (He et al., 2022), using dropout regularization (Webster et al., 2020), through parameterefficient methods (Lauscher et al., 2021; Yang et al., 2022; Xie and Lukasiewicz, 2023) or with a contrastive objective (Li et al., 2023). While some works do not require access to an external corpus or do not require retraining the entire model, most prior methods primarily focus on mitigating bias within the model\u2019s intrinsic characteristics and evaluate the effectiveness of bias mitigation through intrinsic bias benchmarks, e.g., SEAT (May et al., 2019), StereoSet (Nadeem et al., 2021), and CrowSPairs (Nangia et al., 2020). Subsequently, they fine-tune the debiased models on downstream tasks and demonstrate that their debiased models retain the language modeling ability and the performance on downstream tasks or extrinsic bias benchmarks, which evaluate fairness in downstream tasks by\ntesting whether the models exhibit different performances among different populations.\nNevertheless, recent research shows that these debias-then-finetune methods will re-acquire or even amplify biases during the fine-tuning process on downstream tasks and that intrinsic and extrinsic evaluation bias benchmarks correlate poorly (Goldfarb-Tarrant et al., 2021; Cao et al., 2022; Kaneko et al., 2022). They encourage researchers to focus directly on extrinsic measures of bias of specific applications when addressing bias mitigation (Goldfarb-Tarrant et al., 2021).\nThus, we focus in this paper on mitigating bias on downstream tasks and evaluate using extrinsic evaluation benchmarks directly. In addition, different from the previous methods requiring further pre-training on the counterfactually augmented sentences from an external corpus, e.g., English Wikipedia (Zmigrod et al., 2019; Webster et al., 2020; Meade et al., 2022), BookCorpus (Lauscher et al., 2021), News-Commentary v15 (Yang et al., 2022) or NLI (He et al., 2022), our methods achieve time and memory efficiency by eliminating the need for external corpus access or model retraining.\n3 Co2PT: Debiasing via Counterfactual Contrastive Prompt Tuning\nWe propose Co2PT, a debias-while-prompt tuning parameter-efficient method for mitigating biases on downstream tasks via counterfactual contrastive prompt tuning, presented in Figure 1. Concretely, Co2PT mitigates bias in PLMs by leveraging counterfactual pairs from training data to produce debiased representations during prompt tuning.\nDeep Prompt Tuning. First, we introduce the backbone framework of Co2PT \u2013 deep prompt tuning. We incorporate continuous prompts as prefix tokens in every layer of the PLM. By doing this, we have more tunable task-specific parameters to enhance per-task capacity while maintaining parameter efficiency (Li and Liang, 2021; Liu et al., 2022; Wang et al., 2022; Dong et al., 2023a). Besides, it can achieve comparable performance to fine-tuning, outperforming methods that only add trainable continuous prompts into the input embedding layer (Lester et al., 2021; Liu et al., 2021), which underperform the fine-tuning methods, especially when the model size is not large (Liu et al., 2022). The prompt tuning loss of proposed Co2PT on the downstream task is represented as Lpt, e.g., cross-entropy loss for a classification task.\nCounterfactual Pairs Construction. Then, the first key question is: how to interject the debiasing capability into the continuous prompts? An unbiased model should make the same predictions independent of the bias-attribute term, thus we apply counterfactual data augmentation to generate counterparts of training examples from the training data during prompt tuning. Concretely, let S represent the training corpus and let W = {(w1, w2, . . . , wm)i}Ni=1 be a set of N biasattribute term pairs. For each sentence si in S and each pair (w1, w2, . . . , wm) in W , for any wi in s, we replace it with the term along the opposite bias direction. Take the binary-gender debiasing task shown in Figure 1 for example, the bias-attribute terms are {(man, woman), (he, she), . . . }. The \u201cman\u201d is in the input sentence \u201cThe man is playing the piano\u201d. We replace it with \u201cwoman\u201d while leaving non-attribute words unchanged. Then the counterfactually augmented sentence is \u201cThe woman is playing the piano\u201d, and vice versa. The obtained counterfactual sentence of the original sentence si is denoted as s\u2032i. Counterfactual Contrastive Learning. The counterfactual pairs construction allows us to achieve a balance in inputs containing bias-attribute terms. However, how can we ensure that our model generates consistent predictions for both si and s\u2032i, which possess similar semantic meaning but differ in bias direction? To make the model generate predictions independent of biased attributes, it is important for sentences with similar semantics but along different bias directions to be closer (Cheng et al., 2021; He et al., 2022). We apply contrastive learning, of which the objective is to obtain meaningful representations by bringing semantically similar neighbors closer and pushing apart the dissimilar neighbors (Gao et al., 2021; Dong et al., 2023b; Li et al., 2023). In this work, input sentence si and its counterpart s\u2032i are semantically related but in opposite bias directions. We let hi and h\u2032i denote the representations of si and s\u2032i and then concatenate with the continuous prompt representation p as positive pairs. Then we take the cross-entropy objective with in-batch negatives (Gao et al., 2021). The training objective for (hi,h\u2032i) with a mini-batch of N pairs is:\nLcl = \u2212 log esim(p\u2295hi,p\u2295h \u2032 i)/\u03c4\u2211N\nj=1 e sim(p\u2295hi,p\u2295h\u2032j)/\u03c4\n, (1)\nwhere sim(xi, yi) is the cosine similarity of xi and\nyi: sim(xi, yi) = x\u22a4i yi/ \u2225xi\u2225 \u2225yi\u2225, \u2295 is the concatenation of two representations, and \u03c4 is a temperature hyperparameter.\nFor counterfactual pairs (si, s\u2032i) in the singlesentence classification task, si is the original sentence from the training data and s\u2032i is the augmented sentence that has the same semantic meaning as si but in a different bias direction. For sentence-pair classification, like in the SNLI task, with xi as the premise and yi as the hypothesis, si is the original premise-hypothesis pair (xi, yi) while s\u2032i is the counterfactual augmented premise-hypothesis pair (x\u2032i, y \u2032 i). Similarly, the sentence representations are concatenated with continuous prompts to calculate the contrastive loss through Equation 1.\nLearning Objectives. Finally, the continuous prompts learn-to-debias by simultaneously optimizing the prompt tuning loss Lpt on downstream tasks and contrastive loss Lcl between the counterfactual pairs:\nL = Lpt + \u03b1Lcl, (2)\nwhere \u03b1 is a tunable coefficient hyperparameter. As stated before, we only tune the parameters of the debiasing continuous prompts while maintaining the parameters of PLMs frozen throughout the training. After the counterfactual contrastive prompt tuning, the debiasing knowledge is stored in the prompt parameters. This approach not only retains the knowledge within the original parameters of PLMs but is also flexible and adaptable to different downstream tasks. For example, we can train different prompts for different bias dimensions such as gender, race, and religion. These prompts can then be combined and applied to downstream tasks. Moreover, considering that prior research primarily concentrates on binary gender, it is more efficient to extend its application to non-binary gender without requiring re-training new debiased models."
        },
        {
            "heading": "4 Experimental Setup",
            "text": "We design experiments to test the effectiveness of our proposed Co2PT approach toward answering four questions: RQ1: Will Co2PT mitigate bias on downstream tasks effectively? RQ2: How will the existing intrinsic debiased methods perform on the downstream tasks when they are combined with Co2PT? RQ3: What impact do different modules have on the design of Co2PT? RQ4: How do hyperparameters affect Co2PT?"
        },
        {
            "heading": "4.1 Bias Evaluation",
            "text": "Extrinsic bias benchmarks assess bias via performance gap between different groups in downstream tasks. In this work, we evaluate Co2PT on three widely used extrinsic bias benchmarks: Bias-STSB, Bias-NLI, and Bias-in-Bios. Bias-STS-B (Webster et al., 2020) is adapted from the STS-B task to evaluate gendered correlations, which requires models to predict the semantic similarity between pairs of sentences. Specifically, 276 sentences are collected from the test set as templates and then gendered terms (man, woman) and professional terms from Rudinger et al. (2018) are inserted into each template, forming 16,980 sentence pairs. For instance, if the template is \u201cA man is walking\u201d, then the sentence pairs are (\u201cA man is walking\u201d, \u201cA nurse is walking\u201d) and (\u201cA woman is walking\u201d, \u201cA nurse is walking\u201d). If a model is unbiased towards gender terms, it should assign equal similarity scores to both pairs. We calculate the average absolute difference between the similarity scores of sentence pairs containing male and female terms, and how often the difference between \u201cmale\u201d and \u201cfemale\u201d sentence pairs > \u03c4 , where we report the results for \u03c4 = 0.1 and \u03c4 = 0.3 (Webster et al., 2020). A lower value indicates less bias. Bias-NLI (Dev et al., 2020) is a natural language inference dataset consisting of neutral sentence pairs to evaluate the gender-occupation bias. It is constructed by populating the template: \u201cThe subject verb a/an object\u201d, leading to 1,936,512 instances. Concretely, the verb and object slots are filled with activities, e.g., ate a bagel. Then they create neutral entailment pairs by filling the subject slot with an occupation term with a strong gender correlation, e.g., \u201cnurse\u201d, for the hypothesis, and \u201cwoman\u201d, for the premise, resulting in the instance: The woman ate a bagel; The nurse ate a bagel. neutral. Bias is defined as deviation from neutrality and measured by three metrics: (1) Net Neutral (NN): the average probability that the model assigns a neutral label for all instances, (2) Fraction Neutral (FN): the percentage of instances that the model predicts the neutral label and (3) Threshold: \u03c4 (T: \u03c4 ): the fraction of examples whose probability of neutral are above \u03c4 . We report the results for \u03c4 = 0.5 and \u03c4 = 0.7 following (Lauscher et al., 2021; He et al., 2022). All three metrics will attain 1 for a bias-free model. Bias-in-Bios (De-Arteaga et al., 2019) is a largescale English dataset studying gender bias in oc-\ncupation classification from the Common Crawl corpus. We report the overall accuracy of the task as well as the accuracy breakdown based on gender. To quantify gender bias, we compute the difference in true positive rates (TPR) between genders across various occupations, denoted as GAPTPRg and defined as follows:\nGAPTPRg = |TPRg \u2212 TPR\u223cg| , (3)\nwhere TPRg represents the proportion of individuals correctly predicted given their gender g, and g and \u223cg are binary genders. Following (Romanov et al., 2019; Ravfogel et al., 2020; He et al., 2022), we also calculate the root mean square of the peroccupation TPR gender gap GAPTPRg,o over all occupations o:\nGAPRMSg =\n\u221a 1\n|O| \u2211 o\u2208O ( GAPTPRg,o )2 . (4)\nA value closer to 0 indicates a lower degree of bias."
        },
        {
            "heading": "4.2 Datasets and Setup",
            "text": "STS-B and SNLI. We fine-tune the models on SNLI and STS-B training sets and pick the one that performs best on the validation set and then evaluate bias using Bias-STS-B and Bias-NLI, respectively. Bias-in-Bios. We use the same data as Ravfogel et al. (2020) which contains 393,423 biographies and 28 profession classes. We split train/validation/test by 65/25/10 following (DeArteaga et al., 2019; Ravfogel et al., 2020). The dataset statistics are shown in Table 1. For SNLI and Bias-in-Bios, we report accuracy over classification while we report the Pearson and Spearman correlations for STS-B."
        },
        {
            "heading": "4.3 Baseline Models",
            "text": "We compare Co2PT with six upstream debiased models fine-tuned on the downstream tasks, and three baselines fine-tune or prompt-tune BERT models on the downstream tasks: ZariCDA (Webster et al., 2020) is pre-trained from scratch over counterfactual data augmented from English Wikipedia and ZariDO (Webster et al., 2020) is additionally pre-trained with increased dropout rate.\nADELE and its variant ADELE-TA (Lauscher et al., 2021) inject and train debiased adapters via masked language modeling on the counterfactually augmented corpus. Context-Debias (Kaneko and Bollegala, 2021) is fine-tuned via an inner-product loss and squared distance loss. Auto-Debias (Guo et al., 2022) uses a beam search to search for biased prompts and then uses these biased prompts to finetune PLMs by minimizing the disagreement between predicted [MASK] token distributions. MABEL (He et al., 2022) is additionally pre-trained on all entailment pairs that contain gendered terms from SNLI and MNLI data with a contrastive loss, an alignment loss, and an optional masked language modeling loss. BERT (Devlin et al., 2019) is a fine-tuned BERT model on the downstream tasks while BERT+CDA is a fine-tuned BERT model on the counterfactually augmented data from the training sets. PT (Liu et al., 2022) adds continuous prompts to each layer of the models and then tunes the prompts on downstream tasks. The backbone models for ZariCDA and ZariDO are BERT-large-uncased whereas other baselines are BERT-base-uncased (Devlin et al., 2019).1"
        },
        {
            "heading": "4.4 Implementation Details",
            "text": "For fine-tuning debiased baselines and vanilla BERT, we use the models released by the authors. We set the max length of the sentence to be 128, the learning rate to be 2e \u2212 5, the batch size to be 64, and train for 10 epochs. For AT+CDA, learning rate is 2e-5 and batch size is 128. For PT and Co2PT, the backbone models are bert-base-uncased with the learning rate set to 1e \u2212 2 and the prompt length set to 20 and are trained for 30 epochs. The batch size is set to 32. For hyperparameters \u03c4 and \u03b1 in Equation 1 and 2, we set \u03c4 = 0.05 and \u03b1 = 1.0. All experiments are run on a single NVIDIA RTX A5000 24GB GPU. For each run, we save the model that performs the best on the development set and evaluate it on extrinsic benchmarks. We report the average results across three runs. All code and data are available at https://github.com/ dongxiangjue/Co2PT; additional experimental details and standard deviations are in Appendix A and Appendix C, respectively.\n1We keep the results of these two BERT-large-uncased checkpoints since Lauscher et al. (2021) also compares these methods with their BERT-base-uncased method."
        },
        {
            "heading": "5 Debiasing Effectiveness (RQ1)",
            "text": "We now investigate the effectiveness of Co2PT in mitigating bias on three extrinsic bias benchmarks.\nBias-STS-B. First, we focus on the Bias-STS-B benchmark. As Table 2 indicates, Co2PT shows the lowest bias scores across all metrics and achieves similar model performance on downstream tasks as the other debiased baselines. We observe that some debiased models exhibit higher bias scores than the original BERT, indicating that debiased language models can relearn biases during finetuning on downstream tasks. For example, AutoDebias, one of the state-of-the-art debiased models, demonstrates strong fairness on intrinsic benchmarks, such as SEAT, scores 0.312 in average absolute difference, showing a higher bias level than the original BERT and most of the other baselines. On the other hand, MABEL, which shows strong performance in debiasing downstream tasks, achieves a competitive score of 0.081. Furthermore, compared to fine-tuning the original BERT model on the STS-B training set, PT results in a higher bias score of 0.321 compared to 0.282. This suggests that while fine-tuning only the number of prompt tokens may be parameter efficient, it can result in increased bias due to the presence of an unbalanced dataset. For Co2PT, we observe a significant reduction in the bias score with the average absolute difference decreasing from 0.321 to 0.058, from 0.749 to 0.167 when the difference exceeds 0.1, and from 0.369 to 0.005 when the difference exceeds 0.3. These findings indicate a substantial improvement in the ability to mitigate bias.\nBias-NLI. Next, we focus on the Bias-NLI extrinsic benchmark shown in Table 3. BERT+CDA, Context-Debias, and MABEL achieve lower bias\nscores than the original BERT across all metrics while the other baseline methods amplify biases during the fine-tuning. Similarly, Auto-Debias performs well on the SEAT benchmark but experiences an increase in bias when applied to downstream tasks, mirroring the trend observed in the Bias-STS-B extrinsic benchmark. Moreover, ADELE, another parameter-efficient method, performs poorly in both bias mitigation and model accuracy. Similar to Bias-STS-B extrinsic benchmark, PT amplifies biases during the tuning process, resulting in a decline in the NN score from 0.824 to 0.741 and the FN score from 0.868 to 0.812. By employing Co2PT, we observe significant improvements with the NN score rising to 0.877 (from 0.741) and the FN score reaching 0.965 (from 0.812), indicating the effectiveness of Co2PT on bias mitigation.\nBias-in-Bios. Next we show the performance on the Bias-in-Bios benchmark in Table 4. Among all the baselines, the ZariCDA achieves the lowest GAPTPR score of 2.667 while the BERT+CDA\nachieves the lowest GAPRMS score of 0.113. Furthermore, PT exacerbates bias and results in an increase in the GAPTPR score from 2.822 to 3.171 and GAPRMS from 0.119 to 0.129. In contrast, Co2PT reduces the GAPTPR score from 3.171 to 2.537 and GAPRMS from 0.129 to 0.123, demonstrating its effectiveness in mitigating bias in the occupation classification task.\n6 Integrating Co2PT with Existing Debiased Models (RQ2)\nOne benefit of a prompt-then-finetune model like Co2PT is that it can be easily integrated with existing upstream debiasing methods. Here we investigate the applicability of Co2PT to three existing debiased models to bridge the gap in utilizing upstream debiased models for downstream tasks. Based on the comparison results \u2013 before and after applying Co2PT \u2013 shown in Table 5, Co2PT significantly reduces the bias scores for Context-Debias and Auto-Debias: 0.088 versus 0.332, and 0.068 versus 0.312, respectively. For MABEL, which achieves low bias scores, there is no significant effect on the bias score. Additionally, Co2PT improves the model performance on the downstream tasks. These results clearly demonstrate the effectiveness of integrating Co2PT into established debiased models for downstream tasks. This ability enables the existing debiased models to achieve strong performance on downstream tasks while simultaneously maintaining a low bias level."
        },
        {
            "heading": "7 Impact of Design (RQ3)",
            "text": "We perform an extensive ablation study to show how different components affect Co2PT in Table 6. We use Bias-STS-B as the representative task for computational efficiency.\nImpact of counterfactual module. First, we perform counterfactual data augmentation on the training data containing bias-attribute terms. Then we conduct prompt tuning only on these augmented pairs (denoted as PT+CDA). PT+CDA reduces\nthe bias score from 0.321 in PT to 0.291, showing the effectiveness of the straightforward counterfactual data augmentation approach. However, the improvement is less than Co2PT, implying the necessity of the contrastive learning module. Impact of contrastive module. To investigate the impact of the contrastive module, instead of employing constructed counterfactual sentence pairs as positive pairs for contrastive loss, we use unsupervised contrastive loss by encoding the same input twice and get two embeddings with different dropout masks z, z\u2032 (Gao et al., 2021). Then the contrastive objective becomes:\nLscl = \u2212 log esim(p\u2295h\nzi i ,p\u2295h z\u2032i i )/\u03c4\u2211N\nj=1 e sim(p\u2295hzii ,p\u2295h\nz\u2032 j j )/\u03c4\n, (5)\nwhich is optimized with the prompt tuning loss Lpt together (denoted as PT+SCL). PT+SCL surpasses both PT and PT+CDA by achieving a large reduction in bias score to 0.161, demonstrating the effectiveness of the contrastive module. Impact of adding contrastive loss for nonaugmented inputs. In Co2PT, we only consider counterfactually augmented pairs in the contrastive module. To explore the necessity of including inputs without demographic terms in the contrastive module, we incorporate unsupervised contrastive loss for non-augmented input like Equation 5 as L\u2032scl and tuned with contrastive loss Lcl for counterfactually augmented pairs (Equation 1) and L\u2032pt (denoted as Co2PT+SCLn). The bias score of 0.117 achieved by Co2PT+SCLn is higher than Co2PT and this indicates that incorporating a contrastive loss for non-augmented inputs in the training set is unnecessary. Compare Co2PT with task-agnostic counterfactual pairs. To investigate whether integrating taskagnostic neutral entailment pairs can benefit debiasing on the task, we use 142,158 gender-balanced entailment pairs augmented from SNLI and MNLI\ndatasets in He et al. (2022) as task-agnostic entailment pairs for STS-B task instead of using the taskspecific counterfactual pairs augmented from the training set (denoted as PT+NLI+CL). We notice that although PT+NLI+CL does not outperform Co2PT, it shows a strong ability to mitigate bias compared to other baseline methods. Thus, when working on a moderate amount of training data, it is better to use counterfactually augmented pairs from the training data.\nCompare Co2PT with other contrastive objective. For sentence-pair classification tasks, we also explore the contrastive loss that encourages the inter-association of entailment pairs (He et al., 2022). For the original input pair (si1, si2) and its augmented pair (s\u2032i1 , s \u2032 i2), si1 and si2 are treated as positive pairs while si1 and s\u2032i2 and other inbatch sj2 are negatives, and vice versa (denoted as CLp). When using task-specific counterfactual pairs, PT+CDA+CLp decreases the bias score to 0.271. Similarly, using task-agnostic counterfactual pairs, PT+NLI+CLp also reduces the bias score to 0.271. However, the bias mitigation effect is not as significant as that achieved by Co2PT, which indicates the effectiveness of the contrastive module in Co2PT."
        },
        {
            "heading": "8 Impact of Hyperparameters (RQ4)",
            "text": "Finally, we investigate the impact of three hyperparameters: (i) the continuous prompt length; (ii) the temperature \u03c4 of contrastive loss Lcl; and (iii) the coefficient \u03b1 of total learning objective L. Impact of prompt length. First, we experiment with the prompt length varying in {10, 20, 50}, as illustrated in Figure 2. Generally speaking, with more tunable prompt parameters, the model performs better on downstream tasks. In addition, when the prompt length is 10, Co2PT shows a higher increase in bias score compared to the prompt length of 20 and 50. This indicates that a\nlarger prompt length enables the model to achieve better model performance on downstream tasks more rapidly while still maintaining a lower bias score. However, it is important to consider that using larger prompt lengths means tuning more parameters, thus posing a trade-off.\nImpact of \u03c4 . Then, we vary temperature \u03c4 in {0.005, 0.05, 0.5}. Figure 3 shows close significant bias mitigation effects when \u03c4 is set to 0.005 and 0.05 while exhibiting less effectiveness when \u03c4 is 0.5. This observation implies that a higher temperature value corresponds to less weight of the cosine similarity calculation, resulting in decreased effectiveness in bias mitigation.\nImpact of \u03b1. Last, we study the impact of coefficient \u03b1 and vary the value in {0.1, 0.5, 1.0}. Figure 4 emphasizes that reducing the value of \u03b1 at a constant \u03c4 , thus assigning less weight to the contrastive module, leads to decreased bias mitigation effects. This analysis underscores the importance of carefully selecting appropriate hyperparameters."
        },
        {
            "heading": "9 Conclusion and Future Work",
            "text": "We propose Co2PT, an efficient and effective debiasing method for mitigating bias in downstream tasks. We evaluate its effectiveness on bias mitigation and applicability to existing debiased upstream models, and investigate how the design of each component and the selection of hyperparameters impact both its bias reduction capabilities and downstream task performance.\nMitigating non-gender and intersectional bias. Mitigating non-gender biases is challenging as some debiasing methods work well on reducing gender biases but show poor generalization capabilities in addressing biases beyond gender (Meade et al., 2022). Without re-training the model, Co2PT is flexible to apply in order to mitigate different bias types in downstream applications. One can train different debiasing prompts to tackle different bias dimensions such as gender, race, and religion. Furthermore, these debiasing prompts can be applied to mitigate intersectional bias by simply combining the corresponding prompts in downstream tasks.\nLimitations\nWhile this work primarily addresses bias in English, we acknowledge the presence of more complicated bias cases in other languages. Therefore, future exploration of existing methods or the development of new techniques to mitigate bias in other languages would be valuable. Furthermore, despite the efficiency and comparable performance of deep prompt tuning compared to fine-tuning, it still underperforms fine-tuning on certain datasets when the model size is small. This will also limit the model performance of our method.\nEthics Statement\nIn this work, when investigating gender bias in pre-trained language models, we focus on the binary definition of gender as the targeted attribute of discrimination. However, it is important to acknowledge that future research should also consider non-binary genders and other multi-class scenarios to comprehensively address bias."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank Nicholas Meade for the initial discussion and anonymous reviewers for valuable feedback."
        },
        {
            "heading": "A More Implementation Details",
            "text": "A.1 Dataset and Extrinsic Bias Benchmarks\nSTS-B and SNLI datasets are from the Hugging Face Datasets library (Lhoest et al., 2021).2 We use the same Bias-in-Bios dataset used in Ravfogel et al. (2020).3, the Bias-STS-B used in Lauscher et al. (2021) and the Bias-NLI used in He et al. (2022). The code used to assess Bias-STS-B is modified from Lauscher et al. (2021), while the code for evaluating Bias-NLI and Bias-in-Bios is adapted from He et al. (2022). Different from the code employed in He et al. (2022), we conduct our evaluation on the entire Bias-NLI dataset rather than just the top 10% of it.\nA.2 Debiased Baselines\nPlease refer to the footnotes here for the source of released debiased models \u2013 ZariCDA, ZariDO4 (Webster et al., 2020), ContextDebias5 (Kaneko and Bollegala, 2021), AutoDebias6 (Guo et al., 2022), MABEL7 (He et al., 2022) \u2013 used on downstream tasks.\nA.3 Implementation Details\nWe use BERT-base-uncased in our experiments. For the single-sentence classification task, we prepend the [CLS] token before the input sentences and feed it into the BERT model to get the embedding of the [CLS] token as the sentence representation. For the sentence-pair classification task, e.g., SNLI task, we prepend [CLS] before the premise x and [SEP] token to separate premise x and hypothesis y and feed it into the BERT model to get the embedding of the [CLS] token as the sentence representation. Fine-tuning and prompt tuning code rely on the Huggingface implementation.8\n2https://github.com/huggingface/datasets. Apache License 2.0.\n3The data is downloaded through https://github. com/shauli-ravfogel/nullspace_projection/blob/ master/download_data.sh MIT License.\n4The checkpoints of these two models are from https:// github.com/google-research-datasets/Zari. Apache2.0 license.\n5The checkpoints are from https://github.com/ kanekomasahiro/context-debias. MIT license.\n6The checkpoints are from https://github.com/ Irenehere/Auto-Debias.\n7The checkpoints are from https://huggingface.co/ princeton-nlp/mabel-bert-base-uncased MIT License.\n8https://github.com/huggingface"
        },
        {
            "heading": "B More Ablation Studies",
            "text": "Pooling method. Besides using pooled output, we also conduct experiments that use the average token representation from the model\u2019s last hidden state as sentence representation, shown in Table 7. However, upon analyzing the results, Co2PTavg. is considerably more challenging for the prompts to acquire the debiasing capability when using the average token representation compared to when utilizing the CLS token as the sentence representation."
        },
        {
            "heading": "C Standard Deviation of Results",
            "text": "The standard deviation of evaluation results on extrinsic bias benchmarks \u2013 Bias-STS-B, Bias-NLI, and Bias-in-Bios \u2013 are presented in Tables 8 to 10, respectively. In addition, the results of integrating Co2PT with existing debiased models and ablation study are shown in Table 11 and Table 12. These results indicate that Co2PT consistently performs well with relatively low variability, demonstrating its effectiveness and reliability.\nD Visualization of Bias Mitigation Effects along Epochs\nWe visualize the changes of bias scores along epochs in Figure 5. The bias score of PT keeps increasing as the Pearson score increases, while Co2PT consistently maintains a low bias score, which indicates the effectiveness of Co2PT on bias mitigation."
        }
    ],
    "title": "CoPT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning",
    "year": 2023
}