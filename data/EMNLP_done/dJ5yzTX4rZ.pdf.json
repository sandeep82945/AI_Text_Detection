{
    "abstractText": "Modelling how concepts are related is a central topic in Lexical Semantics. A common strategy is to rely on knowledge graphs (KGs) such as ConceptNet, and to model the relation between two concepts as a set of paths. However, KGs are limited to a fixed set of relation types, and they are incomplete and often noisy. Another strategy is to distill relation embeddings from a fine-tuned language model. However, this is less suitable for words that are only indirectly related and it does not readily allow us to incorporate structured domain knowledge. In this paper, we aim to combine the best of both worlds. We model relations as paths but associate their edges with relation embeddings. The paths are obtained by first identifying suitable intermediate words and then selecting those words for which informative relation embeddings can be obtained. We empirically show that our proposed representations are useful for solving hard analogy questions.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Nitesh Kumar"
        },
        {
            "affiliations": [],
            "name": "Steven Schockaert"
        }
    ],
    "id": "SP:f5fa70ab76a92fa09ecf29282fd7031b09effcc4",
    "references": [
        {
            "authors": [
                "Adrian Boteanu",
                "Sonia Chernova."
            ],
            "title": "Solving and explaining analogy questions using semantic networks",
            "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA, pages 1460\u20131466. AAAI",
            "year": 2015
        },
        {
            "authors": [
                "Jiangjie Chen",
                "Rui Xu",
                "Ziquan Fu",
                "Wei Shi",
                "Zhongqiao Li",
                "Xinbo Zhang",
                "Changzhi Sun",
                "Lei Li",
                "Yanghua Xiao",
                "Hao Zhou."
            ],
            "title": "E-KAR: A benchmark for rationalizing natural language analogical reasoning",
            "venue": "Findings of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Tamara Czinczoll",
                "Helen Yannakoudakis",
                "Pushkar Mishra",
                "Ekaterina Shutova."
            ],
            "title": "Scientific and creative analogies in pretrained language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2094\u20132100, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Tamara Czinczoll",
                "Helen Yannakoudakis",
                "Pushkar Mishra",
                "Ekaterina Shutova."
            ],
            "title": "Scientific and creative analogies in pretrained language models",
            "venue": "CoRR, abs/2211.15268.",
            "year": 2022
        },
        {
            "authors": [
                "Cai",
                "Mingyang Ling"
            ],
            "title": "Scene graph genera",
            "year": 2019
        },
        {
            "authors": [
                "Keith Holyoak"
            ],
            "title": "SemEval-2012 task 2: Measur",
            "year": 2012
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Wen-tau Yih",
                "Geoffrey Zweig."
            ],
            "title": "Linguistic regularities in continuous space word representations",
            "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2013
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher Manning."
            ],
            "title": "GloVe: Global vectors for word representation",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar.",
            "year": 2014
        },
        {
            "authors": [
                "Robyn Speer",
                "Joshua Chin",
                "Catherine Havasi"
            ],
            "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
            "venue": "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February",
            "year": 2017
        },
        {
            "authors": [
                "Oren Sultan",
                "Dafna Shahaf."
            ],
            "title": "Life is a circus and we are the clowns: Automatically finding analogies between situations and processes",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3547\u20133562,",
            "year": 2022
        },
        {
            "authors": [
                "Yueqing Sun",
                "Qi Shi",
                "Le Qi",
                "Yu Zhang."
            ],
            "title": "JointLK: Joint reasoning with language models and knowledge graphs for commonsense question answering",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Com-",
            "year": 2022
        },
        {
            "authors": [
                "Peter D. Turney."
            ],
            "title": "Measuring semantic similarity by latent relational analysis",
            "venue": "IJCAI-05, Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence, Edinburgh, Scotland, UK, July 30 - August 5, 2005, pages 1136\u20131141. Profes-",
            "year": 2005
        },
        {
            "authors": [
                "Peter D. Turney."
            ],
            "title": "Domain and function: A dualspace model of semantic relations and compositions",
            "venue": "J. Artif. Intell. Res., 44:533\u2013585.",
            "year": 2012
        },
        {
            "authors": [
                "Peter D. Turney",
                "Michael L. Littman",
                "Jeffrey Bigham",
                "Victor Shnayder."
            ],
            "title": "Combining independent modules in lexical multiple-choice problems",
            "venue": "Recent Advances in Natural Language Processing III, Selected Papers from RANLP 2003, Borovets, Bul-",
            "year": 2003
        },
        {
            "authors": [
                "Asahi Ushio",
                "Jose Camacho-Collados",
                "Steven Schockaert."
            ],
            "title": "Distilling relation embeddings from pretrained language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9044\u20139062, Online",
            "year": 2021
        },
        {
            "authors": [
                "Asahi Ushio",
                "Luis Espinosa Anke",
                "Steven Schockaert",
                "Jose Camacho-Collados"
            ],
            "title": "2021b. BERT is to NLP what AlexNet is to CV: Can pre-trained language models identify analogies",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V. Le",
                "Denny Zhou."
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Michihiro Yasunaga",
                "Hongyu Ren",
                "Antoine Bosselut",
                "Percy Liang",
                "Jure Leskovec."
            ],
            "title": "QA-GNN: Reasoning with language models and knowledge graphs for question answering",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter",
            "year": 2021
        },
        {
            "authors": [
                "Siyu Yuan",
                "Jiangjie Chen",
                "Changzhi Sun",
                "Jiaqing Liang",
                "Yanghua Xiao",
                "Deqing Yang."
            ],
            "title": "ANALOGYKB: unlocking analogical reasoning of language models with A million-scale knowledge base",
            "venue": "CoRR, abs/2305.05994.",
            "year": 2023
        },
        {
            "authors": [
                "Jing Zhang",
                "Xiaokang Zhang",
                "Jifan Yu",
                "Jian Tang",
                "Jie Tang",
                "Cuiping Li",
                "Hong Chen."
            ],
            "title": "Subgraph retrieval enhanced model for multi-hop knowledge base question answering",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Xikun Zhang",
                "Antoine Bosselut",
                "Michihiro Yasunaga",
                "Hongyu Ren",
                "Percy Liang",
                "Christopher D. Manning",
                "Jure Leskovec."
            ],
            "title": "Greaselm: Graph reasoning enhanced language models",
            "venue": "The Tenth International Conference on Learning Representa-",
            "year": 2022
        },
        {
            "authors": [
                "Yilun Zhou",
                "Steven Schockaert",
                "Julie Shah."
            ],
            "title": "Predicting conceptnet path quality using crowdsourced assessments of naturalness",
            "venue": "The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019, pages 2460\u20132471. ACM.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Many applications rely on Knowledge graphs (KGs) to model the relationship between concepts. For instance, KGs have been used to characterise how an answer candidate is related to the concepts that appear in a question (Yasunaga et al., 2021; Zhang et al., 2022b) and to help interpret visual scenes (Gu et al., 2019). In such applications, relations are modelled as KG paths, which has two key advantages: (i) we can easily inject domain-specific knowledge and (ii) the interpretable nature of KG paths offers a degree of transparency. But KGs also have important drawbacks. They use a fixed set of relation types, which may not be fine-grained enough. KGs are also inevitably incomplete and, especially in the case of commonsense KGs such as ConceptNet (Speer et al., 2017), noisy.\n1Source code to reproduce our experimental results and the model checkpoints are available in the following repository: https://github.com/niteshroyal/ SolvingHardAnalogyQuestions.\nThere is also a tradition in Natural Language Processing (NLP) to model relations as embeddings. For instance, DIRT (Lin and Pantel, 2001) and LRA (Turney, 2005) are early examples of methods which used vectors to represent the relation between two words. More recently, relation embeddings have been obtained using fine-tuned language models. For instance, RelBERT (Ushio et al., 2021a) has achieved state-of-the-art results on several analogy datasets using a fine-tuned RoBERTalarge model (Liu et al., 2019). The strengths and weaknesses of relation embeddings are complementary to those of KGs: relation embeddings lack transparency and cannot easily incorporate external knowledge, but they are capable of capturing subtle differences, making it possible to encode relational knowledge in a way that is considerably richer than what can be achieved with KGs. A final drawback of relation embeddings is that they are best suited for concept pairs that have a clear and direct relationship. For instance, they can encode the relation between umbrella and rain, but are less suitable for encoding the relation between umbrella and cloudy (e.g. if it is cloudy, there is a chance of rain, which means that we might take an umbrella).\nIn this paper, we propose a hybrid approach which combines the advantages of KGs with those of relation embeddings. The idea is conceptually straightforward. We represent relations as paths, where nodes correspond to concepts, but rather than associating edges with discrete types, we label them with relation embeddings. We will refer to such paths as relation embedding chains. Clearly, this approach allows us to model indirect relationships, while keeping the flexibility of embeddings, as well as some of the interpretability of KG paths.\nWe are still faced with the challenge of selecting suitable paths. KGs are insufficient for this purpose, given their noisy and incomplete nature. Our solution relies on the following idea: to decide whether a \u2192 x \u2192 b is a suitable path for\nmodelling the relationship between a and b, what matters is whether we have access to an informative relation embedding for the word pairs (a, x) and (x, b). Motivated by this, we first develop a classifier to predict whether a given RelBERT embedding is informative or not. We then generate possible relational paths, using ConceptNet as well as standard word embeddings, and filter these paths based on the informativeness classifier.\nWhile relation embedding chains are expressive, we sometimes need a simpler representation, especially in unsupervised settings. We therefore also study how the information captured by relation embedding chains can be summarised as a single vector, without relying on task-specific supervision.\nTo evaluate the usefulness of relation embedding chains, we focus on word analogy questions. Given a query word pair (e.g. word:language) and a set of candidate word pairs, the task is to select the most analogous candidate (e.g. note,music). We show that relation embedding chains are well-suited for answering hard analogy questions."
        },
        {
            "heading": "2 Related Work",
            "text": "Modelling analogies has been a long-standing topic of interest in NLP (Turney, 2005, 2012). Recent years have seen a significant increase in interest in this topic, fuelled by the success of large language models (LLMs). For instance, Bhavya et al. (2022) used LLMs for generating explanations of scientific concepts involving analogies, while Sultan and Shahaf (2022) used LLMs for identifying analogies between procedural texts. However, most work has focused on modelling analogies between word pairs (Ushio et al., 2021b,a; Chen et al., 2022; Czinczoll et al., 2022a; Li et al., 2023; Yuan et al., 2023), which is also the setting we consider in this paper. We focus in particular on RelBERT (Ushio et al., 2021a), which is the state-of-the-art on several benchmarks. RelBERT is a RoBERTa-large model that was fine-tuned on the relational similarity dataset from SemEval 2012 Task 2 (Jurgens et al., 2012). Given a word pair, RelBERT computes a relation embedding by feeding that word pair as input to the fine-tuned RoBERTa model and averaging the output embeddings.\nThe use of KG paths for modelling relations between concepts has also been extensively studied. For instance, Boteanu and Chernova (2015) proposed the use of ConceptNet paths for explaining why two word pairs are analogous. Zhou et al.\n(2019) highlighted the noisy nature of many ConceptNet paths. To address this issue, they trained a system to predict path quality based on crowdsourced judgments of naturalness. More recently, ConceptNet paths have been used to provide external knowledge to NLP systems, for instance in question answering systems (Lin et al., 2019; Yasunaga et al., 2021; Zhang et al., 2022b; Jiang et al., 2022; Sun et al., 2022). In such cases, a Graph Neural Network is typically used to process the paths, and the resulting representations are then integrated with those obtained by a language model. An important distinction with our work is that we focus on unsupervised settings. KG paths are especially helpful for question answering over KGs. However, most approaches rely on matching chains of discrete relation types (Das et al., 2022; Zhang et al., 2022a). Chains of relation embeddings, as we study in this paper, make it possible to model finer relationships."
        },
        {
            "heading": "3 Scoring RelBERT Embeddings",
            "text": "Given a word pair (a, b), we write rab \u2208 Rd for the corresponding RelBERT embedding. While RelBERT can provide a relation embedding for any pair of words, not all these embeddings are equally informative. In particular, we would like to distinguish embeddings rab which encode a specific relationship from embeddings that rather reflect the lack of any (known) relationship between a and b. Unfortunately, the relation embeddings of different unrelated word pairs are typically not similar.\nTo address this, we want to train a classifier to predict whether a given relation embedding rab is informative or not. However, this requires access to a set of related word pairs Pos and a set of unrelated word pairs Neg. The set Neg can simply be constructed by choosing random word pairs. While we may occasionally end up with word pairs that are actually related in some way, the probability that this is the case for randomly chosen words is sufficiently low for this not to be problematic. However, constructing the set Pos is less straightforward, since we are not looking for words that have a particular relation, but rather for words that have any kind of (clear) relationship. If the examples are not sufficiently diverse, then our classifier will simply detect whether rab corresponds to one of the relations that were considered during training. One of the few relevant large-scale resources we have at our disposal is ConceptNet. However, during\ninitial experiments, we quickly found ConceptNet to be too noisy for this purpose. We therefore instead relied on GPT-42 to generate a diverse set of around 11,000 positive examples. The prompt we used to obtain a sufficiently diverse set of examples is shown in the Appendix D.\nTo create a set of negative examples, of the same size, we corrupted the positive examples. We then trained a logistic regression classifier on the resulting training set. Throughout this paper, we will write inf(rab) for the probability that rab expresses an informative relationship, according to this classifier. We will refer to this value as the informativeness of the embedding rab."
        },
        {
            "heading": "4 Connecting Concepts",
            "text": "The relationship between two concepts a and b is sometimes easiest to explain using an intermediate concept x. For instance, umbrella is related to cloudy because (i) an umbrella protects against rain and (ii) rain is only possible if it is cloudy. A natural strategy for identifying such intermediate concepts is to find paths of length two connecting the target words a and b in ConceptNet. However, the coverage of ConceptNet is limited, and many relevant intermediate concepts might not be found in this way. One possible solution would be to consider a sequence of intermediate concepts, and model the relation between a and b in terms of a path a \u2192 x1 \u2192 ... \u2192 xn \u2192 b. However, longer ConceptNet paths are often too noisy to be useful. Moreover, in practice a single intermediate concept is usually sufficient, so rather than considering longer paths, we propose two strategies to find additional links between concepts based on word embeddings. They are illustrated in Figure 1.\nMissing Link Prediction We add missing links based on the informativeness classifier from Section 3. Specifically, we first use a standard word embedding to find the top-k most similar words to a, with e.g. k = 500. For each of these words y, we add a link between a and y if inf(ray) > 0.75.3 We add missing links from b in the same way. We will refer to this strategy as missing link prediction. Note that this strategy not only finds synonyms or morphological variations of words but also often\n2https://openai.com/research/gpt-4 3Since we focused on unsupervised settings in this paper, we cannot tune this value. Compared to a threshold of 0.5, the choice of 0.75 reflects the idea that we want to be selective: if in doubt it seems better not to add the link.\nfinds words that are related in different ways. For instance, some links that are not present in ConceptNet and have been added using this approach include: (dog, owners), (cashier, grocery store), (helium, noble gases), (drug trafficking, illegal), and (disinfectant, sterilization). Such links clearly enrich ConceptNet in a non-trivial way.\nSemantic Smoothing For the second strategy, we only consider the top-5 neighbours, noting that these are often either synonyms or morphological variations of the same word (e.g. cloud and cloudy). Specifically, rather than only considering x as an intermediate concept if x is connected to both a and b, we now consider x as an intermediate concept as soon as it is connected to one of the 5 nearest neighbours of a (or a itself) and one of the 5 nearest neighbours of b (or b itself). We will refer to this second strategy as semantic smoothing."
        },
        {
            "heading": "5 Condensing Relation Embedding Chains",
            "text": "The strategy from Section 4 allows us to identify intermediate concepts x1, ..., xn that can help to describe the relation between two given words a and b. Each intermediate concept xi corresponds to a chain raxi ; rxib of two relation embeddings. These relation embedding chains can encode relationships in a fine-grained way, but in practice we often need a more compact representation. We therefore train a simple model to summarise a set of relation em-\nbedding chains {rax1 ; rx1b, ..., raxn ; rxnb} for a given concept pair (a, b) as a single vector. Our model has the following form:\nsab = \u03c8 ( f ( \u03d5(rax1 , rx1b), ..., \u03d5(raxn , rxnb) )) The function \u03d5 : Rd \u00d7 Rd \u2192 Rm is intuitively doing a kind of relational composition: it aims to predict a relation embedding for the pair (a, b) from the relation embeddings raxi and rxib. These predicted embeddings are then combined using a pooling function f : Rm\u00d7 ...\u00d7Rm \u2192 Rm. Finally, the resulting vector is mapped back to a d-dimensional embedding using the decoder \u03c8 : Rm \u2192 Rd.\nFor the decoder \u03c8, we simply use a linear layer. For the pooling function f , we experimented with sum-pooling and max-pooling. We found the result to be largely insensitive to this choice. Throughout this paper, we will therefore fix f as summation. Finally, the composition function \u03d5 is implemented as follows:\n\u03d5(rax, rxb) = GeLU(A(rax \u2295 rxb) + b) (1)\nwhere we write \u2295 for vector concatenation, A is a matrix, and b is a bias vector.\nTraining Our focus is on solving analogy questions, which is an unsupervised problem. In the absence of task-specific training data, we train the model to predict the RelBERT embedding rab, using the following loss:\nL = \u2212 \u2211 (a,b) cos(sab, rab) (2)\nwhere sab is the vector predicted from the relation embedding chains, and rab is the RelBERT embedding. The sum in (2) ranges over concept pairs (a, b) from ConceptNet for which the informativeness inf(rab) is sufficiently high. This is important to ensure that the model is not trained on a noisy supervision signal. Specifically, we only considered concept pairs (a, b) for which inf(rab) > 0.75.\nNote that while we train the model on concept pairs (a, b) which have an informative RelBERT embedding, our aim is to use this model for pairs for which this is not the case. We thus rely on the assumption that a composition model which is trained on word pairs with informative RelBERT vectors will generalise in a meaningful way to word pairs for which this is not the case."
        },
        {
            "heading": "6 Solving Analogy Questions",
            "text": "Analogy questions involve a query pair (a, b) and a number of candidate answers (x1, y1), ..., (xk, yk). The aim is to select the candidate (xi, yi) whose relationship is most similar to that of the query pair. When using RelBERT, we simply choose the pair (xi, yi) for which cos(rab, rxiyi) is maximal.\nIdentifying Hard Analogy Questions Our key hypothesis is that the informativeness of the RelBERT vectors, as predicted by the classifier from Section 3, can tell us whether using RelBERT vectors is a reliable strategy for a particular analogy question. The lower the informativeness of rab or rxiyi , with (a, b) the query and (xi, yi) the chosen answer candidate, the less confident we can be about the chosen answer. We define our confidence in RelBERT\u2019s prediction as follows:\nconf(a, b, x, y) = min(inf(rab), inf(rxy)) (3)\nwith (a, b) the query pair and (x, y) the candidate selected by RelBERT. In cases where the confidence is low, we aim to improve the prediction by taking advantage of relation embedding chains.\nCondensed Relation Chain Comparison We can straightforwardly solve analogy questions using condensed relation chains. In particular, we use the model from Section 5 to obtain relation embeddings sxy for the query and the candidate pairs. We then proceed in the same way as with RelBERT, choosing the answer candidate (xi, yi) for which cos(sab, sxiyi) is maximal.\nDirect Relation Chain Comparison We can also solve analogy questions by using relation chains directly. Let c1, ..., cu be the intermediate concepts for the query pair (a, b) and let z1, ..., zv be the intermediate concepts for an answer candidate (x, y). Then we evaluate the compatibility comp(a, b, x, y) of this candidate as follows:\nu\u2211 i=1 v max j=1 sim(raci ; rcib, rxzj ; rzjy)\nThe idea is that when (a, b) and (x, y) are analogous, for most relation chains raci ; rcib connecting a and b there should exist a similar relation chain rxzj ; rzjy connecting x and y. The similarity between relation chains can be evaluated as follows:\nsim1(raci ; rcib, rxzj ; rzjy) (4)\n= min(cos(raci , rxzj), cos(rcib, rzjy))\nIn other words, two relation chains are similar if their first component is similar and their second component is also similar. Our default configuration uses this approach. However, this may be too strict. For instance, suppose that rcib and rzjy capture the located-at relation, while raci captures part-of and rxzj captures is-a. Then the relation chains raci ; rcib and rxzj ; rzjy both essentially encode that a is located at c, but they would not be considered to be similar according to (4). To allow such relation chains to be identified as being similar, we will also consider the following alternative:\nsim2(raci ; rcib, rxzj ; rzjy) (5)\n= cos(\u03c8(\u03d5(raci ; rcib)), \u03c8(\u03d5(rxzj ; rzjy)))\nwith \u03d5 and \u03c8 the composition function and decoder of the model for condensing relation chains. Finally, we will also consider a baseline strategy which ignores the order of the relation embeddings:\nsim3(raci ; rcib, rxzj ; rzjy) (6)\n= cos(raci + rcib, rxzj + rzjy)"
        },
        {
            "heading": "7 Experiments",
            "text": "We empirically analyse our proposed strategies for solving hard analogy questions. We are specifically interested in the following research questions. (i) How suitable is the confidence degree (3) as a proxy for estimating the difficulty of an analogy question? (ii) How suitable are relation embedding chains for answering difficult analogy questions? (iii) What are the best ways for learning and using these relation embedding chains?"
        },
        {
            "heading": "7.1 Experimental Setup",
            "text": "Datasets We evaluate our models on a number of different analogy question datasets. First, we use the 5 datasets that were used by Ushio et al. (2021b): the SAT dataset proposed by Turney et al. (2003); the U2 and U4 datasets which were obtained from an educational website; and reformulations of the Google (Mikolov et al., 2013) and BATS (Gladkova et al., 2016) datasets into the multiple-choice analogy question format. We obtained these datasets from the RelBERT repository4. We also use a reformulation of SCAN (Czinczoll et al., 2022b) into the multiple-choice analogy question format, which is available from the\n4https://huggingface.co/datasets/relbert/ analogy_questions\nsame repository. Finally, we include the English version of E-KAR5 (Chen et al., 2022). E-KAR contains questions questions involving word pairs and questions involving word triples. We only considered the former for our experiments. We use accuracy as the evaluation metric.\nThere are some important differences between these datasets. For instance, Google and BATS focus on morphological, encyclopedic, and in the case of BATS, lexical relations. SCAN focuses on scientific and creative analogies, requiring models to link relationships from one domain (e.g. the solar system) to relationships to a completely different domain (e.g. atoms). The other datasets focus on abstract conceptual relations, but they cover a range of difficulty levels (for humans): SAT is obtained from college entrance tests; U2 is aimed at children from primary and secondary school; U4 has difficult levels ranging from college entrance tests to graduate school admission tests; E-KAR was derived from Chinese civil service exams.\nMethods The RelBERT repository on Huggingface contains a number of different RelBERT variants. We have used the model6 trained using Noise Contrastive Estimation on SemEval 2012 Task 2 (Jurgens et al., 2012), as this variant outperforms the original variant from Ushio et al. (2021a).\nFor our implementation of missing link prediction we combined the top-250 neighbours (in terms of cosine similarity) according to the ConceptNet Numberbatch word embedding (Speer et al., 2017) with the top-250 neighbours according to GloVe (Pennington et al., 2014). For semantic smoothing we used to top-5 neighbours from GloVe.\nIn terms of baselines, our primary emphasis is on comparing the proposed methods with RelBERT, which is the state-of-the-art relation embedding model7. To provide additional context, we have also obtained results with GPT-4.8 Following earlier work on using LLMs for solving analogy questions (Yuan et al., 2023), we ask the model to generate explanations (Wei et al., 2022) and include a few solved questions as part of the prompt. The\n5Available at https://ekar-leaderboard.github.io 6Available from https://huggingface.co/relbert/ relbert-roberta-large-nce-semeval2012-0-400 7While Yuan et al. (2023) have reported higher results on some datasets using another fine-tuned RoBERTa model, their method does not learn relation embeddings but solves the task as a multiple-choice question answering problem. Moreover, their model was fine-tuned on the validation split of each benchmark, which makes the results incomparable.\n8Details of our prompt can be found in the appendix.\nresults of GPT-4 should be interpreted with some caveats, however. For instance, it is well-known that the results of LLMs can be highly sensitive to the choice of the prompt, so it is likely that better results are possible. Moreover, it is possible that GPT-4 has seen some of the datasets during training, which could lead to inflated results. Besides GPT-4, we also include the results that were obtained by Yuan et al. (2023) using ChatGPT and InstructGPT003 with chain-of-thought prompting."
        },
        {
            "heading": "7.2 Results",
            "text": "Table 1 shows our main results, focusing on four methods: RelBERT (RelB), condensed relation embedding chain comparison (Cond), direct relation chain comparison (Dir) and GPT-4. The results are broken down based on our confidence in RelBERT\u2019s prediction, as computed by (3). The overall results per dataset are summarised in Table 2. In this table, we also report the LLM results obtained by Yuan et al. (2023). We furthermore consider\nfour hybrid methods. The idea of these methods is to rely on RelBERT for the easy questions and on either Cond or Direct for the hard questions. For instance, Cond<0.5 uses Cond for questions with difficulty levels below 0.5 (as estimated using (3)) and RelBERT for the others. A number of conclusions can be drawn.\nConfidence scores faithfully predict question difficulty We can see that the performance of RelBERT is closely aligned with the predicted difficulty level. On average, across all datasets, the accuracy ranges from 31.1% for the hardest questions to 71.2% for the easiest questions. This pattern can be observed for all datasets. Moreover, the predicted difficulty level is also aligned with the performance of the other models. For instance, GPT-4 on average also performs best on the questions with high confidence values, especially for SAT, U4, E-KAR and SCAN. For Google and BATS, GPT4 performs well throughout. This suggests that the confidence score (3) is successful in predicting intrinsic question difficulty, rather than merely predicting where RelBERT is likely to fail.\nRelation embedding chains are helpful for hard questions Focusing on the performance for the hardest questions, with confidence levels in [0.0,0.25), we can see that Condensed outperforms RelBERT on all datasets, with the exception of SCAN (where the results are close). Direct outperforms in most datasets as well, but not in SCAN and SAT. For the questions with a difficult range in [0.25,0.50), Direct still outperforms RelBERT in most cases, with E-KAR now the only exception. For the questions with confidence level in the range [0.50,0.75) the performance of RelBERT is\ngenerally quite similar to that of Condensed and Direct. Finally, for the easiest questions, RelBERT is clearly better, with the exception of SCAN. In accordance with these observations, in Table 2 we can see that the hybrid models generally outperform RelBERT, except for SCAN and E-KAR where their performance is similar. For SAT, three of the hybrid models outperform the state-of-the-art result of RelBERT. Finally, comparing Condensed and Direct there is no clear winner, with the former performing better for the easiest questions and the latter performing better for the hardest ones.\nGPT-4 performs best overall It is also notable how similar the GPT-4 results are to the ChatGPT results obtained by Yuan et al. (2023). However, GPT-4 and ChatGPT do not provide relation embeddings and can thus not replace models such as RelBERT in many applications (e.g. for retrieval tasks). In Table 1, we can also see that Condensed and Direct can sometimes outperform GPT-4 on the hardest questions, namely for U4 and E-KAR. Interestingly, these are also the benchmarks with the highest intended difficulty level for humans. Moreover, for SCAN, which requires making suitable abstractions, GPT-4 generally underperforms."
        },
        {
            "heading": "7.3 Analysis",
            "text": "We now provide further analysis about the direct and condensed relation chain comparison methods. We also include a qualitative analysis to better un-\nderstand the nature of relation embedding chains.\nDirect Relation Chain Comparison Table 3 compares our default configuration (Direct), which uses sim1 to measure the similarity between relation embedding chains, with a number of variants. First, the rows labelled sim2 and sim3 show the impact of replacing (4) by, respectively, (5) and (6) for measuring the similarity. The model based on sim2 combines elements of the direct and condensed relation chain comparisons. Accordingly, we can see that its performance is typically in between that of these two methods. However, for BATS, Google and SCAN it actually outperforms both. The comparatively poor results for sim3 show that the order of the relation embeddings matters.\nNext, the table compares a number of variations in how the relation embedding chains themselves are constructed. As we discussed in Section 4, our main method combines ConceptNet with two augmentation strategies: semantic smoothing (ss) and missing link prediction (mlp). The rows labelled CN + ss and CN + mlp show the impact of only using one of these augmentation strategies. Note that our default configuration for Direct is CN + mlp + ss. Furthermore, the row labelled CN only shows the results when we only use ConceptNet paths, without applying either of the two augmentation strategies. Finally, the row labelled mlp only shows the results of a variant which only relies on the links predicted by the missing link prediction strategy, without using ConceptNet at all. As can be seen, the links from ConceptNet and those predicted using mlp lead to the best performance. In fact, the CN + mlp variant achieves the best results in several cases. Interestingly, mlp on its own already performs quite well, which shows that relation chains can be constructed even without the help of an external KG.\nFinally, we also evaluate a variant which does not rely on relation embeddings (CN types). In this case, we only use ConceptNet paths for finding intermediate concepts. We then compute the compatibility comp(a, b, x, y) as follows:\nu\u2211 i=1 v max j=1 1[raci = rxzj \u2227 rcib = rzjy]\nHere rxy is the ConceptNet relation type of the edge that connects x and y, and 1[\u03b1] is 1 if the condition \u03b1 is satisfied and 0 otherwise. As can be seen, this variant performs poorly. The contrast\nbetween this method and CN only reflects the impact of (i) the noisy nature of ConceptNet and (ii) the fact that the fixed relation types are often less informative than relation embeddings.\nCondensed Relation Chain Comparison Table 4 compares our default approach based on condensed relation embedding chains (Condensed) with some variants. Note that our default configuration for Condensed is CN + mlp + ss. Specifically, as in Table 3, we analyse the impact of the semantic smoothing (ss) and missing link prediction (mlp) strategies. We can broadly observe the same patterns; e.g. we again find that CN + mlp achieves the best results in several cases and that it is possible to obtain competitive results without using a KG.\nQualitative Analysis Table 7 shows some examples of informativeness scores for word pairs from ConceptNet. As these examples illustrate, concept pairs with high scores consistently have a clear relationship. Those with the lowest scores either do not have any obvious relationship, or they have a relationship which is not captured by their RelBERT embedding. As an example of this latter case, the table contains pairs that are linked by the \u201csounds like\u201d (chad : chat) and \u201crhymes with\u201d relation (time : fine).\nTable 5 shows examples where RelBERT made an incorrect prediction while Direct picked the right answer. In each case, we also show the most influential intermediate concept for the query and answer pairs. Specifically, we find the intermediate concept c for the query (a, b) and the intermediate concept z for the answer (x, y) for which sim1(rac; rcb, rxz; rzy) is maximal. These examples illustrate some of the different roles that intermediate concepts can play. For instance, in the example from SAT, the intermediate worth condemnable makes it possible to characterise the pair reprehensible:condemn in terms of a near-synonym (reprehensible:condemnable) and a kind of morphological variation (condemnable:condemn). The example from U4 illustrates a case where the close similarity between two terms (vernacular:regional) is more easily recognised by RelBERT as a composition of two antonyms (vernacular:national and national:regional). The example from SCAN illustrates a case where the word pairs involved are only indirectly related.\nThe top half of Table 6 shows examples of questions that were predicted to be easy. As can be seen,\nthe word pairs involved have a clear and direct relationship. The bottom half of the table similarly shows examples of questions that were predicted to be hard. These examples are hard for different reasons. Some examples are challenging because they involve a clear but indirect relationship (e.g. shopping:bank card). Others involve rather abstract relations (e.g. aloof:connected). The example from Google reflects the fact that RelBERT was not trained on named entities."
        },
        {
            "heading": "8 Conclusions",
            "text": "Relations between concepts are typically modelled either as KG paths or as relation embeddings. In this paper, we have proposed a hybrid approach, which uses paths whose edges correspond the relation embeddings. This is useful to model relationships between concepts that are only indirectly related, and more generally for concept pairs whose relation embedding is unreliable. Our approach crucially relies on a classifier which can predict the informativeness of a RelBERT embedding. This classifier is used (i) to identify analogy questions where we should not rely on RelBERT embeddings; (ii) to select reliable RelBERT vectors for training a model for condensing relation embedding chains; and (iii) to identify informative paths beyond those in ConceptNet. We have relied on GPT-4 to generate training data for the informativeness classifier, which allowed us to address the lack of suitable existing datasets.\nLimitations\nOur evaluation has focused on solving analogy questions, which are useful because they allow for a direct evaluation of different approaches for modelling relationships. While analogies play an important role in certain downstream tasks, we essentially regard this as in intrinsic evaluation task. It thus remains a matter for future work to analyse the usefulness of relation embedding chains in downstream tasks (e.g. retrieving relevant cases for case-based reasoning methods). From a practical point of view, using relation embedding chains is more involved than directly using RelBERT embeddings. To address this, the augmentation strategies can be applied offline, by creating a graph of related concepts. While the corresponding RelBERT embeddings can also be computed offline, storing millions of relation embeddings takes up a large amount of space. In certain applications, it may\nthus be preferable to compute the required RelBERT embeddings only when they are needed.\nAcknowledgments This work was supported by EPSRC grants EP/V025961/1 and EP/W003309/1."
        },
        {
            "heading": "A Generating Examples of Related Concept Pairs with GPT-4",
            "text": "Two concepts can be related in various ways, including, but not limited to, the following:\n\u2022 Semantic relationship: This includes synonyms, antonyms, hyponyms, hypernyms, meronyms, holonyms, etc.\n\u2022 Function or purpose, e.g., key and lock.\n\u2022 Manner, way, or style: Concepts that describe the way in which another concept is accomplished, e.g., limp and walk.\n\u2022 Symbol or representation: Concepts where one represents or symbolizes the other, e.g., dove and peace.\n\u2022 Degrees of intensity: Concepts that describe different degrees of intensity of a particular situation, e.g., shower and monsoon.\n\u2022 Cause and effect: Concepts where one causes the other or results from the other, e.g., fire and smoke.\n\u2022 Sequence or hierarchy: Concepts that follow each other in a sequence or are organized hierarchically, e.g., manager and employee.\nIn order to identify a large list of such relation types, we asked GPT-4: ''' Let A and B be two concepts in natural\nlanguage. When can we say that A is related to B, and when is A not related to B?\n'''\nWe identified approximately 100 relationships through this approach, and then again used GPT-4 to generate examples for each of these relationships, using prompts of the following form: ''' A way two concepts can be related is\nTools and materials: Concepts where one is a tool or material used to create or interact with the other, e.g., 'paintbrush' and 'paint.'\nNow, generate 100 high-quality examples of such concept pairs. '''\nWe prompted the model once for each relation type to maintain a balanced number of examples for each relationship. The generated examples were utilized to train our informativeness classifier."
        },
        {
            "heading": "B Additional Training Details",
            "text": "To train the composition model, we used 200,000 concept pairs from ConceptNet (version 5.6.0) with\ninformativeness scores greater than 0.75. The concepts belonged to the English language only. We did not consider ConceptNet relations \"/r/NotCapableOf\", \"/r/NotDesires\", and \"/r/NotHasProperty\" for determining intermediate concepts. Our default augmentation strategy, i.e., CN+MLP+ss, identified approximately 50 intermediate concepts for each pair of concepts on average. We used Numberbatch version 19.08 and Glove model glovewiki-gigaword-300 in our augmentation strategy. The concept pairs for which the strategy could not determine any path were not considered for training. 10% of the selected concept pairs were utilized for validation. We trained a function \u03d5 : R1024 \u00d7 R1024 \u2192 Rn, where the validation set was used to choose the optimal dimension n of the latent space. We observed in particular that high-dimensional latent spaces outperformed lower-dimensional ones, where we found n = 81, 920 to be optimal. The Adam optimizer was used with a learning rate of 0.0025, and the model was trained for 10 epochs. It is important to note that we did not use validation splits from the analogy datasets to tune these hyperparameters, as we consider the unsupervised setting."
        },
        {
            "heading": "C Details about the Analogy Question Datasets",
            "text": "Some basic statistics about the considered datasets are shown in Table 8. Table 9 shows how many questions there are in each difficulty range, for each\nof the datasets, where the difficulty is measured in terms of the confidence score (3) as before."
        },
        {
            "heading": "D Solving Analogy Questions using GPT-4",
            "text": "To solve analogy questions with GPT-4, we used the following prompt, which was inspired by (LearningExpress, 2002). ''' Many standardized tests, including high school\nentrance exams, the SATs, civil service exams, the GREs, and others, use analogy questions to test both logic and reasoning skills and word knowledge. These questions ask test takers to identify relationships between pairs of words."
        },
        {
            "heading": "In order to solve analogy questions, you must first have a clear understanding of the",
            "text": "words' definitions and then use that understanding to determine how the words are related. The key to solving an analogy question is to precisely describe the relationship between the pair of words and then apply the same relationship to determine which word completes the analogy. Most analogy questions rely on your ability to deduce the correct relationship between words and to draw logical conclusions about the possible answer choices.\nThe relationships that are found in analogy questions fall into several general types."
        },
        {
            "heading": "1) Part to Whole. In this type of question, a",
            "text": "pair of words consists of a part and a whole. For example, spoke : wheel. A spoke is part of a wheel."
        },
        {
            "heading": "2) Type and Category. These questions use pairs",
            "text": "of words in which one word is a specific type in a general category. For example, orange : citrus. An orange is a type of citrus."
        },
        {
            "heading": "3) Degree of Intensity. These questions test",
            "text": "your ability to discern nuance of meaning among pairs of words. For example, shower : monsoon. A shower is light rainfall and a monsoon is heavy rainfall."
        },
        {
            "heading": "4) Function. These questions pair words that",
            "text": "are related through function. For example, hammer : build. A hammer is used to build."
        },
        {
            "heading": "5) Manner. This type of analogy describes the",
            "text": "manner, way, or style by which an action is accomplished. For example, shamble : walk. Shamble means to walk in an awkward manner."
        },
        {
            "heading": "6) Symbol or representation. These questions",
            "text": "pair words in which one word is the symbol of the other. For example, dove : peace. A dove is a symbol of peace."
        },
        {
            "heading": "7) Action and significance. In this type of",
            "text": "analogy one word describes an action and\nthe other word indicates the significance of the action. For example, cry : sorrow. To cry signifies sorrow\nAnalogy questions can also be used to test word knowledge and factual content. Word knowledge questions are generally pairs of synonyms or pairs of antonyms. Factual content questions demand a certain level of general knowledge, and cannot be deduced from the relationship alone.\nGiven the word pair, your aim is to choose the word pair from choices that is analogously most similar. Also, give an explanation. The explanation should be precise. I will show some examples then you will have to do it yourself.\nQuery = ['banana', 'peel']; Choices = [['egg', 'crack'], ['carrot', 'uproot'], ['apple', 'core'], ['bread', 'slice'], ['corn', 'husk']]\nAnswer: choice number 4; Explanation: A banana has a peel that can be removed, and corn has a husk that can be removed.\nQuery = ['birds', 'wings']; Choices = [['moose', 'antlers'], ['camel', 'hump'], ['spider', 'legs'], ['alligator', 'tail'], ['cat', 'whiskers']]\nAnswer: choice number 2; Explanation: Birds have wings, and spiders have legs.\nQuery = ['berate', 'criticize']; Choices = [['goad', 'urge'], ['accuse', 'apologize'], ['regret', 'remember'], ['betray', 'follow'], ['evaluate', 'praise']]\nAnswer: choice number 0; Explanation: To berate is to criticize, and to goad is to urge.\nNow, answer the following questions: '''"
        },
        {
            "heading": "E Additional Results",
            "text": "Table 1 reports the macro-averaged accuracy, as a summary of the performance of the different methods across all datasets. In this macro average, each dataset carries the same weight. To complement this result, Table 10 instead shows the microaveraged result, where each analogy question car-\nries the same weight, regardless of the dataset it appears in. The main conclusions remain the same as the ones we could draw based on the macroaverage. One difference is that the spread in performance between the easiest and the hardest questions is even wider for the micro-averaged results.\nFor comparison, in Table 11 we also show the micro-averaged result, but in this case, we break down the results based on an informativeness classifier that was trained on examples from ConceptNet as positive examples. In contrast, for our main experiments, this informativeness classifier was trained on examples we obtained from GPT-4. As can be seen in Table 11, the informativeness classifier based on ConceptNet is far less successful in identifying easy and hard questions. For instance, GPT-4 actually achieves the best results on the questions that are predicted to be hardest."
        }
    ],
    "title": "Solving Hard Analogy Questions with Relation Embedding Chains",
    "year": 2023
}