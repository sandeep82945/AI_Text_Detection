{
    "abstractText": "Multi-head self-attention-based Transformers have shown promise in different learning tasks. Albeit these models exhibit significant improvement in understanding short-term and longterm contexts from sequences, encoders of Transformers and their variants fail to preserve layer-wise contextual information. Transformers usually project tokens onto sparse manifolds and fail to preserve mathematical equivalence among the token representations. In this work, we propose TransJect, an encoder model that guarantees a theoretical bound for layer-wise distance preservation between a pair of tokens. We propose a simple alternative to dot-product attention to ensure Lipschitz continuity. This allows TransJect to learn injective mappings to transform token representations to different manifolds with similar topology and preserve Euclidean distance between every pair of tokens in subsequent layers. Evaluations across multiple benchmark shortand long-sequence classification tasks show maximum improvements of 6.8% and 5.9%, respectively, over the variants of Transformers. Additionally, TransJect displays 79% better performance than Transformer on the language modeling task. We further highlight the shortcomings of multi-head self-attention from the statistical physics viewpoint. Although multihead self-attention was incepted to learn different abstraction levels within the networks, our empirical analyses suggest that different attention heads learn randomly and unorderly. In contrast, TransJect adapts a mixture of experts for regularization; these experts are more orderly and balanced and learn different sparse representations from the input sequences. TransJect exhibits very low entropy and can be efficiently scaled to larger depths.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ayan Sengupta"
        },
        {
            "affiliations": [],
            "name": "Md. Shad Akhtar"
        },
        {
            "affiliations": [],
            "name": "Tanmoy Chakraborty"
        }
    ],
    "id": "SP:1a9418b323574002b5da75267af9162001d1b1f6",
    "references": [
        {
            "authors": [
                "Mohsin S Ahmed",
                "James B Priestley",
                "Angel Castro",
                "Fabio Stefanini",
                "Ana Sofia Solis Canales",
                "Elizabeth M Balough",
                "Erin Lavoie",
                "Luca Mazzucato",
                "Stefano Fusi",
                "Attila Losonczy"
            ],
            "title": "Hippocampal network reorganization underlies the formation",
            "year": 2020
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Yingyu Liang",
                "Tengyu Ma."
            ],
            "title": "Why are deep nets reversible: A simple theory, with implications for training",
            "venue": "arXiv preprint arXiv:1511.05653.",
            "year": 2015
        },
        {
            "authors": [
                "Cenk Baykal",
                "Nishanth Dikkala",
                "Rina Panigrahy",
                "Cyrus Rashtchian",
                "Xin Wang."
            ],
            "title": "A theoretical view on sparsely activated networks",
            "venue": "arXiv preprint arXiv:2208.04461.",
            "year": 2022
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E. Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The Long-Document Transformer",
            "venue": "Number: arXiv:2004.05150 arXiv:2004.05150 [cs].",
            "year": 2020
        },
        {
            "authors": [
                "Graham Brightwell",
                "Claire Kenyon",
                "H\u00e9l\u00e8ne Paugam-Moisy"
            ],
            "title": "Multilayer neural networks: one or two hidden layers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 1996
        },
        {
            "authors": [
                "Bo Chang",
                "Lili Meng",
                "Eldad Haber",
                "Lars Ruthotto",
                "David Begert",
                "Elliot Holtham."
            ],
            "title": "Reversible architectures for arbitrarily deep residual neural networks",
            "venue": "Proceedings of the AAAI conference on artificial intelligence, volume 32.",
            "year": 2018
        },
        {
            "authors": [
                "Yifan Chen",
                "Qi Zeng",
                "Heng Ji",
                "Yun Yang."
            ],
            "title": "Skyformer: Remodel self-attention with gaussian kernel and nystrom method",
            "venue": "Advances in Neural Information Processing Systems, 34:2122\u20132135.",
            "year": 2021
        },
        {
            "authors": [
                "Djork-Arn\u00e9 Clevert",
                "Thomas Unterthiner",
                "Sepp Hochreiter."
            ],
            "title": "Fast and accurate deep network learning by exponential linear units (elus)",
            "venue": "arXiv preprint arXiv:1511.07289.",
            "year": 2015
        },
        {
            "authors": [
                "Ursula Dicke",
                "Gerhard Roth."
            ],
            "title": "Neuronal factors determining high intelligence",
            "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences, 371(1685):20150180.",
            "year": 2016
        },
        {
            "authors": [
                "Mor Geva",
                "Roei Schuster",
                "Jonathan Berant",
                "Omer Levy."
            ],
            "title": "Transformer feed-forward layers are key-value memories",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484\u20135495.",
            "year": 2021
        },
        {
            "authors": [
                "Aidan N Gomez",
                "Mengye Ren",
                "Raquel Urtasun",
                "Roger B Grosse."
            ],
            "title": "The reversible residual network: Backpropagation without storing activations",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "Gaussian error linear units (gelus)",
            "venue": "arXiv preprint arXiv:1606.08415",
            "year": 2016
        },
        {
            "authors": [
                "Sebastian Jaszczur",
                "Aakanksha Chowdhery",
                "Afroz Mohiuddin",
                "Lukasz Kaiser",
                "Wojciech Gajewski",
                "Henryk Michalewski",
                "Jonni Kanerva."
            ],
            "title": "Sparse is enough in scaling transformers",
            "venue": "Advances in Neural Information Processing Systems, 34:9895\u20139907.",
            "year": 2021
        },
        {
            "authors": [
                "Angelos Katharopoulos",
                "Apoorv Vyas",
                "Nikolaos Pappas",
                "Fran\u00e7ois Fleuret."
            ],
            "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
            "venue": "Number: arXiv:2006.16236 arXiv:2006.16236 [cs, stat].",
            "year": 2020
        },
        {
            "authors": [
                "Soheil Keshmiri."
            ],
            "title": "Entropy and the brain: An overview",
            "venue": "Entropy, 22(9):917.",
            "year": 2020
        },
        {
            "authors": [
                "Hyunjik Kim",
                "George Papamakarios",
                "Andriy Mnih."
            ],
            "title": "The lipschitz constant of self-attention",
            "venue": "International Conference on Machine Learning, pages 5562\u20135571. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Nikita Kitaev",
                "\u0141ukasz Kaiser",
                "Anselm Levskaya."
            ],
            "title": "Reformer: The Efficient Transformer",
            "venue": "Number: arXiv:2001.04451 arXiv:2001.04451 [cs, stat].",
            "year": 2020
        },
        {
            "authors": [
                "Christof Koch",
                "Klaus Hepp."
            ],
            "title": "Quantum mechanics in the brain",
            "venue": "Nature, 440(7084):611\u2013611.",
            "year": 2006
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoff Hinton."
            ],
            "title": "Convolutional deep belief networks on cifar-10",
            "venue": "Unpublished manuscript, 40(7):1\u20139.",
            "year": 2010
        },
        {
            "authors": [
                "Alexandre Lacoste",
                "Alexandra Luccioni",
                "Victor Schmidt",
                "Thomas Dandres."
            ],
            "title": "Quantifying the carbon emissions of machine learning",
            "venue": "arXiv preprint arXiv:1910.09700.",
            "year": 2019
        },
        {
            "authors": [
                "Mario Lezcano Casado."
            ],
            "title": "Trivializations for gradient-based optimization on manifolds",
            "venue": "Advances in Neural Information Processing Systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Zonglin Li",
                "Chong You",
                "Srinadh Bhojanapalli",
                "Daliang Li",
                "Ankit Singh Rawat",
                "Sashank J Reddi",
                "Ke Ye",
                "Felix Chern",
                "Felix Yu",
                "Ruiqi Guo"
            ],
            "title": "On emergence of activation sparsity in trained transformers",
            "venue": "In International Conference on Learning Representa-",
            "year": 2023
        },
        {
            "authors": [
                "Drew Linsley",
                "Junkyung Kim",
                "Vijay Veerabadran",
                "Charles Windolf",
                "Thomas Serre."
            ],
            "title": "Learning long-range spatial dependencies with horizontal gated recurrent units",
            "venue": "Advances in neural information processing systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Andrew Maas",
                "Raymond E Daly",
                "Peter T Pham",
                "Dan Huang",
                "Andrew Y Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th annual meeting of the association for computational linguistics: Human language",
            "year": 2011
        },
        {
            "authors": [
                "Karttikeya Mangalam",
                "Haoqi Fan",
                "Yanghao Li",
                "ChaoYuan Wu",
                "Bo Xiong",
                "Christoph Feichtenhofer",
                "Jitendra Malik."
            ],
            "title": "Reversible vision transformers",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages",
            "year": 2022
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Martin Karafi\u00e1t",
                "Lukas Burget",
                "Jan Cernock\u1ef3",
                "Sanjeev Khudanpur."
            ],
            "title": "Recurrent neural network based language model",
            "venue": "Interspeech, volume 2, pages 1045\u20131048. Makuhari.",
            "year": 2010
        },
        {
            "authors": [
                "Nikita Nangia",
                "Samuel R Bowman."
            ],
            "title": "Listops: A diagnostic dataset for latent tree learning",
            "venue": "arXiv preprint arXiv:1804.06028.",
            "year": 2018
        },
        {
            "authors": [
                "Hao Peng",
                "Nikolaos Pappas",
                "Dani Yogatama",
                "Roy Schwartz",
                "Noah Smith",
                "Lingpeng Kong."
            ],
            "title": "Random feature attention",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Samuel S. Schoenholz",
                "Surya Ganguli."
            ],
            "title": "Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice",
            "venue": "Number: arXiv:1711.04735 arXiv:1711.04735 [cs, stat].",
            "year": 2017
        },
        {
            "authors": [
                "Cindy Poo",
                "Jeffry S Isaacson."
            ],
            "title": "Odor representations in olfactory cortex:\u201csparse\u201d coding, global inhibition, and oscillations",
            "venue": "Neuron, 62(6):850\u2013861.",
            "year": 2009
        },
        {
            "authors": [
                "Ben Poole",
                "Subhaneil Lahiri",
                "Maithra Raghu",
                "Jascha Sohl-Dickstein",
                "Surya Ganguli."
            ],
            "title": "Exponential expressivity in deep neural networks through transient chaos",
            "venue": "Advances in neural information processing systems, 29.",
            "year": 2016
        },
        {
            "authors": [
                "Ofir Press",
                "Noah A. Smith",
                "Mike Lewis."
            ],
            "title": "Shortformer: Better Language Modeling using Shorter Inputs",
            "venue": "Number: arXiv:2012.15832 arXiv:2012.15832 [cs].",
            "year": 2021
        },
        {
            "authors": [
                "Haozhi Qi",
                "Chong You",
                "Xiaolong Wang",
                "Yi Ma",
                "Jitendra Malik."
            ],
            "title": "Deep isometric learning for visual recognition",
            "venue": "International Conference on Machine Learning, pages 7824\u20137835. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Dragomir R Radev",
                "Pradeep Muthukrishnan",
                "Vahed Qazvinian",
                "Amjad Abu-Jbara."
            ],
            "title": "The acl anthology network corpus",
            "venue": "Language Resources and Evaluation, 47:919\u2013944.",
            "year": 2013
        },
        {
            "authors": [
                "Glenn N Saxe",
                "Daniel Calderone",
                "Leah J Morales."
            ],
            "title": "Brain entropy and human intelligence: A resting-state fmri study",
            "venue": "PloS one, 13(2):e0191582.",
            "year": 2018
        },
        {
            "authors": [
                "Noam Shazeer",
                "Azalia Mirhoseini",
                "Krzysztof Maziarz",
                "Andy Davis",
                "Quoc Le",
                "Geoffrey Hinton",
                "Jeff Dean."
            ],
            "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
            "venue": "arXiv preprint arXiv:1701.06538.",
            "year": 2017
        },
        {
            "authors": [
                "Yi Tay",
                "Dara Bahri",
                "Donald Metzler",
                "Da-Cheng Juan",
                "Zhe Zhao",
                "Che Zheng."
            ],
            "title": "Synthesizer: Rethinking self-attention for transformer models",
            "venue": "International conference on machine learning, pages 10183\u201310192. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "Yi Tay",
                "Dara Bahri",
                "Liu Yang",
                "Donald Metzler",
                "Da-Cheng Juan."
            ],
            "title": "Sparse sinkhorn attention",
            "venue": "International Conference on Machine Learning, pages 9438\u20139447. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Samira Abnar",
                "Yikang Shen",
                "Dara Bahri",
                "Philip Pham",
                "Jinfeng Rao",
                "Liu Yang",
                "Sebastian Ruder",
                "Donald Metzler."
            ],
            "title": "Long range arena: A benchmark for efficient transformers",
            "venue": "arXiv preprint arXiv:2011.04006.",
            "year": 2020
        },
        {
            "authors": [
                "Joshua B Tenenbaum",
                "Vin de Silva",
                "John C Langford."
            ],
            "title": "A global geometric framework for nonlinear dimensionality reduction",
            "venue": "science, 290(5500):2319\u20132323.",
            "year": 2000
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention Is All You Need",
            "venue": "Number: arXiv:1706.03762 arXiv:1706.03762 [cs].",
            "year": 2017
        },
        {
            "authors": [
                "Petra E V\u00e9rtes",
                "Aaron F Alexander-Bloch",
                "Nitin Gogtay",
                "Jay N Giedd",
                "Judith L Rapoport",
                "Edward T Bullmore."
            ],
            "title": "Simple models of human brain functional networks",
            "venue": "Proceedings of the National Academy of Sciences, 109(15):5868\u20135873.",
            "year": 2012
        },
        {
            "authors": [
                "Elena Voita",
                "Rico Sennrich",
                "Ivan Titov."
            ],
            "title": "The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives",
            "venue": "arXiv preprint arXiv:1909.01380.",
            "year": 2019
        },
        {
            "authors": [
                "James Vuckovic",
                "Aristide Baratin",
                "Remi Tachet des Combes."
            ],
            "title": "A mathematical theory of attention",
            "venue": "arXiv preprint arXiv:2007.02876.",
            "year": 2020
        },
        {
            "authors": [
                "Sinong Wang",
                "Belinda Z Li",
                "Madian Khabsa",
                "Han Fang",
                "Hao Ma."
            ],
            "title": "Linformer: Self-attention with linear complexity",
            "venue": "arXiv preprint arXiv:2006.04768.",
            "year": 2020
        },
        {
            "authors": [
                "John Wu",
                "Yonatan Belinkov",
                "Hassan Sajjad",
                "Nadir Durrani",
                "Fahim Dalvi",
                "James Glass."
            ],
            "title": "Similarity analysis of contextual word representation models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4638\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Yunyang Xiong",
                "Zhanpeng Zeng",
                "Rudrasis Chakraborty",
                "Mingxing Tan",
                "Glenn Fung",
                "Yin Li",
                "Vikas Singh."
            ],
            "title": "Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Haoyi Zhou",
                "Shanghang Zhang",
                "Jieqi Peng",
                "Shuai Zhang",
                "Jianxin Li",
                "Hui Xiong",
                "Wancai Zhang"
            ],
            "title": "Informer: Beyond efficient transformer",
            "year": 2021
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "2021) for the LRA benchmark. All these experiments are run for 50k steps. We use Adam optimizer with a learning rate of 0.01 for the language modeling task and trained the models for 10 epochs",
            "venue": "One Tesla",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Over the past few decades, Deep Neural Networks have greatly improved the performance of various\ndownstream applications. Stacking multiple layers has been proven effective in extracting features at different levels of abstraction, thereby learning more complex patterns (Brightwell et al., 1996; Poole et al., 2016). Since then, tremendous efforts have been made in building larger depth models and in making them faster (Bachlechner et al., 2020; Xiao et al.). Self-attention-based Transformer model (Vaswani et al., 2017) was proposed to parallelize the computation of longer sequences; it has achieved state-of-the-art performance in various sequence modeling tasks. Following this, numerous efforts have been made to reduce computation and make the Transformer model suitable even for longer sequences (Katharopoulos et al., 2020; Peng et al., 2020; Kitaev et al., 2020; Beltagy et al., 2020; Press et al., 2021; Choromanski et al., 2021; Tay et al., 2021). However, very few of these studies discuss information propagation in large-depth models. A recent study (Voita et al., 2019) characterized how Transformer token representations change across layers for different training objectives. The dynamics of layer-wise learning are essential to understand different abstract levels a model could learn, preserve and forget to continue learning throughout the layers. However, recent\nstudies need to shed more light on how Transformers preserve contextual information across different layers (Wu et al., 2020; Voita et al., 2019). To understand how Transformer encodes contextual similarities between tokens and preserves the similarities layer-wise and across different attention heads, we highlight an example in Figure 1. We select three pairs of semantically-related tokens. We observe the Euclidean and cosine distances of the representations learned at different layers of the Transformer trained on the IMDb sentiment classification task. Although the trained Transformer model preserves the semantic similarity among the same tokens across layers, the Euclidean distance between their representations increases in the upper layers. This indicates that Transformer projects the representations to different and sparse subspaces (a subspace with low density), albeit preserving the angle between them. Additionally, we observe that the distances between different token representations vary across different attention heads at different encoder layers in a haphazard manner. Preserving distance and semantic similarity across layers is vital to ensure the continual learning capabilities of deep neural models, which Transformer evidently fails to do.\nNeuroscientists have been working for years to understand how the human brain functions and simulates the behaviours of physical sciences (Koch and Hepp, 2006; V\u00e9rtes et al., 2012). Arguably, the human brain is more capable of \u2018associative learning\u2019 and \u2018behavioural formation\u2019, which can be attributed to the number of neurons and their inter-connectedness (synapses) rather than the size of the brain, or the number of layers through which the information propagates (Dicke and Roth, 2016). Although a fully-grown human brain can have hundreds of billions of neurons, it has been found that at a time, only a tiny fraction of neurons fire (Ahmed et al., 2020; Poo and Isaacson, 2009). This sparse firing can help the brain sustain low entropy (energy). Entropy, a measure that quantifies a state\u2019s randomness, has thus become an essential tool to understand the factors behind human intelligence (Saxe et al., 2018; Keshmiri, 2020) and how the human brain operates.\nUnfortunately, Transformer and its variants are yet to be studied from these interdisciplinary viewpoints. Our study explores the connection between model sparsity and entropy. Towards this, we propose a complete redesign of self-attention-\nbased Transformer with enforced injectivity, aka TransJect. With injectivity, our model imposes the constraint in which the representations of two distinct tokens always differ across all the layers. Unlike Transformer, which only injects regularization through multi-head self-attention and dropout, TransJect does not require explicit regularizers and can be regularized implicitly due to its inherent injectivity. The backbone of TransJect is a non-normalized linear orthogonal attention and an injective residual connection; both ensure Lipschitz continuity. By enforcing Lipschitz continuity in Euclidean and dot-product space, the model can preserve the manifold structure between tokens and perform well on the final predictive task.\nTo validate our hypotheses and empirically justify the superiority of our model, we use two short and five long sequence classification tasks. TransJect outperforms Transformer and other benchmark variants with an average margin of 3.4% and 2.2% on the short- and long-sequence classification tasks, respectively. Our model performs best on the long sequence classification (LRA) benchmark, achieving 0.2% better accuracy than the best baseline, Skyformer (Chen et al., 2021). We further demonstrate TransJect on language modeling task on the Penn TreeBank dataset, in which our model achieves 79% better test perplexity than the vanilla Transformer. Empirical analyses suggest a very low entropy of TransJect representations, indicating that TransJect captures sparse and more orderly representations compared to Transformer. Moreover, TransJect shows 13\u00d7 lower inference runtime than Transformer, indicating its efficiency in encoding long input sequences.1"
        },
        {
            "heading": "2 Related Works",
            "text": "Despite being a ubiquitous topic of study across different disciplines of deep learning, Transformer (Vaswani et al., 2017) models still require better mathematical formalization. On a recent development, Vuckovic et al. (2020) formalized the inner workings of self-attention maps through the lens of measure theory and established the Lipschitz continuity of self-attention under suitable assumptions. However, the Lipschitz condition depends on the boundedness of the representation space and the Lipschitz bound of the\n1The source codes of TransJect can be found at https: //github.com/victor7246/TransJect.git.\nfully-connected feed-forward layer (FFN). A similar study (Kim et al., 2021) also concluded that the dot-product self-attention is neither Lipschitz nor injective under the standard conditions. Injectivity of the transformation map is essential to ensure that the function is bijective and, therefore, reversible. Reversibility within deep neural networks has always been an active area of study (Gomez et al., 2017; Arora et al., 2015; Chang et al., 2018). A reversible network ensures better scaling in large depths and is more efficient than non-reversible structures. Recently, Mangalam et al. (2022) designed a reversible Transformer and empirically highlighted its effectiveness in several image and video classification tasks. However, any similar development has yet to be made for developing scalable reversible sequential models.\nDynamical isometry is a property that mandates the singular values of the input-output Jacobian matrix to be closer to one. Pennington et al. (2017) showed that dynamical isometry could aid faster convergence and efficient learning. Following their idea, Bachlechner et al. (2020) showed that residual connections in Transformers do not often satisfy dynamical isometry, leading to poor signal propagation through the models. To overcome this, they proposed residual with zero initialization (ReZero) and claimed dynamical isometry for developing faster and more efficient large-depth Transformers. Previously, Qi et al. (2020) enforced a stronger condition of isometry to develop deep convolution networks efficiently. However, isometric assumptions are not valid for sequential modeling due to different levels of abstraction within the input signals. Moreover, isometry may not hold between contextually-dissimilar tokens.\nAnother essential aspect behind designing efficient large-depth models is ensuring model sparsity. Several notable contributions have been made (Baykal et al., 2022; Jaszczur et al., 2021; Li et al., 2023; Tay et al., 2020a) to enforce sparse activations within Transformers to make them more efficient and scalable. Li et al. (2023) argued that sparse networks often resemble the sparse activations by the human brain, bearing a similarity between artificial and biological networks. They empirically showed that the trained Transformers are inherently sparse, and the sparsity emerges from all the layers. As discussed in the previous section, Transformers project the representations sparsely onto sparse subspaces. Although sparse models are\ninherently regularized and display lower entropy, projecting the representations onto a sparse subspace pushes them further from being Lipschitz, preventing them from reversible.\nThis work proposes an injective and Lipschitz continuous alternative to vanilla Transformer, namely, TransJect. With the enforced injectivity, our model establishes a theoretical guarantee for reversibility and thus can be scaled to larger depths. Further, TransJect displays significantly lower entropy than Transformer, indicating a lower energy footprint and more efficiency."
        },
        {
            "heading": "3 Designing Injective Transformer",
            "text": "This section formally describes our proposed model, TransJect. It inherits the structure from the vanilla Transformer and achieves a smoother activation plane by utilizing injective maps for transforming token representations across layers. For an L-layered stacked encoder, we aim to learn the representation of a sequence X = {x1,x2, \u00b7 \u00b7 \u00b7 ,xN} at each layer l that preserves the pairwise distance between every pair of words within a theoretical bound. We illustrate the components of TransJect in Figure 2. All the proofs presented in the paper are supplied in Appendix A."
        },
        {
            "heading": "3.1 Background",
            "text": "Activation bound. For any function f : Rn \u2192 Rm, we define the activation bound Kf as supx\u0338=0 ||f(x)||p ||x||p , for a suitable integer p. A linear map M equals the induced matrix norm ||M ||p. Intuitively, this is the maximum scale factor by which a mapping expands a vector x. In Euclidean space, we usually choose p = 2.\nLipschitz Continuity. A function f : Rn \u2192 Rm under ||.||p norm is called Lipschitz continuous if there exists a real number K \u2265 0 such that\n||f(x)\u2212 f(y)||p \u2264 K||x\u2212 y||p. (1)\nfor any x,y \u2208 Rn. Lipschitz continuity can be defined over any metric space, but in this paper, we restrict its definition to only Euclidean space with p = 2. K is called Lipschitz bound."
        },
        {
            "heading": "3.2 Space-Preserving Orthogonal Attention",
            "text": "The backbone of TransJect is the spacepreserving orthogonal attention.\nTheorem 1 (Space-Preserving Orthogonal Attention). Replacing WQ,WK ,W V with real square orthogonal matrices in non-normalized linear selfattention reduces the activation bound to \u03c321(X), where \u03c31(X) is the largest singular value of X . This reduces the attention operation Attn(X) = XU\u03a3V , for learnable orthogonal matrices U and V and the singular values \u03a3.\nNotice that the activation bound of the modified attention mechanism does not depend on any learnable parameters; instead can be bounded by the largest eigenvalue of XTX . Therefore, we assume a stochastic XTX to ensure that the largest eigenvalue is always 1 (see Corollary 2 in Appendix A.1), and the attention operator preserves the pairwise distance between any two tokens. We learn orthogonal projection matrices in each layer, U and V . In contrast, the diagonal matrix containing eigenvalues \u03a3 is learned on the initial embedding obtained from the initial embedding layer defined in Section 3.5, also denoted as l = 0. Therefore, in our proposed non-normalized orthogonal linear attention, we compute the attention matrix (encoded in \u03a3) only once and learn different projections of it in different layers.\nApproximating eigenvalues. Eigenvalue decomposition is computationally expensive with a runtime complexity of O(Bd3), with B being\nthe batch size and d being the hidden dimension. This work uses a simple approximation to compute \u03a3\u0303, the eigenvalues of XTX . Formally, we compute U\u0303 = argminU ||XTX\u2212U\u03a3UT ||, and \u03a3\u0303 = argmin\u03a3 ||XTX \u2212U\u03a3UT ||. To learn the approximate eigenvalues, we can minimize the reconstruction loss ||XTX \u2212U\u03a3UT || for a learnable orthogonal eigenvector U . We use standardization to enforce the stochasticity constraint on \u03a3\u0303. Further, instead of deriving the eigenvalues, we can also initialize a random diagonal matrix \u03a3\u0303, without any approximation that optimizes the only task-specific training objective, without enforcing the reconstruction. We denote this version of the model as Random-TransJect. We compute \u03a3\u0303 once, only on the initial token embeddings."
        },
        {
            "heading": "3.3 Injective Residual (IR)",
            "text": "We fine-tune the hidden representation for every layer l by learning a new attention projection on the hidden state learned in the previous layer. Formally, we define,\nX(l) = X(l\u22121) + \u03b1l L F (X(l\u22121)). (2)\nHere, F is the self-attention operator, followed by a suitable non-linear activation function, and \u03b1i \u2208 (0, 1) is the residual weight. We use a learnable residual weight and a sigmoid activation to\nscale it in (0, 1). In the previous studies, ReLU and GELU (Hendrycks and Gimpel, 2016) have been popular choices for the activation function. In this work, we choose ELU (Clevert et al., 2015), a nonlinear C1 (continuous and differentiable) activation function with a Lipschitz bound of 1. Although ReLU is a Lipschitz function with K = 1, it is not everywhere differentiable and injective. Following Bachlechner et al. (2020), we adopt ReZero (residual with zero initialization) to enforce dynamical isometry and stable convergence.\nLemma 1 (Residual contractions are injective). f : X \u2192 X + \u03b1lL F (X) is injective for L \u2265 3.\nTo maintain the dimensionality, Transformer projects the representations to a lower-dimensional space, which reduces the number of synapses among the neurons by a factor of H , the number of heads. As opposed to this, we devise a Mixture of Expert (MOE) attention (motivated by Shazeer et al. (2017)). With this, we compute X(l,e) for each expert e \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , E} in each layer l using Equation 2, learnable expert weights \u03bb(l)i s, and use a convex combination of them to compute,\nX(l) = E\u2211\ne=1\n\u03bb(l)e X (l,e), s.t. E\u2211 e=1 \u03bb(l)e = 1. (3)\nNote that \u03bb(l)e is computed for each sample, and the same expert weights are used for all tokens within a sample.\nCorollary 1 (Injectivity of MOE). The mapping function defined in Equation 3 is injective."
        },
        {
            "heading": "3.4 Orthogonal Residual FFN (ORF)",
            "text": "We reformulate the position-wise FFNs with orthogonal parameterization. FFN layers in Transformer emulate a key-value memory (Geva et al., 2021). We enforce Lipschitz continuity on the feedforward sublayer to preserve the layer-wise memory. Formally, we define,\nORF (X(l)) =\nX(l)+ \u03b1l L ELU\n( ELU(X(l)W1+b1)W2+b2 ) .\nWith both W1 and W2 being square orthogonal matrices.\nCorollary 2 (Injectivity of ORF). Orthogonal residual FFNs are injective. Proof. Using the Lipschitz continuity of ELU, we can prove the corollary directly using Lemma 1."
        },
        {
            "heading": "3.5 Injective Token Embedding",
            "text": "Transformer introduced conditional encoding to inject tokens\u2019 relative and absolute positional information into the self-attention layer. It leverages sinusoidal representations of every position and adds to the original token embeddings to infuse the positional information. To ensure injectivity at each layer, we need to ensure that the initialization of the token embeddings is also injective, i.e., no two tokens should have the same embedding. Unfortunately, the addition operator is not injective. Therefore, we compute the initial embedding of token xi as Xi(0) = Concat(Emb(xi), PEi,:). Here PE is defined similarly to the positional encoding proposed by Vaswani et al. (2017). The embedding matrix is orthogonally parameterized. Concatenation ensures the injectivity of embeddings. However, to maintain the dimensionality, we learn the initial embedding and positional encoding at a lower dimensional space, R d 2 , in which d is the hidden size in the encoder. We define the final encoder mapping for each sublayer l as a composite mapping defined by,\nSubLayer(l)(X(l\u22121)) =\nORF \u25e6MOE \u25e6 IR(X(l\u22121)). (4)\nTheorem 2. The composite map defined in Equation 4 is an injective Lipschitz with a fixed (dataindependent) upper bound.\nA bounded activation bound ensures that the incremental learning of our encoder model reduces with large depth, which makes our model scalable to larger depths. It further enforces the importance of learning better embeddings in the initial embedding layer, which drives the entire encoding. The runtime complexity of our orthogonal non-normalized attention is O(Nd2), whereas dotproduct self-attention has a runtime complexity of O(N2d + Nd2). In a comparable setting where N >> d, TransJect should have a lower runtime complexity than Transformer."
        },
        {
            "heading": "4 Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1 Tasks and Datasets",
            "text": "We evaluate TransJect and its variants on seven short- and long-sequence classification tasks and one language modeling task. We choose the IMDb movie review sentiment classification (Maas et al., 2011) and the AGnews topic classification (Zhang et al., 2015) datasets for short text classification\n\u2013 the former one is a binary classification task, whereas the latter one contains four classes. To further highlight the effectiveness of TransJect on longer sequences, we evaluate our model on the LRA benchmark (Tay et al., 2020b). LRA benchmark consists of five long sequence classification tasks \u2013 ListOps (Nangia and Bowman, 2018), Byte-level text classification on IMDb review (CharIMDb) dataset (Maas et al., 2011), Byte-level document retrieval on AAN dataset (Radev et al., 2013), Pathfinder (Linsley et al., 2018), and Image classification on the CIFAR-10 dataset (Krizhevsky and Hinton, 2010). For language modeling, we use Penn TreeBank (PTB) (Mikolov et al., 2010) dataset, containing 100M tokens. We provide the details of hyperparameter in Appendix B.1."
        },
        {
            "heading": "4.2 Results",
            "text": "Short sequence classification. Table 1 shows the performance of the competing models. On IMDb classification, TransJect outperforms Transformer with a 6.8% margin. TransJect achieves 3.5% better accuracy than Synthesizer, the best baseline. With zero residual initialization, Transformer can achieve 2.1% better accuracy in the IMDb classification task. We use an additional ablation of Transformer, in which all the learnable weight matrices are parameterized orthogonally. This improves accuracy on IMDb classification by 3.8%. On the AGnews topic classification task, Random-TransJect achieves 90.2% accuracy, 1.1% better than the best baseline. Interestingly, Random-TransJect performs better than TransJect on the AGnews classification task; injecting randomness through randomly initialized eigenvalues aids in 1.4% performance improvement. Limited contextual information can create difficulty reconstructing XTX from the approximate eigenvalues. Therefore, having randomlyinitialized eigenvalues can aid in learning better\ncontext when the context itself is limited. To highlight the effectiveness of the MOE module, we evaluate an ablation of our model by dropping the MOE module (reducing the number of experts to 1). Dropping the MOE module reduces the validation accuracy on the IMDb classification task by 4.7%. On the other hand, using only a single head in the original Transformer model reduces its performance on the IMDb classification task by 2.1%.\nLong sequence classification. We evaluate TransJect against Transformer along with several of its recent variants and report the test accuracy in Table 2. Similar to short sequence classification tasks, TransJect is very effective for long sequences and consistently outperforms all the baselines. Out of five tasks, TransJect achieves the best performance in three and beats the best baseline, Skyformer by 0.2% on the leaderboard. TransJect achieves 2.3% and 0.2% better test accuracies on ListOps and byte-level text classification tasks than the corresponding best baselines, Big Bird and Skyformer, respectively. Interestingly, Random-TransJect achieves the best performance on the Pathfinder task, with a wide margin of 1.9%. The ListOps task evaluates the ability to learn long-range hierarchical dependencies, whereas the Pathfinder task evaluates the ability to learn spatial dependencies. As argued by Chen et al. (2021), learning both these dimensions poses difficulty for self-attention-based methods. With a superior performance on both tasks, TransJect showcases its effectiveness in learning long-term patterns from both temporal sequences and sequences with different hierarchical dimensions. Moreover, Random-TransJect performs better than TransJect on both Pathfinder and Image classification tasks with a margin of 1.1% and 1.3%, respectively. We argue that the sparsity in inputs in these two visual tasks is difficult to be approximated by the eigenvalues, which costs TransJect in these tasks.\nLanguage modeling. We report validation and test perplexity on PTB in Table 3 for TransJect and other baselines. Our model achieves 79% lower test perplexity than the vanilla Transformer. As argued by Bachlechner et al. (2020), loss-free information propagation is required for training large depth models. Due to this, Transformer with ReZero initialization achieves 75% better performance than the vanilla Transformer that uses uni-\nform residual weights. However, it is worth noting that TransJect achieves 14% better performance than ReZero initialization, which could be attributed to the inherent injectivity, that ReZero fails to ensure. On the other hand, TransJect with random eigenvalues performs poorly due to its inability to encode the inter-dependence between tokens."
        },
        {
            "heading": "5 Analysis",
            "text": "To understand the connections behind model depth, activation bounds and entropies, we conduct detailed statistical analyses on our model and Transformer. We use the IMDb and CharIMDb classification tasks for these studies as part of short longrange classification tasks, respectively. We use the outputs inferred by our models on a subsample of the test data for these analyses.\nActivation bounds and entropy. Continuing our initial discussion on preserving layer-wise distances between tokens, we calculate the distribution of activation bounds at different encoding layers. As defined in Section 3.1, we compute the activation factor for each encoding layer for TransJect and Transformer. Formally, for lth layer, we compute the activation factor\nA(l) = EXEi \u0338=j [ ||X (l)i \u2212 X (l)j || ||X (0)i \u2212 X (0) j || ] . (5)\nHere X (l) \u2208 RN\u00d7d is the hidden representation of the sequence X at lth layer.\nSimilarly, we compare the differential entropy (aka entropy) of the hidden representations learned by TransJect at each layer to understand how the entropy state changes across the layers. We calculate differential entropy as,\nentropy(l) ( X (l) ) = Ej,h [ \u2212 logP (X (l)j,h) ] = \u2212Ej [\u222b H P (X (l)j,h) logP (X (l) j,h)dh ] (6)\nwhere, P is the empirical probability density function of X . Note that X (l)j,hi =hi \u0338=hj X (l)j,hj leads the entropy to \u2212\u221e. Therefore, sparser states always have lower entropy and are more deterministic. At the same time, unorderly, random and stochastic states have higher entropy. We highlight the distribution of activation factors and entropy of token embeddings at every layer of TransJect and Transformer in Figure 3a. Under Euclidean and dot-product, TransJect displays an empirical activation bound of \u2248 1. Unlike TransJect, Transformer has much higher empirical activation bounds. Although orthogonal parameterization and ReZero lead to much lower activation bounds, they are still higher than our model. Interestingly, Transformer aims to preserve the semantic similarity at the later layers at the expense of distance; however, TransJect can preserve both of them with a tighter bound, leading to a more robust representation for each token. We hypothesize that restricting the distance between a pair of tokens acts as a regularization, improving the encoder\u2019s final predictive\n1 2 3 4 5 6 1\n2\n3\n4\n5\n6 7 Ac tiv at io n Fa ct\nor IMDb\n1 2 3 4 5 6 1\n2\n3\n4\n5\n6\nCharIMDb\n1 2 3 4 5 6 Layer\n1\n0\n1\n2\n3\n4\n5\n6\nEn tro\npy\n1 2 3 4 5 6 Layer\n1\n2\n3\n4\n5\n6\nTransformer Orthogonal Transformer Transformer+ReZero TransJect\n(a) Distribution of activation and entropy of models on IMDb and CharIMDb classification tasks.\n1 2 3 4 5 6 Transformer Layer\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nEn tro\npy\nIMDb\nHead 1 Head 2 Head 3 Head 4 Head 5 Head 6 Head 7 Head 8\n1 2 3 4 5 6 Transformer Layer\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0 CharIMDb\n1 2 3 4 5 6 TransJect Layer\n0.80\n0.85\n0.90\n0.95\n1.00\n1.05\n1.10\n1.15\n1.20\nEn tro\npy\n1 2 3 4 5 6 TransJect Layer\n0.95\n0.90\n0.85\n0.80\n0.75\nExpert 1 Expert 2 Expert 3 Expert 4\n(b) Attention head/expert level distribution of entropy.\nFigure 3: We observe higher activation factors for Transformer, whereas the median empirical activation factor for TransJect is \u2248 1. Lower entropy for TransJect indicates that at the neuron level as well as at the expert level, the representations are more orderly.\nperformance. We computed the Pearson\u2019s correlation between the activation bound of different encoder models (TransJect, Transformer, Orthogonal Transformer and Transformer+ReZero) and the final predictive performance and obtained a correlation score of \u22120.81 with pvalue 0.09. A similar correlation score of \u22120.68 is obtained on the CharIMDb classification task. These results highlight the negative linear relationship between the final predictive performance and the average activation bound of the encoders. Therefore, we conclude that a tighter activation bound could lead to better predictive performances on short- and long-range encoding tasks.\nThe average entropy obtained by TransJect on IMDb classification is \u22121.3 with a standard deviation of 0.12. On the other hand, Transformer obtains an average entropy of 5.80 with a standard deviation of 0.13. With a higher activation factor, Transformer projects the tokens onto much sparser and random subspaces, increasing the system\u2019s overall entropy. On the other hand, representations learned by TransJect are more orderly and thus have low entropy. Moreover, the entropy of TransJect does not increase perpetually, which according to the second law of thermodynamics, suggests a reversible process. We compute Spearman\u2019s rank and Pearson correlation to understand the relationship between average activation bound and average entropy. We observe a Spearman\u2019s rank correlation value of 1.0 on the IMDb classification task, with a p-value of 0.001. The Pearson\u2019s correlation also stands at 0.69. These analyses indicate the pos-\nitive relationships between the two measures, i.e. having a lower activation bound lowers the overall entropy of the model representations. The same argument can be used to explain the higher entropy in later layers of the Transformer. Our analyses also confirm the positive correlation between model depth, activation and entropy for Transformer(see Figure 6 in Appendix C). It is noteworthy that dotproduct self-attention computes the query-key similarity between different tokens. Contrarily, in our proposed attention mapping, we compute the similarities between neurons i.e., similarities between different projection vectors, which in turn, enforces our model to learn different projection dimensions and reduces the overall entropy of the system. Additionally, we report the entropy of the representations at each attention head and expert level in Figure 3b. Although TransJect displays higher entropy at the expert level, it is still lesser than the corresponding attention heads of the Transformer model. A sparse load-balancing capability of the expert model (see Figure 7 in Appendix C) ensures lower entropy throughout the model training. Further, the changes in inter-quartile ranges in the later layers of the Transformer show increasing randomness in larger depths, which can also be attributed to the higher activation factors. On the other hand, TransJect stabilises the entropy in the later layers.\nPreserving distances between tokens. Figure 4 shows representations obtained on tokens of a sample text at different encoder layers, projected onto 2-D. We use isometric mapping (Tenenbaum et al., 2000) for projecting the high dimensional\nvectors to the 2\u2212D space. TransJect maintains the initial embedding space throughout the layers, showing robustness in learning initial embeddings. On the other hand, Transformer expands the projection subspaces to more sparse subspaces, even though they project semantically similar tokens closer.\nEfficiency comparison. We report the test-time speed on the CharIMDb classification task with different lengths of input sequences in Table 4. Albeit having 50% more parameters (see Table 5 in Appendix C) than vanilla Transformer, on average, we observe 13\u00d7 speedup for TransJect, which increases to 26\u00d7 for longer sequences. From the definitions of thermodynamics, a higher entropy leads an irreversible process. This means that a\nmodel with a high activation bound is more irreversible and, therefore, be less efficient. On the other hand, TransJect exhibits lower entropy, and has higher available energy (from principles of thermodynamics), leading to more efficiency."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we introduced TransJect, a new learning paradigm in language understanding by enforcing a distance-preserving criterion in the multi-layer encoder models. We derived that by enforcing orthogonal parameterization and utilizing smoother activation maps, TransJect can preserve layer-wise information propagation within a theoretical bound, allowing the models to regularize inherently. We further argued in favor of injectivity and Lipschitz continuity for better generalization and efficiency. Our empirical analyses suggested a superior performance of TransJect over other self-attention-based baselines. We observed lower entropy with TransJect with low variance, confirming the reversible process of statistical mechanics. These findings will encourage practitioners to explore the natural laws of science for building better, more intuitive, and cognitive AI."
        },
        {
            "heading": "7 Ethical Considerations",
            "text": "Limitations. As discussed previously, TransJect uses approximated eigenvalues to approximate the gram matrix XTX . Therefore, our proposed model could be ineffective for sequences with limited context. Additionally, our model usually exhibits higher forward pass time than non-parameterized models due to orthogonal parametrization.\nIntended Use and Broader Impact. The empirical analyses and insights gathered in this work encourage us to explore the dependence between sparsity, entropy and model representations. Our study can be further utilized in developing more efficient larger-depth language models.\nUser Privacy. There is no known user privacy concern with this work.\nPotential Misuse. There is no known potential misuse of this work.\nReproducibility. We furnish the experimental details Appendix B.1 for reproducing the results reported in this paper. We have open-sourced the source code at https: //github.com/victor7246/TransJect.git.\nEnvironmental Impact. On the language modeling task, each training iteration takes \u223c 0.6 seconds on a single Tesla T4 GPU. Total CO2 emissions are estimated to be 0.06 kg. Estimations were conducted using the MachineLearning Impact calculator presented in Lacoste et al. (2019)."
        },
        {
            "heading": "A Theoretical Results",
            "text": ""
        },
        {
            "heading": "A.1 Background",
            "text": "We furnish the background materials and proofs of all the theoretical results presented in the main text.\nLemma 2 (Activation bound of linear maps) For a matrix M \u2208 Rn\u00d7m, KM is same as the largest absolute singular value, under ||.||2."
        },
        {
            "heading": "Proof.",
            "text": "KM = sup x \u0338=0 ||Mx||2 ||x||2 = sup ||x||2=1 ||Mx||2.\nSquaring both the side, we decompose M as U\u03a3V T , where U and V are orthogonal matrices, and \u03a3 is the diagonal matrix containing the singular values (square root of eigenvalues of MTM ), \u03a31 \u2265 \u03a32 \u2265 \u03a33 \u00b7 \u00b7 \u00b7 \u2265 0. For an orthogonal matrix U , ||Ux||22 = ||xTUTUx||2 = ||xTx||2 = ||x||22. This leads to\n||Mx||22 = ||U\u03a3V Tx||2 2 = ||\u03a3x||22\n= n\u2211 i=1 \u03a32ix 2 i .\nHence,\nK2M = sup\u2211n i=1 x 2 i=1 n\u2211 i=1 \u03a32ix 2 i .\nBeing a convex sum, K2M \u2264 \u03a321, which completes the proof.\nCorollary 2 (Largest eigenvalue of a square stochastic matrix) The largest absolute value of any eigenvalue of a square stochastic matrix is equal to 1.\nProof. For any square stochastic matrix M \u2208 Rn\u00d7n,\nMI =  \u2211n j=1Mj,1\u2211n j=1Mj,2\n...\u2211n j=1Mj,n  =  1 1 ... 1  = I.\nHence, 1 is an eigenvalue of M . Next, we prove that 1 is the largest eigenvalue of M . For any eigenvalue \u03bb and its corresponding eigenvector v, \u03bbv = Mv. Without loss of generality, we assume argmaxi |vi| = 1. Hence, for the 1st entry in this column vector \u03bbv1 = \u2211n j=1M1,jvj . Using triangle inequality we get, |\u03bb||v1| \u2264 |\u03bbv1| = | n\u2211\nj=1\nM1,jvj | \u2264 n\u2211\nj=1\n|M1,j ||v1|\n\u2264 |v1|.\nHence, for any eigenvalue |\u03bb| \u2264 1. Hence, it proves our corollary that the largest eigenvalue is 1.\nLemma 3 (Lipschitz bound for continuously differentiable functions). Any C1 function f : Rn \u2192 Rm with bounded derivative has Lipschitz bound as supx||\u2207xf ||.\nProof. For any x,y \u2208 Rn, we define g : [0, 1] \u2192 Rm as\ng(t) = f(x+ t(y \u2212 x)). (7)\nIt is easy to verify that g(0) = f(x) and g(1) = f(y).\nf(y)\u2212f(x) = g(1)\u2212g(0) = \u222b 1 0 \u2207tg(t)dt. (8)\nUsing chain rule of differentiation on Equation 7, we get,\n\u2207tg(t) = \u2207tf ( x+ t(y \u2212 x) ) (y \u2212 x).\nUsing this in Equation 8, we get\n||f(y)\u2212 f(x)|| = \u2223\u2223\u2223\u2223\u2223\u2223 \u222b 1\n0 \u2207tf\n( x+ t(y \u2212 x) ) (y \u2212 x)dt \u2223\u2223\u2223\u2223\u2223\u2223 \u2264\n\u222b 1 0 \u2223\u2223\u2223\u2223\u2223\u2223\u2207tf(x+ t(y \u2212 x))(y \u2212 x)dt\u2223\u2223\u2223\u2223\u2223\u2223. As f is C1, the supremum of its derivative exists, allowing us to set supt\n\u2223\u2223\u2223\u2223\u2223\u2223\u2207tf(x+ t(y \u2212 x))\u2223\u2223\u2223\u2223\u2223\u2223 = K and deduce ||f(y)\u2212f(x)|| \u2264 K||y\u2212x|| \u222b 1 0 dt = K||y\u2212x||."
        },
        {
            "heading": "A.2 Main Results",
            "text": ""
        },
        {
            "heading": "A.3 Proof of Theorem 1",
            "text": "Here we use the linear attention (Katharopoulos et al., 2020) with the dot-product similarity between Q and K. Being a real symmetric matrix, XTX can be decomposed into Q\u0303\u03a3Q\u0303T , which leads to\nAttention(Q,K,V ) = XWQWK T Q\u0303\ufe38 \ufe37\ufe37 \ufe38\northogonal\n\u03a3\ufe38\ufe37\ufe37\ufe38 diagonal Q\u0303TW V\ufe38 \ufe37\ufe37 \ufe38 orthogonal . (9)\nAs the product of two orthogonal matrices is orthogonal, Equation 9 reduces to\nAttention(Q,K,V ) = XU\u03a3V (10)\nwith a suitable set of learnable orthogonal matrices U and V , and \u03a3 being the matrix containing the eigenvalues of XTX ."
        },
        {
            "heading": "A.4 Proof of Lemma 1",
            "text": "Let us assume \u2203x \u0338= y such that f(x) = f(y), which implies ||f(x)\u2212 f(y)|| = 0. Using triangle inequality and Equation 2 we get\nx\u2212 y = f(x)\u2212 f(y)\u2212 \u03b1l L \u00b7 (F (x)\u2212 F (y))\nThis implies,\n||x\u2212 y||\n\u2264 ||f(x)\u2212 f(y)||+ | \u2212 \u03b1l L | \u00b7 ||F (x)\u2212 F (y)||.\nTherefore,\n||x\u2212 y|| \u2264 |\u03b1l L | \u00b7 ||F (x)\u2212 F (y)||\n< 1\nL \u00b7 ||F (x)\u2212 F (y)||.\nHere F (X) = ELU(XWXTXW V ), for a suitable choice of orthogonal weights W and W V . Although it is difficult to analytically calculate the matrix derivative of XWXTXW V w.r.t. X , intuitively, we can calculate that XWXTXW V \u2248 O(XXTX), and thus the norm of the inputoutput Jacobian is bounded by 3 \u00b7 ||X||2. Here we can ignore the weight matrices, as they are orthogonally parameterized and therefore has ||.|| = 1. As XTX is stochastic, the norm of the input-output\nJacobian is bounded by 3. Using Lemma 3, we can calculate the Lipschitz bound of ELU activation as 1. Therefore, using the chain rule of calculus and Lemma 3, we can deduce the Lipschitz bound for F as 3. Using this, we get\n||x\u2212 y|| < 3 L \u00b7 ||x\u2212 y||.\nThis contradicts that 3L \u2264 1 for an encoder with L \u2265 3. Hence, we prove the lemma by contradiction."
        },
        {
            "heading": "A.5 Proof of Corollary 1",
            "text": "Let us assume x(l) \u0338= y(l) such that\nE\u2211 e=1 \u03bbex (l\u22121) + \u03b1l L E\u2211 e=1 \u03bbeFe(x (l\u22121))\n= E\u2211\ne=1\n\u03bbey (l\u22121) + \u03b1l L E\u2211 e=1 \u03bbeFe(y (l\u22121)).\nUsing \u2211E\ne=1 \u03bbe = 1 we obtain,\n||x(l\u22121) \u2212 y(l\u22121)||\n= ||\u03b1l L E\u2211 e=1 \u03bbeFe(x (l\u22121))\u2212 \u03b1l L E\u2211 e=1 \u03bbeFe(y (l\u22121))||.\nTherefore,\n||x(l\u22121) \u2212 y(l\u22121)||\n< 1\nL || E\u2211 e=1 \u03bbeFe(x (l\u22121))\u2212 E\u2211 e=1 \u03bbeFe(y (l\u22121))||.\nUsing Lipschitz bound of Fe(x(l\u22121)) as 3, we obtain\n||x(l\u22121) \u2212 y(l\u22121)|| < 3 L E\u2211 e=1 \u03bbe||x(l\u22121) \u2212 y(l\u22121)||\n= 3\nL ||x(l\u22121) \u2212 y(l\u22121)||.\nThis contradicts that fact that 3L \u2264 1. Hence, we prove the corollary by contradiction."
        },
        {
            "heading": "A.6 Proof of Theorem 2",
            "text": "The original input space of the composite mapping is the concatenated token embedding that lies in [\u22121, 1]d, as both orthogonal embedding and position encoding functions have the function image (codomain) in [\u22121, 1] d 2 . As the input space is compact, any continuously differentiable function is Lipschitz. Therefore, the composite function is automatically Lipschitz continuous. Using Lemma 3 and the fact that the composition of multiple injective functions is injective, we can deduce that the encoding function is injective for any encoder with L \u2265 3. The dynamical isometry property can also prove the Lipschitz continuity of the encoding function irrespective of the number of encoding layers. Although ReZero only guarantees dynamical isometry during initialization, we can observe the distribution of residual weights throughout model training. We highlight the distribution of residual weights of TransJect for IMDb and CharIMDb classification tasks in Figure 5. Residual weight values remain < 0.33, highlighting that the residual connections remain injective.\nNext, we compute the Lipschitz bound for f . For the sake of simplicity, let us assume \u03b11L = \u03b12 L \u00b7 \u00b7 \u00b7 = \u03b1l L = \u03b1 < 1 L . We first expand f as\nf(x) = E\u2211 e=1 \u03bbex+ E\u2211 e=1 \u03bbe\u03b1 \u00b7 ELU(xU e\u03a3V e)\n+ \u03b1 \u00b7 ELU(x).\nx = ELU ( E\u2211 e=1 \u03bbexW1+\nE\u2211 e=1 \u03bbe\u03b1 \u00b7 ELU(xU e\u03a3V e)W1 + b1 ) W2 + b2.\nThis implies,\nx = ELU ( xW1+ E\u2211 e=1 \u03bbe\u03b1 \u00b7 ELU(xU e\u03a3V e)W1 + b1 ) W2 + b2.\nUsing the Lipschitz bound of F (X) from the proof of Lemma 1 and the fact \u2211E e=1 \u03bbe = 1, we\nget\n||x\u2212 y|| \u2264 (||x\u2212 y|| \u00b7 ||W1||+ 3|\u03b1| \u00b7 ||x\u2212 y||) \u00b7 ||W2|| \u2264 (1 + 3|\u03b1|)||x\u2212 y||.\nFinally,\n||f(x)\u2212 f(y)|| \u2264 ||x\u2212 y||+ 3|\u03b1| \u00b7 ||x\u2212 y|| + 3|\u03b1|(1 + 3|\u03b1|) \u00b7 ||x\u2212 y|| \u2264 (1 + 3|\u03b1|)2||x\u2212 y||\n< (1 + 3\nL )2||x\u2212 y||.\nHere, f is a layer-wise operator. Hence, for all the L encoder layers,\n||f(x)\u2212 f(y)|| < (1 + 3 L )2L||x\u2212 y||.\nUsing limn\u2192\u221e(1 + 1n) n = e, we get ||f(x) \u2212 f(y)|| < e6||x\u2212y||. Although the theoretical Lipschitz bound is very high, the empirical Lipschitz bound is much smaller due to dynamical isometry. The Lipschitz bound is also independent of the input data, making it theoretically valid. However, as demonstrated in Figure 5, the residual weights are empirically closer to 0, even during the model training, due to which the empirical Lipschitz bound is \u2248 1 << e6."
        },
        {
            "heading": "B Experimental Setup",
            "text": ""
        },
        {
            "heading": "B.1 Hyperparameter settings",
            "text": "For IMDb and AGnews classifications, we choose a maximum text length of 512. In all these two classification tasks, we keep the configuration of our models with L = 6, E = 4, and d = 512. For these two tasks, we use a mean pooling on the hidden representation obtained by the final encoder layer before passing it to the final classification\nlayer. We utilize the BERT pre-trained tokenizer2 to tokenize the texts for these two tasks. We follow the evaluation methodologies followed by Chen et al. (2021). In all these classification tasks, we keep the configuration of our models with L = 2, E = 4, and d = 512. Except for the text classification task, we use max pooling on the hidden representation obtained from the final encoder layer before feeding to the final classification feed-forward layer. For the PTB language modeling task, we use a maximum sequence length of 35 in both the input and the output. For all the models, we set L = 6, and d = 512. All IMDb and AGNews classification experiments are run for 30 epochs. We use an early stopping based on the test loss with the patience of 4 epochs to terminate learning on plateaus. For both these experiments, we use Adam optimizer with a learning rate of 0.0005, \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We use both training and test batches of size 32. We follow the implementation details shared by Chen et al. (2021) for the LRA benchmark. All these experiments are run for 50k steps. We use Adam optimizer with a learning rate of 0.01 for the language modeling task and trained the models for 10 epochs. One Tesla T4 and one Tesla V100 GPU are used for conducting all the experiments. For each task, we report the average performance across three different runs.\nTo enforce orthogonality during the forward pass and after backpropagation, we use PyTorch parameterization3. This implementation uses different orthogonal maps (e.g. householder or Cayley mapping) to ensure orthogonality. It further uses the framework of dynamic trivialization (Lezcano Casado, 2019) to ensure orthogonality after gradient descent. We use the implementation by\n2https://huggingface.co/bert-base-uncased 3https://pytorch.org/docs/stable/generated/\ntorch.nn.utils.parametrizations.orthogonal.html\nScikit-learn4 that uses 1 \u2212 cosine_similarity as the cosine distance. Cosine similarity is the cosine of the angle between two vectors which lies in [\u22121, 1]. Thus the distance lies in [0, 2].\n4https://scikit-learn.org/stable/modules/ generated/sklearn.metrics.pairwise.cosine_ distances.html"
        },
        {
            "heading": "C Analysis",
            "text": "Connections between entropy and model configuration. We highlight the connections between activation bounds and total entropy under different model configurations in Figure 6 for TransJect and Transformer, respectively. We observe that more experts in MOE increase activation at the final layer. On the other hand, with a larger dropout for Transformer, the model regularizes more and creates more sparsity, leading to lower activation factors and entropy. Contrary to TransJect, increasing the number of layers of the Transformer increases activation. This behaviour indicates the layer-agnostic property of our model.\nEffectiveness of the expert model. We observe the individual importance of experts and how they interplay in the mixture model. We illustrate the expert weight distribution in Figure 7, confirming\nthat the experts are well-balanced and that our model does not require an enforced load balancing. Further, we calculate the entropy of each expert representation to understand how it constituents the final representation at every layer. The expert entropy value of TransJect is \u22122.2. Computing an equivalent entropy of different attention heads for the Transformer gives us an entropy of 0.67. The lower entropy displayed by the mixture of experts highlights the generalization capabilities of the experts. Different experts learn differently but orderly, unlike random and sparse learning by the attention heads of Transformer models.\nParameter comparison. We compare the models in terms of total learnable parameters in Table 5. In an equivalent setup, TransJect has 50% more learnable parameters than Transformers."
        }
    ],
    "title": "Manifold-Preserving Transformers are Effective for Short-Long Range Encoding",
    "year": 2023
}