{
    "abstractText": "Temporal Language Grounding seeks to localize video moments that semantically correspond to a natural language query. Recent advances employ the attention mechanism to learn the relations between video moments and the text query. However, naive attention might not be able to appropriately capture such relations, resulting in ineffective distributions where target video moments are difficult to separate from the remaining ones. To resolve the issue, we propose an energy-based model framework to explicitly learn momentquery distributions. Moreover, we propose DemaFormer, a novel Transformer-based architecture that utilizes exponential moving average with a learnable damping factor to effectively encode moment-query inputs. Comprehensive experiments on four public temporal language grounding datasets showcase the superiority of our methods over the state-of-the-art baselines. Our code and data are publicly available at https://github.com/... (the link is hidden now due to the double-blind review).",
    "authors": [
        {
            "affiliations": [],
            "name": "Thong Nguyen"
        },
        {
            "affiliations": [],
            "name": "Xiaobao Wu"
        },
        {
            "affiliations": [],
            "name": "Xinshuai Dong"
        },
        {
            "affiliations": [],
            "name": "Cong-Duy Nguyen"
        },
        {
            "affiliations": [],
            "name": "See-Kiong Ng"
        },
        {
            "affiliations": [],
            "name": "Luu Anh Tuan"
        }
    ],
    "id": "SP:ab10b6a2cb7768680245f61c0e3eabebd2715c3b",
    "references": [
        {
            "authors": [
                "Lisa Anne Hendricks",
                "Oliver Wang",
                "Eli Shechtman",
                "Josef Sivic",
                "Trevor Darrell",
                "Bryan Russell."
            ],
            "title": "Localizing moments in video with natural language",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 5803\u20135812.",
            "year": 2017
        },
        {
            "authors": [
                "Taivanbat Badamdorj",
                "Mrigank Rochan",
                "Yang Wang",
                "Li Cheng."
            ],
            "title": "Joint visual and audio learning for video highlight detection",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8127\u20138137.",
            "year": 2021
        },
        {
            "authors": [
                "Joao Carreira",
                "Andrew Zisserman"
            ],
            "title": "Quo vadis, action recognition? a new model and the kinetics",
            "year": 2017
        },
        {
            "authors": [
                "Ching-Yao Chuang",
                "Joshua Robinson",
                "Yen-Chen Lin",
                "Antonio Torralba",
                "Stefanie Jegelka."
            ],
            "title": "Debiased contrastive learning",
            "venue": "Advances in neural information processing systems, 33:8765\u20138775.",
            "year": 2020
        },
        {
            "authors": [
                "Yilun Du",
                "Igor Mordatch."
            ],
            "title": "Implicit generation and generalization in energy-based models",
            "venue": "arXiv preprint arXiv:1903.08689.",
            "year": 2019
        },
        {
            "authors": [
                "Shiv Ram Dubey",
                "Satish Kumar Singh",
                "Bidyut Baran Chaudhuri."
            ],
            "title": "Activation functions in deep learning: A comprehensive survey and benchmark",
            "venue": "Neurocomputing.",
            "year": 2022
        },
        {
            "authors": [
                "Stefan Elfwing",
                "Eiji Uchibe",
                "Kenji Doya."
            ],
            "title": "Sigmoid-weighted linear units for neural network function approximation in reinforcement learning",
            "venue": "Neural Networks, 107:3\u201311.",
            "year": 2018
        },
        {
            "authors": [
                "Christoph Feichtenhofer",
                "Haoqi Fan",
                "Jitendra Malik",
                "Kaiming He."
            ],
            "title": "Slowfast networks for video recognition",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision, pages 6202\u20136211.",
            "year": 2019
        },
        {
            "authors": [
                "Jialin Gao",
                "Xin Sun",
                "Mengmeng Xu",
                "Xi Zhou",
                "Bernard Ghanem."
            ],
            "title": "Relation-aware video reading comprehension for temporal language grounding",
            "venue": "arXiv preprint arXiv:2110.05717.",
            "year": 2021
        },
        {
            "authors": [
                "Jiyang Gao",
                "Chen Sun",
                "Zhenheng Yang",
                "Ram Nevatia."
            ],
            "title": "Tall: Temporal activity localization via language query",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 5267\u2013 5275.",
            "year": 2017
        },
        {
            "authors": [
                "Kaifeng Gao",
                "Long Chen",
                "Yifeng Huang",
                "Jun Xiao."
            ],
            "title": "Video relation detection via tracklet based visual transformer",
            "venue": "Proceedings of the 29th ACM International Conference on Multimedia, pages 4833\u2013 4837.",
            "year": 2021
        },
        {
            "authors": [
                "Jort F Gemmeke",
                "Daniel PW Ellis",
                "Dylan Freedman",
                "Aren Jansen",
                "Wade Lawrence",
                "R Channing Moore",
                "Manoj Plakal",
                "Marvin Ritter."
            ],
            "title": "Audio set: An ontology and human-labeled dataset for audio events",
            "venue": "2017 IEEE international conference on",
            "year": 2017
        },
        {
            "authors": [
                "Soham Ghosh",
                "Anuva Agarwal",
                "Zarana Parekh",
                "Alexander Hauptmann."
            ],
            "title": "Excl: Extractive clip localization using natural language descriptions",
            "venue": "arXiv preprint arXiv:1904.02755.",
            "year": 2019
        },
        {
            "authors": [
                "Lisa Anne Hendricks",
                "Oliver Wang",
                "Eli Shechtman",
                "Josef Sivic",
                "Trevor Darrell",
                "Bryan Russell."
            ],
            "title": "Localizing moments in video with temporal language",
            "venue": "arXiv preprint arXiv:1809.01337.",
            "year": 2018
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel."
            ],
            "title": "Gaussian error linear units (gelus)",
            "venue": "arXiv preprint arXiv:1606.08415.",
            "year": 2016
        },
        {
            "authors": [
                "Fa-Ting Hong",
                "Xuanteng Huang",
                "Wei-Hong Li",
                "Wei-Shi Zheng."
            ],
            "title": "Mini-net: Multiple instance ranking network for video highlight detection",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceed-",
            "year": 2020
        },
        {
            "authors": [
                "Will Kay",
                "Joao Carreira",
                "Karen Simonyan",
                "Brian Zhang",
                "Chloe Hillier",
                "Sudheendra Vijayanarasimhan",
                "Fabio Viola",
                "Tim Green",
                "Trevor Back",
                "Paul Natsev"
            ],
            "title": "The kinetics human action video",
            "year": 2017
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling."
            ],
            "title": "Autoencoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114.",
            "year": 2013
        },
        {
            "authors": [
                "Jie Lei",
                "Tamara L Berg",
                "Mohit Bansal."
            ],
            "title": "Detecting moments and highlights in videos via natural language queries",
            "venue": "Advances in Neural Information Processing Systems, 34:11846\u201311858.",
            "year": 2021
        },
        {
            "authors": [
                "Jie Lei",
                "Licheng Yu",
                "Mohit Bansal",
                "Tamara L Berg."
            ],
            "title": "Tvqa: Localized, compositional video question answering",
            "venue": "arXiv preprint arXiv:1809.01696.",
            "year": 2018
        },
        {
            "authors": [
                "Jie Lei",
                "Licheng Yu",
                "Tamara L Berg",
                "Mohit Bansal."
            ],
            "title": "Tvr: A large-scale dataset for video-subtitle moment retrieval",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u2013 28, 2020, Proceedings, Part XXI 16, pages 447\u2013463.",
            "year": 2020
        },
        {
            "authors": [
                "Ye Liu",
                "Siyuan Li",
                "Yang Wu",
                "Chang-Wen Chen",
                "Ying Shan",
                "Xiaohu Qie."
            ],
            "title": "Umt: Unified multimodal transformers for joint video moment retrieval and highlight detection",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-",
            "year": 2022
        },
        {
            "authors": [
                "Xuezhe Ma",
                "Chunting Zhou",
                "Xiang Kong",
                "Junxian He",
                "Liangke Gui",
                "Graham Neubig",
                "Jonathan May",
                "Luke Zettlemoyer."
            ],
            "title": "Mega: moving average equipped gated attention",
            "venue": "arXiv preprint arXiv:2209.10655.",
            "year": 2022
        },
        {
            "authors": [
                "Thong Nguyen",
                "Anh Tuan Luu."
            ],
            "title": "Contrastive learning for neural topic model",
            "venue": "Advances in neural information processing systems, 34:11974\u201311986.",
            "year": 2021
        },
        {
            "authors": [
                "Thong Nguyen",
                "Cong-Duy Nguyen",
                "Xiaobao Wu",
                "See-Kiong Ng",
                "Anh Tuan Luu."
            ],
            "title": "Vision-and-language pretraining",
            "venue": "arXiv preprint arXiv:2207.01772.",
            "year": 2022
        },
        {
            "authors": [
                "Thong Nguyen",
                "Xiaobao Wu",
                "Xinshuai Dong",
                "Anh Tuan Luu",
                "Cong-Duy Nguyen",
                "Zhen Hai",
                "Lidong Bing."
            ],
            "title": "Gradient-boosted decision tree for listwise context model in multimodal review helpfulness prediction",
            "venue": "arXiv preprint arXiv:2305.12678.",
            "year": 2023
        },
        {
            "authors": [
                "Thong Nguyen",
                "Xiaobao Wu",
                "Anh-Tuan Luu",
                "CongDuy Nguyen",
                "Zhen Hai",
                "Lidong Bing."
            ],
            "title": "Adaptive contrastive learning on multimodal transformer for review helpfulness predictions",
            "venue": "arXiv preprint arXiv:2211.03524.",
            "year": 2022
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals."
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748.",
            "year": 2018
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher D Manning."
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543.",
            "year": 2014
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Prajit Ramachandran",
                "Barret Zoph",
                "Quoc V Le."
            ],
            "title": "Searching for activation functions",
            "venue": "arXiv preprint arXiv:1710.05941.",
            "year": 2017
        },
        {
            "authors": [
                "Danilo Rezende",
                "Shakir Mohamed."
            ],
            "title": "Variational inference with normalizing flows",
            "venue": "International conference on machine learning, pages 1530\u2013 1538. PMLR.",
            "year": 2015
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman."
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556.",
            "year": 2014
        },
        {
            "authors": [
                "Min Sun",
                "Ali Farhadi",
                "Steve Seitz."
            ],
            "title": "Ranking domain-specific highlights by analyzing edited videos",
            "venue": "Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13, pages 787\u2013802.",
            "year": 2014
        },
        {
            "authors": [
                "Anran Wang",
                "Anh Tuan Luu",
                "Chuan-Sheng Foo",
                "Hongyuan Zhu",
                "Yi Tay",
                "Vijay Chandrasekhar."
            ],
            "title": "Holistic multi-modal memory network for movie question answering",
            "venue": "IEEE Transactions on Image Processing, 29:489\u2013499.",
            "year": 2019
        },
        {
            "authors": [
                "Weining Wang",
                "Yan Huang",
                "Liang Wang."
            ],
            "title": "Language-driven temporal activity localization: A semantic matching reinforcement learning model",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 334\u2013343.",
            "year": 2019
        },
        {
            "authors": [
                "Jie Wei",
                "Guanyu Hu",
                "Luu Anh Tuan",
                "Xinyu Yang",
                "Wenjing Zhu."
            ],
            "title": "Multi-scale receptive field graph model for emotion recognition in conversations",
            "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Process-",
            "year": 2023
        },
        {
            "authors": [
                "Jie Wei",
                "Guanyu Hu",
                "Xinyu Yang",
                "Anh Tuan Luu",
                "Yizhuo Dong."
            ],
            "title": "Audio-visual domain adaptation feature fusion for speech emotion recognition",
            "venue": "INTERSPEECH, pages 1988\u20131992.",
            "year": 2022
        },
        {
            "authors": [
                "Jie Wei",
                "Guanyu Hu",
                "Xinyu Yang",
                "Anh Tuan Luu",
                "Yizhuo Dong."
            ],
            "title": "Learning facial expression and body gesture visual information for video emotion recognition",
            "venue": "Expert Systems with Applications, 237:121419.",
            "year": 2024
        },
        {
            "authors": [
                "Aming Wu",
                "Yahong Han."
            ],
            "title": "Multi-modal circulant fusion for video-to-language and backward",
            "venue": "IJCAI, volume 3, page 8.",
            "year": 2018
        },
        {
            "authors": [
                "Shaoning Xiao",
                "Long Chen",
                "Jian Shao",
                "Yueting Zhuang",
                "Jun Xiao."
            ],
            "title": "Natural language video localization with learnable moment proposals",
            "venue": "arXiv preprint arXiv:2109.10678.",
            "year": 2021
        },
        {
            "authors": [
                "Bo Xiong",
                "Yannis Kalantidis",
                "Deepti Ghadiyaram",
                "Kristen Grauman."
            ],
            "title": "Less is more: Learning highlight detection from video duration",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1258\u20131267.",
            "year": 2019
        },
        {
            "authors": [
                "Minghao Xu",
                "Hang Wang",
                "Bingbing Ni",
                "Riheng Zhu",
                "Zhenbang Sun",
                "Changhu Wang."
            ],
            "title": "Crosscategory video highlight detection via set-based learning",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7970\u20137979.",
            "year": 2021
        },
        {
            "authors": [
                "Yifang Xu",
                "Yunzhuo Sun",
                "Yang Li",
                "Yilei Shi",
                "Xiaoxiang Zhu",
                "Sidan Du."
            ],
            "title": "Mh-detr: Video moment and highlight detection with cross-modal transformer",
            "venue": "arXiv preprint arXiv:2305.00355.",
            "year": 2023
        },
        {
            "authors": [
                "Qinghao Ye",
                "Xiyue Shen",
                "Yuan Gao",
                "Zirui Wang",
                "Qi Bi",
                "Ping Li",
                "Guang Yang."
            ],
            "title": "Temporal cue guided video highlight detection with low-rank audiovisual fusion",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages",
            "year": 2021
        },
        {
            "authors": [
                "Yunan Ye",
                "Zhou Zhao",
                "Yimeng Li",
                "Long Chen",
                "Jun Xiao",
                "Yueting Zhuang."
            ],
            "title": "Video question answering via attribute-augmented attention network learning",
            "venue": "Proceedings of the 40th International ACM SIGIR conference on Research and Develop-",
            "year": 2017
        },
        {
            "authors": [
                "Yitian Yuan",
                "Lin Ma",
                "Jingwen Wang",
                "Wei Liu",
                "Wenwu Zhu."
            ],
            "title": "Semantic conditioned dynamic modulation for temporal sentence grounding in videos",
            "venue": "Advances in Neural Information Processing Systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Runhao Zeng",
                "Haoming Xu",
                "Wenbing Huang",
                "Peihao Chen",
                "Mingkui Tan",
                "Chuang Gan."
            ],
            "title": "Dense regression network for video grounding",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10287\u201310296.",
            "year": 2020
        },
        {
            "authors": [
                "Da Zhang",
                "Xiyang Dai",
                "Xin Wang",
                "Yuan-Fang Wang",
                "Larry S Davis."
            ],
            "title": "Man: Moment alignment network for natural language moment retrieval via iterative graph adjustment",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-",
            "year": 2019
        },
        {
            "authors": [
                "Hao Zhang",
                "Aixin Sun",
                "Wei Jing",
                "Joey Tianyi Zhou."
            ],
            "title": "Span-based localizing network for natural language video localization",
            "venue": "arXiv preprint arXiv:2004.13931.",
            "year": 2020
        },
        {
            "authors": [
                "Songyang Zhang",
                "Houwen Peng",
                "Jianlong Fu",
                "Jiebo Luo."
            ],
            "title": "Learning 2d temporal adjacent networks for moment localization with natural language",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 12870\u201312877.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Temporal Language Grounding (TLG) is a task to determine temporal boundaries of video moments that semantically correspond (relevant) to a language query (Hendricks et al., 2018; Gao et al., 2021a). TLG is a complex and challenging task, since video processing demands understanding across multiple modalities, including image, text, and even audio. However, TLG has received increasing attention in CV and NLP communities because it provides myriad usage for further downstream tasks, e.g. VQA (Lei et al., 2018; Ye et al., 2017; Wang et al., 2019a), relation extraction (Gao et al., 2021b), and information retrieval (Ghosh et al., 2019).\nEarly methods for TLG deploy concatenation with linear projection (Gao et al., 2017; Wang et al.,\n\u2217Corresponding Author\nText query: The woman wearing sunglasses crosses a small colorful bridge over the river.\n2019b) or similarity functions (Anne Hendricks et al., 2017; Hendricks et al., 2018) to fuse textual and visual features. To further enhance the localization performance, recent works divide the video into equal-length moments and employ the attention mechanism of the Transformer model to learn the relations between video moments and the language query. For example, Moment-DETR model (Lei et al., 2021) concatenates the visual moments and the textual tokens, and then passes the concatenated sequence to the Transformer encoder to capture the alignment. The UMT model (Liu et al., 2022) includes an additional audio channel into the Transformer encoder to construct a unified architecture.\nHowever, for the TLG task, previous works have shown that such attention-based approach is still insufficient to capture the rich semantic interaction\nbetween the text query and video moments (Xu et al., 2023). As in Figure 2, the localized video moments hardly align with the query statement. Moreover, the attention mechanism in the Transformer encoder does not assume any prior knowledge towards the input elements (Ma et al., 2022). For language localization, this design choice does not leverage the fact that video moments in temporal neighborhoods tend to exhibit closely related features. Therefore, this approach could lead to ineffective modeling of joint moment-query inputs. As evidence, Figure 1 demonstrates the distribution of joint moment-query representations. Particularly, the representations of target moment-query localizations and the remaining ones mingle together, making the grounding task more challenging.\nTo address these limitations, we dive deeper into polishing the distribution of moment-query representations. In addition to supervised training to correctly localize the language in the video, we perform unsupervised training to explicitly maximize the likelihood of moment-query localizations. This could help the multimodal model focus on capturing the distribution of target moment-query pairs and distinguishing them from others.\nAs such, we propose to model the distribution of moment-query representations under the framework of the Energy-Based Model (EBM). In contrast to other probabilistic models such as normalizing flow (Rezende and Mohamed, 2015) or autoencoder (Kingma and Welling, 2013), the EBM framework allows us to directly integrate the video moment\u2019s salience score into the density function, which results in accurate modeling of momentquery representations. Our implementation develops into a contrastive divergence objective which aims to minimize the energy of the relevant localizations while maximizing the energy of the deviating ones. Accordingly, the framework needs negative samples to represent high-energy regions. Therefore, we adapt the Langevin dynamics equation to directly sample negative inputs from the EBM distribution. Such approach is appropriate because in the beginning the distribution will not match the true distribution, hence the generated samples are assured to be negative. As the training progresses, the distribution will approximate the true distribution, consequently the Langevin equation is able to produce hard negative samples.\nIn addition, we incorporate the inductive bias that captures local dependencies among the\nmoment-query inputs. We propose DemaFormer in which we equip the Damped Exponential Moving Average (DEMA) computation for the TransFormer architecture. Technically, the computation applies exponentially decaying factors that consider the information from adjacent inputs. We further introduce learnable damping coefficients to enable the model to absorb adjacent information in a sufficient manner that ensures distinction among inputs. Eventually, we combine the DEMA computation with the attention mechanism to construct DemaFormer encoder and decoder modules.\nTo sum up, the contributions of our paper can be summarized as follows:\n\u2022 We propose DemaFormer, a novel architecture for temporal language grounding. DemaFormer integrates exponential moving average with learnable damping coefficients into the attention mechanism to appropriately capture dependency patterns among videolanguage inputs.\n\u2022 We propose a novel energy-based learning framework for temporal language grounding. The objective for the energy-based model can be formulated as a contrastive divergence to assist a classical grounding loss for modeling moment-query representations.\n\u2022 We conduct extensive experiments to demonstrate the superiority of our method over previous state-of-the-art baselines. Furthermore, we conduct comprehensive ablation studies to evaluate our component proposals and deliver meaningful insights."
        },
        {
            "heading": "2 Related Work",
            "text": "Temporal Language Grounding (TLG). Introduced by Gao et al. (2017); Anne Hendricks et al. (2017), TLG is to locate relevant video moments given a language query. Early approaches use LSTM to encode language query and CNN for visual clips, and then estimate cross-modal similarity scores (Anne Hendricks et al., 2017; Hendricks et al., 2018). Modern techniques leverage attention mechanism and structured graph network to learn the video-language relationship (Xiao et al., 2021; Gao et al., 2021a; Zhang et al., 2020a; Yuan et al., 2019). Recent works (Liu et al., 2022; Lei et al., 2021) apply Transformer components to eliminate hand-crafted pre-processing and post-processing steps and make the model end-to-end trainable.\nVision-Language Representation Learning. Modeling the vision-language interaction is important for vision-language tasks (Gao et al., 2021a; Nguyen et al., 2022b,a, 2023; Wei et al., 2022, 2023, 2024). Previous works propose diverse techniques, including circular matrices (Wu and Han, 2018), dynamic filters (Zhang et al., 2019), Hadamard product (Zhang et al., 2020a), and contrastive learning (Nguyen and Luu, 2021). To better learn fine-grained token-level and moment-level cross-modal relations, several authors adapt graph neural networks with graph convolutional techniques (Gao et al., 2021a; Zhang et al., 2019)."
        },
        {
            "heading": "3 Methodology",
            "text": "Our task is to localize moments in videos from natural language queries. Formally, given a language query q of Lq tokens and a video v composed of Lv equal-length input moments, where each moment is represented by a visual frame sampled from the moment, we aim to localize Lm time spans from v that is aligned with the query q, noted as {(li, ri)}Lmi=1, where each moment spans from li to ri scaled by the video timelength and Lm < Lv.\nThus, we first describe our proposed damping exponential moving average attention for modeling video-language inputs in Section 3.1, the overall architecture in Section 3.2, and the training strategy empowered with energy-based modeling in Section 3.3 and 3.4."
        },
        {
            "heading": "3.1 Damping Exponential Moving Average (DEMA) Attention",
            "text": "In this section, we consider the input to our DemaFormer encoder Xe and decoder Xd in Section 3.2 as the general input X = {xi}LXi=1 of length LX . We delineate the exponential moving average (EMA) with the damping influence applied on X as follows. DEMA Computation. At first, we use a linear layer to map each input xi to an intermediate space:\ngi = Linear(xi), (1)\nThen, we estimate the current hidden state li as the sum of the previous hidden state li\u22121 and the current intermediate input gi with the weighting coefficients that decrease exponentially and are relaxed by damping coefficients:\nli = \u03b1\u2299 gi + (1\u2212\u03b1\u2299 \u03b4)\u2299 li\u22121, (2)\nwhere \u03b1 \u2208 (0, 1)d denotes the weighting coefficients, \u03b4 \u2208 (0, 1)d the damping coefficients, and \u2299 the elementwise product. Both \u03b1 and \u03b4 are randomly initialized and learnable during training. Subsequently, we project the hidden state li back to the original input space:\nx\u2032i = Linear(li). (3)\nDEMA Attention. Given the input X , we obtain the DEMA output in Eq. (3) and pass the output through a non-linear layer:\nX \u2032 = DEMA(X), (4) Z = SiLU(Linear(X \u2032)), (5)\nwhere SiLU denotes the self-gated activation function (Ramachandran et al., 2017; Elfwing et al., 2018). We experiment with other activation functions in Appendix D. Subsequently, we perform the attention operation and utilize Z which exhibits local dependencies as the value tensor:\nQ = Linear(X), (6)\nK = Linear(X), (7)\nV = Linear(Z), (8) Z \u2032 = softmax ( QKT\u221a dK ) \u00b7 V, (9)\nwhere dK denotes the dimension of K. Thereafter, we aggregate the original input X and the attention output Z \u2032 in an adaptive manner:\n\u03bb = sigmoid(Linear(X \u2032)), (10)\nP = SiLU(Linear(X \u2032) + Linear(Z \u2299 Z \u2032)), (11)\nH = \u03bb\u2299 P + (1\u2212 \u03bb)\u2299X. (12)"
        },
        {
            "heading": "3.2 Overall Architecture",
            "text": "Figure 3 illustrates our DemaFormer model for temporal language grounding. We explain the architecture in details in the following. Uni-modal Encoding. Given a video v consisting of Lv moments and a text query with Lq tokens, we employ pre-trained models to extract visual moment features F = {fi}Lvi=1, textual features T = {ti}Lqi=1, and audio features A = {ai}Lvi=1. Audio-Dependent Video Encoding. For videoaudio encoding, because audio signals possess heavy noisy information (Liu et al., 2022; Badamdorj et al., 2021), we only perform 1-layer\nattention to fuse the audio information into the visual sequence. Particularly, the attention between the video and audio input becomes:\nF \u2032 = F + softmax ( AF T\u221a\nd\n) \u00b7 F. (13)\nDemaFormer Encoder. Inspired by (Lei et al., 2021), we concatenate the audio-dependent video and language tokens to form the input sequence:\nXe = [F \u2032, T ]. (14)\nWe push the input sequence Xe to the DemaFormer encoder of Ne encoder layers. Each encoder layer comprises a DEMA attention layer, a normalization layer, a ReLU non-linear layer, and a residual connection:\nH(i+1)e = Norm ( DEMA ( X(i)e )) , (15)\nX(i+1)e = O (i) e = Norm\n( ReLU ( H(i)e ) +H(i)e ) .\n(16)\nwhere X(i) and O(i)e denote the input and the output of the i-th encoder layer, respectively; H(i)e the intermediate output of the i-th encoder layer.\nWe take the output of the Ne-th encoder layer as the final output Oe of the DemaFormer encoder.\nOe = O (Ne) e . (17)\nDemaFormer Decoder. The input for the decoder is the first Lv DemaFormer encoder outputs, i.e. Xd = {oe,i}Lvi=1. The input sequence is forwarded to Nd decoder layers, each of which is composed of a DEMA attention layer, a normalization layer,\na non-linear layer, and a residual connection:\nH (i) d = Norm\n( DEMA ( X\n(i) d\n)) , (18)\nM (i+1) = O (i) d = Norm\n( ReLU ( H\n(i) d\n) +H\n(i) d\n) .\n(19)\nAnalogous to the encoder, we retrieve the Nd-th layer output as the final output Od of the decoder:\nOd = O (Nd) d . (20)\nPrediction Heads. For each output od,i, we designate four separate linear layers to predict the salience score s\u0302i, the center c\u0302i, the center offset c\u0302oi, and the moment width w\u0302i:\ns\u0302i = Linear(od,i), c\u0302i = Linear(od,i), (21)\nc\u0302oi = Linear(od,i), w\u0302i = Linear(od,i). (22)\nThus, each candidate moment\u2019s temporal bound becomes ( c\u0302t + c\u0302oi \u2212 w\u0302i2 , c\u0302i + c\u0302oi + w\u0302i2 ) . At test time, we extract the top-Lm moments whose salience scores are the largest."
        },
        {
            "heading": "3.3 Energy-Based Models for Modeling Moment-Query Representations",
            "text": "Given our joint video-language decoder outputs Od = {od,i}Lvt=1, we designate the EBM to specify the density of Od via the Boltzmann distribution:\np\u03b8(od,i) = exp(\u2212E\u03b8(od,i))\nZ\u03b8 , (23)\nwhere E\u03b8 denotes the energy function and Z\u03b8 the normalizing constant Z\u03b8 = \u222b exp (\u2212E\u03b8(od,i)) dod,i. Inspired by (Du and Mordatch, 2019), we adopt Langevin\ndynamics to conduct sampling from the above distribution:\no\u0303 (k) d,i = o\u0303 (k\u22121) d,i \u2212\n\u03b3 2 \u2207od,iE\u03b8 ( o\u0303 (k\u22121) d,i ) + \u03f5(k),\n(24)\n\u03f5(k) \u223c N (0, \u03b3), o\u0303d,i = o(0)d,i , (25)\nwhere \u03b3 is a hyperparameter to specify the variance of the noise. We perform the Eq. (24) for K steps and take o\u0303(K)d,i as the sampling outcome. Our target is to better align the video-query representation Od, by minimizing the negative log likelihood of the moment-query representations, i.e. LNLL(\u03b8) = \u2212Eod,i [log p\u03b8(od,i)]. This can be achieved by differentiating the LNLL(\u03b8) and optimize the resulting contrastive divergence for the gradient, as:\n\u2207\u03b8LNLL = Eo+d,i [ \u2207\u03b8E\u03b8(o+d,i) ] \u2212Eo\u2212d,i [ \u2207\u03b8E\u03b8(o\u2212d,i) ] ,\n(26) whose detailed derivation can be found in Appendix A. Because the samples generated by Eq. (24) do not approximate the true distribution in the beginning but will gradually converge to, we take these samples as o\u2212d,i and assign their energy values a decaying weight \u03b1 with minimum value of \u03b1min. We take the moment-query inputs whose groundtruth salience scores are larger than a threshold \u03c1 as positive samples o+d,i.\nMoreover, because we maximize salience scores while minimizing the energy values of the positive input (vice versa for the negative input), we implement the negative salience-energy relation:\nE\u03b8(od,i) = \u2212s\u0302i. (27)\nAs such, \u03b8 becomes the DemaFormer\u2019s parameters and we obtain the final LNLL\u2019s formulation:\n\u03b1 = max\n( 1\n1 + 12nepoch , \u03b1min\n) , (28)\nLNLL = Eo+d,i [ E\u03b8(o + d,i) ] \u2212 \u03b1 \u00b7 Eo\u2212d,i [ E\u03b8(o \u2212 d,i) ] ,\n(29)\nwhere nepoch denotes the current training epoch."
        },
        {
            "heading": "3.4 Training Objective",
            "text": "From a video-language input, we obtain Lm predictions Y\u0302 = {(s\u0302i, c\u0302i, c\u0302oi, w\u0302i)}Lmi=1. During training Lm is the number of groundtruth localizations,\nwhile during testing Lm is selected based on validation. We define the matching loss Lmatch between predictions and groundtruth as:\nLmatch = \u2212 1\nLm Lm\u2211 i=1 (s\u0302i \u2212 \u03bb1||ci \u2212 c\u0302i||\u2212\n\u03bb2||wi \u2212 w\u0302i|| \u2212 \u03bb3||coi \u2212 (c\u0302oi \u2212 c\u0302i)||), (30)\nwhere \u03bb{1,2,3,4} denote the hyperparameter weights for the salience, center, width, and offset losses, respectively. We jointly optimize the matching loss with the EBM negative log-likelihood (NLL) loss as follows:\nL = Lmatch + \u03bbNLLLNLL, (31)\nwhere \u03bbNLL denotes the weight to scale the NLL loss size."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We evaluate our methods on four benchmark datasets for the temporal language grounding task: QVHighlights, Charades-STA, YouTube Highlights, and TVSum. QVHighlights is collected by (Lei et al., 2021) to span diverse content on 3 major topics: daily vlog, travel vlog, and news. There are 10,148 videos with 18,367 moments associated with 10,310 queries. We follow (Lei et al., 2018; Liu et al., 2022) to split the dataset into 70% train, 15% val, and 15% test portions. Charades-STA (Gao et al., 2017) consists of videos about daily indoor activities. The dataset is split into 12,408 and 3,720 moment annotations for training and testing, respectively. YouTube Highlights is prepared by (Sun et al., 2014) to comprise six categories, i.e. dog, gymnastics, parkour, skating, skiing and surfing. In each category, we inherit the original training-testing split as benchmark for the TLG task. TVSum (Hong et al., 2020) is a video summarization dataset possessing 10 event categories. We employ the video title as the language query and the training/testing split of 0.8/0.2 for experiments."
        },
        {
            "heading": "4.2 Experimental Settings",
            "text": "Evaluation Metrics. Our metrics include Rank k@\u00b5, mAP@\u00b5, and Hit@1. Rank k@\u00b5 is the percentage of the testing samples that have at least one correct localization in the top-k choices, where a localization is correct if its IoU with the groundtruth\nwith ASR captions.\nis larger than the threshold \u00b5. In a similar manner, mAP@\u00b5 is the mean average precision of localizations whose IoU is larger than \u00b5. Hit@1 computes the hit ratio for the moment with the highest predicted salience score in a video. We consider a moment is hit if its groundtruth salience is larger than or equal to a threshold \u03c4 . Following previous works (Lei et al., 2021; Liu et al., 2022), we adopt Rank 1@\u00b5 with \u00b5 \u2208 {0.5, 0.75} and Hit@1 with \u03c4 = 4 for the QVHighlights dataset. For the Charades-STA dataset, we use Rank k@\u00b5 with k \u2208 {1, 5} and \u00b5 \u2208 {0.5, 0.75}. We apply mAP for both the TVSum and YouTube Highlights datasets. Implementation Details. For fair comparison with previous works (Liu et al., 2022; Lei et al., 2021), on QVHighlights, we use SlowFast (Feichtenhofer et al., 2019) and CLIP (Radford et al., 2021) to obtain features for the video moments and CLIP text encoder to obtain features for the language queries. For feature extraction of the CharadesSTA dataset, we deploy VGG (Simonyan and Zisserman, 2014) and optical flow features for video moments and GloVe embeddings (Pennington et al., 2014) for language tokens. On YouTube Highlights and TVSum, we utilize the I3D model (Carreira and Zisserman, 2017) pre-trained on Kinetics 400 (Kay et al., 2017) to extract moment-level visual representations, and CLIP text encoder to extract language representations. Furthermore, as in (Liu\net al., 2022; Lei et al., 2021), for QVHighlights dataset, we also experiment with pre-training our architecture with noisy automatic speech recognition (ASR) captions before fine-tuning on the downstream training samples. For all audio features, we use the PANN model pre-trained on AudioSet (Gemmeke et al., 2017). We provide detailed hyperparameter settings in Appendix B."
        },
        {
            "heading": "4.3 Baselines",
            "text": "To evaluate the proposed methods, we compare our performance with a diversity of baselines:\n\u2022 UMT (Liu et al., 2022): a multi-modal transformer model to handle three modalities, including audio, text, and video.\n\u2022 Moment-DETR (Lei et al., 2021): a multimodal transformer model that applies the original self-attention mechanism to encode no human prior and eliminates manuallydesigned pre-processing and post-processing procedures.\n\u2022 CLIP (Radford et al., 2021): a framework of visual CNN and textual transformer models trained with a contrastive objective.\n\u2022 XML (Lei et al., 2020): a framework of visual ResNet and textual RoBERTa models with a late fusion approach to fuse the visual and textual features. We include an additional variant, XML+, which is trained with the combination of our salience loss and the XML\u2019s loss.\n\u2022 RaNet (Gao et al., 2021a): a baseline with BiLSTM text encoder, CNN image encoder, and a graph cross-modalithy interactor to learn moment-query relations and select the target moments.\n\u2022 2D-TAN (Zhang et al., 2020b): a method to specify moment candidates as a 2D map where the row and column indices indicate the starting and ending points, respectively.\n\u2022 DRN (Zeng et al., 2020): a dense regression network which treats all video moments as positive and seeks to predict its distance to the groundtruth starting and ending boundaries.\n\u2022 MAN (Zhang et al., 2019): a baseline with a structured graph network to model the moment-wise temporal relationships.\n\u2022 TCG (Ye et al., 2021): a multi-modal TLG architecture equipped with a low-rank tensor fusion mechanism and hierarchical temporal context encoding scheme.\n\u2022 Joint-VA (Badamdorj et al., 2021): an approach applying attention mechanism to fuse multi-modal features and a sentinel technique to discount noisy signals.\n\u2022 MINI-Net (Hong et al., 2020): a weakly supervised learning approach that trains a positive bag of query-relevant moments to possess higher scores than negative bags of queryirrelevant moments.\n\u2022 LIM-S (Xiong et al., 2019): a TLG approach that leverages video duration as a weak supervision signal.\n\u2022 DL-VHD (Xu et al., 2021): a framework applying dual learners to capture cross-category concepts and video moment highlight notions."
        },
        {
            "heading": "4.4 Comparison with State-of-the-arts",
            "text": "We report results of our DemaFormer and the baselines in Table 1, 2, 3, and 4 on the QVHighlights, Charades-STA, YouTube Highlights, and TVSum datasets, respectively. As can be seen, our methods significantly outperform previous approaches. QVHighlights. Compared with the previous best method UMT, our DemaFormer achieves 2% absolute improvement at least across all evaluation settings of Rank 1@\u00b5, particularly 4.54% for \u00b5 = 0.5 and 2.01% for \u00b5 = 0.7, respectively. When pretrained with the ASR captions, our method outperforms UMT with 2.59% of mAP on average and 2.64 points of Hit@1. These results demonstrate that our method can enhance the TLG operation in diverse settings, including daily vlog, travel vlog, and news. Charades-STA. We increase the performance of UMT with 3.28% in terms of Rank 1@0.5 and 2.53% in terms of Rank 5@0.5. Upon tighter \u00b5 = 0.7, we achieve a larger degree of enhancement with 5.99% for Rank 1 and 5.18% for Rank 5. We hypothesize that this is because our energy-based modeling can focus on separating highly relevant localizations from other video moment candidates. TVSum. In Table 4, we compare our model with other competitive approaches. Our architecture\naccomplishes the highest mAP scores across all categories and in overall as well. In detail, we outperform the second-best UMT up to 19.34% at maximum on the BT portion. Analogous to the QVHighlights experiments, this demonstrates that our framework can better model the video-language inputs in various contexts to polish the temporal language grounding performance. YouTube Highlights. Equivalent to TVSum, our DemaFormer with the energy-based modeling approach outperforms prior competitive models across various subsets. Specifically, we gain mAP increases of 1.16% at minimum on the surfing portion and 6.07% at maximum on the dog portion. We attribute such improvement to the more effective modeling operation of the proposed DEMA computation in attention, since it can exhibit local dependencies of moment-query inputs for appropriate modeling in various contexts."
        },
        {
            "heading": "4.5 Ablation Studies",
            "text": "In this section, we study the impact of (1) Damped Exponential Moving Average (DEMA), (2) EnergyBased Modeling (EBM), (3) Langevin Sampling Steps, and (4) Choice of Energy Functions. With vs. Without DEMA. From Table 5, removing the damping factor results in slight performance decrease, for example 1.04% and 0.26% in terms of Rank 1@0.7 on QVHighlights and CharadesSTA, respectively. The main reason is that without the damping coefficient, the model lacks the ability to adjust the information injected into adjacent input elements, such that the amount could be excessive to make it hard to distinguish video moments. Moreover, we observe that completely eliminating the DEMA computation leads to significant decrease, specifically up to 2.97% and 2.51% of Rank 1@0.5 respectively on QVHighlights and Charades-STA, since the model no longer specifies the moment-query distribution effectively. With vs Without EBM. Investigating the last rows of Table 5, we realize that adopting energybased modeling can improve the temporal language\ngrounding performance. Particularly, adding the EBM training objective brings enhancement of 2.67% on Charades-STA and 3.16% on QVHighlights in terms of Rank 1@0.5. This substantiates that the EBM can successfully capture the distribution of moment-query representations in which relevant localizations are separated from the irrelevant ones. We provide more illustrative analysis in Section 4.6 and Appendix F. Langevin Sampling Steps. We investigate the impact of the number of sampling steps K in our Langevin equation (24) upon DemaFormer. Figure 4 shows that DemaFormer\u2019s performance increases as K grows. However, as long as K passes the threshold 100, the model performance converges with negligible fluctuation. We hypothesize that at K = 100 the added noise is sufficient to segregate target localizations from elsewhere. Choice of Energy Functions. We experiment with different choices of the energy function. Inspired by the contrastive learning works (Chuang et al., 2020; Oord et al., 2018), we compute the negative cosine similarity of video clips Ove = {oe,i}Lvi=1 and query tokens Oqe = {oe,i}Lv+Lqi=Lv+1 in the ele-\nText query: The woman wearing sunglasses crosses a small colorful bridge over the river\nmentwise and pooling-based manner (we provide the formulations in Appendix E), and evaluate the performance of the variants in Table 6. As the comparison shows, directly utilizing the salience score provides localizations with the most accuracy. This suggests that similarity functions do not fully implement the query-relevance concept."
        },
        {
            "heading": "4.6 Qualitative Analysis",
            "text": "We illustrate a prediction example from the QVHighlights dataset by our DemaFormer in Figure 1, 2 and 5. We observe that our model correctly localizes target moments with respect to the user query. Our predicted salience scores also align with the groundtruth scores, which are measured by averaging the three annotated scores in the dataset. In addition, we also utilize t-SNE to visualize the moment-query representations of the example in Figure 1. We realize that the representations of the target localizations stay separately from the remaining ones, whereas those from the UMT model do mingle together. This could explain the accurate localilzation of DemaFormer and verifies the effec-\ntive modeling of the proposed DEMA mechanism combined with the energy-based modeling. We provide more examples in Appendix F."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose DemaFormer, a novel neural architecture for the temporal language grounding (TLG) task. By leveraging the exponential moving average approach with a damping factor, DemaFormer is capable of incorporating local dependencies among moment-query localizations. Additionally, we propose an energy-based strategy to explicitly model localization distribution. On four public benchmarks for the TLG task, our method is effective and outperforms state-of-the-art approaches with a significant margin."
        },
        {
            "heading": "6 Limitations",
            "text": "Our framework requires negative sampling via the Langevin dynamics equation. This incurs additional compute cost while training the language grounding model. Also, although we propose general methods to enhance the grounding performance, we have not studied their impact in crossdomain scenarios, where the model is trained upon one domain (e.g. skiing videos) and tested upon another (e.g. skating videos). We leave these gaps as future work to optimize our framework in more diverse contexts and use cases."
        },
        {
            "heading": "7 Acknowledgement",
            "text": "This research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG3-PhD2023-08-051T). Thong Nguyen is supported by a Google Ph.D. Fellowship in Natural Language Processing."
        },
        {
            "heading": "A Gradient of the Energy-Based Negative Log-Likelihood Objective",
            "text": "We have the specification of the distribution of the moment-query representations:\np\u03b8(o) = exp(\u2212E\u03b8(o))\nZ\u03b8 , (32)\nZ\u03b8 = \u222b exp(\u2212E\u03b8(o))do. (33)\nWe differentiate the negative log-likelihood of the representation:\n\u2212 \u03b4 log(p\u03b8(o)) \u03b4\u03b8\n(34)\n= \u2212 \u03b4 \u03b4\u03b8 log\n( 1\nZ\u03b8 exp(\u2212E\u03b8(o))\n) (35)\n= \u2212 \u03b4 \u03b4\u03b8 (\u2212 logZ\u03b8 \u2212 E\u03b8(o)) (36) = \u2212 \u03b4 \u03b4\u03b8 ( \u2212 log \u222b exp(\u2212E\u03b8(o))do\u2212 E\u03b8(o) ) (37)\n= \u2212 1 Z\u03b8\n(\u222b exp(\u2212E\u03b8)\n\u03b4E\u03b8 \u03b4\u03b8\ndo ) + \u03b4E\u03b8(o)\n\u03b4\u03b8 (38) = \u2212 ( \u03b4E\u03b8(o \u2032)\n\u03b4\u03b8 ) o\u2032\u223cp\u03b8 + \u03b4E\u03b8(o) \u03b4\u03b8 (39)\n= \u2207\u03b8E\u03b8(o+)\u2212\u2207\u03b8E\u03b8(o\u2212). (40)\nWe obtain the gradient formulation for the energy-based modeling component."
        },
        {
            "heading": "B Hyperparameter Settings",
            "text": "Our hidden dimension for the key tensor is dK = 256. Regarding our energy-based models (EBM), we adapt K = 100 and \u03b3 = 0.1 for all datasets. For the training procedure, we adopt \u03bb1 = 13 , \u03bb2 = 0.01, \u03bb3 = 1 3 , \u03bbNLL = 0.1, and \u03b1min = 0.1. To extract positive samples to train the EBMs, we use \u03c1 = 4, 1, 1, and 0.4 for the QVHighlights, Charades-STA, YouTube Highlights, and TVSum datasets, respectively. During testing, we adopt Lm = 10 based on the validation performance. The number of DemaFormer encoder and decoder layers are set as Ne = Nd = 2. For all training settings, we utilize the Adam optimizer with learning rate 1e\u2212 3 and weight decay 1e\u2212 4. We train our DemaFormer model with batch size 32 for 200 epochs on the QVHighlights, batch size 8 for 100 epochs on the Charades-STA, batch size 4 for 100 epochs on the YouTube Highlights, and batch size 1 for 500 epochs on the TVSum dataset, respectively."
        },
        {
            "heading": "C Localization Losses",
            "text": "Following (Liu et al., 2022; Lei et al., 2021), we conduct experiments to study the effect of localization losses on our DemaFormer architecture. We define Ls = \u2212 1Lm Lm\u2211 i=1 s\u0302i to be the salience loss term,\nLc = 1Lm Lm\u2211 i=1 ||ci \u2212 c\u0302i|| to be the center loss term, Lw = 1Lm Lm\u2211 i=1 ||wi \u2212 w\u0302i|| the width loss term, and\nLco = 1Lm Lm\u2211 i=1\n||coi \u2212 c\u0302oi|| the center offset loss term. Because the salience, center and width terms are mandatory, we justify the necessity of the center offset term. As can be seen from Table 7, with the center offset term the localization scores increase from 54.29% to 58.25% of mAP@0.5, and from 40.97% to 43.94% of Rank 1@0.7. This demonstrates that the center offset term helps our DemaFormer architecture predict the localizations more precisely."
        },
        {
            "heading": "D Choice of Activation Functions",
            "text": "In this appendix, we adopt different activation functions for our DEMA attention in Section 3.1 and compare their performances. In detail, we experiment with the Tanh (Dubey et al., 2022), ReLU (Dubey et al., 2022), and GELU functions (Hendrycks and Gimpel, 2016). We report the temporal language grounding performance with these activation functions on the QVHighlights dataset in Table 8. We observe that DemaFormer exhibits negligible performance fluctuation. These results demonstrate the robustness of our proposed DemaFormer with respect to the choice of activation functions."
        },
        {
            "heading": "E Specification of Energy Functions",
            "text": "We provide the formulation of energy functions we experiment with in Table 6.\n\u2022 Element-wise cosine similarity:\nE\u03b8(od,i) = \u2212 1\nLq Lv+Lq\u2211 j=Lv+1 oe,i \u00b7 oe,j |oe,i| \u00b7 |oe,j | . (41)\n\u2022 Pooling-based cosine similarity:\nq = MaxPool{oe,j}Lv+Lqj=Lv+1, (42)\nE\u03b8(od,i) = \u2212 oe,i \u00b7 q\n|oe,i| \u00b7 |q| . (43)\n\u2022 Salience score: E\u03b8(od,i) = \u2212s\u0302i. (44)"
        },
        {
            "heading": "F More prediction examples",
            "text": "In this appendix, we present more predictions of our DemaFormer model in Figure 6, 7, and 8."
        }
    ],
    "title": "DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding",
    "year": 2023
}