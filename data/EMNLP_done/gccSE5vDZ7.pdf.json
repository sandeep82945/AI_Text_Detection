{
    "abstractText": "Automated text simplification aims to produce simple versions of complex texts. This task is especially useful in the medical domain, where the latest medical findings are typically communicated via complex, technical articles. This creates barriers for laypeople seeking access to up-to-date medical findings, consequently impeding progress on health literacy. Most existing work on medical text simplification has focused on monolingual settings, with the result that such evidence would be available only in just one language (most often, English). This work addresses this limitation via multilingual simplification, i.e., directly simplifying complex texts into simplified texts in multiple languages. We introduce MULTICOCHRANE, the first sentence-aligned multilingual text simplification dataset for the medical domain in four languages: English, Spanish, French, and Farsi. We evaluate fine-tuned and zero-shot models across these languages with extensive human assessments and analyses. Although models can generate viable simplified texts, we identify several outstanding challenges that this dataset might be used to address.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sebastian Joseph"
        },
        {
            "affiliations": [],
            "name": "Kathryn Kazanas"
        },
        {
            "affiliations": [],
            "name": "Keziah Reina"
        },
        {
            "affiliations": [],
            "name": "Vishnesh J. Ramanathan"
        },
        {
            "affiliations": [],
            "name": "Wei Xu"
        },
        {
            "affiliations": [],
            "name": "Byron C. Wallace"
        },
        {
            "affiliations": [],
            "name": "Junyi Jessy Li"
        }
    ],
    "id": "SP:d464f846348c25852737f0e4b77113a65c293081",
    "references": [
        {
            "authors": [
                "Emil Abrahamsson",
                "Timothy Forni",
                "Maria Skeppstedt",
                "Maria Kvist."
            ],
            "title": "Medical text simplification using synonym replacement: Adapting assessment of word difficulty to a compounding language",
            "venue": "Proceedings of the 3rd Workshop on Predicting and",
            "year": 2014
        },
        {
            "authors": [
                "Sweta Agrawal",
                "Marine Carpuat."
            ],
            "title": "Controlling text complexity in neural machine translation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Marco Alfano",
                "Biagio Lenzitti",
                "Giosu\u00e8 Lo Bosco",
                "Cinzia Muriana",
                "Tommaso Piazza",
                "Giovanni Vizzini."
            ],
            "title": "Design, development and validation of a system for automatic help to medical text understanding",
            "venue": "International journal of medical informatics,",
            "year": 2020
        },
        {
            "authors": [
                "Fernando Alva-Manchego",
                "Carolina Scarton",
                "Lucia Specia."
            ],
            "title": "Data-driven sentence simplification: Survey and benchmark",
            "venue": "Computational Linguistics, 46(1):135\u2013187.",
            "year": 2020
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Holger Schwenk."
            ],
            "title": "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond",
            "venue": "Transactions of the Association for Computational Linguistics, 7:597\u2013610.",
            "year": 2019
        },
        {
            "authors": [
                "Tal August",
                "Lucy Lu Wang",
                "Jonathan Bragg",
                "Marti A. Hearst",
                "Andrew Head",
                "Kyle Lo"
            ],
            "title": "2022. Paper plain: Making medical research papers approachable to healthcare consumers with natural language processing",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Bakkelund."
            ],
            "title": "An lcs-based string metric",
            "venue": "An LCS-based string metric.",
            "year": 2009
        },
        {
            "authors": [
                "Chandrayee Basu",
                "Rosni Vasu",
                "Michihiro Yasunaga",
                "Qian Yang."
            ],
            "title": "Med-easi: Finely annotated dataset and models for controllable simplification of medical texts",
            "venue": "AAAI Conference on Artificial Intelligence.",
            "year": 2023
        },
        {
            "authors": [
                "Nancy D Berkman",
                "Stacey L Sheridan",
                "Katrina E Donahue",
                "David J Halpern",
                "Karen Crotty."
            ],
            "title": "Low health literacy and health outcomes: an updated systematic review",
            "venue": "Annals of internal medicine, 155(2):97\u2013107.",
            "year": 2011
        },
        {
            "authors": [
                "Yixin Cao",
                "Ruihao Shui",
                "Liangming Pan",
                "Min-Yen Kan",
                "Zhiyuan Liu",
                "Tat-Seng Chua."
            ],
            "title": "Expertise style transfer: A new task towards better communication between experts and laymen",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "R\u00e9mi Cardon",
                "Natalia Grabar."
            ],
            "title": "French biomedical text simplification: When small and precise helps",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 710\u2013716.",
            "year": 2020
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "William Coster",
                "David Kauchak."
            ],
            "title": "Simple English Wikipedia: A new text simplification task",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 665\u2013669, Portland, Ore-",
            "year": 2011
        },
        {
            "authors": [
                "Ashwin Devaraj",
                "Iain Marshall",
                "Byron Wallace",
                "Junyi Jessy Li."
            ],
            "title": "Paragraph-level simplification of medical texts",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2021
        },
        {
            "authors": [
                "Ashwin Devaraj",
                "William Sheffield",
                "Byron Wallace",
                "Junyi Jessy Li."
            ],
            "title": "Evaluating factuality in text simplification",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7331\u20137345,",
            "year": 2022
        },
        {
            "authors": [
                "Natalia Grabar",
                "R\u00e9mi Cardon."
            ],
            "title": "Clear\u2013simple corpus for medical french",
            "venue": "Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA), pages 3\u20139.",
            "year": 2018
        },
        {
            "authors": [
                "Yue Guo",
                "Wei Qiu",
                "Gondy Leroy",
                "Sheng Wang",
                "Trevor Cohen."
            ],
            "title": "Cells: A parallel corpus for biomedical lay language generation",
            "venue": "arXiv preprint arXiv:2211.03818.",
            "year": 2022
        },
        {
            "authors": [
                "Yue Guo",
                "Weijian Qiu",
                "Yizhong Wang",
                "Trevor A. Cohen."
            ],
            "title": "Automated lay language summarization of biomedical scientific reviews",
            "venue": "ArXiv, abs/2012.12573.",
            "year": 2020
        },
        {
            "authors": [
                "Chao Jiang",
                "Mounica Maddela",
                "Wuwei Lan",
                "Yang Zhong",
                "Wei Xu."
            ],
            "title": "Neural CRF model for sentence alignment in text simplification",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7943\u20137960, On-",
            "year": 2020
        },
        {
            "authors": [
                "Ilona Kickbusch",
                "J\u00fcrgen M Pelikan",
                "Franklin Apfel",
                "Agis D Tsouros."
            ],
            "title": "Health literacy: the solid facts",
            "venue": "World Health Organization. Regional Office for Europe.",
            "year": 2013
        },
        {
            "authors": [
                "Joongwon Kim",
                "Mounica Maddela",
                "Reno Kriz",
                "Wei Xu",
                "Chris Callison-Burch."
            ],
            "title": "BiSECT: Learning to split and rephrase sentences with bitexts",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6193\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Nicholas Kloehn",
                "Gondy Leroy",
                "David Kauchak",
                "Yang Gu",
                "Sonia Colina",
                "Nicole P Yuan",
                "Debra Revere"
            ],
            "title": "Improving consumer understanding of medical text: Development and validation of a new subsimplify algorithm to automatically generate term",
            "year": 2018
        },
        {
            "authors": [
                "Wuwei Lan",
                "Chao Jiang",
                "Wei Xu."
            ],
            "title": "Neural semi-Markov CRF for monolingual word alignment",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Chenxi Liu",
                "Dan Wang",
                "Chaojie Liu",
                "Junnan Jiang",
                "Xuemei Wang",
                "Haihong Chen",
                "Xin Ju",
                "Xinping Zhang."
            ],
            "title": "What is the meaning of health literacy? a systematic review and qualitative synthesis",
            "venue": "Family medicine and community health, 8(2).",
            "year": 2020
        },
        {
            "authors": [
                "Louis Martin",
                "Angela Fan",
                "\u00c9ric de la Clergerie",
                "Antoine Bordes",
                "Beno\u00eet Sagot."
            ],
            "title": "MUSS: Multilingual unsupervised sentence simplification by mining paraphrases",
            "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages",
            "year": 2022
        },
        {
            "authors": [
                "Gustavo Paetzold",
                "Fernando Alva-Manchego",
                "Lucia Specia."
            ],
            "title": "MASSAlign: Alignment and annotation of comparable documents",
            "venue": "Proceedings of the IJCNLP 2017, System Demonstrations, pages 1\u20134, Tapei, Taiwan. Association for Computational",
            "year": 2017
        },
        {
            "authors": [
                "Atharva Phatak",
                "David W Savage",
                "Robert Ohle",
                "Jonathan Smith",
                "Vijay Mago."
            ],
            "title": "Medical text simplification using reinforcement learning (teslea): Deep learning\u2013based text simplification approach",
            "venue": "JMIR Medical Informatics, 10(11):e38095.",
            "year": 2022
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013 191, Brussels, Belgium. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Justus J Randolph."
            ],
            "title": "Free-marginal multirater kappa (multirater k [free]): An alternative to fleiss\u2019 fixed-marginal multirater kappa",
            "venue": "Online submission.",
            "year": 2005
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Juliane Ried."
            ],
            "title": "About translation at cochrane",
            "venue": "https://documentation.cochrane.org/ display/TH/About+translation+at+Cochrane.",
            "year": 2023
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Noah Constant",
                "Jan Botha",
                "Aditya Siddhant",
                "Orhan Firat",
                "Jinlan Fu",
                "Pengfei Liu",
                "Junjie Hu",
                "Dan Garrette",
                "Graham Neubig",
                "Melvin Johnson."
            ],
            "title": "XTREME-R: Towards more challenging and nuanced multilingual evaluation",
            "venue": "Proceedings",
            "year": 2021
        },
        {
            "authors": [
                "Michael Ryan",
                "Tarek Naous",
                "Wei Xu."
            ],
            "title": "Revisiting non-English text simplification: A unified multilingual benchmark",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4898\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Thomas Scialom",
                "Paul-Alexis Dray",
                "Sylvain Lamprier",
                "Benjamin Piwowarski",
                "Jacopo Staiano."
            ],
            "title": "MLSUM: The multilingual summarization corpus",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Chantal Shaib",
                "Millicent Li",
                "Sebastian Joseph",
                "Iain Marshall",
                "Junyi Jessy Li",
                "Byron Wallace."
            ],
            "title": "Summarizing, simplifying, and synthesizing medical evidence using GPT-3 (with varying success)",
            "venue": "Proceedings of the 61st Annual Meeting of the As-",
            "year": 2023
        },
        {
            "authors": [
                "Advaith Siddharthan."
            ],
            "title": "A survey of research on text simplification",
            "venue": "ITL-International Journal of Applied Linguistics, 165(2):259\u2013298.",
            "year": 2014
        },
        {
            "authors": [
                "Neha Srikanth",
                "Junyi Jessy Li."
            ],
            "title": "Elaborative simplification: Content addition and explanation generation in text simplification",
            "venue": "Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 5123\u20135137, Online. Association",
            "year": 2021
        },
        {
            "authors": [
                "Teerapaun Tanprasert",
                "David Kauchak."
            ],
            "title": "Flesch-kincaid is not a text simplification evaluation metric",
            "venue": "Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 1\u201314, Online. Association for",
            "year": 2021
        },
        {
            "authors": [
                "Brian Thompson",
                "Philipp Koehn."
            ],
            "title": "Vecalign: Improved sentence alignment in linear time and space",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Jan Trienes",
                "J\u00f6rg Schl\u00f6tterer",
                "Hans-Ulrich Schildhaus",
                "Christin Seifert."
            ],
            "title": "Patient-friendly clinical notes: Towards a new text simplification dataset",
            "venue": "Proceedings of the Workshop on Text Simplification, Accessibility, and Readability (TSAR-2022), pages",
            "year": 2022
        },
        {
            "authors": [
                "Hoang Van",
                "David Kauchak",
                "Gondy Leroy."
            ],
            "title": "AutoMeTS: The autocomplete for medical text simplification",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 1424\u20131434, Barcelona, Spain (Online). International",
            "year": 2020
        },
        {
            "authors": [
                "Laurens Van den Bercken",
                "Robert-Jan Sips",
                "Christoph Lofi."
            ],
            "title": "Evaluating neural text simplification in the medical domain",
            "venue": "The World Wide Web Conference, pages 3286\u20133292.",
            "year": 2019
        },
        {
            "authors": [
                "Tu Vu",
                "Aditya Barua",
                "Brian Lester",
                "Daniel Cer",
                "Mohit Iyyer",
                "Noah Constant."
            ],
            "title": "Overcoming catastrophic forgetting in zero-shot cross-lingual generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Kristian Woodsend",
                "Mirella Lapata."
            ],
            "title": "Learning to simplify sentences with quasi-synchronous grammar and integer programming",
            "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 409\u2013420, Edinburgh,",
            "year": 2011
        },
        {
            "authors": [
                "Wei Xu",
                "Chris Callison-Burch",
                "Courtney Napoles."
            ],
            "title": "Problems in current text simplification research: New data can help",
            "venue": "Transactions of the Association for Computational Linguistics, 3:283\u2013297.",
            "year": 2015
        },
        {
            "authors": [
                "Wei Xu",
                "Courtney Napoles",
                "Ellie Pavlick",
                "Quanze Chen",
                "Chris Callison-Burch."
            ],
            "title": "Optimizing statistical machine translation for text simplification",
            "venue": "Transactions of the Association for Computational Linguistics, 4:401\u2013415.",
            "year": 2016
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "BERTScore: Evaluating text generation with BERT",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Xingxing Zhang",
                "Mirella Lapata."
            ],
            "title": "Sentence simplification with deep reinforcement learning",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 584\u2013 594, Copenhagen, Denmark. Association for Compu-",
            "year": 2017
        },
        {
            "authors": [
                "Zhemin Zhu",
                "Delphine Bernhard",
                "Iryna Gurevych."
            ],
            "title": "A monolingual tree-based translation model for sentence simplification",
            "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353\u20131361, Beijing,",
            "year": 2010
        },
        {
            "authors": [
                "Zhang",
                "Lapata",
                "Xu"
            ],
            "title": "2015), we are not aware of evaluation work for sentence alignment algorithms on medical texts. The construction of MC-NOISY allows us to create a tiered version of MULTICOCHRANE",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Important findings in medicine are typically presented in technical, jargon-laden language in journal articles or reviews, which is difficult for laypeople to understand. This impedes transparent and fair access to critical medical information and ultimately hinders health literacy, which is \u201cone of the most promising and cost-effective approaches to overcome the Non-Communicable Disease challenge\u201d (Liu et al., 2020).\nText simplification models which automatically transform complex texts into simpler versions understandable by lay readers (Siddharthan, 2014; Alva-Manchego et al., 2020) have emerged as a promising means of providing wider access to published medical evidence. Recent work on simplification has fine-tuned large pre-trained models (Van\net al., 2020; Cardon and Grabar, 2020; Devaraj et al., 2021; Guo et al., 2022; Trienes et al., 2022; Basu et al., 2023), explored reinforcement learning (Phatak et al., 2022), and evaluated zero-shot performance via prompting (August et al., 2022).\nHowever, this work has so far exclusively considered monolingual text simplification. Consequently, we do not know how well models perform when the simplified text is not in the same language as the original, complex text. This limits the (potential) availability of simplified information to a few high-resource languages\u2014especially for the medical domain\u2014and leads to equity issues such that individuals who do not speak English will still face a high barrier to information access.1 Work\n1While machine translation, to a certain extent, may be able\nin this direction is impeded by a lack of data. The largest resources in text simplification are not in the medical domain, and parallel corpora for medical simplification only exist in single languages, namely English (Devaraj et al., 2021; Guo et al., 2022; Basu et al., 2023), French (Grabar and Cardon, 2018; Cardon and Grabar, 2020), and German (Trienes et al., 2022).\nThis paper advances the state of multilingual medical simplification, in which complex texts are to be directly simplified into target languages. Our contributions are as follows. We introduce MULTICOCHRANE (Figure 1), the first parallel dataset for medical text simplification across multiple languages: English, Spanish, French, and Farsi. MULTICOCHRANE contains aligned sentence pairs sourced from the Cochrane Library of Systematic Reviews,2 which is a library of meta-analyses of treatment effectiveness.\nThese review articles include both a technical abstract and a plain-language summary (PLS), from which we derive the two subsets of MULTICOCHRANE: (1) MC-CLEAN, 101 technical abstracts with expert-annotated manual alignments first derived in English, then semi-automatically aligned to other languages (partially verified by bilingual speakers). MC-CLEAN contains \u223c5k sentence pairs across all 4 languages. (2) MC-NOISY, a larger but noisier subset created with an automatic sentence alignment model that was trained on MCCLEAN. MC-NOISY is sourced from around 7.8K medical reviews, with \u223c100K sentence pairs across languages.\nMULTICOCHRANE enables systematic evaluations of medical text simplification models. Here we evaluate a range of large pre-trained language models in both zero-shot and fine-tuned settings on the task of text simplification across four languages. In addition to automatic evaluations, we report human assessments covering simplicity, fluency and factuality (Devaraj et al., 2022). We also report the correlation between automatic metrics and these human assessments, which we find to be mostly weak. Our results show that while pre-trained models are effective at simplification in English, their abilities degrade significantly on other languages,\nto overcome this limitation, it is not ideal due to variability in translation quality across languages (Ruder et al., 2021) and other issues such as cost and interpretability as discussed in Vu et al. (2022). This paper evaluates a simplify-then-translate pipeline.\n2https://www.cochranelibrary.com\nwhere they tend to introduce factuality errors. GPT3 (davinci; zero-shot) yields outputs that are comparatively factually accurate, but which tend to be extractive and so not adequately simplified. Outputs from Flan-T5 (Base; zero-shot) significantly degrade in quality when targeting languages other than English, producing many factual and fluency errors. We also analyze the approach of translating English simplifications to other languages, which in many instances is able bypass these issues.\nWe publicly release MULTICOCHRANE, model outputs, and all human judgments collected for this work (https://github.com/SebaJoe/ MultiCochrane), hoping to motivate future work on multilingual medical text simplification to advance model performance across languages."
        },
        {
            "heading": "2 Related Work",
            "text": "The largest resources used to train automatic simplification models are two general domain corpora: the Wikipedia-Simple Wikipedia corpus (Zhu et al., 2010; Woodsend and Lapata, 2011; Coster and Kauchak, 2011; Jiang et al., 2020), and the Newsela corpus (Xu et al., 2015; Jiang et al., 2020). This paper focuses on simplification in the medical domain, which is important if we are to bridge the information gap exemplified by low medical literacy levels worldwide (Kickbusch et al., 2013).\nMedical Simplification Datasets. While there have recently been increasing efforts to create parallel corpora for medical text simplification in English (Van den Bercken et al., 2019; Cao et al., 2020; Devaraj et al., 2021; Guo et al., 2022; Basu et al., 2023), data in other languages remain scarce. Grabar and Cardon (2018) constructed the CLEAR dataset in French, part of which is derived from 13 Cochrane articles; Trienes et al. (2022) introduced a dataset on German consisting of clinical notes. Other prior work in non-English medical text simplification has focused on lexical simplification (Abrahamsson et al., 2014; Kloehn et al., 2018; Alfano et al., 2020), where the primary focus was on substituting synonyms and semantically related terms. In terms of data sourcing, our work is most similar to Devaraj et al. (2021) (which is in English only). However, their work derived roughlyaligned paragraphs consisting of specific sections (methods and conclusions only) of the Cochrane abstracts and PLS; by contrast, we derive manual and automatically aligned sentences for full abstracts.\nMultilingual Simplification Datasets. Multilingual datasets have been introduced for the task of summarization. The MLSUM dataset (Scialom et al., 2020) is a large summarization corpus containing article/summary pairs in five languages, sourced from newspaper articles. General-domain non-English datasets for simplification also exist as extensively surveyed by Ryan et al. (2023). Notably, MUSS (Martin et al., 2022) used Common Crawl to mine millions of sequence pairs in English, Spanish, and French. The significant difference between MUSS and our dataset is MUSS\u2019s lack of cross-lingual alignment, i.e., alignment of sequence pairs from one language to another. Agrawal and Carpuat (2019) derived noisily aligned English-Spanish data from the Newsela corpus (Xu et al., 2015). However, to date, there is no aligned dataset for text simplification where one source sentence is paired with target sentences in multiple languages simultaneously as in our work.\n3 The MULTICOCHRANE Corpus\nWe source MULTICOCHRANE from publicly available technical abstracts and plain-language summaries of Cochrane systematic reviews; these are comparable texts, though not parallel (Devaraj et al., 2021). These abstracts and PLS exist in several languages (detailed in Sections 3.1.2 and 3.2), aligned almost one-to-one with their English counterparts. This allows annotations and alignments to be easily adapted to create a multilingual dataset. In total, we use 7,755 abstract-PLS pairs from the Cochrane Library. We first create sentence-aligned MC-CLEAN (Section 3.1) of 101 abstract-PLS pairs using a mixture of manual alignments for English and semi-automatic alignments with partial verification for other languages. The rest of the abstracts were used for the creation of the large-scale noisy MC-NOISY (Section 3.2) by automatic alignments and filtering. The total number of sentences in all subsets are shown in Table 1.\n3.1 Clean Alignments: MC-CLEAN\nFor MC-CLEAN, we first create a manually aligned subset of 101 abstracts for English. We then automatically align for other languages by exploiting Cochrane\u2019s natural 1-1 sentence alignment from English to other languages; this alignment was verified by annotators on a sizable sample of the data. Our annotation team consists of 5 trained linguistics students with native English proficiency and\nDataset en\u2192en en\u2192es en\u2192fr en\u2192fa\nMC-NOISY 60,058 58,235 34,604 30,822 + filtering 29,703 29,025 17,033 15,389\nMC-CLEAN 1,632 1,602 1,111 646 training set 1,136 1,084 798 424\n+ filtering 329 326 227 125"
        },
        {
            "heading": "3.1.1 English Annotation",
            "text": "Inter-annotator Agreement. We used the aforementioned 25 articles to calculate inter-annotator agreement based on the F1 score, following prior research on word and sentence alignment (Jiang et al., 2020; Lan et al., 2021).4 For each annotator, their alignments were evaluated against the majority vote of alignments from other annotators.\nOverall, the macro-average F1 score across annotators was 0.89, indicating high agreement for the alignment task. Non-overlapping alignments are rare; a major source of disagreement between annotators was related to multiple alignments made to the same simple sentence (i.e., multi-alignments) that were either captured by some annotators but excluded by others. Since the content in the abstracts is highly technical and in some cases contrasted significantly in style and vocabulary with the plainlanguage summary, annotators had difficulties in determining whether some sentences conveyed similar meaning. Nevertheless, annotators were in general conservative, only aligning sentences when they were confident in the alignments. Therefore, there is a low prevalence of incorrect alignments. The overall alignments are high-precision. Annotators typically used ANNO-VIEWER\u2019s sentence similarity sorting function to verify their alignments or to identify additional multi-alignments.\nDataset Summary. Table 2 presents additional statistics on the aligned and unaligned sentences in both complex and simple texts in the English portion of MC-NOISY. For complex texts (i.e., technical abstracts), an unaligned sentence indicates that its information is not present (i.e., deleted) in the simplified texts (i.e., plain-language summaries). On average, just over 60% of sentences in complex texts are deleted, indicating that most of the core information from complex texts are retained\n4Alignment requires N (sentences) by M (sentences) comparisons, while only a small amount of which being positive (i.e., aligned). Thus F1 is more commonly used than correlation measurements for calculating agreement for this task.\nin simple texts. In PLS, an unaligned sentence indicates added information not present in the complex version. We observe that an overwhelming majority of added information elaborates on concepts or defines terms (Srikanth and Li, 2021). The average elaboration ratio (the ratio of unaligned to total sentences in a PLS) is less than half of the average deletion ratio (ratio of unaligned to total sentences in a technical abstract), indicating that simplified texts tend to be mostly concise with this core information. The average token compression ratio (ratio of simple to complex tokens) show the presence of longer simplified sentences."
        },
        {
            "heading": "3.1.2 Multilingual Data",
            "text": "The Cochrane Library provides abstracts and PLS in multiple languages via translations that have been produced through a combination of volunteer and professional work, as well as human-verified machine translation (Ried, 2023). To create the multilingual portion of our dataset, i.e., pairs of semantically equivalent (source, target) sentences where source = English abstract, target \u2208 PLS in {en, es, fr, ...}, we use the English PLS as a bridge to align the English abstract with PLS in other languages. The PLS of non-English languages mostly correspond 1-1 sentence-wise to the English PLS.\nWe use a sentence aligner that combines LASER multilingual sentence embeddings (Artetxe and Schwenk, 2019) with a dynamic programmingbased alignment algorithm (Thompson and Koehn, 2019) to align sentences in the English versions to those in other languages. We did this for the entire set of 7,755 abstract/PLS pairs. Multilingual sentence pairs in MC-CLEAN consist of these alignments that belong to the 101 articles manually aligned for English. The other 7,654 articles were used to create MC-NOISY (Section 3.2).\nHuman Verification. To verify that the multilingual sentence alignments across different languages of the PLS are valid, we asked 3 of our bilingual annotators to evaluate a random sample of 400 sentence pairs to verify that each alignment is a direct translation. Each annotator was highly proficient in both the languages of the alignments being verified. Annotators found zero instances of misalignment. We also analyzed the number of sentences of the English abstract/PLS and their multilingual counterparts to find any instances where is a huge mismatch. We did not find any such instances. The data did include instances where\none or two sentences in the article may have been split or combined in their multilingual versions. The DP-based alignment algorithm that was used took care of these exceptions. These assessments, combined with information about the Cochrane Library\u2019s methodology in making these translations, instills high confidence that the derived multilingual sentence alignments are valid.\nDataset Summary. While Cochrane provides a large amount of diverse multilingual data, every article is not mapped to every available language. Consequently, the number of articles available for each language varies. The distribution of these articles across language is shown in Figure 2.\nFor this work, we selected the top three most frequent non-English languages \u2014 Spanish, French, and Farsi \u2014 to use for training and evaluation in Section 5. We report the resulting number of paired sentences in each language in Table 1.\n3.2 Fully Automated Alignments: MC-NOISY\nWhile MC-CLEAN provides clean and accurate alignments across multiple languages, human alignment of medical documents is challenging to scale. Fortunately, MC-CLEAN enables us to accurately evaluate a variety of automatic sentence alignment methods; a good automatic alignment method will help create silver-standard alignments expanding over the entirety of the Cochrane dataset. This is especially important for multilingual medical simplification, where the number of annotated alignments is significantly lower for non-English languages.\nAutomatic Alignment on English Data The automatic alignments for MC-NOISY were derived using a BERT-based CRF alignment model (Jiang et al., 2020). This method outperformed other alignment methods when evaluated on a subset of MC-CLEAN. Further details about these experiments can be found in Appendix E.\nMultilinguality We use the same 1-1 alignment property between English and other languages in Cochrane abstracts and PLS described above (Section 3.1.2) to derive the multilingual portion of MC-NOISY from the English data described in Section 3.2. As shown in Figure 2, the distribution of the articles and sentence pairs across language in the noisy dataset is similar to that of the humanannotated dataset."
        },
        {
            "heading": "4 Filtering",
            "text": "Although the CRF model handles unaligned sentences by default, the resulting data remains noisy. We therefore adapt the method described in Kim et al. (2021), initially used in the context of sentence-splitting, to further filter misalignments in MC-NOISY. Ultimately, this method was applied to the entire training set, which includes the entirety of MC-NOISY as well as the portion of MC-CLEAN not used for testing or validation. This method can be described as follows: For each sentence pairing in the training set, we derive the set of lemmatized tokens in the complex and simple sentence denoted by Lc and Ls respectively. If the proportion of the overlapping tokens from the simplified sentence exceeds a threshold r, it is included in the filtered dataset: r = |Lc \u2229 Ls|/|Ls|.\nWe used r = 0.5 to strike a balance between the number of potential misalignments in the dataset and the extractiveness of the data. Overall, as evident in Table 1, this filtering process reduced the dataset size by about half. Filtering reduced the MC-CLEAN portion by a larger proportion, indicating that human data is less lexically similar as opposed the noisy data."
        },
        {
            "heading": "5 Evaluation of Simplification Models",
            "text": "MULTICOCHRANE enables extensive evaluation of multilingual sentence simplification systems simultaneously across 4 languages. In this section we present the first study of this kind (to our knowledge), assessing zero-shot and fine-tuned models (Section 5.1) with both automatic metrics (Sec-\ntion 5.2) and human evaluation (Section 5.3) for medical text simplification."
        },
        {
            "heading": "5.1 Models",
            "text": "We experiment with zero-shot and fine-tuned models, as well as a simplify-then-translate pipeline."
        },
        {
            "heading": "Zero-shot models:",
            "text": "\u2022 GPT-3 zero-shot. we evaluate zero-shot simplifications generated by the GPT-3-davinci-003 model using a prompt similar to that used in August et al. (2022), adapted for multilinguality:\nMy fifth grader asked me what this sentence means: [TEXT TO SIMPLIFY] I rephrased it for him in [LANG], in plain language a fifth grader can understand:\n\u2022 GPT-3 simplify-then-translate. GPT-3 has been shown to have strong performance on English simplification for medical texts (Shaib et al., 2023). Thus, we evaluate a pipeline model first using GPT-3 for English simplification using the above prompt, and then translating the simplified text to other languages with a strong translation model (Google\u2019s Cloud Translation API).\n\u2022 Flan-T5 zero-shot. We also evaluate zero-shot Flan-T5base (Chung et al., 2022) performance. We used a simple prompt, specifically prepending \u201cSimplify this sentence:\u201d to the input (complex) sentence.5 When simplifying to languages other than English, the prompt is changed to\n\u201cSimplify this sentence in [LANG]:\u201d. Unfortunately, we were unable to generate simplifications in Farsi using Flan-T5, so we only evaluated this system on English, Spanish, and French."
        },
        {
            "heading": "Fine-tuned models:",
            "text": "\u2022 mT5 We fine-tune separate mT5base models (Xue et al., 2021) on different language pairs: (English, English), (English, Spanish), (English, French), and (English, Farsi).\n\u2022 Flan-T5 fine-tuned We further evaluate finetuned versions of Flan-T5base for each language pair, following the setup of its zero-shot counterpart. This system also failed to generate Farsi outputs.\n5Preliminary experiments showed that August et al. (2022)\u2019s prompt, though worked well on GPT-3, did not yield good results with Flan-T5.\ntrain test validation\nen 7728 (61194/30032) 22 (395) 5 (83) es 7643 (59319/29351) 22 (395) 5 (80) fr 4709 (35402/17260) 11 (214) 4 (65) fa 3117 (31246/15514) 8 (148) 3 (56)"
        },
        {
            "heading": "5.2 Automatic Evaluation",
            "text": "tance metric has an inverse relationship with the actual least common subsequence (lower means more extractive).\nWe do not include Flesch-Kincaid as a metric, for reasons stated in Tanprasert and Kauchak (2021), which provides a compelling analysis on how easily it can be misinterpreted, and instead opt for human evaluation of simplicity in Section 5.3.\nResults. Results of these evaluations are presented in Table 4. The data in this table show a common trend in that filtering (Section 3.2) largely improves performance, indicating that filters improve data quality. A very visible trend in this data, is that across the board, with the exception of the GPT results, English simplifications vastly outperform those of other languages. This probably due to the vast resource gap between English and the other languages, as well a possible bias towards English in the pre-trained mT5-base model.\nWe observe exceedingly low BLEU scores when the simplified version is used as reference, compared to the complex version. Since BLEU is lexical-centric, this reveals that models tend to translate rather than simplify. Simplified sentences in the Cochrane dataset frequently featured insertions and elaborations, creating at times an information mismatch between the model outputs and the corresponding reference simplifications. Moreover, the style in which PLS were written varied greatly (Devaraj et al., 2021).\nAnother attribute that is of importance when judging simplification is the level of extractiveness, or how much the output copied the input. Based on the LCS metric, it seems that while GPT-3 produces less extractive outputs than English mT5, for the other languages the opposite is true. We discuss this further in Section 5.3.\nExtractiveness and poor multilingual simplifications are also evident in the results for zero-shot Flan-T5 generations. For multilingual simplifications, in particular, fine-tuning significantly increased the LCS distance while improving BLEU and BERTScore metrics, indicating higher quality and less extractive simplifications. Interestingly, English simplifications generated by both settings have a significantly lower LCS distance compared those generated by other systems. Why this occurs is unclear to us and requires further analysis.\nThe simplify-translate approach to simplification in different languages elicited similar results as its English counterpart and clearly produces less\nextractive generations compared to zero-shot multilingual simplification. In terms of LCS and SARI, this approach resulted in similar scores as those produced by fine-tuned models. While it is appears that the simplify-translate approach improved upon the zero-shot multilingual performance for GPT, it is difficult to draw conclusions about its performance relative to fine-tuned models using automatic metrics alone."
        },
        {
            "heading": "5.3 Human Evaluation",
            "text": "We perform human evaluation of the factuality, linguistic quality, and simplicity of model outputs. We also evaluate the reference simplifications. 100 outputs were evaluated for each system. While the alignment may be sound, the references may still contain excess deletions or inconsistencies with the original text; prior work Devaraj et al. (2022) found similar phenomena with the Newsela and Wiki datasets. Annotators were blinded to the sys-\ntem ID (or whether they are evaluating the manually aligned pairs from the texts themselves).\nMetrics. (1) Factual faithfulness: We measure overall factuality using a 0-2 scale, where 0 indicates no factual issues and 2 indicates severe factual issues. (2) Fluency: We rate linguistic fluency of outputs using a scale of 0-2, where 0 indicates fluent output and 2 indicates severe linguistic issues. (3) Simplicity: Simplicity is assessed on a scale of -2 to 2, where -2 indicates heavy oversimplification, 0 indicates ideal simplification, and 2 indicates heavy under-simplification. The specific criteria are further described in Appendix A.\nResults. The human evaluation results presented in Table 5 agree with many of the automatic metrics. First, annotators found English outputs from the mT5 models to be more factual and fluent than those from the other languages. Similarly, factuality and fluency also improved with the filtered dataset for the most part (as was also indicated by the automatic metrics). This is probably due to filtering removing less extractive sentence pairs from the training set. However, for French and Farsi, filtering slightly worsened the factuality and fluency, perhaps due to the fewer number of examples in the data. For simplicity, with the exception of English, filtering also seems to make outputs simpler.\nThere is a stark difference between the GPT3 and the mT5 outputs. GPT-3 produced significantly more faithful and fluent text while mT5 outputs were deemed simpler with the exception of English. However, qualitative feedback from annotators suggests that non-English GPT-3 outputs are skewed due to a severe degree of extractiveness; in some cases, the output is simply a word-for-word translation of the input. This inflates the factuality and fluency scores, but this level of extractiveness is manifest in the simplicity scores; GPT-3 does proper simplification only for the English to English case.\nThe trends in the results for Flan-T5 mirror those for GPT-3. When comparing zero-shot generations to fine-tuned generations, it is clear that Flan-T5 is limited in its ability to perform multilingual simplification. Not only are generations less simplified, but they frequently have factual and fluency errors.\nThe simplify-translate approach proved to be fairly strong compared to other systems which directly performed multilingual simplification. The current state of direct multilingual simplification\nin these large language models seems to be not yet surpassing monolingual simplification combined with machine translation. However, as discussed in Section 1, a pipeline system does come with its drawbacks in terms of cost and robustness.\nAnnotators also evaluated the reference labels of each complex input from the dataset. Overall, with the exception of Farsi, these references were deemed relatively fluent and simplified. However, due to insertions and deletions of information that naturally occur in the data, factuality errors were detected by annotators; these findings echo those in Devaraj et al. (2022).\nExamples of common errors. One common error that was found in outputs from all evaluated systems were mistranslated acronyms. Medical acronyms often differ from one language to another. However, many systems are limited in their ability to properly translate this technical knowledge. Usually, the acronym is either not translated, or translated incorrectly, often randomly. The following example in French was generated by mT5 fine-tuned on the unfiltered version of MC-NOISY.\nInput complex sentence: Stress and a sedentary lifestyle are major determinants of cardiovascular disease (CVD). Output: Le stress et le r\u00e8glement inactif sont des causes importantes de maladies cardiovasculaires (AVC) les plus courants. Gloss: The stress and inactive regulation are the most common important causes of cardiovascular disease (AVC)\nThe correct translation of CVD, or cardiovascular disease, in French is MCV, not AVC (French acronym for stroke).\nFluency issues were especially common in outputs generated by mT5 and Flan-T5. In the case of Flan-T5, in particular, misspellings and incorrect applications of gendered language are quite prevalent. Consider the following example in Spanish generated by Flan-T5:\nInput complex sentence: The rates were 24.5 for the sponge and 10.9 for the diaphragm in the UK trial. Output: Las tasas de asistencia respiratoria fueron de un 2,4 para la secreci\u00f3n del vejiga y 10,9 para el diafragmata. Gloss: The rates of respiratory assistance were 2.4 for the secretion of the bladder and 10.9 for the diaphragm.\nThis example has a number of errors. There are many hallucinations that include the insertion and substitution of irrelevant information. However, with respect to fluency, the main issue is the incorrect gender used for the word \u201cvejiga\u201d as well as the misspelling of the word \u201cdiafragma\u201d as \u201cdiafragmata\u201d. The important thing to note here is that\nmost of these errors are localized to technical words or phrases. More work needs to be done to adapt large language models to these niche vocabularies."
        },
        {
            "heading": "6 Conclusions",
            "text": "This paper introduces the first human-aligned multilingual text simplification dataset, MC-CLEAN, for the medical domain for the languages of English, Spanish, French, and Farsi. We further expanded this dataset using automatic alignments to create a much more vast but noisier dataset, MC-NOISY, that could be used for training language models. We also performed an evaluation of multilingual simplification for various systems, testing some of their zero-shot capabilities as well their performance when fine-tuned on MC-NOISY."
        },
        {
            "heading": "Limitations",
            "text": "MULTICOCHRANE has a few limitations. Firstly, for a given article, there does not exist a consistent set of multilingual versions for that article. As exemplified in Figure 2, the number of articles in which there exists a multilingual version varies depending on the language. This uneven distribution does favor more well resourced languages, with Farsi being a notable exception.\nLooking at the wider scope of text simplification, sentence-level text simplification does have a few drawbacks. Especially within the medical domain, contextual information is helpful for generating good simplifications. In MC-CLEAN elaborative information was often left unaligned. Simplifying at a paragraph-level or a document-level could have made use of this additional data to generate more useful simplifications.\nBecause of limitations with the amount of computing capability and time, the largest models available for Flan-T5 and mT5 were not used for evaluation. The models that we used were the best available models that we could have evaluated within our reasonable constraints."
        },
        {
            "heading": "Ethical Concerns",
            "text": "This work does not use any full texts of Cochrane reviews. The research use of publicly available abstracts, including both technical abstracts and multi-lingual plain language summaries, is considered fair use. Note that the Cochrane dataset, in similar capacity as ours, has already been released publicly by prior work (Devaraj et al., 2021; Guo et al., 2020).\nRegarding compensation for annotators, all the annotators were paid a wage of $15 for every hour of work.\nWe recognize the dual use of language models and the possibility of factual errors and hallucinations in their generations, and we believe that this line of work is necessary both to advance technology for social good while also acknowledging these risks. The inaccessibility of medical information and health illiteracy are some of the leading reasons for real health consequences including more hospital admissions, emergency room visits, and poorer overall health (Berkman et al., 2011). Making medical information accessible is one of the best ways to tackle health literacy, and that is the core of what a simplification system is aimed to do. Additionally, medical misinformation is one of the most series issues highlighted in the COVID-19 pandemic; one contributing reason for this is the lack of health literacy among the general public. By simplifying trustworthy evidence, we hope to empower the public with a keener eye for such misinformation. Factual errors is one of the key aspects studied in this work; we perform a thorough evaluation dissecting issues that can come from these models, especially in a multilingual setting. We believe rigorous evaluation, as done in this work, is one of our best tools to demystify language models, and help the community understand the issues at hand. With such understanding, we hope to point to future directions that collectively, we will be able to provide factual, readable, multilingual access to medical texts."
        },
        {
            "heading": "Acknowledgements",
            "text": "We acknowledge Pouya Nekouei for his help with the Farsi language. This research was partially supported by National Science Foundation (NSF) grants IIS-2145479, IIS-2144493 and IIS-2112633, and by the National Institutes of Health (NIH) under the National Library of Medicine (NLM) grant 2R01LM012086."
        },
        {
            "heading": "A Human Evaluation Framework",
            "text": "The following is the exact framework that annotators used for evaluating simplification outputs.\nFactual (faithful to input) Choices - 0-2 Give an overall rating on how factually consistent the model-generated output is on a scale from 0 to 2, where 0 is completely factual, 1 indicates inconsequential factual errors, and 2 indicates severe factual errors.\nExample: Rating 0: Original: We discovered a high percentage of study participants experienced adverse reactions while being treated for ischemic heart disease. Output: We found that many study participants had bad side effects from heart disease treatments.\nRating 1: Original: We discovered a high percentage of study participants experienced adverse reactions while being treated for ischemic heart disease. Output: We discovered some participants had bad side effects from heart disease treatments.\nRating 2: Original: We discovered a high percentage of study participants experienced adverse reactions while being treated for ischemic heart disease. Output: We discovered participants had no side effects from heart disease treatments.\nFluency Choices - 0-2 Give an overall rating on how well the model-generated output follows grammatical rules and appears as a natural sentence, where 0 indicates no fluency issues, 1 indicates superficial issues, and 2 indicates severe fluency issues that obfuscate the meaning of the sentence.\nExample: Rating 1: Original: We discovered a high percentage of study participants experienced adverse reactions while being treated for ischemic heart disease. Output: We found that many study participants had badly side effects from heart disease treatments.\nRating 2: Original: We discovered a high percentage of study participants experienced adverse reactions while being treated for ischemic heart disease. Output: In heart disease treatments, many participants had effects bad on the side.\nSimplicity Choices - (-2 - 2) Rate the level of simplification that occurred from the original English sentence to the model-generated output. Note: since the input readability is varied, please make judgements independent of the input. -2: Output is severely oversimplified to the point where the original intent of the original sentence has been lost. -1: Output is oversimplified, missing some important details. However, the intent of the original sentence is preserved. 0: Output is ideally simplified. It is readable to the average layman and does not omit important details. 1: Output should be simplified more and include uncommon technical terms without any elaboration/explanation. 2: Output should be simplified MUCH more. There is little to no change in the style of the sentence from the original sentence, and the output is difficult for the average layman to understand.\nExample: Rating -2: Original: We discovered a high percentage of study participants experienced adverse reactions while being treated for ischemic heart disease. Output: Study participants had some effects while being treated for some disease.\nRating -1: Original: We discovered a high percentage of study participants experienced adverse reactions while being treated for ischemic heart disease. Output: Study participants had bad effects while being treated for heart disease.\nRating 0: Original: We discovered a high percentage of study participants experienced adverse reactions while being treated for ischemic heart disease. Output: We found that many study participants had bad side effects from heart disease treatments.\nRating 1: Original: We discovered a high percentage of study participants experienced adverse reactions while being treated for ischemic heart disease. Output: We found that many study participants had experienced adverse effects from ischemic heart disease treatments.\nRating 2: Original: We discovered a high percentage of study participants experienced adverse reactions while being treated for ischemic heart disease. Output: We discovered a high percentage of study participants experienced antagonistic reactions while being remedied for ischemic heart disease."
        },
        {
            "heading": "B Human Evaluation Score Distributions",
            "text": "Figure 3 show the distributions of the human evaluation scores for all evaluated systems."
        },
        {
            "heading": "C Human Evaluation Agreement",
            "text": "In addition to evaluating outputs in their respective languages, all human evaluators also evaluated a set of 100 examples in English. These were collected to estimate how similarly annotators would evaluate outputs.\nTo analyze agreement, we use Randolph\u2019s kappa (Randolph, 2005), a free-marginal version of Fleiss\u2019 kappa. These values are presented in Table 7 for the different human evaluation criteria along with one for all of them combined. These results show there is moderate agreement among annotators."
        },
        {
            "heading": "D Alignment Analysis",
            "text": "Since annotators were given total freedom to align sentences, many different alignment relationships are present in MC-CLEAN. To quantify the proportion of alignments that belong to these alignment relationships, two methods were used for analysis. For future reference, 1-1, N-1, 1-N, and N-M alignments refer to number of complex sentence aligned to the number of simple sentences in an alignment group.\nMethod A. This method treats alignments in a document as edges in a bipartite graph, with complex and simple sentences as vertices. Relationships are found by tracking the connected components in this graph through a depth-first search. The type of relationship is determined by a 2-tuple of the number of complex sentences and the number of simple sentences in a connected component.\nHuman Evaluation Across all Systems for English\nMethod B. This method is an extension of Method A that specifically targets on breaking up N-M alignment relationships. While Method A counts any connected component as an alignment group, this method requires that each component must be fully connected (every complex sentence is aligned to every simple sentence in the group) as well. As such, an N-M alignment group that is not fully connected is broken up into smaller fully connected components.\nThe results of this analysis is presented in Table 8. The results from Method A can be viewed as a lower bound for 1-1, N-1, and 1-N, alignment types. Method B could produce different results depending on how the N-M alignment groups are broken up. It is difficult to know exactly how to break up an alignment group in Method B without knowing the annotator\u2019s intentions, so heuristic methods were used. For these results, the alignment groups were broken down such that simple sentences that were aligned to all complex sentences or vice versa were first grouped together. Then, the rest of the alignments were split into 1-N or N-1 groups."
        },
        {
            "heading": "E Automatic Alignment Experiments",
            "text": "While automatic sentence alignment is standard in text simplification (Zhang and Lapata, 2017; Xu et al., 2015), we are not aware of evaluation work for sentence alignment algorithms on medical texts. The construction of MC-NOISY allows us to create a tiered version of MULTICOCHRANE that is large enough for training state-of-the-art neural text simplification models.\nWe evaluate a series of automatic sentence alignment methods to determine which approach is the most compatible with the medical data at hand to derive the English portion of MC-NOISY.\n\u2022 Jaccard Similarity. The Jaccard Similarity score of every possible sentence pair was calculated. We consider pairs that score over a"
        },
        {
            "heading": "F Human Evaluation Correlation",
            "text": "6For both TF-IDF and Sentence-BERT, we tested a version where a similarity threshold is selected; this led to much worse F1 scores, likely due to similarity being used as an absolute measure as opposed to a relative one.\nweak. BLEU and BERTScore, not surprisingly, does have a weak inverse correlation with factuality and fluency measures. The metric with the strongest correlation to human scores is LCS, showing a moderate positive correlation with factuality (more abstractive sentences tend to contain more factual errors), as well as a moderate inverse correlation with the simplicity score (more abstractive sentences tend to be simpler). The other simplification metric, SARI, had varied results depending on the language.\nG Fine-tuning with MC-CLEAN Only\nWe attempted fine-tuning mT5 with just data from MC-CLEAN. The same training methodology was used, with the only difference being the number of epochs being increased to 5. Overall, due to the significantly lower amount of alignments in MCCLEAN as compared to MC-NOISY, the generated output featured many extreme hallucinations. The automatic metrics for these generations is shown in Table 11. The low BLEU score and BERTScore"
        },
        {
            "heading": "H Additional Statistics",
            "text": ""
        },
        {
            "heading": "I Notable Examples",
            "text": ""
        },
        {
            "heading": "I.1 Good Simplifications",
            "text": "Below are some examples of good simplifications made by the systems. GPT3 systems in particular were good at explaining medical terms and information present in parentheses. Meanwhile, other systems like Flan-T5-ft (r=0.5) and Flan-T5 (0-shot) had fluency issues and hallucinations even in the best simplifications and had more difficulty with longer input sentences. mT5 outputs had issues with extractiveness and retained complex medical vocabulary.\nLanguage System Input Output Gloss\nEnglish GPT3 (0-shot) We also found improved success rates within two attempts (RR 1.78, 95% CI 1.25 to 2.51, 134 catheters, two RCTs, moderate-quality evidence) with ultrasound guidance compared with other types of guidance. This sentence is talking about a study that was done to compare two different types of guidance when using a medical tool called a catheter. The study found that if ultrasound guidance was used, it was more successful than other types of guidance. Out of 134 catheters that were used, the success rate was 1.78 times higher when ultrasound guidance was used. This result was reliable because two different tests were done, and the evidence was of moderate quality. -\nSpanish GPT3 (0-shot) Four RCTs compared ultrasound with palpation, and one compared ultrasound with Doppler auditory assistance. Cuatro estudios de investigaci\u00f3n evaluaron la eficacia de usar ultrasonido para detectar problemas m\u00e9dicos, compar\u00e1ndolo con usar la palpaci\u00f3n (tocar con las manos) y uno de los estudios compar\u00f3 el uso del ultrasonido con ayuda de un aparato llamado Doppler para escuchar los sonidos del cuerpo. Four investigative studies evaluated the efficacy of using ultrasound to detect medical problems, comparing it to using the palpation (touching with the hands) and one of the studies compared the use of the ultrasound with the help of an apparatus called Doppler to listen to the sounds of the body.\nEnglish mT5 (r=0.5) There were no serious adverse events related to either nitrous oxide or carbon dioxide pneumoperitoneum (three studies; 196 participants; very low quality of evidence). There were no serious side effects related to either nitrous oxide or carbon dioxide pneumoperitoneum. -\nFrench mT5 (r=0.5) Thiospot was more effective than placebo (SMD - 2.61, 95% CI -3.76 to - 1.47). La thiospots \u00e9tait plus efficace que le placebo. Thiospot was more effective than the placebo.\nSpanish GPT3-simp-trans There were no serious adverse events related to either nitrous oxide or carbon dioxide pneumoperitoneum (three studies; 196 participants; very low quality of evidence). Los tres estudios realizados con 196 personas mostraron que no hubo efectos negativos al respirar gas nitr\u00f3geno o di\u00f3xido de carbono durante un procedimiento m\u00e9dico. The three studies carried out with 196 people showed that there were no negative effects in breathing nitrogen gas or carbon dioxide during a medical procedure."
        },
        {
            "heading": "I.2 Fluency",
            "text": "Fluency errors across systems include repetition and grammatical errors, as well as the use of foreign words in the Flan outputs.\nRepetition. This category includes examples of fluency errors involving the repetition of a single phrase or word that confuses the meaning of the output and, in some cases, makes it completely meaningless. This error was present in all systems except for the GPT3 (0-shot) and GPT3-simp-trans systems.\nGrammar. This category includes examples of grammatical issues in the output sentences, such as the overuse of prepositions, spelling errors, and disagreement between articles and nouns. Multiple prepositions appeared sequentially in a way that was incoherent, and there were instances where articles did not agree with nouns in gender or number.\nForeign words (Flan-T5). This category includes examples of fluency errors involving the use of foreign words in the output sentences. For example, Flan-T5 (0-shot) produced a French output that started with the English phrase \u2018in four studies\u2019, and Flan-T5-ft (r=0.5) produced an English output that started with the Spanish word \u2018otros\u2019. This type of error was mostly found in the Flan-T5-ft (r=0.5) and Flan-T5 (0-shot) systems.\nLanguage System Input Output Gloss\nFrench Flan-T5-ft (r=0.5) It usually involves restricting blood flow to limbs and then releasing the ischaemic blood to promote a neuroprotective effect. L\u2019exercice consiste g\u00e9n\u00e9ralement \u00e0 la restriction du flux sanguin de la membres des limbs et ensuite \u00e0 lib\u00e9rer les bloods isch\u00e9mies pour promouvoir un effet neuroprotecteur. The exercise generally involves the restriction of the blood flow to the members of the limbs and then to free the ischemic blood to promote a neuroprotective effect.\nFrench Flan-T5 (0-shot) In four studies, the use of blunt needles reduced the number of self-reported needle stick injuries with a RR of 0.31 (95% CI 0.14 to 0.68). In four studies, l\u2019utilisation des nej\u00e9ros strru00f7u a r\u00e9duit l\u2019injurage autorempr\u00e9sent\u00e9 aux nej\u00e9ros d\u2019une rRR de 0,31, en moyenne de 0,14 \u00e0 0,68. In four studies, the use of nej\u00e9ros strru00f7u (?) to reduce the autorempr\u00e9sent\u00e9 (?) injury to the nej\u00e9ros (?) of a risk ration of .31, on average from .14 to .68.\nEnglish Flan-T5-ft (r=0.5) Therefore, other gases have been introduced as alternatives to carbon dioxide for establishing pneumoperitoneum. Otros such as carbon dioxide are used as alternative gases for establishing pneumoperitoneum. -"
        },
        {
            "heading": "I.3 Factuality",
            "text": "Factuality errors displayed by the systems could be broadly categorized into 3 types - minor hallucinations, extreme hallucinations, and contradictions.\nMinor hallucinations. This category includes examples where the output contained minor details that were not present in the input (hallucination). The systems would hallucinate details like numbers, dates, medical terms, incorrect expansions of acronyms, etc. GPT3 systems would also misinterpret the context of words present in the input. In one of the examples, the word \u2018rates\u2019 in the input is referring to \u2018pregnancy rates\u2019 but GPT3 misinterpreted the word to mean \u2018cost\u2019.\nLanguage System Input Output Gloss\nFrench mT5 (r=0) To assess interventions used in the management of all types of melasma: epidermal, dermal, and mixed. Cette revue a examin\u00e9 diff\u00e9rents type de m\u00e9lanome, de la peau, d\u2019\u0153il et des organes, et s\u2019est am\u00e9lior\u00e9e dans des ann\u00e9es futures. This review has examined different types of melanoma, of the skin, of the eye and of the organs, and improved in the future years.\nSpanish GPT3 (0-shot) It usually involves restricting blood flow to limbs and then releasing the ischaemic blood to promote a neuroprotective effect. Esta oraci\u00f3n est\u00e1 hablando de un procedimiento m\u00e9dico llamado Preacondicionamiento Isqu\u00e9mico (IPC). El procedimiento consiste en cortar temporalmente el flujo de sangre a ciertas partes del cuerpo y luego devolver la sangre al cuerpo. El objetivo de este procedimiento es proteger el cerebro y el sistema nervioso del da\u00f1o. This sentence is talking about a medical procedure called Ischemic Preconditioning (IPC). The procedure consists of temporarily cutting off the flow of blood to certain parts of the body and then returning the blood to the body. The objective of this procedure is to protect the brain and the nervous system from damage.\nSpanish mT5 (r=0.5) We included five RCTs reporting 444 arterial cannulations in paediatric participants. Se incluyeron cinco ensayos controlados aleatorios que informaron 556 cannulaciones arteriales en los participantes ni\u00f1os. Five randomized controlled trials that reported 556 arterial cannulations in pediatric participants were included.\nFrench mT5 (r=0) The rates were 24.5 for the sponge and 10.9 for the diaphragm in the UK trial. Les taux \u00e9taient de 18,3 pour le diaphragme et de 10,9 pour le diaphragme avec un diaphragme UK dans l\u2019essai. The rates were 18.3 for the diaphragm and 10.9 for the diaphragm with a UK diaphragm in the trial.\nEnglish mT5 (r=0) We included three trials (involving 371 participants) in the analysis of the effects of RIC on ischaemic stroke prevention. We included three trials with 373 participants and provided one trial with seven trials involving 373 participants. -\nExtreme hallucinations. This category includes examples of hallucinations that were extremely irrelevant. Information hallucinated in the output sentences has no connection to the information present in the input. This degree of hallucinations was much more prevalent and extreme in the Flan systems as compared to other systems.\nLanguage System Input Output Gloss\nEnglish Flan-T5 (0-shot) Randomized controlled trials based on uniform and standard methods are needed. The aim of the experiments is to give patients a chance to perform the experiment they want to do. -\nFrench Flan-T5-ft (r=0.5) In acute ischaemic stroke, for people receiving intravenous thrombolysis, the rate of death or dependency was significantly increased by RIC treatment compared with non-RIC treatment (RR 2.34; 95% 1.19 to 4.61; 1 trial, 285 participants, low-quality evidence). Pour les personnes victimes d\u2019un AVC aigu, les femmes admiss\u00e9es de la greffe avait une probabilit\u00e9 accrue d\u2019\u00eatre victimes ou de la maladie gr\u00e2ce \u00e0 un traitement remmanant par injection d\u2019une coagulation r\u00e9currente. For the victims of acute stroke, the women administered (?) the transplant had an increased probability of being victims or of the disease thanks to a remmanant (?) treatment by an injection of a recurrent coagulation.\nContradictions. This category includes examples where the output directly contradicts ideas stated in the input. This error was infrequent in the GPT3 systems but was equally prevalent in other systems.\nLanguage System Input Output Gloss\nFrench Flan-T5 (0-shot) There was considerable heterogeneity between the studies included in this review and studies were small and at some risk of bias Il y a lieu d\u2019\u00e9chantillonner des \u00e9tudes incluant dans cette \u00e9tude et les \u00e9tudes \u00e9taient grandes et en hausse une hausse de la biase. There is a reason to sample the studies included in this study and the studies were big and increasing an increase of bias.\nEnglish mT5 (r=0) There is qualitative evidence that oral morphine has much the same efficacy as other available opioids. It is not clear if oral morphine is as effective as other available opioids. -\nFrench mT5 (r=0.5) In people with acute ischaemic stroke, there was no significant difference between RIC and non-RIC for reducing stroke severity as assessed by the National Institutes of Health Stroke Scale score and the final infarct volume (standardised mean difference (SMD) -0.24 mL, 95% CI -1.02 to 0.54; 2 trials, 175 participants, very low quality evidence). La SEP r\u00e9duisait la gravit\u00e9 du AVC isch\u00e9mique, mais les essais confondus avaient des preuves de tr\u00e8s faible qualit\u00e9. The SEP reduced the severity of ischemic stroke, but the included trials have evidence of very weak quality.\nSpanish mT5 (r=0.5) Our results suggest, but do not confirm, that a possible advantage of ultrasound guidance for the first attempt success rate over other techniques is more pronounced in infants and small children than in older children. Los resultados indican que la gu\u00eda ecogr\u00e1fica redujo la tasa de \u00e9xito del ensayo m\u00e1s amplia en los lactantes y los ni\u00f1os peque\u00f1os en comparaci\u00f3n con otras t\u00e9cnicas. The results indicate that the ultrasound guidance reduced the success rate of the broader trial in the infants and the young children in comparison with other techniques."
        },
        {
            "heading": "I.4 Simplification",
            "text": "Simplification errors included instances of the system being overly extractive, oversimplification of terms and ideas, and excessive deletion of entire parts of the original text.\nExtractive. GPT3 (0-shot) was highly extractive and failed to explain complex medical language in everyday terms that a general audience could understand. The system also left in statistics from the input sentence that are not interpretable to the general public. mT5 (r=0) exhibited similar patterns with regards to extractiveness.\nLanguage System Input Output Gloss\nSpanish GPT3 (0-shot) Interventions included: thrombolytic therapy versus placebo (1 study); low versus high dose thrombolytic therapy (1); alteplase versus urokinase (1); short versus long thrombolytic dwell (1); thrombolytic therapy versus percutaneous fibrin sheath stripping (1); fibrin sheath stripping versus over-the-wire catheter exchange (1); and over-the-wire catheter exchange versus exchange with and without angioplasty sheath disruption (1). Esta frase describe un estudio que se hizo para comparar diferentes tratamientos para ayudar a mejorar la circulaci\u00f3n sangu\u00ednea. Estos tratamientos incluyeron: terapia con trombol\u00edticos en comparaci\u00f3n con un placebo (1 estudio); dosis baja en comparaci\u00f3n con dosis alta de trombol\u00edticos (1); alteplasa en comparaci\u00f3n con uroquinasa (1); tiempo corto de trombol\u00edticos en comparaci\u00f3n con tiempo largo (1); terapia con trombol\u00edticos en comparaci\u00f3n con despeje de fibrina con una aguja (1); despeje de fibrina en comparaci\u00f3n con cambio de cat\u00e9ter con alambre (1); y cambio de cat\u00e9ter con alambre en comparaci\u00f3n con cambio sin despeje de fibrina con angioplastia (1). This sentence describes a study that was conducted to compare different treatments to help improve the blood circulation. These treatments included: therapy with thrombolytics in comparison with a placebo (1 study); low dose in comparison with high dose of thrombolytics (1); alteplase compared to urokinase (1); short time of thrombolytics compared with long time (1); therapy with thrombolytics compared to clearance of fibrin with a needle (1); clearance of fibrin in comparison with wire catheter change (1); and wire catheter change in comparison with change without clearance of fibrin with angioplasty (1).\nSpanish mT5 (r=0) On current data there is no evidence to support physical intervention over the use of pharmaceutical agents in the acute setting. Esta evidencia no apoya la intervenci\u00f3n f\u00edsica sobre la administraci\u00f3n de agentes farmacol\u00f3gicos en la pr\u00e1ctica aguda. This evidence does not support the physical intervention over the administration of pharmaceutical agents in the acute practice.\nOversimplification Flan-T5-ft (r=0.5) had issues with excessive deletions, resulting in fragmented sentences that failed to communicate the meaning of the input sentence. Excessive deletion in mT5 (r=0) and mT5 (r=0.5) manifested as a loss of information that was necessary to preserve the meaning of the input sentence. The resulting outputs were missing critical information, causing them to be generic and vague, such as in the Spanish mT5 (r=0.5) example that discusses the use of RIC for ischemic stroke prevention.\nLanguage System Input Output Gloss\nFrench Flan-T5-ft (r=0.5) We rated the quality of evidence for all outcomes as moderate owing to imprecision due to wide confidence intervals, modest sample sizes and limited numbers of events. La qualit\u00e9 des preuves The quality of the evidence\nFrench mT5 (r=0) Four RCTs compared ultrasound with palpation, and one compared ultrasound with Doppler auditory assistance. Cinq ECR ont compar\u00e9 l\u2019\u00e9chographie avec une palpation. Five RCTs have compared ultrasounds with palpation.\nFrench mT5 (r=0.5) We included three trials (involving 371 participants) in the analysis of the effects of RIC on ischaemic stroke prevention. Les auteurs de la revue ont inclus un total de 371 participants. The authors of the review included a total of 371 participants.\nSpanish mT5 (r=0.5) The quality of the evidence was rated as high. Calidad de la evidencia Quality of the evidence\nSpanish mT5 (r=0.5) We included three trials (involving 371 participants) in the analysis of the effects of RIC on ischaemic stroke prevention. Se incluyeron tres ensayos que incluyeron a 372 participantes. Three trials with 372 participants were included.\nEnglish mT5 (r=0) There were no serious adverse events related to either nitrous oxide or carbon dioxide pneumoperitoneum (three studies; 196 participants; very low quality of evidence). There were no serious adverse events. -"
        },
        {
            "heading": "I.5 Other Errors",
            "text": "Repetition of output (GPT3 (0-shot)). The GPT3 (0-shot) system had a unique error that exclusively appeared in the French outputs where a separate paraphrase was generated in addition to the main output sentence. The system sometimes gave a generated output, then repeated the same information in different words using the phrase \u2018in other words\u2019 or simply generating two different versions of a simplified output.\nAcronyms. Across all systems, a frequent issue that arose when simplifying medical text was acronyms. These issues can be further broken down into failure to translate into the target output language, hallucinated acronym generations, and factually incorrect generations.\nAcronym translation. Systems failed to translate the input sentence\u2019s acronym from English into Spanish or French. GPT3 (0-shot) would explicitly state that the acronym was in English, rather than translating, in some cases.\nAcronym hallucination. Flan-T5-ft (r=0.5) hallucinated acronyms in the output sentence that do not exist in the target language. This occurred across all languages and in both Flan and mT5 systems.\nLanguage System Input Output Gloss\nEnglish Flan-T5-ft (r=0.5) In acute ischaemic stroke, for people receiving intravenous thrombolysis, the rate of death or dependency was significantly increased by RIC treatment compared with non-RIC treatment (RR 2.34; 95% 1.19 to 4.61; 1 trial, 285 participants, low-quality evidence). For acute ischaemic stroke, for people receiving intravenous thrombolysis, the rate of death or dependency was significant increased by RAC treatment compared with non-RAC treatment (low-quality evidence).\nFrench Flan-T5-ft (r=0.5) Preclinical studies have suggested that RIC may have beneficial effects in ischaemic stroke patients and those at risk of ischaemic stroke. Les r\u00e9sultats de cette revue ont sugg\u00e9r\u00e9 que les BRA pourrait avoir des effets b\u00e9n\u00e9fiques dans les patients souffrant d\u2019une PIO et chez les personnes ayant un accident c\u00e9r\u00e9bral isch\u00e9mique. The results of this review suggested that the BRA could have beneficial effects in patients suffering a PIO and in people who had an ischemic stroke.\nAcronym factuality. Flan-T5 (0-shot) and mT5 (r=0) produced factuality issues in the output sentences. In the Spanish output example, Flan generated the acronym for AIDS (SIDA), though the input was referring to randomized controlled trials (RCTs). In the French example, mT5 (r=0) generated the acronym for a stroke (AVC), but the input sentence refers to cardiovascular disease (CVD).\nInput hallucination (GPT3). The systems GPT3-simp-trans and GPT3 (0-shot) show some minor hallucination by referencing the input sentence in the output. Some output generations would start with \u2018this phrase\u2019 or \u2018this sentence\u2019 in talking about the input. This error was not found in any of the other systems."
        },
        {
            "heading": "J ANNO-VIEWER",
            "text": "ANNO-VIEWER is inspired by a similar annotation tool that was used in Jiang et al. (2020) for correcting crowdsourced alignment labels used for training and evaluating a neural CRF model for aligning sentences.\nThe primary function of this tool is to enable annotators to make alignments efficiently. ANNO-VIEWER also enables annotators to make factuality annotations for alignments. These annotations are largely based from the factuality annotations used in Devaraj et al. (2022), and also includes an additional field to annotate for elaborations in a similar manner.\nThis annotation tool also has the additional functionality of allowing annotators to use existing similarity\nmeasures to help find sentence alignments faster. When a sentence is selected for alignment, the tool can sort the other document by which sentences are most similar (determined by the automatic similarity measure) to the selected sentence."
        }
    ],
    "title": "Multilingual Simplification of Medical Texts",
    "year": 2023
}