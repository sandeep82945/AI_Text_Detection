{
    "abstractText": "Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We capture ambiguity in a sentence through its effect on entailment relations with another sentence, and collect AMBIENT, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AMBIENT, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in crowdworker evaluation, compared to 90% for disambiguations in our dataset. Finally, to illustrate the value of ambiguity-sensitive tools, we show that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity. We encourage the field to rediscover the importance of ambiguity for NLP.",
    "authors": [
        {
            "affiliations": [],
            "name": "Alisa Liu"
        },
        {
            "affiliations": [],
            "name": "Zhaofeng Wu"
        },
        {
            "affiliations": [],
            "name": "Julian Michael"
        },
        {
            "affiliations": [],
            "name": "Alane Suhr"
        },
        {
            "affiliations": [],
            "name": "Peter West"
        },
        {
            "affiliations": [],
            "name": "Alexander Koller"
        },
        {
            "affiliations": [],
            "name": "Swabha Swayamdipta"
        },
        {
            "affiliations": [],
            "name": "Noah A. Smith"
        },
        {
            "affiliations": [],
            "name": "Yejin Choi"
        },
        {
            "affiliations": [],
            "name": "Paul G. Allen"
        }
    ],
    "id": "SP:a28e0931595c951505f656c979c51e866ebb6f81",
    "references": [
        {
            "authors": [
                "Pangbo Ban",
                "Yifan Jiang",
                "Tianran Liu",
                "Shane Steinert-Threlkeld."
            ],
            "title": "Testing pre-trained language models\u2019 understanding of distributivity via causal mediation analysis",
            "venue": "Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and In-",
            "year": 2022
        },
        {
            "authors": [
                "Beata Beigman Klebanov",
                "Eyal Beigman."
            ],
            "title": "From annotator agreement to noise models",
            "venue": "Computational Linguistics, 35(4):495\u2013503.",
            "year": 2009
        },
        {
            "authors": [
                "Samuel R. Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D. Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2015
        },
        {
            "authors": [
                "Andrew Carnie."
            ],
            "title": "Syntax: A Generative Introduction",
            "venue": "Introducing Linguistics. Wiley.",
            "year": 2013
        },
        {
            "authors": [
                "Jifan Chen",
                "Aniruddh Sriram",
                "Eunsol Choi",
                "Greg Durrett."
            ],
            "title": "Generating literal and implied subquestions to fact-check complex claims",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3495\u20133516,",
            "year": 2022
        },
        {
            "authors": [
                "Tongfei Chen",
                "Zhengping Jiang",
                "Adam Poliak",
                "Keisuke Sakaguchi",
                "Benjamin Van Durme."
            ],
            "title": "Uncertain natural language inference",
            "venue": "Proceedings",
            "year": 2020
        },
        {
            "authors": [
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language models",
            "year": 2022
        },
        {
            "authors": [
                "Kenneth Church",
                "Ramesh Patil."
            ],
            "title": "Coping with syntactic ambiguity or how to put the block in the box on the table",
            "venue": "American Journal of Computational Linguistics, 8(3-4):139\u2013149.",
            "year": 1982
        },
        {
            "authors": [
                "Jeremy R. Cole",
                "Michael J.Q. Zhang",
                "Daniel Gillick",
                "Julian Martin Eisenschlos",
                "Bhuwan Dhingra",
                "Jacob Eisenstein"
            ],
            "title": "Selectively answering ambiguous questions",
            "year": 2023
        },
        {
            "authors": [
                "Ann Copestake",
                "Dan Flickinger."
            ],
            "title": "An open source grammar development environment and broadcoverage English grammar using HPSG",
            "venue": "Proceedings of the Second International Conference on Language Resources and Evaluation (LREC\u201900), Athens,",
            "year": 2000
        },
        {
            "authors": [
                "Samuel Gehman",
                "Suchin Gururangan",
                "Maarten Sap",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "Mitchell L. Gordon",
                "Michelle S. Lam",
                "Joon Sung Park",
                "Kayur Patel",
                "Jeff Hancock",
                "Tatsunori Hashimoto",
                "Michael S. Bernstein."
            ],
            "title": "Jury learning: Integrating dissenting voices into machine learning models",
            "venue": "Proceedings of the 2022 CHI Conference on Hu-",
            "year": 2022
        },
        {
            "authors": [
                "Barbara J. Grosz."
            ],
            "title": "The Representation and Use of Focus in Dialogue Understanding",
            "venue": "Ph.D. thesis.",
            "year": 1977
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Paloma Jeretic",
                "Alex Warstadt",
                "Suvrat Bhooshan",
                "Adina Williams."
            ],
            "title": "Are natural language inference models IMPPRESsive? Learning IMPlicature and PRESupposition",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Nan-Jiang Jiang",
                "Marie-Catherine de Marneffe."
            ],
            "title": "Investigating Reasons for Disagreement in Natural Language Inference",
            "venue": "Transactions of the Association for Computational Linguistics, 10:1357\u2013 1374.",
            "year": 2022
        },
        {
            "authors": [
                "Kate Kearns."
            ],
            "title": "Semantics",
            "venue": "St. Martin\u2019s Press.",
            "year": 2000
        },
        {
            "authors": [
                "Alexander Koller",
                "Michaela Regneri",
                "Stefan Thater."
            ],
            "title": "Regular tree grammars as a formalism for scope underspecification",
            "venue": "Proceedings of ACL-08: HLT, pages 218\u2013226, Columbus, Ohio. Association for Computational Linguistics.",
            "year": 2008
        },
        {
            "authors": [
                "Dmitrii Krasheninnikov",
                "Egor Krasheninnikov",
                "David Krueger."
            ],
            "title": "Assistance with large language models",
            "venue": "Proceedings of the ML Safety Workshop at NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Lorenz Kuhn",
                "Yarin Gal",
                "Sebastian Farquhar."
            ],
            "title": "Clam: Selective clarification for ambiguous questions with generative language models",
            "venue": "Proceedings of the Workshop on Challenges in Deployable Generative AI at International Conference on Ma-",
            "year": 2023
        },
        {
            "authors": [
                "Mina Lee",
                "Percy Liang",
                "Qian Yang."
            ],
            "title": "Coauthor: Designing a human-AI collaborative writing dataset for exploring language model capabilities",
            "venue": "CHI Conference on Human Factors in Computing Systems, New Orleans, LA, USA.",
            "year": 2022
        },
        {
            "authors": [
                "Alisa Liu",
                "Swabha Swayamdipta",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "WANLI: Worker and AI collaboration for natural language inference dataset creation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6826\u20136847, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Johannes Mario Meissner",
                "Napat Thumwanit",
                "Saku Sugawara",
                "Akiko Aizawa."
            ],
            "title": "Embracing ambiguity: Shifting the training target of NLI models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th",
            "year": 2021
        },
        {
            "authors": [
                "Sewon Min",
                "Julian Michael",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer."
            ],
            "title": "AmbigQA: Answering ambiguous open-domain questions",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5783\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Yixin Nie",
                "Xiang Zhou",
                "Mohit Bansal"
            ],
            "title": "What can we learn from collective human opinions on natural language inference data",
            "year": 2020
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Ellie Pavlick",
                "Tom Kwiatkowski."
            ],
            "title": "Inherent disagreements in human textual inferences",
            "venue": "Transactions of the Association for Computational Linguistics, 7:677\u2013694.",
            "year": 2019
        },
        {
            "authors": [
                "Sandro Pezzelle."
            ],
            "title": "Dealing with semantic underspecification in multimodal NLP",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12098\u201312112, Toronto, Canada. Association",
            "year": 2023
        },
        {
            "authors": [
                "Steven T. Piantadosi",
                "Harry Tily",
                "Edward Gibson."
            ],
            "title": "The communicative function of ambiguity in language",
            "venue": "Cognition, 122(3):280\u2013291.",
            "year": 2012
        },
        {
            "authors": [
                "Barbara Plank."
            ],
            "title": "The \u201cproblem\u201d of human label variation: On ground truth in data, modeling and evaluation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10671\u201310682, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Massimo Poesio",
                "Ron Artstein."
            ],
            "title": "The reliability of anaphoric annotation, reconsidered: Taking ambiguity into account",
            "venue": "Proceedings of the Workshop on Frontiers in Corpus Annotations II: Pie in the Sky, pages 76\u201383, Ann Arbor, Michigan. Association for",
            "year": 2005
        },
        {
            "authors": [
                "Maarten Sap",
                "Swabha Swayamdipta",
                "Laura Vianna",
                "Xuhui Zhou",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "Annotators with attitudes: How annotator beliefs and identities bias toxic language detection",
            "venue": "Proceedings of the 2022 Conference of the North Amer-",
            "year": 2022
        },
        {
            "authors": [
                "Emily Sheng",
                "Kai-Wei Chang",
                "Premkumar Natarajan",
                "Nanyun Peng."
            ],
            "title": "The woman worked as a babysitter: On biases in language generation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Jason Weston"
            ],
            "title": "Blenderbot 3: a deployed",
            "year": 2022
        },
        {
            "authors": [
                "Goodman"
            ],
            "title": "Task ambiguity in humans",
            "year": 2023
        },
        {
            "authors": [
                "Yuewei Yuan",
                "Chaitanya Malaviya",
                "Mark Yatskar."
            ],
            "title": "AmbiCoref: Evaluating human and model sensitivity to ambiguous coreference",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 1023\u20131030, Dubrovnik, Croatia. Associ-",
            "year": 2023
        },
        {
            "authors": [
                "Sheng Zhang",
                "Rachel Rudinger",
                "Kevin Duh",
                "Benjamin Van Durme."
            ],
            "title": "Ordinal common-sense inference",
            "venue": "Transactions of the Association for Computational Linguistics, 5:379\u2013395.",
            "year": 2017
        },
        {
            "authors": [
                "Shujian Zhang",
                "Chengyue Gong",
                "Eunsol Choi."
            ],
            "title": "Learning with different amounts of annotation: From zero to many labels",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7620\u20137632, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Xinliang Frederick Zhang",
                "Marie-Catherine de Marneffe."
            ],
            "title": "Identifying inherent disagreement in natural language inference",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Zhou",
                "Yixin Nie",
                "Mohit Bansal."
            ],
            "title": "Distributed NLI: Learning to predict human opinion distributions for language reasoning",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 972\u2013987, Dublin, Ireland. Association",
            "year": 2022
        },
        {
            "authors": [
                "George Kingsley Zipf"
            ],
            "title": "Human behavior and the principle of least effort",
            "year": 1949
        },
        {
            "authors": [
                "de Marneffe"
            ],
            "title": "2022) is trained on the development",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Ambiguity seems to be an essential, indispensable element for the transfer of information from one place to another by words. \u2014 Thomas (1974), as referenced in the epilogue of Grosz (1977)\nAmbiguity is an intrinsic feature of language, allowing speakers to balance efficiency and clarity in communication (Zipf, 1949; Piantadosi et al., 2012). Language understanding thus requires recognizing the presence of multiple interpretations:\n1Data and code can be found at https://github.com/ alisawuffles/ambient\nas communicators, we anticipate the possibility of misunderstanding; as listeners, we ask clarifying questions, disambiguate meanings on the basis of a wide range of contextual factors, and backtrack and revise our earlier interpretations as needed. Beyond unintended miscommunication, ambiguity is also an effective tool for sending covert messages, e.g., out of politeness or to mislead one\u2019s listeners while avoiding accountability (see Figure 1).\nAs language models (LMs) are increasingly employed to act as dialogue agents (OpenAI, 2022; Shuster et al., 2022) or to aid human communication as writing aids (Lee et al., 2022), being able to work with ambiguous language will make them more effective. This skill would support adaptation to different contexts, clearer communication, and identification of misleading or deceptive language. Yet, the ability of pretrained LMs to recognize ambiguity and disentangle possible meanings remains unstudied, partly because ambiguous instances are systematically excluded in the curation of benchmarks (Beigman Klebanov and Beigman, 2009).\nWe present AMBIENT, Ambiguity in Entailment, an English benchmark of 1,645 examples covering a variety of lexical, syntactic, and pragmatic ambiguities, and more broadly sentences which can be plausibly read as conveying one of multiple different messages. Formally characterizing ambiguity requires a choice of meaning representation to distinguish between possible interpretations, and enumerating the full set of interpretations can be tricky or impractical.2 Thus, we adopt a functional approach: using the natural language inference (NLI) task format, we characterize ambiguity in the premise and/or hypothesis by its effect on entailment relations.3\nEach AMBIENT example consists of a premise and hypothesis pair, assigned a set of labels (among entailment, neutral, and contradiction), along with disambiguating rewrites corresponding to each label when multiple are plausible (see Table 1 for examples). Examples are collected through two approaches: manual curation to target textbook ambiguities, and expert annotation of automatically generated unlabeled examples to uncover more diverse phenomena. Through analysis, we find that crowdworkers can reliably distinguish different readings of an ambiguous sentence and their impact on entailment choices; thus we can explicitly characterize the underlying reasons for uncertainty that would otherwise surface as \u201cdisagreement\u201d (\u00a73).\nWe design a suite of tests based on AMBIENT to investigate the extent to which understanding of ambiguity is acquired during pretraining of large LMs (\u00a74). These tests evaluate whether LMs can directly produce relevant disambiguations, recognize possible interpretations, and model different interpretations in their continuation distributions. We find that these tasks remain extremely challenging, including for the recent GPT-4 (OpenAI, 2023).\nTherefore, we additionally investigate whether LMs can be finetuned on existing NLI data for the less demanding task of ambiguity recognition, without explicit disambiguation (\u00a75). We adapt several finetuned NLI models to a multilabel setting, and find that the best model predicts the exact label set in only 43.6% of instances, suggesting that the NLI\n2For example, Koller et al. (2008) find that including all possible quantifier scope readings in the Rondane Treebank (Copestake and Flickinger, 2000) results in 5% of sentences having \u2265650,000 possible semantic analyses.\n3By design, we only consider ambiguities in NLI examples that affect the determination of the label, to provide a natural setting where ambiguities are contextually relevant.\ntask is much more challenging when formulated to account for ambiguity.\nFinally, to illustrate the value of ambiguitysensitive tools, we present a case study of how a multilabel NLI model can be used to detect misleading political claims in the wild. We find that the strongest model from \u00a75, despite its limitations, can not only recover claims flagged by fact-checkers as ambiguous, but highlight previously unidentified ambiguous claims, indicating the promise of such tools to aid real-world communication.\nThe simplifying assumption that text has only one interpretation has facilitated the development of large-scale benchmarks, yet limits the depth of what these benchmarks can evaluate. In this work we show that sensitivity to ambiguity\u2014a fundamental aspect of human language understanding\u2014 is lacking in our ever-larger models, and illustrate the value such understanding could bring."
        },
        {
            "heading": "2 AMBIENT",
            "text": "Traditionally, the NLI task requires predicting whether a premise entails, contradicts, or is neutral with respect to a hypothesis. Yet, ambiguities in the premise and/or hypothesis (as in Table 1) may impact the determination of the label.\nWe present AMBIENT, a dataset of 1,645 NLI examples, each annotated with a set of labels, reflecting potentially multiple readings of the premise and/or hypothesis. Ambiguous examples, i.e., those having more than one label, make up 35.2% of the dataset and include a disambiguating rewrite corresponding to each label; unambiguous examples have a single label. The inclusion of unambiguous examples facilitates evaluating model abilities to first detect the presence of relevant ambiguity, and then resolve it to distinct interpretations.\nWe use two approaches to collect source examples: manual curation and automatic generation. Manual curation (\u00a72.1) involves crafting a small set of examples targeting specific types of ambiguity. Further, to cover more diverse forms of ambiguity, we produce a larger collection of examples via text generation and heuristic filtering (\u00a72.2), followed by expert manual annotation (\u00a72.3), forming the bulk of AMBIENT. Details are in \u00a7A."
        },
        {
            "heading": "2.1 Curated Examples",
            "text": "The authors curate a set of 142 examples, which are either handwritten or sourced from existing NLI datasets and linguistics textbooks (Kearns, 2000;\nExample Disambiguation 1 Disambiguation 2 Type\nP: I\u2019m afraid the cat was hit by a car. H: The cat was not hit by a car. *NEUTRAL, CONTRADICT+ : [7 N, 2 C]\nP: I\u2019m worried... NEUTRAL : [9 N]\nP: I\u2019m sorry to share that... CONTRADICT : [9 C]\nP ragm atic (44.8% )\nP: John and Anna are married. H: John and Anna are not a couple. *NEUTRAL, CONTRADICT+ : [5 N, 4 C]\nP: ... are both married. NEUTRAL : [7 N, 2 E]\nP: ... are married to each other. CONTRADICT : [9 C] Lexical (20.0% )\nP: This seminar is full now, but interesting seminars are being offered next quarter too. H: There will be more interesting seminars... *ENTAIL, NEUTRAL+ : [7 E, 2 N] H: There will be more seminars ... that are interesting. ENTAIL : [9 E] H: There will be seminars... that are more interesting. NEUTRAL : [9 N] Syntactic (8.6% )\nP: The novel has been banned in many schools because of its explicit language. H: The novel has not been banned in many schools. *NEUTRAL, CONTRADICT+ : [4 N, 5 C] H: There are many schools where the novel has not been banned. NEUTRAL : [9 N] H: It is not the case that the novel has been banned in many schools. CONTRADICT : [9 C] Scopal (7.6% )\nP: It is currently March, and they plan to schedule their wedding for next December. H: They plan to schedule... for next year. *ENTAIL, CONTRADICT+ : [3 E, 2 N, 4 C] P: ... for December next year. ENTAIL : [9 E] P: ... for the coming December. CONTRADICT : [9 C] C oreference (2.9% )\nP: It is difficult to believe that the author of such a masterpiece could have been only 23 years old. H: The author of the masterpiece was only 23. *ENTAIL, NEUTRAL+ : [3 E, 6 N] P: It is shocking that... ENTAIL : [9 E]\nP: It is questionable that... NEUTRAL : [9 N]\nFigurative (1.9% )\nP: A new study has found that nearly half of all Americans are in favor of gun control. H: The study found that half of all Americans are in favor of gun control. *ENTAIL, CONTRADICT+ : [1 E, 2 N, 6 C] H: ... that exactly half of all Americans... CONTRADICT : [8 C, 1 N]\nH: ... that about half of all Americans... ENTAIL : [9 E]\nO ther (14.3% )\nTable 1: Ambiguous examples in AMBIENT with linguist-annotated *GOLD LABELS+. As analysis, we collect the : [distribution of NLI labels] as judged by nine crowdworkers under the traditional single-label annotation scheme (\u00a73), finding that disagreement on ambiguous examples is largely resolved on disambiguations. The Type column indicates the ambiguity type for each example, along with its estimated representation in the dataset (\u00a72.5).\nCarnie, 2013). We choose examples ad hoc from the synthetic NLI datasets DistNLI (Ban et al., 2022) for predicate distributivity (e.g., \u201cSam and Frank gave a talk\u201d may either mean separately or jointly) and IMPPRES (Jeretic et al., 2020) for implicatures. We also include some instances with differing pragmatic and literal readings from NLI Diagnostics (Wang et al., 2018), and ones leading to disagreement from large-scale NLI datasets like MNLI (Williams et al., 2018) and WANLI (Liu et al., 2022). The authors directly annotate these examples with the set of labels and disambiguations (examples in \u00a7A.1)."
        },
        {
            "heading": "2.2 Generated Examples",
            "text": "To cover more ambiguities, we use overgeneration and filtering to automatically create a large corpus of unlabeled NLI examples that are likely\nto be ambiguous. Inspired by WANLI (Liu et al., 2022), we automatically identify groups of premisehypothesis pairs that share a reasoning pattern, to encourage the creation of new examples with the same pattern. We use WANLI as our source of examples; each group contains a randomly chosen example on which its two annotators disagreed (indicating possible ambiguity), along with its 4 nearest neighbors according to the final-layer embedding of a WANLI-trained NLI model. We observe that these groups can share interpretable ambiguity patterns, such as sentences about the past (e.g., \u201cWhen I was young, I was obsessed\u201d) inducing a cancellable implicature about the present (that \u201cI\u201d am no longer obsessed; full prompt in \u00a7A).\nThese groups of examples are formatted into a prompt with the instruction, \u201cWrite pairs of sentences that are related to each other in the same\nway.\u201d For each prompt, we sample 5 continuations from InstructGPT (Ouyang et al., 2022), discarding those that cannot be parsed into a premise and hypothesis.\nTo further filter for likely-ambiguous instances, we use a multilabel RoBERTa-large model trained on WANLI and retain all examples where the model assigns probability \u2265 0.05 to more than one NLI label, indicating at least slight uncertainty in whether there can be multiple possible readings."
        },
        {
            "heading": "2.3 Annotation and Validation",
            "text": "Examples acquired in \u00a72.2 consist of unlabeled premise-hypothesis pairs, which we next annotate with label sets and relevant disambiguations. Following AMBIGQA (Min et al., 2020) and as shown in Figure 2, each example is first annotated by two experts, then presented to a third expert for validation and consolidation.\nWe recruit 37 university-level linguistics students for the annotation phase,4 as identifying ambiguities of a sentence then delineating its possible interpretations is a challenging task. They select a set of labels for each example, including the singleton set when the example is unambiguous; when more than one label is chosen, they provide a disambiguating rewrite for each one. They are asked to discard the example if it is offensive or low-quality due to issues in fluency or coherence.\nThe validation phase is performed by a subset of the authors to ensure high quality (details in \u00a7A.4). The authors review the two sets of annotations to revise and aggregate them into a single coherent annotation, optionally adding interpretations missed by both annotators. Validation is skipped when ei-\n4We refer to them as \u201clinguists\u201d elsewhere.\nther annotator discarded an example; the validators may additionally discard examples themselves.\nLinguists annotate a total of 2,616 examples. Due to the option for discarding, 2,020 examples emerge from the annotation phase, and after validation, there are a total of 1,503 final examples."
        },
        {
            "heading": "2.4 Agreement",
            "text": "To calculate inter-annotator agreement for validation, the four validators annotate a subset of 50 examples in common. The Fleiss \u03ba agreement score on the binary classification task for each label is 0.62 for contradiction, 0.65 for entailment, and 0.44 for neutral, thus ranging from \u201cmoderate\u201d to \u201csubstantial\u201d agreement."
        },
        {
            "heading": "2.5 AMBIENT Statistics",
            "text": "The final dataset, which combines curated and generated-then-annotated examples, consists of 1,645 examples. We sample 100 for a development set and treat the rest as the test set. The label distribution is shown in Figure 3.\nTo understand the types of ambiguity present in AMBIENT, the authors annotate a random subset of 100 ambiguous examples with the ambiguity type, among lexical, syntactic, figurative, pragmatic, scopal, coreference, and other (described in \u00a7A.6). Results are shown in the Type column of Table 1."
        },
        {
            "heading": "3 Does Ambiguity Explain Disagreement?",
            "text": "We conduct an analysis to understand how annotators behave on ambiguous input, under the traditional 3-way annotation scheme for NLI. We find that ambiguity is recognizable to individual workers and explains much of the label variation that emerges, thus challenging the popular assumption that example uncertainty should be modeled as \u201cdisagreement\u201d among annotators."
        },
        {
            "heading": "3.1 Setup",
            "text": "We recruit crowdworkers on Amazon Mechanical Turk to review ambiguous examples in AMBIENT.\nEach example is reviewed by 9 workers. The task is split into three steps, each appearing only after the earlier steps are complete.\n(i) Annotation of ambiguous example Following the traditional NLI labeling setup, crowdworkers are presented with the original ambiguous example alone, and asked to choose a single label.\n(ii) Recognition of disambiguations The ambiguous sentence of the example (either the premise or hypothesis) is isolated for consideration.5 Three candidate interpretations are presented in a random order, composed of the two disambiguations and a semantically similar \u201cdistractor\u201d. (In the case where an example has three interpretations, no distractor is included.) Workers are asked to indicate whether each sentence is a \u201cpossible interpretation\u201d of the isolated sentence. We instruct that this is subjective, and they should use their best judgment.\nThe distractor ensures that workers do not consider all sentences as valid readings, and is obtained by back-translating the ambiguous sentence with Yor\u00f9b\u00e1 using NLLB (Meta, 2022). A low-resource language is chosen so that the back-translation is a close, but often not entirely faithful, paraphrase.\n(iii) Annotation of disambiguated examples Three new NLI examples are obtained by substituting the ambiguous sentence of the original example with each candidate interpretation from (ii). Workers select a single NLI label for each new example."
        },
        {
            "heading": "3.2 Results",
            "text": "As hypothesized, the original ambiguous examples produce high disagreement, with a Fleiss \u03ba score of 0.12, considered \u201cslight\u201d agreement (step (i)). Disagreement is largely resolved on the corresponding disambiguated examples (step (iii)), with \u03ba increasing to 0.67, representing \u201csubstantial\u201d agreement.\nMoreover, annotators overwhelmingly recognize disambiguations as plausible interpretations of the ambiguous sentence (step (ii)). True disambiguations are marked plausible 96.7% of the time, compared to 46.7% for the distractor. On average, 93.7% of annotators accept all true interpretations, thus recognizing the full set of possibilities.\nThrough this experiment, we additionally establish crowdworker agreement with AMBIENT as the rate at which the majority vote recognizes the full\n5For simplicity, we only include examples where either the premise or the hypothesis is ambiguous (93.1% of examples).\nset of ambiguities (step (ii)) and verifies their labels (step (iii)). In this sense, the agreement rate is 89.7%. This points to the quality of the dataset and is used as a reference point for later experiments.\nOverall, input ambiguity is indeed a source of \u201cdisagreement\u201d in NLI under a single-label annotation scheme. However, we have shown that individual annotators overwhelmingly can recognize multiple possible readings of the input and their corresponding output labels, and much of this disagreement can be resolved in practice by incorporating disambiguation into the task. In this way, input ambiguity can be disentangled from annotator subjectivity."
        },
        {
            "heading": "4 Evaluating Pretrained Language Models",
            "text": "In our experiments, we investigate the extent to which understanding of ambiguity is acquired during the course of pretraining. Our three tests evaluate if LMs can directly generate relevant disambiguations (\u00a74.1), recognize the validity of plausible interpretations (\u00a74.2), and finally, model openended continuations reflecting different interpretations (\u00a74.3). For these tests, we consider only the ambiguous instances in AMBIENT.\nAs our set of LMs, we evaluate LLaMa (65B; Touvron et al., 2023) and GPT-3 (davinci), as well as instruction-tuned models FLAN-T5 (xxl; Chung et al., 2022), InstructGPT (text-davinci-003), ChatGPT (gpt-3.5-turbo), and the recent GPT-4."
        },
        {
            "heading": "4.1 Generating Disambiguations",
            "text": "We first study whether LMs can learn in-context to directly generate disambiguations and corresponding labels. We construct a natural prompt (see Table 2) by explaining that there is some ambiguity that makes the correctness of a \u201cclaim\u201d (hypothesis) difficult to resolve given the \u201ccontext\u201d (premise). For each test instance, we randomly sample 4 other test instances as in-context examples.\nAs there are multiple ways to express the same disambiguation, we perform both automatic and human evaluation. For the former, we match each generated disambiguation with a reference disambiguation based on the generated label.6 Following AMBIGQA, we score generations using the EDITF1 metric, which represents a disambiguation by\n6If the label verbalizer for a disambiguation does not correspond to any label in the reference label set, then the model receives a score of 0 for that disambiguation.\nits added and deleted unigrams, and computes the F1 score between the reference and the prediction.\nFor human evaluation, we use the same setup as the crowdworker experiment in \u00a73 on 50 randomly sampled examples, except without step (i). We use three workers per example, and consider the LM correct on an example if the majority vote indicates that each disambiguation is plausible (step (ii)) and selects the model-predicted NLI labels (step (iii)). Crowdworkers are not informed that disambiguations are model-generated.\nResults Shown in Table 4, the best model is GPT-4, achieving an EDIT-F1 score of 18.0% and human-judged correctness of 32.0%. The latter can be directly compared to crowdworker agreement with AMBIENT itself at 89.7% (\u00a73).\nOne strategy for attempting disambiguation we observe across models is restating the ambiguous sentence with additional context that directly affirms or negates the hypothesis, rather than making a targeted revision to clarify the ambiguity. In some cases, this \u201cshortcut\u201d does lead to technically correct disambiguations (and marked as such in human evaluation). For instance, for\nP: He always ignores his mother\u2019s advice to follow his own dreams. H: He follows his dreams.\nChatGPT disambiguates the premise by restating it, followed by \u201cand therefore does follow his dreams\u201d versus \u201cand therefore does not follow his dreams.\u201d The former forces the interpretation that he ignores her advice in order to follow his dreams; the latter the interpretation that his mother\u2019s advice is for him to follow his dreams. Thus, the human-judged correctness may overestimate the models\u2019 ability to precisely report the source of ambiguity."
        },
        {
            "heading": "4.2 Recognizing Disambiguations",
            "text": "For the next test, we focus on the ambiguous sentences alone (without the rest of the NLI example), and create a series of templated true and false statements about possible interpretations as shown in Table 3. For instance, it is both true that an ambiguous sentence may mean a particular interpretation, but also that it does not necessarily mean it. We consider the model prediction to be the token with the greater logit between True and False.7 We\n7As the API for ChatGPT and GPT-4 does not return token logits, we simply consider whether the top-1 token is correct. We find either True or False is the top token in 97.6% and 99.7% of examples, respectively, indicating the task is clear."
        },
        {
            "heading": "Template Correct Answer",
            "text": "where {a} denotes the ambiguous sentence and {d} a possible disambiguation. Given the infilled template followed by \u201cTrue or False? Answer:\u201d, the LM is expected to choose the correct answer.\nexecute this task zero-shot as the prompt template completely determines the label.\nResults The T/F Acc. column of Table 4 shows the accuracy averaged across the four templates. The best model (GPT-4) achieves only 63.0% compared to the random accuracy of 50%, with other models ranging between 49.6% and 57.7%. When we consider the proportion of disambiguations for which GPT-4 answers all four templates correctly, performance drops to 2.5%, below random guessing of 6.25%. We do not observe consistent trends across models on the per-template accuracy (shown in \u00a7C.2), though four of six models achieve the highest accuracy on template 1.\nFurthermore, we observe that LMs are not internally consistent across the questions. For example, for 76% of pairs of disambiguations (d1, d2) for the same ambiguous sentence a, GPT-4 both acknowledges that a may mean d1 and may mean d2 (template 1), yet also asserts that a can only mean d1 and can only mean d2 (template 4)."
        },
        {
            "heading": "4.3 Modeling Interpretation-Specific Continuations",
            "text": "Finally, we determine whether LMs, when conditioned on an ambiguous sentence, implicitly model different interpretations in their distributions of text continuations. Since LMs are trained to model words given context, understanding ambiguity should mean recognizing the union of the contexts for a sentence\u2019s interpretations.\nTo measure this, we obtain continuations for each interpretation, and quantify how \u201csurprised\u201d the LM is to see them when conditioned on the ambiguous sentence.8 Specifically, we first sample 100 continuations c \u223c P (\u22c5 \u2223 di) conditioned on each disambiguation di as context. Then, we compare the likelihood of c under the ambiguous sentence a versus the corresponding disambiguation di by computing logP (c \u2223 di) \u2212 logP (c \u2223 a). This describes how much the LM \u201csuffers\u201d by seeing the ambiguous instead of the unambiguous context,9 and is an unbiased estimate of the KL divergence between P (\u22c5 \u2223 di) and P (\u22c5 \u2223 a) (proof in \u00a7C.3):\nD(P (\u22c5 \u2223 di) \u2223\u2223 P (\u22c5 \u2223 a))\n= lim N\u2192\u221e\n1\nN\nN\n\u2211 j=1\ncj\u223cP (\u22c5\u2223di)\nlog P (cj \u2223 di) P (cj \u2223 a) .\nIntuitively, we want the KL divergence not to be too large \u2014 the LM should reasonably expect to see continuations for either interpretation. To quantify this, we introduce a \u201cdistractor\u201d sentence d\u0303 formed by replacing a randomly selected noun in a with a same-category word from ConceptNet (Speer et al.,\n8We exclude ChatGPT and GPT-4 from evaluation as the API does not enable calculating likelihood under the model.\n9This method assumes that the likelihood of a continuation is based on its meaning alone, but surface-form attributes like style are a confounding factor. See further discussion in \u00a7C.3.\n2017), e.g., replacing \u201cschool\u201d with \u201clibrary.\u201d We expect the LM to model continuations from both disambiguations di better than those from the distractor d\u0303, i.e., for all true disambiguations di,\nD(P (\u22c5 \u2223 d\u0303) \u2223\u2223 P (\u22c5 \u2223 a)) > D(P (\u22c5 \u2223 di) \u2223\u2223 P (\u22c5 \u2223 a)).\nWe call the fraction of ambiguous contexts for which this is true the KL ranking accuracy.\nResults The KL Rank. Acc. column of Table 4 shows that FLAN-T5 demonstrates the correct preference of continuations for 81.0% of examples, making it the best model here despite its poor performance in other settings. The inconsistent trends suggest that results are heavily dependent on how competence on ambiguity is operationalized. Nonetheless, ambiguity remains a severe challenge across models and across the suite of tests."
        },
        {
            "heading": "5 Evaluating Multilabel NLI Models",
            "text": "Given that language models still struggle to process ambiguity in \u00a74, we next investigate the effectiveness of finetuning them on existing NLI data collected in the line of work on underspecification and subjectivity in NLI.10 Here, we consider the discriminative task of multilabel NLI prediction, across both ambiguous and unambiguous examples in AMBIENT. Experimental details are in \u00a7D."
        },
        {
            "heading": "5.1 Methods",
            "text": "We experiment with methods that predict a single probability value, a distribution over labels, or a set of labels. We use the development set of AMBIENT to tune threshold(s) that map the output of these models onto a set of labels (see \u00a7D.1). All models are based on roberta-large, and we report results over 5 random seeds for model training.\nRegression models We train a regression model on Uncertain NLI (UNLI; Chen et al., 2020) that predicts a value on [0, 1] representing the probability of the hypothesis being true given the premise.\nDistributional models Distributional models aim to predict the distribution of annotator judgments. We use two models from prior work: 1) one trained on AmbiNLI (Meissner et al., 2021), with examples with multiple annotations from SNLI (Bowman et al., 2015) and MNLI, and 2)\n10The size of AMBIENT is not large enough for a training split; future efforts to annotate data in the fashion of AMBIENT might be able to address this issue."
        },
        {
            "heading": "Method and Train Set EM Macro F1 Group EM",
            "text": "a model trained through distribution distillation (Zhou et al., 2022), where a teacher model trained on SNLI + MNLI is used to re-annotate the data with soft labels then used to train a new model.\nMultilabel models Prior work trained a multilabel model (Jiang and de Marneffe, 2022) on the development sets of MNLI + ChaosNLI by turning distributional labels into discrete ones with a threshold of 0.2. In addition, we train a multilabel model on WANLI\u2019s train set (which has two annotations per example), as well as a classifier over sets which performs 7-way classification over the power set of NLI labels, minus the empty set."
        },
        {
            "heading": "5.2 Metrics",
            "text": "On the original examples, we calculate the macro F1 score and the exact match accuracy (EM); the latter requires the model to exactly predict the label set. We also report the group EM accuracy as the fraction of examples where the model exactly predicts the label set for both the original NLI example and all of its disambiguations."
        },
        {
            "heading": "5.3 Results",
            "text": "As shown in Table 5, the multilabel model trained on WANLI achieves the highest macro F1 score of 72.5%, and the classifier over sets achieves the best EM accuracy of 43.6% and group EM accuracy of 37.8%. While the EM accuracy is substantially higher than the random-guessing baseline of 1/7 = 14.3%, it is considerably short of 89.7%, the rate at which crowdworkers correctly predict the set of labels when presented with possible disambiguations (\u00a73). Overall, finetuning NLI models on existing data with label variation still leaves large room for improvement on the multilabel NLI task."
        },
        {
            "heading": "6 Case Study: Detecting Misleading Political Claims",
            "text": "We illustrate the value of ambiguity-sensitive models via a case study in detecting misleading political claims in the wild. Here, we use the key insight that for ambiguous sentences, some paraphrases are naturally disambiguating, as paraphrases must either preserve the ambiguity or paraphrase a particular interpretation. Therefore, if we cast a given sentence as the premise and a paraphrase as the hypothesis, a multilabel NLI model predicting two or more labels should indicate the presence of ambiguity. Moreover, the paraphrase resulting in this prediction should reveal the source of ambiguity.\nWe experimentally evaluate this idea on the development set of CLAIMDECOMP (Chen et al., 2022), which contains 200 claims with their PolitiFact fact-checks. The authors read each instance and mark whether the fact-check describes an issue of ambiguity or factuality (regardless of whether we perceive ambiguity ourselves). Then we paraphrase each claim 5 times with InstructGPT zeroshot, and apply the multilabel WANLI model from \u00a75, which achieved the highest F1 score, on each resulting NLI example. A claim is considered ambiguous if the model predicts more than one label for any paraphrase. Examples in Table 6.\nThis method recalls 88.8% of ambiguous claims. While precision is lower at 12.4%, qualitative inspection of false positives reveals many ambiguities that were left unmentioned in the fact-check, illustrating the potential of these tools to anticipate sources of misunderstanding. Ultimately, our analysis suggests that fact-checking as a more general problem may need refinement, due to the possible presence of both true and false interpretations. This case study shows only one use case of ambiguitysensitive models, and we hope for AMBIENT for benchmark further progress on this front."
        },
        {
            "heading": "7 Related Work",
            "text": "Ambiguity Ambiguity is a longstanding and well-studied issue for NLP tasks involving symbolic analyses of sentences, such as syntactic and semantic parsing (Church and Patil, 1982; Koller et al., 2008) or coreference resolution (Poesio and Artstein, 2005). However, as the field has recently shifted toward higher-level understanding and reasoning problems, ambiguity in language has been largely overlooked.\nIn the space of open-domain question-answering,\nthere are often issues of ambiguous or underspecified event and entity references (Min et al., 2020; Cole et al., 2023), leading to work on generating clarification questions (Kuhn et al., 2023; Krasheninnikov et al., 2022). In particular, our approach to ambiguity is inspired by AMBIGQA (Min et al., 2020), where the task input is disambiguated in natural language to account for variation in possible outputs. In contrast to open-domain questions, AMBIENT contains more diverse linguistic ambiguities whose resolution is a prerequisite to understanding meaning.\nRecent work has also studied ambiguous language in multi-modal settings: Stengel-Eskin et al. (2023) collected a set of ambiguous questions about images, and Pezzelle (2023) consider how visionlanguage models handle underspecified captions.\nOther work studies whether the confidence of coreference and NLI models is sensitive to ambiguity in synthetically-constructed input (Yuan et al., 2023; Ban et al., 2022). Going beyond task-specific models, we evaluate pretrained LMs for the language skill of managing ambiguity.\nHuman label variation Human label variation (Plank, 2022) is a broad phenomenon with three distinct sources, as summarized by Jiang and de Marneffe (2022): task ambiguity, subjectivity of annotator attitudes, and input ambiguity (our focus). Explored in contemporary work (Tamkin et al., 2023), task ambiguity arises when the task is underspecified with respect to the desired output; subjectivity is observed when different people disagree, such as for toxic language detection (Sap et al., 2022).\nThere is growing recognition of and interest in studying this variation, where the dominant approach is to model the distribution of human judgments (Pavlick and Kwiatkowski, 2019; Nie et al.,\n2020; Uma et al., 2021), potentially as a function of their demographic characteristics (Gordon et al., 2022). In our work, we argue that when uncertainty is in the input, we should instead directly characterize the underlying reasons for the uncertainty.\nNLI beyond three-way classification For NLI, the seminal work investigating label variation was Pavlick and Kwiatkowski (2019), and subsequent work collected more annotations (Nie et al., 2020) and modeled this variation (Zhou et al., 2022; Zhang et al., 2021). Other approaches aim to predict the probability of entailment (Chen et al., 2020; Zhang et al., 2017) or a fourth \u201cdisagreement\u201d label (Zhang and de Marneffe, 2021). We contribute another approach, where NLI models predict the set of labels for plausible readings.\nJiang and de Marneffe (2022) investigate MNLI data to taxonomize sources of disagreement, and identify \u201cuncertainty in sentence meaning\u201d as one source, though they named only lexical and implicature ambiguities. Our benchmark includes a wider coverage of ambiguities and our analysis further sheds light on the nature of the \u201cdisagreement.\u201d"
        },
        {
            "heading": "8 Conclusion",
            "text": "Ambiguity in language will become increasingly conspicuous as we push the limits of LM capabilities and build tools that engage with the nuances of natural language communication. We develop the first benchmark to evaluate whether language models recognize different readings of ambiguous text, and demonstrate that the task remains extremely challenging. We encourage future work to study the sensitivity of LMs to context and emphasis, investigate the presence of systematic biases in interpretation, and explore the promising space of real-world applications enabled by ambiguity-sensitive tools."
        },
        {
            "heading": "Acknowledgments",
            "text": "We would like to thank Nathan Schneider, Ellie Pavlick, Doug Downey, Ewin Tang, Roy Schwartz, Yanai Elazar, Valentina Pyatkin, and Ari Holtzman, as well as the greater UW NLP and AI2 community, for valuable feedback and discussion at different stages of this work. Our dataset would not have been possible without the expertise of our linguist annotators, which include Emma Miller, Sofia Y. Ahmed, Wendy Kempsell Jacinto, Maxine Appel, Edi Xin, Magdelina Thornton, Huijae Seo, Gita Dhungana, and Aldrich Gran Lapid, and 28 others.\nThis work was funded in part by the DARPA MCS program through NIWC Pacific (N66001-192-4031). We thank OpenAI for offering access to various models through the API. The first author is supported by the National Science Foundation Graduate Research Fellowship Program."
        },
        {
            "heading": "Limitations",
            "text": "In this work we collect a broad-coverage dataset of ambiguities, but the size and diversity are nonetheless limited due to the data sources and the effort required for expert annotation. We thus encourage future work to collect more data in the format of AMBIENT, especially for naturally-occurring ambiguities. In addition, we only study ambiguity phenomena in English, but how ambiguity manifests in other languages can vary greatly due to systematic typological factors or idiosyncratic differences. For example, while AMBIENT does not contain many instances of morphological ambiguity, these are very common in morphologically richer languages such as Turkish and Finnish. A systematic extension of our dataset and analyses to other languages would be exciting future work.\nThough LMs struggle across the board on our evaluations, this does not guarantee that they will not handle ambiguity well in other task settings or using other extraction methods. We observe that GPT-4 is the highest-performing model on two of the three evaluations (\u00a74.1, \u00a74.2), while the smallest FLAN-T5 performs best on the last evaluation (\u00a74.3). Scaling up general-purpose pretraining and reinforcement learning from human feedback (Ouyang et al., 2022) may lead to further gains, though we hypothesize that the trend will be unclear as larger LMs may overfit to more common interpretations at the expense of recognizing less common ones, which is especially detrimental for reasoning about misleading language."
        },
        {
            "heading": "Ethics Statement",
            "text": "We acknowledge that text generated from language models is susceptible to perpetuating social harms and containing toxic language (Sheng et al., 2019; Gehman et al., 2020). To address this, the annotators and validators of our dataset (\u00a72.3) were instructed to discard any examples that may be perceived as offensive. Nonetheless, it is possible that subtly harmful examples may have been overlooked and included in the final dataset.\nIn addition, we are cognizant of the asymmetrical relationship between requesters and workers in crowdsourcing (\u00a73). We took great care to pay fair wages, with a median hourly rate of $19.13, and were responsive to feedback and questions throughout the process (see \u00a7B.1 for details). The only personal information we collect is the worker IDs from Amazon Mechanical Turk, which we will not release. Both the linguist annotation (\u00a72.3) and crowdworker experiment (\u00a73) received IRB exemption."
        },
        {
            "heading": "A Dataset Creation Details",
            "text": ""
        },
        {
            "heading": "A.1 Curated Examples",
            "text": "The first author skimmed through several existing NLI datasets and manually identified examples that were both natural and contained salient ambiguities. Only a few examples were chosen from each dataset, to avoid overly redundant examples in AMBIENT. In Table 7, we show an example from each of the sources we drew from. They are directly annotated with the set of labels and disambiguations by the first author."
        },
        {
            "heading": "A.2 Generated Examples",
            "text": "The template for prompting GPT-3 to generate unlabeled NLI examples is shown in Table 8. The model we used is InstructGPT (text-davinci-002), queried on September 4, 2022, with top p = 0.9 (Holtzman et al., 2020), max tokens 120, and stop sequence \u201c\\n\\n\u201d. If the generated output is not correctly formatted with\n\u201c\\nSentence 2:\u201d in the sequence (which separates the premise and hypothesis), we discard the output. We sample 5 outputs per 21,273 possible prompts to obtain a total of 104,071 unlabeled examples.\nWe first employ simple heuristics to discard examples exhibiting observable failure cases. That is, we discard examples if 1) either the premise or hypothesis is shorter than 5 characters, 2) the premise and hypothesis are identical, 3) the generated example is copied from an in-context example, or 4) the examples contain some redundant patterns observed in the development phase. For instance, there are an abundance of generations with the exact premise, \u201cMary wants to try a little bit of every country\u2019s food on her trip around the world.\u201d After filtering based on these rules, 77,564 examples remain.\nNext, to further filter for likely-ambiguous instances, we use a multilabel RoBERTa-large model trained on WANLI and retain all examples where the model assigns probability \u2265 0.05 to more than one NLI label, indicating at least slight uncertainty in whether there can be multiple possible readings. Finally, to approximately balance the resulting examples (of course, exactly balancing would be impossible without gold labels), we keep an equal number of examples where the multilabel model predicts (according to the low threshold of 0.05) *ENTAIL, NEUTRAL+ and *CONTRADICT, NEUTRAL+, and all other examples with multiple labels predicted. Thus, the final set of generated examples is 16,826."
        },
        {
            "heading": "A.3 Linguist annotation",
            "text": "Of the generated examples, we ultimately annotate only 2,616 of them. This is due to the slow pace of expert annotation as the project went on, and the diminishing returns of annotating more data. Each example was annotated by two linguists. We discard an example if either linguist chose to discard it. The final set of examples is 2,020.\nOur expert annotators were 37 university-level linguistics students at the University of Washington, recruited through a Linguistics mailing list. They were paid $20/hour, in addition to $0.05 per example.\nA.4 Validation by authors The authors review all 2,020 examples, each with two annotations, combining and revising them into a single coherent annotation, or optionally discarding the example. The authors validated the ex-\nExample Disambiguation 1 Disambiguation 2 Source\nP: It is the only possibility of making the law the servant of the people, not the other way around. H: The law should be the servant of the people, not the other way around. *ENTAIL, NEUTRAL+ P: ... of making the law the servant of the people, as it should be, ... ENTAIL P: It is the only possibility that would lead to the law... NEUTRAL WANLI test\nP: Then he sobered. H: He was drunk. *ENTAIL, NEUTRAL+\nP: Then he sobered after drinking alcohol. ENTAIL P: Then he became more sensible. NEUTRAL MNLI dev\nP: Patrick did not manage to leave. H: Patrick tried to leave. *ENTAIL, NEUTRAL+\nP: ..., despite his attempt. ENTAIL\nP: ... , whether or not he tried. NEUTRAL\nIMPPRES\nP: LaBeouf had tried to bum a smoke from two strangers, unaware that one of them was a police officer. H: LaBeouf had tried to bum a smoke from a police officer. *ENTAIL, CONTRADICTION+ H: LaBeouf had tried to find a police officer to bum a smoke from. ENTAIL H: LeBeouf had tried to bum a smoke from someone who happened to be a police officer. CONTRADICTION NLI Diagnostics\nP: Jenny and Zoe solved the puzzle. H: They solved it together. *ENTAIL, CONTRADICTION+\nP: ... solved the puzzle together. ENTAIL P: ... each solved the puzzle. CONTRADICTION\nDistNLI\nP: John opened the door again. H: John opened the door before. *ENTAIL, NEUTRAL+\nP: John opened the door before, and did it again. ENTAIL P: The door was open before, and John opened the door again. NEUTRAL\nCarnie\nP: John wishes to marry Adrienne, a Frenchwoman. H: John wants to marry a Frenchwoman. *ENTAIL, NEUTRAL+ P: John wants to marry a certain woman who is French. ENTAIL P: John wants for his future wife to be French. NEUTRAL\nKearns\nP: You should visit Norway in the summer. H: Summer is a good season to visit Norway. *ENTAIL, NEUTRAL+ P: You should visit Norway the coming summer. ENTAIL P: You should visit Norway in the summer season. NEUTRAL Handwritten\nTable 7: An example in AMBIENT from each of the sources we draw from for the curated examples (\u00a72.1).\namples together on Zoom calls over the course of several weeks, actively discussing examples that they were unsure about and developing consistent standards. For instance, we chose to discard examples that boiled down to temporal ordering (e.g., \u201cI didn\u2019t realize that I left my keys at home\u201d either entails or contradicts \u201cI realized I left my keys at home\u201d, depending on the ordering of the sentences) or vagueness (e.g., \u201cHe is six feet tall\u201d may or may not entail \u201cHe is tall\u201d due to the vagueness of the word \u201ctall\u201d). This process revealed to us the extent of task underspecification in NLI (something we do not directly study in this work), and allowed us to focus on linguistic ambiguity.\nUltimately, 1,503 examples emerge from this phase."
        },
        {
            "heading": "A.5 Additional statistics",
            "text": "The disambiguating rewrites are, on average, 2.36 words longer than their ambiguous counterparts. Among the ambiguous examples, 74.3% have ambiguity in the premise and 32.6% in the hypothesis, with 6.9% having ambiguity in both. 97.5% of ambiguous sentences are labeled with two disambiguating rewrites, with the rest having three or more."
        },
        {
            "heading": "A.6 Ambiguity category annotation",
            "text": "The authors construct a taxonomy of ambiguity types by reviewing AMBIENT examples and categorizing possible sources of ambiguity, described in Table 9.\nThen two of the authors annotate 100 randomly\nsampled examples from AMBIENT for the ambiguity type. Each ambiguity is labeled with one category; examples may have multiple categories when they contain multiple ambiguities (e.g., both premise and hypothesis are ambiguous, or one sentence has multiple ambiguous parts). When multiple categories are plausible for a single ambiguity (e.g., a word is lexically ambiguous but pragmatics encourages the reading of one over the other), we choose the first one in the order of the table (here, lexical).\nNote that the distribution of ambiguity in AMBIENT does not necessarily reflect that of naturally-occurring ambiguity."
        },
        {
            "heading": "B Crowdworker experiment details",
            "text": ""
        },
        {
            "heading": "B.1 The crowdworkers",
            "text": "To qualify workers, we designed a qualification test with 5 questions that paid $5.00, open to the 64 annotators who revised and labeled NLI examples for the creation of WANLI. Of the 43 workers taking the test, 34 passed, though only 29 participated\nin the actual project. Through a poll taken after the annotation phase was completed, we find that all but one of the participants spoke English as a native language.\nFor the remainder of the study, crowdworkers were paid $0.40 per NLI example, which involved labeling the original ambiguous example, assessing the plausibility of three interpretations, and finally labeling three (closely related) NLI examples. At the end of data collection, we aggregate the earning and time spent from each crowdworker, and find that the median hourly rate was $19.13."
        },
        {
            "heading": "B.2 Setup details",
            "text": "To create a \u201cdistractor\u201d sentence among the true disambiguations, we use back-translation with Yor\u00f9b\u00e1 by employing the NLLB model (Meta, 2022) with greedy decoding for both Eng\u2192Yor and Yor\u2192Eng.\nIn case the generated distractor was an exact copy of the original ambiguous sentence, we repeat the Yor\u2192Eng leg of backtranslation with multinomial beam search, with a beam size of 5.0, top p = 1.0, and temperature t = 2.0. Of the 5 sequences returned, we randomly choose a sequence that is distinct from the original source sentence.\nFor instance, \u201cIt is currently March, and they plan to have their wedding scheduled for next December\u201d is back-translated to \u201cIt is March, and they are to be married in December,\u201d which is a faithful though somewhat lossy paraphrase, and 8/9 crowdworkers consider this a possible interpretation. On the other hand, \u201cThere will be more interesting seminars next quarter\u201d is back-translated to \u201cThere will be many more exciting conventions in the next half,\u201d which is not a faithful paraphrase and considered a possible interpretation by 1/9 workers."
        },
        {
            "heading": "C LM Experiment details and discussion",
            "text": ""
        },
        {
            "heading": "C.1 Generating Disambiguations",
            "text": "For the test in \u00a74.1, there is a different template for when the premise is ambiguous and when the\nhypothesis is ambiguous. For simplicity, we exclude the 6.9% of examples where both the premise and hypothesis are ambiguous. The former template is shown in Table 2; the latter contains only minor modifications. The instruction is \u201cIn each example, you will be given some context and a claim. Unfortunately, the claim has some ambiguity that affects whether it is correct. Enumerate two or three interpretations of the claim that lead to different judgments about its correctness.\u201d Then, immediately following the statement of the context and claim, \u201cWe don\u2019t know, because the claim can be interpreted in many different ways:\u201d.\nEDIT-F1 The EDIT-F1 metric represents a disambiguation by its added and deleted unigrams, and computes the F1 score between the reference and the prediction. For instance, the ambiguous sentence \u201cWe\u2019re afraid that LMs aren\u2019t modeling ambiguity\u201d can be disambiguated with edits * -afraid , +worried +. Predicted edits * -modeling , +representing + would receive an EDIT-F1 of zero, whereas sentence-similarity metrics like BLEU would give undue credit for the high overlap between preserved portions of the ambiguous sentence.\nAnalysis One strategy for attempting disambiguation we observe across model classes is restating the ambiguous sentence with additional context that directly affirms or negates the hypothesis, rather than making a targeted revision to clarify the ambiguity. In some cases, this \u201cshortcut\u201d does lead to technically correct disambiguations (and marked as such in human evaluation). For instance, for\nP: He always ignores his mother\u2019s advice to follow his own dreams. H: He follows his dreams.\nChatGPT disambiguates the premise by restating it, followed by \u201cand therefore does follow his dreams\u201d versus \u201cand therefore does not follow his dreams.\u201d The former forces the interpretation that he ignores her advice in order to follow his dreams; the latter the interpretation that his mother\u2019s advice is for him to follow his dreams. Thus, the human-judged correctness may overestimate the models\u2019 ability to precisely report the source of ambiguity."
        },
        {
            "heading": "C.2 Recognizing Disambiguations",
            "text": "For the test in \u00a74.2, accuracy on each template is shown in Table 10."
        },
        {
            "heading": "C.3 Recognizing Interpretation-Specific Continuations",
            "text": "This section includes implementation details and discussion for the test in \u00a74.3.\nKL divergence For a given disambiguation di, let X be a random variable equal to\nxc = log P (c \u2223 di) P (c \u2223 a) with prob. pc = P (c \u2223 di)\nIn \u00a74.3, we calculate the mean over Xj , independent and identically distributed copies of X:\nX\u0304n = 1\nN\nN\n\u2211 j=1 Xj\nFirst we show that X is an unbiased estimator for the KL divergence.\nE[X\u0304n] = E[X] = \u2211\nc\u2208X pcxc\n= \u2211 c\u2208X\nP (c \u2223 di) log P (c \u2223 di) P (c \u2223 a)\n= D(P (\u22c5 \u2223 di) \u2223\u2223 P (\u22c5 \u2223 a))\nwhere the first step follows from the linearity of expectation.\nAnd from the law of large numbers, we observe that X\u0304n tends to the KL divergence in the limit.\nlim n\u2192\u221e\nX\u0304n = E[X] = D(P (\u22c5 \u2223 di) \u2223\u2223 P (\u22c5 \u2223 a))\nPrepending a stem We append one of two stems to the beginning of the disambiguation (or distractor), for both generating continuations and measuring the likelihood of generated continuations. For instruction-tuned models, we append the prompt \u201cWrite a story. ,\u201d so that generating ontopic continuations is consistent with its instructionfollowing objective. For vanilla LMs, we append\na start quotation mark \u201c, which we find leads to significantly more topical continuations; otherwise, models may generate a newline and proceed to a new topic.\nCreating the distractor To create the distractor for an ambiguous sentence, we tokenize the sentence using spacy and randomly select a word w with the tag NOUN or PROPN (proper noun). Then we find the category node c where w has the IsA relation to c, i.e., w \u2192 c, with the largest weight. Finally, we randomly sample a same-category node w\n\u2032 \u2260 w, representing a single word, such that w\n\u2032 \u2192 c. Sometimes this replacement is not viable, e.g., when there are no nouns in the sentence, the noun is not in ConceptNet, or there are no same-category words. In this case, we next attempt to replace a pronoun with another heuristically-determined pronoun; failing all else, we randomly replace any noun or pronoun with the word \u201ccorgi.\u201d\nGenerating continuations Given either a true disambiguation or distractor as context, we generate continuations by sampling 100 single-sentence continuations from the full probability distribution, i.e., with top p = 1.0. To obtain a single sentence, we stop generation when a sentence-ending punctuation mark (one of !, ?, and .) is generated, and append a period back.\nLimitations Finally we discuss some limitations we observed with this test. First, the likelihood of a continuation conditioned on context depends not only on the meaning of the context, but also surfaceform attributes like the style and tone, which is a confounding factor in this experiment. Indeed, we observe that there can be a stylistic mismatch between original ambiguous sentence and its disambiguation, often with the latter being more stinted and formal. Generated continuations thus match the formal style, and have lower likelihood under the ambiguous sentence than a semantically equivalent, more casual paraphrase.\nIn addition, the \u201ccloseness\u201d of the distractor affects how easy or challenging the test is. We find that in most cases, the noun replacement procedure creates a sentence which we would expect to have a substantially different set of plausible continuations, potentially leading the test to be too \u201ceasy\u201d. Yet this varies with the noun being replaced, the replacement chosen, as well as the overall sentence in which it appears. Nonetheless, we require the\ndistractor for this test in order to make a judgment about the performance of the model."
        },
        {
            "heading": "D Multilabel Model Experiments",
            "text": ""
        },
        {
            "heading": "D.1 Methods",
            "text": "Here we describe the setup of NLI models that predict multiple labels as output (\u00a75.1). Multilabel models train separate binary classifier heads for each label on top of the transformer output. During inference, the labels are independently selected based on a threshold (shared across labels) tuned on the development set to maximize F1. Regression models train a regressor into [0, 1] that represents the probability of hypothesis being true given the premise. The development set is used to select a mapping from each NLI label into a continuous sub-range, and at inference time we pick all labels whose ranges overlap with the regressed value. Classifier over sets is a seven-way classifier over the power set of NLI labels minus the empty set. As it directly predicts a set of labels, this model requires no threshold tuning.\nThe median thresholds across 5 seeds from our experiments are shown in Table 11."
        },
        {
            "heading": "D.2 Training Details",
            "text": "For models from prior work, we replicate the training details to the best of our ability. All models are based on roberta-large.\nThe UNLI model (Chen et al., 2020) is trained on SNLI\u2019s training set (heuristically mapped to regression labels) for 1 epoch, then trained on uSNLI (human-annotated with regression labels) for 3 epochs.\nThe AmbiNLI model (Meissner et al., 2021) is first pretrained on single-label data from SNLI + MNLI for 3 epochs, then further finetuned on AmbiNLI for 2 epochs. AmbiNLI examples have distributional outputs, and is sourced from the development set of SNLI and MNLI (which contain 5 labels) and train set of UNLI (which are heuristically mapped to soft labels).\nThe Distribution Distillation model (Zhou et al., 2022) is trained for 2 epochs on SNLI + MNLI training examples that are re-annotated with the distributional output of a teacher model. The teacher model is a traditional three-way classification model trained on SNLI + MNLI.\nFinally, the multilabel model from Jiang and de Marneffe (2022) is trained on the development\nset of MNLI and ChaosNLI, where a label is considered present if 20% of annotators choose the label. The model with the lowest loss on held-out data over 30 epochs is selected as the final model."
        },
        {
            "heading": "E Political Claims Case Study",
            "text": "To paraphrase each political claim, we use InstructGPT (text-davinci-003) zero-shot with the simple prompt \u201cParaphrase the text. {Claim} Paraphrase:\u201d, and decode with top p = 0.9, to encourage both correctness and diversity among generated paraphrases."
        }
    ],
    "title": "We\u2019re Afraid Language Models Aren\u2019t Modeling Ambiguity",
    "year": 2023
}