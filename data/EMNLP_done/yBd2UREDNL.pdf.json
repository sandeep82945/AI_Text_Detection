{
    "abstractText": "Semi-supervised entity alignment (EA) is a practical and challenging task because of the lack of adequate labeled mappings as training data. Most works address this problem by generating pseudo mappings for unlabeled entities. However, they either suffer from the erroneous (noisy) pseudo mappings or largely ignore the uncertainty of pseudo mappings. In this paper, we propose a novel semi-supervised EA method, termed as MixTEA, which guides the model learning with an end-to-end mixture teaching of manually labeled mappings and probabilistic pseudo mappings. We firstly train a student model using few labeled mappings as standard. More importantly, in pseudo mapping learning, we propose a bi-directional voting (BDV) strategy that fuses the alignment decisions in different directions to estimate the uncertainty via the joint matching confidence score. Meanwhile, we also design a matching diversity-based rectification (MDR) module to adjust the pseudo mapping learning, thus reducing the negative influence of noisy mappings. Extensive results on benchmark datasets as well as further analyses demonstrate the superiority and the effectiveness of our proposed method.",
    "authors": [
        {
            "affiliations": [],
            "name": "Feng Xie"
        },
        {
            "affiliations": [],
            "name": "Xin Song"
        },
        {
            "affiliations": [],
            "name": "Xiang Zeng"
        },
        {
            "affiliations": [],
            "name": "Xuechen Zhao"
        },
        {
            "affiliations": [],
            "name": "Lei Tian"
        },
        {
            "affiliations": [],
            "name": "Bin Zhou(B"
        },
        {
            "affiliations": [],
            "name": "Yusong Tan"
        }
    ],
    "id": "SP:560b373aff07824f0c00b9fc3c4a1c78deb87bd6",
    "references": [
        {
            "authors": [
                "Antoine Bordes",
                "Nicolas Usunier",
                "Alberto GarciaDur\u00e1n",
                "Jason Weston",
                "Oksana Yakhnenko."
            ],
            "title": "Translating embeddings for modeling multirelational data",
            "venue": "Proceedings of the 26th International Conference on Neural Information Processing",
            "year": 2013
        },
        {
            "authors": [
                "Weishan Cai",
                "Wenjun Ma",
                "Lina Wei",
                "Yuncheng Jiang."
            ],
            "title": "Semi-supervised entity alignment via relation-based adaptive neighborhood matching",
            "venue": "IEEE Transactions on Knowledge and Data Engineering.",
            "year": 2022
        },
        {
            "authors": [
                "Liyi Chen",
                "Zhi Li",
                "Yijun Wang",
                "Tong Xu",
                "Zhefeng Wang",
                "Enhong Chen."
            ],
            "title": "Mmea: entity alignment for multi-modal knowledge graph",
            "venue": "Knowledge Science, Engineering and Management: 13th",
            "year": 2020
        },
        {
            "authors": [
                "Muhao Chen",
                "Yingtao Tian",
                "Kai-Wei Chang",
                "Steven Skiena",
                "Carlo Zaniolo."
            ],
            "title": "Co-training embeddings of knowledge graphs and entity descriptions for cross-lingual entity alignment",
            "venue": "Proceedings of the 27th International Joint Conference on Artificial",
            "year": 2018
        },
        {
            "authors": [
                "Muhao Chen",
                "Yingtao Tian",
                "Mohan Yang",
                "Carlo Zaniolo."
            ],
            "title": "Multilingual knowledge graph embeddings for cross-lingual knowledge alignment",
            "venue": "Proceedings of the 26th International Joint Conference on Artificial Intelligence, pages 1511\u20131517.",
            "year": 2017
        },
        {
            "authors": [
                "Avigdor Gal",
                "Haggai Roitman",
                "Tomer Sagi."
            ],
            "title": "From diversity-based prediction to better ontology & schema matching",
            "venue": "Proceedings of the 25th International Conference on World Wide Web, pages 1145\u20131155.",
            "year": 2016
        },
        {
            "authors": [
                "Yuqing Gao",
                "Jisheng Liang",
                "Benjamin Han",
                "Mohamed Yakout",
                "Ahmed Mohamed."
            ],
            "title": "Building a large-scale, accurate and fresh knowledge graph",
            "venue": "KDD-2018, Tutorial, 39:1939\u20131374.",
            "year": 2018
        },
        {
            "authors": [
                "Thomas N Kipf",
                "Max Welling."
            ],
            "title": "Semisupervised classification with graph convolutional networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "Zhenxi Lin",
                "Ziheng Zhang",
                "Meng Wang",
                "Yinghui Shi",
                "Xian Wu",
                "Yefeng Zheng."
            ],
            "title": "Multi-modal contrastive representation learning for entity alignment",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 2572\u20132584.",
            "year": 2022
        },
        {
            "authors": [
                "Fangyu Liu",
                "Muhao Chen",
                "Dan Roth",
                "Nigel Collier."
            ],
            "title": "Visual pivoting for (unsupervised) entity alignment",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 4257\u20134266.",
            "year": 2021
        },
        {
            "authors": [
                "Zhiyuan Liu",
                "Yixin Cao",
                "Liangming Pan",
                "Juanzi Li",
                "Tat-Seng Chua."
            ],
            "title": "Exploring and evaluating attributes, values, and structures for entity alignment",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Xin Mao",
                "Wenting Wang",
                "Huimin Xu",
                "Man Lan",
                "Yuanbin Wu."
            ],
            "title": "Mraea: an efficient and robust entity alignment approach for cross-lingual knowledge graph",
            "venue": "Proceedings of the 13th International Conference on Web Search and Data Mining, pages",
            "year": 2020
        },
        {
            "authors": [
                "Shichao Pei",
                "Lu Yu",
                "Robert Hoehndorf",
                "Xiangliang Zhang."
            ],
            "title": "Semi-supervised entity alignment via knowledge graph embedding with awareness of degree difference",
            "venue": "The World Wide Web Conference, pages 3130\u20133136.",
            "year": 2019
        },
        {
            "authors": [
                "Shichao Pei",
                "Lu Yu",
                "Guoxian Yu",
                "Xiangliang Zhang."
            ],
            "title": "Graph alignment with noisy supervision",
            "venue": "Proceedings of the ACM Web Conference 2022, pages 1104\u20131114.",
            "year": 2022
        },
        {
            "authors": [
                "Fabian M Suchanek",
                "Serge Abiteboul",
                "Pierre Senellart."
            ],
            "title": "Paris: Probabilistic alignment of relations, instances, and schema",
            "venue": "Proceedings of the VLDB Endowment (PVLDB), 5(3):157\u2013168.",
            "year": 2011
        },
        {
            "authors": [
                "Zequn Sun",
                "Wei Hu",
                "Chengkai Li."
            ],
            "title": "Crosslingual entity alignment via joint attribute-preserving embedding",
            "venue": "The Semantic Web\u2013ISWC 2017: 16th International Semantic Web Conference, Vienna, Austria, October 21\u201325, 2017, Proceedings, Part I 16,",
            "year": 2017
        },
        {
            "authors": [
                "Zequn Sun",
                "Wei Hu",
                "Chengming Wang",
                "Yuxin Wang",
                "Yuzhong Qu."
            ],
            "title": "Revisiting embedding-based entity alignment: A robust and adaptive method",
            "venue": "IEEE Transactions on Knowledge and Data Engineering.",
            "year": 2022
        },
        {
            "authors": [
                "Zequn Sun",
                "Wei Hu",
                "Qingheng Zhang",
                "Yuzhong Qu."
            ],
            "title": "Bootstrapping entity alignment with knowledge graph embedding",
            "venue": "Proceedings of the 27th International Joint Conference on Artificial Intelligence, pages 4396\u20134402.",
            "year": 2018
        },
        {
            "authors": [
                "Zequn Sun",
                "Chengming Wang",
                "Wei Hu",
                "Muhao Chen",
                "Jian Dai",
                "Wei Zhang",
                "Yuzhong Qu."
            ],
            "title": "Knowledge graph alignment network with gated multi-hop neighborhood aggregation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Zequn Sun",
                "Qingheng Zhang",
                "Wei Hu",
                "Chengming Wang",
                "Muhao Chen",
                "Farahnaz Akrami",
                "Chengkai Li."
            ],
            "title": "A benchmarking study of embeddingbased entity alignment for knowledge graphs",
            "venue": "Proceedings of the VLDB Endowment.",
            "year": 2020
        },
        {
            "authors": [
                "Antti Tarvainen",
                "Harri Valpola."
            ],
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Lio",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Zhichun Wang",
                "Qingsong Lv",
                "Xiaohan Lan",
                "Yu Zhang."
            ],
            "title": "Cross-lingual knowledge graph alignment via graph convolutional networks",
            "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing, pages 349\u2013357.",
            "year": 2018
        },
        {
            "authors": [
                "Feng Xie",
                "Xiang Zeng",
                "Bin Zhou",
                "Yusong Tan."
            ],
            "title": "Improving knowledge graph entity alignment with graph augmentation",
            "venue": "Advances in Knowledge Discovery and Data Mining: 27th Pacific-Asia Conference on Knowledge Discovery and Data Min-",
            "year": 2023
        },
        {
            "authors": [
                "Kexuan Xin",
                "Zequn Sun",
                "Wen Hua",
                "Wei Hu",
                "Xiaofang Zhou."
            ],
            "title": "Informed multi-context entity alignment",
            "venue": "Proceedings of the 15th ACM International Conference on Web Search and Data Mining, pages 1197\u20131205.",
            "year": 2022
        },
        {
            "authors": [
                "Kexuan Xin",
                "Zequn Sun",
                "Wen Hua",
                "Bing Liu",
                "Wei Hu",
                "Jianfeng Qu",
                "Xiaofang Zhou."
            ],
            "title": "Ensemble semi-supervised entity alignment via cycle-teaching",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 4281\u20134289.",
            "year": 2022
        },
        {
            "authors": [
                "Hsiu-Wei Yang",
                "Yanyan Zou",
                "Peng Shi",
                "Wei Lu",
                "Jimmy Lin",
                "Xu Sun."
            ],
            "title": "Aligning cross-lingual entities with multi-aspect information",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Donghan Yu",
                "Yiming Yang",
                "Ruohong Zhang",
                "Yuexin Wu."
            ],
            "title": "Knowledge embedding based graph convolutional network",
            "venue": "Proceedings of the Web Conference 2021, pages 1619\u20131628.",
            "year": 2021
        },
        {
            "authors": [
                "Weixin Zeng",
                "Xiang Zhao",
                "Jiuyang Tang",
                "Changjun Fan."
            ],
            "title": "Reinforced active entity alignment",
            "venue": "Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 2477\u20132486.",
            "year": 2021
        },
        {
            "authors": [
                "Qingheng Zhang",
                "Zequn Sun",
                "Wei Hu",
                "Muhao Chen",
                "Lingbing Guo",
                "Yuzhong Qu."
            ],
            "title": "Multi-view knowledge graph embedding for entity alignment",
            "venue": "Proceedings of the 28th International Joint Conference on Artificial Intelligence.",
            "year": 2019
        },
        {
            "authors": [
                "Zhedong Zheng",
                "Yi Yang."
            ],
            "title": "Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation",
            "venue": "International Journal of Computer Vision, 129(4):1106\u20131120.",
            "year": 2021
        },
        {
            "authors": [
                "Hao Zhu",
                "Ruobing Xie",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Iterative entity alignment via joint knowledge embeddings",
            "venue": "Proceedings of the 26th International Joint Conference on Artificial Intelligence, pages 4258\u20134264.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Entity alignment (EA) is a task at the heart of integrating heterogeneous knowledge graphs (KGs) and facilitating knowledge-driven applications, such as question answering, recommender systems, and semantic search (Gao et al., 2018). Embeddingbased EA methods (Chen et al., 2017; Wang et al., 2018; Sun et al., 2020a; Yu et al., 2021; Xin et al., 2022a) dominate current EA research and achieve promising alignment performance. Their general pipeline is to first encode the entities from different KGs as embeddings (latent representations) in a uni-space, and then find the most likely counterpart for each entity by performing all pairwise comparison. However, the pre-aligned mappings (i.e.,\ntraining data) are oftentimes insufficient, which is challenging for supervised embedding-based EA methods to learn informative entity embeddings. This happens because it is time-consuming and labour-intensive for technicians to manually annotate entity mappings in the large-scale KGs.\nTo remedy the lack of enough training data, some existing efforts explore alignment signals from the cheap and valuable unlabeled data in a semi-supervised manner. The most common semisupervised EA solution is using the self-training strategy, i.e., iteratively generating pseudo mappings and combining them with labeled mappings to augment the training data. For example, Zhu et al. (2017) propose IPTransE which involves an iterative process of predicting on unlabeled data and then treats the predictions above an elaborate threshold (confident predictions) as pseudo mappings for retraining. To further improve the accuracy of pseudo mappings, Sun et al. (2018) design a heuristic editing method to remove wrong alignment by considering one-to-one alignment constraint, while Mao et al. (2020) and Cai et al. (2022) utilize a bi-directional iterative strategy to determine pseudo mapping if and only if the two entities are mutually nearest neighbors of each other. Despite the encouraging results, existing semisupervised EA methods still face the following problems: (1) Uncertainty of pseudo mappings. Prior works have largely overlooked the uncertainty of pseudo mappings during semi-supervised training. Revisiting the self-training process, the generation of pseudo mappings is either black or white, i.e., an entity pair is either determined as a pseudo mapping or not. While in fact, different pseudo mappings have different uncertainties and contribute differently to model learning (Zheng and Yang, 2021). (2) Noisy pseudo mapping learning. The performance of semi-supervised EA methods depends heavily on the quality of pseudo mappings, while these pseudo mappings inevitably\ncontain much noise (i.e., False Positive mappings). Even worse, adding them into the training data would misguide the subsequent training process, thus causing error accumulation and further hurting the alignment performance.\nTo tackle the aforementioned limitations, in this paper, we propose a simple yet effective semisupervised EA solution, termed as MixTEA. To be specific, our method is based on a Teacher-Student architecture (Tarvainen and Valpola, 2017), which aims to generate pseudo mappings from a gradually evolving teacher model and guides the learning of a student model with a mixture teaching of labeled mappings and pseudo mappings. We explore the uncertainty of pseudo mappings via probabilistic pseudo mapping learning rather than directly adding \u201creliable\u201d pseudo mappings into the training data, which lends us to flexibly learn from pseudo mappings with different uncertainties. To achieve that, we propose a bi-directional voting (BDV) strategy that utilizes the consistency and confidence of alignment decisions in different directions to estimate the uncertainty via the joint matching confidence score (converted to matching probability after a softmax). Meanwhile, a matching diversity-based rectification (MDR) module is designed to adjust the pseudo mapping learning, thus reducing the influence of noisy mappings. Our contributions are summarized as follows:\n(I) We propose a novel semi-supervised EA framework, termed as MixTEA1, which guides the model\u2019s alignment learning with an end-to-end mixture teaching of manually labeled mappings and probabilistic pseudo mappings.\n(II) We introduce a bi-directional voting (BDV) strategy which utilizes the alignment decisions in different directions to estimate the uncertainty of pseudo mappings and design a matching diversitybased rectification (MDR) module to adjust the pseudo mapping learning, thus reducing the negative impacts of noise mappings.\n(III) We conduct extensive experiments and thorough analyses on benchmark datasets OpenEA (Sun et al., 2020b). The results demonstrate the superiority and effectiveness of our proposed method."
        },
        {
            "heading": "2 Related Works",
            "text": ""
        },
        {
            "heading": "2.1 Embedding-based Entity Alignment",
            "text": "While the recent years have witnessed the rapid development of deep learning techniques, embedding-\n1https://github.com/Xiefeng69/MixTEA\nbased EA approaches obtain promising results. Among them, some early studies (Chen et al., 2017; Sun et al., 2017) are based on the knowledge embedding methods, in which entities are embedded by exploring the fine-grained relational semantics. For example, MTransE (Chen et al., 2017) applies TransE (Bordes et al., 2013) as the KG encoder to embed different KGs into independent vector spaces and then conducts transitions via designed alignment modules. However, they need to carefully balance the weight between the encoder and alignment module in one unified optimization problem. Due to the powerful structure learning capability, Graph Neural Networks (GNNs) like GCN (Kipf and Welling, 2017) and GAT (Velic\u030ckovic\u0301 et al., 2018) have been employed as the encoder with Siamese architecture (i.e., shared-parameter) for many embedding-based models. GCN-Align (Wang et al., 2018) applies Graph Convolution Network (GCN) for the first time to capture neighborhood information and embed entities into a unified vector space, but it suffers from the structural heterogeneity of different KGs. To mitigate this issue and improve the structure learning, AliNet (Sun et al., 2020a) adopts multi-hop aggregation with a gating mechanism to expand neighborhood ranges for better structure modeling, and KE-GCN (Yu et al., 2021) combines GCN and knowledge embedding methods to jointly capture the rich structural features and relation semantics of entities. More recently, IMEA (Xin et al., 2022a) designs a Transformer-like architecture to encode multiple structural contexts in a KG while capturing alignment interactions across different KGs.\nIn addition, some works further improve the EA performance by introducing the side information about entities, such as entity names (Zhang et al., 2019), attributes (Liu et al., 2020), and literal descriptions (Yang et al., 2019). Afterward, a series of methods were proposed to integrate knowledge from different modalities (e.g., relational, visual, and numerical) to obtain joint entity representation for EA (Chen et al., 2020; Liu et al., 2021; Lin et al., 2022). However, these discriminative features are usually hard to collect, noise polluted, and privacy sensitive (Pei et al., 2022)."
        },
        {
            "heading": "2.2 Semi-supervised Entity Alignment",
            "text": "Since the manually labeled mappings used for training are usually insufficient, many semi-supervised EA methods have been proposed to take advan-\ntage of labeled mappings and the large amount of unlabeled data for alignment, which can provide a more practical solution in real scenarios. The mainstream solutions focus on iteratively generating pseudo mappings to compensate for the lack of training data. IPTransE (Zhu et al., 2017) applies threshold filtering-based self-training to yield pseudo mappings but it fails to obtain satisfactory performance since it brings much noise data, which would misguide the subsequent training. Besides, it is also hard to determine an appropriate threshold to select \u201cconfident\u201d pseudo mappings. KDCoE (Chen et al., 2018) performs co-training of KG embedding model and literal description embedding model to gradually propose new pseudo mappings and thus enhance the supervision of alignment learning for each other. To further improve the quality of pseudo mappings, BootEA (Sun et al., 2018) designs an editable strategy based on the one-to-one matching rule to deal with matching conflicts and MRAEA (Mao et al., 2020) proposes a bi-directional iterative strategy which imposes a mutually nearest neighbor constraint. Inspired by the success of self-training, RANM (Cai et al., 2022) proposes a relation-based adaptive neighborhood matching method for entity alignment and combines a bi-directional iterative co-training strategy, making become a natural semisupervised model. Moreover, CycTEA (Xin et al., 2022b) devises an effective ensemble framework to enable multiple alignment models (called aligners) to exchange their reliable entity mappings for more robust semi-supervised training, but it requires high complementarity among different aligners.\nAdditionally, other effective semi-supervised EA methods, such as SEA (Pei et al., 2019), RAC (Zeng et al., 2021), GAEA (Xie et al., 2023), focus on introducing specific loss terms (e.g., reconstruction loss, contrastive loss) via auxiliary tasks."
        },
        {
            "heading": "3 Problem Statement",
            "text": "A knowledge graph (KG) is formalized as G = (E ,R, T ), where E andR refer to the set of entities and the set of relations, respectively. T = E \u00d7 R \u00d7 E = {(h, r, t)|h, t \u2208 E \u2227 r \u2208 R} is the set of triples, where h, r, and t denote head entity (subject), relation, tail entity (object), respectively. Given a source KG Gs = (Es,Rs, Ts), a target KG Gt = (Et,Rt, Tt), and a small set of pre-aligned mappings (called training data) S = {(es, et)|es \u2208 Es\u2227et \u2208 Et\u2227es \u2261 et}, where\u2261means equivalence\nrelationship, entity alignment (EA) task pairs each source entity ei \u2208 Es via nearest neighbor (NN) search to identify its corresponding target entity ej \u2208 Et:\nej = arg min e\u0303j\u2208Et d(ei, e\u0303j) (1)\nwhere d(\u00b7) denotes distance metrics (e.g., Manhattan or Euclidean distance). Moreover, to mitigate the inadequacy of training data, semi-supervised EA methods make effort to explore more potential alignment signals over the vast unlabeled entities, i.e., E\u0302s and E\u0302t, which denote the unlabeled entity set of source KG and target KG, respectively."
        },
        {
            "heading": "4 Proposed Method",
            "text": "In this section, we present our proposed semisupervised EA method, called MixTEA, in Figure 1. MixTEA follows the teacher-student training scheme. The teacher model is performed to generate probabilistic pseudo mappings on unlabeled entities and student model is trained with an endto-end mixture teaching of manually labeled mappings and probabilistic pseudo mappings. Compared to previous methods that require filtering pseudo mappings via thresholds or constraints, the end-to-end training gradually improves the quality of pseudo mappings, and the more and more accurate pseudo mappings in turn benefit EA training."
        },
        {
            "heading": "4.1 KG Encoder",
            "text": "We first introduce the KG encoder (denoted as f(; \u03b8)) which utilizes neighborhood structures and relation semantics to embed entities from different KGs into a unified vector space. We randomly initialize the trainable entity embeddings Hent \u2208 R(|Es|+|Et|)\u00d7de and relation embeddings Hrel \u2208 R|Rs\u222aRt|\u00d7dr , where de and dr are the dimension of entities and relations, respectively.\nStructure modeling. Structural features are crucial since equivalent entities tend to have similar neighborhood contexts. Besides, leveraging multirange neighborhood structures is capable of providing more alignment evidence and mitigating the structural heterogeneity issue. In this work, we apply Graph Attention Network (GAT) (Velic\u030ckovic\u0301 et al., 2018) to allow an entity to selectively aggregate its surrounding information via attentive mechanism and we then recursively capture multirange structural features by stacking L layers:\nh(l)ei = \u03c3 (\u2211\nej\u2208Nei \u03b1ijWgh(l\u22121)ej\n) (2)\n\u03b1ij = exp(\u03c3(a\u22a4[Wghei \u2295Wghej ]))\u2211\nez\u2208Nei exp(\u03c3(a\u22a4[Wghei \u2295Wghez ]))\n(3) where \u22a4 represents transposition, \u2295 means concatenation, Wg and a are the layer-specific transformation parameter and attention transformation vector, respectively. Nei means the neighbor set of ei (including ei itself by adding a self-connection), and \u03b1ij indicates the learned importance of entity ej to entity ei. H(l) denotes the entity embedding matrix at l-th layer with H(0) = Hent. \u03c3(\u00b7) is the nonlinear function and we use ELU here.\nRelation modeling. Relation-level information which carries rich semantics is vital to align entities in KGs because two equivalent entities may share overlapping relations. Considering that relation directions, i.e., outward (ei \u2192 ej) and inward (ei \u2190 ej), have delicate impacts on characterizing the given target entity ei, we use two mean aggregators to gather outward and inward relation semantics separately to provide supplementary features for heterogeneous KGs:\nhr+ei = 1 |N r+ei | \u2211\nr\u2208N r+ei\nhrelr (4)\nhr\u2212ei = 1 |N r\u2212ei | \u2211\nr\u2208N r\u2212ei\nhrelr (5)\nwhere N r+ei and N r\u2212 ei are the sets of outward and inward relations of entity ei, respectively. Weighted concatenation. After capturing the contextual information of entities in terms of neighborhood structures and relation semantics, we con-\ncatenate intermediate features for entity ei to obtain the final entity representation:\nhei = \u2295 k\u2208K\n[ exp(wk)\u2211\nexp(w) \u00b7 hkei\n] (6)\nwhere K = {(1), ..., (L), r+, r\u2212} and w \u2208 R|K| is the trainable attention vector to adaptively control the flow of each feature. We feed w to a softmax before multiplication to ensure that the normalized weights sum to 1."
        },
        {
            "heading": "4.2 Alignment Learning with Mixture Teaching",
            "text": "In the following, we will introduce mixture teaching, which is reached by the supervised alignment learning and probabilistic pseudo mapping learning in an end-to-end training manner.\nTeacher-student architecture. Following Mean Teacher (Tarvainen and Valpola, 2017), we build our method which consists of two KG encoders with identical structure, called student model f(; \u03b8stu) and teacher model f(; \u03b8tea), respectively. The student model constantly updates its parameters supervised by the manually labeled mappings as standard and the teacher model is updated via the exponential moving average (EMA) (Tarvainen and Valpola, 2017) weights of the student model. Moreover, the student model also learns from the pseudo mappings generated by the teacher model to further improve its performance, in which the uncertainty of pseudo mappings is formalized as calculated matching probabilities. Specifically, we update the teacher model as follows:\n\u03b8tea \u2190 m\u03b8tea + (1\u2212m)\u03b8stu,m \u2208 [0, 1) (7)\nwhere \u03b8 denotes model weights, and m is a preset momentum hyperparameter that controls the teacher model to update and evolve smoothly.\nSupervised alignment learning. In order to make equivalent entities close to each other and unmatched entities pull away from each other in a unified space, we apply a margin-based alignment loss (Wang et al., 2018; Mao et al., 2020; Yu et al., 2021) supervised by pre-aligned mappings:\nLa = \u2211\n(es,et)\u2208S \u2211 (e\u0304s,e\u0304t)\u2208S\u0304 [||hes \u2212 het ||2\n+ \u03c1\u2212 ||he\u0304s \u2212 he\u0304t ||2]+ (8)\nwhere \u03c1 is a hyperparameter of margin, [x]+ = max{0, x} is to ensure non-negative output, S\u0304 denotes the set of negative entity mappings, and || \u00b7 ||2 means L2 distance (Euclidean distance). Negative mappings are sampled according to the cosine similarity of two entities (Sun et al., 2018).\nProbabilistic pseudo mapping learning. As mentioned above, the teacher model is responsible for generating probabilistic pseudo mappings for the student model to provide more alignment signals and thus enhance the alignment performance. Benefiting from the EMA update, the predictions of the teacher model can be seen as an ensemble version of the successive student models\u2019 predictions. Therefore it is more robust and stable for pseudo mapping generation. Moreover, bi-directional iterative strategy (Mao et al., 2020) reveals the asymmetric nature of alignment directions (i.e., sourceto-target and target-to-source), which can produce pseudo mappings based on the mutually nearest neighbor constraint. Inspired by this, we propose a bi-directional voting (BDV) strategy which fuses alignment decisions in each direction to yield more comprehensive pseudo mappings and model their uncertainty via the joint matching confidence score. Concretely, after encoding, we can first obtain the similarity matrix by performing pairwise similarity calculation between the unlabeled source and target entities as follows:\nMteas\u2192t = sim(E\u0302s, E\u0302t, \u03b8tea) \u2208 R|E\u0302s|\u00d7|E\u0302t| (9)\nwhere sim(\u00b7) denotes cosine similarity function. Mteas\u2192t and Mteat\u2192s represent similarity matrices in different directions between source and target entities, and Mteat\u2192s is the transposition of Mteas\u2192t (i.e., Mteat\u2192s = (Mteas\u2192t)\u22a4). Next, for each matrix, we pick up the entity pair which has the maximum\npredicted similarity in each row as the pseudo mapping and then we combine the results of the pseudo mappings in different directions weighted by their last Hit@1 scores on validation data to obtain the final pseudo mapping matrix:\nPtea = \u03b2 \u00b7 g(Mteas\u2192t) + (1\u2212 \u03b2) \u00b7 g(Mteat\u2192s)\u22a4 (10)\n\u03b2 = valid(Mteas\u2192t)\nvalid(Mteas\u2192t) + valid(Mteat\u2192s) (11)\ng(M) = [mi,j ] = {\n1, if j = argmaxj Mi,j 0, otherwise\n(12) where g(\u00b7) is the function that converts the similarity matrix to a one-hot matrix (i.e., the only position with a value 1 at each row of the matrix indicates the pseudo mapping). In this manner, we arrive at the final pseudo mapping matrix Ptea generated by the teacher model, in which each pseudo-mapping is associated with a joint matching confidence score (the higher the joint matching confidence, the less the uncertainty). Different from the bi-directional iterative strategy, we use the voting consistency and matching confidence of alignment decisions in different directions to facilitate uncertainty estimation. Specifically, given an entity pair (e\u0302i, e\u0302j), its confidence Pteai,j is 1 when and only when both directions unanimously vote this entity pair as a pseudo mapping, otherwise its confidence is in the interval (0,1) when only one direction votes for it and 0 when no direction votes for it (i.e., this entity pair will not be regarded as a pseudo mapping).\nIn addition, the ideal predictions of EA need to satisfy the one-to-one matching constraint (Suchanek et al., 2011; Sun et al., 2018), i.e., a source entity can be matched with at most one target entity, and vice versa. However, the joint\ndecision voting process inevitably yields matching conflicts due to the existence of erroneous (noisy) mappings. Inspired by Gal et al. (2016), we further propose a matching diversity-based rectification (MDR) module to adjust the pseudo mapping learning, thus mitigating the influence of noisy mappings dynamically. We denote Mstu (i.e., Mstui,j = sim(e\u0302i, e\u0302j ; \u03b8stu)) as the similarity matrix calculated based on the student model and define a Cross-Entropy (CE) loss between Mstu and Ptea rectified by matching diversity:\nP\u0303teai,j = Pteai,j\u2211 Pteai: + \u2211 Ptea:j \u2212 Pteai,j (13)\nLu = \u2211|E\u0302s|\ni=1 Cross-Entropy(Mstui: , P\u0303 tea i: ) (14)\nwhere P\u0303tea denotes the rectified pseudo mapping matrix. To be specific, the designed rectification term (Eq. (13)) measures how much a potential pseudo mapping deviates (in terms of joint matching confidence score) from other competing pseudo mappings in Pteai: and Ptea:j . The larger the deviation, the greater the penalty for this pseudo mapping, and vice versa. Notably, both Mstu and P\u0303tea are fed into a softmax to be converted to probability distributions before CE to implement probabilistic pseudo mapping learning. Besides, an illustrative example of generating the probabilistic pseudo mapping matrix is provided in Figure 2.\nOptimization. Finally, we minimize the following combined loss function (final objective) to optimize the student model in an end-to-end training manner:\nmin \u03b8stu\nLa + \u03bbLu (15)\nwhere \u03bb is a ramp-up weighting coefficient used to weight between the supervised alignment learning (i.e., La) and pseudo mappings learning (i.e., Lu). In the beginning, the optimization is dominated by La and during the ramp-up period, Lu will gradually participate in the training to provide more alignment signals. The overall optimization process is outlined in Algorithm 1 (Appendix A), where the student model and the teacher model are updated alternately, and the final student model is utilized for EA inference (Eq. (1)) on test data."
        },
        {
            "heading": "5 Experimental Setup",
            "text": ""
        },
        {
            "heading": "5.1 Data and Evaluation Metrics",
            "text": "We evaluate our method on the 15K benchmark dataset (V1) in OpenEA (Sun et al., 2020b) since\nthe entities thereof follow the degree distribution in real-world KGs. The brief information of experimental data is shown in Table 3 (Appendix B). It contains two cross-lingual settings, i.e., EN-FR15K (English-to-French) and EN-DE-15K (Englishto-German), and two monolingual settings, i.e., D-W-15K (DBPedia-to-Wikidata) and D-Y-15K (DBPedia-to-YAGO). Following the data splits in OpenEA, we use the same split setting where 20%, 10%, and 70% pre-aligned mappings are utilized for training, validation, and testing, respectively.\nEntity alignment is a typical ranking problem, where we obtain a target entity ranking list for each source entity by sorting the similarity scores in descending order. We use Hits@k (k=1, 5) and Mean Reciprocal Rank (MRR) as the evaluation metrics (Sun et al., 2020b; Xin et al., 2022a). Hits@k is to measure the alignment accuracy, while MRR measures the average performance of ranking over all test samples. The higher the Hits@k and MRR, the better the alignment performance."
        },
        {
            "heading": "5.2 Baseline Methods",
            "text": "We choose the methods from the related work as baselines and divide them into two classes below:\n\u2022 Structure-based methods. These methods focus on capturing useful structural context to enrich entity representation, such as (1) MTransE (Chen et al., 2017), (2) GCN-Align (Wang et al., 2018), (3) AliNet (Sun et al., 2020a), (4) KE-GCN (Yu et al., 2021) and (5) IMEA (Xin et al., 2022a).\n\u2022 Semi-supervised methods. These methods aim to explore alignment signals from unlabeled entities, such as (1) IPTransE (Zhu et al., 2017), (2) SEA (Pei et al., 2019), (3) KDCoE (Chen et al., 2018), (4) BootEA (Sun et al., 2018), (5) MRAEA (Mao et al., 2020), (6) RANM (Cai et al., 2022) and (7) GAEA (Xie et al., 2023).\nAs our method and the above baselines only contain a single model and mainly rely on structural information, for a fair comparison, we do not compare with ensemble-based frameworks (e.g., CycTEA (Xin et al., 2022b)) and models infusing side information from multi-modality (e.g., EVA (Liu et al., 2021), RoadEA (Sun et al., 2022)). For the baseline RANM, we remove the name channel to guarantee a fair comparison."
        },
        {
            "heading": "5.3 Implementation Details",
            "text": "All the experiments are performed in PyTorch on an NVIDIA GeForce RTX 3090 GPU. Following\nModels EN-FR-15K EN-DE-15K Hit@1 Hit@5 MRR Hit@1 Hit@5 MRR w/o rel. .471 .732 .586 .679 .839 .749 w/o Lu .485 .716 .589 .671 .836 .744 w/o BDV .556 .781 .656 .707 .863 .777 w/o MDR .560 .791 .663 .711 .863 .779 w/o B&M .542 .771 .644 .694 .857 .766 MixTEA .582 .807 .680 .724 .877 .797 Models D-W-15K D-Y-15K Hit@1 Hit@5 MRR Hit@1 Hit@5 MRR w/o rel. .552 .756 .641 .700 .841 .761 w/o Lu .520 .708 .605 .541 .684 .605 w/o BDV .629 .812 .709 .725 .856 .783 w/o MDR .635 .822 .718 .731 .862 .788 w/o B&M .609 .802 .693 .712 .849 .774 MixTEA .647 .832 .731 .748 .871 .802\nTable 2: Ablation test results.\nOpenEA (Sun et al., 2020b), we report the average results of five-fold cross-validation. The embedding dimensions of entities de and relations dr are set to 256 and 128, respectively, the number of GAT layer L is 2, the margin \u03c1 is 2.0, and the momentum m is 0.9. In the EA inference phase, we use Cosine distance as the distance metric and apply Faiss2 to perform NN search efficiently. The default alignment direction is from left to right, e.g., in D-W-15K, we regard DBpedia as the source KG and seek to find the counterparts of source entities in the target KG Wikidata. The details of hyperparameter settings are shown in Appendix C.\n2https://github.com/facebookresearch/faiss"
        },
        {
            "heading": "6 Experimental Results",
            "text": ""
        },
        {
            "heading": "6.1 Performance Comparison",
            "text": "Table 1 reports the experimental results of all the methods on the OpenEA 15K datasets. Even utilizing only the structure information, KE-GCN and IMEA achieve inspiring performance by exploring the rich structural contexts. However, they are hard to further improve the performance because suffer from the lack of enough training data. We also observe that some semi-supervised EA methods (e.g., IPTransE and SEA) fail to outperform these structure-based EA methods, reflecting the fact that both encoder design and semi-supervised strategy are important components of facilitating high-accuracy EA. IPTransE obtains unsatisfactory alignment results since it produces many noise pseudo mappings during the self-training process but does not design an appropriate mechanism to eliminate the influence of noise. Besides, the performance of KDCoE is unstable. According to Sun et al. (2020b), this is because many entities lack textual descriptions, thus preventing the model from finding complementary mappings for co-training from the textual description embedding model. BootEA and MRAEA are competitive baselines in semi-supervised EA domain. Nevertheless, BootEA needs a carefully fine-tuned confidence threshold to filter pseudo mappings, which often leads to unstability, while MRAEA and RANM still follows the data augmentation paradigm, which ignores the uncertainty of pseudo mappings and\nis prone to cause error accumulation. Although GAEA learns representations of vast unseen entities via contrastive learning, its performance is unstable. The bottom part of Table 1 shows our method consistently achieves the best performance in all tasks with a small standard deviation (std.). More precisely, our model surpasses state-of-theart baselines averagely by 3.1%, 3.3%, and 3.5% in terms of Hit@1, Hit@5, and MRR, respectively."
        },
        {
            "heading": "6.2 Ablation Study",
            "text": "To verify the effectiveness of our method, we perform the ablation study with the following variant settings: (1) w/o rel. removes the relation modeling. (2) w/o Lu removes probabilistic pseudo mapping learning. (3) w/o BDV only considers EA decisions in the default alignment direction to generate pseudo mappings instead of applying the bidirectional voting strategy (i.e., Ptea = g(Mteas\u2192t)). (4) w/o MDR removes matching diversity-based rectification module in pseudo mappings learning. (5) w/o B&M denotes that the complete model deletes both BDV and MDR module.\nThe ablation results are shown in Table 2. We can observe that the complete model achieves the best experimental results, which indicates that each component in our model design contributes to the performance improvement. Removing relation modeling from entity representation causes performance drops, which identifies the relation semantics can help in enriching the expressiveness of entity representations. W/o Lu caused the most significant performance degradation, especially in monolingual settings, showing the crucial role of the pseudo mapping learning in general. The results of w/o BDV and w/o MDR suggest that the bidirectional voting strategy and matching diversitybased rectification module can do benefit to improving the quality of pseudo mapping learning. W/o B&M also demonstrates that the combination of BDV and MDR can further improve the alignment performance. Although w/o BDV only takes EA decisions in one direction into account, it still inevitably brings matching conflicts since NN search neglects the inter-dependency between different EA decisions. Compared to w/o B&M, the MDR in w/o BDV has a certain positive effect, which indicates that our proposed MDR can be applied to other pseudo mapping generation algorithms and help the models to train better."
        },
        {
            "heading": "6.3 Auxiliary Experiments",
            "text": "Training visualization. To inspect our method comprehensively, we also plot the test Hit@1 curve throughout the training epochs in Figure 3. KG Encoder (th=0.9) represents the KG Encoder described in Sec. 4.1 applying the self-training with threshold=0.9 to generate pseudo mappings every 20 epochs. We control the same experimental settings to remove the performance perturbations induced by different parameters. From Figure 3, we observe that our method converges quickly and achieves the best and most stable alignment performance. The performance of the KG Encoder gradually decreases in the later stages since it gets stuck in overfitting to the limited training data. Although self-training brings some performance gains after data augmentation in the early stages, the performance drops dramatically in the later stages. This is because it involves many noise pseudo mappings and causes error accumulation as the self-training continues. In the later stages, self-training has difficulty in further generating new mappings while existing erroneous mappings constantly misguide the model training, thus hurting the performance.\nHyperparameter analysis. We design hyperparameter experiments to investigate the performance varies with some hyperparameters. Due to the space limitation, these experimental results and analyses are listed in Appendix D."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we propose a novel semi-supervised EA framework, termed as MixTEA, which guides the model learning with an end-to-end mixture teaching of manually labeled mappings and probabilistic pseudo mappings. Meanwhile, we propose a bi-directional voting (BDV) strategy and a matching diversity-based rectification (MDR) module to assist the probabilistic pseudo mapping learning. Experimental results on benchmark datasets show the effectiveness of our proposed method.\nLimitations\nAlthough we have demonstrated the effectiveness of MixTEA, there are still some limitations that should be addressed in the future: (1) Currently, we only utilize structural contexts which are abundant and always available in KGs to embed entities. However, when side information (e.g., visual contexts, literal contexts) is available, MixTEA needs to be extended into a more comprehensive EA framework and ensure that it does not become over-complex in the teacher-student architecture. Therefore, how to involve this side information is our future work. (2) Vanilla self-training iteratively generates pseudo mappings and adds them to the training data, where the technicians can perform spot checks during model training to monitor the quality of pseudo mappings. While MixTEA computes probabilistic pseudo mapping matrix and performs end-to-end training, thus making it hard to provide explicit entity mappings for the technicians to check their correctness. Therefore, it is imperative to design a strategy to combine the selftraining and probabilistic pseudo mapping learning to enhance the interpretability and operability.\nEthics Statement\nThis work does not involve any discrimination, social bias, or private data. Therefore, we believe that our study complies with the ACL Ethics Policy."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank the anonymous reviewers for their comments. This work is supported by the National Natural Science Foundation of China No. 62172428."
        },
        {
            "heading": "A Pseudocode of Training Procedure",
            "text": "Algorithm 1 Training Procedure Input: Knowledge graphs Gs and Gt; pre-aligned entity map-\npings S; unlabeled entity set E\u0302s and E\u0302t; momentum m; margin \u03c1. Output: the student encoder parameters \u03b8stu. 1: Initialize entity embeddings and relation embeddings; 2: Initialize teacher model \u03b8tea and student model \u03b8stu; 3: for each epoch do 4: Encode entities using f(; \u03b8stu) and f(; \u03b8tea); 5: Calculate La by supervised learning in Eq. (8); 6: Generate pseudo mapping matrix via BDV strategy; 7: Rectify pseudo mapping matrix via MDR module; 8: Calculate Lu by pseudo mapping learning in Eq. (14);\n9: \u03b8stu \u2190 BackProp(La + \u03bbLc); Adam Update 10: \u03b8tea \u2190 m\u03b8tea + (1\u2212m)\u03b8stu; 11: end for 12: return the student encoder parameters \u03b8stu"
        },
        {
            "heading": "B Dataset Statistics",
            "text": ""
        },
        {
            "heading": "C Hyperparameter Details",
            "text": "We tune the hyperparameters for our proposed MixTEA. The setting values and search ranges of hyperparameters are described in Table 4."
        },
        {
            "heading": "D Hyperparameter Analysis",
            "text": "The impact of different GAT layers. We vary the number of GAT layer L from 1 to 4 and the\nquantitative results are illustrated in Figure 4 (a). L=1 results in poor alignment performance due to the limited structural modeling power. The best performance is achieved when L=2, except for the D-Y-15K task. Increasing L will not bring further performance improvement, we infer that there is overfitting or oversmoothing during neighborhood aggregation. In D-Y-15K, the optimal performance is obtained when L=4. The possible reason is that in D-Y-15K, the two KGs have relatively sparse structure information (as shown in Table 3 in Appendix B), therefore they need to capture more alignment evidence from distant neighbors.\nThe impact of momentum parameter. We investigate the momentum parameter m in [0, 0.9, 0.99] and the results in EN-DE-15K and D-W-15K tasks are presented in Figure 4 (b). We found that our method maintains good performance under different momentum settings, demonstrating that our method is insensitive to m. In addition, a proper m such as 0.9 brings certain performance improvements, which indicates that the EMA update manner can facilitate the teacher model to yielding stable and robust pseudo mappings."
        }
    ],
    "title": "MixTEA: Semi-supervised Entity Alignment with Mixture Teaching",
    "year": 2023
}