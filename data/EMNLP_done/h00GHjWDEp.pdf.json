{
    "abstractText": "Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gains on FOLIO and a balanced subset of ProofWriter for three different models in nearly all experimental conditions we evaluate. On ProofWriter, augmenting the comparatively small open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5 and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%, respectively. When used with GPT-4, LINC scores 26% higher than CoT on ProofWriter while performing comparatively on FOLIO. Further analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes. We thus provide promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers. All corresponding code is publicly available.1 \u2217 Author order randomized; all reserve the right to list their name first. 1 https://github.com/benlipkin/linc",
    "authors": [
        {
            "affiliations": [],
            "name": "Theo X. Olausson"
        },
        {
            "affiliations": [],
            "name": "Alex Gu"
        },
        {
            "affiliations": [],
            "name": "Benjamin Lipkin"
        },
        {
            "affiliations": [],
            "name": "Cedegao E. Zhang"
        },
        {
            "affiliations": [],
            "name": "Armando Solar-Lezama"
        },
        {
            "affiliations": [],
            "name": "Joshua B. Tenenbaum"
        },
        {
            "affiliations": [],
            "name": "Roger Levy"
        }
    ],
    "id": "SP:6c105f2f3514dbf3fe87f186d49d050062b6e54a",
    "references": [
        {
            "authors": [
                "Cem Anil",
                "Yuhuai Wu",
                "Anders Andreassen",
                "Aitor Lewkowycz",
                "Vedant Misra",
                "Vinay Ramasesh",
                "Ambrose Slone",
                "Guy Gur-Ari",
                "Ethan Dyer",
                "Behnam Neyshabur."
            ],
            "title": "Exploring length generalization in large language models",
            "venue": "Advances in Neural",
            "year": 2022
        },
        {
            "authors": [
                "Zhangir Azerbayev",
                "Bartosz Piotrowski",
                "Hailey Schoelkopf",
                "Edward W Ayers",
                "Dragomir Radev",
                "Jeremy Avigad."
            ],
            "title": "ProofNet: Autoformalizing and formally proving undergraduate-level mathematics",
            "venue": "arXiv preprint arXiv:2302.12433. (Cited on pg.",
            "year": 2023
        },
        {
            "authors": [
                "David Barker-Plummer",
                "Jon Barwise",
                "John Etchemendy."
            ],
            "title": "Language, proof, and logic, 2 edition",
            "venue": "Center for the Study of Language and Information. (Cited on pg. 2)",
            "year": 2011
        },
        {
            "authors": [
                "Mohammad Bavarian",
                "Heewoo Jun",
                "Nikolas Tezak",
                "John Schulman",
                "Christine McLeavey",
                "Jerry Tworek",
                "Mark Chen."
            ],
            "title": "Efficient training of language models to fill in the middle",
            "venue": "arXiv preprint arXiv:2207.14255. (Cited on pg. 15)",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Berant",
                "Andrew Chou",
                "Roy Frostig",
                "Percy Liang."
            ],
            "title": "Semantic parsing on Freebase from question-answer pairs",
            "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533\u20131544, Seattle, Wash-",
            "year": 2013
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "year": 2020
        },
        {
            "authors": [
                "John P Burgess."
            ],
            "title": "Philosophical logic",
            "venue": "Princeton University Press. (Cited on pg. 10)",
            "year": 2009
        },
        {
            "authors": [
                "Wenhu Chen",
                "Xueguang Ma",
                "Xinyi Wang",
                "William W Cohen."
            ],
            "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "venue": "arXiv preprint arXiv:2211.12588. (Cited on pg. 10)",
            "year": 2022
        },
        {
            "authors": [
                "Xinyun Chen",
                "Maxwell Lin",
                "Nathanael Sch\u00e4rli",
                "Denny Zhou."
            ],
            "title": "Teaching large language models to self-debug",
            "venue": "arXiv preprint arXiv:2304.05128. (Cited on pg. 9)",
            "year": 2023
        },
        {
            "authors": [
                "Yongchao Chen",
                "Rujul Gandhi",
                "Yang Zhang",
                "Chuchu Fan."
            ],
            "title": "NL2TL: Transforming natural languages to temporal logics using large language models",
            "venue": "arXiv preprint arXiv:2305.07766. (Cited on pg. 9)",
            "year": 2023
        },
        {
            "authors": [
                "Zhoujun Cheng",
                "Tianbao Xie",
                "Peng Shi",
                "Chengzu Li",
                "Rahul Nadkarni",
                "Yushi Hu",
                "Caiming Xiong",
                "Dragomir Radev",
                "Mari Ostendorf",
                "Luke Zettlemoyer",
                "Noah A. Smith",
                "Tao Yu."
            ],
            "title": "Binding language models in symbolic languages",
            "venue": "The Eleventh In-",
            "year": 2023
        },
        {
            "authors": [
                "Matthias Cosler",
                "Christopher Hahn",
                "Daniel Mendoza",
                "Frederik Schmitt",
                "Caroline Trippel."
            ],
            "title": "nl2spec: Interactively translating unstructured natural language to temporal logics with large language models",
            "venue": "Computer Aided Verification, pages 383\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Antonia Creswell",
                "Murray Shanahan",
                "Irina Higgins."
            ],
            "title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "venue": "The Eleventh International Conference on Learning Representations. (Cited on pg. 1, 3, 8)",
            "year": 2023
        },
        {
            "authors": [
                "David Dohan",
                "Winnie Xu",
                "Aitor Lewkowycz",
                "Jacob Austin",
                "David Bieber",
                "Raphael Gontijo Lopes",
                "Yuhuai Wu",
                "Henryk Michalewski",
                "Rif A Saurous",
                "Jascha Sohl-Dickstein"
            ],
            "title": "Language model cascades",
            "year": 2022
        },
        {
            "authors": [
                "Iddo Drori",
                "Sarah Zhang",
                "Reece Shuttleworth",
                "Leonard Tang",
                "Albert Lu",
                "Elizabeth Ke",
                "Kevin Liu",
                "Linda Chen",
                "Sunny Tran",
                "Newman Cheng"
            ],
            "title": "A neural network solves, explains, and generates university math problems by program synthesis and few",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Drozdov",
                "Nathanael Sch\u00e4rli",
                "Ekin Aky\u00fcrek",
                "Nathan Scales",
                "Xinying Song",
                "Xinyun Chen",
                "Olivier Bousquet",
                "Denny Zhou."
            ],
            "title": "Compositional semantic parsing with large language models",
            "venue": "arXiv preprint arXiv:2209.15003. (Cited on pg. 8)",
            "year": 2022
        },
        {
            "authors": [
                "Nouha Dziri",
                "Ximing Lu",
                "Melanie Sclar",
                "Xiang Lorraine Li",
                "Liwei Jian",
                "Bill Yuchen Lin",
                "Peter West",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Jena D Hwang"
            ],
            "title": "Faith and fate: Limits of transformers on compositionality",
            "venue": "arXiv preprint arXiv:2305.18654",
            "year": 2023
        },
        {
            "authors": [
                "Monireh Ebrahimi",
                "Aaron Eberhart",
                "Federico Bianchi",
                "Pascal Hitzler."
            ],
            "title": "Towards bridging the neurosymbolic gap: Deep deductive reasoners",
            "venue": "Applied Intelligence, 51:6326\u20136348. (Cited on pg. 8)",
            "year": 2021
        },
        {
            "authors": [
                "Herbert B Enderton."
            ],
            "title": "A mathematical introduction to logic",
            "venue": "Elsevier. (Cited on pg. 2)",
            "year": 2001
        },
        {
            "authors": [
                "Luyu Gao",
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Pengfei Liu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig."
            ],
            "title": "PAL: Program-aided language models",
            "venue": "International Conference on Machine Learning, pages 10764\u201310799. PMLR. (Cited on pg.",
            "year": 2023
        },
        {
            "authors": [
                "Herbert P Grice."
            ],
            "title": "Logic and conversation",
            "venue": "Speech acts, pages 41\u201358. Brill. (Cited on pg. 15)",
            "year": 1975
        },
        {
            "authors": [
                "Christopher Hahn",
                "Frederik Schmitt",
                "Julia J Tillman",
                "Niklas Metzger",
                "Julian Siber",
                "Bernd Finkbeiner."
            ],
            "title": "Formal specifications from natural language",
            "venue": "arXiv preprint arXiv:2206.01962. (Cited on pg. 9)",
            "year": 2022
        },
        {
            "authors": [
                "Simeng Han",
                "Hailey Schoelkopf",
                "Yilun Zhao",
                "Zhenting Qi",
                "Martin Riddell",
                "Luke Benson",
                "Lucy Sun",
                "Ekaterina Zubova",
                "Yujie Qiao",
                "Matthew Burtell"
            ],
            "title": "2022. Folio: Natural language reasoning with firstorder logic. arXiv preprint arXiv:2209.00840",
            "year": 2022
        },
        {
            "authors": [
                "James Higginbotham."
            ],
            "title": "On higher-order logic and natural language",
            "venue": "Proceedings of the British Academy, volume 95, pages 1\u201327. (Cited on pg. 10)",
            "year": 1998
        },
        {
            "authors": [
                "Jie Huang",
                "Kevin Chen-Chuan Chang."
            ],
            "title": "Towards reasoning in large language models: A survey",
            "venue": "arXiv preprint arXiv:2212.10403. (Cited on pg. 1)",
            "year": 2022
        },
        {
            "authors": [
                "Jie Huang",
                "Xinyun Chen",
                "Swaroop Mishra",
                "Huaixiu Steven Zheng",
                "Adams Wei Yu",
                "Xinying Song",
                "Denny Zhou."
            ],
            "title": "Large language models cannot self-correct reasoning yet",
            "venue": "arXiv preprint arXiv:2310.01798. (Cited on pg. 10)",
            "year": 2023
        },
        {
            "authors": [
                "Borja Ibarz",
                "Vitaly Kurin",
                "George Papamakarios",
                "Kyriacos Nikiforou",
                "Mehdi Bennani",
                "R\u00f3bert Csord\u00e1s",
                "Andrew Joseph Dudzik",
                "Matko Bo\u0161njak",
                "Alex Vitvitskyi",
                "Yulia Rubanova"
            ],
            "title": "A generalist neural algorithmic learner",
            "venue": "In Learning on Graphs Confer-",
            "year": 2022
        },
        {
            "authors": [
                "Aishwarya Kamath",
                "Rajarshi Das."
            ],
            "title": "A survey on semantic parsing",
            "venue": "Automated Knowledge Base Construction (AKBC). (Cited on pg. 8)",
            "year": 2019
        },
        {
            "authors": [
                "Mehran Kazemi",
                "Najoung Kim",
                "Deepti Bhatia",
                "Xin Xu",
                "Deepak Ramachandran."
            ],
            "title": "LAMBADA: Backward chaining for automated reasoning in natural language",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Denis Kocetkov",
                "Raymond Li",
                "Loubna Ben Allal",
                "Jia Li",
                "Chenghao Mou",
                "Carlos Mu\u00f1oz Ferrandis",
                "Yacine Jernite",
                "Margaret Mitchell",
                "Sean Hughes",
                "Thomas Wolf"
            ],
            "title": "The Stack: 3 TB of permissively licensed source code. arXiv preprint arXiv:2211.15533",
            "year": 2022
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in neural information processing systems, volume 35, pages 22199\u201322213. (Cited on pg. 1, 4, 8)",
            "year": 2022
        },
        {
            "authors": [
                "Raymond Li",
                "Loubna Ben Allal",
                "Yangtian Zi",
                "Niklas Muennighoff",
                "Denis Kocetkov",
                "Chenghao Mou",
                "Marc Marone",
                "Christopher Akiki",
                "Jia Li",
                "Jenny Chim"
            ],
            "title": "Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161",
            "venue": "(Cited on pg",
            "year": 2023
        },
        {
            "authors": [
                "Percy Liang",
                "Rishi Bommasani",
                "Tony Lee",
                "Dimitris Tsipras",
                "Dilara Soylu",
                "Michihiro Yasunaga",
                "Yian Zhang",
                "Deepak Narayanan",
                "Yuhuai Wu",
                "Ananya Kumar"
            ],
            "title": "Holistic evaluation of language models. arXiv preprint arXiv:2211.09110",
            "year": 2022
        },
        {
            "authors": [
                "Percy Liang",
                "Michael I Jordan",
                "Dan Klein."
            ],
            "title": "Learning dependency-based compositional semantics",
            "venue": "Computational Linguistics, 39(2):389\u2013446. (Cited on pg. 8)",
            "year": 2013
        },
        {
            "authors": [
                "Ruibo Liu",
                "Jason Wei",
                "Shixiang Shane Gu",
                "Te-Yen Wu",
                "Soroush Vosoughi",
                "Claire Cui",
                "Denny Zhou",
                "Andrew M. Dai."
            ],
            "title": "Mind\u2019s eye: Grounded language model reasoning through simulation",
            "venue": "The Eleventh International Conference on Learning Representa-",
            "year": 2023
        },
        {
            "authors": [
                "Xuantao Lu",
                "Jingping Liu",
                "Zhouhong Gu",
                "Hanwen Tong",
                "Chenhao Xie",
                "Junyang Huang",
                "Yanghua Xiao",
                "Wenguang Wang."
            ],
            "title": "Parsing natural language into propositional and first-order logic with dual reinforcement learning",
            "venue": "Proceedings of the 29th",
            "year": 2022
        },
        {
            "authors": [
                "Aman Madaan",
                "Niket Tandon",
                "Prakhar Gupta",
                "Skyler Hallinan",
                "Luyu Gao",
                "Sarah Wiegreffe",
                "Uri Alon",
                "Nouha Dziri",
                "Shrimai Prabhumoye",
                "Yiming Yang"
            ],
            "title": "Self-refine: Iterative refinement with selffeedback",
            "year": 2023
        },
        {
            "authors": [
                "Robin Manhaeve",
                "Sebastijan Dumancic",
                "Angelika Kimmig",
                "Thomas Demeester",
                "Luc De Raedt."
            ],
            "title": "DeepProbLog: Neural probabilistic logic programming",
            "venue": "Advances in neural information processing systems, volume 31. (Cited on pg. 8)",
            "year": 2018
        },
        {
            "authors": [
                "Giuseppe Marra",
                "Francesco Giannini",
                "Michelangelo Diligenti",
                "Marco Gori."
            ],
            "title": "Integrating learning and reasoning with deep logic models",
            "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 517\u2013532.",
            "year": 2019
        },
        {
            "authors": [
                "W. McCune"
            ],
            "title": "Prover9 and mace4. |http://www.cs.unm.edu/ mccune/prover9/|",
            "venue": "(Cited on pg",
            "year": 2005
        },
        {
            "authors": [
                "Ian McKenzie",
                "Alexander Lyzhov",
                "Alicia Parrish",
                "Ameya Prabhu",
                "Aaron Mueller",
                "Najoung Kim",
                "Sam Bowman",
                "Ethan Perez."
            ],
            "title": "The inverse scaling prize",
            "venue": "(Cited on pg. 1)",
            "year": 2022
        },
        {
            "authors": [
                "Quinn McNemar."
            ],
            "title": "Note on the sampling error of the difference between correlated proportions or percentages",
            "venue": "Psychometrika, 12(2):153\u2013157. (Cited on pg. 5)",
            "year": 1947
        },
        {
            "authors": [
                "Gr\u00e9goire Mialon",
                "Roberto Dess\u00ec",
                "Maria Lomeli",
                "Christoforos Nalmpantis",
                "Ram Pasunuru",
                "Roberta Raileanu",
                "Baptiste Rozi\u00e8re",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Asli Celikyilmaz"
            ],
            "title": "Augmented language models: a survey",
            "venue": "arXiv preprint arXiv:2302.07842",
            "year": 2023
        },
        {
            "authors": [
                "Dale A. Miller",
                "Gopalan Nadathur."
            ],
            "title": "Some uses of higher-order logic in computational linguistics",
            "venue": "Proceedings of the 24th Annual Meeting on Association for Computational Linguistics, page 247\u2013256, New York, USA. Association for Computational Lin-",
            "year": 1986
        },
        {
            "authors": [
                "Maxwell Nye",
                "Anders Johan Andreassen",
                "Guy Gur-Ari",
                "Henryk Michalewski",
                "Jacob Austin",
                "David Bieber",
                "David Dohan",
                "Aitor Lewkowycz",
                "Maarten Bosma",
                "David Luan"
            ],
            "title": "Show your work: Scratchpads for intermediate computation with language",
            "year": 2021
        },
        {
            "authors": [
                "Theo X Olausson",
                "Jeevana Priya Inala",
                "Chenglong Wang",
                "Jianfeng Gao",
                "Armando Solar-Lezama"
            ],
            "title": "Is self-repair a silver bullet for code generation? arXiv preprint arXiv:2306.09896",
            "venue": "(Cited on pg",
            "year": 2023
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "(Cited on pg. 1, 4, 15)",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Liangming Pan",
                "Alon Albalak",
                "Xinyi Wang",
                "William Yang Wang."
            ],
            "title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "venue": "arXiv preprint arXiv:2305.12295. (Cited on pg. 9)",
            "year": 2023
        },
        {
            "authors": [
                "Guilherme Penedo",
                "Quentin Malartic",
                "Daniel Hesslow",
                "Ruxandra Cojocaru",
                "Alessandro Cappelli",
                "Hamza Alobeidli",
                "Baptiste Pannier",
                "Ebtesam Almazrouei",
                "Julien Launay"
            ],
            "title": "The RefinedWeb dataset for Falcon LLM: outperforming curated corpora",
            "year": 2023
        },
        {
            "authors": [
                "Baolin Peng",
                "Michel Galley",
                "Pengcheng He",
                "Hao Cheng",
                "Yujia Xie",
                "Yu Hu",
                "Qiuyuan Huang",
                "Lars Liden",
                "Zhou Yu",
                "Weizhu Chen",
                "Jianfeng Gao"
            ],
            "title": "Check your facts and try again: Improving large language models with external knowledge and automated feed",
            "year": 2023
        },
        {
            "authors": [
                "Gabriel Poesia",
                "Alex Polozov",
                "Vu Le",
                "Ashish Tiwari",
                "Gustavo Soares",
                "Christopher Meek",
                "Sumit Gulwani."
            ],
            "title": "Synchromesh: Reliable code generation from pre-trained language models",
            "venue": "International Conference on Learning Representations. (Cited on",
            "year": 2022
        },
        {
            "authors": [
                "Graham Priest."
            ],
            "title": "An introduction to non-classical logic: From if to is",
            "venue": "Cambridge University Press. (Cited on pg. 10)",
            "year": 2008
        },
        {
            "authors": [
                "Abulhair Saparov",
                "Richard Yuanzhe Pang",
                "Vishakh Padmakumar",
                "Nitish Joshi",
                "Seyed Mehran Kazemi",
                "Najoung Kim",
                "He He."
            ],
            "title": "Testing the general deductive reasoning capacity of large language models using ood examples",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom."
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "venue": "arXiv preprint arXiv:2302.04761. (Cited on pg. 2, 9)",
            "year": 2023
        },
        {
            "authors": [
                "Noam Shazeer."
            ],
            "title": "Fast transformer decoding: One write-head is all you need",
            "venue": "arXiv preprint arXiv:1911.02150. (Cited on pg. 15)",
            "year": 2019
        },
        {
            "authors": [
                "Richard Shin",
                "Benjamin Van Durme."
            ],
            "title": "Fewshot semantic parsing with language models trained on code",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        },
        {
            "authors": [
                "Aarohi Srivastava",
                "Abhinav Rastogi",
                "Abhishek Rao",
                "Abu Awal Md Shoeb",
                "Abubakar Abid",
                "Adam Fisch",
                "Adam R Brown",
                "Adam Santoro",
                "Aditya Gupta",
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabili",
            "year": 2023
        },
        {
            "authors": [
                "Robert Stalnaker."
            ],
            "title": "Common ground",
            "venue": "Linguistics and philosophy, 25(5/6):701\u2013721. (Cited on pg. 15)",
            "year": 2002
        },
        {
            "authors": [
                "Oyvind Tafjord",
                "Bhavana Dalvi",
                "Peter Clark."
            ],
            "title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3621\u20133634, Online. Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Oyvind Tafjord",
                "Bhavana Dalvi Mishra",
                "Peter Clark."
            ],
            "title": "Entailer: Answering questions with faithful and truthful chains of reasoning",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2078\u20132093, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Romal Thoppilan",
                "Daniel De Freitas",
                "Jamie Hall",
                "Noam Shazeer",
                "Apoorv Kulshreshtha",
                "Heng-Tze Cheng",
                "Alicia Jin",
                "Taylor Bos",
                "Leslie Baker",
                "Yu Du"
            ],
            "title": "LaMDA: Language models for dialog applications",
            "year": 2022
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Adri\u00e0 Puigdom\u00e8nech Badia",
                "David Budden",
                "Razvan Pascanu",
                "Andrea Banino",
                "Misha Dashevskiy",
                "Raia Hadsell",
                "Charles Blundell."
            ],
            "title": "The clrs algorithmic reasoning benchmark",
            "venue": "International Conference on Machine Learning, pages",
            "year": 2022
        },
        {
            "authors": [
                "Bailin Wang",
                "Zi Wang",
                "Xuezhi Wang",
                "Yuan Cao",
                "Rif A Saurous",
                "Yoon Kim."
            ],
            "title": "Grammar prompting for domain-specific language generation with large language models",
            "venue": "arXiv preprint arXiv:2305.19234. (Cited on pg. 8)",
            "year": 2023
        },
        {
            "authors": [
                "Qingxiang Wang",
                "Chad Brown",
                "Cezary Kaliszyk",
                "Josef Urban."
            ],
            "title": "Exploration of neural machine translation in autoformalization of mathematics in mizar",
            "venue": "Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and",
            "year": 2020
        },
        {
            "authors": [
                "Qingxiang Wang",
                "Cezary Kaliszyk",
                "Josef Urban."
            ],
            "title": "First experiments with neural translation of informal to formal mathematics",
            "venue": "Intelligent Computer Mathematics: 11th International Conference, CICM 2018, Hagenberg, Austria, August 13-17, 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V Le",
                "Ed H Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "The Eleventh International Conference on Learning",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Nathaniel Weir",
                "Benjamin Van Durme."
            ],
            "title": "Dynamic generation of interpretable inference rules in a neuro-symbolic expert system",
            "venue": "arXiv preprint arXiv:2209.07662. (Cited on pg. 9)",
            "year": 2022
        },
        {
            "authors": [
                "Lionel Wong",
                "Gabriel Grand",
                "Alexander K Lew",
                "Noah D Goodman",
                "Vikash K Mansinghka",
                "Jacob Andreas",
                "Joshua B Tenenbaum."
            ],
            "title": "From word models to world models: Translating from natural language to the probabilistic language of thought",
            "venue": "arXiv",
            "year": 2023
        },
        {
            "authors": [
                "Yuhuai Wu",
                "Albert Qiaochu Jiang",
                "Wenda Li",
                "Markus Rabe",
                "Charles Staats",
                "Mateja Jamnik",
                "Christian Szegedy."
            ],
            "title": "Autoformalization with large language models",
            "venue": "Advances in Neural Information Processing Systems, volume 35, pages 32353\u201332368.",
            "year": 2022
        },
        {
            "authors": [
                "Shunyu Yao",
                "Dian Yu",
                "Jeffrey Zhao",
                "Izhak Shafran",
                "Thomas L Griffiths",
                "Yuan Cao",
                "Karthik Narasimhan."
            ],
            "title": "Tree of thoughts: Deliberate problem solving with large language models",
            "venue": "arXiv preprint arXiv:2305.10601. (Cited on pg. 10)",
            "year": 2023
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik Narasimhan",
                "Yuan Cao."
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "venue": "arXiv preprint arXiv:2210.03629. (Cited on pg. 9)",
            "year": 2022
        },
        {
            "authors": [
                "Michihiro Yasunaga",
                "Xinyun Chen",
                "Yujia Li",
                "Panupong Pasupat",
                "Jure Leskovec",
                "Percy Liang",
                "Ed H Chi",
                "Denny Zhou."
            ],
            "title": "Large language models as analogical reasoners",
            "venue": "arXiv preprint arXiv:2310.01714. (Cited on pg. 10)",
            "year": 2023
        },
        {
            "authors": [
                "Xi Ye",
                "Qiaochu Chen",
                "Isil Dillig",
                "Greg Durrett."
            ],
            "title": "Satisfiability-aided language models using declarative prompting",
            "venue": "arXiv preprint arXiv:2305.09656. (Cited on pg. 9)",
            "year": 2023
        },
        {
            "authors": [
                "Eric Zelikman",
                "Yuhuai Wu",
                "Jesse Mu",
                "Noah Goodman."
            ],
            "title": "STaR: Bootstrapping reasoning with reasoning",
            "venue": "Advances in Neural Information Processing Systems, volume 35, pages 15476\u201315488. (Cited on pg. 8)",
            "year": 2022
        },
        {
            "authors": [
                "John M Zelle",
                "Raymond J Mooney."
            ],
            "title": "Learning to parse database queries using inductive logic programming",
            "venue": "Proceedings of the National Conference on Artificial Intelligence, pages 1050\u20131055. (Cited on pg. 8)",
            "year": 1996
        },
        {
            "authors": [
                "Luke S Zettlemoyer",
                "Michael Collins."
            ],
            "title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
            "venue": "Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence, UAI\u201905, page",
            "year": 2005
        },
        {
            "authors": [
                "Hanlin Zhang",
                "Ziyang Li",
                "Jiani Huang",
                "Mayur Naik",
                "Eric Xing."
            ],
            "title": "Improved logical reasoning of language models via differentiable symbolic programming",
            "venue": "First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Honghua Zhang",
                "Meihua Dang",
                "Nanyun Peng",
                "Guy Van den Broeck."
            ],
            "title": "Tractable control for autoregressive language generation",
            "venue": "International Conference on Machine Learning, pages 40932\u201340945. PMLR. (Cited on pg. 8)",
            "year": 2023
        },
        {
            "authors": [
                "Kechi Zhang",
                "Zhuo Li",
                "Jia Li",
                "Ge Li",
                "Zhi Jin."
            ],
            "title": "Self-edit: Fault-aware code editor for code generation",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 769\u2013787, Toronto,",
            "year": 2023
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Claire Cui",
                "Olivier Bousquet",
                "Quoc V Le",
                "Ed H Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "The",
            "year": 2023
        },
        {
            "authors": [
                "cetkov"
            ],
            "title": "2022), and (3) a Wikipedia dataset. Its base model, StarCoderBase, is an open-access model with a GPT-2 architecture using multi-query attention (Shazeer, 2019) and fill-in-the-middle objective (Bavarian et al., 2022)",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gains on FOLIO and a balanced subset of ProofWriter for three different models in nearly all experimental conditions we evaluate. On ProofWriter, augmenting the comparatively small open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5 and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%, respectively. When used with GPT-4, LINC scores 26% higher than CoT on ProofWriter while performing comparatively on FOLIO. Further analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes. We thus provide promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers. All corresponding code is publicly available.1\n\u2217 Author order randomized; all reserve the right to list their name first.\n1 https://github.com/benlipkin/linc"
        },
        {
            "heading": "1 Introduction",
            "text": "Widespread adoption of large language models (LLMs) such as GPT-3 (Brown et al., 2020), GPT4 (OpenAI, 2023), and PaLM (Chowdhery et al., 2022) have led to a series of remarkable successes in tasks ranging from text summarization to program synthesis. Some of these successes have encouraged the hypothesis that such models are able to flexibly and systematically reason (Huang and Chang, 2022), especially when using prompting strategies that explicitly encourage verbalizing intermediate reasoning steps before generating the final answer (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022; Wang et al., 2023b). However, this reasoning ability appears to be unreliable for tasks that require reasoning out of domain (Liang et al., 2022; Saparov et al., 2023), understanding negation (Anil et al., 2022), and following long reasoning chains (Dziri et al., 2023). Furthermore, while the standard approach of \u201cscaling up\u201d seems to improve performance across some reasoning domains, other domains, e.g., reasoning involving use of Modus Tollens, show no such improvements (McKenzie et al., 2022). These findings suggest that such models may be relying on approximate heuristics based on surface-level statistical patterns in reasoning tasks, rather than consistent, generalizable representations and strategies (Srivastava et al., 2023; Creswell et al., 2023).\nAt the same time, the ability to accurately and soundly perform logical reasoning is important for AI and NLP due to its impact on downstream tasks. For example: retrieval-augmented chatbots may become more truthful if it can be verified that their answers logically follow from the retrieved facts; data-driven models capable of logical reasoning may speed up progress across mathematics and the sciences through automated theorem proving and knowledge discovery; and AI tutoring systems which ensure internal logical consistency might\nmake for better educational platforms, teaching students to think more clearly and rigorously. The question of how to enable state-of-the-art LLMs to become more reliable logical reasoners is thus one of great importance, with far-reaching implications.\nIn this work, we analyze LINC: Logical Inference via Neurosymbolic Computation (Fig. 1). In LINC, logical reasoning is tackled through a modular, two-step neurosymbolic process. First, the language model converts the natural language premises and desired conclusion into first-order logic (FOL) expressions (Enderton, 2001; BarkerPlummer et al., 2011). Second, a symbolic FOL theorem prover algorithmically determines the truth value of the conclusion given the formalized premises. In practice, we also incorporate a third majority voting step, which is shown to improve performance. LINC is a natural extension of recent work augmenting lanugage models with symbolic tools such as calculators or interpreters (Schick et al., 2023).\nLINC has a key advantage: the language model itself no longer needs to perform any deductive reasoning, which is offloaded to the theorem prover. However, there are also clear drawbacks: the formalization from natural language to first-order logic must perfectly capture all relevant information contained in the premises, and any loss of information in the formalization procedure may lead\nthe solver astray, leading to an incorrect conclusion. As it is not clear whether the task of formalization is more or less difficult than that of end-to-end natural language reasoning, our core interest in this work is to compare and contrast our neurosymbolic approach to existing reasoning strategies like Chain-of-Thought. Our contributions are thus three-fold:\n\u2022 First, we propose LINC, a two-stage neurosymbolic approach for logical reasoning tasks (Sec. 2).\n\u2022 Second, we compare LINC to three baseline LLM strategies (Fig. 2), across three models (StarCoder+, GPT-3.5, GPT-4) and two datasets (FOLIO and ProofWriter) (Sec. 4). We find that LINC significantly improves performance over every baseline in all experimental conditions except for GPT-4 on FOLIO.\n\u2022 Third, we provide a thorough error analysis of both LINC and Chain-of-Thought, identifying three high-level failure modes of each. We discover that these failure modes are distinct, highlighting the potential for a synergy of the two methods (Sec. 5).\nOverall, we present strong evidence for the potential of future neurosymbolic logical reasoning systems based on integrating language models and theorem provers.\n2 LINC: Logical Inference via Neurosymbolic Computation\nOur neurosymbolic approach to end-to-end logical reasoning consists of two stages. In the first stage, the LLM acts as a semantic parser, translating NL statements into FOL expressions in our supported logic language. In the second stage, these expressions are parsed from the text generated by the LLM and then get passed to an automated theorem prover; we use Prover9, a high-performance prover widely used in the logic community (McCune, 2005\u20132010). The external solver then executes a symbolic deduction algorithm, which either returns a value from the set {True, False, Uncertain} or raises an exception due to improper FOL syntax (e.g., if the model fails to balance parantheses in the formulae).\nAt its core, the strength of this approach lies in the reformulation of the problem space. End-toend NL-based reasoning allows for operation over a highly flexible expression space, but leaves the LLM with the difficult task of performing explicit deductive inference over expressions in this space. Using LINC, we instead trade off the flexible expression space of NL for syntactically strict logic formulas, allowing us to leverage symbolic algorithms with provable guarantees that the deductive chains will be correct with respect to the semantics of the intermediate representation. Making effective use of this reformulation thus requires the logic expressions generated by the LLM to be 1) syntactically valid, such that they are accepted by the prover, and 2) semantically valid, such that their evaluation results in the correct conclusion. In our experiments, we mitigate these risks by using a Kway majority voting procedure, which is discussed further in Sec. 3.\nThe significance of this problem space reformulation can\u2014beyond the numerical increases in performance observed across our experiments\u2014 perhaps best be seen through an in-depth comparison of how LINC and traditional end-to-end LLM reasoning approaches such as Chain-of-Thought (CoT) fail. To foreshadow our latter analysis, we find that compared to CoT, LINC has worse recall but better precision on True/False predictions. We discuss this further in Sec. 5 and highlight that this suggests that LINC, as well as neurosymbolic computation more generally, has the potential to reduce LLM overconfidence and hallucination."
        },
        {
            "heading": "3 Experiments",
            "text": "In this section, we present our experimental setup, the models we use, and the three baselines to which we compare LINC.\nDatasets: Our experiments use tasks from two existing datasets: FOLIO (Han et al., 2022) and ProofWriter (Tafjord et al., 2021), both of which have been shown to be challenging for off-the-shelf LLMs (Han et al., 2022; Creswell et al., 2023). FOLIO is an expert-written, open-domain, logically complex and diverse dataset for natural language reasoning with first-order logic. We use its validation set for our evaluation. However, of the 204 samples in the validation set, we discover that 22 have errors (details in Appendix C), leaving us with 182 examples for our evaluation. ProofWriter, meanwhile, is a synthetically generated dataset for logical reasoning over natural language. For our evaluation, we use the OWA (Open-World Assumption) portion of ProofWriter, since this setting best matches that of FOLIO. Since we are running a large number of experiments, we randomly select 360 data points to evaluate on in order to reduce costs. We sample these in such a way that the resulting data set is balanced across both the number of reasoning steps in the shortest ground truth proof (depth 0-5; 50 samples each) and across the three labels (True/False/Uncertain; 120 samples each; 20 each per depth).\nIn-context learning examples: We hand-pick eight diverse samples from the FOLIO training set to be used as few-shot in-context examples. Because ProofWriter does not come with ground truth FOL statements, we use these eight samples for both evaluations. Compared to FOLIO, questions in ProofWriter generally have more premises per question (in our validation sets: an average of 5.3 in FOLIO vs. 18.8 in ProofWriter). Thus, our evaluation on FOLIO is an in-distribution task, whereas ProofWriter requires generalizing out-ofdistribution to reasoning over considerably larger sets of premises than are given in the prompt.\nMajority voting: K-way majority voting, in which K samples are taken i.i.d. from the model and the mode is used as the final prediction, has previously been shown to improve the performance of prompting-based strategies in logical reasoning tasks (Wang et al., 2023b). We implement such a strategy in our work, with reported accuracies reflecting K=10-way majority voting, unless otherwise stated. In the case of ties between two la-\nbels, we arbitrarily select the first of the two to have been generated. We report the effect of K on performance across our conditions and briefly discuss trends in Appendix H.\nModels: We use three models pre-trained on both natural language and code: GPT-3.5 (Ouyang et al., 2022), GPT-42 (OpenAI, 2023), and StarCoder+3 (Li et al., 2023) with a decoding temperature of T = 0.8 for all experiments. We defer model, hyperparameter, and hardware details to Appendix B. We opt for StarCoder+ for three reasons: firstly, unlike the other models we consider, it is a free, open-access model. Secondly, it has a dataset search functionality4, with which we verify that FOLIO and ProofWriter are not in StarCoder+\u2019s training set, giving further assurance in the validity of our findings. Thirdly, with its 15.5B parameters it is likely considerably smaller than GPT-3.5 and GPT-45, allowing us to compare performance at different model scales.\nControlled baselines: We compare LINC to three baselines, which we call Na\u00efve, Scratchpad, and Chain-of-Thought (CoT), as illustrated in Fig. 2. In the Na\u00efve baseline, the model is given the natural language premises and is asked to directly generate the label (True/False/Uncertain).\n2We use gpt-3.5-turbo-16k-0613 and gpt-4-0613. 3https://huggingface.co/bigcode/starcoderplus 4See https://huggingface.co/spaces/bigcode/ in-the-stack and https://huggingface.co/spaces/ bigcode/search.\n5Although the exact size of these models has not been made public, their common predecessor GPT-3 was known to have 175B parameters; see Brown et al. (2020).\nIn the Scratchpad baseline (Nye et al., 2021), the model is asked to first generate FOL expressions corresponding to the premises, and then generate the label. This baseline is thus an ablation of LINC, where we use the LLM instead of Prover9 as the logic solver. Finally, in the CoT baseline, we use the standard technique of CoT prompting (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2023b), where the model is asked to generate step-by-step natural language reasoning to arrive at the conclusion. The prompts we use for all approaches can be found in Appendix D."
        },
        {
            "heading": "4 Results & Discussion",
            "text": "Our main results are shown in Figure 3. Each bar represents either LINC or one of the three baselines, while each group of bars indicates the language model used (in {StarCoder+, GPT-3.5, GPT-4}).\nWe note first that in the FOLIO domain (Figure 3a), StarCoder+\u2014the smallest model we experiment with\u2014benefits the most from LINC, achieving a mean accuracy that is 14.2 points higher than the closest controlled baseline (56.0% vs. 41.8% with CoT). We find that Scratchpad, where the intermediate logical formulae are still generated but the call to the symbolic solver is ablated and replaced by the model\u2019s own prediction, does not appear to benefit performance; for StarCoder+, neither Scratchpad nor the Na\u00efve baseline perform better than simply deterministically predicting the most common label (\u201cUncertain\u201d). For GPT-3.5, the trend is similar, although the gap between LINC and the closest baseline shrinks (62.6% vs. 54.9%\naverage accuracies). For GPT-4, the trend reverses: LINC underperforms CoT. However, we perform a McNemar\u2019s test (McNemar, 1947) to get the pvalue on this GPT-4 LINC vs. CoT comparison, and we find that the difference is not significant (p = 0.58). Meanwhile, for our balanced subset of ProofWriter, we see significant performance gains across the board (Figure 3b); particularly so for GPT-3.5 and GPT-4, which achieve mean accuracies of 96.4% and 98.3% when paired with LINC.\nIn light of the high accuracies obtained with LINC on ProofWriter, we offer two plausible reasons why LINC is particularly favorable on this dataset. Firstly, ProofWriter is\u2014unlike FOLIO\u2014 completely synthetically generated, with relatively short sentences, perhaps lending itself particularly well to being formalized in FOL. However, it is noteworthy that the Scratchpad mode does not seem to improve performance over the Na\u00efve baseline, indicating that even if the NL-to-FOL task were particularly easy in this domain, this is not something that the model is itself capable of leveraging to improve its predictions. The second reason might be that the baseline strategies struggle in this out-of-distribution setting, in which the model must generalize to a larger set of premises (with potentially longer deductive chains) than those found in the prompt. This distribution shift makes it harder for the model to ignore irrelevant premises in the question and carry out all deductive chains correctly. Meanwhile, with LINC, the symbolic solver robustly handles irrelevant premises and long deductive chains, since the LLM only needs to translate each sentence into FOL.\nTo test this last explanation further, we plot each model\u2019s performance across ProofWriter as a func-\ntion of the necessary proof depth in Figure 4. We note first that StarCoder+\u2019s performance remains flat and close to chance with all three baseline methods (Figure 4a). Meanwhile, with LINC the performance remains far above chance, although it drops somewhat as necessary proof depth increases; this performance drop suggests that StarCoder+ struggles somewhat with the NL-to-FOL translation task as the problem at hand gets larger. For GPT-3.5, all baselines perform above chance at proof depth 0 (i.e., where the conclusion can immediately be reached from the premises), but then quickly drop back down (Figure 4b). While Chain-of-Thought prompting allows the model to complete some depth-1 tasks, even this strategy then performs equivalently to chance (within 1 standard deviation) for higher depths. When augmented with LINC, however, GPT-3.5 is able to achieve near-perfect performance across all proof depths, providing evidence for the scalability of this approach to longer deductive chains. Finally, for GPT-4 we observe much stronger performance from the baselines; in particular, CoT performs above chance for all proof depths, and all baselines perform well for shallow proofs (Figure 4c). However, even with GPT-4 the performance drops as the necessary proof depth increases with every configuration except for LINC, which performs near or at ceiling through the maximum proof depth available in the dataset."
        },
        {
            "heading": "5 Error Analysis",
            "text": "Having established that LINC can improve performance in many settings, we now move on to our final research question: How do the failure modes of LINC compare to those of in-context reasoning\nmethods? We focus on comparing GPT-4+CoT vs. GPT-4+LINC on FOLIO, since their overall performance is very similar (75.3% vs. 72.5% average accuracy). We leave an analysis of StarCoder+\u2019s predictions on FOLIO to Appendix G."
        },
        {
            "heading": "5.1 Qualitative Analysis",
            "text": "Qualitatively, we find that LINC and CoT have completely different failure modes. We give a highlevel overview and abbreviated examples of each failure mode here, leaving full detailed examples to Appendix E.\nFirst, we detail the failure modes for LINC: L1: FOL fails to capture implicit information not mentioned in the premises. Often, there is obvious information not explicitly listed in the premises that is necessary to explicitly encode in FOL in order to successfully make a desired deduction. For example, in the snippet below one must encode in FOL the implicit assumption that Harry is a person (Person(Harry)).\nPremise 1: When a person reads a book, that person gains knowledge.\nFOL: all x. all y. (Person(x) & Reads(x, y) & Book(y) -> Gains(x, Knowledge))\nPremise 2: Harry read the book \"Walden\" by Henry Thoreau.\nFOL: Reads(Harry, Walden)\nConclusion (Prover9: Uncertain): Harry gains knowledge.\nFOL: Gains(Harry, Knowledge)\nL2: FOL fails to capture information explicitly mentioned in the premises due to the choice of representation. Even when information is explicitly written in the premises, the choice of how the NL is represented in FOL can lead to lost information. In the example below, the fact that Heinrich was a Nazi German politician is captured by\none symbol NaziGermanPolitician, causing the information that he was independently Nazi, German, or a politician to be lost. As a result, LINC predicted Uncertain instead of the ground truth label True.\nPremise: Heinrich Scmidt was a Nazi German politician.\nFOL: NaziGermanPolitician (HeinrichSchmidt)\nConclusion (Prover9: Uncertain): Heinrich Schmidt was German.\nFOL: German(HeinrichSchmidt)\nL3: FOL contains syntax errors. Across all generations, we find that the FOL expressions sometimes contain syntax errors: 38% for StarCoderPlus, 24% for GPT-3.5, and 13% for GPT-4. The most common error is that the same symbol is used with multiple arities. As an example, if Summer(July4) and Loves(Alex, Summer) were both present in a FOL translation, Summer would have a multiple arity violation.\nNext, we detail three failure modes for CoT: C1: CoT concludes something different than it suggests. One common mistake we often see with the CoT approach is that it generates a chain of reasoning indicating uncertainty, but then asserts that the conclusion is false. An example of this is shown below, where GPT-4 suggests there is \u201cno evidence\u201d for the conclusion but then deduces it false.\n(GPT-4 Incorrect CoT): Let\u2019s think step by step. We want to evaluate if platypus are reptiles. We know from the premises that platypus are mammals and that they have no teeth. The fact that some mammals have teeth does not contradict the fact that platypus, although mammals, do not have teeth. Moreover, the statement about humans having teeth is unrelated to the classification of platypus. There\u2019s no evidence in the premises that would suggest platypus are reptiles. Therefore, the statement is false.\nTr ue\nFa lse\nUn ce\nrta in\nEr ro\nr\nReference Label\nTrue\nFalse Uncertain\nError\nPr ed\nic te\nd La\nbe l\n39 4 1 0\n0 31 0 0\n23 18 62 0\n1 1 2 0\n0\n20\n40\n60\n80\n100\n(a) Confusion matrix for LINC.\nTr ue\nFa lse\nUn ce\nrta in\nEr ro\nr\nReference Label\nTrue\nFalse Uncertain\nError\nPr ed\nic te\nd La\nbe l\n50 1 7 0\n3 38 9 0\n10 15 49 0\n0 0 0 0\n0\n20\n40\n60\n80\n100\n(b) Confusion matrix for Chain-of-Thought.\nCo rre\nct\nIn co\nrre ct\nLINC Outcome\nCorrect\nIncorrectC oT\nO ut\nco m\ne\n108 29\n24 21\n0\n20\n40\n60\n80\n100\n(c) Comparing the consistency of LINC vs. Chain-of-Thought.\nLI NC Co T\nSc ra\ntch pa d Na ive\nLINC\nCoTScratchpad\nNaive\n1\n0.22 1\n0.21 0.54 1\n0.14 0.52 0.56 1 0.2\n0.4\n0.6\n0.8\n1.0\n(d) Similarity between incorrect predictions of each method, i.e., (A wrong == B wrong) / (A wrong or B wrong).\nFigure 5: Analyzing and comparing the mistakes made by GPT-4 on the FOLIO dataset.\nC2: CoT makes incorrect logical deductions. While there are many instances where CoT makes incorrect logical deductions, one common deduction mistake is the structure \u201cif B is true and A \u2192 B, then A is true\u201d. In the example shown below, we see this incorrect deduction where A is \u201cMike has an F1 visa\u201d and B is \u201cMike needs to apply for CPT or OPT if he wants to work in the US\u201d. We note that making this type of logical mistake is often called the fallacy of the converse, and that humans are not immune to it, either.\nPremise 1: International students in US have either an F1 visa or a J1 visa.\nPremise 2: An international student in US with an F1 visa needs to apply for CPT or OPT if the student wants to work in the US.\nPremise 3: Mike is an international student.\nPremise 4: Mike needs to apply for CPT if he wants to work in the US.\nConclusion (Ground Truth: Uncertain): Mike has an F1 visa.\n(GPT-4 Incorrect CoT): Let\u2019s think step by step. We want to evaluate if Mike has an F1 visa. From the premises, we know that Mike is an international student and needs to apply for CPT if he wants to work in the US. We also know that an international student in the US with an F1 visa needs to apply for CPT or OPT if the student wants to work in the US. So, it is clear that Mike has an F1 visa since he fits the criteria. Therefore, the conclusion is true.\nC3: CoT fails to find complex paths of reasoning. We find that with all three models, CoT fails when the path of reasoning necessary to make the deduction is complex. Sometimes, CoT has difficulty getting started, and other times, it gets stuck in the middle of a reasoning chain."
        },
        {
            "heading": "5.2 Quantitative Analysis",
            "text": "1. Compared to CoT, LINC has worse recall but better precision on True/False predictions. To see this, we plotted the confusion matrices in Figure 5b (CoT) and Figure 5a (LINC). Looking just at the distributions of predicted labels of the two methods, we see that CoT predicts 32% True, 27%\nFalse, and 41% Uncertain, while LINC predicts 24% True, 17% False, and 57% Uncertain (with 2% of predictions throwing an error). Notably, we observe that LINC predicts Uncertain much more frequently than CoT (57% vs. 41%). To understand why, note that the translation from natural language to FOL is a lossy process: recall that in L1 and L2, we saw that the information conveyed through the FOL is sometimes a subset of the information in the original premises. Removing pieces of crucial information that were on the critical path to deducing True/False may then leave an uncertain conclusion. At the same time, while the FOL translations sometimes do not retain all of the information in the NL, they rarely contain false information that was not provided in the original premises. Therefore, LINC\u2019s precision when predicting True or False is very high (93%) compared to that of CoT (81%), but this comes at the cost of lower recall on True/False predictions (60% for LINC vs. 75% for CoT).\n2. LINC and CoT mispredict on different examples. Earlier in Sec. 5.1, we saw that LINC and CoT exhibit different failure modes, which suggests they should fail on different examples. Indeed, we find that this is the case in our experiments on FOLIO: Figure 5c shows a 2 \u00d7 2 confusion matrix which compares whether or not each method\u2019s prediction was correct. We observe that out of the 24 + 29 + 21 = 74 samples where at least one method makes an incorrect prediction, only 21 are shared. On a closer examination of these 21 samples, we find that 16 are ambiguous or incorrect in their specification (details in Appendix E.4), so the two methods only agree on 5 well-formed samples. This suggests that LINC and CoT are complementary methods which fail under distinct circumstances.\n3. Mispredictions of in-context reasoning baselines are more similar to each other than they are with mispredictions of LINC. As an extension of the previous analysis, we next investigate the correlation between the mispredictions of each pair of methods. To do so, we define a similarity score between two methods A and B as follows: Given a dataset D with N rows and ground truth labels {Ai}Ni=1 and {Bi}Ni=1 from two methods A and B, we define\nsimD(A,B) \u225c \u2211N\ni=1 1 [Ai = Bi \u0338= Ri]\u2211N i=1 1 [Ai \u0338= Ri or Bi \u0338= Ri]\nIn words, simD(A,B) measures the number of instances where A and B are wrong in identical ways\nvs. the number of instances where at least one of them is wrong.\nFigure 5d shows the pairwise similarity between our four methods, highlighting that the similarity between LINC\u2019s mispredictions and the other methods\u2019 mispredictions (0.14, 0.21, 0.22) is much lower than the similarity between any pair of the in-context reasoning methods (0.52, 0.54, 0.56). These results suggest that for GPT-4 on FOLIO, LINC is the only method we evaluate which significantly alters the ways in which the model fails to reason."
        },
        {
            "heading": "6 Related Work",
            "text": "Reasoning in LLMs: Our work contributes to the wider literature on eliciting natural language reasoning capabilities in models. Although we have focused here on comparing a neurosymbolic approach to Scratchpad (Nye et al., 2021) and Chain-of-Thought prompting (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2023b), many other similar or related techniques have been developed in recent years; these include least-to-most prompting (Zhou et al., 2023), selection-inference (Creswell et al., 2023), backward chaining (Tafjord et al., 2022; Kazemi et al., 2023), and self-taught reasoning (Zelikman et al., 2022). Some of these techniques have been formalized under the language model cascades framework (Dohan et al., 2022).\nSemantic parsing: The notion of a semantic parser rests on a long tradition of research (Kamath and Das, 2019) whose aim is to map fragments of natural language into useful, symbolic meaning representations (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013; Liang et al., 2013; Wong et al., 2023). Unlike earlier works in this tradition, we use a language model to generate the semantic parse, which is a method under active investigation in recent years (Shin and Van Durme, 2022; Drozdov et al., 2022; Lu et al., 2022; Wang et al., 2023a).\nNeurosymbolic approaches for reasoning: Methods which combine neural networks with symbolic techniques have seen broad uptake in domains adjacent to logical reasoning, such as generating outputs consistent with a pre-existing symbolic knowledge base (Marra et al., 2019; Manhaeve et al., 2018; Zhang et al., 2023a) and performing algorithmic reasoning over symbolically grounded inputs (Ebrahimi et al., 2021; Ibarz et al., 2022;\nVelic\u030ckovic\u0301 et al., 2022). As for logical reasoning with LLMs in particular, there have been a few different proposals for when and how to best combine the LLM with a symbolic component. Zhang et al. (2022) finetune a language model to synthesize potential facts paired with likelihoods and then use a handwritten differentiable symbolic reasoner in order to deduce other facts. Weir and Van Durme (2022) relax the solver by instead training neural \u201centailment\u201d models to decide if and how a given inference rule applies at each stage. Concurrently to this work, Logic-LM (Pan et al., 2023) and SATLM (Ye et al., 2023) propose neurosymbolic approaches which have much in common with LINC. However, other than the models and datasets considered, their contributions have a few key differences to ours. First, we place particular emphasis on establishing an in-depth understanding of the relative benefits and drawbacks of a neurosymbolic approach to reasoning when compared to traditional in-context reasoning strategies like Chain-of-Thought. Second, Logic-LM employs a self-refinement strategy, which has shown promise across code generation and NLP tasks (Zhang et al., 2023b; Chen et al., 2023a; Peng et al., 2023; Madaan et al., 2023; Olausson et al., 2023) but which we do not consider here. Third, SATLM studies arithmetic reasoning in addition to logical reasoning, showcasing the versatility of the neurosymbolic approach. Fourth, and finally, we use an FOL representation that we believe is easier for humans to read and models to learn. We highly encourage interested readers to study these two contemporary works in detail.\nAutoformalization: The idea of automatically translating natural language into structured symbolic representations that programs can reason about has gained popularity in the domain of formal mathematics, leading to autoformalization systems for several theorem provers including Mizar (Wang et al., 2018, 2020), Lean 3 (Azerbayev et al., 2023), and Isabelle (Wu et al., 2022). Outside formal mathematics, autoformalization has also been applied to translating natural language into system specification languages such as temporal logic (Hahn et al., 2022; Cosler et al., 2023; Chen et al., 2023b).\nTool usage: Our work is heavily inspired by recent work on tool usage. The central idea in this line of research is to augment language models with external tools such as calculators, code interpreters and information retrieval systems. We further divide these works into two classes. In the first class,\nthe model does not need to learn how or where to invoke the tool: instead, the tool is predefined and is applied after the generation step finishes. For example, Gao et al. (2023) and Drori et al. (2022) solve mathematical reasoning tasks by generating Python programs and using the Python interpreter as the tool, Liu et al. (2023) approach physical reasoning tasks with a physical simulator as the tool, and Wong et al. (2023) tackle cognitivelyinspired probabilistic reasoning tasks with Church (a probabilistic programming language) as the tool. In the second class, the model must learn to invoke the tool by itself, meaning that the model must generate explicit API calls to the tool which are then executed when those calls are decoded (Schick et al., 2023; Thoppilan et al., 2022; Yao et al., 2022; Cheng et al., 2023). Our work belongs to the former class, with the task at hand being logical reasoning and the tool available for use being a FOL solver (Prover9). We refer the reader to Mialon et al. (2023) for a more thorough survey of recent work in the tool-usage literature."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this work, we present LINC: Logical Inference via Neurosymbolic Computation, a neurosymbolic approach for scalable logical reasoning with large language models. Our experiments show that LINC leads to significant performance gains in nearly every setting we consider, and that it supports generalization to settings where the model has to reason about a much larger set of premises than it is shown in the in-context learning examples. Furthermore, carrying out a quantitative and qualitative analysis of the mistakes made by LINC, we find evidence that it may complement purely in-context reasoning strategies such as Chain-of-Thought prompting, since they differ greatly in the types and frequencies of mistakes made. This work thus supports the efficacy of neurosymbolic approaches to natural language reasoning, setting the stage for continued advances in combining large language models and symbolic reasoning engines; we discuss several promising future directions in Appendix A."
        },
        {
            "heading": "8 Limitations",
            "text": "Narrow scope of logical reasoning task considered: In this work, we focus exclusively on one aspect of logical reasoning: predicting the truth value of a conclusion given a set of natural language premises. Here, we consider a setting where the\npremises and conclusion are expressed in relatively short statements, which makes the formalization task tractable. In particular, ProofWriter\u2019s natural language statements are synthetically generated, so they can be easily and accurately parsed into FOL. FOLIO reflects a more naturalistic dataset, so we see a higher failure rate in LINC\u2019s semantic parsing step. However, the formalization task becomes more difficult if the premises are in longer paragraph form, such as in question answering or contradiction detection from context passages. This is because the same piece of information can be formalized in a variety of ways, and there is a lot of information that must be pragmatically inferred to arrive at the proper conclusion.\nGeneralizability of qualitative evaluation: While we find that LINC and CoT produce complementary mistakes for our natural language reasoning task, it is unclear if this result also holds true in similar scenarios, such as the ones considered in PAL (Gao et al., 2023), Logic-LM, and SATLM. This is due to the difference in intermediate language and overall logical reasoning task. However, we hypothesize that it will and encourage future investigation in this direction.\nMore sophisticated reasoning techniques: Recent work has proposed more sophisticated techniques beyond chain-of-thought, such as tree of thoughts (Yao et al., 2023), program of thoughts, (Chen et al., 2022), or using retrieval in chain-ofthought prompting (Yasunaga et al., 2023). These have potential to improve and eliminate some of the failure modes of the traditional CoT method. In addition, ideas such as self-repair may also serve to improve these failure modes. It remains future work to do a more thorough investigation of the efficacy of these techniques, though there is also preliminary evidence that they still lack reasoning capabilities (Huang et al., 2023).\nScalability: It is unclear how well LINC will perform as the number of premises scales. First, one mistake in formalization can lead to an incorrect deduction, and more premises lead to a higher probability of errors. Second, in the deduction stage, while many fast algorithms (e.g., forwardand backward-chaining) exist for logical deduction, the general problem is still NP-hard. Therefore, the theorem prover may take a long time in practice.\nOther logics beyond first-order logic: In this work, we exclusively focus on first-order logic. However, FOL is not expressive enough to han-\ndle problems requiring higher-order logics (Miller and Nadathur, 1986; Higginbotham, 1998). Also, in many settings it is desirable to work with nonclassical logics (Priest, 2008; Burgess, 2009). Alternative theorem provers would be needed for such problems. A method like LINC can be naturally extended to those settings, but exactly how well it works there requires further investigations.\nComputational costs: Implementing our approach with both GPT models and the StarCoder+ model requires non-trivial resources. The former requires reliance on costly API requests and the latter dedicated GPUs for inference. Especially as we use majority voting, many generations must be made for each query, increasing the computational requirements."
        },
        {
            "heading": "9 Acknowledgements",
            "text": "T.X. Olausson is supported by the Defense Advanced Research Projects Agency (DARPA) under the ASKEM program, award HR00112220042. A. Gu is supported by the National Science Foundation (NSF) Graduate Research Fellowship under Grant No. 2141064. B. Lipkin and C.E. Zhang are supported by MIT Presidential Fellowships. A. Solar-Lezama is supported by the National Science Foundation (NSF) and Intel Corporation through NSF Grant CCF:2217064. J.B. Tenenbaum is supported by AFOSR Grant #FA9550-22-1-0387 and the MIT-IBM Watson AI Lab. R.P. Levy is supported by a grant from the Simons Foundation to the Simons Center for the Social Brain at MIT.\nWe thank our anonymous reviewers for their insightful feedback and recommendations. We thank the members of the Computer Aided Programming, Computational Psycholinguistics, and Computational Cognitive Science groups for constructive commentary at various stages of this project. We thank Yoon Kim for helpful suggestions and comments on pieces of the initial project proposal. In addition, we thank Zhaofeng Wu and Simeng Han for discussions regarding the FOLIO dataset."
        },
        {
            "heading": "A Future Directions",
            "text": "Of the error types we catalog in our analysis of LINC, the key opportunity for improvement is more elegant handling of naturalistic language use. While the errors observed with CoT result from faulty deductive inferences, in the case of LINC, all errors have been localized to the semantic parsing procedure. This process flows primarily unimpeded in the evaluation of the synthetic ProofWriter dataset, yet leaves room for improvement with the naturalistic FOLIO. In follow-up work, we hope to deeply explore naturalistic evaluation settings, as when data get the most messy is also where improvements become the most valuable. Here, we propose three strategies for further improvement on naturalistic settings.\nFirst, in naturalistic communication, \u201cobvious\u201d information is often left out of explicit productions, left to be inferred in the \u201ccommon ground\u201d of the communicative act (Grice, 1975; Stalnaker, 2002). Implicit premise rediscovery through controlled exploration on the logical neighborhood of the existing explicit premises promises to be a powerful strategy for improving performance in underspecified settings.\nSecond, while a number of samples are lost to syntax errors, recent work has proposed restricting the sampling space of an LLM to that which is consistent with term expansions in a context-freegrammar (CFG) (Poesia et al., 2022). Doing so in this setting would eliminate all syntax errors.\nThird, sometimes, the translation process to FOL is lossy, throwing away valuable information present in the original sentence. We propose improving the faithfulness of FOL translations by asking the LLM to translate the FOL back to natural language and comparing with the original. Forward translations that rank highly when back-translated would be those which have effectively captured the intricacies of a particular sentence\u2019s semantics.\nOverall, we believe that shifting to evaluations on more naturalistic datasets, and incorporating strategies such as those presented here, will help pave the path forward for neurosymbolic approaches to formal reasoning."
        },
        {
            "heading": "B Model Details and Parameters",
            "text": "We use a decoding temperature T = 0.8 for all models. For GPT-3.5 and GPT-4, we limit the maximum number of tokens to generate to 1024 for FOLIO and 4096 for ProofWriter (to accommo-\ndate for the previously mentioned larger number of premises involved in a typical question). For the StarCoder+ model, we allow generation up until the 8192 context window length, since this model is run locally. In either case, decoding is halted early whenever the stop token </EVALUATE> is produced. All local experiments were executed on a cluster equipped with NVIDIA A100 GPUs.\nGPT Models: We use the gpt-3.5-turbo-16k0613 and gpt-4-0613 checkpoints of the GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023) models, respectively, invoking both models via the OpenAI API.\nStarCoder+: StarCoder+ (15.5B)6 is a version of StarCoderBase (Li et al., 2023) which has been finetuned on 600B tokens from a combination of (1) Falcon RefinedWeb (Penedo et al., 2023) (filtered version of CommonCrawl), (2) The Stack v1.2 (Kocetkov et al., 2022), and (3) a Wikipedia dataset. Its base model, StarCoderBase, is an open-access model with a GPT-2 architecture using multi-query attention (Shazeer, 2019) and fill-in-the-middle objective (Bavarian et al., 2022). StarCoderBase has a 8192 context window and is trained on 1T code tokens of permissively licensed text from GitHub across 80 programming languages (Li et al., 2023). We use StarCoder+ instead of StarCoderBase because it is finetuned on natural language, which should improve the performance on our task. We run StarCoder+ with bf16 precision to reduce its memory footprint."
        },
        {
            "heading": "C FOLIO Dataset Preprocessing",
            "text": "We use the publicly available FOLIO dataset on https://github.com/Yale-LILY/FOLIO. We choose representative samples from the training split of the dataset to be our few-shot examples and use the validation split of the dataset in our evaluation. The testing split is not publicly available. The original dataset has 204 validation examples. However, we discovered that there are errors in 22 of the samples. We remove these samples for our evaluation and use the remaining 182 examples. The errors as follows:\n\u2022 In 4 samples, one or more of the ground truth FOL expressions have unbalanced parentheses (samples 3, 109, 110, 111).\n\u2022 In 8 samples, the label obtained by executing the ground-truth FOL expressions does not\n6https://huggingface.co/bigcode/starcoderplus\nmatch the provided ground truth label. We double-checked this, first by executing the FOL expressions through Prover9 and second by checking it manually. (samples 6, 28, 30, 48, 113, 115, 139, 140).\n\u2022 In 10 samples, the number of premises does not match the number of FOL expressions (samples 10, 11, 12, 88, 106, 107, 108, 174, 175, 176).\nThe sample numbers above refer to the line index in the validation file located at https://github.com/Yale-LILY/FOLIO/blob/ main/data/v0.0/folio-validation.jsonl."
        },
        {
            "heading": "D FOLIO Few-Shot Prompts",
            "text": "The methodologies we investigate do not require any finetuning on domain-specific data. Instead, we use in-context learning (ICL) with pretrained models. We prompt the model with a set of instructions and 1-8 ICL examples, which adhere to a structured text format designed to scaffold generations and ease postprocessing. In particular, we begin each ICL example with each of the NL premises wrapped in an HTML-style tag <PREMISES>. . . </PREMISES> followed by the NL conclusion wrapped in <CONCLUSION>. . . </CONCLUSION>. The requisite evaluation steps for each evaluation paradigm are then outlined in a subsequent section wrapped <EVALUATE>. . . </EVALUATE>. Following the inclusion of ICL examples, a test example is added, with the <PREMISES> and <CONCLUSION> sections. Then, the <EVALUATE> tag is opened, and the LM is allowed to proceed with causal generation until the </EVALUATE> tag is generated. Upon generation of this stop token, the <EVALUATE> block is segmented for post-processing according to the method being evaluated ({na\u00efve, scratchpad, chain-of-thought, neuro-symbolic}).\nFor the few-shot examples, we use samples from the publicly available FOLIO training set. We select a set of diverse samples that are balanced across labels. Since the FOLIO training set does not come with FOL expressions for the conclusions or chain of thought prompts, we manually add both for each sample. For the k-shot setting (k<8), we use the first k samples from the following list of sample indices: 126, 24, 61, 276, 149, 262, 264, 684. Here, sample i refers to the ith line in https://github.com/Yale-LILY/FOLIO/blob/\nmain/data/v0.0/folio-train.jsonl. We do not optimize for the choice of few-shot examples, and this is the only set of examples we evaluated with, so it is likely that there exist better choices for few-shot examples that would lead to improved performance across the board.\nD.1 FOLIO, 1-shot (baseline)\nThe following is a first-order logic (FOL) \u21aa\u2192 problem. The problem is to determine whether the \u21aa\u2192 conclusion follows from the premises. The premises are given in the form of a set of \u21aa\u2192 first-order logic sentences. The conclusion is given in the form of a single \u21aa\u2192 first-order logic sentence. The task is to evaluate the conclusion as 'True', \u21aa\u2192 'False', or 'Uncertain' given the \u21aa\u2192 premises.\n<PREMISES> All dispensable things are environment-friendly. All woodware is dispensable. All paper is woodware. No good things are bad. All environment-friendly things are good. A worksheet is either paper or is environment-\n\u21aa\u2192 friendly. </PREMISES> <CONCLUSION> A worksheet is not dispensable. </CONCLUSION> <EVALUATE> Uncertain </EVALUATE>\n<PREMISES> ...premises for sample here, one premise per\n\u21aa\u2192 line </PREMISES> <CONCLUSION> ...conclusion for sample here </CONCLUSION> <EVALUATE>\nListing 1: todo\nD.2 FOLIO, 1-shot (scratchpad)\nThe following is a first-order logic (FOL) \u21aa\u2192 problem. The problem is to determine whether the \u21aa\u2192 conclusion follows from the premises. The premises are given in the form of a set of \u21aa\u2192 first-order logic sentences. The conclusion is given in the form of a single \u21aa\u2192 first-order logic sentence. The task is to translate each of the premises \u21aa\u2192 and conclusions into FOL expressions, and \u21aa\u2192 then to evaluate the conclusion as 'True \u21aa\u2192 ', 'False', or 'Uncertain' given the \u21aa\u2192 premises.\n<PREMISES>\nAll dispensable things are environment-friendly. All woodware is dispensable. All paper is woodware. No good things are bad. All environment-friendly things are good. A worksheet is either paper or is environment-\n\u21aa\u2192 friendly. </PREMISES> <CONCLUSION> A worksheet is not dispensable. </CONCLUSION> <EVALUATE> TEXT: All dispensable things are environment-\n\u21aa\u2192 friendly. FOL: all x. (Dispensable(x) ->\n\u21aa\u2192 EnvironmentFriendly(x)) TEXT: All woodware is dispensable. FOL: all x. (Woodware(x) -> Dispensable(x)) TEXT: All paper is woodware. FOL: all x. (Paper(x) -> Woodware(x)) TEXT: No good things are bad. FOL: all x. (Good(x) -> -Bad(x)) TEXT: All environment-friendly things are good. FOL: all x. (EnvironmentFriendly(x) -> Good(x)) TEXT: A worksheet is either paper or is\n\u21aa\u2192 environment-friendly. FOL: ((Paper(Worksheet) & -EnvironmentFriendly(\n\u21aa\u2192 Worksheet)) | (-Paper(Worksheet) & \u21aa\u2192 EnvironmentFriendly(Worksheet)))\nTEXT: A worksheet is not dispensable. FOL: -Dispensable(Worksheet) ANSWER: Uncertain </EVALUATE>\n<PREMISES> ...premises for sample here, one premise per\n\u21aa\u2192 line </PREMISES> <CONCLUSION> ...conclusion for sample here </CONCLUSION> <EVALUATE>\nD.3 FOLIO, 1-shot (chain of thought)\nThe following is a first-order logic (FOL) \u21aa\u2192 problem. The problem is to determine whether the \u21aa\u2192 conclusion follows from the premises. The premises are given in the form of a set of \u21aa\u2192 first-order logic sentences. The conclusion is given in the form of a single \u21aa\u2192 first-order logic sentence. The task is to translate each of the premises \u21aa\u2192 and conclusions into FOL expressions,\n<PREMISES> All dispensable things are environment-friendly. All woodware is dispensable. All paper is woodware. No good things are bad. All environment-friendly things are good. A worksheet is either paper or is environment-\n\u21aa\u2192 friendly. </PREMISES> <CONCLUSION> A worksheet is not dispensable. </CONCLUSION>\n<EVALUATE> Let's think step by step. We want to evaluate if\n\u21aa\u2192 a worksheet is not dispensable. From \u21aa\u2192 premise 6, we know that a worksheet is \u21aa\u2192 either paper or is environment-friendly. \u21aa\u2192 If it is paper, then from premise 3, a \u21aa\u2192 worksheet is woodware, and from premise \u21aa\u2192 2, a worksheet is dispensable. If it is \u21aa\u2192 environment-friendly, we know it is good \u21aa\u2192 from premise 5, but we know nothing about \u21aa\u2192 whether it is dispensable. Therefore, we \u21aa\u2192 don't know if a worksheet is dispensible \u21aa\u2192 or not, so the statement is uncertain.\nANSWER: Uncertain </EVALUATE>\n<PREMISES> ...premises for sample here, one premise per\n\u21aa\u2192 line </PREMISES> <CONCLUSION> ...conclusion for sample here </CONCLUSION> <EVALUATE>\nD.4 FOLIO, 1-shot (neurosymbolic)\nThe following is a first-order logic (FOL) \u21aa\u2192 problem. The problem is to determine whether the \u21aa\u2192 conclusion follows from the premises. The premises are given in the form of a set of \u21aa\u2192 first-order logic sentences. The conclusion is given in the form of a single \u21aa\u2192 first-order logic sentence. The task is to translate each of the premises \u21aa\u2192 and conclusions into FOL expressions, so \u21aa\u2192 that the expressions can be evaluated by \u21aa\u2192 a theorem solver to determine whether the \u21aa\u2192 conclusion follows from the premises. Expressions should be adhere to the format of \u21aa\u2192 the Python NLTK package logic module.\n<PREMISES> All dispensable things are environment-friendly. All woodware is dispensable. All paper is woodware. No good things are bad. All environment-friendly things are good. A worksheet is either paper or is environment-\n\u21aa\u2192 friendly. </PREMISES> <CONCLUSION> A worksheet is not dispensable. </CONCLUSION> <EVALUATE> TEXT: All dispensable things are environment-\n\u21aa\u2192 friendly. FOL: all x. (Dispensable(x) ->\n\u21aa\u2192 EnvironmentFriendly(x)) TEXT: All woodware is dispensable. FOL: all x. (Woodware(x) -> Dispensable(x)) TEXT: All paper is woodware. FOL: all x. (Paper(x) -> Woodware(x)) TEXT: No good things are bad. FOL: all x. (Good(x) -> -Bad(x)) TEXT: All environment-friendly things are good. FOL: all x. (EnvironmentFriendly(x) -> Good(x))\nTEXT: A worksheet is either paper or is \u21aa\u2192 environment-friendly. FOL: ((Paper(Worksheet) & -EnvironmentFriendly( \u21aa\u2192 Worksheet)) | (-Paper(Worksheet) & \u21aa\u2192 EnvironmentFriendly(Worksheet))) TEXT: A worksheet is not dispensable. FOL: -Dispensable(Worksheet) </EVALUATE>\n<PREMISES> ...premises for sample here, one premise per\n\u21aa\u2192 line </PREMISES> <CONCLUSION> ...conclusion for sample here </CONCLUSION> <EVALUATE>"
        },
        {
            "heading": "E FOLIO Error Analysis",
            "text": "E.1 Ambiguity of \u201cEither\u201d statements Depending on the context, the phrase \u201ceither x or y\u201d could mean x XOR y, x OR y, or be ambiguous. Throughout our experiments, we found that models had many creatively incorrect ways of translating these statements. One reoccurring error was that statements that clearly intended x XOR y (such as, \u201can animal is either a rabbit or a squirrel\u201d) were translated into x OR y. We tried to account for this into account by including multiple samples with this construct in the few shot examples (see Sec. D.4). However, the models still handle this construct inconsistently and incorrectly.\nIn addition, we find that throughout the FOLIO dataset, by matching the natural language premises to the FOL premises, we find no consistent or predictable pattern as to how \u201ceither x or y\u201d statements are translated. For example, \u201can animal is either a rabbi or a squirrel\u201d is translated as all x. Rabbit(x) | Squirrel(x), while we believe this instance should clearly be XOR. Therefore, we believe that some of these samples are inherently ambiguous or malformed.\nTo highlight model behavior on these examples, four representative examples from the FOLIO validation set are shown below; examples have multiple translations because we used temperature T = 0.8. Here, Correct/Incorrect indicate whether the translations match the ground truth (which doesn\u2019t necessarily match how we would translate it).\nPremise: an animal is either a rabbit or a \u21aa\u2192 squirrel (Ground Truth) Translation: all x. (Rabbit(x) | \u21aa\u2192 Squirrel(x)) (Correct) Translation 1 (GPT-3.5): all x. ( \u21aa\u2192 Animal(x) -> (Rabbit(x) | Squirrel(x)))\n(Incorrect) Translation 2 (StarCoderPlus): (( \u21aa\u2192 Rabbit(Animal) & -Squirrel(Animal)) | (- \u21aa\u2192 Rabbit(Animal) & Squirrel(Animal))) (Incorrect) Translation 3 (GPT-4): all x. (( \u21aa\u2192 Animal(x) & Rabbit(x)) | (Animal(x) & \u21aa\u2192 Squirrel(x)))\nPremise: a person either studys or teaches (Ground Truth) Translation: all x. (Study(x) |\n\u21aa\u2192 Teaches(x)) (Incorrect) Translation 1 (StarCoderPlus):\n\u21aa\u2192 Studys(Person) | Teaches(Person) (Incorrect) Translation 2 (StarCoderPlus): ((\n\u21aa\u2192 Study(Person) & -Teach(Person)) | (-Study \u21aa\u2192 (Person) & Teach(Person)))\n(Correct) Translation 3 (GPT-4): all x. (Studies \u21aa\u2192 (x) | Teaches(x))\nPremise: A man is either kind or evil. (Ground Truth) Translation: all x. (Kind(x) & -\n\u21aa\u2192 Evil(x)) | (-Kind(x) & Evil(x)) (Incorrect) Translation 1 (GPT-3.5): ((Man(x) & -\n\u21aa\u2192 Kind(x)) | (-Man(x) & Evil(x))) (Incorrect) Translation 2 (StarCoderPlus): Kind(\n\u21aa\u2192 AMan) | Evil(AMan) (Incorrect) Translation 3 (StarCoderPlus): (Kind\n\u21aa\u2192 (x) | Evil(x))\nPremise: Ben is either from The Simpsons or \u21aa\u2192 funny. (Ground Truth) Translation: (Simpsons(Ben) & - \u21aa\u2192 Funny(Ben)) | (-Simpsons(Ben) & Funny(Ben \u21aa\u2192 )) (Correct) Translation 1 (StarCoderPlus): (( \u21aa\u2192 Simpsons(Ben) & -Funny(Ben)) | (-Simpsons \u21aa\u2192 (Ben) & Funny(Ben))) (Incorrect) Translation 2 (GPT-3.5): ( \u21aa\u2192 FromTheSimpsons(Ben) | Funny(Ben)) (Incorrect) Translation 3 (GPT-4): \u21aa\u2192 FromTheSimpsons(Ben) | Funny(Ben)\nE.2 GPT-4 LINC Failure Modes\nL1: FOL fails to capture implicit information not mentioned in the premises. Three examples of errors from the FOLIO validation set are shown below. The first two occurring in both GPT-3.5 and GPT-4, and the latter only occurs in GPT-3.5 and interestingly, is correct in GPT-4. In Example 1, to make the correct conclusion, we must encode in FOL that Harry is a person (Person(Harry)) and that Walden is a book (Book(\"Walden\")). Harry being a person is implicit, but \u201cWalden\u201d being a book is explicitly mentioned in premise 4 but fails to be explicitly encoded by the model. In Example 2, we must encode that KiKi is an animal to make the correct deduction. One can argue that this example is ambiguous, but from the context, most would make this inference. In Example 3, we need a clause that says LGA and LGA are the same airport (SameAirport(LGA, LGA)).\nExample 1 (GPT-4)\nPremises: Books contain tons of knowledge. When a person reads a book, that person gains\n\u21aa\u2192 knowledge. If a person gains knowledge, they become smarter.\n\u21aa\u2192 Harry read the book \"Walden\" by Henry Thoreau.\nConclusion (Ground Truth: True) Harry is smarter than before.\nFOLs: all x. (Book(x) -> Contains(x, Knowledge)) all x. all y. (Person(x) & Reads(x, y) & Book(y)\n\u21aa\u2192 -> Gains(x, Knowledge)) all x. (Gains(x, Knowledge) -> Smarter(x)) Reads(Harry, Walden)\nConclusion (Prover9 Deduction: Uncertain) Smarter(Harry)\nExample 2 (GPT-4) Premises: If an animal barks, then Jane does not like it. All dogs are animals that bark. If an animal jumps, then Jane likes it. If an animal has legs, then it jumps. If an animal is terricolous, then it has legs. If KiKi neither jumps nor has legs, then KiKi is\n\u21aa\u2192 terricolous or has legs.\nConclusion (Ground Truth: True) KiKi neither barks nor is a dog.\nFOLs: all x. (Animal(x) & Barks(x) -> NotLike(Jane, x)\n\u21aa\u2192 ) all x. (Dog(x) -> Animal(x) & Barks(x)) all x. (Animal(x) & Jumps(x) -> Like(Jane, x)) all x. (Animal(x) & HasLegs(x) -> Jumps(x)) all x. (Animal(x) & Terricolous(x) -> HasLegs(x)\n\u21aa\u2192 ) (-Jumps(KiKi) & -HasLegs(KiKi)) -> (Terricolous(\n\u21aa\u2192 KiKi) | HasLegs(KiKi))\nConclusion (Prover9 Deduction: Uncertain) -Barks(KiKi) & -Dog(KiKi)\nExample 3 (GPT-3.5): Premises: Susan flies to LGA airport. The departure and arrival can not be the same\n\u21aa\u2192 airport. John flies from LGA airport.\nConclusion (Ground Truth: False) Susan flies from LGA airport.\nFOLs: Flies(Susan, LGA) all x. all y. (Departure(x) & Arrival(y) & -\n\u21aa\u2192 SameAirport(x, y)) Flies(John, LGA)\nConclusion (Prover9 Deduction: Uncertain): Flies(Susan, LGA)\nL2: FOL fails to capture information explicitly mentioned in the premises due to the\nchoice of representation. We show four illustrative examples: the first two examples occur in both GPT-3.5 and GPT-4, while the last two examples only occur in GPT-3.5 and are correct when using GPT-4. The first example is described in the main text. In example 2, we know that Barutin Cove is a cove, but this lost information is not encoded in the FOL, so we cannot conclude Cove(BarutinCove), although this is crucial information for making the deduction. In Example 3, we must deduce that Tom\u2019s license plate is a license plate (FromIstanbul(TomPlate)), which is obvious but is not encoded by the model. In the final example, the idea that an object has four sides is expressed both as FourSided(x) and FourSides(x), so the model is unable to make the correct deduction.\nExample 1 (GPT-4) Premises: Heinrich Scmidt was a Nazi German politician. Heeinrich Scmidt was also a member of the\n\u21aa\u2192 Prussian State Parliament and the Nazi \u21aa\u2192 Reichstag.\nConclusion (Ground Truth: True): Heinrich Schmidt was either German or Russian.\nFOLs: GermanPolitician(HeinrichSchmidt) & Nazi(\n\u21aa\u2192 HeinrichSchmidt) MemberPrussianStateParliament(HeinrichSchmidt) &\n\u21aa\u2192 MemberNaziReichstag(HeinrichSchmidt)\nConclusion (Prover9 Deduction: Uncertain): German(HeinrichSchmidt) | Russian(\n\u21aa\u2192 HeinrichSchmidt)\nExample 2 (GPT-4) Premises: Barutin Cove is named after the Bulgarian\n\u21aa\u2192 settlement of Barutin. Barutin Cove is on the southwest coast of Snow\n\u21aa\u2192 Island. The South Shetland Islands include Snow Island,\n\u21aa\u2192 Greenwich Island, and Deception Island. The South Shetland Islands are considered part\n\u21aa\u2192 of Antarctica. If A is located in B and B is located in C, then\n\u21aa\u2192 A is located in C.\nConclusion (Ground Truth: True) There is at least one cove in Antarctica named\n\u21aa\u2192 after a place in Bulgaria.\nFOLs: NamedAfter(BarutinCove, Barutin) & In(Barutin,\n\u21aa\u2192 Bulgaria) On(BarutinCove, SnowIsland) In(SnowIsland, SouthShetlandIslands) & In(\n\u21aa\u2192 GreenwichIsland, SouthShetlandIslands) & \u21aa\u2192 In(DeceptionIsland, SouthShetlandIslands)\nIn(SouthShetlandIslands, Antarctica) all x. all y. all z. ((In(x, y) & In(y, z)) ->\n\u21aa\u2192 In(x, z))\nConclusion (Prover9 Deduction: Uncertain) exists x. (Cove(x) & In(x, Antarctica) & exists\n\u21aa\u2192 y. (In(y, Bulgaria) & NamedAfter(x, y)))\nExample 3 (GPT-3.5) Premises: All vehicle registration plates in Istanbul\n\u21aa\u2192 begin with the number 34. Plates that do not begin with the number 34 are\n\u21aa\u2192 not from Istanbul. Joe's vehicle registration plate is from\n\u21aa\u2192 Istanbul. Tom's license plate begins with the number 35. If a license plate begins with the number 35,\n\u21aa\u2192 then it does not begin with the number \u21aa\u2192 34.\nConclusion (Ground Truth: False): Tom's license plate is from Istanbul.\nFOLs: all x. (VehicleRegistrationPlate(x) ->\n\u21aa\u2192 BeginsWith(x, 34)) all x. (-BeginsWith(x, 34) -> -FromIstanbul(x)) FromIstanbul(JoePlate) BeginsWith(TomPlate, 35) all x. (LicensePlate(x) & BeginsWith(x, 35) -> -\n\u21aa\u2192 BeginsWith(x, 34))\nConclusion (Prover9 Deduction: Uncertain): FromIstanbul(TomPlate)\nExample 4 (GPT-3.5) Premises: All squares have four sides. All four-sided things are shapes.\nConclusion (Ground Truth: True): All squares are shapes.\nFOLs: all x. (Square(x) -> FourSides(x)) all x. (FourSided(x) -> Shape(x))\nConclusion (Prover9 Deduction: Uncertain) all x. (Square(x) -> Shape(x))\nL3: FOL contains syntax errors. This is one reason that we observe majority voting helps significantly for neurosymbolic methods, as multiple generations increase the likelihood that at one least one FOL representation successfully compiles. As stated in the main text, the most common error is that the same symbol is used with multiple arities. Two examples of this are shown below: in the first example, the primitive Summer is both used as a function representing that a year has a summer (in the first premise) and as a constant representing that a student loves summer. A similar phenomenon occurs in the second example with the symbol Badults.\nExample 1\nPremises: There are four seasons in a year: Spring, Summer,\n\u21aa\u2192 Fall, and Winter. All students who want to have a long vacation\n\u21aa\u2192 love summer the most.\nFOLs: all x. (Year(x) -> (Spring(x) | Summer(x) | Fall\n\u21aa\u2192 (x) | Winter(x))) all x. (Student(x) & WantsLongVacation(x) ->\n\u21aa\u2192 Loves(x, Summer))\nProver9 Error: The following symbols are used with multiple\n\u21aa\u2192 arities: Summer/1, Summer/0.\nExample 2 Premises: Badults is a British Sitcom series, starring\n\u21aa\u2192 members of Pappy's. Badults piloted in July 2013 on BBC Three.\nFOLs: exists x. (Badults(x) & BritishSitcom(x) &\n\u21aa\u2192 StarringMembersOf(x, Pappys)) Piloted(Badults, July2013, BBCThree)\nProver9 Error: The following symbols are used with multiple\n\u21aa\u2192 arities: Badults/1, Badults/0.\nWhile rarer, there are also syntax errors. For illustration, two examples from GPT-4 are shown below: Premise: Ailton Silva foes not play for a\n\u21aa\u2192 football club. FOL: -Exists x. (FootballClub(x) & PlaysFor(\n\u21aa\u2192 AiltonSilva, x)) Error: Exists should not be capitalized\nPremise: The security deposit can be either \u21aa\u2192 equal to one month's rent or more. FOL: all x. all y. (Rent(x, y) -> (Deposit(x) = \u21aa\u2192 y | Deposit(x) > y)) Error: > is not allowed in Prover9's FOL\nL4: There are mistakes in the FOL translation. For GPT-4, there are very few translation mistakes, but we still find a couple instances where GPT-3.5 fails to capture the semantic meaning of the NL examples properly. Two examples are shown here: Premise: Platypus have no teeth Incorrect FOL: -NoTeeth(Platypus)\nPremise: Imperium doesn't have a feud with a \u21aa\u2192 professional wrestling stable that \u21aa\u2192 includes Ivy Nile. Incorrect FOL: -all x. (Includes(x, IvyNile) -> - \u21aa\u2192 HasFeud(Imperium, x))\nE.3 GPT-4 CoT Failure Modes"
        },
        {
            "heading": "C1: CoT concludes something different than it",
            "text": "suggests. We show two examples of this below\n(both of which LINC solves correctly).\nExample 1 Premises: Plungers suck. Vacuums suck. Vampires suck. Space is a vacuum. A duster is a household appliance that doesn't\n\u21aa\u2192 suck.\nConclusion: Space is a vampire.\n(GPT-4 Incorrect CoT): Let's think step by step. \u21aa\u2192 We want to evaluate if Space is a \u21aa\u2192 vampire. We know that Space is a vacuum \u21aa\u2192 and that vacuums suck. However, while it' \u21aa\u2192 s true that vampires also suck, this \u21aa\u2192 doesn't mean that everything that sucks \u21aa\u2192 is a vampire. Therefore, there is no \u21aa\u2192 reason to believe that Space is a vampire \u21aa\u2192 based on these premises, so the \u21aa\u2192 statement is false. ANSWER: False\nExample 2 Premises: Some mammals have teeth. Platypus have no teeth. Platypus are mammals. Humans have teeth.\nConclusion: Platypus are reptiles.\n(GPT-4 Incorrect CoT): Let's think step by step. \u21aa\u2192 We want to evaluate if platypus are \u21aa\u2192 reptiles. We know from the premises that \u21aa\u2192 platypus are mammals and that they have \u21aa\u2192 no teeth. The fact that some mammals have \u21aa\u2192 teeth does not contradict the fact that \u21aa\u2192 platypus, although mammals, do not have \u21aa\u2192 teeth. Moreover, the statement about \u21aa\u2192 humans having teeth is unrelated to the \u21aa\u2192 classification of platypus. There's no \u21aa\u2192 evidence in the premises that would \u21aa\u2192 suggest platypus are reptiles. Therefore, \u21aa\u2192 the statement is false.\nC2: CoT makes incorrect logical deductions.\nExample 1 Premises: International students in US have either an F1\n\u21aa\u2192 visa or a J1 visa. An international student in US with an F1 visa\n\u21aa\u2192 needs to apply for CPT or OPT if the \u21aa\u2192 student wants to work in the US.\nMike is an international student. Mike needs to apply for CPT if he wants to work\n\u21aa\u2192 in the US.\nConclusion (Ground Truth: Uncertain): Mike has an F1 visa.\n(GPT-4 Incorrect CoT): Let's think step by step. \u21aa\u2192 We want to evaluate if Mike has an F1 \u21aa\u2192 visa. From the premises, we know that \u21aa\u2192 Mike is an international student and\n\u21aa\u2192 needs to apply for CPT if he wants to \u21aa\u2192 work in the US. We also know that an \u21aa\u2192 international student in the US with an \u21aa\u2192 F1 visa needs to apply for CPT or OPT if \u21aa\u2192 the student wants to work in the US. So, \u21aa\u2192 it is clear that Mike has an F1 visa \u21aa\u2192 since he fits the criteria. Therefore, \u21aa\u2192 the conclusion is true.\nExample 2 Premises: If people perform in school talent shows often,\n\u21aa\u2192 then they attend and are very engaged \u21aa\u2192 with school events.\nPeople either perform in school talent shows \u21aa\u2192 often or are inactive and disinterested \u21aa\u2192 members of their community. If people chaperone high school dances, then \u21aa\u2192 they are not students who attend the \u21aa\u2192 school. All people who are inactive and disinterested \u21aa\u2192 members of their community chaperone high \u21aa\u2192 school dances. All young children and teenagers who wish to \u21aa\u2192 further their academic careers and \u21aa\u2192 educational opportunities are students \u21aa\u2192 who attend the school. Bonnie either both attends and is very engaged \u21aa\u2192 with school events and is a student who \u21aa\u2192 attends the school, or she neither \u21aa\u2192 attends and is very engaged with school \u21aa\u2192 events nor is a student who attends the \u21aa\u2192 school.\nConclusion (Ground Truth: Uncertain): Bonnie performs in school talent shows often.\n(GPT-4 Incorrect CoT): Let's think step by step. \u21aa\u2192 We want to evaluate if Bonnie performs \u21aa\u2192 in school talent shows often. Based on \u21aa\u2192 the last premise, if Bonnie is a student \u21aa\u2192 who attends the school and is very \u21aa\u2192 engaged with school events, this is only \u21aa\u2192 possible if she performs in school talent \u21aa\u2192 shows often, as per the first premise. \u21aa\u2192 If she doesn't attend and is not engaged \u21aa\u2192 with school events, she is an inactive \u21aa\u2192 and disinterested member of her community \u21aa\u2192 . But the fourth premise states that \u21aa\u2192 these individuals chaperone high school \u21aa\u2192 dances, meaning they cannot be students, \u21aa\u2192 which would contradict the last premise. \u21aa\u2192 Therefore, Bonnie performs in school \u21aa\u2192 talent shows often, so the statement is \u21aa\u2192 true.\nC3: CoT fails to find complex paths of reasoning. We highlight two examples below: in the first example, the ground truth is false. To make this deduction, one must reason that if a Greyhound is a Boeing 707, then it is a plane, which means it is empty, which means it cannot transport multiple passengers, which means it is not an airline, which means there are no Greyhound planes, which is a contradiction. Looking at the CoT generations, the first CoT attempt gives up after failing to find any\nlink between Greyhound and Boeing 707. The second generation attempts to make deductions from the premises. In this case, none of the 10 CoT reasoning chains begin with the correct step of starting from the negation of the conclusion and deducing it false.\nIn the second example, to make the correct deduction, we need to start from the fact that Rose is young or a student. If Rose is young, then they do not teach, which means they study, which means they are a student, which means they are a human. If Rose is a student, then they are humans. Neither of the CoT generation is able to make progress on the deduction from the information that Rose is young. In addition, the first CoT generation also has a logical error at the last step, where it asserts that \u201cA or False\u201d is False when the truthness of A is uncertain.\nExample 1 Premises: If something can transport multiple passengers,\n\u21aa\u2192 then they are not empty. All airlines can transport multiple passengers. All planes are empty. All Boeing 707s are planes. If Greyhound is not an airline, then there are\n\u21aa\u2192 no Greyhound planes.\nConclusion (Ground Truth: False) A Greyhound is a Boeing 707.\n(GPT-4 Incorrect CoT, 1): Let's think step by \u21aa\u2192 step. We want to evaluate if a Greyhound \u21aa\u2192 is a Boeing 707. Looking at our premises, \u21aa\u2192 we have no direct or indirect \u21aa\u2192 information linking Greyhounds to Boeing \u21aa\u2192 707s. Given the information available, we \u21aa\u2192 cannot make any certain conclusion about \u21aa\u2192 a Greyhound being a Boeing 707. \u21aa\u2192 Therefore, it is uncertain whether a \u21aa\u2192 Greyhound is a Boeing 707.\n(GPT-4 Incorrect CoT, 2): Let's think step by \u21aa\u2192 step. We want to evaluate if a Greyhound \u21aa\u2192 is a Boeing 707. The first three premises \u21aa\u2192 don't provide any information about \u21aa\u2192 Greyhound or Boeing 707s. The fourth \u21aa\u2192 premise tells us that all Boeing 707s are \u21aa\u2192 planes but doesn't tell us anything \u21aa\u2192 about Greyhound. The fifth premise tells \u21aa\u2192 us that if Greyhound is not an airline, \u21aa\u2192 then there are no Greyhound planes, but \u21aa\u2192 this still doesn't imply that a Greyhound \u21aa\u2192 is a Boeing 707. Therefore, we do not \u21aa\u2192 have enough information to conclude \u21aa\u2192 whether a Greyhound is a Boeing 707 or \u21aa\u2192 not. So, the statement is uncertain.\nExample 2 Premises: All students are humans. If someone studies, then they are a student. A person either studies or teaches.\nNo young person teaches. Jerry neither teaches nor is a manager. Rose is young or a student.\nConclusion (Ground Truth: True) Rose is a human or Jerry is a manager.\n(GPT-4 Incorrect CoT, 1): Let's think step by \u21aa\u2192 step. We want to evaluate if Rose is a \u21aa\u2192 human or Jerry is a manager. Let's first \u21aa\u2192 look at Rose. From premise 6, we know \u21aa\u2192 that Rose is either young or a student. \u21aa\u2192 If Rose is a student, then from premise 1 \u21aa\u2192 we know that Rose is a human. If Rose is \u21aa\u2192 young, we don't know from the premises \u21aa\u2192 whether she is a human. Therefore, it is \u21aa\u2192 uncertain whether Rose is a human. Now \u21aa\u2192 let's look at Jerry. From premise 5, we \u21aa\u2192 know that Jerry is not a manager. \u21aa\u2192 Therefore, Jerry is not a manager. So, it \u21aa\u2192 is uncertain whether Rose is a human, \u21aa\u2192 but we know that Jerry is not a manager, \u21aa\u2192 so the statement is false.\n(GPT-4 Incorrect CoT, 2): Let's think step by \u21aa\u2192 step. We want to evaluate if Rose is a \u21aa\u2192 human or Jerry is a manager. First, we \u21aa\u2192 know that Jerry neither teaches nor is a \u21aa\u2192 manager, which means Jerry is not a \u21aa\u2192 manager. Therefore, the only way for the \u21aa\u2192 conclusion to be true is if Rose is a \u21aa\u2192 human. We know that Rose is either young \u21aa\u2192 or a student. If she's a student, since \u21aa\u2192 all students are humans, then she is a \u21aa\u2192 human. However, if Rose is young, we don' \u21aa\u2192 t have enough information to determine \u21aa\u2192 whether she's a human. Therefore, it's \u21aa\u2192 uncertain whether Rose is a human or \u21aa\u2192 Jerry is a manager.\nE.4 Shared mistakes between GPT-4 CoT and LINC\nAs shown in Fig. 5c, there are 21 common errors between GPT-4 CoT and GPT-4 LINC. After an indepth analysis of the examples, we see that 16 of these arise due to inherent errors in the dataset:\n\u2022 2 of these samples contain the sentence fragment \u201cEither Zaha Hadid\u2019s design style or Kelly Wearstler\u2019s design style.\u201d as a premise. This premise is likely intended to mean all design styles are one of these two styles, but this is hard for the model to grasp from just the fragment (sample 41, 42).\n\u2022 2 of these samples contain the sentence fragment \u201cEither female tennis players at Roland Garros 2022 or male tennis players at Roland Garros 2022.\u201d as a premise (sample 43, 45).\n\u2022 2 of these samples have an ambiguous use of \u201ceither\u201d: \u201cBen is either from The Simpsons or\nfunny,\u201d which in this case ambiguously means XOR. We believe this sentence is an unnatural usage of \u201ceither\u201d (sample 142, 143).\n\u2022 In 4 samples, there is a name that is implicitly an animal, but this is not clear (sample 126, 127, 128, 199). Also, in samples 126-128, there is a statement \u201cIf Rock is neither a fly nor a bird, then Rock neither flies nor breathes.\u201d that should likely say \u201cIf Rock neither flies nor is a bird, ...\u201d. With the original formulation, everything could be uncertain because nothing is known in the case that Rock is a fly (and this is a reasonable interpretation).\n\u2022 In 5 samples, the ground-truth FOL representation of the natural language premise is incorrect, causing the label to be incorrect (samples 29, 79, 85, 86, 87).\n\u2022 There is a sample where the conclusion is likely mis-worded and should be \u201cBarutin Cove is not located in Antarctica.\u201d instead of \u201cBarutin is not located in Antarctica.\u201d This changes the ground truth label. (sample 121).\nThe remaining 5 samples are examples where both methods fail. In all cases, CoT fails to find the correct reasoning chain, as the premises/reasoning path is convoluted and complex. Meanwhile, LINC fails as follows:\n\u2022 In sample 104, LINC generates separate symbols for NotLiveAtHome and -LiveAtHome, which are semantically equivalent.\n\u2022 In sample 149, LINC generates FOLs that miss implicit information; i.e. that people with names Rose and Jerry are humans.\n\u2022 In samples 172 and 173, LINC does not generate a single valid set of FOL premises because all generated expressions have a mismatched arity issue.\n\u2022 In sample 193, LINC fails to express the conclusion correctly: the conclusion is \u201cIf Yuri is not an American basketball player, then Yuri is a basketball player.\u201d, but the FOLs outputted are either -American(Yuri) & -BasketballPlayer(Yuri) or -American(Yuri) & BasketballPlayer(Yuri)"
        },
        {
            "heading": "F Proofwriter StarCoder+ Errors",
            "text": "We also analyzed, qualitatively, the errors that StarCoder+ makes on the ProofWriter dataset. Since premises from the ProofWriter dataset are relatively simple and direct, lossy translation is rarer. Most errors are due to syntax or improper representations. We show five representative errors below:\nPremise: All quiet people are round. StarCoder+ FOL: all x. Quiet(x) -> Round(x) Ground Truth FOL: all x. (Quiet(x) -> Round(x))\nPremise: If someone is quiet and nice then they \u21aa\u2192 are cold. Starcoder+ FOL: all x. (Quiet(x) \\& Nice(x)) -> \u21aa\u2192 Cold(x) Ground Truth FOL: all x. ((Quiet(x) \\& Nice(x)) \u21aa\u2192 -> Cold(x))\nPremise: If something is furry then it is quiet. Starcoder+ FOL: Furry(x) -> Quiet(x) Ground Truth FOL: all x. (Furry(x) -> Quiet(x))\nPremise: The cow eats the dog. Starcoder+ FOL: Cow(Eats(Dog)) Ground Truth FOL: Eats(Cow, Dog)\nPremise: The cow is blue. Starcoder+ FOL: Cow(x) \\& Blue(x) Ground Truth FOL: Blue(Cow)"
        },
        {
            "heading": "G StarCoderPlus FOLIO Error Analysis",
            "text": "For StarCoder+, we see a slightly different trend. In Fig. 6a, we see the same pattern as for GPT-4, with a majority of uncertain predictions. In Fig. 6b, however, we see that CoT for StarCoder+ primarily predicts true. This is likely because the model was trained on much more code than text, and may not have picked up sophisticated textual chain-ofthought reasoning capabilities. In Fig. 6c, we can see that the mispredictions between CoT and LINC differ much more for StarCoder+ than GPT4. Finally, in Fig. 6d, we see the same trends as we saw with GPT-4 but more pronounced, as the similarity between mispredictions in LINC and those in the baseline methods is even lower than they were for GPT-4.\nH The effect of K-way majority voting on LINC and CoT\nIn all our experiments, we use 10-way majority voting, inspired by prior work which found that Chain-of-Thought prompting benefited therefrom (Wang et al., 2023b). However, one might wonder how robust the performance gains seen with LINC are to the precise value of K. Figure 7 thus\nTr ue\nFa lse\nUn ce\nrta in\nEr ro\nr\nReference Label\nTrue\nFalse Uncertain\nError\nPr ed\nic te\nd La\nbe l\n31 7 6 0\n1 16 0 0\n25 29 55 0\n6 2 4 0\n0\n20\n40\n60\n80\n100\n(a) Confusion matrix for LINC.\nTr ue\nFa lse\nUn ce\nrta in\nEr ro\nr\nReference Label\nTrue\nFalse Uncertain\nError\nPr ed\nic te\nd La\nbe l\n61 38 55 0\n1 12 7 0\n1 4 3 0\n0 0 0 0\n0\n20\n40\n60\n80\n100\n(b) Confusion matrix for Chain-of-Thought.\nCo rre\nct\nIn co\nrre ct\nLINC Outcome\nCorrect\nIncorrectC oT\nO ut\nco m\ne\n37 39\n65 41\n0\n20\n40\n60\n80\n100\n(c) Comparing the consistency of LINC vs Chain-of-Thought.\nLI NC Co T\nSc ra\ntch pa d Na ive\nLINC\nCoTScratchpad\nNaive\n1\n0.11 1\n0.1 0.62 1\n0.21 0.44 0.48 1 0.2\n0.4\n0.6\n0.8\n1.0\n(d) Similarity between incorrect predictions of each method, i.e., (A wrong == B wrong) / (A wrong or B wrong).\nFigure 6: Analyzing and comparing the mistakes made by StarCoder+ on the FOLIO dataset.\nshows, for each model equipped with either LINC or Chain-of-Thought, how the accuracy on FOLIO varies with K \u2208 {1, 2, 3, . . . , 10}. We note that, generally speaking, LINC makes good use of increased values of K. This is especially true for the weaker models; these are more prone to generating syntactically invalid FOL expressions, which cause the solver to return an Error. Taking the majority vote over many samples thus lessens the risk of predicting Error, which is of course always the wrong label. Notably, our results do not indicate that CoT benefits from majority voting in this domain. Future work is needed to establish how this relates to the findings in the previously mentioned prior work."
        }
    ],
    "title": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers",
    "year": 2023
}