{
    "abstractText": "By routing input tokens to only a few split experts, Sparse Mixture-of-Experts has enabled efficient training of large language models. Recent findings suggest that fixing the routers can achieve competitive performance by alleviating the collapsing problem, where all experts eventually learn similar representations. However, this strategy has two key limitations: (i) the policy derived from random routers might be suboptimal, and (ii) it requires extensive resources during training and evaluation, leading to limited efficiency gains. This work introduces HyperRouter, which dynamically generates the router\u2019s parameters through a fixed hypernetwork and trainable embeddings to achieve a balance between training the routers and freezing them to learn an improved routing policy. Extensive experiments across a wide range of tasks demonstrate the superior performance and efficiency gains of HyperRouter compared to existing routing methods. Our implementation is publicly available at https://github.com/ giangdip2410/HyperRouter.",
    "authors": [
        {
            "affiliations": [],
            "name": "Giang Do"
        },
        {
            "affiliations": [],
            "name": "Khiem Le"
        },
        {
            "affiliations": [],
            "name": "Quang Pham"
        },
        {
            "affiliations": [],
            "name": "TrungTin Nguyen"
        },
        {
            "affiliations": [],
            "name": "Thanh-Nam Doan"
        },
        {
            "affiliations": [],
            "name": "Binh T. Nguyen"
        },
        {
            "affiliations": [],
            "name": "Chenghao Liu"
        },
        {
            "affiliations": [],
            "name": "Savitha Ramasamy"
        },
        {
            "affiliations": [],
            "name": "Xiaoli Li"
        },
        {
            "affiliations": [],
            "name": "Steven Hoi"
        }
    ],
    "id": "SP:319f8221d0c0e060e71e893deff8d9e57631a9df",
    "references": [
        {
            "authors": [
                "Koura",
                "Brian O\u2019Horo",
                "Jeffrey Wang",
                "Luke Zettlemoyer",
                "Mona Diab",
                "Zornitsa Kozareva",
                "Veselin Stoyanov"
            ],
            "title": "Efficient Large Scale Language Modeling with Mixtures of Experts",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2022
        },
        {
            "authors": [
                "Hangbo Bao",
                "Li Dong",
                "Songhao Piao",
                "Furu Wei"
            ],
            "title": "2022a. BEiT: BERT Pre-Training of Image Transformers",
            "venue": "Abu Dhabi, United Arab Emirates",
            "year": 2022
        },
        {
            "authors": [
                "tations. Hangbo Bao",
                "Wenhui Wang",
                "Li Dong",
                "Qiang Liu",
                "Owais Khan Mohammed",
                "Kriti Aggarwal",
                "Subhojit Som",
                "Songhao Piao",
                "Furu Wei"
            ],
            "title": "VLMo: Unified Vision-Language Pre-Training with Mixture-of",
            "year": 2022
        },
        {
            "authors": [
                "I\u00f1igo Casanueva",
                "Tadas Tem\u010dinas",
                "Daniela Gerz",
                "Matthew Henderson",
                "Ivan Vuli\u0107."
            ],
            "title": "Efficient intent detection with dual sentence encoders",
            "venue": "Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pages 38\u201345, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Association for Computational Linguistics. Oscar Chang",
                "Lampros Flokas",
                "Hod Lipson."
            ],
            "title": "Principled Weight Initialization for Hypernetworks",
            "venue": "International Conference on Learning Representations. Tianlong Chen, Zhenyu Zhang, AJAY KUMAR",
            "year": 2020
        },
        {
            "authors": [
                "JAISWAL",
                "Shiwei Liu",
                "Zhangyang Wang."
            ],
            "title": "Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers",
            "venue": "The Eleventh International Conference on Learning Representations. Zixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, and",
            "year": 2023
        },
        {
            "authors": [
                "Yuanzhi Li."
            ],
            "title": "Towards Understanding the Mixtureof-Experts Layer in Deep Learning",
            "venue": "Advances in Neural Information Processing Systems. Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Ba-",
            "year": 2022
        },
        {
            "authors": [
                "jaj",
                "Xia Song",
                "Xian-Ling Mao",
                "Heyan Huang",
                "Furu Wei"
            ],
            "title": "On the Representation Collapse of Sparse Mixture of Experts",
            "venue": "In Advances in Neural Information Processing Systems. Yinlam",
            "year": 2022
        },
        {
            "authors": [
                "Dhawal Gupta",
                "Moonkyung Ryu",
                "Mohammad Ghavamzadeh",
                "Craig Boutilier."
            ],
            "title": "A Mixture-of-Expert Approach to RL-based Dialogue Management",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "sirer",
                "Chris Jones",
                "Elena Buchatskaya",
                "David Budden",
                "Laurent Sifre",
                "Simon Osindero",
                "Oriol Vinyals",
                "Marc\u2019Aurelio Ranzato",
                "Jack Rae",
                "Erich Elsen",
                "Koray Kavukcuoglu",
                "Karen Simonyan"
            ],
            "title": "Unified Scaling Laws for Routed Language Models",
            "year": 2022
        },
        {
            "authors": [
                "Zihang Dai",
                "Zhilin Yang",
                "Yiming Yang",
                "Jaime Carbonell",
                "Quoc Le",
                "Ruslan Salakhutdinov."
            ],
            "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An Image is Worth 16x16 Words",
            "year": 2021
        },
        {
            "authors": [
                "Fedus",
                "Maarten P Bosma",
                "Zongwei Zhou",
                "Tao Wang",
                "Emma Wang",
                "Kellie Webster",
                "Marie Pellat",
                "Kevin Robinson",
                "Kathleen Meier-Hellstern",
                "Toju Duke",
                "Lucas Dixon",
                "Kun Zhang",
                "Quoc Le",
                "Yonghui Wu",
                "Zhifeng Chen",
                "Claire Cui"
            ],
            "title": "GLaM: Efficient Scaling",
            "year": 2022
        },
        {
            "authors": [
                "Anmol Gulati",
                "James Qin",
                "Chung-Cheng Chiu",
                "Niki Parmar",
                "Yu Zhang",
                "Jiahui Yu",
                "Wei Han",
                "Shibo Wang",
                "Zhengdong Zhang",
                "Yonghui Wu",
                "Ruoming Pang"
            ],
            "title": "Conformer: Convolution-augmented Transformer",
            "venue": "Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Dmitry Lepikhin",
                "HyoukJoong Lee",
                "Yuanzhong Xu",
                "Dehao Chen",
                "Orhan Firat",
                "Yanping Huang",
                "Maxim Krikun",
                "Noam Shazeer",
                "Zhifeng Chen."
            ],
            "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
            "venue": "International Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Learning Representations. Mike Lewis",
                "Shruti Bhosale",
                "Tim Dettmers",
                "Naman Goyal",
                "Luke Zettlemoyer."
            ],
            "title": "BASE Layers: Simplifying Training of Large, Sparse Models",
            "venue": "Proceedings of the 38th International Conference on Ma-",
            "year": 2021
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Learning Word Vectors for Sentiment Analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language",
            "year": 2011
        },
        {
            "authors": [
                "Matt Mahoney"
            ],
            "title": "Large text compression benchmark",
            "year": 2011
        },
        {
            "authors": [
                "Stephen Merity",
                "Caiming Xiong",
                "James Bradbury",
                "Richard Socher."
            ],
            "title": "Pointer Sentinel Mixture Models",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "Hao Peng",
                "Lili Mou",
                "Ge Li",
                "Yunchuan Chen",
                "Yangyang Lu",
                "Zhi Jin."
            ],
            "title": "A comparative study on regularization strategies for embedding-based neural networks",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2106\u2013",
            "year": 2015
        },
        {
            "authors": [
                "Quang Pham",
                "Chenghao Liu",
                "Doyen Sahoo",
                "Steven CH Hoi."
            ],
            "title": "Learning fast and slow for online time series forecasting",
            "venue": "arXiv preprint arXiv:2202.11672.",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Rives",
                "Joshua Meier",
                "Tom Sercu",
                "Siddharth Goyal",
                "Zeming Lin",
                "Jason Liu",
                "Demi Guo",
                "Myle Ott",
                "C. Lawrence Zitnick",
                "Jerry Ma",
                "Rob Fergus"
            ],
            "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
            "year": 2021
        },
        {
            "authors": [
                "Stephen Roller",
                "Sainbayar Sukhbaatar",
                "Arthur Szlam",
                "Jason E. Weston."
            ],
            "title": "Hash Layers For Large Sparse Models",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2021
        },
        {
            "authors": [
                "Carlos Riquelme Ruiz",
                "Joan Puigcerver",
                "Basil Mustafa",
                "Maxim Neumann",
                "Rodolphe Jenatton",
                "Andr\u00e9 Susano Pinto",
                "Daniel Keysers",
                "Neil Houlsby."
            ],
            "title": "Scaling Vision with Sparse Mixture of Experts",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2021
        },
        {
            "authors": [
                "Noam Shazeer",
                "*Azalia Mirhoseini",
                "*Krzysztof Maziarz",
                "Andy Davis",
                "Quoc Le",
                "Geoffrey Hinton",
                "Jeff Dean"
            ],
            "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
            "venue": "In International Conference on Learning Representations",
            "year": 2017
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
            "venue": "Proceedings of the 2013 Conference on Empirical Meth-",
            "year": 2013
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov."
            ],
            "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
            "venue": "Journal of Machine Learning Research, 15(56):1929\u20131958.",
            "year": 2014
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is All you Need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Haiyan Yin",
                "Ping Li"
            ],
            "title": "Mitigating forgetting in online continual learning with neuron calibration",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zhengyan Zhang",
                "Yankai Lin",
                "Zhiyuan Liu",
                "Peng Li",
                "Maosong Sun",
                "Jie Zhou."
            ],
            "title": "MoEfication: Transformer Feed-forward Layers are Mixtures of Experts",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 877\u2013890, Dublin, Ireland.",
            "year": 2022
        },
        {
            "authors": [
                "Yanqi Zhou",
                "Tao Lei",
                "Hanxiao Liu",
                "Nan Du",
                "Yanping Huang",
                "Vincent Zhao",
                "Andrew M Dai",
                "zhifeng Chen",
                "Quoc V Le",
                "James Laudon"
            ],
            "title": "Mixture-ofExperts with Expert Choice Routing",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Simiao Zuo",
                "Xiaodong Liu",
                "Jian Jiao",
                "Young Jin Kim",
                "Hany Hassan",
                "Ruofei Zhang",
                "Jianfeng Gao",
                "Tuo Zhao."
            ],
            "title": "Taming Sparsely Activated Transformer with Stochastic Experts",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent years have witnessed tremendous successes of the Transformer model (Vaswani et al., 2017) and its variants across a wide range of tasks, ranging from natural language and speech processing (Bao et al., 2022b; Gulati et al., 2020), computer vision (Dosovitskiy et al., 2021; Ruiz et al., 2021; Bao et al., 2022a), reinforcement learning (Chow et al., 2023), to life sciences (Rives et al., 2021). Since then, scaling up to larger models has become the prevailing approach for advancing the state-of-the-art in pre-training and finetuning tasks. However, training such large models comes with a high computational cost (Lin et al., 2022); therefore, there is a growing need to develop efficient strategies that facilitate the training of large language models (LLMs) (Fedus et al., 2022a). One\nof the most effective strategies thus far is the Sparse Mixture-of-Experts (SMoE) (Shazeer et al., 2017; Fedus et al., 2022b), which utilizes routers to direct each input token to a small subset of network parameters (experts). SMoE improves both efficiency and performance compared to dense training approaches (Lewis et al., 2021; Artetxe et al., 2022; Zhang et al., 2022; Du et al., 2022). Despite the encouraging results, SMoE has been found to encounter the issue of representation collapse, where all experts converge to similar representations (Chi et al., 2022; Chen et al., 2022). This problem arises due to the learning process of the router encouraging experts to cluster around a centroid (Chi et al., 2022). Therefore, significant efforts have been dedicated to addressing the representation collapse issue while preserving the simplicity and efficiency of SMoE training. To this end, one of the most effective strategies is freezing the router, as demonstrated by SMoE-Dropout (Chen et al., 2023), where a randomly initialized router remains fixed throughout the training process. Additionally, Chen et al. (2023) found that progressively increasing the number of selected experts can be beneficial. However, we argue that such a naive strategy exhibits two key limitations. Firstly, the random routing policy may be sub-optimal, which\nhinders the overall training process. Secondly, we will show in Section 3.3 that a fixed router will restrict the model\u2019s representation capabilities, necessitating a progressive increase in the number of chosen experts to achieve satisfactory performance. Therefore, SMoE-Dropout inherently suffers from limited representation and does not offer efficiency gains during inference. This work introduces a novel approach called HyperRouter to address the trade-off between fixed and trainable routers in SMoE training. Instead of employing a random but fixed router, HyperRouter utilizes a random but fixed hypernetwork (Ha et al., 2017) to generate the router\u2019s parameters based on a trainable router embedding. By doing so, HyperRouter can improve the routing policy throughout training while mitigating the representation collapse issues. To demonstrate its effectiveness, we conduct extensive evaluations on various NLP tasks, comparing HyperRouter to several state-of-the-art SMoE routing strategies. Moreover, HyperRouter achieves the same performance threshold with fewer experts (compute) during inference, thereby significantly enhancing the efficiency of deploying LLMs in real-world applications. Fig. 1 shows that HyperRouter consistently outperforms other competitors when the same number of experts is used during inference."
        },
        {
            "heading": "2 Related Work",
            "text": "Existing Routing Mechanism for SMoE. One of the most important components for training SMoE is the expert routing strategy, which specifies experts to process input tokens. There are two common classes of token-expert assignment algorithms for SMoE training: (1) letting tokens select the top-k experts and (2) letting experts select the topk tokens. For the first approach, (Fedus et al., 2022b, Switch Transformer), (Lepikhin et al., 2021, GShard), (Zuo et al., 2022, THOR), (Lewis et al., 2021, BASE), (Clark et al., 2022, S-BASE), and SMoE-Dropout (Chen et al., 2023) are representative methods. Meanwhile, Zhou et al. (2022) introduced expert choice that enables selecting different experts for each token and demonstrates the potential of the second approach."
        },
        {
            "heading": "On the Representation Collapse of SMoE. A",
            "text": "major research focus is how to improve the tokenexpert assignment to avoid the representation collapse issue where: (i) all the inputs are routed to the same expert (Zuo et al., 2022) or (ii) all\nexperts converge to similar representations (Chi et al., 2022; Chen et al., 2022). Such issues result in poor specialization among experts, parameter redundancy, and limited performance gains. Existing works address the issue (i) by employing various ad hoc-heuristics, e.g., adding Gaussian noise (Shazeer et al., 2017), limiting the maximum number of inputs that can be routed to an expert (Lepikhin et al., 2021), imposing a load balancing loss (Fedus et al., 2022b), using linear assignment (Lewis et al., 2021), or eliminating the necessity for router networks ((Zuo et al., 2022) employed a consistent regularized loss for stochastic expert assignment, (Roller et al., 2021) incorporated deterministic hashing assignments). To resolve the issue (ii), (Chi et al., 2022) proposed an X-MOE layer to improve the routing algorithm via dimension reduction, \u21132 normalization, and gating temperature. Furthermore, (Dai et al., 2022) proposed StableMoE with two training stages to reduce the router\u2019s fluctuations. The work related to ours the most is SMoE-Dropout (Chen et al., 2023), which is considered a randomly initialized and fixed router to route tokens. Thus, the routing policy is stable during training and deterministic during inference. Our work improves upon SMoE-Dropout by extending the fixed router to a hypernetwork to further improve the routing policy during training while alleviating the collapsing issues."
        },
        {
            "heading": "3 Methodology",
            "text": "This section describes SMoE training and details of the proposed HyperRouter method."
        },
        {
            "heading": "3.1 SMoE Training",
            "text": "We first describe SMoE training of LLMs, which consists of a router R(\u00b7) with parameter Wr and N expert networks {Ei(\u00b7)}Ni=1 with parameters Wei . We follow the most common implementation (Fedus et al., 2022b) to use a linear network as the router and split the feedforward networks in LLMs into N separated experts. Switch Transformer. Given an input token x with its representation h \u2208 Rd, the SMoE output y is calculated by routing h only to k-most suitable experts determined by the router, i.e.,\ny = N\u2211 j=1 R(h)j \u00b7 Ej(h) and (1)\nR(h) = TopK(\u03c3(Wr \u00d7 h), k), where the TopK(\u00b7, k) function keeps the largest k\na fixed hypernetwork. Yellow modules including the router embedding and experts are trainable while the gray module , the hypernetwork, is frozen. Best viewed in colors.\nvalues of a given vector while setting the remaining values to zero; \u03c3(\u00b7) is the standard Softmax function. In practice, only a few experts are activated for the current token (k \u226a N ) for efficient training. SMoE-Dropout. SMoE-Dropout (Chen et al., 2023) is a state-of-the-art strategy that addresses the representation collapse issues (Sec. 2) by simply fixing a randomly initialized router to improve the token routing consistency. Furthermore, SMoEDropout gradually increases the number of experts chosen throughout training, starting with k = 2 and ending at k = N . During the evaluation, SMoEDropout proposes to use half or all experts to balance its efficiency and performance."
        },
        {
            "heading": "3.2 HyperRouter",
            "text": "We describe our proposed HyperRouter strategy that balances between a random but fixed router and a router optimized during training. HyperRouter employs a fixed hypernetwork (Ha et al., 2017) H(\u00b7) to dynamically generate the router\u2019s parameters conditioned on a trainable router embedding e. Specifically, HyperRouter obtains the output y as:\ny = N\u2211 j=1 R(h)j \u00b7 Ej(h) and (2)\nR(h) = TopK(\u03c3(Wr \u00d7 h), k) and Wr = H(e), where e is a low dimensional trainable vector associated with the current layer. All parameters are jointly optimized to minimize the task loss. Lastly, like SMoE-Dropout, HyperRouter also gradually increases the number of activated experts through-\nout training, starting at k = 2 and ending at k = N . Fig. 2 visually illustrates our proposed HyperRouter method."
        },
        {
            "heading": "3.3 A Deeper Analysis of HyperRouter",
            "text": "We now investigate the representation capabilities of HyperRouter compared to the naive SMoE and SMoE-Dropout. Due to space constraints, we only present the main results here and provide the detailed calculations in Appendix B. First, the naive SMoE jointly trains the router and expert parameters, leading to entanglement and eventually the representation collapse issue (Zuo et al., 2022). SMoE-Dropout proposed to fix the router parameter Wr, which we will show to result in a restricted representation. To do so, we need to calculate the Jacobian with respect to h, which characterizes how the feature h is involved during training. Jacobian of SMoE and SMoE-Dropout. Let SSMOEj (h) = \u03c3(Wr\u00d7h)j , S Hyper j (h) = \u03c3(H(e)\u00d7 h)j , 1ji be the indicator function, and Wr,i be the icolumn of Wr, the Jacobians for SMoE and SMoEDropout are calculated as:\n\u2207hL =J\u22a41 \u2207yL+ J\u22a42 \u2207yL\n=J\u22a41 \u2207yL+ k\u2211\ni=1\nciWr,i, where (3)\nci = k\u2211\nj=1\nSSMOEj (h) ( 1ji \u2212 SSMOEi (h) ) \u00d7 ( Ej(h)\u22a4\u2207yL ) .\nHere, the Jacobian \u2207hL is a combination of two terms related to J1 and J2. The J1 component represents how h contributes to the output y and is the same for all methods. The second component related to J2 characterizes learning better experts\u2019 representation of h and is the main difference. Since SMoE-Dropout fixes Wr, its J\u03042 = J\u22a42 \u2207yL term is expressed as a linear combination of columns in Wr, which lies in a lower dimensional subspace because Wr \u2208 Rk\u00d7d with k \u226a d - number of experts is usually smaller than the feature dimension. Thus, SMoE-Dropout alleviates the entanglement between the router and experts at the cost of restricting the experts\u2019 representation capabilities. Jacobian of HyperRouter. By a similar calculation, the Jacobian of HyperRouter is shown in equation 4. Given that e is trainable, HyperRouter\u2019s Jacobian is not expressed as a simple linear combination in a much lower-dimensional subspace. In contrast,\nHyperRouter maps the feature to a subspace whose dimension can be controlled by e, which we can easily set to be greater than k. Furthermore, by fixing the hypernetwork, the Jacobian does not change freely as in SMoE, which helps alleviate the entanglement. Overall, HyperRouter can alleviate the representation collapse while not sacrificing the representation capabilities. Jacobian of HyperRouter:\n\u2207hL =J\u22a41 \u2207yL+ J\u22a42 \u2207yL\n=J\u22a41 \u2207yL+ k\u2211\ni=1\nciHi(e), where (4)\nJ1 = k\u2211\nj=1\nS Hyper j (h)\u2207h (Ej(h)) ,\nci = k\u2211\nj=1\nS Hyper j (h) ( 1ji \u2212 SHyperi (h) ) \u00d7 ( Ej(h)\u22a4\u2207yL ) ."
        },
        {
            "heading": "4 Experiments",
            "text": "We conduct experiments to investigate the following hypotheses. First, HyperRouter improves efficiency compared to existing routing strategies, requiring less computation to achieve similar performance thresholds during evaluation. Second, with the same amount of experts used during inference, HyperRouter can achieve better performances compared to other competitors."
        },
        {
            "heading": "4.1 Experiment Setting",
            "text": "Most of our experiments follow Chen et al. (2023) and consider a small TransformerXL (Dai et al., 2019) with four layers in our experiments due to the limited resources. We also investigate the scalability by considering larger variants of the TransformerXL with eight or twelve layers. We compare our HyperRouter with several state-of-the-art routing strategies: (i) SMoE (Fedus et al., 2022b) - training with trainable routers; (ii) THOR (Zhou et al., 2022) - replacing routers with a stochastic experts selection strategy and a consistency regularizer; (iii) SMoE-Dropout (Chen et al., 2023) - using a random router and gradually increasing the number of selected experts during training. We also include two traditional baselines: (i) Dense - the standard training of transformers where no routing mechanisms are implemented; and (ii) Dense+Dropout similar to Dense but with Dropout (Srivastava et al., 2014) inserted in the fully connected layers. All\nWikiText-103\n1 - - 896.47 560.93 65.17 2 - - 146.54 89.13 37.75 4 - - 54.69 40.00 30.45 8 - - 35.67 29.68 28.05 16 32.13 31.55 32.27 27.66 27.57\nbaselines use the same amount of trainable parameters, while our HyperRouter introduces a neglectable 1024 trainable parameters for the router embeddings. We consider two training tasks. First, pretraining on the enwik8 (Mahoney, 2011) and WikiText-103 (Merity et al., 2017) datasets, where we follow the same pre-training procedure as (Chen et al., 2023) to train the small and medium models for 400K iterations. We only train the large models for 100K iterations due to resource constraints. Second, finetuning on the SST-2 (Socher et al., 2013), SST-5 (Socher et al., 2013), IMDB (Maas et al., 2011), and BANKING77 (Casanueva et al., 2020) datasets using the model pre-trained on enwik8. Similar to (Chen et al., 2023), we also perform dense finetuning (always choosing all experts) for several epochs. In all experiments, we report the evaluation metrics on the test set when using different numbers of experts for routing methods. More implementation details and additional results are provided in the Appendix."
        },
        {
            "heading": "4.2 Pre-training Result",
            "text": "Tab. 1 reports the bit-per-character and perplexity on the enwik8 and WikiText-103 datasets, respectively. First, we observe that SMoE-based training offers improvements over Dense training. Furthermore, adding Dropout can help with Dense training, which achieves comparable performances with SMoE-Dropout, though its improvements diminish on the larger dataset of WikiText-103. The benefit of SMoE training is the efficiency during inference by using fewer experts. Our HyperRouter substantially outperforms both SMoE and SMoE-Dropout on both datasets in this regime. Notably, HyperRouter significantly\noutperforms SMoE-Dropout when using only one expert, reducing BPC from 3.02 to 1.48 on enwik8, and perplexity from 560.93 to 65.17 on WikiText-103. Overall, with eight experts or less, HyperRouter performs competitively with SMoEDropout while only using half of the experts during evaluation. Tab. 2 reports the BPC on the enwik8 dataset using the medium and large TransformerXL. Notably, we only train the large model 100K iterations due to resource constraints. With larger backbone networks, we observe the gap between our HyperRouter and the baselines becomes more significant, indicating our HyperRouter enjoys good scalability with the model complexity. Lastly, we observe that SMoEDropout does not achieve satisfactory results with the large backbone when only trained with 100K and is outperformed by the naive SMoE. This result shows that SMoE-Dropout requires significant resources to achieve good performance. On the other hand, our HyperRouter consistently outperforms both baselines, regardless of the backbone size and number of experts activated."
        },
        {
            "heading": "4.3 Finetuning Result",
            "text": "Tab. 3 reports the results of the finetuning experiment on the SST-2, SST-5, IMDB, and BANKING77 datasets. Although we perform dense finetuning,\nwe also report the results of using only half the experts during evaluation. Overall, we observe consistent accuracy gains from HyperRouter compared to other baselines on all datasets. Notably, on SST-2 and IMDB datasets, HyperRouter with only eight experts could substantially outperform other strategies when using all experts."
        },
        {
            "heading": "4.4 Router Analysis",
            "text": "We now investigate the distributional output from the routers (softmax of router\u2019s output) trained with different strategies. Such outputs determine how tokens are assigned to experts. We hypothesize that high-entropy distributions are not preferred since they are closer to being uniform, indicating the router\u2019s low confidence in choosing experts for the current token. In the extreme, the router outputs a uniform distribution and assigns tokens to all experts, eventually causing the collapse issue. Therefore, a router with a lower entropy is preferred since it confidently assigns tokens to a few experts, which can improve experts\u2019 specialization. To this end, we report the entropy of each router in the small TransformerXL trained on the enwik8 dataset in Tab. 4. The entropy is calculated on all samples in the test set using the model obtained after training. The result clearly shows that HyperRouter achieved much lower entropy than SMoE and SMoE-Dropout. Due to space constraints, we will provide additional visualizations and discussions in Appendix D."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we investigated the potentials and limitations of SMoE for training LLMs and proposed HyperRouter to balance between two extremes of SMoE: training and freezing the router. Consequently, HyperRouter can learn better routing policies while alleviating the representation collapse of conventional SMoE training. Experiments on both pre-training and fine-tuning tasks demonstrated promising HyperRouter\u2019s capabilities in facilitating efficient, effective training and inference compared to state-of-the-art strategies.\nLimitations\nOur work focuses on the efficiency and efficacy of training LLMs using SMoE. Despite the encouraging results, our experiments are conducted only on medium-scale datasets with a small TransformerXL due to computation limitations. Thus, further empirical evaluations are required to validate the scalability of HyperRouter and other SMoE strategies on recent LLMs and larger datasets.\nEthics Statement\nDespite encouraging results, training large-scale LLMs is inevitably costly and requires extensive computational resources, which need to be properly managed. Moreover, our work used data collected on the web, which has been known to suffer from gender and racial biases and requires additional efforts to mitigate its negative impacts. Lastly, our study is a promising step towards facilitating the development of new LLMs, which still requires careful regularization to avoid potential misuses in harmful applications."
        },
        {
            "heading": "Acknowledgement",
            "text": "This research/project is supported by the National Research Foundation, Singapore, under its AI Singapore Programme (AISG Award No: AISG2-RP2021-027)."
        },
        {
            "heading": "A Additional Figures",
            "text": "Fig. 3 illustrates the conceptual difference between the traditional SMoE (Fedus et al., 2022b), SMoE-Dropout (Chen et al., 2023), and our HyperRouter. In summary, SMoE uses a trainable router to send tokens to experts. On the other hand, SMoE-Dropout fixed a randomly initialized router and gradually increased the number of experts chosen throughout training. Lastly, HyperRouter improves upon SMoE-Dropout by replacing the router with a fixed hypernetwork and trainable router embedding."
        },
        {
            "heading": "B Derivation of the Jacobian",
            "text": "This Section details the calculations of the Jacobians presented in Section. 3.3. The Jacobian of SMoE and SMoE-Dropout is similar to the Jacobian of HyperRouter. Therefore, only the later detailed calculation is shown. Recall\nthat HyperRouter obtains the output y as follows:\ny = N\u2211 j=1 R(h)j \u00b7 Ej(h),\nR(h) =TopK(\u03c3(H(e)\u00d7 h), k). Here, \u03c3(\u00b7) is the standard Softmax function given by:\n\u03c3(H(e)\u00d7 h)j = exp((H(e)\u00d7 h)j)\u2211N i=1 exp((H(e)\u00d7 h)i)\n\u2261 SHyperj (h), \u2200j = 1, . . . , N. Without loss of generality and for simplicity, by rearranging the index of the top k-experts, HyperRouter can be rewritten as follows:\ny = k\u2211 j=1 S Hyper j (h) \u00b7 Ej(h), where\nS Hyper j (h) = exp((H(e)\u00d7 h)j)\u2211k i=1 exp((H(e)\u00d7 h)i) ,\n\u2200j = 1, . . . , k. The Jacobian matrix J of the output y with respect to h can be decomposed into two terms as follows:\n\u2207hy =\u2207h  k\u2211 j=1 S Hyper j (h) \u00b7 Ej(h)  =\nk\u2211 j=1 S Hyper j (h)\u2207h (Ej(h))\n+ k\u2211 j=1 \u2207h ( S Hyper j (h) ) Ej(h)\n= k\u2211 j=1 S Hyper j (h)\u2207h (Ej(h))\n+ k\u2211 j=1 k\u2211 i=1 S Hyper j (h) ( 1ji \u2212 SHyperi (h) ) \u00d7 Ej(h)Hi(e)\u22a4 \u2261 J1 + J2 (5)\n(since e is independent of h). Note that the last equation in (5) is obtained by using the chain rule, the logarithmic derivative, and the inner product as follows: \u2202sj \u2202zi = sj \u00b7 \u2202 \u2202zi log(sj) = sj \u00b7 (1ji \u2212 si), where sj = exp(zj)\u2211k i=1 exp(zi) , \u2200j = 1, . . . , k. It is worth mentioning that the first term J1 means to produce a better token representation given the current activation router SHyperj (h), while the second term J2 represents learning a better gating function for the appropriate activation score router S\nHyper j (h). After the back-propagation, the gradi-\nrepresentation vector (could be output from the previous layer). Yellow modules are trainable. Gray modules are frozen, indicated by a lock symbol ( ). Best viewed in colors.\nent of the loss function L is obtained from the two paths mentioned above and is written as follows\n\u2207hL =\u2207yL \u00d7\u2207hy = \u2207hy\u22a4 \u00d7\u2207yL =(J1 + J2)\n\u22a4 \u00d7\u2207yL (using (5)) =J\u22a41 \u2207yL+ J\u22a42 \u2207yL. (6)\nFinally, by expanding the second term as follows, we obtain the desired result:\nJ\u22a42 \u2207yL = k\u2211\ni=1 k\u2211 j=1 S Hyper j (h) ( 1ji \u2212 SHyperi (h) ) \u00d7 ( Ej(h)\u22a4\u2207yL ) Hi(e)\n= k\u2211\ni=1\nciHi(e), where (7)\nci = k\u2211\nj=1\nS Hyper j (h) ( 1ji \u2212 SHyperi (h) ) \u00d7 ( Ej(h)\u22a4\u2207yL ) ."
        },
        {
            "heading": "C Additional Experiments",
            "text": "This section provides the implementation details of our experiments in Sec. 4.\nC.1 General Setting\nOur experiments are conducted based on the publicly available SMoE-Dropout (Chen et al., 2023) implementation1. Moreover, the pre-training experiments were conducted on a single A40 or A100 GPU, while the finetuning experiments were conducted on a single GeForce RTX 3090 GPU. We also emphasize that parallel training on multiple GPUs might yield different results.\n1https://github.com/VITA-Group/ Random-MoE-as-Dropout\nModel architecture. The small TransformerXL variant (Chen et al., 2023) consists of 4 Transformer decoder layers with an input dimension of 256. Each layer consists of a self-attention layer with 8 attention heads, followed by a feedforward network with an inner dimension of 512. The dropout ratio is kept at 0.1. We split the feedforward network into 16 experts with the same dimension. For the HyperRouter , we initialize the embeddings with size 256 and employ a 2-layer perceptron with an inner dimension of 256 as the hypernetwork. The ReLU function is used as the activation function for the hypernetwork. Totally our HyperRouter introduces an additional 1024 trainable parameters in the TransformerXL model. It is worth noting that the parameter overhead is fixed as we scale up to larger transformer models. For the medium and large variants, we scale the model to eight and twelves layers, respectively.\nC.2 Pre-training Experiments\nTab. A1 provides the implementation details for pre-training our TransformerXL small and medium on enwik8 and WikiText-103. The TransformerXL large network was trained in the same maner, but only for 100K iterations.\nTable A1: Implementation details for pre-training experimentson enwik8 and WikiText-103 datasets.\nDataset Input length Batch size Optimizer Lr # Iterations\nenwik8 512 22 Adam 2.5e-4 400000 WikiText-103 512 22 Adam 2.5e-4 400000\nC.3 Finetuning Experiments\nTo conduct finetuning experiments, we use the same model architecture as in pre-training. Tab. A2 provides the implementation details used for finetuning experiments on four different datasets.\nTable A2: Implementation details for finetuning experiments on four different datasets.\nDataset Input length Batch size Optimizer Lr # Epochs\nSST-2 512 16 Adam 1e-4 3 SST-5 512 16 Adam 1e-4 3 IMDB 512 4 Adam 1e-4 3 BANKING77 512 16 Adam 1e-4 15\nC.4 Inference Efficiency\nWe discuss an implementation detail that makes our HyperRouter have the same efficiency during inference as SMoE and SMoE-Dropout. Particularly, before performing inference on any input samples, we use the hypernetwork to generate the router for each layer and then store them. During inference, HyperRouter does not need to re-generate routers. Therefore, HyperRouter enjoys the same inference complexity as SMoE and SMoE-Dropout because their routers have the same dimensionality. Tab. A3 reports the number of FLOPs of different methods during inference on the enwik8 dataset according to a different number of experts chosen (k). Note that the FLOPs do not scale linearly with k since SMoE training is only applied to the fully connected layers, while other layers (multi-head self-attention, layer normalization, etc.) remain the same across baselines. In the extreme, using only one expert only incurs 45% of the FLOPs compared to using all 16 experts.\nTable A3: Inference FLOPS (1010) on the enwik8 dataset, k denotes the number of experts used during inference.\nk Dense Dense+Dropout THOR SMoE SMoE-Dropout HyperRouter\n1 - - - 3.47 3.47 3.47 2 - - - 4.00 4.00 4.00 4 - - - 4.54 4.54 4.54 8 - - - 5.61 5.61 5.61 16 7.76 7.76 7.66 7.66 7.66 7.66\nC.5 Parameter Comparison\nTab. A4 provides the number of parameters in different components of SMoE-Dropout and HyperRouter . There are three categories: (i) trainable parameters which are the transformer\nTable A4: Number of parameters in different components of SMoE-Dropout and HyperRouter during training. Blue parameters are trainable, red parameters are frozen, and underline parameter are dynamically generated in each iteration.\nMethod Router Emb. Router HyperNetwork Transformer\nSMoE-Dropout \u2212 16.448 \u2212 19, 505.350 HyperRouter 1.024 16.448 1, 112.576 19, 505.350\nbackbone and the router embeddings; (ii) frozen parameters which are the routers SMoE-Dropout and hypernetworks in HyperRouter ; (iii) dynamic parameters that are dynamically generated in each iteration, such as the routers in HyperRouter . Overall, the additional trainable parameters in HyperRouter is neglectable. Moreover, the number of frozen parameters (hypernetworks) is quite small compared to the transformer backbone (5.7%). Investigating into sharing the hypernetworks across layers or generating the routers coordinately (Pham et al., 2022) can further reduce this cost while improving the performance, which we will leave for the future work."
        },
        {
            "heading": "D Routing Visualization",
            "text": "Table A5: Average entropy of the distribution of the routers\u2019 output on the enwik8 dataset. Lower is better.\nMethod Router 1 Router 2 Router 3 Router 4\nSMoE 2.5164 2.2126 2.2424 2.2583 \u00b1 0.17 \u00b1 0.31 \u00b1 0.30 \u00b1 0.25 SMoE-Dropout 2.5849 2.1624 2.2579 2.2690 \u00b1 0.11 \u00b1 0.37 \u00b1 0.28 \u00b1 0.27 HyperRouter 1.4393 0.8894 1.1269 1.3477 \u00b1 0.42 \u00b1 0.46 \u00b1 0.46 \u00b1 0.50\nThis Section provides the full details of the routers\u2019 entropy and visualizes its distributional output. This is supplementary to Section 4.4. Tab. A5 reports the mean and standard deviations of the entropy at each router. This table is the full version of Tab. A5. We can see that all methods have rather low standard deviation, indicating that the differences are significant. We also provide an illustrative example of the routers\u2019 outputs using a randomly picked sample on the test set in Fig. 4. Here we can clearly see that the policies from SMoE and SMoE-Dropout are much closer to uniform while HyperRouter \u2019s policies are much sharper and have lower entropy. We emphasize that this example is not cherry-picked since we already calculated the averaged entropy\non all samples in Tab. A5. Overall, this result shows insights into how HyperRouter can perform better than other state-of-the-art SMoE strategies."
        },
        {
            "heading": "E Future Work",
            "text": "Our HyperRouter opens several promising venues for future research. Particularly, we believe that investigating two HyperRouter components: (i) fixed hypernetwork, and (ii) trainable embedding can yield further performance and efficiency gains. Potentials directions include incorporating regularization such as \u21132-penalty or dropout (Peng et al., 2015) or using better hypernetwork initialization techniques (Chang et al., 2020). Furthermore, the current implementation uses a hypernetwork for each transformer layer, and it generates all parameters of the router. Sharing hypernetworks among layers or generating the router coordinate-wise can offer knowledge sharing (Yin et al., 2021; Pham et al., 2022), which can further improve the result."
        }
    ],
    "title": "HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts via HyperNetwork",
    "year": 2023
}