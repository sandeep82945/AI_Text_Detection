{
    "abstractText": "Neural \u2018dense\u2019 retrieval models are state of the art for many datasets, however these models often exhibit limited domain transfer ability. Existing approaches to adaptation are unwieldy, such as requiring explicit supervision, complex model architectures, or massive external models. We present ABEL, a simple but effective unsupervised method to enhance passage retrieval in zero-shot settings. Our technique follows a straightforward loop: a dense retriever learns from supervision signals provided by a reranker, and subsequently, the reranker is updated based on feedback from the improved retriever. By iterating this loop, the two components mutually enhance one another\u2019s performance. Experimental results demonstrate that our unsupervised ABEL model outperforms both leading supervised and unsupervised retrievers on the BEIR benchmark. Meanwhile, it exhibits strong adaptation abilities to tasks and domains that were unseen during training. By either fine-tuning ABEL on labelled data or integrating it with existing supervised dense retrievers, we achieve state-of-the-art results.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Fan Jiang"
        },
        {
            "affiliations": [],
            "name": "Qiongkai Xu"
        },
        {
            "affiliations": [],
            "name": "Tom Drummond"
        },
        {
            "affiliations": [],
            "name": "Trevor Cohn"
        }
    ],
    "id": "SP:6bf6cfbe24eed8d1dcff090409fc012a47d6ac71",
    "references": [
        {
            "authors": [
                "Chris Alberti",
                "Daniel Andor",
                "Emily Pitler",
                "Jacob Devlin",
                "Michael Collins."
            ],
            "title": "Synthetic QA corpora generation with roundtrip consistency",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6168\u20136173, Flo-",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Anthony",
                "Zheng Tian",
                "David Barber."
            ],
            "title": "Thinking fast and slow with deep learning and tree search",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Akari Asai",
                "Timo Schick",
                "Patrick Lewis",
                "Xilun Chen",
                "Gautier Izacard",
                "Sebastian Riedel",
                "Hannaneh Hajishirzi",
                "Wen tau Yih"
            ],
            "title": "Task-aware retrieval with instructions",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Berant",
                "Andrew Chou",
                "Roy Frostig",
                "Percy Liang."
            ],
            "title": "Semantic parsing on Freebase from question-answer pairs",
            "venue": "Proceedings of the 2013",
            "year": 2013
        },
        {
            "authors": [
                "Xilun Chen",
                "Kushal Lakhotia",
                "Barlas Oguz",
                "Anchit Gupta",
                "Patrick Lewis",
                "Stan Peshterliev",
                "Yashar Mehdad",
                "Sonal Gupta",
                "Wen-tau Yih"
            ],
            "title": "Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one",
            "year": 2022
        },
        {
            "authors": [
                "Zhuyun Dai",
                "Vincent Y Zhao",
                "Ji Ma",
                "Yi Luan",
                "Jianmo Ni",
                "Jing Lu",
                "Anton Bakalov",
                "Kelvin Guu",
                "Keith Hall",
                "Ming-Wei Chang."
            ],
            "title": "Promptagator: Fewshot dense retrieval from 8 examples",
            "venue": "The Eleventh International Conference on Learning Representa-",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Thibault Formal",
                "Carlos Lassance",
                "Benjamin Piwowarski",
                "St\u00e9phane Clinchant."
            ],
            "title": "From distillation to hard negative sampling: Making sparse neural ir models more effective",
            "venue": "Proceedings of the 45th International ACM SIGIR Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Thibault Formal",
                "Carlos Lassance",
                "Benjamin Piwowarski",
                "St\u00e9phane Clinchant"
            ],
            "title": "Splade v2: Sparse lexical and expansion model for information retrieval",
            "year": 2021
        },
        {
            "authors": [
                "Revanth Gangi Reddy",
                "Vikas Yadav",
                "Md Arafat Sultan",
                "Martin Franz",
                "Vittorio Castelli",
                "Heng Ji",
                "Avirup Sil."
            ],
            "title": "Towards robust neural retrieval with source domain synthetic pre-finetuning",
            "venue": "Proceedings of the 29th International Conference on Computational",
            "year": 2022
        },
        {
            "authors": [
                "Luyu Gao",
                "Jamie Callan."
            ],
            "title": "Condenser: a pretraining architecture for dense retrieval",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 981\u2013993, Online and Punta Cana, Dominican Republic. Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Luyu Gao",
                "Jamie Callan."
            ],
            "title": "Unsupervised corpus aware language model pre-training for dense passage retrieval",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2843\u20132853,",
            "year": 2022
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Gillick",
                "Alessandro Presta",
                "Gaurav Singh Tomar"
            ],
            "title": "End-to-end retrieval in continuous space",
            "year": 2018
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Mingwei Chang."
            ],
            "title": "Retrieval augmented language model pre-training",
            "venue": "International conference on machine learning, pages 3929\u20133938. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "R. Hadsell",
                "S. Chopra",
                "Y. LeCun."
            ],
            "title": "Dimensionality reduction by learning an invariant mapping",
            "venue": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906), volume 2, pages 1735\u20131742.",
            "year": 2006
        },
        {
            "authors": [
                "Junxian He",
                "Jiatao Gu",
                "Jiajun Shen",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "Revisiting self-training for neural sequence generation",
            "venue": "In International Conference on Learning Representations",
            "year": 2020
        },
        {
            "authors": [
                "Hamel Husain",
                "Ho-Hsiang Wu",
                "Tiferet Gazit",
                "Miltiadis Allamanis",
                "Marc Brockschmidt"
            ],
            "title": "Codesearchnet challenge: Evaluating the state of semantic code search",
            "year": 2019
        },
        {
            "authors": [
                "Gautier Izacard",
                "Mathilde Caron",
                "Lucas Hosseini",
                "Sebastian Riedel",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Edouard Grave."
            ],
            "title": "Unsupervised dense information retrieval with contrastive learning",
            "venue": "Transactions on Machine Learning Research.",
            "year": 2022
        },
        {
            "authors": [
                "Kalervo J\u00e4rvelin",
                "Jaana Kek\u00e4l\u00e4inen."
            ],
            "title": "Cumulated gain-based evaluation of ir techniques",
            "venue": "ACM Trans. Inf. Syst., 20(4):422\u2013446.",
            "year": 2002
        },
        {
            "authors": [
                "Jeff Johnson",
                "Matthijs Douze",
                "Herve Jegou."
            ],
            "title": "Billion-scale similarity search with gpus",
            "venue": "IEEE Transactions on Big Data, 7(3):535\u2013547.",
            "year": 2021
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel Weld",
                "Luke Zettlemoyer."
            ],
            "title": "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2017
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Amos Ng",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Hannaneh Hajishirzi",
                "Chris CallisonBurch."
            ],
            "title": "GooAQ: Open question answering with diverse answer types",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages",
            "year": 2021
        },
        {
            "authors": [
                "Omar Khattab",
                "Matei Zaharia."
            ],
            "title": "Colbert: Efficient and effective passage search via contextualized late interaction over bert",
            "venue": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR",
            "year": 2020
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics, 7:452\u2013466.",
            "year": 2019
        },
        {
            "authors": [
                "Kenton Lee",
                "Ming-Wei Chang",
                "Kristina Toutanova."
            ],
            "title": "Latent retrieval for weakly supervised open domain question answering",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086\u20136096, Florence, Italy.",
            "year": 2019
        },
        {
            "authors": [
                "Sheng-Chieh Lin",
                "Akari Asai",
                "Minghan Li",
                "Barlas Oguz",
                "Jimmy Lin",
                "Yashar Mehdad",
                "Wen tau Yih",
                "Xilun Chen"
            ],
            "title": "How to train your dragon: Diverse augmentation towards generalizable dense retrieval",
            "year": 2023
        },
        {
            "authors": [
                "Frederick Liu",
                "Terry Huang",
                "Shihang Lyu",
                "Siamak Shakeri",
                "Hongkun Yu",
                "Jing Li"
            ],
            "title": "Enct5: A framework for fine-tuning t5 as non-autoregressive models",
            "year": 2021
        },
        {
            "authors": [
                "Xueqing Liu",
                "Chi Wang",
                "Yue Leng",
                "ChengXiang Zhai."
            ],
            "title": "Linkso: A dataset for learning to retrieve similar question answer pairs on software development forums",
            "venue": "Proceedings of the 4th ACM SIGSOFT International Workshop on NLP for Software",
            "year": 2018
        },
        {
            "authors": [
                "Ji Ma",
                "Ivan Korotkov",
                "Yinfei Yang",
                "Keith Hall",
                "Ryan McDonald."
            ],
            "title": "Zero-shot neural passage retrieval via domain-targeted synthetic question generation",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Antoine Miech",
                "Jean-Baptiste Alayrac",
                "Ivan Laptev",
                "Josef Sivic",
                "Andrew Zisserman."
            ],
            "title": "Thinking fast and slow: Efficient text-to-visual retrieval with transformers",
            "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Sewon Min",
                "Julian Michael",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer."
            ],
            "title": "AmbigQA: Answering ambiguous open-domain questions",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5783\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Jianmo Ni",
                "Chen Qu",
                "Jing Lu",
                "Zhuyun Dai",
                "Gustavo Hernandez Abrego",
                "Ji Ma",
                "Vincent Zhao",
                "Yi Luan",
                "Keith Hall",
                "Ming-Wei Chang",
                "Yinfei Yang."
            ],
            "title": "Large dual encoders are generalizable retrievers",
            "venue": "Proceedings of the 2022 Conference on Empirical",
            "year": 2022
        },
        {
            "authors": [
                "Rodrigo Nogueira",
                "Zhiying Jiang",
                "Ronak Pradeep",
                "Jimmy Lin."
            ],
            "title": "Document ranking with a pretrained sequence-to-sequence model",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 708\u2013718, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Yingqi Qu",
                "Yuchen Ding",
                "Jing Liu",
                "Kai Liu",
                "Ruiyang Ren",
                "Wayne Xin Zhao",
                "Daxiang Dong",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "RocketQA: An optimized training approach to dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Ori Ram",
                "Gal Shachaf",
                "Omer Levy",
                "Jonathan Berant",
                "Amir Globerson."
            ],
            "title": "Learning to retrieve passages without supervision",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2022
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza."
            ],
            "title": "The probabilistic relevance framework: Bm25 and beyond",
            "venue": "Found. Trends Inf. Retr., 3(4):333\u2013389.",
            "year": 2009
        },
        {
            "authors": [
                "Zettlemoyer."
            ],
            "title": "Improving passage retrieval with zero-shot question generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3781\u20133797, Abu Dhabi, United Arab Emirates. Association for Com-",
            "year": 2022
        },
        {
            "authors": [
                "Devendra Sachan",
                "Mostofa Patwary",
                "Mohammad Shoeybi",
                "Neel Kant",
                "Wei Ping",
                "William L. Hamilton",
                "Bryan Catanzaro."
            ],
            "title": "End-to-end training of neural retrievers for open-domain question answering",
            "venue": "Proceedings of the 59th Annual Meeting of the",
            "year": 2021
        },
        {
            "authors": [
                "Keshav Santhanam",
                "Omar Khattab",
                "Jon Saad-Falcon",
                "Christopher Potts",
                "Matei Zaharia."
            ],
            "title": "ColBERTv2: Effective and efficient retrieval via lightweight late interaction",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of",
            "year": 2022
        },
        {
            "authors": [
                "Christopher Sciavolino",
                "Zexuan Zhong",
                "Jinhyuk Lee",
                "Danqi Chen."
            ],
            "title": "Simple entity-centric questions challenge dense retrievers",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6138\u20136148, Online",
            "year": 2021
        },
        {
            "authors": [
                "Nandan Thakur",
                "Nils Reimers",
                "Andreas R\u00fcckl\u00e9",
                "Abhishek Srivastava",
                "Iryna Gurevych."
            ],
            "title": "BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models",
            "venue": "Thirty-fifth Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Kexin Wang",
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "TSDAE: Using transformer-based sequential denoising auto-encoderfor unsupervised sentence embedding learning",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages",
            "year": 2021
        },
        {
            "authors": [
                "Kexin Wang",
                "Nandan Thakur",
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Shitao Xiao",
                "Zheng Liu",
                "Yingxia Shao",
                "Zhao Cao."
            ],
            "title": "RetroMAE: Pre-training retrieval-oriented language models via masked auto-encoder",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 538\u2013548, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Lee Xiong",
                "Chenyan Xiong",
                "Ye Li",
                "Kwok-Fung Tang",
                "Jialin Liu",
                "Paul N. Bennett",
                "Junaid Ahmed",
                "Arnold Overwijk."
            ],
            "title": "Approximate nearest neighbor negative contrastive learning for dense text retrieval",
            "venue": "International Conference on Learning",
            "year": 2021
        },
        {
            "authors": [
                "Yi Yang",
                "Wen-tau Yih",
                "Christopher Meek."
            ],
            "title": "WikiQA: A challenge dataset for open-domain question answering",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2013\u20132018, Lisbon, Portugal. As-",
            "year": 2015
        },
        {
            "authors": [
                "Yue Yu",
                "Chenyan Xiong",
                "Si Sun",
                "Chao Zhang",
                "Arnold Overwijk."
            ],
            "title": "COCO-DR: Combating the distribution shift in zero-shot dense retrieval with contrastive and distributionally robust learning",
            "venue": "Proceedings of the 2022 Conference on Empirical Meth-",
            "year": 2022
        },
        {
            "authors": [
                "Eric Zelikman",
                "Yuhuai Wu",
                "Jesse Mu",
                "Noah Goodman."
            ],
            "title": "STar: Bootstrapping reasoning with reasoning",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "lowing Wang"
            ],
            "title": "The best and secondbest results are marked in bold and underlined",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Remarkable progress has been achieved in neural information retrieval through the adoption of the dual-encoder paradigm (Gillick et al., 2018), which enables efficient search over vast collections of passages by factorising the model such that the encoding of queries and passages are decoupled, and calculating the query-passage similarity using dot product. However, the efficacy of training dualencoders heavily relies on the quality of labelled data, and these models struggle to maintain competitive performance on retrieval tasks where dedicated training data is scarce (Thakur et al., 2021).\nVarious approaches have been proposed to enhance dense retrievers (Karpukhin et al., 2020) in\n\u2217Now at Google DeepMind 1Source code is available at https://github.com/\nFantabulous-J/BootSwitch.\nzero-shot settings while maintaining the factorised dual-encoder structure, such as pre-training models on web-scale corpus (Izacard et al., 2022) and learning from cross-encoders through distillation (Qu et al., 2021). Other alternatives seek to trade efficiency for performance by using complex model architectures, such as fine-grained token interaction for more expressive representations (Santhanam et al., 2022) and scaling up the model size for better model capacity (Ni et al., 2022). Another line of work trains customised dense retrievers on target domains through query generation (Wang et al., 2022; Dai et al., 2023). This training paradigm is generally slow and expensive, as it employs large language models to synthesise a substantial number of high-quality queries.\nIn this paper, we present ABEL, an Alternating Bootstrapping training framework for unsupervised dense rEtrievaL. Our method alternates the distillation process between a dense retriever and a reranker by switching their roles as teachers and students in iterations. On the one hand, the dense retriever allows for efficient retrieval due to its factorised encoding, accompanied by a compromised model performance. On the other hand, a reranker has no factorisation constraint, allowing for more fine-grained and accurate scoring, but at the cost of intractable searches. Our work aims to take advantage of both schools by equipping the dense retriever with accurate scoring by the reranker while maintaining search efficiency. Specifically, i) the more powerful but slower reranker is used to assist in the training of the less capable but more efficient retriever; ii) the dense retriever is employed to improve the performance of the reranker by providing refined training signals in later iterations. This alternating learning process is repeated to iteratively enhance both modules.\nCompared with conventional bootstrapping approaches (Alberti et al., 2019; Zelikman et al., 2022), wherein the well-trained model itself is\nused to discover additional solutions for subsequent training iterations, our method considers one model (i.e., teacher) as the training data generator to supervise another model (i.e., student), and their roles as teacher and student are switched in every next step. This mechanism naturally creates a mutual-learning paradigm to enable iterative bidirectional knowledge flow between the retriever and the reranker, in contrast to the typical singlestep and unidirectional distillation where a student learns from a fixed teacher (Miech et al., 2021).\nThrough extensive experiments on various datasets, we observe that ABEL demonstrates outstanding performance in zero-shot settings by only using the basic BM25 model as an initiation. Additionally, both the retriever and reranker components involved in our approach can be progressively enhanced through the bootstrapping learning process, with the converged model outperforming more than ten prominent supervised retrievers and achieving state-of-the-art performance. Meanwhile, ABEL is efficient in its training process by exclusively employing sentences from raw texts as queries, rather than generating queries from large language models. The use of the simple dual-encoder architecture further contributes to its efficient operation. In summary, our contributions are:\n1. We propose an iterative approach to bootstrap the ability of a dense retriever and a reranker without relying on manually-created data. 2. The empirical results on the BEIR benchmark show that the unsupervised ABEL outperforms a variety of prominent sparse and supervised dense retrievers. After fine-tuning ABEL using supervised data or integrating it with off-theshelf supervised dense retrievers, our model achieves new state-of-the-art performance. 3. When applying ABEL on tasks that are unseen in training, we observe it demonstrates remarkable generalisation capabilities in comparison to other more sophisticated unsupervised dense retrieval methods. 4. To the best of our knowledge, we are the first to show the results that both dense retriever and cross-encoder reranker can be mutually improved in a closed-form learning loop, without the need for human-annotated labels."
        },
        {
            "heading": "2 Preliminary",
            "text": "Given a short text as a query, the passage-retrieval task aims at retrieving a set of passages that in-\nclude the necessary information. From a collection of passages as the corpus, P = {p1, p2, \u00b7 \u00b7 \u00b7 , pn}, a retriever D fetches top-k passages PkD(q) = {p1, p2, \u00b7 \u00b7 \u00b7 , pk} from P that are most relevant to a specific query q. Optionally, a reranker R is also employed to fine-grain the relevance scores for these retrieved passages."
        },
        {
            "heading": "2.1 Dense Retrieval Model",
            "text": "The dense retrieval model (retriever) encodes both queries and passages into dense vectors using a dual-encoder architecture (Karpukhin et al., 2020). Two distinct encoders are applied to transform queries and passages separately, then, a relevance score is calculated by a dot product,\nD(q, p; \u03b8) = E(q; \u03b8q)\u22a4 \u00b7E(p; \u03b8p), (1)\nwhere E(\u00b7; \u03b8) are encoders parameterised by \u03b8p for passages and \u03b8q for queries. The asymmetric dualencoder works better than the shared-encoder architecture in our preliminary study. For efficiency, all passages in P are encoded offline, and an efficient nearest neighbour search (Johnson et al., 2021) is employed to fetch top-k relevant passages."
        },
        {
            "heading": "2.2 Reranking Model",
            "text": "The reranking model (reranker) adopts a crossencoder architecture, which computes the relevance score between a query and a passage by jointly encoding them with cross-attention. The joint encoding mechanism is prohibitively expensive to be deployed for large-scale retrieval applications. In practice, the joint-encoding model is usually applied as a reranker to refine the relevance scores for the results by the retriever. The relevance score by the reranker is formalised as,\nR(q, p;\u03d5) = FFN(E(q, p;\u03d5)), (2)\nwhere E(\u00b7, \u00b7;\u03d5) is a pre-trained language model parameterised by \u03d5. In this work, we adopt the encoder of T5 (EncT5) (Liu et al., 2021) as E. A startof-sequence token is appended to each sequence, with its embedding fed to a randomly initialised single-layer feed-forward network (FFN) to calculate the relevance score."
        },
        {
            "heading": "3 Alternating Distillation",
            "text": "We propose an unsupervised alternating distillation approach that iteratively boosts the ability of a retriever and a reranker, as depicted in Fig. 1.\nAlg. 1 outlines the proposed method with three major steps. Our approach starts with warming up a retriever by imitating BM25. Subsequently, a recursive learning paradigm consisting of two steps is conducted: (1) training a reranker based on the labels extracted from the retriever by the last iteration; (2) refining the dense retriever using training signals derived from the reranker by the last step."
        },
        {
            "heading": "3.1 Retriever Warm-up",
            "text": "The training starts with constructing queries from raw texts (line 1) and training a warm-up dense retriever D0\u03b8 by imitating an unsupervised BM25 model (lines 3-5).\nQuery Construction We use a sentence splitter to chunk all passages into multiple sentences, then we consider these sentences as croppingsentence queries (Chen et al., 2022). Compared to queries synthesised from fine-tuned query generators (Wang et al., 2022) or large language models (Dai et al., 2023), cropping-sentence queries i) can be cheaply scaled up without relying on supervised data or language models and ii) have been shown effectiveness in pre-training dense retrievers (Gao and Callan, 2022; Izacard et al., 2022).\nTraining Data Initialisation The first challenge to our unsupervised method is how to extract effective supervision signals for each cropping-sentence query to initiate training. BM25 (Robertson and Zaragoza, 2009) is an unsupervised sparse retrieve model, which has demonstrated outstanding performance in low-resource and out-of-domain settings (Thakur et al., 2021). Specifically, for a given query q \u2208 Q, we use BM25 to retrieve the topk predictions PkBM25(q), among which the highest ranked k+ \u2208 Z+ passages are considered as\nAlgorithm 1: Alternating Bootstrapping Training for Zero-Shot Dense Retrieval\nInput :Pre-trained language models (e.g., T5), and Passage collection P .\n1 Split passages into sentences as queriesQ. 2 /* Step 1: Warm-up Retriever Training */ 3 Retrieve top-k predictions PkBM25(q) for each q \u2208 Q using BM25 model. 4 Extract initial training data Y0 using Eq. 3 and 4. 5 Train a warm-up retriever D0\u03b8 using Y0. 6 for t\u2190 1 to T do 7 /* Step 2: Reranker R\u03d5 Training */ 8 Retrieve top-k predictions PkDt\u22121\n\u03b8\n(q) for each\nq \u2208 Q using the most recent retriever Dt\u22121\u03b8 . 9 Extract soft labels for each (q, p) using Dt\u22121\u03b8 :\nYtsoft = {D(q, p;Dt\u22121\u03b8 )|p \u2208 P k\nDt\u22121 \u03b8 (q)}. 10 Train an rerankerRt\u03d5 with Ytsoft using Eq. 5. 11 /* Step 3: Retriever D\u03b8 Training */ 12 Rerank PkDt\u22121\n\u03b8 (q) usingRt\u03d5. 13 Extract updated training data Yt from the reranking list using Eq. 3 and 4. 14 Fine-tune Dt\u03b8 using Yt from D0\u03b8 . 15 end for 16 return DT\u03b8 .\npositive P+(q), while the bottom k\u2212 \u2208 Z+ passages are treated as hard negatives P\u2212(q), following Chen et al. (2022),\nP+(q) = {p|p \u2208 PkD(q), r(p) \u2264 k+}, (3) P\u2212(q) = {p|p \u2208 PkD(q), r(p) \u2265 k \u2212 k\u2212}, (4)\nwhere the initial D uses BM25 and r(p) means the rank of a passage p. Then, we train a warm-up retriever based on these extracted training examples."
        },
        {
            "heading": "3.2 Iterative Bootstrapping Training",
            "text": "We iteratively improve the capability of the retriever and the reranker by alternating their roles as teacher and student in each iteration.\nIn the t-th iteration, we first use the most recent retriever Dt\u22121\u03b8 to retrieve top-k passages P\nk Dt\u22121\u03b8 (q)\nthat are most relevant to each query q \u2208 Q and generate soft labels D(q, p; \u03b8t\u22121) for each (q, p) pair accordingly. Then we use all such soft labels to train a reranker Rt\u03d5 as described in Alg. 1 lines 8-10 and \u00a73.3.1. The second step is to train the t-th retriever. We employ Rt\u03d5 to rerank PkDt\u22121\u03b8 (q) to obtain a refined ranking list, from which updated supervision signals are derived as Alg. 1 lines 12- 13. We train a new retriever Dt\u03b8 with these examples as discussed in Alg. 1 line 14 and \u00a73.3.2. Training iterations are repeated until no performance improvement is observed. Note that, in order to mitigate the risk of overfitting the retriever, for all iterations we refine the warm-up retriever D0\u03b8 using the newest training examples but refresh top-k predictions and update soft labels for reranker training with the improved retriever. Similarly, to avoid the accumulation of errors in label generation, we reinitialise the reranker using pre-trained language models at the start of each iteration, rather than finetuning the model obtained from the last iteration. Please refer to Table 3 for the empirical ablations."
        },
        {
            "heading": "3.3 Retriever and Reranker Training",
            "text": "In each iteration, we fine-tune the reranker and then the retriever as follows."
        },
        {
            "heading": "3.3.1 Reranker Training",
            "text": "The reranker is trained to discriminate positive samples from hard negative ones using a cross-entropy (CE) loss,\nLCE = \u2212 log exp(s(q, p+;\u03d5))\u2211\np\u2208{p+}\u222aP\u2212q exp(s(q, p;\u03d5)) .\nIn our preliminary experiments, we observed that using hard labels to train the rereanker yields poor results. The reason is that hard labels use binary targets by taking one query-passage pair as positive and the rest pairs as negative, failing to provide informative signals for discerning the subtle distinctions among passages. To address this problem, we consider using the soft labels generated by a dense retriever D\u03b8 to guide the reranker training. These labels effectively capture the nuanced semantic relatedness among multiple relevant passages. Specifically, we employ the KL divergence loss:\nLKL = DKL(S(\u00b7|q,Pq, \u03d5)||T (\u00b7|q,Pq, \u03b8)),\nwhere Pq is the retrieved set of passages regarding to query q, which are sampled from PkDt\u22121\u03b8 (q).\nT (\u00b7|q,Pq, \u03b8) and S(\u00b7|q,Pq, \u03d5) are the distributions from the teacher retriever and the student reranker, respectively. Our preliminary experiment shows that adding noise to the reranker\u2019s inputs (e.g., word deletion) can further enhance the performance. For more details, please refer to Table 4 in Appendix B."
        },
        {
            "heading": "3.3.2 Retriever Training",
            "text": "For each query q, we randomly sample one positive p+ and one hard negative p\u2212 from P+(q) (Eq. 3) and P\u2212(q) (Eq. 4), respectively. In practice, we use in-batch negatives (Karpukhin et al., 2020) for efficient training. The dense retriever D\u03b8 is trained by minimising the negative log-likelihood of the positive passages using contrastive learning (CL) (Hadsell et al., 2006),\nLCL = \u2212 log exp(s(qi, p + i ; \u03b8))\u2211|B|\nj=1 \u2211 p\u2208{p+j ,p \u2212 j } exp(s(qi, p; \u03b8)) .\nwhere |B| is the size of a batch B. Similarly, we find that injecting noise into the inputs of the retriever during training results in improved performance, as demonstrated in Table 4 in Appendix B. We believe that manipulating words in queries and passages can be seen as a smoothing method to facilitate the learning of generalisable matching signals (He et al., 2020), therefore, preventing the retriever from simply replying on examining the text overlaps between the cropping-sentence query and passages. Additionally, adding noise tweaks the semantics of texts, which encourages the dense retriever to acquire matching patterns based on lexical overlaps in addition to semantic similarities."
        },
        {
            "heading": "3.4 Discussion",
            "text": "The novelty of our approach lies in two aspects: i) We introduce a separate reranker model for training label refinement. This allows the retriever training to benefit from an advanced reranker, which features a more sophisticated cross-encoding architecture. This design is effective compared to training the retriever on labels extracted from its own predictions (see the blue line in Figure 5(a)); ii) We create a mutual-learning paradigm through iterative alternating distillation. In our case, we consider the retriever as a proposal distribution generator, which selects relatively good candidates, and the reranker as a reward scoring model, which measures the quality of an example as a good answer. At each iteration, the improved reranker can be used to correct the predictions from the retriever,\nand underlined. Methods that train dedicated models for each of the datasets are noted with \u2020. Highlighted rows represent semantic relatedness tasks. QGen. means query generator. Please refer to Appendix A.2 for baseline details.\ntherefore reducing the number of inaccurate labels in retriever training data. Meanwhile, the improved retriever is more likely to include more relevant passages within its top-k predictions and provide more nuanced soft labels, which can enhance the reranker training and thus, increase the chance of finding correct labels by the reranker in the next iteration. As the training continues, (1) the candidates of correct answers can be narrowed down by the retriever (i.e., better proposal distributions), as shown in Fig. 4 and (2) the accuracy of finding correct answers from such sets can be increased by the reranker, as shown in Fig. 2(b). Overall, our framework (i.e., ABEL) facilitates synergistic advancements in both components, ultimately enhancing the overall effectiveness of the retrieval system."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "Our method is evaluated on the BEIR benchmark (Thakur et al., 2021), which contains 18 datasets in multiple domains such as wikipedia and biomedical, and with diverse task formats including question answering, fact verification and paraphrase retrieval. We use nDCG@10 (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002) as our primary metric and the average nDCG@10 score over all 18 datasets is used for comprehensive comparison between models. BEIR-13 (Formal et al., 2021) and PTR-11 (Dai et al., 2023) are two subsets of tasks, where BEIR13 excludes CQADupStack, Robust04, Signal-1M, TREC-NEWS, and BioASQ from the calculation of average score and PTR-11 further removes NQ and\nQuora. We also partition the datasets into query search (e.g., natural questions) and semantic relatedness (e.g., verification) tasks, in accordance with Santhanam et al. (2022),"
        },
        {
            "heading": "4.2 Experimental Settings",
            "text": "We use Contriever to initialise the dense retriever. The passages from all tasks in BEIR are chunked into cropping-sentence queries, on which a single retriever ABEL is trained. We initialise the reranker from t5-base-lm-adapt (Raffel et al., 2020) and train a single reranker ABEL-Rerank similarly. We conduct the alternating distillation loop for three iterations and observe that the performance of both the retriever and reranker converges, and we take ABEL in iteration 2 and ABEL-Rerank in iteration 3 for evaluation and model comparison. We further fine-tune ABEL on MS-MARCO (Bajaj et al., 2016) to obtain ABEL-FT, following the same training recipe outlined in Izacard et al. (2022). In addition, we also evaluate ABEL and ABEL-Rerank on various datasets that are unseen during training to test the generalisation ability. More details are in Appendix A."
        },
        {
            "heading": "4.3 Experimental Results",
            "text": "Retriever Results Both unsupervised and supervised versions of our model are compared with a range of corresponding baseline models in Table 1. The unsupervised ABEL outperforms various leading dense models, including models that are supervised by MS-MARCO training queries. The supervised model, ABEL-FT, achieves 1.0% better overall performance than ABEL. ABEL-FT also surpasses models using the target corpora for pre-training (COCO-DR), employing fine-grained token interaction (ColBERTv2), using sparse representations (SPLADE++), with larger sizes (GTR-XXL), and using sophisticated training recipes with diverse supervision signals (DRAGON+).\nConsidering semantic relatedness tasks, such as Signal-1M, Climate-FEVER, and SciFact, ABEL generally achieves results superior to other supervised dense retrievers. For query-search tasks, particularly question-answering tasks like NQ and HotpotQA, ABEL underperforms many dense retrievers. We attribute such outcomes to the differences in query styles. Semantic relatedness tasks typically use short sentences as queries, which aligns with the cropping-sentence query format employed in our work. However, query-search tasks often involve natural questions that deviate significantly\nfrom the cropping-sentence queries, and such format mismatch leads to the inferior performance of ABEL. For lexical matching tasks, such as Touch\u00e92020, ABEL surpasses the majority of dense retrievers by a considerable margin. We attribute this success to the model\u2019s ability to capture salient phrases, which is facilitated by learning supervision signals from BM25 in retriever warm-up training and well preserved in the following training iterations. Finally, ABEL outperforms GPL and PTR, even though they have incorporated high-quality synthetic queries and cross-encoder distillation in training. This observation demonstrates that a retriever can attain promising results in zero-shot settings without relying on synthetic queries.\nFor the supervised setting, the performance of ABEL can be improved by fine-tuning it on supervised data in dealing with natural questions. The major performance gains are from query-search tasks and semantic relatedness tasks, which involve human-like queries, such as NQ (42.0 to 50.2) and DBPedia (37.5 to 41.4). On other datasets with short sentences as queries (e.g., claim verification), the performance of ABEL-FT degrades but is comparable to other supervised retrievers. This limitation can be alleviated by combining ABEL and ABEL-FT, thereby achieving performance improvements on both types of tasks, as illustrated in the last two bars of Figure 3.\nReranker Results Figure 2 shows the averaged reranking performance on 9 subsets of BEIR, excluding FEVER and HotpotQA from PTR-11. As shown in Figure 2(b), the reranker is able to achieve improvements as the bootstrapping iteration progresses, providing strong evidence for the effec-\ntiveness of our iterative alternating distillation approach. The final reranker model, ABEL-Rerank (i.e., t = 3), as illustrated in Figure 2(a), enhances the performance of ABEL by 1.6%, surpassing supervised models of similar parameter sizes. It is noteworthy that ABEL-Rerank outperforms unsupervised SGPT (Muennighoff, 2022) and zero-shot UPR (Sachan et al., 2022), despite having much fewer parameters, and is comparable to PTR, which creates dedicated models for each task and employs high-quality synthetic queries. With the increase in model size, we consistently observe improvements with ABEL-Rerank and it outperforms TART (Asai et al., 2022) and MonoT5 (Nogueira et al., 2020) using only a fraction of the parameters. This finding demonstrates the potential of incorporating more capable rerankers to train better retrievers. We leave this exploration to future work. Please refer to Tables 5 and 7 in Appendix D for further details.\nCross-Task Results As shown in Table 2, when directly evaluating ABEL on various tasks unseen during training, it consistently achieves significant improvements over Contriever (+7.8%) and outperforms BM25 and other advanced unsupervised retrievers. These results show that ABEL is capable of capturing matching patterns between queries and passages that can be effectively generalised to unseen tasks, instead of memorising training corpus to achieve good performance. Please refer to Appendix E for more results."
        },
        {
            "heading": "4.4 Analysis",
            "text": "Pre-trained Models Table 3 #1 compares our approach with DRAGON when using different pretrained models for initialisation. ABEL outperforms DRAGON consistently using aligned pre-trained models, with up to +0.6% gains. Moreover, our method exhibits continued improvement as we\nuse more advanced pre-trained checkpoints. This demonstrates that our approach is orthogonal to existing unsupervised pre-training methods, and further gains are expected when more sophisticated pre-trained models are available.\nTraining Corpus We compare models trained using cropped sentences from different corpus as the training data. As shown in Table 3 #2, the method trained using MS-MARCO corpus is significantly better than the vanilla Contriever (+5.1%) but is inferior to the one using diverse corpus from BEIR, and we believe that the diverse corpora we used in training is one of the factors to our success.\nModel Re-initialisation We use re-initialisation to avoid the accumulation from biased errors in early iterations. At the start of each iteration, we re-initialise the retriever with the warm-up retriever and the reranker using pre-trained language models, respectively, rather than continuously fine-tuning the models obtained from the last iteration. Table 3 #3 shows the overall performance is increased by 0.8% using this re-initialisation technology.\nCombination with Supervised Models We investigate whether ABEL can advance supervised retrievers. We merge the embeddings from ABEL with different supervised models through concatenation. Specifically, for a given query q and passage p, and\ndense retrievers Eiq and E i p, we compute the relevance score as s(q, p) = [E1q ,E 1 p] \u22a4 \u00b7 [E2q ,E2p] =\u22112\ni=1E i\u22a4 q \u00b7 Eip. The results shown in Figure 3 indicate ABEL can be easily integrated with other models to achieve significant performance improvement, with an average increase of up to 4.5% on RetroMAE and even a 1% gain on ABEL-FT. Besides, the benefit is also remarkable (+10%) when combining ABEL with weak retrievers (i.e., ANCE). Overall, we observe that the ensembling can result in performance gains in both query-search and semantic-relatedness tasks, as demonstrated in Table 11 in Appendix F.\nEffect of Bootstrapping Figure 4 presents the comparison of ABEL\u2019s performance throughout all bootstrapping iterations against the BM25 and the supervised Contriever. We observe that the accuracy of the retriever consistently improves as the training iteration t progresses. Specifically, ABEL matches the performance of the supervised Contriever on the first iteration, and further gains are achieved with more iterations t \u2264 2. The performance converges at iteration 3, where the results on six tasks are inferior to those achieved at iteration 2. Please refer to Figure 6 in Appendix G for results on each individual task.\nSelf Supervision We investigate the necessity of taking the reranker as an expert in ABEL. Specifically, we use the top-k predictions of the latest retriever to extract training data at each iteration, instead of employing a separate reranker (i.e., without lines 8-10 in Alg.1). The blue line in Figure 5(a) indicates that the retriever struggles to improve when using itself as the supervisor. By investigating a small set of generated labels, we notice the extracted positive passages for most queries quickly converge to a stable set, failing to offer new signals in the new training round. This highlights the essential role of the expert reranker, which iteratively provides more advanced supervision signals.\nSynthetic Queries We assess the utility of synthetic queries in our approach by replacing cropping-sentence queries with synthetic queries from a query-generator fine-tuned on MSMARCO.2 The results in Figure 5(a) show that using synthetic queries is less effective and exhibits similar trends, with the performance improving consistently as the iterative alternating distillation progresses.3 Splitting task groups, we observe synthetic queries yield a larger performance drop on semantic-relatedness tasks than query-search tasks, in Figure 5(b). We attribute this disparity to the stylistic differences between the training and test queries. Synthetic queries exhibit similarities to natural questions in terms of style, akin to those found in question-answering datasets. In contrast, semantic relatedness tasks usually involve shortsentence queries (e.g., claim) that are closer to cropping-sentence queries. This finding empha-\n2https://public.ukp.informatik.tu-darmstadt. de/kwang/gpl/generated-data/beir\n3Using large language models for query generation may yield better results. We leave this exploration to future work.\nsises the importance of aligning the formats of training queries with test queries in zero-shot settings. Please refer to Figure 7 in Appendix H for results comparison in each individual task."
        },
        {
            "heading": "5 Related Work",
            "text": "Neural Information Retrieval Neural retrievers adopt pre-trained language models and follow a dual-encoder architecture (Karpukhin et al., 2020) to generate the semantic representations of queries and passages and then calculate their semantic similarities. Some effective techniques have been proposed to advance the neural retrieval models, such as hard negative mining (Xiong et al., 2021), retrieval-oriented pre-training objectives (Izacard et al., 2022), and multi-vector representations (Khattab and Zaharia, 2020). All of these approaches require supervised training data and suffer from performance degradation on outof-domain datasets. Our work demonstrates the possibility that an unsupervised dense retriever can outperform a diverse range of state-of-the-art supervised methods in zero-shot settings.\nZero-shot Dense Retrieval Recent research has demonstrated that the performance of dense retrievers under out-of-domain settings can be improved through using synthetic queries (Ma et al., 2021; Gangi Reddy et al., 2022; Dai et al., 2023). Integrating distillation from cross-encoder rerankers further advances the current state-of-the-art models (Wang et al., 2022). However, all of these methods rely on synthetic queries, which generally implies expensive inference costs on the usage of large language models. Nonetheless, the quality of the synthetic queries is worrying, although it can be improved by further efforts, such as fine-tuning the language model on high-quality supervised data (Wei et al., 2022). In contrast, our work does not have such reliance, and thus offers higher training efficiency. We show that effective large-scale training examples can be derived from raw texts in the form of cropping-sentence queries (Chen et al., 2022), and their labels can be iteratively refined by the retriever and the reranker to enhance the training of the other model.\nIterated Learning Iterated Learning refers to a family of algorithms that iteratively use previously learned models to update training labels for subsequent rounds of model training. These algorithms may also involve filtering out examples of low qual-\nity by assessing whether the solution aligns with the desired objective (Alberti et al., 2019; Zelikman et al., 2022; Dai et al., 2023). This concept can also be extended to include multiple models. One method is the iterated expert introduced by Anthony et al. (2017), wherein an apprentice model learns to imitate an expert and the expert builds on improved apprentice to find better solutions. Our work adapts such a paradigm to retrieval tasks in a zero-shot manner, where a notable distinction from previous work is the novel iterative alternating distillation process. For each iteration, the roles of the retriever and the reranker as apprentice and expert are alternated, enabling bidirectional knowledge transfer to encourage mutual learning."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we introduce ABEL, an unsupervised training framework that iteratively improves both retrievers and rerankers. Our method enhances a dense retriever and a cross-encoder reranker in a closed learning loop, by alternating their roles as teachers and students. The empirical results on various tasks demonstrate that this simple technique can significantly improve the capability of dense retrievers without relying on any human-annotated data, surpassing a wide range of competitive sparse and supervised dense retrievers. We believe that ABEL is a generic framework that could be easily combined with other retrieval augmenting techniques, and benefits a range of downstream tasks.\nLimitations\nThe approach proposed in this work incurs additional training costs on the refinement of training labels and the iterative distillation process when compared to standard supervised dense retriever training. The entire training pipeline requires approximately one week to complete on a server with 8 A100 GPUs. This configuration is relatively modest according to typical academic settings.\nWe focus on the standard dual-encoder paradigm, and have not explored other more advanced architectures, such as ColBERT, which offer more expressive representations. We are interested in investigating whether incorporating these techniques would yield additional benefits to our approach. Furthermore, existing research (Ni et al., 2022) has demonstrated that increasing the model size can enhance the performance and generalisability of dense retrievers. Our analysis also shows that\nscaling up the model size improves reranking performance. Therefore, we would like to see whether applying the scaling effect to the retriever side can result in further improvement of the performance.\nMoreover, we mainly examined our method on the BEIR benchmark. Although BEIR covers various tasks and domains, there is still a gap to industrial scenarios regarding the diversity of the retrieval corpora, such as those involving web-scale documents. We plan to explore scaling up the corpus size in our future work. Additionally, BEIR is a monolingual corpus in English, and we are interested in validating the feasibility of our method in multi-lingual settings."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the anonymous reviewers for their helpful feedback and suggestions. The first author is supported by the Graduate Research Scholarships funded by the University of Melbourne. This work was funded by the Australian Research Council, Discovery grant DP230102775. This research was undertaken using the LIEF HPCGPGPU Facility hosted at the University of Melbourne, which was established with the assistance of LIEF Grant LE170100200."
        },
        {
            "heading": "A Experimental Settings",
            "text": "A.1 Implementation Details\nFor training queries, we divide the passages in the corpus of each task in BEIR into individual sentences, treating them as cropping-sentence queries. In datasets with a large corpus size (e.g., ClimateFEVER), we randomly select 2 million sentences for training. However, for datasets with fewer passages (~5k-50k), we use all of them for training. For each query, we follow Chen et al. (2022) to extract its positive and negative passages from the top-k predictions of a specific retriever, where the top-10 are considered as positives while passages ranked between 46 and 50 are regarded as negatives. For the reranking process in line 12 of Algorithm 1, we always rerank the top 100 retrievals returned by a specific retriever, from which refined labels are extracted using the above rules.\nWe use Contriever4 to initialise the dense retriever and train it for 3 epochs on 8 A100 GPUs, with a per-GPU batch size 128 and learning rate 3\u00d7 10\u22125. Each query is paired with one positive and one negative passage together with in-batch negatives for efficient training. A single retriever ABEL is trained on the union of queries from all tasks on BEIR. We initialise the reranker from t5-base-lm-adapt (Raffel et al., 2020) checkpoint.5 For each query, we sample one positive and 7 negative passages. Similarly, we train a single reranker ABEL-Rerank using a batch size 64 and learning rate 3\u00d7 10\u22125 for 20k steps, with roughly 1.2k steps on each task. We conduct the iterative alternating distillation for three iterations and take ABEL in iteration 2 and ABEL-Rerank in iteration 3 for evaluation and result comparison. We set the maximum query and passage lengths to 128 and 256 for training and set both input lengths to 512 for evaluation.\nWe further fine-tune ABEL on MS-MARCO (Bajaj et al., 2016) using in-batch negatives for 10k steps, with a batch size 1024 and learning rate 1\u00d710\u22125. The maximum query and passage lengths for training are set to 32 and 128, respectively. Following Izacard et al. (2022), we first train an initial model with each query paired with one gold positive and a randomly-sampled negative. We then mine hard negatives with this model and retrain a second model ABEL-FT in the same manner but\n4https://huggingface.co/facebook/contriever 5https://huggingface.co/google/\nt5-base-lm-adapt\nwith a hard negative 10% of the time.\nA.2 Baseline Retrievers\nWe compare our method with a wide range of unsupervised and supervised models. Unsupervised models include: (1) BM25 (Robertson and Zaragoza, 2009); (2) Contriever (Izacard et al., 2022) that is pre-trained on unlabelled text with contrastive learning; (3) SimCSE6 (Gao et al., 2021) that uses contrastive learning to learn unsupervised sentence representations by taking the encodings of a single sentence with different dropout masks as positive pairs; (4) REALM7 (Guu et al., 2020) that trains unsupervised dense retrievers using the masked language modelling signals from a separate reader component. Supervised models include (1) ANCE (Xiong et al., 2021) trained on MS-MARCO with self-mined dynamic hard negatives; (2) Contriever (MS) and COCO-DR (Yu et al., 2022) that are first pretrained on unlabelled corpus with contrastive learning and then finetuned on MS-MARCO; (3) RetroMAE (Xiao et al., 2022) uses masked auto-encoding for model pretraining and MS-MARCO for fine-tuning; (4) GTRXXL (Ni et al., 2022), ColBERTv2 (Santhanam et al., 2022) and SPLADE++ (Formal et al., 2022) that use significantly larger model size, multivector and sparse representations, along with distillation from cross-encoder on MS-MARCO; (5) DRAGON+ (Lin et al., 2023) that learns progressively from diverse supervisions provided by above models on MS-MARCO with both crop-sentence and synthetic queries; (6) QGen: GPL (Wang et al., 2022) and PTR (Dai et al., 2023) that create customised models for each target task by using synthetic queries and pseudo relevance labels.\n6https://huggingface.co/princeton-nlp/ unsup-simcse-roberta-large\n7https://huggingface.co/google/ realm-cc-news-pretrained-embedder"
        },
        {
            "heading": "B Effects of Noise Injection and Soft Labels",
            "text": "We find that injecting noise into the inputs for model training leads to better performance. Specifically, we corrupt the query and passage texts by sequentially applying random shuffling, deletion, and masking on 10% of the words. Table 4 shows the results of injecting noise into the inputs during the training of the dense retriever and the reranker, along with the use of soft labels for reranker training. We observe that noise injection results in positive effects on both retriever and reranekr training. Furthermore, incorporating soft labels in reranker training leads to additional benefits."
        },
        {
            "heading": "C Comparison of Unsupervised Pre-training Methods",
            "text": "We also compare ABEL with a range of unsupervised domain-adaption methods that employ pretraining on the target corpus, SimCSE, ICT (Lee et al., 2019), MLM (Devlin et al., 2019), TSDAE (Wang et al., 2021), Condenser (Gao and Callan, 2021) and COCO-DR. These methods follow a two-stage training paradigm, which first pretrains a dense retriever on the target corpus (i.e., BEIR) and then fine-tunes the model using MSMARCO training data. As shown in Table 6, our ABEL model, which solely uses unlabelled text corpus for unsupervised training, already outperforms 5 out of 6 models without requiring any supervised fine-tuning. Building on ABEL, ABEL-FT achieves the best results among all models that adopt the two-stage training paradigm. These results clearly demonstrate the superiority of our method compared to existing ones that rely on the target corpus for unsupervised pre-training."
        },
        {
            "heading": "D Reranking Performance",
            "text": "We show the reranking results of ABEL-Rerank on different subsets of BEIR, comparing them with various state-of-the-art rerankers. Table 5 demonstrates that ABEL-Rerank achieves comparable or superior performance than both supervised and unsupervised rerankers, using similar model sizes across all subsets. In addition, Table 7 presents the detailed results of ABEL-Rerank (110M) for different iterations. The reranker models trained with\nmore than one iteration (i.e., t = 2, 3) improve the performance significantly. ABEL-Rerank (t = 2) performs the best on more tasks (10/18), while ABEL-Rerank (t = 3) achieves a marginally higher overall score."
        },
        {
            "heading": "E Cross-Task Evaluation",
            "text": "To validate the generalisation ability of ABEL, we conduct evaluation on a wide range of datasets (Table 8) without any further training, including AmbigQA (Min et al., 2020), WikiQA (Yang et al., 2015), GooAQ-Technical (Khashabi et al., 2021), LinkSO-Python (Liu et al., 2018), CodeSearchNetPython (Husain et al., 2019), and five opendomain question answering datasets, namely Natural Questions (NQ) (Kwiatkowski et al., 2019),\nTriviaQA (Joshi et al., 2017), WebQuestions (WebQ) (Berant et al., 2013) , SQuAD (Rajpurkar et al., 2016) and EntityQuestions (EQ) (Sciavolino et al., 2021). Since these datasets were unseen when training ABEL, their data (i.e., the text corpus) will only be accessed during the testing phase. Consequently, we consider this as a way to evaluate the capacity of ABEL to be generalised to unseen tasks and domains.\nE.1 Cross-Task Reranking Results We compare our ABEL-Rerank model with various supervised rerankers on five datasets that were not encountered during training. The results, as shown in Table 9, indicate that ABEL-Rerank achieves the highest performance and outperforms supervised rerankers of comparable sizes (i.e., MonoT5 and TART). This finding provides compelling evidence that the reranker component involved in our approach demonstrates the ability to generalise to unfamiliar domains and tasks, rather than simply relying on memorising the corpus of each task in the BEIR benchmark to achieve promising results.\nE.2 Cross-Dataset Results on Open-Domain Question Answering Datasets\nWe further evaluate ABEL on five open-domain question-answering datasets that were not encountered during training to test its cross-dataset generalisation ability. We compare ABEL with a wide\nrange of unsupervised retrievers, including BM25, REALM, SimCSE, ICT, MSS (Sachan et al., 2021), Spider (Ram et al., 2022) and Contriever. We use Top-k (k = 20, 100) as the main metrics for evaluation according to Karpukhin et al. (2020). As shown in Table 10, ABEL significantly outperforms other unsupervised retrievers that have been sophisticatedly pre-trained, with an average accuracy improvement of +8.4% for Top-20 and +4.0% for Top-100 over Contriever. Note that ABEL exhibits promising results and surpasses BM25 on SQuAD, a dataset with high lexical overlaps between questions and answer paragraphs, and EQ, a dataset consisting of entity-centric queries. This further confirms that ABEL effectively retains and enhances the lexical-matching ability acquired through learning from BM25 and this strength generalises well to unseen datasets."
        },
        {
            "heading": "F Combination with Supervised Models",
            "text": "Table 11 shows the results in each individual task when combining ABEL with supervised dense retrievers. The findings indicate that on both types of tasks (i.e., query-search and semantic-relatedness), the ensembling with ABEL leads to performance improvements on all supervised retrievers, with the gains being generally more significant on semanticrelatedness tasks. This demonstrates the complementarity between ABEL and existing supervised dense retrievers, which are commonly trained using labelled data in the form of natural questions."
        },
        {
            "heading": "G Effects of Boostrapping",
            "text": "Figure 6 shows the detailed results of each individual task throughout the iterative alternating distillation process. We observe that ABEL-1 outperforms ABEL-0 on almost all datasets, and the performance consistently improves from ABEL-1 to ABEL-2. For\nfiqa\nscifact\narguana\nclimate-fever\ndbpedia-entity\ncqadupstack\nABEL-3, it achieves improvement on 9 datasets but the results degrade on 6 datasets, with the overall performance merely competitive to ABEL-2."
        },
        {
            "heading": "H The Effects of Synthetic Queries",
            "text": "Figure 7 shows that synthetic queries demonstrate greater efficacy on tasks involving natural questions (e.g., FiQA), while cropping-sentence queries are more effective on semantic-relatedness tasks with short sentence queries (e.g., Climate-FEVER). Furthermore, when considering query-search tasks whose domains are substantially distinct from MSMARCO, where the query generator is fine-tuned, employing synthetic queries leads to significantly worse results, such as touch\u00e9-2020 and BioASQ.\nfiqa\nscifact\narguana\nclimate-fever\ndbpedia-entity\ncqadupstack\nABEL synthetic-queries\nMotivation Practical Cognitive Intrinsic Fairness\n\u25a1\nGeneralisation type Compositional Structural Cross Task Cross Language Cross Domain Robustness\n\u25a1 \u25a1\nShift type Covariate Label Full Assumed\n\u25a1\nShift source Naturally occuring Partitioned natural Generated shift Fully generated\n\u25a1\nShift locus Train\u2013test Finetune train\u2013test Pretrain\u2013train Pretrain\u2013test\n\u25a1"
        }
    ],
    "title": "Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval",
    "year": 2023
}