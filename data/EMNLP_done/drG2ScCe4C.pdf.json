{
    "abstractText": "Engaging with discussion of TV shows online often requires individuals to refrain from consuming show-related content for extended periods to avoid spoilers. While existing research on spoiler detection shows promising results in safeguarding viewers from general spoilers, it fails to address the issue of users abstaining from show-related content during their watch. This is primarily because the definition of a spoiler varies depending on the viewer\u2019s progress in the show, and conventional spoiler detection methods lack the granularity to capture this complexity. To tackle this challenge, we propose the task of spoiler matching, which involves assigning an episode number to a spoiler given a specific TV show. We frame this task as semantic text matching and introduce a dataset comprised of comments and episode summaries to evaluate model performance. Given the length of each example, our dataset can also serve as a benchmark for longrange language models.1 2",
    "authors": [
        {
            "affiliations": [],
            "name": "Ryan Tran"
        },
        {
            "affiliations": [],
            "name": "Canwen Xu"
        },
        {
            "affiliations": [],
            "name": "Julian McAuley"
        }
    ],
    "id": "SP:f8109724c7b25639984a976e2b5351530d671bc0",
    "references": [
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv preprint arXiv:2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Jordan L. Boyd-Graber",
                "Kimberly Glasgow",
                "Jackie Sauter Zajac."
            ],
            "title": "Spoiler alert: Machine learning approaches to detect social media posts with revelatory information",
            "venue": "ASIST, volume 50 of Proc. Assoc. Inf. Sci. Technol., pages 1\u20139. Wiley.",
            "year": 2013
        },
        {
            "authors": [
                "Buru Chang",
                "Inggeol Lee",
                "Hyunjae Kim",
                "Jaewoo Kang."
            ],
            "title": "killing me\" is not a spoiler: Spoiler detection model using graph neural networks with dependency relation-aware attention mechanism",
            "venue": "EACL, pages 3613\u20133617. Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Nick Craswell."
            ],
            "title": "Mean reciprocal rank",
            "venue": "Encyclopedia of Database Systems, page 1703. Springer US.",
            "year": 2009
        },
        {
            "authors": [
                "Zhuyun Dai",
                "Jamie Callan."
            ],
            "title": "Deeper text understanding for IR with contextual neural language modeling",
            "venue": "SIGIR, pages 985\u2013988. ACM.",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen."
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "ACL/IJCNLP (1), pages 3816\u20133830. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin K Johnson",
                "Judith E Rosenbaum."
            ],
            "title": "Spoiler alert: Consequences of narrative spoilers for dimensions of enjoyment, appreciation, and transportation",
            "venue": "Communication Research, 42(8):1068\u2013 1088.",
            "year": 2015
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Nandan Thakur",
                "Nils Reimers",
                "Andreas R\u00fcckl\u00e9",
                "Abhishek Srivastava",
                "Iryna Gurevych."
            ],
            "title": "Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models",
            "venue": "arXiv preprint arXiv:2104.08663.",
            "year": 2021
        },
        {
            "authors": [
                "Mengting Wan",
                "Rishabh Misra",
                "Ndapa Nakashole",
                "Julian J. McAuley."
            ],
            "title": "Fine-grained spoiler detection from large-scale review corpora",
            "venue": "ACL (1), pages 2605\u20132610. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "EMNLP (Demos), pages 38\u201345. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Anna Wr\u00f3blewska",
                "Pawe\u0142 Rzepi\u0144ski",
                "Sylwia SyskoRoma\u0144czuk"
            ],
            "title": "Spoiler in a textstack: How much can transformers help? arXiv preprint arXiv:2112.12913",
            "year": 2021
        },
        {
            "authors": [
                "Yunyang Xiong",
                "Zhanpeng Zeng",
                "Rudrasis Chakraborty",
                "Mingxing Tan",
                "Glenn Fung",
                "Yin Li",
                "Vikas Singh."
            ],
            "title": "Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention",
            "venue": "AAAI, pages 14138\u201314148. AAAI Press.",
            "year": 2021
        },
        {
            "authors": [
                "Manzil Zaheer",
                "Guru Guruganesh",
                "Kumar Avinava Dubey",
                "Joshua Ainslie",
                "Chris Alberti",
                "Santiago Onta\u00f1\u00f3n",
                "Philip Pham",
                "Anirudh Ravula",
                "Qifan Wang",
                "Li Yang",
                "Amr Ahmed."
            ],
            "title": "Big bird: Transformers for longer sequences",
            "venue": "NeurIPS.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Many online social platforms (e.g., Reddit, Discord) provide opportunities for fans to discuss a particular TV show and share their thoughts about details in episodes. However, engaging in such discussions comes with the risk of spoilers, which may lead to unsatisfactory viewing experiences (Johnson and Rosenbaum, 2015). As a result, many viewers avoid these communities altogether until they have caught up with the latest episode of the show.\nAs an attempt to resolve this problem, some platforms (such as Reddit) have built-in functionality allowing users to tag their content as containing\n\u2217Equal contribution. 1Code and model weights are publicly available at https: //github.com/bobotran/spoiler-matching 2The data is available at https://huggingface.co/ datasets/bobotran/spoiler-matching\nOnline community comments\na spoiler. However, for long shows with a large number of episodes, this proves unsatisfactory: A user could be halfway through a show but will still be afraid to click on spoiler-tagged content for fear that it might contain spoilers for events later in the show when in reality, it might pertain to events that the viewer has already seen and with which they can engage. Again, some websites allow users to tag spoilers with more granularity, but it is far from guaranteed that users will be both accurate and consistent in tagging their content. This highlights the need for automatic spoiler matching. Unlike traditional spoiler detection (Boyd-Graber et al., 2013; Wan et al., 2019; Chang et al., 2021; Wr\u00f3blewska et al., 2021), which determines whether a comment is a spoiler, spoiler matching aims to match a given spoiler to an episode number. A spoiler matching model working hand-in-hand with a spoiler detection model could provide much more fine-grained protection from spoilers.\nIn this work, we consider the setting where the show is known, and we would like to determine the episode to which a comment is referring, as shown in Figure 1. Specifically, we pose the problem as a semantic text matching (Cer et al., 2018) task between comments and episode summaries. We obtain a high-quality evaluation dataset by manually\nlabeling the posts with the corresponding episode. To obtain a larger training set, we use prompt learning to efficiently fine-tune an auto-labeler model to automatically label another 200k examples, in addition to 5.9k manually labeled examples. The task is to match the comments to the correct episodes, with pairs of {episode summary, comment} as the input. As the number of episodes in a show is often limited, this task form is practically feasible. Also, as the median length of the concatenated summary and comment is 1,538 tokens, it can be a good dataset for benchmarking long-range Transformer models (Beltagy et al., 2020; Zaheer et al., 2020; Xiong et al., 2021)."
        },
        {
            "heading": "2 Spoiler Matching Dataset",
            "text": "Data Collection We crawl 522,991 comments across 13 TV shows from discussion threads on Reddit. We also crawl 496 episode summaries from their respective episode pages on the website Fandom.3 Reddit threads have a hierarchical discussion format where users reply to each other\u2019s posts, and each reply increases the indentation level. Toplevel comments are comments that are not made in reply to any other comment. Threads focused around discussion of a particular episode are handpicked by the human annotators, and the top-level comments are scraped. We take only top-level comments to minimize collecting comments that are incomplete thoughts continued from another part of a conversation. We then remove links and markup elements and apply other cleaning preprocessing before proceeding.\nManual Labeling After data cleaning, we group the comments by show name and episode number\n3https://fandom.com\nbased on the discussion thread from which they are scraped, but we do not yet consider them labeled at this step. This is because we find that about half of the comments scraped this way are irrelevant. We take this time to note the subtle difference in labels at this step compared to traditional spoiler detection.\nWhile traditional spoiler detection classifies text as spoiler or non-spoiler, we at this step look to separate the irrelevant comments we have scraped from the relevant ones. We define a relevant comment as one that describes events from the episode (discussion thread) from which it is scraped. Examples of irrelevant comments are discussions about music, acting quality, personal feelings about the episode, etc. Table 1 shows some examples. While the first irrelevant comment is straightforward, the second is a more nuanced example: The episode in question is about dodgeball, but the comment does not discuss any events that occurred in the episode, so it is not considered relevant. Finally, we manually label 11,032 comments, reserved for the validation and test set.\nAutomatic Labeling To obtain a larger-scale training set, we split the 11,032 manually labeled examples into a small training, validation, and test set by the ratio of 7:2:1, to train an auto-\nlabeler. To maximize data efficiency with this small dataset, we perform prompt-based fine-tuning to train RoBERTa (Liu et al., 2019). Specifically, we use LM-BFF (Gao et al., 2021) to fine-tune the model to predict if the comment is relevant.\nWe evaluate LM-BFF under two settings. For the first, we perform prompt-based fine-tuning on a set of hand-crafted templates, then select the best one according to validation set AUC. For the second, we use the automatic template search (auto-T) to generate and rank 40 templates based on validation AUC. Under both settings, we do not perform the auto-L search and instead fix the label words as \u201crelevant\u201d and \u201cirrelevant\u201d. From this ranking, the top 20 templates are selected, and the logits from the corresponding models are averaged. Table 2 shows the evaluation of various auto-labelers. We find LM-BFF with template ensembling to be effective for our use case. Critically, a higher recall score means more irrelevant comments caught and filtered, resulting in a cleaner downstream training set for matching.\nIt is important to note that at this step, summaries are not concatenated with comments before being fed to the auto-labeler; the auto-labeler predicts relevant/irrelevant based on the words in the comment alone. This is because the job of the auto-labeler is to filter out generic irrelevant comments; unlike matching, episode-specific context is not required to perform this task.\nUsing our LM-BFF Ensemble auto-labeler, the 511,959 unlabeled comments are auto-labeled, separating 204,475 relevant comments from 307,484 irrelevant ones. Using our test set, we estimate that about 12% of the relevant comments are actually irrelevant. As we will demonstrate later, this number is low enough that the auto-labeled comments still serve as an effective training set for fine-tuning a spoiler matching model.\nDataset Construction To recap, we have 496 episode summaries, 511,959 auto-labeled comments and 11,032 hand-labeled comments. Among the auto-labeled comments, we have 204,475 relevant comments and among the hand-labeled we have 5,892. Relevant comments are converted to the spoiler matching dataset format by assigning them the episode number of the discussion thread from which they were scraped. To test the ability of the matching models to generalize to unseen shows, the test set is constructed such that it contains 3,105 hand-labeled comments from 4 shows that are nei-\nther in the validation set nor the training set. The remaining 2,787 hand-labeled comments are used for validation. The statistics of the resulting dataset are shown in Table 3."
        },
        {
            "heading": "3 Experiments",
            "text": "Task Formulation Our task is to match the comment to a certain episode in the show. Since there are only a limited number of episodes in a show, we iterate through all episode summaries of the show and concatenate each episode summary with the comment with a special token (e.g., [SEP]) inserted. After inference for each summary-comment pair, we rank the episodes by the predicted matching scores. We use Mean Reciprocal Rank (MRR, Craswell, 2009) as the metric for evaluation.\nBaselines We select 6 baselines, including BM25 as a lexical matching baseline, RoBERTa (Liu et al., 2019) as a pretrained language model baseline, and three long-range pretrained Transformers: BigBird (Zaheer et al., 2020), LongFormer (Beltagy et al., 2020), and Nystromformer (Xiong et al., 2021). We truncate the summary if the input exceeds the maximum tokens allowed for each model. For RoBERTa, we also experiment with the MaxP passage aggregation strategy (Dai and Callan, 2019). All models are base size. We implement the training and evaluation pipeline with Hugging Face Transformers (Wolf et al., 2020). The models are fine-tuned with the AdamW optimizer,\na batch size of 32 and learning rate of 2e-5.\nSettings We experiment with two settings to verify the effectiveness of the auto-labeling. In the human labels-only setting, we re-split the validation set to a small training set of 2,229 comments and a validation set of 558 comments. The second setting uses 204,475 automatically labeled training examples and 2,787 manually labeled examples for validation. The two settings share the same test set, which contains 3,105 manually labeled examples from 4 unseen shows.\nExperimental Results Experimental results are shown in Table 4. In the human labels only setting, BM25 outperforms language models except MaxPRoBERTa and Longformer. This finding is consistent with prior studies on text retrieval (Thakur et al., 2021) that BM25 can be a strong baseline when the training examples are insufficient. Compared with the human labels only setting, autolabeling successfully improves model performance, verifying the effectiveness of our auto-labeling pipeline. Also, we observe that RoBERTa suffers from a short context length and underperforms BM25 under both settings. This is especially evident considering the large improvement resulting from the adoption of the passage aggregation strategy. Indeed, MaxP-RoBERTa is competitive with the long-range models. Among the long-range models, Longformer (Beltagy et al., 2020) achieves the best performance and outperforms the other models by a large margin. We hope our dataset can also serve as a benchmark for long-range pretrained language models, as the task requires interactions with long sentences."
        },
        {
            "heading": "4 Case Study",
            "text": "In this section, we analyze comments from two shows in the validation set, Dr. Stone and Spy \u00d7\nFamily to understand behaviors of the models.4\nTable 5 lists several challenging comments along with Longformer\u2019s output. The first example refers to a scene where Santa very briefly flies across the sky in the background. It is treated as unimportant, fantastical garnish on an event from the episode: None of the characters acknowledge it, so it is understood to have not actually occurred. Thus, the episode 21 summary does not mention it at all. This represents a class of comments that references relevant but obscure events, which only a recent viewer of the episode might remember. Interestingly, the score is relatively low, suggesting that the model understands that it is outputting a low-quality prediction.\nThe second comment describes an event from the show but references a character from an entirely different show; it is drawing a comparison between two characters, one of them in the show, based on physical likeness. The reference is fairly well-known within the community, but without additional external information, it would be difficult for the model to understand this comment beyond just context clues.\nThe third comment is challenging because it concerns predictions. Comments on ongoing shows often contain predictions, and if they happen to be correct, will likely match better lexically to the future episode when the events occur than the current episode when they are foreshadowed/predicted. This is not necessarily a bad thing for the end user, but it poses a challenge for training and evaluating our models. For this example, it is visually hinted in episode 11 that the dog has the ability to see the future but not confirmed until episode 13, so the summary for episode 11 does not mention this ability explicitly but the summary for episode 13 does, posing a possible explanation for the model\u2019s behavior.\n4This section contains spoilers from these two shows.\nTaken together, these examples give a glimpse into the challenges posed by spoiler matching. The hope is that this analysis motivates new lines of work into the study."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we define a new task of spoiler matching that formulates spoiler detection as a semantic text matching task and construct a large scale dataset with 223k comments and 496 episode summaries by mixing human and automatic labeling. We benchmark the performance of BM25 and four language models on the proposed dataset.\nLimitations\nOne limitation of our work is that due to resource restrictions, we only benchmark four language models on our dataset, leaving many other longrange language models untested. Also, our dataset only covers 13 shows and the comments annotated by human annotators are relatively limited."
        },
        {
            "heading": "Acknowledgement",
            "text": "The authors would like to thank Ching Tan and Minh Nguyen for their help with annotating the validation set."
        }
    ],
    "title": "Spoiler Detection as Semantic Text Matching",
    "year": 2023
}