{
    "abstractText": "Keyphrase generation (KG) aims to generate a set of summarizing words or phrases given a source document, while keyphrase extraction (KE) aims to identify them from the text. Because the search space is much smaller in KE, it is often combined with KG to predict keyphrases that may or may not exist in the corresponding document. However, current unified approaches adopt sequence labeling and maximization-based generation that primarily operate at a token level, falling short in observing and scoring keyphrases as a whole. In this work, we propose SIMCKP, a simple contrastive learning framework that consists of two stages: 1) An extractor-generator that extracts keyphrases by learning context-aware phraselevel representations in a contrastive manner while also generating keyphrases that do not appear in the document; 2) A reranker that adapts scores for each generated phrase by likewise aligning their representations with the corresponding document. Experimental results on multiple benchmark datasets demonstrate the effectiveness of our proposed approach, which outperforms the state-of-the-art models by a significant margin.",
    "authors": [
        {
            "affiliations": [],
            "name": "Minseok Choi"
        },
        {
            "affiliations": [],
            "name": "Chaeheon Gwak"
        },
        {
            "affiliations": [],
            "name": "Seho Kim"
        },
        {
            "affiliations": [],
            "name": "Si Hyeong Kim"
        },
        {
            "affiliations": [],
            "name": "Jaegul Choo"
        }
    ],
    "id": "SP:cc46abed2af125dc4c4826f5309ac9b44197a888",
    "references": [
        {
            "authors": [
                "Wasi Ahmad",
                "Xiao Bai",
                "Soomin Lee",
                "Kai-Wei Chang."
            ],
            "title": "Select, extract and generate: Neural keyphrase generation with layer-wise coverage attention",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Rabah Alzaidy",
                "Cornelia Caragea",
                "C. Lee Giles."
            ],
            "title": "Bi-lstm-crf sequence labeling for keyphrase extraction from scholarly documents",
            "venue": "The World Wide Web Conference, WWW \u201919, page 2551\u20132557, New York, NY, USA. Association for Computing",
            "year": 2019
        },
        {
            "authors": [
                "Kamil Bennani-Smires",
                "Claudiu Musat",
                "Andreea Hossmann",
                "Michael Baeriswyl",
                "Martin Jaggi."
            ],
            "title": "Simple unsupervised keyphrase extraction using sentence embeddings",
            "venue": "Proceedings of the 22nd Conference on Computational Natural Language Learn-",
            "year": 2018
        },
        {
            "authors": [
                "Adrien Bougouin",
                "Florian Boudin",
                "B\u00e9atrice Daille."
            ],
            "title": "TopicRank: Graph-based topic ranking for keyphrase extraction",
            "venue": "Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 543\u2013551, Nagoya, Japan. Asian",
            "year": 2013
        },
        {
            "authors": [
                "Cornelia Caragea",
                "Florin Adrian Bulgarov",
                "Andreea Godea",
                "Sujatha Das Gollapalli."
            ],
            "title": "Citationenhanced keyphrase extraction from research papers: A supervised approach",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural",
            "year": 2014
        },
        {
            "authors": [
                "Hou Pong Chan",
                "Wang Chen",
                "Lu Wang",
                "Irwin King."
            ],
            "title": "Neural keyphrase generation via reinforcement learning with adaptive rewards",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2163\u20132174, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "2020a. A simple framework",
            "year": 2020
        },
        {
            "authors": [
                "Wang Chen",
                "Hou Pong Chan",
                "Piji Li",
                "Lidong Bing",
                "Irwin King."
            ],
            "title": "An integrated approach for keyphrase generation via exploring the power of retrieval and extraction",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the",
            "year": 2019
        },
        {
            "authors": [
                "Wang Chen",
                "Hou Pong Chan",
                "Piji Li",
                "Irwin King."
            ],
            "title": "Exclusive hierarchical decoding for deep keyphrase generation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1095\u20131105, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Wang Chen",
                "Yifan Gao",
                "Jiani Zhang",
                "Irwin King",
                "Michael R Lyu."
            ],
            "title": "Title-guided encoding for keyphrase generation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6268\u20136275.",
            "year": 2019
        },
        {
            "authors": [
                "S. Chopra",
                "R. Hadsell",
                "Y. LeCun."
            ],
            "title": "Learning a similarity metric discriminatively, with application to face verification",
            "venue": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905), volume 1, pages 539\u2013546 vol. 1.",
            "year": 2005
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Gillick",
                "Sayali Kulkarni",
                "Larry Lansing",
                "Alessandro Presta",
                "Jason Baldridge",
                "Eugene Ie",
                "Diego Garcia-Olano."
            ],
            "title": "Learning dense representations for entity retrieval",
            "venue": "Proceedings of the 23rd Conference on Computational Natural Language Learn-",
            "year": 2019
        },
        {
            "authors": [
                "Sujatha Das Gollapalli",
                "Xiao-Li Li",
                "Peng Yang."
            ],
            "title": "Incorporating expert knowledge into keyphrase extraction",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 31.",
            "year": 2017
        },
        {
            "authors": [
                "R. Hadsell",
                "S. Chopra",
                "Y. LeCun."
            ],
            "title": "Dimensionality reduction by learning an invariant mapping",
            "venue": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906), volume 2, pages 1735\u20131742.",
            "year": 2006
        },
        {
            "authors": [
                "Anette Hulth."
            ],
            "title": "Improved automatic keyword extraction given more linguistic knowledge",
            "venue": "Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 216\u2013223.",
            "year": 2003
        },
        {
            "authors": [
                "Su Nam Kim",
                "Olena Medelyan",
                "Min-Yen Kan",
                "Timothy Baldwin."
            ],
            "title": "SemEval-2010 task 5 : Automatic keyphrase extraction from scientific articles",
            "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation, pages 21\u201326, Uppsala, Sweden.",
            "year": 2010
        },
        {
            "authors": [
                "Youngsam Kim",
                "Munhyong Kim",
                "Andrew Cattle",
                "Julia Otmakhova",
                "Suzi Park",
                "Hyopil Shin."
            ],
            "title": "Applying graph-based keyword extraction to document retrieval",
            "venue": "Proceedings of the Sixth International Joint Conference on Natural Language Processing,",
            "year": 2013
        },
        {
            "authors": [
                "Mikalai Krapivin",
                "Aliaksandr Autaeu",
                "Maurizio Marchese"
            ],
            "title": "Large dataset for keyphrase extraction",
            "year": 2009
        },
        {
            "authors": [
                "Kalpesh Krishna",
                "Yapei Chang",
                "John Wieting",
                "Mohit Iyyer."
            ],
            "title": "RankGen: Improving text generation with large ranking models",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 199\u2013232, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Mayank Kulkarni",
                "Debanjan Mahata",
                "Ravneet Arora",
                "Rajarshi Bhowmik."
            ],
            "title": "Learning rich representation of keyphrases from text",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 891\u2013906, Seattle, United States. Associ-",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Xinnian Liang",
                "Shuangzhi Wu",
                "Mu Li",
                "Zhoujun Li."
            ],
            "title": "Unsupervised keyphrase extraction by jointly modeling local and global context",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 155\u2013164, Online",
            "year": 2021
        },
        {
            "authors": [
                "Rui Liu",
                "Zheng Lin",
                "Weiping Wang"
            ],
            "title": "Addressing extraction and generation",
            "year": 2021
        },
        {
            "authors": [
                "Yixin Liu",
                "Pengfei Liu."
            ],
            "title": "SimCLS: A simple framework for contrastive learning of abstractive summarization",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Zhiyuan Liu",
                "Xinxiong Chen",
                "Yabin Zheng",
                "Maosong Sun."
            ],
            "title": "Automatic keyphrase extraction by bridging vocabulary gap",
            "venue": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 135\u2013144, Portland, Oregon,",
            "year": 2011
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Yi Luan",
                "Mari Ostendorf",
                "Hannaneh Hajishirzi."
            ],
            "title": "Scientific information extraction with semisupervised neural tagging",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2641\u20132651, Copen-",
            "year": 2017
        },
        {
            "authors": [
                "Olena Medelyan",
                "Eibe Frank",
                "Ian H. Witten."
            ],
            "title": "Human-competitive tagging using automatic keyphrase extraction",
            "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1318\u20131327, Singapore. As-",
            "year": 2009
        },
        {
            "authors": [
                "Olena Medelyan",
                "Ian H Witten",
                "David Milne."
            ],
            "title": "Topic indexing with wikipedia",
            "venue": "Proceedings of the AAAI WikiAI workshop, volume 1, pages 19\u201324.",
            "year": 2008
        },
        {
            "authors": [
                "Rui Meng",
                "Xingdi Yuan",
                "Tong Wang",
                "Sanqiang Zhao",
                "Adam Trischler",
                "Daqing He."
            ],
            "title": "An empirical study on neural keyphrase generation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Rui Meng",
                "Sanqiang Zhao",
                "Shuguang Han",
                "Daqing He",
                "Peter Brusilovsky",
                "Yu Chi."
            ],
            "title": "Deep keyphrase generation",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 582\u2013592, Vancouver,",
            "year": 2017
        },
        {
            "authors": [
                "Rada Mihalcea",
                "Paul Tarau."
            ],
            "title": "TextRank: Bringing order into text",
            "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404\u2013411, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Thuy Dung Nguyen",
                "Min-Yen Kan."
            ],
            "title": "Keyphrase extraction in scientific publications",
            "venue": "Asian Digital Libraries. Looking Back 10 Years and Forging New Frontiers, pages 317\u2013326, Berlin, Heidelberg. Springer Berlin Heidelberg.",
            "year": 2007
        },
        {
            "authors": [
                "Ramakanth Pasunuru",
                "Mohit Bansal."
            ],
            "title": "Multireward reinforced summarization with saliency and entailment",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2018
        },
        {
            "authors": [
                "Jishnu Ray Chowdhury",
                "Seo Yeon Park",
                "Tuhin Kundu",
                "Cornelia Caragea."
            ],
            "title": "KPDROP: Improving absent keyphrase generation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4853\u20134870, Abu Dhabi, United Arab Emirates.",
            "year": 2022
        },
        {
            "authors": [
                "Mingyang Song",
                "Liping Jing",
                "Lin Xiao."
            ],
            "title": "Importance Estimation from Multiple Perspectives for Keyphrase Extraction",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2726\u20132736, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Mingyang Song",
                "Lin Xiao",
                "Liping Jing."
            ],
            "title": "Learning to extract from multiple perspectives for neural keyphrase extraction",
            "venue": "Computer Speech & Language, 81:101502.",
            "year": 2023
        },
        {
            "authors": [
                "Yixuan Su",
                "Tian Lan",
                "Yan Wang",
                "Dani Yogatama",
                "Lingpeng Kong",
                "Nigel Collier."
            ],
            "title": "A contrastive framework for neural text generation",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Si Sun",
                "Zhenghao Liu",
                "Chenyan Xiong",
                "Zhiyuan Liu",
                "Jie Bao."
            ],
            "title": "Capturing global informativeness in open domain keyphrase extraction",
            "venue": "Natural Language Processing and Chinese Computing: 10th CCF International Conference, NLPCC 2021, Qing-",
            "year": 2021
        },
        {
            "authors": [
                "Yi Sun",
                "Hangping Qiu",
                "Yu Zheng",
                "Zhongwei Wang",
                "Chaoran Zhang."
            ],
            "title": "Sifrank: a new baseline for unsupervised keyphrase extraction based on pre-trained language model",
            "venue": "IEEE Access, 8:10896\u201310906.",
            "year": 2020
        },
        {
            "authors": [
                "Avinash Swaminathan",
                "Haimin Zhang",
                "Debanjan Mahata",
                "Rakesh Gosangi",
                "Rajiv Ratn Shah",
                "Amanda Stent."
            ],
            "title": "A preliminary exploration of GANs for keyphrase generation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Yaohua Tang",
                "Fandong Meng",
                "Zhengdong Lu",
                "Hang Li",
                "Philip L.H. Yu."
            ],
            "title": "Neural machine translation with external phrase memory",
            "venue": "CoRR, abs/1606.01792.",
            "year": 2016
        },
        {
            "authors": [
                "Santosh Tokala",
                "Debarshi Kumar Sanyal",
                "Plaban Kumar Bhowmick",
                "Partha Pratim Das."
            ],
            "title": "SaSAKE: Syntax and semantics aware keyphrase extraction from research papers",
            "venue": "Proceedings of the 28th International Conference on Computational Linguis-",
            "year": 2020
        },
        {
            "authors": [
                "Laurens van der Maaten",
                "Geoffrey Hinton."
            ],
            "title": "Visualizing data using t-SNE",
            "venue": "Journal of Machine Learning Research, 9:2579\u20132605.",
            "year": 2008
        },
        {
            "authors": [
                "Xiaojun Wan",
                "Jianguo Xiao."
            ],
            "title": "Single document keyphrase extraction using neighborhood knowledge",
            "venue": "AAAI, volume 8, pages 855\u2013860.",
            "year": 2008
        },
        {
            "authors": [
                "Minmei Wang",
                "Bo Zhao",
                "Yihua Huang."
            ],
            "title": "Ptr: Phrase-based topical ranking for automatic keyphrase extraction in scientific publications",
            "venue": "Neural Information Processing: 23rd International Conference, ICONIP 2016, Kyoto, Japan, October 16\u201321, 2016,",
            "year": 2016
        },
        {
            "authors": [
                "Ian H Witten",
                "Gordon W Paynter",
                "Eibe Frank",
                "Carl Gutwin",
                "Craig G Nevill-Manning."
            ],
            "title": "Kea: practical automatic keyphrase extraction",
            "venue": "Proceedings of the fourth ACM conference on Digital libraries, pages 254\u2013255.",
            "year": 1999
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Di Wu",
                "Wasi Ahmad",
                "Sunipa Dev",
                "Kai-Wei Chang."
            ],
            "title": "Representation learning for resourceconstrained keyphrase generation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 700\u2013716, Abu Dhabi, United Arab Emi-",
            "year": 2022
        },
        {
            "authors": [
                "Huanqin Wu",
                "Wei Liu",
                "Lei Li",
                "Dan Nie",
                "Tao Chen",
                "Feng Zhang",
                "Di Wang."
            ],
            "title": "UniKeyphrase: A unified extraction and generation framework for keyphrase prediction",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 825\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Huanqin Wu",
                "Baijiaxin Ma",
                "Wei Liu",
                "Tao Chen",
                "Dan Nie."
            ],
            "title": "Fast and constrained absent keyphrase generation by prompt-based learning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 11495\u201311503.",
            "year": 2022
        },
        {
            "authors": [
                "Binbin Xie",
                "Xiangpeng Wei",
                "Baosong Yang",
                "Huan Lin",
                "Jun Xie",
                "Xiaoli Wang",
                "Min Zhang",
                "Jinsong Su."
            ],
            "title": "WR-One2Set: Towards well-calibrated keyphrase generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-",
            "year": 2022
        },
        {
            "authors": [
                "Hai Ye",
                "Lu Wang."
            ],
            "title": "Semi-supervised learning for neural keyphrase generation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4142\u20134153, Brussels, Belgium. Association for Computational",
            "year": 2018
        },
        {
            "authors": [
                "Jiacheng Ye",
                "Ruijian Cai",
                "Tao Gui",
                "Qi Zhang."
            ],
            "title": "Heterogeneous graph neural networks for keyphrase generation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2705\u20132715, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Jiacheng Ye",
                "Tao Gui",
                "Yichao Luo",
                "Yige Xu",
                "Qi Zhang."
            ],
            "title": "One2Set: Generating diverse keyphrases as a set",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Xingdi Yuan",
                "Tong Wang",
                "Rui Meng",
                "Khushboo Thaker",
                "Peter Brusilovsky",
                "Daqing He",
                "Adam Trischler."
            ],
            "title": "One size does not fit all: Generating and evaluating variable number of keyphrases",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "Linhan Zhang",
                "Qian Chen",
                "Wen Wang",
                "Chong Deng",
                "ShiLiang Zhang",
                "Bing Li",
                "Wei Wang",
                "Xin Cao."
            ],
            "title": "MDERank: A masked document embedding rank approach for unsupervised keyphrase extraction",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Qi Zhang",
                "Yang Wang",
                "Yeyun Gong",
                "Xuanjing Huang."
            ],
            "title": "Keyphrase extraction using deep recurrent neural networks on Twitter",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 836\u2013845, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Guangzhen Zhao",
                "Guoshun Yin",
                "Peng Yang",
                "Yu Yao."
            ],
            "title": "Keyphrase generation via soft and hard semantic corrections",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7757\u20137768, Abu Dhabi, United Arab",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Keyphrase prediction (KP) is a task of identifying a set of relevant words or phrases that capture the main ideas or topics discussed in a given document. Prior studies have defined keyphrases that appear in the document as present keyphrases and the opposites as absent keyphrases. High-quality keyphrases are beneficial for various applications such as information retrieval (Kim et al., 2013), text summarization (Pasunuru and Bansal, 2018), and translation (Tang et al., 2016). KP methods are generally divided into keyphrase extraction (KE) (Witten\n*Work done during an internship at Naver Webtoon. 1Our code is publicly available at https://github.\ncom/brightjade/SimCKP.\net al., 1999; Hulth, 2003; Nguyen and Kan, 2007; Medelyan et al., 2009; Caragea et al., 2014; Zhang et al., 2016; Alzaidy et al., 2019) and keyphrase generation (KG) models (Meng et al., 2017; Ye and Wang, 2018; Chan et al., 2019; Chen et al., 2020b; Yuan et al., 2020; Ye et al., 2021b; Zhao et al., 2022), where the former only extracts present keyphrases from the text and the latter generates both present and absent keyphrases.\nRecently, several methods integrating KE and KG have been proposed (Chen et al., 2019a; Liu et al., 2021; Ahmad et al., 2021; Wu et al., 2021, 2022b). These models predict present keyphrases using an extractor and absent keyphrases using a generator, thereby effectively exploiting a relatively small search space in extraction. However, current integrated models suffer from two limitations. First, they employ sequence labeling models that predict the probability of each token being a constituent of a present keyphrase, where such token-level predictions may be a problem when the target keyphrase is fairly long or overlapping. As shown in Figure 1, the sequence labeling model makes an incomplete prediction for the term \u201cmulti-\nply connected problem\u201d because only the tokens for \u201cmultiply connected\u201d have yielded a high probability. We also observe that the model is prone to miss the keyphrase \u201cintegral equations\u201d every time because it overlaps with another keyphrase \u201cboundary integral equation\u201d in the text. Secondly, integrated or even purely generative models are usually based on maximum likelihood estimation (MLE), which predicts the probability of each token given the past seen tokens. This approach scores the most probable text sequence the highest, but as pointed out by Zhao et al. (2022), keyphrases from the maximumprobability sequence are not necessarily aligned with target keyphrases. In Figure 1, the MLE-based model predicts \u201cmagnetostatic energy analysis\u201d, which is semantically similar to but not aligned with the target keyphrase \u201cnonlinear magnetostatic analysis\u201d. This may be a consequence of greedy search, which can be remedied by finding the target keyphrases across many beams during beam search, but it would also create a large number of noisy keyphrases being generated in the top-k predictions.\nExisting KE approaches based on representation learning may address the above limitations (Bennani-Smires et al., 2018; Sun et al., 2020; Liang et al., 2021; Zhang et al., 2022; Sun et al., 2021; Song et al., 2021, 2023). These methods first mine candidates that are likely to be keyphrases in the document and then rank them based on the relevance between the document and keyphrase embeddings, which have shown promising results. Nevertheless, these techniques only tackle present keyphrases from the text, which may mitigate the overlapping keyphrase problem from sequence labeling, but they are not suitable for handling MLE and the generated keyphrases.\nIn this work, we propose a two-stage contrastive learning framework that leverages context-aware phrase-level representations on both extraction and generation. First, we train an encoder-decoder network that extracts present keyphrases on top of the encoder and generates absent keyphrases through the decoder. The model learns to extract present keyphrases by maximizing the agreement between the document and present keyphrase representations. Specifically, we consider the document and its corresponding present keyphrases as positive pairs and the rest of the candidate phrases as negative pairs. Note that these negative candidate phrases are mined from the document using\na heuristic algorithm (see Section 4.1). The model pulls keyphrase embeddings to the document embedding and pushes away the rest of the candidates in a contrastive manner. Then during inference, top-k keyphrases that are semantically close to the document are predicted. After the model has finished training, it generates candidates for absent keyphrases. These candidates are simply constructed by overgenerating with a large beam size for beam search decoding. To reduce the noise introduced by beam search, we train a reranker that allocates new scores for the generated phrases via another round of contrastive learning, where this time the agreement between the document and absent keyphrase representations is maximized. Overall, major contributions of our work can be summarized as follows:\n\u2022 We present a contrastive learning framework that learns to extract and generate keyphrases by building context-aware phrase-level representations.\n\u2022 We develop a reranker based on the semantic alignment with the document to improve the absent keyphrase prediction performance.\n\u2022 To the best of our knowledge, we introduce contrastive learning to a unified keyphrase extraction and generation task for the first time and empirically show its effectiveness across multiple KP benchmarks."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Keyphrase Extraction",
            "text": "Keyphrase extraction focuses on predicting salient phrases that are present in the source document. Existing approaches can be broadly divided into two-step extraction methods and sequence labeling models. Two-step methods first determine a set of candidate phrases from the text using different heuristic rules (Hulth, 2003; Medelyan et al., 2008; Liu et al., 2011; Wang et al., 2016). These candidate phrases are then sorted and ranked by either supervised algorithms (Witten et al., 1999; Hulth, 2003; Nguyen and Kan, 2007; Medelyan et al., 2009) or unsupervised learning (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Bougouin et al., 2013; Bennani-Smires et al., 2018). Another line of work is sequence labeling, where a model learns to predict the likelihood of each word being a keyphrase word (Zhang et al., 2016; Luan et al., 2017; Gollapalli et al., 2017; Alzaidy et al., 2019)."
        },
        {
            "heading": "2.2 Keyphrase Generation",
            "text": "The task of keyphrase generation is introduced to predict both present and absent keyphrases. Meng et al. (2017) first proposed CopyRNN, a seq2seq framework with attention and copy mechanism under the ONE2ONE paradigm, where a model is trained to generate a single keyphrase per document. However, due to the problem of having to fix the number of predictions, Yuan et al. (2020) proposed the ONE2SEQ paradigm where a model learns to predict a dynamic number of keyphrases by concatenating them into a single sequence. Several ONE2SEQ-based models have been proposed using semi-supervised learning (Ye and Wang, 2018), reinforcement learning (Chan et al., 2019), adversarial training (Swaminathan et al., 2020), hierarchical decoding (Chen et al., 2020b), graphs (Ye et al., 2021a), dropout (Ray Chowdhury et al., 2022), and pretraining (Kulkarni et al., 2022; Wu et al., 2022a) to improve keyphrase generation.\nFurthermore, there have been several attempts to unify KE and KG tasks into a single learning framework. These methods not only focus on generating only the absent keyphrases but also perform presumably an easier task by extracting present keyphrases from the document, instead of having to generate them from a myriad of vocabularies. Current methodologies utilize external source (Chen et al., 2019a), selection guidance (Zhao et al., 2021), salient sentence detection (Ahmad et al., 2021), relation network (Wu et al., 2021), and prompt-based learning (Wu et al., 2022b)."
        },
        {
            "heading": "2.3 Contrastive Learning",
            "text": "Methods to extract rich feature representations based on contrastive learning (Chopra et al., 2005; Hadsell et al., 2006) have been widely studied in numerous literature. The primary goal of the learn-\ning process is to pull semantically similar data to be close while pushing dissimilar data to be far away in the representation space. Contrastive learning has shown great success for various computer vision tasks, especially in self-supervised training (Chen et al., 2020a), whereas Gao et al. (2021) have devised a contrastive framework to learn universal sentence embeddings for natural language processing. Furthermore, Liu and Liu (2021) formulated a seq2seq framework employing contrastive learning for abstractive summarization. Similarly, a contrastive framework for autoregressive language modeling (Su et al., 2022) and open-ended text generation (Krishna et al., 2022) have been presented.\nThere have been endeavors to incorporate contrastive learning in the context of keyphrase extraction. These methods generally utilized the pairwise ranking loss to rank phrases with respect to the document to extract present keyphrases (Sun et al., 2021; Song et al., 2021, 2023). In this paper, we devise a contrastive learning framework for keyphrase embeddings on both extraction and generation to improve the keyphrase prediction performance."
        },
        {
            "heading": "3 Problem Definition",
            "text": "Given a document x, the task of keyphrase prediction is to identify a set of keyphrases Y = {yi}i=1,...,\u2223Y\u2223, where \u2223Y\u2223 is the number of keyphrases. In the ONE2ONE training paradigm, each sample pair (x, Y) is split into multiple pairs {(x,yi)}i=1,...,\u2223Y\u2223 to train the model to generate one keyphrase per document. In ONE2SEQ, each sample pair is processed as (x, f(Y)), where f(Y) is a concatenated sequence of keyphrases. In this work, we train for extraction and generation simultaneously; therefore, we decompose Y into a present keyphrase set Yp = {yip}i=1,...,\u2223Yp\u2223 and an absent keyphrase set Ya = {yia}i=1,...,\u2223Ya\u2223."
        },
        {
            "heading": "4 SIMCKP",
            "text": "In this section, we elaborate on our approach to building a contrastive framework for keyphrase prediction. In Section 4.1, we delineate our heuristic algorithm for constructing a set of candidates for present keyphrase extraction; in Section 4.2, we describe the multi-task learning process for extracting and generating keyphrases; and lastly, we explain our method for reranking the generated keyphrases in Section 4.3. Figure 2 illustrates the overall architecture of our framework."
        },
        {
            "heading": "4.1 Hard Negative Phrase Mining",
            "text": "To obtain the candidates for present keyphrases, we employ a similar heuristic approach from existing extractive methods (Hulth, 2003; Mihalcea and Tarau, 2004; Wan and Xiao, 2008; BennaniSmires et al., 2018). A notable difference between prior work and ours is that we keep not only noun phrases but also verb, adjective, and adverb phrases, as well as phrases containing prepositions and conjunctions. We observe that keyphrases are actually made up of diverse parts of speech, and extracting only the noun phrases could lead to missing a significant number of keyphrases. Following the common practice, we assign partof-speech (POS) tags to each word using the Stanford POSTagger2 and chunk the phrase structure tree into valid phrases using the NLTK RegexpParser3.\nAs shown in Algorithm 1, each document is converted to a phrase structure tree where each word w is tagged with a POS tag t. The tagged document is then split into possible phrase chunks based on our predefined regular expression rules, which must include one or more valid tags such as nouns, verbs, adjectives, etc. Nevertheless, such valid tag sequences are sometimes nongrammatical, which cannot be a proper phrase and thus may introduce noise during training. In response, we filter out such nongrammatical phrases by first categorizing tags as independent or dependent. Phrases generally do not start or end with a preposition or conjunction; therefore, preposition and conjunction tags belong to a dependent tag set Tdep. On the other hand, noun, verb, adjective, and adverb tags can stand alone by themselves, making them belong to an independent tag set Tindep. There are also\n2https://stanfordnlp.github.io/ CoreNLP/\n3https://www.nltk.org/\nAlgorithm 1 Hard Negative Phrase Mining Input: Source document x, maximum n-gram length n, regular expression pattern p, POS tagging function tag(\u22c5), phrase parsing function parse(\u22c5), stemming function stem(\u22c5) Output: Present keyphrase candidate set Cpre 1: Cpre \u2190 \u2205 2: word_tag_pairs \u2190 tag(x) 3: phrase_tree \u2190 parse(p,word_tag_pairs) 4: for phrase \u2208 phrase_tree do 5: for wi, ti \u2208 phrase do 6: if ti \u2209 Tindep and ti \u2209 Tend_dep then 7: continue 8: spani \u2190 stem(wi) 9: Cpre \u2190 Cpre \u222a spani 10: for wij , tij \u2208 phrase[i + 1:] do 11: if len(spani.split()) \u2265 n then 12: break 13: if tij \u2208 Tdep or tij \u2208 Tend_dep then 14: spani += stem(wij) 15: continue 16: if tij \u2208 Tindep or tij \u2208 Tstart_dep then 17: spani += stem(wij) 18: Cpre \u2190 Cpre \u222a spani 19: return Cpre\ntag sets Tstart_dep and Tend_dep, which include tags that cannot start but end a phrase and tags that can start but not end a phrase, respectively. Lastly, each candidate phrase is iterated over to acquire all ngrams that make up the phrase. For example, if the phrase is \u201capplications of machine learning\u201d, we select n-grams \u201capplications\u201d, \u201cmachine\u201d, \u201clearning\u201d, \u201capplications of machine\u201d, \u201cmachine learning\u201d, and \u201capplications of machine learning\u201d as candidates. Note that phrases such as \u201capplications of\u201d, \u201cof\u201d, \u201cof machine\u201d, and \u201cof machine learning\u201d are not chosen as candidates because they are not proper phrases. As noted by Gillick et al. (2019), hard negatives are important for learning a high-quality encoder, and we claim that our mining accomplishes this objective."
        },
        {
            "heading": "4.2 Extractor-Generator",
            "text": "In order to jointly train for extraction and generation, we adopt a pretrained encoder-decoder network. Given a document x, it is tokenized and fed as input to the encoder where we take the last hidden states of the encoder to obtain the contextual embeddings of a document:\n[h0,h1, ...,hT ] = Encoderp(x), (1) where T is the token sequence length of the document and h0 is the start token (e.g., <s>) representation used as the corresponding document embedding. For each candidate phrase, we construct its embedding by taking the sum pooling of the token span representations: hp =\nSumPooling([hs, ...,he]), where s and e denote the start and end indices of the span. The document and candidate phrase embeddings are then passed through a linear layer followed by nonlinear activation to obtain the hidden representations:\nzdp = tanh(Wdph0 + bdp) zp = tanh(Wphp + bp),\n(2)\nwhere Wdp , Wp, bdp , bp are learnable parameters.\nContrastive Learning for Extraction To extract relevant keyphrases given a document, we train our model to learn representations by pulling keyphrase embeddings to the corresponding document while pushing away the rest of the candidate phrase embeddings in the latent space. Specifically, we follow the contrastive framework in Chen et al. (2020a) and take a cross-entropy objective between the document and each candidate phrase embedding. We set keyphrases and their corresponding document as positive pairs, while the rest of the phrases and the document are set as negative pairs. The training objective for a positive pair (zdp , z + p,i) (i.e., document and present keyphrase yip) with Np candidate pairs is defined as\nLiCL = \u2212 log e sim(zdp ,z\n+ p,i)/\u03c4\ne sim(zdp ,z + p,i)/\u03c4+\u2211Npj=1 e sim(zdp ,z \u2212 p,j )/\u03c4\n,\n(3) where \u03c4 is a temperature hyperparameter and sim(u,v) is the cosine similarity between vectors u and v. The final loss is then computed across all positive pairs for the corresponding document (i.e., LCL = \u2211\u2223Yp\u2223i=1 LiCL). Joint Learning Our model generates keyphrases by learning a probability distribution p\u03b8(ya) over an absent keyphrase text sequence ya = {ya,1, ..., ya,\u2223y\u2223} (i.e., in an ONE2SEQ fashion), where \u03b8 denotes the model parameters. Then, the MLE objective used to train the model to generate absent keyphrases is defined as\nLMLE = \u2212 1\n\u2223ya\u2223 \u2223ya\u2223 \u2211 t=1 log p\u03b8(ya,t\u2223ya,<t). (4)\nLastly, we combine the contrastive loss with the negative log-likelihood loss to train the model to both extract and generate keyphrases:\nL = LMLE + \u03bbLCL, (5)\nwhere \u03bb is a hyperparameter balancing the losses in the objective."
        },
        {
            "heading": "4.3 Reranker",
            "text": "As stated by Zhao et al. (2022), MLE-driven models predict candidates with the highest probability, disregarding the possibility that target keyphrases may appear in suboptimal candidates. This problem can be resolved by setting a large beam size for beam search; however, this approach would also result in a substantial increase in the generation of noisy keyphrases among the top-k predictions. Inspired by Liu and Liu (2021), we aim to reduce this noise by assigning new scores to the generated keyphrases.\nCandidate Generation We employ the finetuned model from Section 4.2 to generate candidate phrases that are highly likely to be absent keyphrases for the corresponding document. We perform beam search decoding using a large beam size on each training document, resulting in the overgeneration of absent keyphrase candidates. The model generates in an ONE2SEQ fashion where the outputs are sequences of phrases, which means that many duplicate phrases are present across the beams. We remove the duplicates and arrange the phrases such that each unique phrase is independently fed to the encoder. We realize that the generator sometimes fails to produce even a single target keyphrase, in which we filter out such documents for the second-stage training.\nDual Encoder We adopt two pretrained encoderonly networks and obtain the contextual embeddings of a document, as well as each candidate phrase c: [h0d,h1d, ...,hTd ] = Encodera1(x) and [h0c ,h1c , ...,hTcc ] = Encodera2(c), where Tc is the token sequence length of the candidate phrase and h 0 d and h 0 c are the start token representations used as the document and candidate phrase embedding, respectively. Consequently, their hidden representations are obtained by zda = tanh(Wdah 0 d + bda)\nand za = tanh(Wah0c + ba), where Wda , Wa, bda , ba are learnable parameters.\nContrastive Learning for Generation To rank relevant keyphrases high given a document, we train the dual-encoder framework via contrastive learning. Following a similar process as before, we train our model to learn absent keyphrase representations by semantically aligning them with the corresponding document. Specifically, we set the correctly generated keyphrases and their corresponding document as positive pairs, whereas the rest of the generated candidates and the document become negative pairs. The training objective for a positive pair (zda , z + a,i) (i.e., document and absent keyphrase yia) with Na candidate pairs then follows Equation 3, where the cross-entropy objective maximizes the similarity of positive pairs and minimizes the rest. The final loss is computed across all positive pairs for the corresponding document with a summation."
        },
        {
            "heading": "5 Experimental Setup",
            "text": ""
        },
        {
            "heading": "5.1 Datasets",
            "text": "We evaluate our framework on five scientific article datasets: Inspec (Hulth, 2003), Krapivin (Krapivin et al., 2009), NUS (Nguyen and Kan, 2007), SemEval (Kim et al., 2010), and KP20k (Meng et al., 2017). Following previous work (Meng et al., 2017; Chan et al., 2019; Yuan et al., 2020), we concatenate the title and abstract of each sample as a source document and use the training set of KP20k to train all the models. Data statistics are shown in Table 1."
        },
        {
            "heading": "5.2 Baselines",
            "text": "We compare our framework with two kinds of KP models: Generative and Unified.\nGenerative Models Generative models predict both present and absent keyphrases through generation. Most models follow catSeq (Yuan et al., 2020), a seq2seq framework under the ONE2SEQ paradigm. We report the performance of catSeq along with its variants such as catSeqTG (Chen et al., 2019b), catseqTG-2RF1 (Chan et al., 2019), and ExHiRD-h (Chen et al., 2020b). We also compare with two state-of-the-arts SetTrans (Ye et al., 2021b) and CorrKG (Zhao et al., 2022).\nUnified Models Unified models combine extractive and generative methods to predict keyphrases. We compare with the latest models including SEGNet (Ahmad et al., 2021), UniKeyphrase (Wu et al., 2021), and PromptKP (Wu et al., 2022b)."
        },
        {
            "heading": "5.3 Evaluation Metrics",
            "text": "Following Chan et al. (2019), all models are evaluated on macro-averaged F1@5 and F1@M . F1@M compares all the predicted keyphrases with the ground truth, taking the number of predictions into account. F1@5 measures only the top five predictions, but if the model predicts less than five keyphrases, we randomly append incorrect keyphrases until it obtains five. The motivation is to avoid F1@5 and F1@M reaching similar results when the number of predictions is less than five. We stem all phrases using the Porter Stemmer and remove all duplicates after stemming."
        },
        {
            "heading": "5.4 Implementation Details",
            "text": "Our framework is built on PyTorch and Huggingface\u2019s Transformers library (Wolf et al., 2020). We use BART (Lewis et al., 2020) for the encoderdecoder model and uncased BERT (Devlin et al., 2019) for the reranking model. We optimize their weights with AdamW (Loshchilov and Hutter,\n2019) and tune our hyperparameters to maximize F1@M\n4 on the validation set, incorporating techniques such as early stopping and linear warmup followed by linear decay to 0. We set the maximum n-gram length of candidate phrases to 6 during mining and fix \u03bb to 0.3 for scaling the contrastive loss. When generating candidates for absent keyphrases, we use beam search with the beam size 50. During inference, we take the candidate phrases as predictions in which the cosine similarity with the corresponding document is higher than the threshold found in the validation set. The threshold is calculated by taking the average of the F1@M - maximizing thresholds for each document. If the number of predictions is less than five, we retrieve the top similar phrases until we obtain five. We conduct our experiments with three different random seeds and report the averaged results."
        },
        {
            "heading": "6 Results and Analyses",
            "text": ""
        },
        {
            "heading": "6.1 Present and Absent Keyphrase Prediction",
            "text": "The present and absent keyphrase prediction results are demonstrated in Table 2 and Table 3, respectively. The performance of our model mostly exceeds that of previous state-of-the-art methods by a large margin, showing that our method is effective in predicting both present and absent keyphrases. Particularly, there is a notable improvement in the F1@5 performance, indicating the effectiveness of our approach in retrieving the top-k predictions. On the other hand, we observe that F1@M values are not much different from F1@5, and we believe this is due to the critical limitation of a global threshold. The number of keyphrases varies significantly for each document, and finding op-\n4We compared with F1@5 and found no difference in determining the best hyperparameter configuration.\ntimal thresholds seems necessary for improving the F1@M performance. Nonetheless, real-world applications are often focused on identifying the top-k keywords, which we believe our model effectively accomplishes."
        },
        {
            "heading": "6.2 Ablation Study",
            "text": "We investigate each component of our model to understand their effects on the overall performance and report the effectiveness of each building block in Table 4. Following Xie et al. (2022), we report on two kinds of test sets: 1) KP20k, which we refer to as in-domain, and 2) the combination of Inspec, Krapivin, NUS, and SemEval, which is outof-domain.\nEffect of CL We notice a significant drop in both present and absent keyphrase prediction performance after decoupling contrastive learning (CL). For a fair comparison, we set the beam size to 50, but our model still outperforms the purely generative model, demonstrating the effectiveness of CL. We also compare our model with two extractive methods: sequence labeling and binary classifica-\ntion. For sequence labeling, we follow previous work (Tokala et al., 2020; Liu et al., 2021) and employ a BiLSTM-CRF, a strong sequence labeling baseline, on top of the encoder to predict a BIO5 tag for each token, while for binary classification, a model takes each phrase embedding to predict whether each phrase is a keyphrase or not. CL outperforms both approaches, showing that learning phrase representations is more efficacious.\nEffect of Reranking We remove the reranker and observe the degradation of performance in absent keyphrase prediction. Note that the vanilla BART (i.e., w/o CL) is trained to generate both present and absent keyphrases, while the other model (i.e., w/o RERANKER) is trained to generate only the absent keyphrases. The former performs slightly better in out-of-domain scenarios, as it is trained to generate diverse keyphrases, while the latter excels in in-domain since absent keyphrases resemble those encountered during training. Nevertheless, the reranker outperforms the two, indicating that it plays a vital role in the KG part of our method."
        },
        {
            "heading": "6.3 Performance over Max N-Gram Length",
            "text": "We conduct experiments on various maximum lengths of n-grams for extraction and compare the\n5We use the BIO format for our sequence labeling baseline. For example, if the phrase \u201cvoip conferencing system\u201d is tokenized into \u201cv ##oi ##p con ##fer ##encing system\u201d, it is labeled as \u201cB I I I I I I\u201d.\npresent keyphrase prediction performance from unigrams to 6-grams, as shown in Figure 3. For all datasets, the performance steadily increases until the length of 3, which then plateaus to the rest of the lengths. This indicates that the testing datasets are mostly composed of unigrams, bigrams, and trigrams. The performance increases slightly with the length of 6 for some datasets, such as Inspec and SemEval, suggesting that there is a non-negligible number of 6-gram keyphrases. Therefore, the length of 6 seems feasible for maximum performance in all experiments."
        },
        {
            "heading": "6.4 Impact of Hard Negative Phrase Mining",
            "text": "In order to assess the effectiveness of our hard negative phrase mining method, we compare it with other negative mining methods and report the results in Table 5. First, utilizing in-batch document embeddings as negatives yields the poorest performance. This is likely due to ineffective differentiation between keyphrases and other phrase embeddings. Additionally, we experiment with using random text spans as negatives and observe that although it aids in representation learning to some degree, the performance improvement is limited. The outcomes of these two baselines demonstrate that our approach successfully mines hard negatives, enabling our encoder to acquire high-quality representations of keyphrases."
        },
        {
            "heading": "6.5 Visualization of Semantic Space",
            "text": "To verify that our model works as intended, we visualize the representation space of our model with t-SNE (van der Maaten and Hinton, 2008) plots, as depicted in Figure 4. From the visualizations, we find that our model successfully pulls keyphrase embeddings close to their corresponding document in both extractor and generator space. Note that the generator space displays a lesser number of phrases than the beam size 50 because the duplicates after stemming have been removed."
        },
        {
            "heading": "6.6 Upper Bound Performance",
            "text": "Following previous work (Meng et al., 2021; Ray Chowdhury et al., 2022), we measure the upper bound performance after overgeneration by calculating the recall score of the generated phrases and report the results in Table 6. The high recall demonstrates the potential for reranking to increase precision, and we observe that there is room for improvement by better reranking, opening up an opportunity for future research."
        },
        {
            "heading": "7 Conclusion",
            "text": "This paper presents a contrastive framework that aims to improve the keyphrase prediction performance by learning phrase-level representations, rectifying the shortcomings of existing unified models that score and predict keyphrases at a token level. To effectively identify keyphrases, we divide our framework into two stages: a joint model for extracting and generating keyphrases and a reranking model that scores the generated outputs based on the semantic relation with the corresponding document. We empirically show that our method significantly improves the performance of both present and absent keyphrase prediction against existing state-of-the-art models.\nLimitations\nDespite the promising prediction performance of the framework proposed in this paper, there is still room for improvement. A fixed global threshold has limited the potential performance of the framework, especially when evaluating F1@M . We expect that adaptively selecting a threshold value via an auxiliary module for each data sample might overcome such a challenge. Moreover, the result of the second stage highly depends on the performance of the first stage model, directing the next step of research towards an end-to-end framework."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by Institute for Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea\ngovernment (MSIT) (No. 2020-0-00368, A NeuralSymbolic Model for Knowledge Acquisition and Inference Techniques and No. 2021-0-02068, Artificial Intelligence Innovation Hub), and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2022R1A2B5B02001913). We thank all researchers at NAVER WEBTOON Ltd. for their valuable discussions."
        },
        {
            "heading": "A Additional Details for SIMCKP",
            "text": "A.1 Details for Hard Negative Phrase Mining In order to effectively extract present keyphrases, we categorize POS tags to the corresponding tag set as the following:\n\u2022 Tindep: {\u201cCD\u201d, \u201cFW\u201d, \u201cGW\u201d, \u201cNN*\u201d, \u201cVB*\u201d, \u201cJJ*\u201d, \u201cRB*\u201d, \u201cADD\u201d}\n\u2022 Tdep: {\u201cCC\u201d, \u201cPOS\u201d, \u201cHYPH\u201d, \u201cIN\u201d}\n\u2022 Tstart_dep: {\u201cRP\u201d}\n\u2022 Tend_dep: {\u201cDT\u201d, \u201cAFX\u201d, \u201cLS\u201d}\nwhere each tag is defined in the NLTK library. An asterisk (*) refers to any character(s) that come after the tag name. For example, \u201cJJ\u201d, \u201cJJR\u201d, and \u201cJJS\u201d are adjectives, comparative adjectives, and superlative adjectives, respectively, and all of them are elements of the independent tag set Tindep.\nA.2 Hyperparameter Search We perform a grid search to find the best hyperparameter configuration and report the tuning range used for our experiments in Table 7. The evaluation on the validation set is performed for every 5,000 gradient accumulating steps, and the tolerance increases by 1 when the validation loss or F1@M is worse than the previous evaluation."
        }
    ],
    "title": "SimCKP: Simple Contrastive Learning of Keyphrase Representations",
    "year": 2023
}