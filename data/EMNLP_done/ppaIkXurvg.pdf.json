{
    "abstractText": "The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a finegrained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation ( ), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based \u2217Corresponding author. \u2020 Work does not relate to position at Amazon. on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess and measure which LLM is more vulnerable towards hallucination. We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations. 1 Hallucination: The What and Why",
    "authors": [
        {
            "affiliations": [],
            "name": "Vipula Rawte"
        },
        {
            "affiliations": [],
            "name": "Swagata Chakraborty"
        },
        {
            "affiliations": [],
            "name": "Agnibh Pathak"
        },
        {
            "affiliations": [],
            "name": "Anubhav Sarkar"
        },
        {
            "affiliations": [],
            "name": "S.M Towhidul Islam Tonmoy"
        },
        {
            "affiliations": [],
            "name": "Aman Chadha"
        },
        {
            "affiliations": [],
            "name": "Amit Sheth"
        },
        {
            "affiliations": [],
            "name": "Amitava Das"
        }
    ],
    "id": "SP:cfe98f09657630a6316fa54583f388bfcfae7aa7",
    "references": [],
    "sections": [
        {
            "text": "The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a finegrained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation ( ), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based\n\u2217Corresponding author. \u2020 Work does not relate to position at Amazon.\non their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). Amidst the extensive deliberations on policy-making for regulating AI development, it is of utmost importance to assess and measure which LLM is more vulnerable towards hallucination. We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.\n1 Hallucination: The What and Why\nThe extraordinary benefits of large generative AI models such as GPT (Brown et al., 2020; OpenAI, 2023a), Stable Diffusion (Rombach et al., 2022), DALL-E (Ramesh et al., 2021, 2022), and Midjourney (Midjourney, 2022) also come with\na substantial risk of misuse. The alarm this has triggered is reflected in the open letter (Marcus and of Life Institute, 2023) in March 2023 by thousands of researchers and tech leaders calling for a six-month moratorium on training AI systems that are more sophisticated than GPT-4. The key underlying concern is \u201cshould we let machines flood our information channels with propaganda and untruth?\u201d. In fact, the majority of these falsehoods are widely recognized as hallucination, which can be defined as the generation of content that deviates from the real facts, resulting in unfaithful outputs (Maynez et al., 2020).\nTo address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office (Copyright-Office, 2023) released a statement stating that if the content is traditional elements of authorship produced by a machine, the work lacks human authorship and the office will not register it for copyright. OpenAI\u2019s response to the prevalent societal pressure led them to issue a public statement (OpenAI, 2023b) emphasizing their commitment to AI safety and their determination to implement improved controls on hallucination in future iterations of GPT. The recent roll-out of Google\u2019s highly anticipated ChatGPT rival, Bard, led to a fiasco owing to it hallucinating a factually inaccurate answer in the company\u2019s advertisement, which cost Google a $140 billion wipeout in terms of market value (Reuters, 2023). In the ad, Bard is prompted: What new discoveries from the James Webb Space Telescope (JWST)... Bard responds with a number of answers, including one suggesting the JWST was used to take the very first pictures of a planet outside the Earth\u2019s solar system.... The first pictures of exoplanets were, however, taken by the European Southern Observatory\u2019s VLT in 2004. In another incident, a lawyer used ChatGPT to help him prepare a filing in a lawsuit against a US airline. However, ChatGPT quoted a fabricated previous case precedent, which led the judge to consider imposing sanctions (Forbes, 2023). Amidst these happenings, NVIDIA introduced NeMo Guardrails (nVIDIA, 2023), an\nopen-source toolkit, based on the Self-Check GPT framework (Manakul et al., 2023), designed to address hallucinations in conversational AI systems.\nThe remarkable capabilities of generative AI have undeniably propelled it to a superpower status! Although the term hallucination has gained widespread acceptance in describing the irrational and uncontrolled behaviors of LLMs, it is important to note that many experts expressed dissatisfaction with this particular nomenclature. Within the AI community, efforts persist to find a more suitable alternative name to describe this phenomenon accurately. During an interview (You, 2023), Prof. Christopher Manning briefly expressed his discontent with the term \u201challucination\u201d, indicating a preference for an alternative term. In the ongoing conversation, Prof. Gary Marcus has advocated for a reframing of \u201challucination\u201d as confabulation, a term that some fellow researchers have already embraced. However, in this paper, we have decided to uphold the use of the term \u201challucination\u201d. In order to offer a comprehensive and precise description of the various types of hallucinations, we will introduce a few new terms. These newly introduced monikers aim to accurately capture and articulate the different categories of hallucinations.\nContrary to the common belief that hallucinations are negative, certain researchers (Cao et al., 2022) propose that hallucinations in LLMs could have positive implications for text summarization. The authors argue that in certain cases, factual hallucinations can be advantageous in a summary by offering valuable background information. Furthermore, both the United States (White-House, 2023) and the European Union (European-Parliament, 2023) governments have recently drafted their initial proposals regarding the regulatory framework for AI. With the widespread adoption of LLMs in a plethora of real-world use cases, it is essential to understand which LLM is more vulnerable than others in terms of hallucination \u2013 by doing so policymakers can decide the potential risks of certain LLMs. To this end, we introduce a quantifiable spectrum \u2013 Hallucination Vulnerability Index\n(HVI), which facilitates the evaluation and ranking of LLMs according to their hallucination vulnerability levels.\nOur Contributions: Deciphering the spectrum of hallucination over a range of LLMs based on HVI\n\u27a0 Presenting a detailed study to unveil how different (15) LLMs hallucinate when given a factually correct prompt vs. a factually incorrect prompt. We name them as factual mirage and silver lining \u2013 each sub-categorized into intrinsic and extrinsic, with three degrees of severity: (a) mild, (b) moderate, and (c) alarming (cf. Section 2).\n\u27a0 Meticulously categorizing hallucination into six types: (a) acronym ambiguity, (b) numeric nuisance, (c) generated golem, (d) virtual voice, (e) geographic erratum, and (f) time wrap (cf. Section 2).\n\u27a0 Introducing (HallucInation eLiciTation), a publicly available dataset comprising of 75,000 text snippets generated using 15 contemporary LLMs along with human annotations for the aforementioned categories (cf. Section 3).\n\u27a0 Introducing HVI (Hallucination Vulnerability Index) to perform a quantitative analysis of the inclination of various LLMs to hallucination. (cf. Section 4). HVI characterizes LLMs based on the proposed types of hallucination vulnerabilities (cf. Fig. 2).\n\u27a0 While complete mitigation can be a herculean task, we suggest 2 mitigation strategies to alleviate hallucination. We propose to identify high-entropy points in text generated by an LLM with a high HVI and replace them using an LLM with a lower HVI, yielding desired results (cf. Section 6).\n\u27a0 We firmly believe that the dataset and HVI measure will serve as valuable resources for future researchers interested in studying the hallucination behaviors of LLMs and seeking to design effective mitigation techniques. HVI will prove to be a useful tool for assessing the categorical impacts of these proposed mitigation techniques.\n2 A Holistic View of the Hallucination Spectrum: its Types and Scales\nThe issue of hallucination garnered research attention as early as (Maynez et al., 2020). However, with the growing size of LLMs (empirical evidence provided in Section 6), there is a corresponding increase in LLMs\u2019 susceptibility to hallucination. Consequently, there is a growing interest within\nthe research community to study and understand hallucination to design mitigation techniques.\nResearchers have loosely defined hallucinations and studied various notions of hallucinations in isolation. Early exploration of factual vs. nonfactual prompts for checking factuality of LMs is addressed in (Lee et al., 2022). A recent survey conducted by (Maynez et al., 2020), categorized hallucination into two limited classes: intrinsic and extrinsic. Another recent paper (Ladhak et al., 2023b), delved into an intriguing type of hallucination known as name-nationality category hallucination. Several other papers (Raunak et al., 2021; Maynez et al., 2020) have explored taskspecific categories of hallucination, such as summarization, question-answering, machine translation, etc. Preliminary exploration of factual versus non-factual prompts is also addressed in (Lee et al., 2022). However, we purposefully avoid confining our study to a specific task to study the nuances of hallucination. Our main contention is that hallucination can occur in any NLG task, necessitating a thorough examination based on the fundamental principles of text generation from a given prompt. The findings from this study can be applied and extended to various NLP tasks. Thus, this paper aims to offer a comprehensive categorization of hallucination, as outlined below (see Fig. 1). 2.1 Orientations of Hallucination We introduce two primary orientations of hallucination: (i) Factual Mirage (FM) and (ii) Silver Lining (SL), defined and exemplified below.\n2.1.1 Factual Mirage Factual mirage (FM) is defined as the phenomenon wherein an LLM engages in hallucination or distortion of a given prompt that is factually correct. FM can be subdivided into two distinct sub-categories.\nMILD Prompt: Capital of France\nAI-generated text: ...Paris is also the world fashion capital...\nFact: Paris.\nIntrinsic factual mirage (IFM): In the following example, the LLM is providing a correct\nresponse while adding additional supplementary facts such as \u201cthe world fashion capital,\u201d resulting in distortion or hallucination, has also been described in (Cao et al., 2022).\nExtrinsic factual mirage (EFM): EFM refers to the phenomenon where an LLM deviates from factual accuracy. For example:\nALARMING\nPrompt: Engineering effort to build Eiffel tower\nAI-generated text: ...Designed by Gustave Eiffel, it was inaugurated in 1889 to celebrate the 100th anniversary of the European Civil War...\nFact: Eiffel tower was built to celebrate the 100th anniversary of the French Revolution.\n2.1.2 Silver Lining (SL)\nSilver lining (SL) is defined as the phenomenon in which an LLM indulges in hallucination by conjuring an elaborate and captivating narrative based on a given prompt that is factually incorrect.\nIntrinsic silver lining (ISL): ISL is the category when in some cases LLM does not generate a convincing story. As an example:\nMODERATE\nPrompt: Cell phone numbers go public this month.\nAI-generated text: ...Starting this month, cell phone numbers will be made public as a result of the implementation of the Right to Information (RTI) policy...\nFact: This statement is false.\nExtrinsic silver lining (ESL): When an LLM generates a highly detailed and persuasive narrative in response to a factually incorrect prompt, it falls under the category of Extrinsic Silver Lining.\nALARMING\nPrompt: Kamala Harris and Elon Musk are getting married\nAI-generated text: Elon Musk got married to Kamala Harris in California on Thursday. The ceremony took place with 50 guests present, including Kamala\u2019s sister and brotherin-law. Apart from Joe Biden all were present from the White House. Kamala\u2019s ex-husband played the saxophone at the ceremony. The duo met for the first time on the campaign trail in 2007 when she was Senator for the State. Elon proposed to Kamala Harris in August 2019 on a beach. Kamala accepted and has been dating him ever since...\nFact: The incident never happened!\n2.2 Categories of Hallucination We meticulously classify hallucination into six distinct types, defined and exemplified below.\nNumeric Nuisance (NN): This issue occurs when an LLM generates numeric values related to past events, such as dates, ages, or monetary amounts, that are inconsistent with the actual facts.\nMODERATE\nPrompt: Twitter is said to have laid off more workers.\nAI-generated text: The layoffs come after Twitter announced earlier this month that it would be cutting its global workforce by 8% or roughly 600 people...\nFact: Twitter laid off 200 people which is roughly 10%\nAcronym Ambiguity (AA): This issue pertains to instances in which LLMs generate an imprecise expansion for an acronym.\nMODERATE\nPrompt: RLHF in reinforcement learning\nAI-generated text: ...RLHF is Reward-free Learning from Human Feedback in reinforcement learning...\nFact: RLHF stands for \u201cReinforcement Learning from Human Feedback\u201d\nGenerated Golem (GG): This issue arises when an LLM fabricates an imaginary personality in relation to a past event, without concrete evidence.\nMODERATE\nPrompt: Earthquake in Turkey\nAI-generated text: ...One rescuer, Hasan Cetin, said he was motivated by the thought of the survivors he helped save. \u201cThe moment when a man or a child is pulled out of the ruins, the joy of seeing them come back to life is...\nFact: There is no evidence that the person Hasan Cetin exists.\nVirtual Voice (VV): At times LLMs generate quotations attributed to either fictional or real characters without sufficient evidence to verify the authenticity of such statements.\nALARMING\nPrompt: Pfizer Press Release on COVID-19 vaccine\nAI-generated text: ...Pfizer emphasized that their vaccine demonstrated an impressive efficacy rate... Pfizer CEO said, \u201cThis is a giant leap for humanity..\u201d...\nFact: Pfizer CEO never said this.\nIn this instance, the expression giant leap for humanity is quoted from Neil Armstrong\u2019s renowned historical statement upon stepping onto the moon.\nGeographic Erratum (GE): This problem occurs when LLMs generate an incorrect location associated with an event. For example here in the first of the following two examples, Pentagon is stated to be situated in India.\nTime Wrap (TW): This problem entails LLMs generating text that exhibits a mashed fusion of events from different timelines. in the following case (2nd example) the U.S. president during the Ukraine-Russia war is Joe Biden, not Barack Obama, thus contradicting the factual reality.\nALARMING\nPrompt: Prime Minister of the United Kingdom visiting India\nAI-generated text: ...The visit of the United Kingdom\u2019s Prime Minister to India holds immense significance...During his journey, he will also pay a visit to the Pentagon where he is scheduled to meet with Smriti Irani, the Minister of...\nFact: Pentagon is the defense headquarters of the USA, located in Washington DC, USA \u2013 not in India.\nALARMING Prompt: USA on Ukraine war\nAI-generated text: ...U.S. President Barack Obama says the U.S. will not put troops in Ukraine...\nFact: The actual U.S. president during the Ukraine-Russia war is Joe Biden.\n2.3 Degrees of Hallucination\nWe annotate the degree of hallucination using three levels: mild, moderate, and alarming (labeled as 0, 1, and 2 respectively). Mild indicates minor hallucination which is superficial in terms of its impact. Moderate indicates a level of hallucination that introduces facts that are either fictitious or tangential to the topic at hand. Alarming indicates added information pieces that bear a radical dissemblance from the topic fed via the prompt. Please refer to Appendix B for more details.\n3 : HallucInation eLiciTation dataset HILT is a first-of-its-kind publicly available hallucination dataset. To construct this dataset, we have utilized two primary sources of data as prompts: (i) NYTimes tweets (NYT) (factually correct \u2013 FM) and (ii) the Politifact dataset (Politifact) (factually incorrect \u2013 SL). We selected 15 LLMs, based on\nthe criteria delineated in Section 3.1, and used them to generate a total of 75,000 text passages, with each LLM producing 5,000 text prose entries. These entries were categorized as 2,500 each for FM and SL. The text prompts provided to these LLMs consisted of tweets from NYTimes and headlines sourced from the Politifact dataset. Table 1 reports detailed statistics about .\n3.1 Choice of LLMs: Rationale and Coverage We chose 15 contemporary LLMs that have exhibited exceptional results on a wide range of NLP tasks, including: (i) GPT-4 (OpenAI, 2023a), (ii) GPT-3.5 (OpenAI, 2022), (iii) GPT-3 (Brown et al., 2020), (iv) GPT-2 (Radford et al., 2019), (v) MPT (Wang et al., 2023), (vi) OPT (Zhang et al., 2022), (vii) LLaMA (Touvron et al., 2023), (viii) BLOOM (Scao et al., 2022), (ix) Alpaca (Taori et al., 2023), (x) Vicuna (Chiang et al., 2023), (xi) Dolly (databricks, 2023), (xii) StableLM (AI, 2023), (xiii) XLNet (Yang et al., 2019), (xiv) T5 (Raffel et al., 2020), and (xv) T0 (Deleu et al., 2022). Appendix C.1 discusses additional details behind our selection criteria. Given the ever-evolving nature of the field, and HVI benchmark leaderboards will remain accessible to the research community, fostering an environment of continuous updates and contributions.\n3.2 Annotating Hallucination For the annotation task of the 75,000 text snippets, we utilized Amazon Mechanical Turk (Amazon). We obtain sentence-level annotations for hallucination orientations and categories. We record four annotations per sentence and adopt the MACE tool\n(Hovy et al., 2013) to assess inter-annotator agreement and aggregate data. MACE has been empirically demonstrated to outperform majority voting, exhibiting superior performance (cf. Appendix B).\n4 Hallucination Vulnerability Index (HVI) Given the growing usage of LLMs and their likeliness to hallucinate, there exists no uniform evaluation metric to measure these LLMs\u2019 hallucinations. To address this gap, we define HVI, a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations. HVI is calculated as in Eq. (1): HV Ix = 100U\u22172 [ \u2211Ux=1(N(x)\u2212N(EFM))\u2217 (1\u2212P(EFM)+\u03b41)+\n(N(x)\u2212N(ESL))\u2217 (1\u2212P(ESL)+\u03b42)] (1) When defining HVI, we take several factors into account. Firstly, not all sentences generated by an LLM are hallucinated, so it is important to determine the ratio of actual hallucinated sentences with the total number of sentences. In this context, we consider U as the total number of sentences and N(x) as the total number of hallucinated sentences produced by an LLM. Secondly, LLMs can exhibit different characteristics, such as higher EFM or ESL tendencies, or they can have varying levels of overall hallucination. This notion is captured by introducing the terms N(x)\u2212N(EFM) and N(x)\u2212N(ESL) in the equation. It is worth noting that we did not consider variations of intrinsic hallucinations in HVI calculation, as they are relatively minor and exhibit lower vulnerability overall. Lastly, comparative measures are needed to rank LLMs based on their vulnerability to hallucination. This is achieved using multiplicative damping factors, \u03b41 and \u03b42, which are calculated based on \u00b5 \u00b1 rankx \u00d7\u03c3 . Initially, we calculate the HVI for all 15 LLMs, considering \u03b41 and \u03b42 as zero. With these initial HVIs, we obtain the mean (\u00b5) and standard deviation (\u03c3 ), allowing us to recalculate the HVIs for all the LLMs. The resulting HVIs are then ranked and scaled providing a comparative spectrum as presented in Fig. 3, similar to z-score normalization (Wikipedia_zscore) and/or min-max normalization\n(Wikipedia_min_max). Having damping factors enables easy exponential smoothing with a handful of data points, 15 in this case. Finally, for ease of interpretability, HVI is scaled between 0\u2212100.\nLLM Size HVI (0-100)\nGPT-3 175B 90 - StableLM 7B 82 - GPT-2 1.5B 70 - Vicuna 13B 62 - MPT 7B 59 - LLaMA 65B 57 - GPT-3.5 175B 53 - Dolly 12B 49 - OPT 175B 48 - GPT-4 1.7T 47 - Alpaca 65B 40 - BLOOM 176B 38 - T0 11B 36 - XLNet 340M 36 - T5 11B 32 -\nhigher hallucination\nlower hallucination\nfragments, C - replaced text, D - highlighted text for no information found, and E - refuted text fragments by textual entailment. Appendix F contains more examples.\n5 HVI vs. LLMs size for different LLMs: An insight from\nThere is a general observation that LLMs may exhibit a higher tendency towards generating hallucinations or producing outputs that deviate from factual or coherent information. However, it is important to note that the relationship between LLM size and hallucination is not necessarily a direct correlation, but rather a consideration based on certain factors such as (a) training data quality, (b) lack of explicit training on facts, and (c) overconfi-\ndence in generated responses. A noteworthy pattern that emerges is that LLMs without RLHF (Reinforcement Learning from Human Feedback) (Ziegler et al., 2019) tend to exhibit a higher tendency for hallucination. Although we did not extensively evaluate this phenomenon, we have a keen interest in investigating it further in the near future. While we tried to examine the effect of size on HVI, it looks like there are several other factors contributing to HVI behavior as evident in Fig. 6.\n6 Hallucination Mitigation Strategies Thus far, two classes of approaches have been proposed to address the issue of hallucination: (i) preventing LLMs from hallucinating, which involves implementing strategies during the training and/or generation processes; (ii) mitigating hallucination after generation. (Manakul et al., 2023) introduced another taxonomy of classification, categorizing methods into black-box and gray-box. Factuality checks during and/or after generation without relying on external resources are known as black-box methods, while those using external resources are referred to as gray-box methods.\nOther hallucination mitigation techniques involve reranking the generated sample responses (Dale et al., 2022) and improving beam search (Sridhar and Visser, 2022). Some recent mitigation techniques (Li et al., 2023; M\u00fcndler et al., 2023; Pfeiffer et al., 2023; Chen et al., 2023; Zhang et al., 2023b,a; Ladhak et al., 2023a; Manakul et al., 2023; Agrawal et al., 2023) show initial attempts at reducing hallucination.\nAlthough the complete elimination of hallucination is a complex challenge, this paper explores two plausible directions for mitigation: (i) automatic and (ii) human-in-the-loop. The former is a black-box method where we identify high-entropy words in a given hallucinated text (generated by a high-HVI LLM) and replace them with predictions from another LLM (lower-HVI). The latter is a gray-box method that involves sentence-level factchecking using textual entailment techniques. This method aims to identify sentences that are deemed\nsusceptible, urging them for human review.\n6.1 High Entropy Word Spotting and Replacement (ENTROPYBB): A Black-box approach\nWhile detecting high entropy words may seem to be technically feasible, there is an inherent challenge that many modern LLMs are not open-source (their APIs are subscription-based). The feasible solution we propose here is the utilization of opensource LLMs to identify high entropy words. A lower HVI-based LLM is then used to replace the detected words (see Fig. 4). The outcomes of the detection and replacement strategies discussed earlier are presented in Table 2 for GPT-3. The results indicate that albert-large-v2 (Lan et al., 2020) performs exceptionally well in detecting high entropy words in GPT-3-generated content. On the other hand, distilroberta-base (Sanh et al., 2019) demonstrates superior performance in replacing high entropy words, which in turn, manifests as a lower hallucination. A crucial aspect of our approach is treating consecutive high-entropy words as a single unit. In such cases, these words are masked together before replacement. This strategy proves to be effective, particularly for hallucinations related to Generated Golem or Acronym Ambiguity (cf. Appendix F.1).\n6.1.1 Lowering Concreteness of Language It is observed in (Varshney et al., 2023) that higher uncertainty in the model\u2019s prediction (indicated by a low probability score) suggests a higher likelihood of the model hallucinating about that particular concept. In this context, we suggest that substituting high entropy points with less concrete\nwords can help prevent hallucinations. Concreteness (Paivio, 2013) measures how much a word embodies a tangible or perceivable concept. Concrete words are simpler to comprehend than abstract ones. The level of concreteness for each word is denoted on a 5-point scale, ranging from abstract to concrete. Concreteness ratings cover 39,954 entries, including 37,058 individual English words and 2,896 two-word expressions (Brysbaert et al., 2014), being used here.\n6.2 Factuality Check of Sentences (FACTUALITYGB): A Gray-box approach\nWe use Google Search API (Search) to search for a given prompt, which has been utilized to generate the text and retrieve the top 20 documents. Then each sentence of AI-generated text has been validated either into support, refute, or not enough information using RoBERTa Large (Liu et al., 2019), a SoTA textual entailment model trained on the SNLI (Bowman et al., 2015) (cf. Section 6.2.1). Inevitably, sentences with higher scores in the refute and not enough information categories are flagged for additional human checking. Empirically, we observe an overall alert rate of 26% on sentences generated by an LLM, implying 26% of the text required rewriting in order to mitigate.\n6.2.1 FACTUALITYGB Gray-box model does require output token-level probabilities (Manakul et al., 2023). Fig. 7 shows FACTUALITYGB, representing AI-generated text (from our HILT benchmark) based on a given prompt. In this method, the prompt is sent to the Google Search API to obtain the top 20 relevant search results. Out of these 20 results, we evaluate a total of n sentences for their relevance to the prompt using a similarity measure. The top 20 sentences most similar to the prompt are selected. For each of the m sentences in the AI-generated text and the top 20 ranked sentences, we employ a textual entailment model to assess their trustworthiness individually. Based on their entailment scores, we categorize the AI-generated text into\nthree groups: (i) support, (ii) refute, and (iii) not enough information.\nPerformance of ENTROPYBB vs. FACTUALITYGB: Fig. 5 offers a comparative analysis of the proposed approaches. While ENTROPYBB addresses simpler hallucinations such as Acronym Ambiguity and Numeric Nuisance, FACTUALITYGB handles more complex cases. It is clear that a balanced combination of black-box and gray-box approaches is the inevitable future avenue (cf. Appendix F.3).\n7 Conclusion and Future Avenues The enthusiasm and achievements surrounding LLMs have led to their widespread adoption, and this trend is only expected to flourish. However, one of the most significant challenges faced by LLMs today is hallucination. In light of this,\nbenchmark and Hallucination Vulnerability Index (HVI) will continue to serve the wider scientific community and aid policy-makers. benchmark and HVI will be publicly open for further collaborative updates. Two proposed mitigation techniques can serve as baselines.\n8 Discussion and Limitations\nDiscussion: On June 14th, 2023, the European Parliament successfully passed its version of the EU AI Act (European-Parliament, 2023). Subsequently, a team of researchers from the Stanford Institute for Human-Centered Artificial Intelligence (HAI) embarked on investigating the extent to which Foundation Model Providers comply with the EU AI Act. Their initial findings are presented in the publication (Bommasani et al., 2023). In this study, the authors put forward a grading system consisting of 12 aspects for evaluating LLMs. These aspects include (i) data sources, (ii) data governance, (iii) copyrighted data, (iv) compute, (v) energy, (vi) capabilities & limitations, (vii) risk & mitigations, (viii) evaluation, (ix) testing, (x) machine-generated content, (xi) member states, and (xii) downstream documentation. The overall grading of each LLM can be observed in Fig. 8. While this study is commendable, it appears to be inherently incomplete due to the ever-evolving nature of LLMs. Since all scores are assigned manually, any future changes will require a reassessment of this rubric, while HVI is auto-computable. Furthermore, we propose that HVI should be considered the most suitable category for assessing risk and mitigations, as well as the evaluation of machine-generated content.\nLimitations: In this paper, we present a unique and extensive benchmark corpus for hallucination called . We propose two main types of hallucination: (i) Factual Mirage and (ii) Silver Lining, each further divided into intrinsic and extrinsic subcategories. Additionally, we introduce six detailed categories of hallucination along with a measure of its intensity. We believe the following aspects require critical attention in future endeavors.\nLimitation 1: For the sake of simplicity, we have only considered one category per sentence during annotation, although we acknowledge the presence of multi-class and multi-label instances. For instance, in the following example, there are two kinds of hallucination, namely Time Wrap and\nNumeric Nuisance present in the shown sentence. We would like to explore this direction in the immediate future.\nALARMING\nPrompt: Engineering effort to build Eiffel tower\nAI-generated text: ...Designed by Gustave Eiffel, it was inaugurated in 1889 to celebrate the 100th anniversary of the European Civil War, while construction began a decade prior to its inauguration....\nFact 1: Eiffel tower was built to celebrate the 100th anniversary of the French Revolution. Fact 2: Eiffel Tower construction was started in 1887, not in 1879.\nLimitation 2: While we have meticulously defined the categories of hallucination, we recognize the potential for new categories to emerge in the future with the advancement of LLMs. An instance of this is the introduction of name-nationality hallucination by (Ladhak et al., 2023b), where a person named Jung Lee is falsely attributed with French nationality. Although one could argue that Mr Lee may indeed be a French national, considering his birth and upbringing there, the authors confirm that no such individual exists. We posit that namenationality hallucination falls under the sub-class of generated golems. It is plausible that a combination of our defined categories may exist, although we did not extensively studied these possibilities.\nLimitation 3: For this study, we have chosen 15 contemporary LLMs. In the dynamic landscape of LLM development, new models are constantly emerging, and we acknowledge that our selection may not encompass all available options. Keeping this in mind, we will make the benchmark and the HVI publicly accessible for collaborative updates and contributions.\nLimitation 4: The FACTUALITYGB technique operates based on entailment, allowing it to distinguish between sentences containing different entities such as Barack Obama and Joe Biden. However, it is unable to differentiate sentences that involve similar entities like AI and AAAI. In contrast, the ENTROPYBB technique operates at the token level and is capable of handling cases like 1789 vs. 1889. These distinctions become evident in the observed results.\n9 Ethical Considerations\nThrough our experiments, we have uncovered the susceptibility of LLMs to hallucination. In developing HVI, we intend to provide a framework that can inform future research and policies in this domain. However, we must address the potential misuse of our findings by malicious entities who may exploit AI-generated text, such as creating indistinguishable fake news from human-written content. We vehemently discourage such misuse and strongly advise against it.\nReferences 2023. The future of computational linguistics.\nAbien Fred Agarap. 2019. Deep learning using rectified linear units (relu).\nAyush Agrawal, Lester Mackey, and Adam Tauman Kalai. 2023. Do language models know when they\u2019re hallucinating references?\nStability AI. 2023. Stability ai launches the first of its stable lm suite of language models.\nAmazon. Amazon mechanical turk.\nRishi Bommasani, Kevin Klyman, Daniel Zhang, and Percy Liang. 2023. Do foundation model providers comply with the eu ai act?\nSamuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. The snli corpus.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\nMarc Brysbaert, Amy Beth Warriner, and Victor Kuperman. 2014. Concreteness ratings for 40 thousand generally known english word lemmas. Behavior research methods, 46:904\u2013911.\nMeng Cao, Yue Dong, and Jackie Chi Kit Cheung. 2022. Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3340\u20133354.\nAnthony Chen, Panupong Pasupat, Sameer Singh, Hongrae Lee, and Kelvin Guu. 2023. Purr: Efficiently\nediting language model hallucinations by denoising language model corruptions.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality.\nCopyright-Office. 2023. Copyright registration guidance: Works containing material generated by artificial intelligence. Library of Congress.\nDavid Dale, Elena Voita, Lo\u00efc Barrault, and Marta R. Costa-juss\u00e0. 2022. Detecting and mitigating hallucinations in machine translation: Model internal workings alone do well, sentence similarity even better.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359.\ndatabricks. 2023. Dolly.\nTristan Deleu, David Kanaa, Leo Feng, Giancarlo Kerg, Yoshua Bengio, Guillaume Lajoie, and Pierre-Luc Bacon. 2022. Continuous-time meta-learning with forward mode differentiation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\nEuropean-Parliament. 2023. Proposal for a regulation of the european parliament and of the council laying down harmonised rules on artificial intelligence (artificial intelligence act) and amending certain union legislative acts.\nForbes. 2023. Lawyer used chatgpt in court\u2014and cited fake cases. a judge is considering sanctions.\nDirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with MACE. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1120\u20131130, Atlanta, Georgia. Association for Computational Linguistics.\nHuggingFace_InferenceAPI. Huggingface inference api.\nFaisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi Zhang, Dan Jurafsky, Kathleen McKeown, and Tatsunori Hashimoto. 2023a. When do pre-training biases propagate to downstream tasks? a case study in text summarization. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 3206\u20133219, Dubrovnik, Croatia. Association for Computational Linguistics.\nFaisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi Zhang, Dan Jurafsky, Kathleen Mckeown, and Tatsunori B Hashimoto. 2023b. When do pre-training biases propagate to downstream tasks? a case study in text summarization. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 3198\u2013 3211.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. Albert: A lite bert for self-supervised learning of language representations.\nNayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N Fung, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. In Advances in Neural Information Processing Systems, volume 35, pages 34586\u201334599. Curran Associates, Inc.\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Halueval: A largescale hallucination evaluation benchmark for large language models.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\nPotsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.\nGary Marcus and Future of Life Institute. 2023. Pause giant ai experiments: An open letter.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906\u20131919, Online. Association for Computational Linguistics.\nMidjourney. 2022. https://www.midjourney.com.\nNiels M\u00fcndler, Jingxuan He, Slobodan Jenko, and Martin Vechev. 2023. Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation.\nnVIDIA. 2023. https://nvidia.github.io/nemo/.\nNYT. https://www.nytimes.com/topic/company/twitter.\nOpenAI. 2022. Introducing chatgpt.\nOpenAI. 2023a. Gpt-4 technical report.\nOpenAI. 2023b. Our approach to ai safety.\nAllan Paivio. 2013. Dual coding theory, word abstractness, and emotion: a critical review of kousta et al.(2011).\nJonas Pfeiffer, Francesco Piccinno, Massimo Nicosia, Xinyi Wang, Machel Reid, and Sebastian Ruder. 2023. mmt5: Modular multilingual pre-training solves source language hallucinations.\nPolitifact. https://www.politifact.com/.\nOfir Press, Noah A. Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821\u20138831. PMLR.\nVikas Raunak, Arul Menezes, and Marcin JunczysDowmunt. 2021. The curious case of hallucinations in neural machine translation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1172\u20131183, Online. Association for Computational Linguistics.\nReuters. 2023. Alphabet shares dive after google ai chatbot bard flubs answer in ad.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. ArXiv, abs/1910.01108.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic\u0301, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.\nGoogle Search. Google search api.\nNoam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150.\nNoam Shazeer. 2020. Glu variants improve transformer.\nArvind Krishna Sridhar and Erik Visser. 2022. Improved beam search for hallucination mitigation in abstractive summarization.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2022. Roformer: Enhanced transformer with rotary position embedding.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\nNeeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. arXiv preprint arXiv:2307.03987.\nZhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, and Yoon Kim. 2023. Multitask prompt tuning enables parameter-efficient transfer learning. In The Eleventh International Conference on Learning Representations.\nWhite-House. 2023. Blueprint for an ai bill of rights: Making automated systems work for the american people.\nWikipedia_Fleiss\u2019s_Kappa. Fleiss\u2019s kappa.\nWikipedia_Krippendorff\u2019s_Alpha. Krippendorff\u2019s alpha.\nWikipedia_min_max. Normalization.\nWikipedia_zscore. Normalization.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Huggingface\u2019s transformers: State-of-the-art natural language processing.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32.\nBiao Zhang and Rico Sennrich. 2019. Root mean square layer normalization.\nMuru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A. Smith. 2023a. How language model hallucinations can snowball.\nShuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang Wang. 2023b. Mitigating language model hallucination with interactive questionknowledge alignment.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pretrained transformer language models.\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul F. Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. CoRR, abs/1909.08593.\nFrequently Asked Questions (FAQs)\n\u273d This study explores the unintended, negative aspects of hallucination; how about the useful effects that arise as a result of hallucination? \u27a0 While hallucinating has beneficiary effects in some computer vision use cases, where a generative vision model could perform in-painting of an occluded content in an image or generate an image of a scenario it hasn\u2019t seen in its training set (for example, a generated image corresponding to the prompt, \u201cwater on Mars\u201d), but it is usually undesirable in the context of the text. The downstream impact as a result of the model\u2019s is exacerbated by the fact that there is a lack of a programmatic method in the research community to distinguish the hallucinated vs. factually correct output. For this reason, this study focuses on characterizing the problem of hallucination particularly in the context of text.\n\u273d Why do you select those 15 large language models? \u27a0 We want to select several language models with varying parameter sizes for our experiments - ranging from large to small. Hence, the above chosen 14 models consist of large models like GPT-3 and smaller ones like T5 and T0.\n\u273d Why would extrinsic hallucination be riskier? \u27a0 According to the \u201cextrinsic hallucination\u201d definition, this kind of hallucination does not have any way to verify it from the source prompt. Hence, it is likely to be more harmful than the intrinsic ones.\n\u273d What is the purpose of constructing Factual Mirage and Silver Lining hallucination data?\n\u27a0 We want to show that hallucinations can happen in both cases, factually correct and incorrect prompts. Hence, in this paper, we construct an exhaustive dataset called .\n\u273d Why do you select high-entropy points for mitigation techniques? \u27a0 High entropy points are more uncertain points in the context of text generation and hence, more likely places where the LLM hallucinates. Hence, our mitigation approach works by detecting and replacing such high entropy points.\n\u273d Why would HVI be a better hallucination evaluation metric for the LLMs (as compared to the existing ones like accuracy, precision, recall, F1, etc.)? \u27a0 Although the commonly used evaluation metrics like accuracy, precision, etc. can be used for downstream tasks, HVI can be more specifically used to determine the LLMs\u2019 hallucination tendency. HVI will serve as a uniform hallucination score for all the present and future LLMs.\n\u273d What are the insights on using black-box vs. gray-box models for mitigation hallucinations? \u27a0 Both black-box and gray-box models have their own advantages and disadvantages in terms of reducing hallucinations. Therefore, the choice of the appropriate method to minimize hallucination would be LLM- and task-dependent.\nA Appendix\nThis section provides supplementary material in the form of additional examples, implementation details, etc. to bolster the reader\u2019s understanding of the concepts presented in this work.\nB Annotation Process, and agreement\nB.1 Pilot in-house annotation\nCrowdsourcing platforms are widely recognized for their speed and cost-effectiveness in annotation tasks. However, it is important to note that they can also introduce noise or inaccuracies in the annotations. To mitigate this, prior to utilizing crowdsourcing services, we conducted an in-house annotation process involving 2,000 samples. These samples included prompts and generated text snippets from five different LLMs. This in-house annotation process served two purposes: firstly, it allowed us to formulate comprehensive annotation guidelines, and secondly, it helped us develop an annotation interface tailored to our specific needs. By undertaking this internal annotation process, we aimed to ensure the quality and reliability of the annotations before moving on to crowdsourcing.\nB.2 Annotation Steps\nWhen annotating an AI-generated text snippet, we follow a sentence-wise approach. Our annotation process involves three layers of annotation: (i) Orientation: This layer captures the orientation of hallucinations. (ii) Category: This layer classifies the category of hallucination, and (iii) Degree: This layer quantifies the intensity or magnitude of hallucination. By employing these three layers, we aim to provide a comprehensive and detailed annotation for hallucination in AI-generated text.\nAlgorithm 1: Annotation Guidelines 1 Split the paragraph into a list of sentences. 2 Annotate the orientation of hallucination as intrinsic or extrinsic. 3 Annotate the category of hallucination. 4 Annotate the degree of hallucination.\n\u2022 Step 1: In order to analyze the legitimacy of an AI-Generated paragraph and identify any potential hallucination, we begin with a sentence-level approach. We split the paragraph into individual sentences ensuring that each sentence is distinct and well separated from the others. Each sentence undergoes rigorous scrutiny to determine its legitimacy. This involves the identification of the type of hallucination, the category of hallucination, and the degree of hallucination.\n\u2022 Step 2: In this step, we identify whether the sentence has no hallucination, intrinsic hallucination, or extrinsic hallucination. The absence of both intrinsic and extrinsic hallucination implies no hallucination. To identify whether the sentence has intrinsic hallucination or extrinsic hallucination, we refer to the definitions in Section 2. We annotate each sentence using the annotations listed in Table 3\n\u2022 Step 3: In this step, we identify whether the detected hallucinated sentences of the previous step belong to any of the categories mentioned in Fig. 1. To identify the categories we refer to the definitions mentioned in Section 2. If the hallucinated sentence does not fall under any of the identified categories,\nit implies a miscellaneous category. Once we have identified the category, we annotate each sentence using the annotations listed in Table 3.\n\u2022 Step 4: This step involves categorizing the degree of hallucination as mild, moderate, or alarming, based on the level of delusional information in the sentence. A high degree refers to completely delusional information, a moderate degree to partially delusional information, and a low degree to minimal delusional information. Once we have identified the degree of hallucination, we annotate it as listed in Table 3.\nB.3 Web Interface for Annotation\nIn order to facilitate the annotation process for the annotators, it is crucial to provide them with a user-friendly interface that enables easy navigation. Fig. 9 shows our annotation web interface used to construct the HILT dataset. The interface is designed to offer a comprehensive view to the annotator. For instance, at the top of the interface, the actual prompt used for generating the text snippet is displayed. Directly below the prompt, the complete AI-generated text is shown. On the right-hand side, the sentence breakup is presented, with the currently selected sentence highlighted in red. Below the sentence breakup, all the relevant categories are displayed as radio buttons, allowing the annotators to easily annotate each category. This interface aims to enhance the efficiency and effectiveness of the annotation process. We have gone through a few rounds of iterations before finalizing the current version of the web interface.\nB.4 Selecting quality annotators on AMT\nIt is widely acknowledged that platforms like AMT can be noisy, making the selection of high-quality annotators a critical step in ensuring accurate annotations. The in-house annotation of 2,000 data points\nplayed a significant role in achieving this goal. To identify reliable annotators, we initiated a pilot task and only selected those with an accuracy rate of over 90% based on our in-house annotated dataset.\nAnother crucial consideration when annotating data on crowdsourcing platforms is the compensation offered to annotators. While offering too little may deter interest, excessively high wages may attract undesirable spammers. Achieving the ideal balance required several rounds of iteration to determine an appropriate compensation scheme.\nBy carefully addressing factors such as selecting qualified annotators and establishing suitable compensation rates, we improved the quality of annotations obtained from crowdsourcing platforms.\nB.5 Inter-Annotator Agreement\nWe report Fleiss\u2019s kappa (\u03ba) (Wikipedia_Fleiss\u2019s_Kappa) and Krippendorff\u2019s alpha (\u03b1) (Wikipedia_Krippendorff\u2019s_Alpha) scores (see Tables 4 and 5) to access the reliability of agreement between the three annotators1. We compute agreement on 10% of total annotated entities and obtain substantial to almost perfect agreement in all three annotation types in both datasets, namely NYT and Politifact. We have obtained nearly or more than 80% agreement in the case of orientation and category. The agreement on degree exhibits slight variation, as it relies on the subjective assessment of the percentage of hallucination in the sentence. This interpretation of percentage tends to differ among individuals. We report both Fleiss\u2019s kappa and Krippendorff\u2019s alpha score because Fleiss\u2019s kappa is a statistical measure that allows us to find agreement among multiple annotators and Krippendorff\u2019s alpha is a statistical measure that allows us to handle various types of data like nominal (orientation and category) and ordinal (degree).\n1Three graduate students\nC Details on chosen LLMs\nC.1 Criteria for choosing LLMs Beyond the primary criteria for choosing performant LLMs, our selection was meant to cover a wide gamut of LLMs that utilize a repertoire of recent techniques under the hood that have enabled their exceptional capabilities, namely:\n\u2022 FlashAttention (Dao et al., 2022) for memory-efficient exact attention.\n\u2022 Multi-Query Attention (Shazeer, 2019) for memory bandwidth efficiency.\n\u2022 SwiGLU (Shazeer, 2020) as the activation function instead of ReLU (Agarap, 2019).\n\u2022 ALiBi (Press et al., 2022) for larger context width.\n\u2022 RMSNorm (Zhang and Sennrich, 2019) for per-normalization.\n\u2022 RoPE (Su et al., 2022) to improve the expressivity of positional embeddings, etc.\nC.2 Details on Large Language Models Model details are given in Table 6. We use HuggingFace (Wolf et al., 2020) and OpenAI for implementing the large language models to generate the dataset.\nD - Prompt sources\nWe used two datasets to curate HILT: (i) New York Times Tweets (NYT) for factually correct and (ii) Politifact dataset (Politifact) for factually incorrect prompts.\nE What is a high entropy vs. low entropy word?\nIn the context of language modeling, a high entropy word refers to a word or token that has a high level of uncertainty or unpredictability in its occurrence. In other words, it is a word that is relatively rare or has a low probability of appearing in a given context. Entropy is often used to quantify the level of\nunpredictability associated with generating specific words or tokens. When a language model encounters a high entropy word, it means that the model has greater difficulty in accurately predicting or generating that word based on the context or preceding words. High entropy words are often less frequent in the training data or have complex patterns of occurrence. For example, in a language model trained on news articles, words like \u201cpneumonoultramicroscopicsilicovolcanoconiosis\u201d (a technical term for a lung disease) would likely have high entropy, as they are infrequent and occur in specific contexts.\nOn the other hand, a low entropy word refers to a word or token that has a relatively high predictability or a limited range of potential next words given the context. In other words, it is a word that occurs frequently and is highly expected in a specific context. When a language model encounters a low entropy word, it means that the model has a higher confidence or accuracy in predicting or generating that word based on the context or preceding words. For example, in a language model trained on English text, common words like \u201cthe,\u201d \u201cand,\u201d or \u201cis\u201d have low entropy because they occur frequently and are highly predictable in many contexts. These words tend to have a limited set of likely next words based on the preceding context. Fig. 10 illustrates an example of the entropy distribution over the set of words in an input sentence. In sentence 1, [MASK] token has low entropy since capital is the highest probable word in that sentence. However, [MASK] token in sentence 2 has high entropy since it is quite uncertain as to what the masked token could be. The probability distributions for this illustration are created using the HuggingFace Inference API (HuggingFace_InferenceAPI).\nF Mitigation Techniques: The specifics\nIn addition to the Fig. 4 provided in Section 6, we now present two additional illustrative examples in Table 7 and Table 8 to demonstrate the practical application of two different mitigation techniques.\nfragments, C - replaced text, D - highlighted text for no information found, and E - refuted text fragments by textual entailment.\nfragments, C - replaced text, D - highlighted text for no information found, and E - refuted text fragments by textual entailment.\nF.1 ENTROPYBB Building upon Section 6, Tables 9 to 23 illustrate the examples of the nuanced categorization of hallucination proposed in the paper.\nF.2 Evaluation strategy - how to determine no hallucination after mitigation?\nIn order to assess the absence of hallucination following the implementation of the mitigation techniques ENTROPYBB and FACTUALITYGB, a random sample of 2,000 data points was taken. This sample included 500 instances each of IFM, EFM, ISL, and ESL, ensuring a well-balanced distribution of the six hallucination categories within the data. Following the implementation of the ENTROPYBB method, which involved replacing words and phrases, we conducted a manual evaluation of the 2,000 samples. This evaluation, carried out by six annotators, aimed to assess whether hallucination was alleviated or not.\nFor the FACTUALITYGB method, we assumed that if the sentences were rewritten by humans, there would be no presence of hallucination. Therefore, for the highlighted sentences, hallucination was deemed waived. Results of this evaluation are reported in Fig. 5 and Table 24.\nF.3 Performance of ENTROPYBB vs. FACTUALITYGB\nFig. 5 and Table 24 give a relative analysis of our two proposed mitigation techniques described in Section 6. We report the actual values in the Table 24. Our empirical findings indicate that ENTROPYBB technique primarily tackles less complex hallucination categories such as acronym ambiguity and numeric issues. However, FACTUALITYGB technique is more applicable for dealing with more complex cases of hallucinations. Therefore, it is quite evident that a combination of both black- and gray-box methods would be the future direction of research.\nG Examples from\nThe following Tables 25 to 30 illustrate the examples of the nuanced categorization of hallucination proposed in the paper."
        }
    ],
    "title": "The Troubling Emergence of Hallucination in Large Language Models \u2013 An Extensive Definition, Quantification, and Prescriptive Remediations",
    "year": 2023
}