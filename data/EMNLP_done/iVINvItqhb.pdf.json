{
    "abstractText": "Most languages of the world pose low-resource challenges to natural language processing models. With multilingual training, knowledge can be shared among languages. However, not all languages positively influence each other and it is an open research question how to select the most suitable set of languages for multilingual training and avoid negative interference among languages whose characteristics or data distributions are not compatible. In this paper, we propose GradSim, a language grouping method based on gradient similarity. Our experiments on three diverse multilingual benchmark datasets show that it leads to the largest performance gains compared to other similarity measures and it is better correlated with cross-lingual model performance. As a result, we set the new state of the art on AfriSenti, a benchmark dataset for sentiment analysis on low-resource African languages. In our extensive analysis, we further reveal that besides linguistic features, the topics of the datasets play an important role for language grouping and that lower layers of transformer models encode language-specific features while higher layers capture task-specific information.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mingyang Wang"
        },
        {
            "affiliations": [],
            "name": "Heike Adel"
        },
        {
            "affiliations": [],
            "name": "Lukas Lange"
        },
        {
            "affiliations": [],
            "name": "Jannik Str\u00f6tgen"
        },
        {
            "affiliations": [],
            "name": "Hinrich Sch\u00fctze"
        }
    ],
    "id": "SP:6868d722702b15b0d9cc1fef243401c338492ff1",
    "references": [
        {
            "authors": [
                "natius Ezeani",
                "Chiamaka Chukwuneke",
                "Mofetoluwa Oluwaseun Adeyemi",
                "Gilles Quentin Hacheme",
                "Idris Abdulmumin",
                "Odunayo Ogundepo",
                "Oreen Yousuf",
                "Tatiana Moteu",
                "Dietrich Klakow"
            ],
            "title": "MasakhaNER 2.0: Africa-centric transfer learning",
            "year": 2022
        },
        {
            "authors": [
                "Jesujoba O. Alabi",
                "David Ifeoluwa Adelani",
                "Marius Mosbach",
                "Dietrich Klakow."
            ],
            "title": "Adapting pretrained language models to African languages via multilingual adaptive fine-tuning",
            "venue": "Proceedings of the 29th International Conference on Computational",
            "year": 2022
        },
        {
            "authors": [
                "Tyler Chang",
                "Zhuowen Tu",
                "Benjamin Bergen."
            ],
            "title": "The geometry of multilingual language model representations",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 119\u2013136, Abu Dhabi, United Arab Emirates.",
            "year": 2022
        },
        {
            "authors": [
                "Zhao Chen",
                "Vijay Badrinarayanan",
                "Chen-Yu Lee",
                "Andrew Rabinovich"
            ],
            "title": "Gradnorm: Gradient normalization for adaptive loss balancing in deep",
            "year": 2018
        },
        {
            "authors": [
                "Alexandra Chronopoulou",
                "Dario Stojanovski",
                "Alexander Fraser."
            ],
            "title": "Language-family adapters for low-resource multilingual neural machine translation",
            "venue": "Proceedings of the The Sixth Workshop on Technologies for Machine Translation of Low-",
            "year": 2023
        },
        {
            "authors": [
                "Rotem Dror",
                "Gili Baumer",
                "Segev Shlomov",
                "Roi Reichart."
            ],
            "title": "The hitchhiker\u2019s guide to testing statistical significance in natural language processing",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2018
        },
        {
            "authors": [
                "Chris Fifty",
                "Ehsan Amid",
                "Zhe Zhao",
                "Tianhe Yu",
                "Rohan Anil",
                "Chelsea Finn."
            ],
            "title": "Efficiently identifying task groupings for multi-task learning",
            "venue": "Advances in Neural Information Processing Systems, 34:27503\u2013 27516.",
            "year": 2021
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Ana Marasovi\u0107",
                "Swabha Swayamdipta",
                "Kyle Lo",
                "Iz Beltagy",
                "Doug Downey",
                "Noah A. Smith."
            ],
            "title": "Don\u2019t stop pretraining: Adapt language models to domains and tasks",
            "venue": "Proceedings of the 58th Annual Meeting of the",
            "year": 2020
        },
        {
            "authors": [
                "Michael A. Hedderich",
                "Lukas Lange",
                "Heike Adel",
                "Jannik Str\u00f6tgen",
                "Dietrich Klakow."
            ],
            "title": "A survey on recent approaches for natural language processing in low-resource scenarios",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Junjie Hu",
                "Sebastian Ruder",
                "Aditya Siddhant",
                "Graham Neubig",
                "Orhan Firat",
                "Melvin Johnson"
            ],
            "title": "Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization",
            "year": 2020
        },
        {
            "authors": [
                "Ganesh Jawahar",
                "Beno\u00eet Sagot",
                "Djam\u00e9 Seddah."
            ],
            "title": "What does BERT learn about the structure of language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651\u20133657, Florence, Italy",
            "venue": "Association for",
            "year": 2019
        },
        {
            "authors": [
                "Olga Kovaleva",
                "Alexey Romanov",
                "Anna Rogers",
                "Anna Rumshisky."
            ],
            "title": "Revealing the dark secrets of BERT",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Lukas Lange",
                "Heike Adel",
                "Jannik Str\u00f6tgen",
                "Dietrich Klakow."
            ],
            "title": "FAME: Feature-based adversarial meta-embeddings for robust input representations",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Lukas Lange",
                "Anastasiia Iurshina",
                "Heike Adel",
                "Jannik Str\u00f6tgen."
            ],
            "title": "Adversarial alignment of multilingual models for extracting temporal expressions from text",
            "venue": "Proceedings of the 5th Workshop on Representation Learning for NLP, pages 103\u2013109,",
            "year": 2020
        },
        {
            "authors": [
                "Lukas Lange",
                "Jannik Str\u00f6tgen",
                "Heike Adel",
                "Dietrich Klakow."
            ],
            "title": "To share or not to share: Predicting sets of sources for model transfer learning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Peiqin Lin",
                "Chengzhi Hu",
                "Zheyu Zhang",
                "Andr\u00e9 F.T. Martins",
                "Hinrich Sch\u00fctze"
            ],
            "title": "mplm-sim: Unveiling better cross-lingual similarity and transfer in multilingual pretrained language models",
            "year": 2023
        },
        {
            "authors": [
                "Patrick Littell",
                "David R. Mortensen",
                "Ke Lin",
                "Katherine Kairis",
                "Carlisle Turner",
                "Lori Levin."
            ],
            "title": "URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors",
            "venue": "Proceedings of the 15th Conference of the European Chap-",
            "year": 2017
        },
        {
            "authors": [
                "Yihong Liu",
                "Haotian Ye",
                "Leonie Weissweiler",
                "Philipp Wicke",
                "Renhao Pei",
                "Robert Zangenfeind",
                "Hinrich Sch\u00fctze"
            ],
            "title": "A crosslingual investigation of conceptualization",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101.",
            "year": 2017
        },
        {
            "authors": [
                "Dan Malkin",
                "Tomasz Limisiewicz",
                "Gabriel Stanovsky."
            ],
            "title": "A balanced data approach for evaluating cross-lingual transfer: Mapping the linguistic blood bank",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Bernard Opoku",
                "Steven Arthur"
            ],
            "title": "2023a. AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages",
            "year": 2023
        },
        {
            "authors": [
                "Joakim Nivre",
                "Marie-Catherine de Marneffe",
                "Filip Ginter",
                "Yoav Goldberg",
                "Jan Haji\u010d",
                "Christopher D. Manning",
                "Ryan McDonald",
                "Slav Petrov",
                "Sampo Pyysalo",
                "Natalia Silveira",
                "Reut Tsarfaty",
                "Daniel Zeman"
            ],
            "title": "Universal Dependencies v1: A multilingual",
            "year": 2016
        },
        {
            "authors": [
                "Arturo Oncevay",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Bridging linguistic typology and multilingual machine translation with multi-view language representations",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoman Pan",
                "Boliang Zhang",
                "Jonathan May",
                "Joel Nothman",
                "Kevin Knight",
                "Heng Ji."
            ],
            "title": "Cross-lingual name tagging and linking for 282 languages",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
            "year": 2017
        },
        {
            "authors": [
                "Telmo Pires",
                "Eva Schlinger",
                "Dan Garrette"
            ],
            "title": "How multilingual is multilingual BERT",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Alessandro Raganato",
                "J\u00f6rg Tiedemann."
            ],
            "title": "An analysis of encoder representations in transformerbased machine translation",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages",
            "year": 2018
        },
        {
            "authors": [
                "Sebastian Ruder."
            ],
            "title": "An overview of multi-task learning in deep neural networks",
            "venue": "arXiv preprint arXiv:1706.05098.",
            "year": 2017
        },
        {
            "authors": [
                "Ozan Sener",
                "Vladlen Koltun."
            ],
            "title": "Multi-task learning as multi-objective optimization",
            "venue": "Advances in neural information processing systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Kyle Shaffer."
            ],
            "title": "Language clustering for multilingual named entity recognition",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 40\u201345, Punta Cana, Dominican Republic. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "V\u00e9steinn Sn\u00e6bjarnarson",
                "Annika Simonsen",
                "Goran Glava\u0161",
                "Ivan Vuli\u0107."
            ],
            "title": "Transfer to a lowresource language via close relatives: The case study on Faroese",
            "venue": "Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa),",
            "year": 2023
        },
        {
            "authors": [
                "Trevor Standley",
                "Amir Zamir",
                "Dawn Chen",
                "Leonidas Guibas",
                "Jitendra Malik",
                "Silvio Savarese."
            ],
            "title": "Which tasks should be learned together in multi-task learning? In International Conference on Machine Learning, pages 9120\u20139132",
            "venue": "PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Xu Tan",
                "Jiale Chen",
                "Di He",
                "Yingce Xia",
                "Tao Qin",
                "Tie-Yan Liu."
            ],
            "title": "Multilingual neural machine translation with language clustering",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-",
            "year": 2019
        },
        {
            "authors": [
                "Ian Tenney",
                "Dipanjan Das",
                "Ellie Pavlick."
            ],
            "title": "BERT rediscovers the classical NLP pipeline",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593\u2013 4601, Florence, Italy. Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Mingyang Wang",
                "Heike Adel",
                "Lukas Lange",
                "Jannik Str\u00f6tgen",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Nlnde at semeval-2023 task 12: Adaptive pretraining and source language selection for low-resource multilingual sentiment analysis",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Zirui Wang",
                "Yulia Tsvetkov",
                "Orhan Firat",
                "Yuan Cao."
            ],
            "title": "Gradient vaccine: Investigating and improving multi-task optimization in massively multilingual models",
            "venue": "arXiv preprint arXiv:2010.05874.",
            "year": 2020
        },
        {
            "authors": [
                "Haoran Xu",
                "Kenton Murray."
            ],
            "title": "Por qu\u00e9 n\u00e3o utiliser alla spr\u00e5k? mixed training with gradient optimization in few-shot cross-lingual transfer",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 2043\u20132059, Seattle, United",
            "year": 2022
        },
        {
            "authors": [
                "Michihiro Yasunaga",
                "Jungo Kasai",
                "Dragomir Radev."
            ],
            "title": "Robust multilingual part-of-speech tagging via adversarial training",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
            "year": 2018
        },
        {
            "authors": [
                "Tianhe Yu",
                "Saurabh Kumar",
                "Abhishek Gupta",
                "Sergey Levine",
                "Karol Hausman",
                "Chelsea Finn."
            ],
            "title": "Gradient surgery for multi-task learning",
            "venue": "Advances in Neural Information Processing Systems, 33:5824\u2013 5836.",
            "year": 2020
        },
        {
            "authors": [
                "Amir R Zamir",
                "Alexander Sax",
                "William Shen",
                "Leonidas J Guibas",
                "Jitendra Malik",
                "Silvio Savarese."
            ],
            "title": "Taskonomy: Disentangling task transfer learning",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Lange"
            ],
            "title": "WikiAnn for NER following prior work (Shaffer, 2021) and 27 languages for POS tagging following Yasunaga et al",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Most natural language processing (NLP) research today still focuses on a small number of languages. Extending NLP models to further languages poses different challenges, i.a., little (annotated) data (Hedderich et al., 2021). Multilingual training can help in those cases by sharing knowledge across languages. However, adding new languages to the multilingual training set may not necessarily lead to performance gains. In fact, certain languages might actually hurt the performance on downstream tasks in a specific target language (Adelani et al., 2022; Sn\u00e6bjarnarson et al., 2023), for instance, due to unrelatedness to the target language.\nAs a solution, previous work investigates different measures for language similarity and selects only languages similar to the target language for the multilingual training set (i.a. Tan et al., 2019; Lin et al., 2019; Pires et al., 2019; Oncevay et al., 2020; Shaffer, 2021; Sn\u00e6bjarnarson et al., 2023). However, it is an open research question whether language similarity translates into performance gains of multilingual models. For multilingual training, other characteristics might play a role, such as topical shifts of the training data. As a result, it is still unclear how to select the set of languages that leads to the most effective multilingual training setup.\nIn this paper, we study multilingual fine-tuning of language models with a diverse set of training languages.1 In particular, we show that linguisticsbased language similarities are only weakly correlated with cross-lingual transfer performance. Figure 1 illustrates a sample case in which neither\n1Note that our proposed method is generic and could also be applied for multilingual pre-training.\nlanguage family information (indicated by node colors) nor similarity of language embeddings (indicated by proximity in the vector space) is helpful for finding languages that have a positive crosslingual transfer score with the target language. Thus, prior information about the languages, such as their language families or typological features, alone is not enough for an effective multilingual training. Instead, similarity measures that capture additional information about the data and task beyond linguistics similarity may achieve better performance. Wang et al. (2020), for instance, show that gradient similarity across languages measured along the optimization trajectory correlates with language proximity and cross-lingual performance.\nWe draw inspiration from this observation. However, instead of projecting conflicting gradients throughout the training process, we propose to leverage the gradient similarity to group languages with a branch-and-bound-like algorithm that optimizes the overall similarity score of all languages. This approach has the following advantages: (i) It can be applied without any prior knowledge of the languages or topics of the given datasets, (ii) it is well correlated with downstream task performance of the multilingual model, (iii) it finds the best language groups from a global perspective, i.e., instead of selecting source languages independently of each other (which may create groups of mutually interfering languages), we form each group based on a criterion that evaluates the group as a whole.\nIn our experiments, we show the superior performance of our grouping method compared to various baseline approaches on three multilingual datasets with different tasks and set the new state of the art on AfriSenti, a sentiment analysis dataset in 12 low-resource African languages.\nFurthermore, we extensively analyze our models with a topic analysis, a correlation-based analysis and an ablation study, revealing important insights, for instance that the topic distribution of the training data heavily affects multilingual training and that lower layers of transformer models encode language-specific features while higher layers capture task-specific information. This confirms results from prior work (i.a., Raganato and Tiedemann, 2018; Jawahar et al., 2019; Tenney et al., 2019; Kovaleva et al., 2019) from another (correlation-based) perspective.\nThe code base for GradSim is available online.2\n2https://github.com/boschresearch/gradsim"
        },
        {
            "heading": "2 Related Work",
            "text": "Multilingual and multi-task training. A growing number of research projects investigates multilingual training to cover a variety of languages, including low-resource languages (Hu et al., 2020; Lange et al., 2020; Hedderich et al., 2021; FitzGerald et al., 2022). In the context of low-resource sentiment analysis, Wang et al. (2023) recently use the cross-lingual transfer score between pairs of languages to select source languages for multilingual training. Our approach differs from these works in that we investigate language interactions from a global optimization perspective.\nConsidering each language as a separate task, multilingual training can be treated as a multi-task learning (MTL) problem (Ruder, 2017). A line of existing work utilizes gradient-based techniques to improve multi-task learning (Chen et al., 2018; Sener and Koltun, 2018; Yu et al., 2020; Wang et al., 2020). They show that negative cosine similarity between gradients leads to negative interference for MTL optimization, and projecting out the conflicting gradients can improve the optimization dynamics. Our work follows this insightful observation. However, in contrast to their work, we propose to leverage multilingual gradients for language grouping to ensure that gradients are aligned in each language group.\nLanguage similarity measures. In order to group languages for multilingual training or transfer learning, related work has proposed different ways to estimate the similarity between languages, e.g., leveraging the language family taxonomy (Tan et al., 2019; Shaffer, 2021; Chronopoulou et al., 2023; Sn\u00e6bjarnarson et al., 2023) or representing languages as information-rich vectors based on their typological or conceptual features (Littell et al., 2017; Lin et al., 2019; Oncevay et al., 2020; Liu et al., 2023).\nAnother line of works measures language similarity based on embeddings from multilingual pretrained language models (mPLMs) (Raganato and Tiedemann, 2018; Lange et al., 2021b; Chang et al., 2022; Lin et al., 2023). Tan et al. (2019) and Shaffer (2021), for instance, perform language grouping for multilingual named entity recognition and neural machine translation based on embeddings. In contrast to these studies, we propose to use the gradient cosine similarity between languages as the similarity measure for language grouping. This\nmodel-based similarity measure reflects how each language interacts in the optimization process, with no need of any prior knowledge of the languages."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we describe our proposed language grouping approach and the general multilingual training in which we apply its results. Note that the gradient-based similarity estimation is purely model-based, thus, can be applied to other settings, e.g., multi-domain or multi-task problems, as well."
        },
        {
            "heading": "3.1 Step I: Gradient Similarities",
            "text": "Due to the high discrepancy among languages, multilingual optimization often suffers from the conflicting gradient issue (Wang et al., 2020; Xu and Murray, 2022), i.e., gradients of different languages point into different directions. Previous works show that gradient similarity is correlated with model performance (Chen et al., 2018; Sener and Koltun, 2018; Wang et al., 2020; Yu et al., 2020). Inspired by this observation, we propose to use gradient similarity for grouping languages.\nGiven a set of languages L = {l1, l2, . . . , lN}, we study the gradient similarities across languages by training a multilingual model jointly on all languages and measure the language gradients G = {g1, g2, . . . , gN} along the optimization process. To reduce computational costs, we average language gradients first at the epoch level and calculate the per-epoch gradient cosine similarity between languages. Then we average the gradient similarity over all epochs. Finally, we get a gradient similarity matrix S \u2208 RN\u00d7N across N languages, with si,j = cos(gi, gj) =\ngi\u00b7gj |gi||gj | .\nSince it is very expensive to calculate the gradient similarity based on the gradients w.r.t. all parameters, we choose to only use the gradients based on the classification layer of the model. An analysis of gradients of different layers and ablations studies can be found in Sections 5.2 and 5.3."
        },
        {
            "heading": "3.2 Step II: Language Grouping",
            "text": "Based on the pairwise similarity matrix S from Step I, we next determine the best grouping into a pre-defined number of K groups.\nIn particular, our goal is to find the K language groups which (i) cover all languages of the given language set L, and (ii) maximize the overall similarity score of all languages, which is a reduction from the Set-Cover problem. We solve it using the\nbranch-and-bound-like algorithm as in Standley et al. (2020) and Fifty et al. (2021).3 The algorithm evaluates different combinations of K language groups under the constraint that each language is included in at least one, but potentially multiple groups. We finally select the language grouping that leads to the highest overall similarity score.\nGiven \u0393 = {\u03b31, . . . , \u03b3K} as a potential grouping result, we define the overall similarity score for \u0393 as\u2211N\ni=1 Sim(li|\u0393) where Sim(li|\u0393) is the collective similarity score of language li in its language group \u03b3j \u2208 \u0393. The collective similarity score of li \u2208 \u03b3j is defined as the average of all pair-wise similarities between li and the other languages in \u03b3j ."
        },
        {
            "heading": "3.3 Steps III : Training and Inference",
            "text": "Given the language groups \u0393 from Step II, we train one multilingual model per group \u03b3j \u2208 \u0393, using the training data from the respective languages. For inference, we select the appropriate multilingual model for each target language and apply it to the test data. If a target language li appears in more than one group, we select the group with the highest collective similarity score of li for inference."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we describe our experimental settings as well as our results for three tasks."
        },
        {
            "heading": "4.1 Tasks and Datasets",
            "text": "We experiment with the following three datasets covering different languages as well as text classification and sequence tagging tasks. (Dataset statistics are given in Table 8 and 9 in Appendix A.)\nAfriSenti (Muhammad et al., 2023a,b): This shared task dataset provides a challenging testbed for sentiment analysis: Both the languages (12 African languages) and the text genre (Twitter) pose challenges to NLP models. To investigate multilingual training results, we focus on the multilingual subtask of the shared task (Subtask B), and report macro-weighted F1 scores following Muhammad et al. (2023b).\nWikiAnn (Pan et al., 2017): This dataset offers automatically extracted labels for named entity recognition (NER). Following Shaffer (2021), we select 15 languages for our experiments and use micro-averaged F1 as the evaluation metric.\n3Alternatively, the binary integer program (BIP) solver could be used as in Zamir et al. (2018).\nIntern | CR/AIR3 | 20.10.2022 \u00a9 Robert Bosch GmbH 2022. Alle Rechte vorbehalten, auch bzgl. jeder Verf\u00fcgung, Verwertung, Reproduktion, Bearbeitung, Weitergabe sowie f\u00fcr den Fall von Schutzrechtsanmeldungen.\nUniversal Dependency (UD) treebank v1.2 (Nivre et al., 2016): We experiment with part-ofspeech (POS) tagging using the 17 Universal POS labels. Following prior work (Yasunaga et al., 2018; Lange et al., 2021a), we use 27 languages from the dataset with 21 high-resource and 6 low-resource languages and report accuracy for evaluation."
        },
        {
            "heading": "4.2 Training Details",
            "text": "For our experiments on AfriSenti, we use the pretrained AfroXLM-R large transformer (Alabi et al., 2022), an XLM-R model adapted to African languages, as our base model. To measure language gradients in Step I, we use 25% of the training data in AfriSenti and set the batch size to 8 for computational efficiency. For multilingual training and inference (Step III ), we use all training data and a batch size of 32. In both stages, we finetune the model with a learning rate of 1e-5 and set the maximum sequence length to 128. We set the number of language groups to K = 4 which equals the number of language families in the dataset. We further provide a comparison of different K in Section 5.3.\nFor the NER task, we follow the training setup used in Shaffer (2021) for a fair comparison. Specifically, we use XLM-R as our base model and finetune it for 3 epochs. We set the batch size to 20 with a learning rate of 2e-5 and a max sequence length of 300. Following Shaffer (2021), we set the number of language groups to K = 4.\nFor the POS tagging task, we use the XLM-R model as well and set the training epoch to 20. We use a batch size of 8, a learning rate of 2e-5 and a maximum sequence length of 128. Here, we specify K = 6 for language grouping, as 6\nlanguage families are covered by the 27 languages we study.\nOn all three datasets, we use the AdamW optimizer (Loshchilov and Hutter, 2017). The training was performed on Nvidia A100 GPUs.4 All reported results are averaged over 5 random seeds."
        },
        {
            "heading": "4.3 Baselines",
            "text": "Besides a monolingual model (trained only on the target language) and a purely multilingual model (trained on all available languages), we consider the following baselines for language grouping that have been presented by prior work:\nLanguage family. We group languages based on their language family information and train one multilingual model per language family. Language family-based grouping is also studied by, i.a., Tan et al. (2019); Shaffer (2021); Chronopoulou et al. (2023); Sn\u00e6bjarnarson et al. (2023).\nTypological similarity. Languages can be represented by typological features, e.g., the syntax, phonology or inventory features. Using the lang2vec tool and the URIEL knowledge base (Littell et al., 2017), we retrieve language vectors and use the pairwise distances among them as the similarity measure of our algorithm. This is similar to Lin et al. (2019) and Oncevay et al. (2020).\nEmbedding distance. Multilingual pretrained language models (mPLMs) also encode languagespecific information (Raganato and Tiedemann, 2018; Chang et al., 2022). Tan et al. (2019) and Shaffer (2021) use mPLM-based language embeddings to determine language similarities for language grouping. Following this idea, we compute\n4All experiments ran on a carbon-neutral GPU cluster.\nsentence embeddings using the pretrained encoder from our base model and average sentence embeddings of the same language. Then, we use the embedding distance across languages as the similarity measure in Step I (denoted by Embedding distance (PLM)). As an alternative, we also consider embeddings from the language model fine-tuned on the task (denoted by Embedding distance (FT)).\nOracle upper bound. As an upper bound, we group languages based on the post-hoc crosslingual transfer performance. The cross-lingual transfer performance is often used for source language selection as in Adelani et al. (2022) and Wang et al. (2023). We consider this an oracle upper bound as it is a direct indication of how knowledge learned from one language affects the performance on another language. Note that this approach is computationally expensive as it requires N\u00d7N transfer experiments for N languages, while our gradient-based approach only needs a single training run for collecting gradient information."
        },
        {
            "heading": "4.4 Results",
            "text": "Text classification. Table 1 shows our experimental results on the AfriSenti dataset (per language and on average). While for a few languages, a grouping based on our baseline approaches performs best (e.g., embedding distance for am and\npcm, or typological similarity for sw), GradSim performs best or second best for most languages and, as a result, best on average. Its average result comes very close to the oracle upper bound, which, in contrast to our approach, requires prior knowledge about cross-lingual transfer performance.\nWe also compare GradSim with the state-of-theart method on AfriSenti (Wang et al., 2023), which uses AfroXLM-R with task-adaptive pretraining (TAPT) (Gururangan et al., 2020) and performs transfer learning after selecting the best source languages based on their cross-lingual transfer score. For a direct comparison, we also apply TAPT and use GradSim to group languages for multilingual training. As shown in Table 2, GradSim sets the new state of the art on AfriSenti. It is superior to the previous approach of Wang et al. (2023) that only considers the pairwise transfer scores, neglecting possible interactions of different source languages. Instead, GradSim maximizes the overall gradient similarities from a global perspective.\nSequence tagging. Table 3 provides our results for multilingual named entity recognition. We report the state-of-the-art results from Shaffer (2021) as baseline results. Our approach GradSim outperforms the prior state of the art on most highresource languages and all low-resource languages, again leading to the best overall results.\nOur results for POS tagging are provided in Table 4. GradSim outperforms multilingual and monolingual training without language grouping as well as language grouping based on other metrics. It performs best on average over the low-resource languages as well as on average over all languages.\nGiven the results on sequence tagging tasks, we find that low-resource languages benefit more from language grouping. For high-resource languages, additional training sources from other languages\nhave a less prominent impact when enough inlanguage training data is available. It highlights the value of multilingual learning with well-suited languages to enhance the performance of low-resource languages, providing a key strategy for advancing future low-resource NLP research.\nSignificance tests. We run permutation-based significance tests following Dror et al. (2018) with a significance level of 0.05 between GradSim and the respective second-best system on all three datasets. In Tables 1, 3 and 4, settings with statistically significant improvements when using GradSim are marked with *. The results show that GradSim is significantly better than the second-best system in 32 out of 37 single language settings where GradSim outperforms the second-best system across three datasets. In addition, its average performance across all languages is significantly better than the other systems on all three datasets."
        },
        {
            "heading": "5 Analysis",
            "text": "To analyze the behavior of the model, we perform the following analyses on the AfriSenti dataset: A qualitative analysis of the data in order to better un-\nderstand differences coming from data peculiarities (Section 5.1), a correlation analysis to explain why some grouping methods work better than others (Section 5.2), and an ablation study to investigate the impact of our design choices (Section 5.3)."
        },
        {
            "heading": "5.1 Topic Analysis",
            "text": "Although data analysis is valuable for research progress, it is challenging for foreign languages. Therefore, we choose a semi-automatic approach involving machine translation and manual inspection for better understanding the input data of our models: For each language, we first extract the 50 most relevant keywords via a term frequencyinverse document frequency (TF-IDF) method. Then, we use the pygoogletranslate API5 to translate the keywords into English and remove duplicate words and stop words. Table 6 provides exem-\n5https://github.com/Saravananslb/ py-googletranslation\nplary results for three languages of the AfriSenti dataset. The complete set of keywords for all languages is provided in Appendix D (see Table 15).\nWhile the keywords extracted for Swahili (sw) and Amharic (am) are mainly centered around political and administrative topics, e.g., national, minister, education, government etc, the keywords for Xitsonge (ts) are more related to every-day life aspects. The multilingual model performance reveals that indeed Swahili and Amharic can effectively be trained together while Swahili and Xitsonga rather harm each other, even though Swahili and Xitsonga belong to the same language family and Swahili and Amharic do not. When looking at the language grouping results, GradSim indeed groups sw and am together (see Table 12 in Appendix D), thus, is able to capture their topical similarity, while a\nlanguage family-based grouping would cluster sw and ts into the same group."
        },
        {
            "heading": "5.2 Correlation Analysis",
            "text": "Table 5 provides the results of our correlation analysis that we perform on the AfriSenti dataset. In particular, we compute the Pearson correlation coefficient between the different grouping methods (similarity measures) that we study and different characteristics, such as model-specific characteristics (measured by cross-lingual transfer score), language-specific characteristics (measured by typological similarity) and topic-specific characteristics (measured by keyword embedding distance).\nFrom the results, we can draw a number of interesting conclusions, namely:\n(i) The transfer score is not correlated with language family information and only weakly correlated with embedding-based similarity measures often used in related work. For gradient similarity, we see considerably higher correlation values, supporting our proposal of using this similarity measure for language grouping.\n(ii) There is a relatively weak correlation between the cross-lingual transfer score and the typological similarity, while language family and embedding-based similarity measures show a high correlation with typological language similariy. This indicates that these similarity measures capture the linguistics-based language information well, which, however, does not translate into better transfer performance. Similar to the oracle measure (transfer score), gradient similarity based on the classifier parameters is only weakly correlated\nwith typological language similarity. (iii) Based on the keywords extracted for our analysis in Section 5.1, we retrieve keyword embeddings from the pretrained model encoder and average them for each language. We then compare the similarities of the keyword-based language embeddings with our different similarity measures using Pearson correlation. We find that they are only weakly correlated with language family information and even weakly negatively correlated with embedding distances. However, the correlation with the cross-transfer score and our proposed gradient similarity is larger, indicating that the gradient similarity can indeed pick up the topic information of the data.\n(iv) While higher layers are higher correlated with task performance, lower layers show a higher correlation with typological distance. This indicates that lower layers encode rather general language-specific information while higher layers capture task-related information."
        },
        {
            "heading": "5.3 Ablation Study",
            "text": "Gradients from different layers. Table 7 shows an ablation study of our model. The main design choice of our approach is the position in the model where to take the gradients. In our analysis, we compare model performance when using gradients from different layers. We see a clear trend that higher layers are better suited than lower layers. In particular, taking the gradients directly from the classification layers leads to the best results.\nNumber of language groups. For our experiments, we choose the number of language groups K to be the same as the number of language families covered in the datasets.6 However, K is a hyperparameter of our method. Therefore, we in-\n6Except for WikiAnn, where we set K to the same number as prior work (Shaffer, 2021) for a fair comparison.\nvestigate the performance for K \u2208 {1 . . . 8} in Figure 3. Choosing K = 2 can already improve the performance compared to purely multilingual training (K = 1). Until K = 4, the performance further improves and then converges for larger K.\nGradient from layer Task performance"
        },
        {
            "heading": "6 Discussion",
            "text": "In this section, we summarize our main findings.\nLanguage similarity is not enough to determine transfer suitability. When sharing knowledge across languages, the information about linguisticsbased language similarity (e.g., whether the languages come from the same language family or how languages are typologically similar to each other) is not enough for optimal performance. This observation is in line with the findings by Tan et al. (2019), Shaffer (2021) and Malkin et al. (2022) that languages from the same family may still exhibit distinct linguistic features and, thus, languagefamily based grouping can enhance model performance only to a certain extent. In addition, we find that there are other aspects that will affect multilingual model performance and, therefore, need to be taken into account, such as the topical distribution of the data.\nGradient-based method does not require any prior knowledge. Our proposed gradient-based approach for grouping languages is a pure modelbased approach, thus, does not require any prior knowledge about the language, task or data. As a result, it can be successfully applied, even when the data distribution (e.g., topical distribution) is unknown (e.g., because we are dealing with foreign languages). While our current work only presents results for language grouping for multilingual models, the method itself is more general and can be applied to other settings as well, such as multi-task learning or multi-domain setups.\nLower layers capture language, upper layers task information. Adding to previous work on analyzing transformer-based pretrained language models (Raganato and Tiedemann, 2018; Jawahar et al., 2019; Tenney et al., 2019), our correlation analysis shows that gradient similarity between languages from lower layers are more correlated to language-specific distances, i.e., low layers seem to encode language-specific information, while gradient similarity from upper layers are more correlated to task-specific performance, i.e., upper layers tend to capture task-specific information."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we addressed the challenging problem of grouping languages for effective multilingual training. We proposed a gradient-based grouping approach and showed in our experiments that it is better correlated to cross-lingual transfer performance than language family or language embedding-based grouping. In our analysis, we identified topical distribution differences as one potential challenge that can be addressed effectively by our approach. Furthermore, our correlation analysis confirmed results from prior work that lower layers of transformer-based pretrained models seem to encode language-specific features, while upper layers capture task-specific information. Our method shows superior performance compared to a variety of baseline methods for language grouping on three diverse datasets and, in particular, sets the new state of the art on a multilingual sentiment analysis benchmark dataset consisting of low-resource African languages.\nLimitations\nOne limitation of our work is the scope of evaluation. While we performed experiments on three diverse text classification and sequence tagging tasks, GradSim is generally applicable to a wide range of tasks and could thus be evaluated on even further tasks.\nBesides, our experiments currently focus on multilingual settings and datasets. Experiments for multi-domain and multi-task settings are outside the scope of this work, however, an interesting direction for future work.\nFinally, compared to the large number of languages in the world, the set of languages in our work is still limited and, thus, our results might not be representative for all languages of the world.\nHowever, we chose the datasets for our experiments with the aim of covering a broad variety of languages, including African languages which are typically under-explored in NLP research.\nEthics Statement\nOur work focuses on multilingual and low-resource settings. For instance, we investigate our models on African languages which are typically underrepresented and under-explored in NLP research. Including them into NLP research is important from an ethical point of view."
        },
        {
            "heading": "A Dataset Information",
            "text": "Table 8 provides statistics of AfriSenti, the benchmark dataset we chose for our text classification experiments. Table 9 gives language information for the WikiAnn and Universal Dependencies datasets. We selected 15 languages from WikiAnn for NER following prior work (Shaffer, 2021) and 27 languages for POS tagging following Yasunaga et al. (2018); Lange et al. (2021a)."
        },
        {
            "heading": "B Additional Baseline Experimental Results",
            "text": "Here we provide the experimental results on the WikiAnn and UD POS tagging dataset with grouping methods based on cross-lingual transfer score, typological language similarity and language embeddings distance.\nComparing GradSim to other baseline methods in Table 10 and 11 , we can draw similar conclusions as in Section 4.4: First, GradSim achieves the best average performance and performs especially well in low-resource languages, revealing the importance of multilingual learning with suitable sets of source languages for low-resource languages. Additionally, GradSim comes very close to the oracle upper bound without any prior knowledge about the cross-lingual transfer performance."
        },
        {
            "heading": "C Language Grouping Reults",
            "text": "In this section, we provide further details in Table 12, 13 and 14 of our language grouping results with the best language groups based on different similarity measures on the three datasets."
        },
        {
            "heading": "D Keyword extraction results",
            "text": "In Table 15, we give the full set of keywords extracted from the AfriSenti dataset of 12 African languages."
        }
    ],
    "title": "GradSim: Gradient-Based Language Grouping for Effective Multilingual Training",
    "year": 2023
}