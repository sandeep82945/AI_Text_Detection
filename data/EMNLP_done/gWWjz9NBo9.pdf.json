{
    "abstractText": "Data augmentation is a widely used technique to address the problem of text classification when there is a limited amount of training data. Recent work often tackles this problem using large language models (LLMs) like GPT3 that can generate new examples given already available ones. In this work, we propose a method to generate more helpful augmented data by utilizing the LLM\u2019s abilities to follow instructions and perform few-shot classifications. Our specific PromptMix method consists of two steps: 1) generate challenging text augmentations near class boundaries; however, generating borderline examples increases the risk of false positives in the dataset, so we 2) relabel the text augmentations using a prompting-based LLM classifier to enhance the correctness of labels in the generated data. We evaluate the proposed method in challenging 2-shot and zero-shot settings on four text classification datasets: Banking77, TREC6, Subjectivity (SUBJ), and Twitter Complaints. Our experiments show that generating and, crucially, relabeling borderline examples facilitates the transfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and cheaper classifiers like DistilBERTbase and BERTbase. Furthermore, 2-shot PromptMix outperforms multiple 5-shot data augmentation methods on the four datasets. Our code is available at https://github.com/ ServiceNow/PromptMix-EMNLP-2023.",
    "authors": [
        {
            "affiliations": [],
            "name": "Gaurav Sahu"
        },
        {
            "affiliations": [],
            "name": "Olga Vechtomova"
        },
        {
            "affiliations": [],
            "name": "Dzmitry Bahdanau"
        },
        {
            "affiliations": [],
            "name": "Issam H. Laradji"
        }
    ],
    "id": "SP:02f36635d8d89eeb2f69277f6ff82ec6819cc0bd",
    "references": [
        {
            "authors": [
                "Ateret Anaby-Tavor",
                "Boaz Carmeli",
                "Esther Goldbraich",
                "Amir Kantor",
                "George Kour",
                "Segev Shlomov",
                "Naama Tepper",
                "Naama Zwerdling"
            ],
            "title": "Do not have enough data? deep learning to the rescue",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelli-",
            "year": 2020
        },
        {
            "authors": [
                "Soumya Barikeri",
                "Anne Lauscher",
                "Ivan Vuli\u0107",
                "Goran Glava\u0161."
            ],
            "title": "Redditbias: A real-world resource for bias evaluation and debiasing of conversational language models",
            "venue": "arXiv preprint arXiv:2106.03521.",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Bragg",
                "Arman Cohan",
                "Kyle Lo",
                "Iz Beltagy."
            ],
            "title": "Flex: Unifying evaluation for few-shot nlp",
            "venue": "Advances in Neural Information Processing Systems, 34:15787\u201315800.",
            "year": 2021
        },
        {
            "authors": [
                "Tom B Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In Proceedings of the 34th International",
            "year": 2020
        },
        {
            "authors": [
                "I\u00f1igo Casanueva",
                "Tadas Tem\u010dinas",
                "Daniela Gerz",
                "Matthew Henderson",
                "Ivan Vuli\u0107."
            ],
            "title": "Efficient intent detection with dual sentence encoders",
            "venue": "Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pages 38\u201345, On-",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-",
            "year": 2019
        },
        {
            "authors": [
                "Xingping Dong",
                "Jianbing Shen."
            ],
            "title": "Triplet loss in siamese network for object tracking",
            "venue": "Proceedings of the European conference on computer vision (ECCV), pages 459\u2013474.",
            "year": 2018
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeffrey Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "NIPS Deep Learning and Representation Learning Workshop.",
            "year": 2015
        },
        {
            "authors": [
                "Tushar Khot",
                "Ashish Sabharwal",
                "Peter Clark."
            ],
            "title": "Scitail: A textual entailment dataset from science question answering",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.",
            "year": 2018
        },
        {
            "authors": [
                "Yekyung Kim",
                "Seohyeong Jeong",
                "Kyunghyun Cho."
            ],
            "title": "Linda: Unsupervised learning to interpolate in natural language processing",
            "venue": "arXiv preprint arXiv:2112.13969.",
            "year": 2021
        },
        {
            "authors": [
                "Varun Kumar",
                "Ashutosh Choudhary",
                "Eunah Cho."
            ],
            "title": "Data augmentation using pre-trained transformer models",
            "venue": "Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems, pages 18\u201326.",
            "year": 2020
        },
        {
            "authors": [
                "Varun Kumar",
                "Hadrien Glaude",
                "Cyprien de Lichy",
                "Wlliam Campbell."
            ],
            "title": "A closer look at feature space data augmentation for few-shot intent classification",
            "venue": "EMNLP-IJCNLP 2019, page 1.",
            "year": 2019
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Kevin J Liang",
                "Weituo Hao",
                "Dinghan Shen",
                "Yufan Zhou",
                "Weizhu Chen",
                "Changyou Chen",
                "Lawrence Carin."
            ],
            "title": "Mixkd: Towards efficient distillation of largescale language models",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Yen-Ting Lin",
                "Alexandros Papangelis",
                "Seokhwan Kim",
                "Sungjin Lee",
                "Devamanyu Hazarika",
                "Mahdi Namazifar",
                "Di Jin",
                "Yang Liu",
                "Dilek Hakkani-Tur."
            ],
            "title": "Selective in-context data augmentation for intent detection using pointwise v-information",
            "venue": "Proceed-",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
            "venue": "arXiv preprint cs/0409058.",
            "year": 2004
        },
        {
            "authors": [
                "Daniel Preo\u0163iuc-Pietro",
                "Mihaela Gaman",
                "Nikolaos Aletras."
            ],
            "title": "Automatically identifying complaints in social media",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5008\u20135019, Florence, Italy. Associa-",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392.",
            "year": 2016
        },
        {
            "authors": [
                "Cyrus Rashtchian",
                "Peter Young",
                "Micah Hodosh",
                "Julia Hockenmaier."
            ],
            "title": "Collecting image annotations using amazon\u2019s mechanical turk",
            "venue": "Proceedings of the NAACL HLT 2010 workshop on creating speech and language data with Amazon\u2019s Mechanical Turk,",
            "year": 2010
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Gaurav Sahu",
                "Pau Rodriguez",
                "Issam Laradji",
                "Parmida Atighehchian",
                "David Vazquez",
                "Dzmitry Bahdanau."
            ],
            "title": "Data augmentation for intent classification with off-the-shelf large language models",
            "venue": "Proceedings of the 4th Workshop on NLP for Conver-",
            "year": 2022
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "venue": "arXiv preprint arXiv:1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "It\u2019s not just size that matters: Small language models are also fewshot learners",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2021
        },
        {
            "authors": [
                "Timo Schick",
                "Sahana Udupa",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp",
            "venue": "Transactions of the Association for Computational Linguistics, 9:1408\u2013 1424.",
            "year": 2021
        },
        {
            "authors": [
                "Florian Schroff",
                "Dmitry Kalenichenko",
                "James Philbin."
            ],
            "title": "Facenet: A unified embedding for face recognition and clustering",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815\u2013823.",
            "year": 2015
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Improving Neural Machine Translation Models with Monolingual Data",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2016
        },
        {
            "authors": [
                "Victor S Sheng",
                "Foster Provost",
                "Panagiotis G Ipeirotis."
            ],
            "title": "Get another label? improving data quality and data mining using multiple, noisy labelers",
            "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2008
        },
        {
            "authors": [
                "Kumar Shridhar",
                "Alessandro Stolfo",
                "Mrinmaya Sachan."
            ],
            "title": "Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions",
            "venue": "arXiv preprint arXiv:2212.00193.",
            "year": 2022
        },
        {
            "authors": [
                "Siqi Sun",
                "Zhe Gan",
                "Yuwei Fang",
                "Yu Cheng",
                "Shuohang Wang",
                "Jingjing Liu."
            ],
            "title": "Contrastive distillation on intermediate representations for language model compression",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Swabha Swayamdipta",
                "Roy Schwartz",
                "Nicholas Lourie",
                "Yizhong Wang",
                "Hannaneh Hajishirzi",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "Dataset cartography: Mapping and diagnosing datasets with training dynamics",
            "venue": "Proceedings of the 2020 Conference on Empirical",
            "year": 2020
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Lewis Tunstall",
                "Nils Reimers",
                "Unso Eun Seo Jo",
                "Luke Bates",
                "Daniel Korat",
                "Moshe Wasserblat",
                "Oren Pereg."
            ],
            "title": "Efficient few-shot learning without prompts",
            "venue": "arXiv preprint arXiv:2209.11055.",
            "year": 2022
        },
        {
            "authors": [
                "Ellen M Voorhees",
                "Dawn M Tice"
            ],
            "title": "The trec-8 question answering track evaluation",
            "venue": "In TREC,",
            "year": 1999
        },
        {
            "authors": [
                "Yufei Wang",
                "Jiayi Zheng",
                "Can Xu",
                "Xiubo Geng",
                "Tao Shen",
                "Chongyang Tao",
                "Daxin Jiang."
            ],
            "title": "KnowDA: All-in-one knowledge mixture model for data augmentation in low-resource NLP",
            "venue": "The Eleventh International Conference on Learning Rep-",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Chengyu Huang",
                "Soroush Vosoughi",
                "Yu Cheng",
                "Shiqi Xu."
            ],
            "title": "Few-shot text classification with triplet networks, data augmentation, and curriculum learning",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Kai Zou."
            ],
            "title": "EDA: Easy data augmentation techniques for boosting performance on text classification tasks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-",
            "year": 2019
        },
        {
            "authors": [
                "Peter West",
                "Chandra Bhagavatula",
                "Jack Hessel",
                "Jena Hwang",
                "Liwei Jiang",
                "Ronan Le Bras",
                "Ximing Lu",
                "Sean Welleck",
                "Yejin Choi."
            ],
            "title": "Symbolic knowledge distillation: from general language models to commonsense models",
            "venue": "Proceedings of the",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-ofthe-art natural language processing",
            "year": 2019
        },
        {
            "authors": [
                "Xing Wu",
                "Shangwen Lv",
                "Liangjun Zang",
                "Jizhong Han",
                "Songlin Hu."
            ],
            "title": "Conditional bert contextual augmentation",
            "venue": "Computational Science\u2013ICCS 2019: 19th International Conference, Faro, Portugal, June 12\u201314, 2019, Proceedings, Part IV 19, pages",
            "year": 2019
        },
        {
            "authors": [
                "Yinfei Yang",
                "Daniel Cer",
                "Amin Ahmad",
                "Mandy Guo",
                "Jax Law",
                "Noah Constant",
                "Gustavo Hernandez Abrego",
                "Steve Yuan",
                "Chris Tar",
                "Yun-hsuan Sung",
                "Brian Strope",
                "Ray Kurzweil."
            ],
            "title": "Multilingual universal sentence encoder for semantic retrieval",
            "venue": "Proceedings",
            "year": 2020
        },
        {
            "authors": [
                "Kang Min Yoo",
                "Dongju Park",
                "Jaewook Kang",
                "Sang-Woo Lee",
                "Woomyoung Park."
            ],
            "title": "Gpt3mix: Leveraging large-scale language models for text augmentation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2225\u20132239.",
            "year": 2021
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N. Dauphin",
                "David Lopez-Paz."
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Jianguo Zhang",
                "Trung Bui",
                "Seunghyun Yoon",
                "Xiang Chen",
                "Zhiwei Liu",
                "Congying Xia",
                "Quan Hung Tran",
                "Walter Chang",
                "Philip Yu."
            ],
            "title": "Few-shot intent detection via contrastive pre-training and fine-tuning",
            "venue": "arXiv preprint arXiv:2109.06349.",
            "year": 2021
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Banghua Zhu",
                "Hiteshi Sharma",
                "Felipe Vieira Frujeri",
                "Shi Dong",
                "Chenguang Zhu",
                "Michael I Jordan",
                "Jiantao Jiao."
            ],
            "title": "Fine-tuning language models with advantage-induced policy alignment",
            "venue": "arXiv preprint arXiv:2306.02231.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Data scarcity is a key challenge in numerous reallife scenarios, such as deploying intent detection systems in task-oriented conversational agents and identifying hateful instances of speech on social media platforms. Crowdsourcing has been a traditionally popular choice to obtain additional data, but it is a financially expensive procedure incurring a high cost of human labor (Sheng et al., 2008;\nRashtchian et al., 2010; Rajpurkar et al., 2016; Khot et al., 2018). With the recent surge in the development of generative large language models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Zhang et al., 2022; Touvron et al., 2023), a large body of literature has emerged that employs LLMs to generate additional data for various tasks (Kumar et al., 2020; Schick and Sch\u00fctze, 2021; Wang et al., 2023).\nIn this work, we focus on the task of few-shot text classification (Schick and Sch\u00fctze, 2021; Alex et al., 2021; Bragg et al., 2021). Specifically, we explore zero-shot and 2-shot settings. Early works employing LLMs to generate additional data samples for text classification first fine-tune a generative language model on an initial seed dataset and then use it to synthesize new training data (Wu et al., 2019; Kumar et al., 2019, 2020; Anaby-Tavor et al., 2020); however, the fine-tuning step quickly becomes a bottleneck in the absence of sufficient\nseed examples. More recent works sidestep finetuning by designing natural language prompts for off-the-shelf LLMs (Yoo et al., 2021; Sahu et al., 2022; Lin et al., 2023). A key limitation of such works is that their prompts only focus on using information from a single class when generating augmentations. Additionally, they do not incentivize the LLM to diversify the generated examples, and recent works show that instruction-tuned LLMs like InstructGPT (Ouyang et al., 2022) and Vicuna (Chiang et al., 2023) are prone to mode collapse (Zhu et al., 2023).\nTo address the previous limitations, we propose a two-step prompting-based technique, PromptMix. First, PromptMix instructs an LLM (in our case, GPT3.5-turbo1) to generate new examples by mixing information from multiple classes. The degree of mixing is controlled by a parameter \u03b1, and using a range of \u03b1 values diversifies the generated examples; however, promoting mixup increases the risk of false positive generations, so PromptMix uses a relabelling scheme in the second step to improve the faithfulness of the generated examples. In particular, it uses an LLM as a classifier to assign new labels to the generated examples. We find that training a classifier on these relabeled examples effectively transfers the knowledge of a massive LLM like GPT3.5 into much smaller models like BERT (Devlin et al., 2019) and DistilBERT (Sanh et al., 2019). Figure 2 demonstrates the complete PromptMix framework.\nWe summarize our contributions as follows: a) we propose PromptMix, a novel two-step prompting-based method to generate a diverse\n1https://platform.openai.com/docs/models/gpt-3-5\nset of labeled examples for any text classification dataset; and b) we demonstrate that generating borderline examples and relabeling them improves knowledge transfer from a massive LLM like GPT3.5 into much smaller models like DistilBERT and BERT, even without abundant seed examples. We also show that 2-shot PromptMix outperforms multiple data augmentation baselines that use 5-shot or more seed data."
        },
        {
            "heading": "2 Related work",
            "text": "Our work intersects with the topics of data augmentation, few-shot classification, and knowledge distillation, which we explain in detail below."
        },
        {
            "heading": "2.1 LLM-based Data Augmentation for Few-shot Text Classification",
            "text": "Kumar et al. (2019) evaluate different feature space data augmentation techniques, such as upsampling, perturbation, and linear interpolation, for varying levels of data scarcity (ranging from 5% data availability to 20%); however, their performance gains are minor compared to a no-augmentation setting. Kumar et al. (2020) consider 10-shot, 50-shot, and 100-shot text classification setups where they fine-tune pretrained language models like BERT, BART (Lewis et al., 2020), and GPT-2 (Radford et al., 2019) as data generators on k\u2212shot data. Next, they condition the fine-tuned data generators to synthesize new training examples for individual classes in the dataset. This method improves over classical data augmentation techniques, such as easy data augmentation (Wei and Zou, 2019) and back translation (Sennrich et al., 2016), but the experiments were performed on datasets with very\nfew classes (up to seven). In reality, classification setups can have hundreds of classes, and the initial fine-tuning step would become a bottleneck.\nTo alleviate the fine-tuning bottleneck, Yoo et al. (2021) use natural language prompts for data augmentation; however, they also experiment with less challenging classification setups (with fewer classes). Sahu et al. (2022) propose a promptingbased approach using off-the-shelf GPT-3, to generate a large labeled corpus of augmented data. Their method improves across multiple classification setups with a large number of classes (up to 150) and varying levels of granularity; however, their method struggles on tasks where the classes carry very close semantic meanings. In particular, the generated samples have incorrect labels. In our method, we use a language model as a classifier to improve the label accuracy of the generated dataset.\nSome non-prompting-based approaches include Wei et al. (2021), who propose curriculum data augmentation, where they first train a model on limited k\u2212shot data and then incrementally introduce augmented data as training progresses. They use triplet loss (Schroff et al., 2015), which minimizes the distance between data points with the same label and maximizes for differently labeled examples. Tunstall et al. (2022) propose SetFit that first fine-tunes sentence transformers on a small number of text pairs in a contrastive Siamese manner (Dong and Shen, 2018) and then generate rich text embeddings for training a classification head. Finally, Kim et al. (2021) propose LINDA, which is first trained on one million sentence pairs randomly drawn from English Wikipedia. Later, they use the Mixup algorithm (Zhang et al., 2018) to interpolate between two English sentences of varying lengths. LINDA significantly improves the performance of a BERTbase model across multiple few-shot classification setups. In our work, we consider more extreme few-shot setups (2-shot, zero-shot) and perform more controlled interpolation that promotes the generation of examples near class boundaries."
        },
        {
            "heading": "2.2 Knowledge Distillation",
            "text": "Knowledge distillation (Bucila et al., 2006; Hinton et al., 2015; West et al., 2022) refers to training a smaller student model mimicking the behavior of a much larger teacher model. In particular, the objective function aims to match the output distribution of the student model to that of the teacher model. By doing so, knowledge of the teacher model is\neffectively distilled into a much smaller student model, allowing a similar level of performance as the teacher at a lower computational cost. Shridhar et al. (2022) distill a GPT3 (6B) model into a GPT-2 model for a Chain-of-thought (CoT) reasoning task. Liang et al. (2021) propose MixKD to encourage the student model to mimic the teacher\u2019s behavior on not only the available training examples but also on interpolated examples. Finally, Sun et al. (2020) distill knowledge through intermediate layers of the teacher via a contrastive objective. In comparison, our approach allows knowledge distillation of a massive teacher model like GPT3.5 (175B parameters) into significantly smaller student models like DistilBERT and BERT (67M and 110M parameters, respectively)."
        },
        {
            "heading": "3 Methodology",
            "text": "We hypothesize that training a robust text classifier requires the training data to have a good mix of borderline examples (Swayamdipta et al., 2020). This section describes our method PromptMix, where we first instruct an LLM (GPT3.5-turbo, in our case) to generate difficult examples near class boundaries then relabel those generations to improve the faithfulness of the generated data (see Figure 2)."
        },
        {
            "heading": "3.1 Step 1: Generating examples",
            "text": "First, we manually write short descriptions for every class in the dataset. We use descriptions in our prompts to facilitate the usage of the approach in the extremely data-scarce zero-shot and twoshot settings. Next, we randomly select a group of t(= 4) 2 classes c \u2286 C classes, where C denotes the set of all classes in the dataset. For each class in c, we combine the description and k examples in the prompt, k being the k\u2212shot setting (see part 1 in Figure 3). Lastly, for each class ci\u2200i \u2208 [1, t] in the subset, we instruct GPT3.5-turbo 3 to generate n example utterances that are a mix of two classes: ci and a randomly selected class cj \u2208 c \\ {ci}. In particular, we instruct the LLM to generate utterances that belong \u03b1% to class ci and (1 \u2212 \u03b1)% to class cj (see part 2 in Figure 3). Figure 5 in Appedix A.1 shows the distribution \u03b1 is sampled from.\n2We choose t = 4 based on the results in Section A.2 3specifically, we use the gpt-3.5-turbo-0613 engine"
        },
        {
            "heading": "3.2 Step 2: Improving Fidelity",
            "text": "Since borderline examples are inherently difficult to put into a category, the LLM may generate examples for the minority class in the prompt (cj). In other words, the LLM can generate false positives. To address this issue, we employ GPT3.5-turbo as a classifier and relabel the generated examples. When constructing the classification prompt, we choose the top-5 closest classes according to the similarity between the SBERT sentence embedding (Reimers and Gurevych, 2019) of the generated example and available examples in the fewshot training set. For the zero-shot setting, we use the class name\u2019s embedding instead of the available examples. We then follow a similar process as in Step 1 to construct the prompt, but instead ask the LLM to classify the provided sentence into one of the classes in the prompt (see Figure 4). To ensure a valid prediction, we retrieve the closest class in the dataset based on the cosine similarity of the SBERT embedding of the GPT-generated class and the ground-truth classes in the dataset 4. We do\n4we use the sentence-transformers/all-mpnet-base-v2 model from the sentence-transformers library\nInput Prompt: Consider the task of classifying between the following classes(along with some examples):\nnot include all the classes in the prompt because a) some datasets can have hundreds of classes that would not fit in the context size of the LLM, and b) we found in our preliminary experiments that long contexts degraded GPT\u2019s classification ability. Figure 4 shows the prompt structure for relabeling a generated example.\nAfter generating borderline examples and relabeling them, we train a text classifier on the combined PromptMix-generated data and the original seed data. Specifically, we fine-tune DistilBERTbase and BERTbase classification models. In the subsequent sections, we show that our method achieves text augmentation, pseudolabeling, and knowledge distillation in one cohesive pipeline."
        },
        {
            "heading": "4 Does PromptMix Generate Borderline Examples?",
            "text": "Table 1 shows that using mixup generates sentences that contain information from the majority and the minority class, compared to sentences generated without mixup that only contain information about the majority class. This demonstrates that mixup\nabout age_limit in yellow and parts about atm_support in cyan . We see clear evidence of mixup at work as sentences generated using mixup contain information about both classes.\nleads to the generation of borderline examples. Table 1 also shows some false positives, where GPT generates sentences like, \u201cDo I need to be over a certain age to use an ATM?\" for the majority class age_limit. The class age_limit covers all age-related queries about opening a bank account, whereas the generated sentence is an age-related query about using an ATM, making it a better fit for the minority class atm_support. By relabelling such false positives, we aim to rectify the mismatch between the generated examples and their assigned class labels. Figure 4 shows that GPT correctly predicts the new class of the sentence as atm_support, verifying the importance of the relabeling step in our approach.\nOverall, we find the observations in this section to be strong evidence that PromptMix can generate high-quality datasets even in aggressive few-shot text classification setups. Next, we conduct an extensive suite of experiments to verify the effectiveness of PromptMix."
        },
        {
            "heading": "5 Experimental Setup",
            "text": ""
        },
        {
            "heading": "5.1 Datasets",
            "text": "We use four text classification datasets with varying levels of granularity among the classes. Banking77 (B77) (Casanueva et al., 2020) is a single-domain dataset with 77 banking-related classes, where the difference between multiple classes is nuanced. The fine-grained nature combined with a high number of target classes makes Banking77 a good test\nbed for verifying the scalability and effectiveness of our approach. The following three datasets have coarse labels but cover a variety of domains, allowing us to test the adaptability of our method across different domains. TREC6 (Voorhees et al., 1999) is a question classification dataset with six broad classes of questions in English. The subjectivity dataset (SUBJ) (Pang and Lee, 2004) contains movie reviews with objectivity labels. Lastly, the twitter complaints dataset (TC) (Preot\u0327iuc-Pietro et al., 2019) contains tweets annotated by whether they contain a complaint or not. We refer the reader to Table 2 for exact statistics of all the datasets."
        },
        {
            "heading": "5.2 Few-shot Setup",
            "text": "For our experiments, we consider 1) a 2-shot setup, where only k = 2 training examples are available for every class, and 2) a zero-shot setup, where we do not have access to any training examples.\nNotations. We will use Dpart to refer to the dataset parts, i.e., train, validation, and test. When augmenting the training data using any method, we generate N examples per class and refer to the resulting data as D\u0303A,train (obtained after Step 1). We refer to the relabeled version of the resulting data (obtained after Step 2) as D\u0303A+R,train."
        },
        {
            "heading": "5.3 Training and Evaluation",
            "text": "Training. We fine-tune DistilBERTbase and BERTbase models for text classification by adding a linear layer on top of the [CLS] token (Wolf et al., 2019). In all the setups, we fine-tune the classifier for 5 epochs. We use a learning rate of 6\u00d710\u22125 and weight decay of 1\u00d7 10\u22123 for B77 and a learning rate of 4\u00d710\u22125 and weight decay of 1\u00d710\u22122 for all the other datasets. Finally, we generate N = 50 examples per class for B77 and TREC6 and N = 100 examples per class for SUBJ and TC. We used the validation sets of TREC6 and SUBJ to choose the specific N values as they provide a good cost-toperformance ratio. We chose TREC6 over B77 to\nminimize our costs for using GPT3.5-turbo.\nWe perform all hyperparameter tuning using DistilBERTbase on B77 and TREC6. We limit our tuning to the two datasets to obtain two sets of hyperparameters: one for large-scale datasets like B77 and another for small-scale datasets like TREC6, SUBJ, and TC. Additionally, we use the same set of hyperparameters for the BERTbase classification model. We use the full validation set for tuning instead of a few-shot one to avoid issues with unstable hyperparameters.\nWe run experiments for the following scenarios: 1) Baseline (2-shot). All the classes are reduced to 2 examples per class, and we fine-tune a DistilBERTbase/BERTbase model on the reduced dataset. 2) NN+GPT3.5. We use the nearestneighbor approach to populate the prompt as described in Section 3.2 and then prompt GPT3.5turbo to classify the test set examples (see Figure 4 for reference). 3) Sahu et al. (2022). A prompting-based approach for data augmentation that lists all the seed examples for a single class in the prompt and prompts the LLM to generate more examples based on it. It does not promote mixup in the generated examples. 4) PromptMix. Our proposed approach with multiple classes in the prompt, instructing the LLM to generate borderline examples. 5) PromptMix (zero-shot). We remove seed examples from PromptMix but still use the manually written class descriptions. 6) Easy Data Augmentation (EDA). An edit-based augmentation technique proposed by Wei and Zou (2019) that applies rule-based changes to existing training examples to generate additional examples. 7) GPT3Mix. A mixup-based augmentation method using soft labels for pseudolabeling proposed by Yoo et al. (2021). Notably, Yoo et al. (2021) measure the degree of few-shot in terms of the percentage of training examples used for augmentation (sub-sample percentage) and experiment with different percentages. We report their best-performing model for a sub-sample percentage of 1%, where GPT3Mix uses 55 training examples in TREC6 and 80 training examples in SUBJ (combined for all the classes). 8) CPFT. Zhang et al. (2021) use contrastive pre-training before fine-tuning a model for few-shot intent detection using RoBERTabase. 9) USE. A large multilingual model pretrained on 16 languages (Yang et al., 2020). 10) CONVERT. An intent detection model from Casanueva et al. (2020), which uses a dual encoder setup pretrained\non 654 million Reddit sentences. Several works in the past have explored 5-shot, 10-shot, and even 100-shot settings for data augmentation (Kumar et al., 2020; Zhang et al., 2021). But, to the best of our knowledge, we are the first to explore data augmentation in 2-shot and zeroshot settings. Through our experiments, we aim to measure the extent of data scarcity that an LLM like GPT3.5-turbo can handle, which is optimized to follow instructions. Evaluation. We measure the performance of the classifiers in terms of test classification accuracy and report the full set of results in Table 3. To note, A1 denotes the accuracy of the classifier on Dtrain \u222a DA,train (augmented dataset after Step 1 in Section 3.1) and A2 denotes the accuracy of the classifier on Dtrain \u222a DA+R,train (augmented+relabeled dataset after Step 2 in Section 3.2). We run each experiment for three random seeds and report the mean accuracy in Table 3."
        },
        {
            "heading": "6 Results",
            "text": "Referring to Table 3, we first note that A1 performs significantly better than baseline (2-shot) for DistilBERTbase and BERTbase across all four datasets. This confirms that data augmentation is helpful in data-scarce setups."
        },
        {
            "heading": "6.1 On the Effect of Relabeling Step",
            "text": "Table 5 shows the percentage of generated examples relabeled by GPT3.5-turbo for different augmentation methods. We note that relabeling percentage is higher when we add mixup to the prompt (PromptMix v/s PromptMix w/o Mixup). High relabeling percentages suggest that mixup generates more borderline examples than any other augmentation method. This verifies the importance of the relabeling step in our method, which is highly effective in remedying the problem of false positive generations. This is further demonstrated by higher A2 values than A1 across the board. Specifically, for PromptMix, we observe an improvement of 7.4% on B77, 8.1% on TREC6, 12.4% on SUBJ, and 13.5% on TC, when we use DistilBERTbase; and 5.9% on B77, 10.4% on TREC6, 14.4% on SUBJ, and 11.6% on TC when we use BERTbase. In addition to these significant improvements, we note that the degree of improvement increases as the classification task gets easier in terms of the number of target classes. Table 6 shows some examples of leaked generations that GPT rectifies\nduring the relabeling step."
        },
        {
            "heading": "6.2 Borderline Examples Aid Knowledge Distillation",
            "text": "In Table 3, we notice that PromptMix achieves almost similar performance as NN+GPT3.5 on three out of four datasets. Specifically, PromptMix outperforms NN+GPT3.5 on B77 (80.1 v/s\n79.9) and SUBJ (91.7 v/s 90.4) and is competitive on TREC6 (73.7 v/s 74.4). The gap between PromptMix and NN+GPT3.5 is larger on TC compared to other datasets (78.4 v/s 88.6). This might be due to the nature of the dataset, which contains extensive use of social media language that GPT knows about, but two examples might be too few to cover the vast diversity of complaints in the wild. Overall, these results are promising as GPT3.5-turbo with 175B parameters is \u223c 2600\u00d7 larger than DistilBERTbase and \u223c 1600\u00d7 larger than BERTbase, which only have 67M and 110M parameters, respectively. In both cases, our final classifiers are > 99.9% smaller than GPT3.5turbo. We do not see such strong performance for any other generation method, even after relabeling. This confirms that generating borderline examples in PromptMix makes for a high-quality dataset generation method that aids the transfer of knowledge of large-scale models such as GPT3.5-turbo into much smaller classifiers in data-scarce settings."
        },
        {
            "heading": "6.3 PromptMix v/s Other Augmentation Methods",
            "text": "In Table 3, we compare the 2-shot performance of PromptMix against several strong baselines on the four datasets. First, we note that PromptMix is significantly better than the EDA baseline on all datasets. Next, we compare the results of PromptMix (which uses GPT3.5-turbo for generation) with\nGPT3Mix (which uses GPT3\u2019s Davinci model for generation) and LINDA as they are the closest methods to ours in terms of motivation. PromptMix outperforms both LINDA and GPT3Mix by a huge margin even though LINDA is 5-shot and uses pretraining, and GPT3Mix utilizes 1% of training samples for augmentation, translating to 55 total seed examples compared to our 12 seed examples on TREC6, and 80 seed examples compared to our 4 seed examples on the SUBJ dataset.\nTable 4 shows that 2-shot PromptMix outperforms USE, CONVERT, and USE+CONVERT on the Banking77 dataset. It also achieves an accuracy of 80.1% compared to 80.9% by CPFT, which is highly competitive. To emphasize, all the considered baselines are 5-shot, with some models using additional data for pretraining. Moreover, it is possible to further improve the performance of PromptMix with more dataset- and model- specific hyperparameter-tuning; however, our core message is to show that generating borderline examples combined with relabeling is a highly promising method for data augmentation in 2-shot and zero-shot text classification setups."
        },
        {
            "heading": "6.4 Descriptions are Impactful",
            "text": "Our experiments that were used to evaluate the effectiveness of using human-written descriptions show promising results. First, we note that simply\nadding a class description to the prompt leads to decent performance gains on all the datasets (Sahu et al. (2022) w/ and w/o desc). Comparing PromptMix (zero-shot) with and without Mixup against Sahu et al. (2022) + desc. shows that using only descriptions in the prompt (no seed examples) leads to a significant boost in model performance. We also observe that using multiple class descriptions in a prompt is either better or equivalent to using a single class with its description and examples. This suggests that providing more context about other classes helps LLM generate better quality augmentations for a given class. Therefore, it follows intuitively that PromptMix, which combines the usage of multiple classes with descriptions as well as seed examples, leads to the best performance on all the datasets."
        },
        {
            "heading": "6.5 Open-source v/s Closed-source models",
            "text": "Table 3 and 4 showcase strong capabilities of GPT3.5-turbo for our task. However, GPT3.5turbo is a closed-source language model provided by OpenAI, and it can quickly get costly as we increase the number of classes in the dataset. Therefore, we conduct a small-scale experiment on the TREC6 dataset with open-source LLMs. In particular, we prompt open-source LLMs instead of GPT3.5-turbo to synthesize new examples and to relabel them. Next, we finetune a BERTbase classi-\nfier on the augmented+relabeled dataset. GPT-Neo (1.3B), GPT-J (6B), and the more recent instruction-tuned Vicuna and Stable-vicuna (13B) (Chiang et al., 2023) models achieve very poor performance compared to GPT3.5-turbo. We find that even for medium-sized LLMs like Vicuna and Stable-vicuna, inference time is a major bottleneck 5. However, our experiments with LLama-2 show promising results. We used different-sized Llama-2 models (7b, 13b, 70b) on the TREC6 dataset and observe a strong correlation between model size and test accuracy. In particular, we note that LLama-2-70b-chat-hf might be a decent alternative to the closed-source GPT3.5-turbo model. We also test GPT-4 (larger than the GPT3.5 model) and observe a significant boost in classification accuracy. Table 7 shows the results 6."
        },
        {
            "heading": "7 Conclusion",
            "text": "To conclude, we propose PromptMix, a novel two-step prompting-based method for generating borderline augmented examples in extreme fewshot text classification setups. Our method combines the process of text augmentation, pseudolabeling, and knowledge distillation in a cohesive pipeline. We show that by generating borderline training examples and relabeling them using a large teacher model like GPT3.5-turbo, we can transfer the knowledge of such massive LLMs into much smaller models like DistilBERTbase and BERTbase. Furthermore, 2-shot PromptMix beats multiple 5- shot or higher data augmentation baselines, making\n5we tried on a 32G V100 with 8-bit quantization 6We use Anyscale endpoints to access Llama-2 models.\nit a highly promising data augmentation approach.\nLimitations\nOur results indicate a promising potential to use LLMs for generating data in highly-aggressive fewshot setups; however, this work has a few limitations, as detailed in this section. First, we rely completely on GPT\u2019s pseudolabels to tackle false positive generations during the augmentation step. Secondly, since we use SBERT embeddings to ensure that pseudolabel is a valid class in the dataset, we ignore potential out-of-scope/out-of-domain (OOS/OOD) generations. We did observe a few instances where GPT pseudolabels were of the form, \u201cThis sentence does not belong to any of the provided classes.\" This presents a good avenue to introduce human interventions that can judge GPT pseudolabels and can identify OOS/OOD examples, which can be helpful in many real-life tasks. We also want to emphasize that while human interventions can greatly help, they also present the challenge of minimizing human labor.\nSecond, while our experiments with open-source models suggest that the larger open-source models like LLama-2-70b might be a good alternative to closed-source models like GPT3.5, thereby greatly cutting API costs, they still demand significant computational resources. Therefore, efforts towards distilling the knowledge of bigger LLMs like Llama-2-70b into smaller models would greatl aid in making these models more accessible and feasible for use.\nEthics Statement\nWe use GPT to generate new examples, and even though GPT3.5-turbo is instruction-tuned, some generations might depict undesirable biases with respect to the current objective. For the stated reason, we recommend using our proposed classification models with human monitoring to avoid any ethical issues due to false positive predictions of the downstream classifiers. Practitioners may also consider explicitly debiasing language models for their specific use cases (Barikeri et al., 2021; Schick et al., 2021)."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Distribution of \u03b1 for Mixup We use \u03b1 = round(10(x+ 1))/20 \u2200 x \u223c \u03b2(5, 2) in our experiments. We modify the standard \u03b2\u2212distribution by restricting its range to the half interval of (0.5, 1.0] with a peak near 1.0. We choose \u03b1 > 0.5 to incentivize the LLM to generate examples that are mixed up but still belong to ci. We round off the \u03b1 to the nearest 0.05 to avoid decimal values that would be arbitrary, given we are operating with natural language instructions.\nA.2 Choice of t We experiment with different values of t (classes to include in the prompt) and choose t = 4 based on the validation performance in Table 8."
        }
    ],
    "title": "PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation",
    "year": 2023
}