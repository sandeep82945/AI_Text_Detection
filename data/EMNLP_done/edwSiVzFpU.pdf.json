{
    "abstractText": "End-to-end task-oriented dialogue (EToD) can directly generate responses in an end-to-end fashion without modular training, which attracts escalating popularity. The advancement of deep neural networks, especially the successful use of large pre-trained models, has further led to significant progress in EToD research in recent years. In this paper, we present a thorough review and provide a unified perspective to summarize existing approaches as well as recent trends to advance the development of EToD research. The contributions of this paper can be summarized: (1) First survey: to our knowledge, we take the first step to present a thorough survey of this research field; (2) New taxonomy: we first introduce a unified perspective for EToD, including (i) Modularly EToD and (ii) Fully EToD; (3) New Frontiers: we discuss some potential frontier areas as well as the corresponding challenges, hoping to spur breakthrough research in EToD field; (4) Abundant resources: we build a public website1, where EToD researchers could directly access the recent progress. We hope this work can serve as a thorough reference for the EToD research community.",
    "authors": [
        {
            "affiliations": [],
            "name": "Libo Qin"
        },
        {
            "affiliations": [],
            "name": "Wenbo Pan"
        },
        {
            "affiliations": [],
            "name": "Qiguang Chen"
        },
        {
            "affiliations": [],
            "name": "Lizi Liao"
        },
        {
            "affiliations": [],
            "name": "Zhou Yu"
        },
        {
            "affiliations": [],
            "name": "Yue"
        },
        {
            "affiliations": [],
            "name": "Wanxiang Che"
        },
        {
            "affiliations": [],
            "name": "Min Li"
        }
    ],
    "id": "SP:a6ed7cf6409019996ebae7f18c52d954b3455a08",
    "references": [
        {
            "authors": [
                "Vevake Balaraman",
                "Seyedmostafa Sheikhalishahi",
                "Bernardo Magnini."
            ],
            "title": "Recent neural methods on dialogue state tracking for task-oriented dialogue systems: A survey",
            "venue": "Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse",
            "year": 2021
        },
        {
            "authors": [
                "Vevake Balaraman",
                "Seyedmostafa Sheikhalishahi",
                "Bernardo Magnini."
            ],
            "title": "Recent neural methods on dialogue state tracking for task-oriented dialogue systems: A survey",
            "venue": "Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse",
            "year": 2021
        },
        {
            "authors": [
                "Siqi Bao",
                "Huang He",
                "Fan Wang",
                "Hua Wu",
                "Haifeng Wang",
                "Wenquan Wu",
                "Zhen Guo",
                "Zhibin Liu",
                "Xinchao Xu"
            ],
            "title": "PLATO-2: Towards Building an OpenDomain Chatbot via Curriculum Learning",
            "year": 2021
        },
        {
            "authors": [
                "Antoine Bordes",
                "Y.-Lan Boureau",
                "Jason Weston."
            ],
            "title": "Learning End-to-End Goal-Oriented Dialog",
            "venue": "arXiv:1605.07683 [cs].",
            "year": 2017
        },
        {
            "authors": [
                "Pawe\\l Budzianowski",
                "Ivan Vuli\u0107"
            ],
            "title": "Hello, It\u2019s GPT-2 - How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems",
            "venue": "In Proceedings of the 3rd Workshop on Neural Generation and Translation,",
            "year": 2019
        },
        {
            "authors": [
                "Pawe\\l Budzianowski",
                "Tsung-Hsien Wen",
                "Bo-Hsiang Tseng",
                "I\u00f1igo Casanueva",
                "Stefan Ultes",
                "Osman Ramadan",
                "Milica Ga\u0161i\u0107"
            ],
            "title": "MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling",
            "year": 2018
        },
        {
            "authors": [
                "Hongshen Chen",
                "Xiaorui Liu",
                "Dawei Yin",
                "Jiliang Tang."
            ],
            "title": "A survey on dialogue systems: Recent advances and new frontiers",
            "venue": "SIGKDD Explor. Newsl., 19(2):25\u201335.",
            "year": 2017
        },
        {
            "authors": [
                "Lu Chen",
                "Zhi Chen",
                "Bowen Tan",
                "Sishan Long",
                "Milica Gasic",
                "Kai Yu."
            ],
            "title": "AgentGraph: Toward universal dialogue management with structured deep reinforcement learning",
            "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc., 27(9):1378\u20131391.",
            "year": 2019
        },
        {
            "authors": [
                "Xiuyi Chen",
                "Jiaming Xu",
                "Bo Xu."
            ],
            "title": "A Working Memory Model for Task-oriented Dialog Response Generation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2687\u20132693, Florence, Italy. Association for",
            "year": 2019
        },
        {
            "authors": [
                "Zhi Chen",
                "Lu Chen",
                "Bei Chen",
                "Libo Qin",
                "Yuncong Liu",
                "Su Zhu",
                "Jian-Guang Lou",
                "Kai Yu."
            ],
            "title": "UniDU: Towards a unified generative dialogue understanding framework",
            "venue": "Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and",
            "year": 2022
        },
        {
            "authors": [
                "Yinpei Dai",
                "Hangyu Li",
                "Yongbin Li",
                "Jian Sun",
                "Fei Huang",
                "Luo Si",
                "Xiaodan Zhu."
            ],
            "title": "Preview, attend and review: Schema-aware curriculum learning for multi-domain dialogue state tracking",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Yinpei Dai",
                "Huihua Yu",
                "Yixuan Jiang",
                "Chengguang Tang",
                "Yongbin Li",
                "Jian Sun."
            ],
            "title": "A survey on dialog management: Recent advances and challenges",
            "venue": "arXiv preprint arXiv:2005.02233.",
            "year": 2020
        },
        {
            "authors": [
                "Bhuwan Dhingra",
                "Lihong Li",
                "Xiujun Li",
                "Jianfeng Gao",
                "Yun-Nung Chen",
                "Faisal Ahmed",
                "Li Deng."
            ],
            "title": "Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for",
            "year": 2017
        },
        {
            "authors": [
                "Bosheng Ding",
                "Junjie Hu",
                "Lidong Bing",
                "Mahani Aljunied",
                "Shafiq Joty",
                "Luo Si",
                "Chunyan Miao."
            ],
            "title": "GlobalWoZ: Globalizing MultiWoZ to develop multilingual task-oriented dialogue systems",
            "venue": "Proceedings of the 60th Annual Meeting of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Li Dong",
                "Nan Yang",
                "Wenhui Wang",
                "Furu Wei",
                "Xiaodong Liu",
                "Yu Wang",
                "Jianfeng Gao",
                "Ming Zhou",
                "HsiaoWuen Hon"
            ],
            "title": "Unified Language Model Pretraining for Natural Language Understanding",
            "year": 2019
        },
        {
            "authors": [
                "Vanhoucke",
                "Karol Hausman",
                "Marc Toussaint",
                "Klaus Greff",
                "Andy Zeng",
                "Igor Mordatch",
                "Peter R. Florence."
            ],
            "title": "Palm-e: An embodied multimodal language model",
            "venue": "International Conference on Machine Learning.",
            "year": 2023
        },
        {
            "authors": [
                "Haihong E.",
                "Wenjing Zhang",
                "Meina Song."
            ],
            "title": "KB-Transformer: Incorporating Knowledge into Endto-End Task-Oriented Dialog Systems",
            "venue": "2019 15th International Conference on Semantics, Knowledge and Grids (SKG), pages 44\u201348.",
            "year": 2019
        },
        {
            "authors": [
                "Mihail Eric",
                "Rahul Goel",
                "Shachi Paul",
                "Adarsh Kumar",
                "Abhishek Sethi",
                "Peter Ku",
                "Anuj Kumar Goyal",
                "Sanchit Agarwal",
                "Shuyang Gao",
                "Dilek HakkaniTur"
            ],
            "title": "MultiWOZ 2.1: A Consolidated MultiDomain Dialogue Dataset with State Corrections and",
            "year": 2019
        },
        {
            "authors": [
                "Mihail Eric",
                "Christopher D. Manning"
            ],
            "title": "KeyValue Retrieval Networks for Task-Oriented Dialogue",
            "year": 2017
        },
        {
            "authors": [
                "Yanjie Gou",
                "Yinjie Lei",
                "Lingqiao Liu",
                "Yong Dai",
                "Chunxu Shen."
            ],
            "title": "Contextualize Knowledge Bases with Transformer for End-to-end TaskOriented Dialogue Systems",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural",
            "year": 2021
        },
        {
            "authors": [
                "Jiatao Gu",
                "Zhengdong Lu",
                "Hang Li",
                "Victor O.K. Li"
            ],
            "title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning",
            "year": 2016
        },
        {
            "authors": [
                "Jinyu Guo",
                "Kai Shuang",
                "Jijie Li",
                "Zihan Wang",
                "Yixuan Liu."
            ],
            "title": "Beyond the granularity: Multiperspective dialogue collaborative selection for dialogue state tracking",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Donghoon Ham",
                "Jeong-Gwan Lee",
                "Youngsoo Jang",
                "Kee-Eung Kim."
            ],
            "title": "End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 583\u2013592,",
            "year": 2020
        },
        {
            "authors": [
                "Wanwei He",
                "Yinpei Dai",
                "Min Yang",
                "Jian Sun",
                "Fei Huang",
                "Luo Si",
                "Yongbin Li"
            ],
            "title": "2022a. SPACE-3: Unified Dialog Model Pre-training for Task-Oriented Dialog Understanding and Generation",
            "year": 2022
        },
        {
            "authors": [
                "Wanwei He",
                "Yinpei Dai",
                "Yinhe Zheng",
                "Yuchuan Wu",
                "Zheng Cao",
                "Dermot Liu",
                "Peng Jiang",
                "Min Yang",
                "Fei Huang",
                "Luo Si",
                "Jian Sun",
                "Yongbin Li"
            ],
            "title": "2022b. GALAXY: A Generative Pre-trained Model for TaskOriented Dialog with Semi-Supervised Learning",
            "year": 2022
        },
        {
            "authors": [
                "Wanwei He",
                "Min Yang",
                "Rui Yan",
                "Chengming Li",
                "Ying Shen",
                "Ruifeng Xu."
            ],
            "title": "Amalgamating Knowledge from Two Teachers for Task-oriented Dialogue System with Adversarial Training",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods",
            "year": 2020
        },
        {
            "authors": [
                "Zhenhao He",
                "Yuhong He",
                "Qingyao Wu",
                "Jian Chen."
            ],
            "title": "Fg2seq: Effectively Encoding Knowledge for End-To-End Task-Oriented Dialog",
            "venue": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8029\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Michael Heck",
                "Nurul Lubis",
                "Benjamin Matthias Ruppik",
                "Renato Vukovic",
                "Shutong Feng",
                "Christian Geishauser",
                "Hsien chin Lin",
                "Carel van Niekerk",
                "Milica Gavsi\u2019c"
            ],
            "title": "Chatgpt for zero-shot dialogue state tracking: A solution or an opportunity? ArXiv",
            "year": 2023
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber."
            ],
            "title": "Long Short-term Memory",
            "venue": "Neural computation, 9:1735\u2013",
            "year": 1997
        },
        {
            "authors": [
                "Teakgyu Hong",
                "Oh-Woog Kwon",
                "Young-Kil Kim"
            ],
            "title": "End-to-End Task-Oriented Dialog System Through Template Slot Value Generation",
            "venue": "In Interspeech",
            "year": 2020
        },
        {
            "authors": [
                "Ehsan Hosseini-Asl",
                "Bryan McCann",
                "Chien-Sheng Wu",
                "Semih Yavuz",
                "Richard Socher"
            ],
            "title": "A Simple Language Model for Task-Oriented Dialogue",
            "year": 2020
        },
        {
            "authors": [
                "Guanhuan Huang",
                "Xiaojun Quan",
                "Qifan Wang"
            ],
            "title": "Autoregressive Entity Generation for Endto-End Task-Oriented Dialog",
            "year": 2022
        },
        {
            "authors": [
                "Vojtvech Hudevcek",
                "Ondrej Dusek"
            ],
            "title": "Are large language models all you need for task-oriented dialogue",
            "venue": "In SIGDIAL Conferences",
            "year": 2023
        },
        {
            "authors": [
                "L\u00e9o Jacqmin",
                "Lina M Rojas-Barahona",
                "Benoit Favre."
            ],
            "title": " Do you follow me?\": A survey of recent approaches in dialogue state tracking",
            "venue": "arXiv preprint arXiv:2207.14627.",
            "year": 2022
        },
        {
            "authors": [
                "L\u00e9o Jacqmin",
                "Lina M. Rojas Barahona",
                "Benoit Favre."
            ],
            "title": "do you follow me?\u201d: A survey of recent approaches in dialogue state tracking",
            "venue": "Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages",
            "year": 2022
        },
        {
            "authors": [
                "Satwik Kottur",
                "Seungwhan Moon",
                "Alborz Geramifard",
                "Babak Damavandi"
            ],
            "title": "SIMMC 2.0: A taskoriented dialog dataset for immersive multimodal conversations",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2021
        },
        {
            "authors": [
                "Jon\u00e1\u0161 Kulh\u00e1nek",
                "Vojt\u011bch Hude\u010dek",
                "Tom\u00e1\u0161 Nekvinda",
                "Ond\u0159ej Du\u0161ek."
            ],
            "title": "AuGPT: Auxiliary Tasks and Data Augmentation for End-To-End Dialogue with Pre-Trained Language Models",
            "venue": "Proceedings of the 3rd Workshop on Natural Language Processing",
            "year": 2021
        },
        {
            "authors": [
                "Wai-Chung Kwan",
                "Hongru Wang",
                "Huimin Wang",
                "Kam-Fai Wong."
            ],
            "title": "A survey on recent advances and challenges in reinforcement LearningMethods for task-oriented dialogue policy learning",
            "venue": "arXiv preprint arXiv:2202.13675.",
            "year": 2022
        },
        {
            "authors": [
                "Stefan Larson",
                "Kevin Leach"
            ],
            "title": "A survey of intent classification and slot-filling datasets for taskoriented dialog",
            "year": 2022
        },
        {
            "authors": [
                "Hung Le",
                "Doyen Sahoo",
                "Chenghao Liu",
                "Nancy Chen",
                "Steven C.H. Hoi."
            ],
            "title": "UniConv: A Unified Conversational Neural Architecture for Multi-domain Task-oriented Dialogues",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Yohan Lee."
            ],
            "title": "Improving End-to-End Task-Oriented Dialog System with A Simple Auxiliary Task",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1296\u20131303, Punta Cana, Dominican Republic. Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Wenqiang Lei",
                "Xisen Jin",
                "Min-Yen Kan",
                "Zhaochun Ren",
                "Xiangnan He",
                "Dawei Yin."
            ],
            "title": "Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation",
            "year": 2019
        },
        {
            "authors": [
                "Xiujun Li",
                "Yun-Nung Chen",
                "Lihong Li",
                "Jianfeng Gao",
                "Asli Celikyilmaz"
            ],
            "title": "End-to-End TaskCompletion Neural Dialogue Systems",
            "year": 2018
        },
        {
            "authors": [
                "Yangming Li",
                "Kaisheng Yao",
                "Libo Qin",
                "Wanxiang Che",
                "Xiaolong Li",
                "Ting Liu."
            ],
            "title": "Slot-consistent NLG for task-oriented dialogue systems with iterative rectification network",
            "venue": "Proceedings of the 58th",
            "year": 2020
        },
        {
            "authors": [
                "Yu Li",
                "Kun Qian",
                "Weiyan Shi",
                "Zhou Yu."
            ],
            "title": "Endto-End Trainable Non-Collaborative Dialog System",
            "venue": "arXiv:1911.10742 [cs].",
            "year": 2019
        },
        {
            "authors": [
                "Weixin Liang",
                "Youzhi Tian",
                "Chengcai Chen",
                "Zhou Yu"
            ],
            "title": "MOSS: End-to-End Dialog System Framework with Modular Supervision",
            "year": 2019
        },
        {
            "authors": [
                "Lizi Liao",
                "Le Hong Long",
                "Zheng Zhang",
                "Minlie Huang",
                "Tat-Seng Chua."
            ],
            "title": "MMConv: An Environment for Multimodal Conversational Search across Multiple Domains",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and",
            "year": 2021
        },
        {
            "authors": [
                "Zhaojiang Lin",
                "Andrea Madotto",
                "Genta Indra Winata",
                "Pascale Fung."
            ],
            "title": "MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems",
            "venue": "arXiv:2009.12005 [cs].",
            "year": 2020
        },
        {
            "authors": [
                "Zhaojiang Lin",
                "Andrea Madotto",
                "Genta Indra Winata",
                "Peng Xu",
                "Feijun Jiang",
                "Yuxiang Hu",
                "Chen Shi",
                "Pascale Fung."
            ],
            "title": "Bitod: A bilingual multi-domain dataset for task-oriented dialogue modeling",
            "venue": "arXiv preprint arXiv:2106.02787.",
            "year": 2021
        },
        {
            "authors": [
                "Bing Liu",
                "Ian Lane"
            ],
            "title": "An End-to-End Trainable Neural Network Model with Belief Tracking for TaskOriented Dialog",
            "year": 2017
        },
        {
            "authors": [
                "Bing Liu",
                "Ian Lane."
            ],
            "title": "End-to-End Learning of Task-Oriented Dialogs",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 67\u201373, New Orleans,",
            "year": 2018
        },
        {
            "authors": [
                "Bing Liu",
                "Gokhan Tur",
                "Dilek Hakkani-Tur",
                "Pararth Shah",
                "Larry Heck"
            ],
            "title": "End-to-End Optimization of Task-Oriented Dialogue Model with Deep Reinforcement Learning",
            "year": 2017
        },
        {
            "authors": [
                "Bing Liu",
                "Gokhan T\u00fcr",
                "Dilek Hakkani-T\u00fcr",
                "Pararth Shah",
                "Larry Heck."
            ],
            "title": "Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems",
            "venue": "Proceedings of the 2018 Conference of the North Amer-",
            "year": 2018
        },
        {
            "authors": [
                "Nelson F. Liu",
                "Kevin Lin",
                "John Hewitt",
                "Ashwin Paranjape",
                "Michele Bevilacqua",
                "Fabio Petroni",
                "Percy Liang."
            ],
            "title": "Lost in the middle: How language models use long contexts",
            "venue": "ArXiv, abs/2307.03172.",
            "year": 2023
        },
        {
            "authors": [
                "Samuel Louvan",
                "Bernardo Magnini."
            ],
            "title": "Recent neural methods on slot filling and intent classification for task-oriented dialogue systems: A survey",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, pages 480\u2013496,",
            "year": 2020
        },
        {
            "authors": [
                "Nurul Lubis",
                "Christian Geishauser",
                "Michael Heck",
                "Hsien-chin Lin",
                "Marco Moresi",
                "Carel van Niekerk",
                "Milica Ga\u0161i\u0107"
            ],
            "title": "LAVA: Latent Action Spaces via Variational Auto-encoding for Dialogue Policy Optimization",
            "year": 2020
        },
        {
            "authors": [
                "Zhiyuan Ma",
                "Jianjun Li",
                "Zezheng Zhang",
                "Guohui Li",
                "Yongjing Cheng."
            ],
            "title": "Intention Reasoning Network for Multi-Domain End-to-end Task-Oriented Dialogue",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-",
            "year": 2021
        },
        {
            "authors": [
                "Andrea Madotto",
                "Samuel Cahyawijaya",
                "Genta Indra Winata",
                "Yan Xu",
                "Zihan Liu",
                "Zhaojiang Lin",
                "Pascale Fung."
            ],
            "title": "Learning Knowledge Bases with Parameters for Task-Oriented Dialogue Systems",
            "venue": "page 23.",
            "year": 2021
        },
        {
            "authors": [
                "Andrea Madotto",
                "Chien-Sheng Wu",
                "Pascale Fung"
            ],
            "title": "Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems",
            "year": 2018
        },
        {
            "authors": [
                "Shikib Mehri",
                "Tejas Srinivasan",
                "Maxine Eskenazi."
            ],
            "title": "Structured Fusion Networks for Dialog",
            "venue": "arXiv:1907.10016 [cs].",
            "year": 2019
        },
        {
            "authors": [
                "Jinjie Ni",
                "Tom Young",
                "Vlad Pandelea",
                "Fuzhao Xue",
                "Erik Cambria."
            ],
            "title": "Recent advances in deep learning based dialogue systems: A systematic survey",
            "venue": "Artificial intelligence review, 56(4):3055\u20133155.",
            "year": 2023
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "ArXiv, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Wenbo Pan",
                "Qiguang Chen",
                "Xiao Xu",
                "Wanxiang Che",
                "Libo Qin."
            ],
            "title": "A preliminary evaluation of chatgpt for zero-shot dialogue understanding",
            "venue": "ArXiv, abs/2304.04256.",
            "year": 2023
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: A Method for Automatic Evaluation of Machine Translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Soham Parikh",
                "Quaizar Vohra",
                "Prashil Tumbade",
                "Mitul Tiwari."
            ],
            "title": "Exploring zero and few-shot techniques for intent classification",
            "venue": "Annual Meeting of the Association for Computational Linguistics.",
            "year": 2023
        },
        {
            "authors": [
                "Joon Sung Park",
                "Joseph C. O\u2019Brien",
                "Carrie J. Cai",
                "Meredith Ringel Morris",
                "Percy Liang",
                "Michael S. Bernstein"
            ],
            "title": "Generative agents: Interactive simulacra of human behavior. ArXiv, abs/2304.03442",
            "year": 2023
        },
        {
            "authors": [
                "Shishir G. Patil",
                "Tianjun Zhang",
                "Xin Wang",
                "Joseph E. Gonzalez."
            ],
            "title": "Gorilla: Large language model connected with massive apis",
            "venue": "ArXiv, abs/2305.15334.",
            "year": 2023
        },
        {
            "authors": [
                "Baolin Peng",
                "Michel Galley",
                "Pengcheng He",
                "Chris Brockett",
                "Lars Liden",
                "Elnaz Nouri",
                "Zhou Yu",
                "Bill Dolan",
                "Jianfeng Gao."
            ],
            "title": "Godel: Large-scale pre-training for goal-directed dialog",
            "venue": "arXiv preprint arXiv:2206.11309.",
            "year": 2022
        },
        {
            "authors": [
                "Baolin Peng",
                "Chunyuan Li",
                "Jinchao Li",
                "Shahin Shayandeh",
                "Lars Liden",
                "Jianfeng Gao."
            ],
            "title": "Soloist : BuildingTask Bots at Scale with Transfer Learning and Machine Teaching",
            "venue": "Transactions of the Association for Computational Linguistics, 9:807\u2013824.",
            "year": 2021
        },
        {
            "authors": [
                "Bowen Qin",
                "Min Yang",
                "Lidong Bing",
                "Qingshan Jiang",
                "Chengming Li",
                "Ruifeng Xu."
            ],
            "title": "Exploring Auxiliary Reasoning Tasks for Task-oriented Dialog Systems with Meta Cooperative Learning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Libo Qin",
                "Wanxiang Che",
                "Yangming Li",
                "Haoyang Wen",
                "Ting Liu."
            ],
            "title": "A stack-propagation framework with token-level intent detection for spoken language understanding",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Hong Kong",
                "China"
            ],
            "title": "Association for Computational Linguistics",
            "venue": "cessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 2087
        },
        {
            "authors": [
                "Libo Qin",
                "Zhouyang Li",
                "Qiying Yu",
                "Lehan Wang",
                "Wanxiang Che."
            ],
            "title": "Towards complex scenarios: Building end-to-end task-oriented dialogue system across multiple knowledge bases",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Libo Qin",
                "Tailu Liu",
                "Wanxiang Che",
                "Bingbing Kang",
                "Sendong Zhao",
                "Ting Liu."
            ],
            "title": "A co-interactive transformer for joint slot filling and intent detection",
            "venue": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing",
            "year": 2021
        },
        {
            "authors": [
                "Libo Qin",
                "Yijia Liu",
                "Wanxiang Che",
                "Haoyang Wen",
                "Yangming Li",
                "Ting Liu."
            ],
            "title": "EntityConsistent End-to-end Task-Oriented Dialogue System with KB Retriever",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Libo Qin",
                "Tianbao Xie",
                "Wanxiang Che",
                "Ting Liu"
            ],
            "title": "2021c. A survey on spoken language understanding",
            "year": 2021
        },
        {
            "authors": [
                "Libo Qin",
                "Xiao Xu",
                "Wanxiang Che",
                "Ting Liu."
            ],
            "title": "AGIF: An adaptive graph-interactive framework for joint multiple intent detection and slot filling",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1807\u20131816, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Libo Qin",
                "Xiao Xu",
                "Wanxiang Che",
                "Yue Zhang",
                "Ting Liu."
            ],
            "title": "Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog",
            "venue": "arXiv:2004.11019 [cs].",
            "year": 2020
        },
        {
            "authors": [
                "Libo Qin",
                "Xiao Xu",
                "Lehan Wang",
                "Yue Zhang",
                "Wanxiang Che."
            ],
            "title": "Modularized pre-training for endto-end task-oriented dialogue",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:1601\u20131610.",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "year": 2020
        },
        {
            "authors": [
                "Dinesh Raghu",
                "Nikhil Gupta",
                "Mausam"
            ],
            "title": "Disentangling Language and Knowledge in TaskOriented Dialogs",
            "year": 2019
        },
        {
            "authors": [
                "Dinesh Raghu",
                "Atishya Jain",
                "Mausam",
                "Sachindra Joshi."
            ],
            "title": "Constraint based Knowledge Base Distillation in End-to-End Task Oriented Dialogs",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 5051\u20135061, On-",
            "year": 2021
        },
        {
            "authors": [
                "Janarthanan Rajendran",
                "Jatin Ganhotra",
                "Lazaros C. Polymenakos."
            ],
            "title": "Learning End-to-End GoalOriented Dialog with Maximal User Task Success and Minimal Human Agent Use",
            "venue": "Transactions of the Association for Computational Linguistics, 7:375\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Revanth Reddy",
                "Danish Contractor",
                "Dinesh Raghu",
                "Sachindra Joshi"
            ],
            "title": "Multi-level Memory for Task Oriented Dialogs",
            "year": 2018
        },
        {
            "authors": [
                "Md Rashad Al Hasan Rony",
                "Ricardo Usbeck",
                "Jens Lehmann"
            ],
            "title": "DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation",
            "year": 2022
        },
        {
            "authors": [
                "Sashank Santhanam",
                "Samira Shaikh."
            ],
            "title": "A survey of natural language generation techniques with a focus on dialogue systems - past, present and future directions",
            "venue": "CoRR, abs/1906.00500.",
            "year": 2019
        },
        {
            "authors": [
                "Lei Shu",
                "Piero Molino",
                "Mahdi Namazifar",
                "Hu Xu",
                "Bing Liu",
                "Huaixiu Zheng",
                "Gokhan Tur"
            ],
            "title": "FlexiblyStructured Model for Task-Oriented Dialogues",
            "year": 2019
        },
        {
            "authors": [
                "Jason Weston"
            ],
            "title": "BlenderBot 3: A deployed conversational agent that continually learns to responsibly engage",
            "year": 2022
        },
        {
            "authors": [
                "Yixuan Su",
                "Lei Shu",
                "Elman Mansimov",
                "Arshit Gupta",
                "Deng Cai",
                "Yi-An Lai",
                "Yi Zhang."
            ],
            "title": "MultiTask Pre-Training for Plug-and-Play Task-Oriented Dialogue System",
            "venue": "arXiv:2109.14739 [cs].",
            "year": 2021
        },
        {
            "authors": [
                "Sainbayar Sukhbaatar",
                "Arthur Szlam",
                "Jason Weston",
                "Rob Fergus"
            ],
            "title": "End-To-End Memory Networks",
            "year": 2015
        },
        {
            "authors": [
                "Haipeng Sun",
                "Junwei Bao",
                "Youzheng Wu",
                "Xiaodong He"
            ],
            "title": "BORT: Back and Denoising Reconstruction for End-to-End Task-Oriented Dialog",
            "year": 2022
        },
        {
            "authors": [
                "Xin Tian",
                "Yingzhan Lin",
                "Mengfei Song",
                "Fan Wang",
                "Huang He",
                "Shuqi Sun",
                "Hua Wu"
            ],
            "title": "Q-TOD: A Query-driven Task-oriented Dialogue System",
            "year": 2022
        },
        {
            "authors": [
                "lien Rodriguez",
                "Robert Stojnic",
                "Sergey Edunov",
                "Thomas Scialom"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288",
            "year": 2023
        },
        {
            "authors": [
                "Fanqi Wan",
                "Weizhou Shen",
                "Ke Yang",
                "Xiaojun Quan",
                "Wei Bi"
            ],
            "title": "Multi-grained knowledge retrieval for end-to-end task-oriented dialog",
            "year": 2023
        },
        {
            "authors": [
                "Jian Wang",
                "Junhao Liu",
                "Wei Bi",
                "Xiaojiang Liu",
                "Kejing He",
                "Ruifeng Xu",
                "Min Yang."
            ],
            "title": "Dual Dynamic Memory Network for End-to-End Multi-turn Taskoriented Dialog Systems",
            "venue": "Proceedings of the 28th International Conference on Computational Linguis-",
            "year": 2020
        },
        {
            "authors": [
                "Jianhong Wang",
                "Yuan Zhang",
                "Tae-Kyun Kim",
                "Yunjie Gu"
            ],
            "title": "Modelling Hierarchical Structure between Dialogue Policy and Natural Language Generator with Option Framework for Task-oriented Dialogue System",
            "year": 2021
        },
        {
            "authors": [
                "Weikang Wang",
                "Jiajun Zhang",
                "Qian Li",
                "Mei-Yuh Hwang",
                "Chengqing Zong",
                "Zhifei Li."
            ],
            "title": "Incremental Learning from Scratch for Task-Oriented Dialogue Systems",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Weizhi Wang",
                "Zhirui Zhang",
                "Junliang Guo",
                "Yinpei Dai",
                "Boxing Chen",
                "Weihua Luo."
            ],
            "title": "Task-Oriented Dialogue System as Natural Language Generation",
            "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Haoyang Wen",
                "Yijia Liu",
                "Wanxiang Che",
                "Libo Qin",
                "Ting Liu"
            ],
            "title": "Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue State Representation",
            "year": 2018
        },
        {
            "authors": [
                "Tsung-Hsien Wen",
                "Milica Ga\u0161i\u0107",
                "Dongho Kim",
                "Nikola Mrk\u0161i\u0107",
                "Pei-Hao Su",
                "David Vandyke",
                "Steve Young."
            ],
            "title": "Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking",
            "venue": "Proceedings of the 16th",
            "year": 2015
        },
        {
            "authors": [
                "Tsung-Hsien Wen",
                "David Vandyke",
                "Nikola Mrksic",
                "Milica Gasic",
                "Lina M. Rojas-Barahona",
                "Pei-Hao Su",
                "Stefan Ultes",
                "Steve Young"
            ],
            "title": "A Network-based End-to-End Trainable Task-oriented Dialogue System",
            "year": 2017
        },
        {
            "authors": [
                "Jason D. Williams",
                "Kavosh Asadi",
                "Geoffrey Zweig"
            ],
            "title": "Hybrid Code Networks: Practical and efficient end-to-end dialog control with supervised and reinforcement learning",
            "year": 2017
        },
        {
            "authors": [
                "Chien-Sheng Wu",
                "Richard Socher",
                "Caiming Xiong"
            ],
            "title": "Global-to-local Memory Pointer Networks for Task-Oriented Dialogue",
            "year": 2019
        },
        {
            "authors": [
                "Jie Wu",
                "Ian G Harris",
                "Hongzhi Zhao."
            ],
            "title": "GraphMemDialog: Optimizing End-to-End Task-Oriented Dialog Systems Using Graph Memory Networks",
            "venue": "page 9.",
            "year": 2021
        },
        {
            "authors": [
                "Qingyang Wu",
                "Yichi Zhang",
                "Yu Li",
                "Zhou Yu."
            ],
            "title": "Alternating Recurrent Dialog Model with Large-scale Pre-trained Language Models",
            "venue": "arXiv:1910.03756",
            "year": 2021
        },
        {
            "authors": [
                "Xiong",
                "Lingpeng Kong",
                "Rui Zhang",
                "Noah A. Smith",
                "Luke Zettlemoyer",
                "Tao Yu"
            ],
            "title": "UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models",
            "year": 2022
        },
        {
            "authors": [
                "Shiquan Yang",
                "Rui Zhang",
                "Sarah Erfani."
            ],
            "title": "GraphDialog: Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems",
            "venue": "arXiv:2010.01447 [cs].",
            "year": 2020
        },
        {
            "authors": [
                "Yunyi Yang",
                "Yunhao Li",
                "Xiaojun Quan"
            ],
            "title": "2020b. UBAR: Towards Fully End-to-End Task-Oriented Dialog Systems with GPT-2",
            "year": 2020
        },
        {
            "authors": [
                "Chenchen Ye",
                "Lizi Liao",
                "Fuli Feng",
                "Wei Ji",
                "TatSeng Chua."
            ],
            "title": "Structured and Natural Responses Co-generation for Conversational Search",
            "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information",
            "year": 2022
        },
        {
            "authors": [
                "Ya Zeng",
                "Li Wan",
                "Qiuhong Luo",
                "Mao Chen."
            ],
            "title": "A Hierarchical Memory Model for TaskOriented Dialogue System",
            "venue": "IEICE Trans. Inf. & Syst., E105.D(8):1481\u20131489.",
            "year": 2022
        },
        {
            "authors": [
                "Linhao Zhang",
                "Dehong Ma",
                "Xiaodong Zhang",
                "Xiaohui Yan",
                "Houfeng Wang."
            ],
            "title": "Graph lstm with context-gated mechanism for spoken language understanding",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9539\u20139546.",
            "year": 2020
        },
        {
            "authors": [
                "Yichi Zhang",
                "Zhijian Ou",
                "Huixin Wang",
                "Junlan Feng"
            ],
            "title": "2020b. A Probabilistic End-To-End TaskOriented Dialog Model with Latent Belief States towards Semi-Supervised Learning",
            "year": 2020
        },
        {
            "authors": [
                "Yichi Zhang",
                "Zhijian Ou",
                "Zhou Yu"
            ],
            "title": "TaskOriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context",
            "year": 2019
        },
        {
            "authors": [
                "Zheng Zhang",
                "Ryuichi Takanobu",
                "Minlie Huang",
                "Xiaoyan Zhu."
            ],
            "title": "Recent advances and challenges in task-oriented dialog systems",
            "venue": "Science China Technological Sciences, 63:2011 \u2013 2027.",
            "year": 2020
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Alex Smola."
            ],
            "title": "Automatic chain of thought prompting in large language models",
            "venue": "arXiv preprint arXiv:2210.03493.",
            "year": 2022
        },
        {
            "authors": [
                "Tiancheng Zhao",
                "Maxine Eskenazi."
            ],
            "title": "Towards End-to-End Learning for Dialog State Tracking and Management using Deep Reinforcement Learning",
            "venue": "Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,",
            "year": 2016
        },
        {
            "authors": [
                "Tiancheng Zhao",
                "Kaige Xie",
                "Maxine Eskenazi"
            ],
            "title": "Rethinking Action Spaces for Reinforcement Learning in End-to-end Dialog Agents with Latent Variable Models",
            "year": 2019
        },
        {
            "authors": [
                "Wen"
            ],
            "title": "2017) is a relatively small-scale restaurant domain dataset, which consists of 408/136/136 dialogues for training/validation/test",
            "year": 2017
        },
        {
            "authors": [
                "Budzianowski"
            ],
            "title": "2018) is one of the most widely used ToD dataset. It contains over 8,000 dialogue sessions and 7 different domains including: restaurant, hotel, attraction, taxi, train, hospital and police",
            "year": 2018
        },
        {
            "authors": [
                "Eric"
            ],
            "title": "2019) is an improved version of MultiWOZ2.0, where incorrect slot annotations and dialogue acts were fixed. A.1.2 Metrics The widely used metrics for modularly EToD",
            "year": 2019
        },
        {
            "authors": [
                "SMD Eric",
                "Manning"
            ],
            "title": "2017) proposed a Stanford Multi-turn Multi-domain Task-oriented Dialogue Dataset, which includes three domains: navigation, weather, and calendar",
            "year": 2017
        },
        {
            "authors": [
                "MultiWOZ2.1. Qin"
            ],
            "title": "2020b) introduces an extension of MultiWOZ2.1 where they annotate the corresponding KB for each dialogue",
            "venue": "Metrics Fully EToD adopts BLEU and Entity F1",
            "year": 2020
        },
        {
            "authors": [
                "Louvan",
                "Magnini"
            ],
            "title": "2021c) summarize the recent progress of neural-based models for SLU",
            "venue": "On DST, Balaraman et al. (2021b) and Jacqmin et al",
            "year": 2022
        },
        {
            "authors": [
                "guage generation (NLG",
                "Santhanam",
                "Shaikh"
            ],
            "title": "2019) provides a comprehensive overview of the past, present, and future directions of NLG",
            "venue": "Zhang et al. (2020c) and Ni et al",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "End-to-end task-oriented dialogue (EToD) can directly generate responses in an end-to-end fashion without modular training, which attracts escalating popularity. The advancement of deep neural networks, especially the successful use of large pre-trained models, has further led to significant progress in EToD research in recent years. In this paper, we present a thorough review and provide a unified perspective to summarize existing approaches as well as recent trends to advance the development of EToD research. The contributions of this paper can be summarized: (1) First survey: to our knowledge, we take the first step to present a thorough survey of this research field; (2) New taxonomy: we first introduce a unified perspective for EToD, including (i) Modularly EToD and (ii) Fully EToD; (3) New Frontiers: we discuss some potential frontier areas as well as the corresponding challenges, hoping to spur breakthrough research in EToD field; (4) Abundant resources: we build a public website1, where EToD researchers could directly access the recent progress. We hope this work can serve as a thorough reference for the EToD research community."
        },
        {
            "heading": "1 Introduction",
            "text": "Task-oriented dialogue systems (ToD) can assist users in achieving particular goals with natural language interaction such as booking a restaurant or navigation inquiry. This area is seeing growing interest in both academic research and indus-\n1We collect the related papers, baseline projects, and leaderboards for the community at https://etods.net/.\ntry deployment. As shown in Figure 1(a), conventional ToD systems utilize a pipeline approach that includes four connected modular components: (1) natural language understanding (NLU) for extracting the intent and key slots of users (Qin et al., 2020a, 2021b); (2) dialogue state tracking (DST) for tracing users\u2019 belief state given dialogue history (Balaraman et al., 2021a; Jacqmin et al., 2022a); (3) dialogue policy learning (DPL) to determine the next step to take (Kwan et al., 2022); (4) natural language generation (NLG) for generating dialogue system response (Wen et al., 2015; Li et al., 2020).\nWhile impressive results have been achieved in\nprevious pipeline ToD approaches, they still suffer from two major drawbacks. (1) Since each module (i.e., NLU, DST, DPL, and NLG) is trained separately, pipeline ToD approaches cannot leverage shared knowledge across all modules; (2) As the pipeline ToD solves all sub-tasks in sequential order, the errors accumulated from the previous module are propagated to the latter module, resulting in an error propagation problem. To solve these issues, dominant models in the literature shift to end-to-end task-oriented dialogue (EToD). A critical difference between traditional pipeline ToD and EToD methods is that the latter can train a neural model for all the four components simultaneously (see Fig. 1(b)) or directly generate the system response via a unified sequence-to-sequence framework (see Fig. 1(c)).\nThanks to the advances of deep learning approaches and the evolution of pre-trained models, recent years have witnessed remarkable success in EToD research. However, despite its success, there remains a lack of a comprehensive review of recent approaches and trends. To bridge this gap, we make the first attempt to present a survey of this research field. According to whether the intermediate supervision is required and KB retrieval is differentiable or not, we provide a unified taxonomy of recent works including (1) modularly EToD (Mehri et al., 2019; Le et al., 2020) and (2) fully EToD (Eric and Manning, 2017; Wu et al., 2019; Qin et al., 2020b). Such taxonomy can cover\nall types of EToD , which help researchers to track the progress of EToD comprehensively. Furthermore, we present some potential future directions and summarize the challenges, hoping to provide new insights and facilitate follow-up research in the EToD field.\nOur contributions can be summarized as follows:\n(1) First survey: To our knowledge, we are the first to present a comprehensive survey for end-to-end task-oriented dialogue system;\n(2) New taxonomy: We introduce a new taxonomy for EToD including (1) modularly EToD and (2) fully EToD (as shown in Fig. 2);\n(3) New frontiers: We discuss some new frontiers and summarize their challenges, which shed light on further research;\n(4) Abundant resources: we make the first attempt to organize EToD resources including open-source implementations, corpora, and paper lists at https://etods.net/.\nWe hope that this work can serve as quick access to existing works and motivate future research2."
        },
        {
            "heading": "2 Background",
            "text": "This section describes the definition of modularly end-to-end task-oriented dialogue (Modularly\n2Due to the page limitation, the detailed related work section can be found in the Appendix B.\nEToD \u00a72.1) and fully end-to-end task-oriented dialogue (Fully EToD \u00a72.2), respectively."
        },
        {
            "heading": "2.1 Modularly EToD",
            "text": "Modularly EToD typically generates system response through sub-components (e.g., dialog state tracking (DST), dialogue policy learning (DPL) and natural language generation NLG)). Unlike traditional ToD which trains each component (e.g., DST, DPL, NLG) separately, modularly EToD trains all components in an end-to-end manner where the parameters of all components are optimized simultaneously.\nFormally, each dialogue turn consists of a user utterance u and system utterance s. For the n-th dialog turn, the agent observes the dialogue history H = (u1, s1), (u2, s2), ..., (un\u22121, sn\u22121), un and the corresponding knowledge base (KB) as KB while it aims to predict a system response sn, denoted as S.\nModularly EToD first reads the dialogue history H to generate a belief state B:\nB = Modularly_EToD(H), (1)\nwhere B consists of various slot value pairs (e.g., price: cheap) for each domain.\nThe generated belief state B is used to query the corresponding KB to obtain the database query results D:\nD = Modularly_EToD(B,KB), (2)\nThen, H, B, and D is used to decide dialogue action A. Finally, modularly EToD generates the final dialogue system response S conditioning on H, B, D and A:\nS = Modularly_EToD(H,B,D,A), (3)"
        },
        {
            "heading": "2.2 Fully End-to-end Task-oriented Dialogue",
            "text": "In comparison to modularly EToD, Fully EToD (Eric and Manning, 2017) has two crucial differences: (1) modularly EToD leverages the generated beliefs as API to query KB, which is non-differentiable. In contrast, fully EToD directly encodes KB and uses a neural network to query the KB in a differentiable manner. (2) Unlike modularly EToD which requires modular annotation (e.g., DST, DPL annotation) for intermediate supervision, fully EToD can directly generate system response given only dialogue history and the corresponding KB;\nFormally, fully EToD can be denoted as:\nS = Fully_EToD(H,KB). (4)"
        },
        {
            "heading": "3 Taxonomy of EToD Research",
            "text": "This section describes the progress of EToD according to the new taxonomy including modularly EToD (\u00a73.1) and Fully EToD (\u00a73.2)."
        },
        {
            "heading": "3.1 Modularly EToD",
            "text": "We further divide the modularly EToD into two sub-categories (1) modularly EToD without a pretrained model (\u00a73.1.1) and (2) modularly EToD with a pre-trained model (\u00a73.1.2) according to whether or not a pre-trained model is used, which are shown in Fig. 3 (a) and (b)."
        },
        {
            "heading": "3.1.1 Modularly EToD without PLM",
            "text": "One line of work mainly focuses on optimizing the whole dialogue with supervised learning (SL) while another line considers incorporating a reinforcement learning (RL) approach for optimizing.\nSupervised Learning. Liu and Lane (2017) first presented an LSTM-based (Hochreiter and Schmidhuber, 1997) model which jointly learns belief tracking and KB retrieval. Wen et al. (2017) also proposed an EToD model with a modularized design, in which each module transmits its latent representation instead of predicted labels to the next module. Lei et al. (2018) introduced Sequicity, a two-stage CopyNet (Gu et al., 2016), merging belief tracking and response generation in a sequence-to-sequence model. MOSS (Liang et al., 2019) expanded Sequicity with NLU and DPL modules for comprehensive dialogue supervision. Shu et al. (2019) modeled language understanding and state tracking tasks jointly using a unified seq2seq approach and separate GRUs for different slot types. Mehri et al. (2019) explicitly incorporated the dialogue structure information into EToD, enhancing the domain generalizability. Zhang et al. (2019) considered multiple appropriate responses under the same context in ToD and improved dialogue policy diversity by balancing the valid output action distribution. LABES (Zhang et al., 2020b) leveraged unlabeled dialogue data (i.e., without belief state labels) to achieve semi-supervised learning of ToD.\nReinforcement Learning. Reinforcement Learning (RL) has been explored as a supplement to supervised learning in dialogue policies optimization. Li et al. (2018) demonstrated less error propagation using RL-optimized networks than SL settings. SL signals have also been incorporated into RL frameworks, either by modifying rewards (Zhao and Eskenazi, 2016) or adding SL cycles (Liu et al., 2017). Approaches like LAVA (Lubis et al., 2020), LaRL (Zhao et al., 2019), CoGen (Ye et al., 2022) and HDNO (Wang et al., 2021) have explored the modeling of latent representations. Work on RLoptimized EToD training with human intervention includes HCNs (Williams et al., 2017), humancorrected model predictions (Liu et al., 2018; Liu and Lane, 2018), and determining optimal time for human intervention (Rajendran et al., 2019; Wang et al., 2019)."
        },
        {
            "heading": "3.1.2 Modularly End-to-end Task-oriented Dialogue with Pre-trained Model",
            "text": "There are two main streams of PLM for modularly EToD including (1) Decoder-only PLM (Radford et al.) and (2) Encoder-Decoder PLM (Lewis et al., 2019; Raffel et al., 2020).\nDecoder-only PLM. Some works adopted GPT2 (Radford et al.) as the backbone of EToD models. Budzianowski and Vulic\u0301 (2019) first attempted to employ a pretrained GPT model for EToD, which considers dialogue context, belief state, and database state as raw text input for the GPT model to generate the final system response. Wu et al.\n(2021b) introduced two separate GPT-2 models to learn the user and system utterance distribution effectively. Hosseini-Asl et al. (2020) proposed SimpleToD, recasting all ToD subtasks as a single sequence prediction paradigm by optimizing for all tasks in an end-to-end manner. Wang et al. (2022) re-formulated the task-oriented dialogue system as a natural language generation task. UBAR (Yang et al., 2020b) followed the similar paradigm with SimpleTOD. The core difference is that UBAR incorporated all belief states in all dialogue turns while SimpleToD only utilized belief states of the last turn.\nAnother series of works tried to modify the pre-training objective of autoregressive transformers. To this end, Li et al. (2019) replaced system response ground truth with random distractor at a possibility during training and leveraged a next utterance classifier to distinguish them. Soloist (Peng et al., 2021) proposed an auxiliary task where the target belief state is replaced with the belief state from unrelated samples for consistency prediction. Kulh\u00e1nek et al. (2021) further augmented GPT-2 by presenting a new dialogue consistency classification task. The experimental results show that these more challenging training objectives bring significant improvements.\nEncoder-decoder PLM. PLMs with an encoderdecoder architecture such as BART (Lewis et al., 2019), T5 (Raffel et al., 2020) and UniLM (Dong et al., 2019) are also explored in\nMultiWOZ2.0 MultiWOZ2.1 Model Inform (%) Success (%) BLEU Combined Inform (%) Success (%) BLEU Combined Modularly End-to-end Task-oriented Dialogue without Pre-trained Model MD-Sequicity (Lei et al., 2018) - - - - 66.4 45.3 15.5 71.4 SFN+RL (Mehri et al., 2019) 73.8 58.6 16.9 83.0 73.8 58.6 16.9 83.0 DAMD (Zhang et al., 2019) 76.3 60.4 16.6 85.0 76.4 60.4 16.6 85.0 UniConv (Le et al., 2020) - - - - 72.6 62.9 19.8 87.6 LABES (Zhang et al., 2020b) - - - - 78.1 67.1 18.1 90.7 LAVA (Lubis et al., 2020) 91.8 81.8 12.0 98.8 - - - - Modularly End-to-end Task-oriented Dialogue with Pre-trained Model SimpleToD (Hosseini-Asl et al., 2020) 84.4 70.1 15.0 92.3 85.0 70.5 15.2 93.0 UBAR (Yang et al., 2020b) 95.4 80.7 17.0 105.1 95.7 81.8 16.5 105.3 MinTL-BART (Lin et al., 2020) 84.9 74.9 17.9 97.8 - - - - AuGPT (Kulh\u00e1nek et al., 2021) 83.1 70.1 17.2 93.8 83.5 67.3 17.2 92.6 SOLOIST (Peng et al., 2021) 85.5 72.9 16.5 95.7 85.5 72.9 16.5 95.7 MTTOD (Lee, 2021) 91.0 82.6 21.6 108.3 91.0 82.1 21.0 107.5 PPTOD (Su et al., 2021) 89.2 79.4 18.6 102.9 87.1 79.1 19.2 102.3 SimpleToD-ACN (Wang et al., 2022) 85.8 72.1 15.5 94.5 - - - - GALAXY (He et al., 2022b) 94.4 85.3 20.5 110.4 95.3 86.2 20.0 110.8 SPACE3 (He et al., 2022a) 95.3 88.0 19.3 111.0 95.6 86.1 19.9 110.8 BORT (Sun et al., 2022) 93.8 85.8 18.5 108.3 - - - -\nTable 1: Modularly EToD performance on MultiWOZ2.0 and MultiWOZ2.1. The highest scores are marked with underlines. We adopted reported results from published literature (Zhang et al., 2020b, 2019; He et al., 2022b).\nEToD. MinTL (Lin et al., 2020) considered training EToD with PLMs in the Seq2Seq manner, where two different decoders are introduced to track belief state and predict response, respectively. PPToD (Su et al., 2021) recast ToD subtasks into prompts and leveraged the multitask transfer learning of T5 (Raffel et al., 2020). Huang et al. (2022) embedded KB information into the language model for implicit knowledge access.\nIn addition, another series of works devised unique pre-training objectives for encoder-decoder transformers. GALAXY (He et al., 2022b) introduced a dialog act prediction pre-training task for policy optimization. Godel (Peng et al., 2022) leveraged a new phase of grounded pre-training\ndesigned to improve adaptation ability. BORT (Sun et al., 2022) added a denoising reconstruction task to reconstruct the original context from generated dialogue states. MTToD (Lee, 2021) introduced a span prediction pre-training task. SPACE-3 (He et al., 2022a) further improved over GALAXY with UniLM backbone, where five pre-training objectives are applied to better understand semantic information for task-oriented dialogue. Recently, encoder-decoder PLMs have shown the potential of converting EToD into other task forms like QA (Tian et al., 2022; Xie et al., 2022)."
        },
        {
            "heading": "3.1.3 Leaderboard and Takeaway.",
            "text": "Leaderboard: Leaderboard for the widely used datasets: MultiWOZ2.0, MultiWOZ2.1 and Camrest676 is shown in Table 1 and Table 2. The widely used metrics are BLEU, Inform, Success, and Combined. Detailed descriptions of datasets and metrics are shown in Appendix A.1.\nTakeaway: As seen, we have the following observations: (1) PLM Attains Improvement. We observe that most modularly EToD with PLM outperforms the modularly EToD without PLM, which indicates that knowledge inferred from a pre-trained model can benefit EToD; (2) Shared Knowledge Leverage. Since each module (i.e., NLU, DST, PL, NLG) is highly related, modularly EToD can enable the model to fully utilize shared knowledge across all modules."
        },
        {
            "heading": "3.2 Fully EToD",
            "text": "In the following, we describe the recent dominant fully EToD works according to the category of KB representation, which is illustrated in Fig. 3(c)."
        },
        {
            "heading": "3.2.1 Triplet Representation.",
            "text": "Specifically, given a knowledge base (KB), triplet representation stores each KB entity in a (subject, relation, object) representation. For example, all triplets can be formularized as (centric entity of ith row, slot title of jth column, entity of ith row in jth column). (e.g., (Valero, Type, Gas Station)). The KB entity representation is calculated by the sum of the word embedding of the subject and relation using bag-of-words approaches. It is one of the most widely used approaches for representing KB. Specifically, Eric and Manning (2017) employed a key-value retrieval mechanism to retrieve KB knowledge triplets. Other works treat KB and dialogue history equally as triplet memories (Madotto et al., 2018; Wu et al., 2019; Chen et al., 2019b; He et al., 2020a; Qin et al., 2021a). Memory networks (Sukhbaatar et al., 2015) have been applied to model the dependency between related entity triplets in KB (Bordes et al., 2017; Wang et al., 2020) and improves domain scalability (Qin et al., 2020b; Ma et al., 2021). To improve the response quality with triplet KB representation, Raghu et al. (2019) proposed BOSS-NET to disentangle NLG and KB retrieval and Hong et al. (2020) generated\nresponses through a template-filling decoder."
        },
        {
            "heading": "3.2.2 Row-level Representation.",
            "text": "While triplet representation is a direct approach for representing KB entities, it has the drawback of ignoring the relationship across entities in the same row. To migrate this issue, some works investigated the row-level representation for KB.\nIn particular, KB-InfoBot (Dhingra et al., 2017) first utilized posterior distribution over KB rows. Reddy et al. (2018) proposed a three-step retrieval model, which can select relevant KB rows in the first step. Wen et al. (2018) used entity similarity as the criterion for selecting relevant KB rows. Qin et al. (2019b) employed a two-step retrieving procedure by first selecting relevant KB rows and then choosing the relevant KB column. Recently, Zeng et al. (2022) proposed to store KB rows and dialogue history into two separate memories."
        },
        {
            "heading": "3.2.3 Graph Representation",
            "text": "Though row-level representation achieves promising performance, they neglect the correlation between KB and dialogue history. To solve this issue, a series of works focus on better contextualizing entity embedding in KB by densely connecting entities and corresponding slot titles in dialogue history. This can be done with either graph-based reasoning or attention mechanism, where entity presentations are fully aware of other entities or dialogue context. To this end, Yang et al. (2020a) facilitated\nSMD MultiWOZ2.1 Model BLEU Ent.F1(%) Sch.F1(%) Wea.F1(%) Nav.F1(%) BLEU Ent.F1(%) Res.F1(%) Att.F1(%) Hot.F1(%) Entity Triplet Representation KVRet (Eric and Manning, 2017) 13.2 48.0 62.9 53.3 44.5 - - - - - Mem2Seq (Madotto et al., 2018) 12.6 33.4 49.3 32.8 20.0 6.6 21.6 22.4 22.0 21.0 GLMP (Wu et al., 2019) 14.8 60.0 69.6 62.6 53.0 6.9 32.4 38.4 24.4 28.1 BossNet (Raghu et al., 2019) 8.3 35.9 50.2 34.5 21.6 5.7 25.3 26.2 24.8 23.4 KB-Transformer (E. et al., 2019) 13.9 37.1 51.2 48.2 23.3 - - - - - DDMN (Wang et al., 2020) 17.7 55.6 65.0 58.7 47.2 12.4 31.4 30.6 32.9 30.6 DFNet (Qin et al., 2020b) 14.4 62.7 73.1 57.6 57.9 9.4 35.1 40.9 28.1 30.6 TToS (He et al., 2020a) 17.4 55.4 63.5 64.1 45.9 - - - - - IR-Net (Ma et al., 2021) 16.3 63.2 - - - 10.9 37.5 - - - MCL (Qin et al., 2021a) 17.2 60.9 70.6 62.6 59.0 - - - - - Row-level Representation DSR (Wen et al., 2018) 12.7 51.9 52.1 50.4 52.0 9.1 30.0 33.4 28.0 27.1 MLM (Reddy et al., 2018) 15.6 55.5 67.4 54.8 45.1 9.2 27.8 29.8 27.4 25.2 KB-retriever (Qin et al., 2019b) 13.9 53.7 55.6 52.2 54.5 - - - - - HM2Seq (Zeng et al., 2022) 14.6 63.1 73.9 64.4 56.2 - - - - - Graph Representation Fg2Seq (He et al., 2020b) 16.8 61.1 73.3 57.4 56.1 13.5 36.0 40.4 41.7 30.9 GraphDialog (Yang et al., 2020a) 13.7 60.7 72.8 55.2 54.2 - - - - - GraphMemDialog (Wu et al., 2021a) 18.8 64.5 75.9 62.3 56.3 14.9 40.2 42.8 48.8 36.4 GPT2+KE (Madotto et al., 2021) 17.4 59.8 72.6 57.7 53.5 - - - - - COMET (Gou et al., 2021) 17.3 63.6 77.6 58.3 56.0 - - - - - Modularized Pre-Training (Qin et al., 2023b) 18.8 63.8 75.0 58.4 59.1 13.6 36.3 41.5 36.2 31.2 DialoKG (Rony et al., 2022) 20.0 65.9 - - - - - - - - UnifiedSKG (Xie et al., 2022) - 67.9 - - - - - - - - MAKER (Wan et al., 2023) 25.9 71.3 - - - 18.8 54.7 - - -\nTable 4: Fully EToD performance on SMD and MultiWOZ2.1. Ent.F1, Sch.F1, Wea.F1, Nav.F1, Res.F1, Att F1.and Hot.F1 stand for the abbreviation of Entity F1, Schedule F1, Weather F1, Navigation F1, Restaurant F1 and Hotel F1, respectively. We adopted reported results from published literature (Qin et al., 2020b; Wu et al., 2021a; Wang et al., 2020; Gou et al., 2021)\nentity contextualization by applying graph-based multi-hop reasoning on the entity graph. Wu et al. (2021a) proposed a graph-based memory network to yield context-aware representations. Another series of works leveraged transformer architecture to learn better entity representation, where the dependencies between dialogue history and KB were learned via self-attention (He et al., 2020b; Gou et al., 2021; Rony et al., 2022; Qin et al., 2023b; Wan et al., 2023)."
        },
        {
            "heading": "3.2.4 Leaderboard and Takeaway",
            "text": "Leaderboard: A comprehensive leaderboard for the widely used dataset: SMD and Multi-WOZ2.1 is shown in Table 4. The widely used metrics for fully EToD are BLEU and F1. Detailed information of datasets and metrics are shown in Appendix A.2.\nTakeaway: Compaunderline to modular EToD, fully EToD brings two major advantages. (1) Human Annotation Efforts Underlineuction. Modularly EToD still requires modular annotation data for intermediate supervision. In contrast, fully EToD only requires the dialogue-response pairs, which can greatly underlineuce human annotation efforts; (2) KB Retrieval End-to-end Training. Unlike the non-differentiable KB retrieval in modularly EToD, fully EToD can optimize the KB retrieval process in a fully end-to-end paradigm, which can enhance the KB retrieval ability."
        },
        {
            "heading": "4 Future Directions",
            "text": "This section will discuss new frontiers for EToD, hoping to facilitate follow-up research in this field."
        },
        {
            "heading": "4.1 LLM for EToD",
            "text": "Recently, Large Language Models (LLMs) have gained considerable attention for their impressive performance across various Natural Language Processing (NLP) benchmarks (Touvron et al., 2023; OpenAI, 2023; Driess et al., 2023). These models are capable to execute predetermined instructions and interface with external resources, such as APIs (Patil et al., 2023) and knowledge databases. This positions LLMs as promising candidates for endto-end dialogue systems (EToD). Existing research has also explored to apply LLMs in task-oriented dialogue (ToD) scenarios, using both few-shot and zero-shot learning paradigms (Pan et al., 2023; Heck et al., 2023; Hudevcek and Dusek, 2023; Parikh et al., 2023).\nHowever, several critical challenges remain to be addressed in EToD in future research. We summarize the main challenges as follows:\n1. Safety and Risk Mitigation: LLMs like chatbots can sometimes generate harmful or biased responses (OpenAI, 2023), posing serious safety concerns. It is crucial to improve their controllability and interpretability. One promising approach is integrating human feed-\nback during training (Bai et al., 2022; Chung et al., 2022).\n2. Complex Conversations Management: LLMs have limitations in managing complex, multi-turn dialogues (Heck et al., 2023; Pan et al., 2023). EToDs often require advanced context modeling and reasoning abilities, which is an area ripe for improvement.\n3. Domain Adaptation: For task-oriented dialogue, LLMs need to gain specific domain knowledge. However, simply suppling knowledge with finetuning or prompting may lead to problems like catastrophic forgetting or biased attention (Liu et al., 2023). Finding a balanced approach for knowledge adaptation remains a challenge.\nIn addition to these challenges, there are also emerging opportunities that could further enhance the capabilities of LLMs in EToD systems. These opportunities are summarized below:\n1. Meta-learning & Personalization: LLMs can adapt quickly with limited examples. This paves the way for personalized dialogues through meta-learning algorithms.\n2. Multi-agent Collaboration & Self-learning from Interactions: The strong language modeling capabilities of LLMs make self-learning from real-world user interactions more feasible (Park et al., 2023). This can advance collaborative, task-solving dialogue agents"
        },
        {
            "heading": "4.2 Multi-KB Settings",
            "text": "Recent EToD models are limited to single-KB settings where a dialogue is supported by a single KB, which is far from the real-world scenario. Therefore, endowing EToD with the ability of reasoning over multiple KBs for each dialogue plays a vital role in a real-world deployment. To this end, Qin et al. (2023a) take the first meaningful step to the multi-KB EToD.\nThe main challenges for multi-KB settings are as follows: (1) Multiple KBs Reasoning: How to reason across multiple KBs to retrieve relevant knowledge entries for dialogue generation is a unique challenge; (2) KB Scalibility: When the number of KBs becomes larger in real-world scenarios, how to effectively represent all the KBs in a single model is non-trivial."
        },
        {
            "heading": "4.3 Pre-training Paradigm for Fully EToD",
            "text": "Pre-trained Language Models have shown remarkable success in open-domain dialogues. ((Bao et al., 2021; Shuster et al., 2022)). However, there is relatively little research addressing how to pre-train a fully EToD. We argue that the main reason for hindering the development of pre-training fully EToD is the lack of large amounts of knowledgegrounded dialogue for pre-training.\nWe summarize the core challenges for pretraining fully EToD: (1) Data Scarcity: Since the annotated KB-grounded dialogues are scarce, how to automatically augment a large amount of training data is a promising direction; (2) Task-specific Pre-training: Unlike the traditional general-purpose mask language modeling pre-training objective, the unique challenge for a task-oriented dialogue system is how to make KB retrieval. Therefore, how to inject KB retrieval ability in the pre-training stage is worth exploring."
        },
        {
            "heading": "4.4 Knowledge Transfer",
            "text": "With the development of traditional pipeline taskoriented dialogue systems, there exist various powerful modularized ToD models, such as NLU (Qin et al., 2019a; Zhang et al., 2020a), DST (Dai et al., 2021; Guo et al., 2022; Chen et al., 2022), DPL (Chen et al., 2019a; Kwan et al., 2022) and NLG (Wen et al., 2015; Li et al., 2020). A natural and interesting research question is how to transfer the dialogue knowledge from well-trained modularized ToD models to modularly or fully EToD.\nThe main challenge for knowledge transfer is Knowledge Preservation: How to balance the knowledge learned from previous modularized dialogue models and current data is an interesting direction to explore."
        },
        {
            "heading": "4.5 Reasoning Interpretability",
            "text": "Current fully EToD models perform knowledge base (KB) retrieval via a differentiable attention mechanism. While appealing, such a black-box retrieval method makes it difficult to analyze the process of KB retrieval, which can seriously hurt the user\u2019s trust. Inspired by Wei et al. (2022); Zhang et al. (2022), employing a chain of thought in KB reasoning in fully EToD is a promising direction to improve the interpretability of KB retrieval.\nThe main challenge for the direction is design of reasoning steps: how to propose an ap-\npropriate chain of thought (e.g., when to retrieve rows and when to retrieve columns) to express the KB retrieval process is non-trivial."
        },
        {
            "heading": "4.6 Cross-lingual EToD",
            "text": "Current success heavily relies on large amounts of annotated data that is only available for highresource language (i.e., English), which makes it difficult to scale to other low-resource languages. Actually, with the acceleration of globalization, task-oriented dialogue systems like Google Home and Apple Siri are required to serve a diverse user base worldwide, across various languages, which cannot be achieved by the previous monolingual dialogue. Therefore, zero-shot cross-lingual direction that can transfer knowledge from highresource language to low-resource languages is a promising direction to solve the problem. To this end, Lin et al. (2021) and Ding et al. (2022) introduced BiToD and GlobalWoZ benchmarks to promote cross-lingual task-oriented dialogue.\nThe main challenge for zero-shot crosslingual EToD includes: (1) Knowledge base Alignment: A unique challenge for crosslingual EToD is the knowledge base (KB) alignment. How to effectively align the KB structure information across different languages is an interesting research question to investigate; (2) Unified Cross-lingual Model: Since different modules (e.g., DST, DPL, and NLG) have heterogeneous structural information, how to build a unified cross-lingual model to align dialogue information across heterogeneous input in all languages is a challenge."
        },
        {
            "heading": "4.7 Multi-modal EToD",
            "text": "Current dialogue systems mainly handle plain text input. Actually, we experience the world with multiple modalities (e.g., language and image). Therefore, building a multi-modal EToD system that is able to handle multiple modalities is an important direction to investigate. Unlike the traditional single-modal dialogue system which can be supported by the corresponding KB, multi-modal EToD requires both the KB and image features to yield an appropriate response.\nThe main challenges for multi-modal EToD are as follows: (1) Multimodal Feature Alignment and Complementary: How to effectively make a multimodal feature alignment and complementary to better understand the dialogue is a crucial ability for multi-modal EToD; (2)\nBenchmark Scale Limited: Current multimodal dataset such as MMConv (Liao et al., 2021) and SIMMC 2.0 (Kottur et al., 2021) are slightly limited in size and diversity, which hinders the development of multi-modal EToD. Therefore, building a large benchmark plays a vital role for promoting multi-modal EToD."
        },
        {
            "heading": "5 Conclusion",
            "text": "We made a first attempt to summarize the progress of end-to-end task-oriented dialogue systems (EToD) by introducing a new perspective of recent work, including modularly EToD and fully EToD. In addition, we discussed some new trends as well as their challenges in this research field, hoping to attract more breakthroughs on future research.\nAcknowledgements\nThis work was supported by the National Natural Science Foundation of China (NSFC) via grant 62306342, 62236004 and 61976072 and sponsored by CCF-Baidu Open Fund. This work was also supported by the Science and Technology innovation Program of Hunan Province under Grant No. 2021RC4008. We are grateful for resources from the High Performance Computing Center of Central South University.\nLimitation\nThis study presented a comprehensive review and unified perspective on existing approaches and recent trends in end-to-end task-oriented dialogue systems (EToD). We have also created the first public resources website to help researchers stay updated on the progress of EToD. However, the current version primarily focuses on high-level comparisons of different approaches, such as overall system performance, rather than a fine-grained analysis. In the future, we intend to include more in-depth comparative analyses to gain a better understanding of the advantages and disadvantages of various models, such as comparing KB retrieval results and performance across different domains."
        },
        {
            "heading": "A Datasets and Metrics",
            "text": "A.1 Datasets and Metrics for Modularly EToD\nA.1.1 Dataset Three commonly used datasets for modularly EToD are CamRest676, MultiWOZ2.0, and MultiWOZ2.1.\nCamRest676 (Wen et al., 2017) is a relatively small-scale restaurant domain dataset, which consists of 408/136/136 dialogues for training/validation/test.\nMultiWOZ2.0 (Budzianowski et al., 2018) is one of the most widely used ToD dataset. It contains over 8,000 dialogue sessions and 7 different domains including: restaurant, hotel, attraction, taxi, train, hospital and police domain.\nMultiWOZ2.1 (Eric et al., 2019) is an improved version of MultiWOZ2.0, where incorrect slot annotations and dialogue acts were fixed.\nA.1.2 Metrics The widely used metrics for modularly EToD are BLEU, Inform, Success, and Combined.\nBLEU (Papineni et al., 2002) is used to measure the fluency of generated response by calculating n-gram overlaps between the generated response and the gold response.\nInform and Success (Budzianowski et al., 2018). Inform measures whether the system provides an appropriate entity and Success measures whether the system answers all requested attributes.\nCombined (Budzianowski et al., 2018) is a comprehensive metric considering BLEU, Inform, and Success, which can be calculated by: Combined = (Inform + Success ) \u00d7 0.5 + BLEU).\nA.2 Datasets and Metrics for Fully EToD A.2.1 Dataset SMD (Eric and Manning, 2017) and MultiWOZ2.1 (Qin et al., 2020b) are two popular datasets for evaluating fully EToD.\nSMD Eric and Manning (2017) proposed a Stanford Multi-turn Multi-domain Task-oriented Dialogue Dataset, which includes three domains: navigation, weather, and calendar.\nMultiWOZ2.1. Qin et al. (2020b) introduces an extension of MultiWOZ2.1 where they annotate the corresponding KB for each dialogue.\nA.2.2 Metrics Fully EToD adopts BLEU and Entity F1 to evaluate the fluent generation and KB retrieval ability, respectively.\nBLEU has been described in Section A.1.1.\nEntity F1 Eric and Manning (2017) is used to measure the difference between entities in the system and gold responses by micro-averaging the precision and recall."
        },
        {
            "heading": "B Related Work",
            "text": "Modular task-oriented dialogues typically consist of spoken language understanding (SLU), dialogue state tracking (DST), dialogue manager (DM) and natural language generation (NLG), which have achieved significant success. Recently, numerous surveys summaries the recent progress of modular task-oriented dialogue systems. Specifically, Louvan and Magnini (2020); Larson and Leach (2022) and Qin et al. (2021c) summarize the recent progress of neural-based models for SLU. On DST, Balaraman et al. (2021b) and Jacqmin et al. (2022b) review the recent neural approaches and highlight the need for greater exploration on generalizability within the field. In terms of dialogue management, Dai et al. (2020) concentrates on challenges like model scalability, data scarcity, and improving training efficiency. For natural language generation (NLG), Santhanam and Shaikh (2019) provides a comprehensive overview of the past, present, and future directions of NLG. Finally, Chen et al. (2017), Zhang et al. (2020c) and Ni et al. (2023) provide an overarching review of the dialogue system as a whole, emphasising the impact of deep learning technologies.\nCompared to the existing work, we focus on the end-to-end task-oriented dialogue system. To the best of our knowledge, this is the first comprehensive survey of the end-to-end task-oriented dialogue system. We hope that this survey can attract more breakthroughs on future research."
        }
    ],
    "title": "End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions",
    "year": 2023
}