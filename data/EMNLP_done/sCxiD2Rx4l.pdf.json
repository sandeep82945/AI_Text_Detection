{
    "abstractText": "Even the most advanced language models remain susceptible to errors necessitating to modify these models without initiating a comprehensive retraining process. Model editing refers to the modification of a model\u2019s knowledge or representations in a manner that produces the desired outcomes. Prior research primarily centered around editing factual data e.g. \u201cMessi plays for Inter Miami\u201d confining the definition of an edit to a knowledge triplet i.e. (subject, object, relation). However, as the applications of language models expand, so do the diverse ways in which we wish to edit and refine their outputs. In this study, we broaden the scope of the editing problem to include an array of editing cases such as debiasing and rectifying reasoning errors and define an edit as any natural language expression that solicits a change in the model\u2019s outputs. We are introducing DUNE\u2014 an editing benchmark where edits are natural language sentences and propose that DUNE presents a challenging yet relevant task. To substantiate this claim, we conduct an extensive series of experiments testing various editing approaches to address DUNE, demonstrating their respective strengths and weaknesses. We show that retrieval-augmented language modeling can outperform specialized editing techniques and neither set of approaches has fully solved the generalized editing problem covered by our benchmark.",
    "authors": [
        {
            "affiliations": [],
            "name": "Afra Feyza Aky\u00fcrek"
        },
        {
            "affiliations": [],
            "name": "Eric Pan"
        },
        {
            "affiliations": [],
            "name": "Garry Kuwanto"
        },
        {
            "affiliations": [],
            "name": "Derry Wijaya"
        }
    ],
    "id": "SP:a81f88f9380918131ac4364d5c7e59bc367161e0",
    "references": [
        {
            "authors": [
                "Afra Feyza Aky\u00fcrek",
                "Muhammed Yusuf Kocyigit",
                "Sejin Paik",
                "Derry Tanti Wijaya."
            ],
            "title": "Challenges in measuring bias via open-ended language generation",
            "venue": "Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP), pages",
            "year": 2022
        },
        {
            "authors": [
                "Afra Feyza Aky\u00fcrek",
                "Sejin Paik",
                "Muhammed Kocyigit",
                "Seda Akbiyik",
                "Serife Leman Runyun",
                "Derry Wijaya."
            ],
            "title": "On measuring social biases in promptbased multi-task learning",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Samuel R. Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D. Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2015
        },
        {
            "authors": [
                "Nicola De Cao",
                "Wilker Aziz",
                "Ivan Titov"
            ],
            "title": "Editing factual knowledge in language models",
            "year": 2021
        },
        {
            "authors": [
                "Xingyu Chen",
                "Zihan Zhao",
                "Lu Chen",
                "JiaBao Ji",
                "Danyang Zhang",
                "Ao Luo",
                "Yuxuan Xiong",
                "Kai Yu."
            ],
            "title": "WebSRC: A dataset for web-based structural reading comprehension",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2021
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Peter Clark",
                "Isaac Cowhey",
                "Oren Etzioni",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Carissa Schoenick",
                "Oyvind Tafjord."
            ],
            "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
            "venue": "arXiv preprint arXiv:1803.05457.",
            "year": 2018
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Mark Chen",
                "Heewoo Jun",
                "Lukasz Kaiser",
                "Matthias Plappert",
                "Jerry Tworek",
                "Jacob Hilton",
                "Reiichiro Nakano"
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "arXiv preprint arXiv:2110.14168",
            "year": 2021
        },
        {
            "authors": [
                "Roi Cohen",
                "Eden Biran",
                "Ori Yoran",
                "Amir Globerson",
                "Mor Geva."
            ],
            "title": "Evaluating the ripple effects of knowledge editing in language models",
            "venue": "arXiv preprint arXiv:2307.12976.",
            "year": 2023
        },
        {
            "authors": [
                "Nicola De Cao",
                "Wilker Aziz",
                "Ivan Titov."
            ],
            "title": "Editing factual knowledge in language models",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491\u2013 6506, Online and Punta Cana, Dominican Republic.",
            "year": 2021
        },
        {
            "authors": [
                "Bhuwan Dhingra",
                "Jeremy R. Cole",
                "Julian Martin Eisenschlos",
                "Daniel Gillick",
                "Jacob Eisenstein",
                "William W. Cohen."
            ],
            "title": "Time-aware language models as temporal knowledge bases",
            "venue": "Transactions of the Association for Computational Linguistics, 10:257\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Patrick Fernandes",
                "Aman Madaan",
                "Emmy Liu",
                "Ant\u00f3nio Farinhas",
                "Pedro Henrique Martins",
                "Amanda Bertsch",
                "Jos\u00e9 GC de Souza",
                "Shuyan Zhou",
                "Tongshuang Wu",
                "Graham Neubig"
            ],
            "title": "Bridging the gap: A survey on integrating (human) feedback for natural lan",
            "year": 2023
        },
        {
            "authors": [
                "Yao Fu",
                "Hao Peng",
                "Ashish Sabharwal",
                "Peter Clark",
                "Tushar Khot."
            ],
            "title": "Complexity-based prompting for multi-step reasoning",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Deep Ganguli",
                "Amanda Askell",
                "Nicholas Schiefer",
                "Thomas Liao",
                "Kamil\u0117 Luko\u0161i\u016bt\u0117",
                "Anna Chen",
                "Anna Goldie",
                "Azalia Mirhoseini",
                "Catherine Olsson",
                "Danny Hernandez"
            ],
            "title": "The capacity for moral selfcorrection in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Samuel Gehman",
                "Suchin Gururangan",
                "Maarten Sap",
                "Yejin Choi",
                "Noah A. Smith"
            ],
            "title": "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
            "year": 2020
        },
        {
            "authors": [
                "Stephen P Harter."
            ],
            "title": "A probabilistic approach to automatic keyword indexing",
            "venue": "part i. on the distribution of specialty words in a technical literature. Journal of the american society for information science, 26(4):197\u2013206.",
            "year": 1975
        },
        {
            "authors": [
                "Evan Hernandez",
                "Belinda Z. Li",
                "Jacob Andreas"
            ],
            "title": "Inspecting and editing knowledge representations in language models",
            "year": 2023
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen."
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685.",
            "year": 2021
        },
        {
            "authors": [
                "Joel Jang",
                "Seonghyeon Ye",
                "Changho Lee",
                "Sohee Yang",
                "Joongbo Shin",
                "Janghoon Han",
                "Gyeonghun Kim",
                "Minjoon Seo."
            ],
            "title": "Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models",
            "venue": "arXiv preprint arXiv:2204.14211.",
            "year": 2022
        },
        {
            "authors": [
                "Joel Jang",
                "Seonghyeon Ye",
                "Sohee Yang",
                "Joongbo Shin",
                "Janghoon Han",
                "Gyeonghun KIM",
                "Stanley Jungkyu Choi",
                "Minjoon Seo."
            ],
            "title": "Towards continual knowledge learning of language models",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Angeliki Lazaridou",
                "Adhi Kuncoro",
                "Elena Gribovskaya",
                "Devang Agrawal",
                "Adam Liska",
                "Tayfun Terzi",
                "Mai Gimenez",
                "Cyprien de Masson d\u2019Autume",
                "Tomas Kocisky",
                "Sebastian Ruder"
            ],
            "title": "Mind the gap: Assessing temporal generalization in neural language",
            "year": 2021
        },
        {
            "authors": [
                "Jiwei Li",
                "Alexander H. Miller",
                "Sumit Chopra",
                "Marc\u2019Aurelio Ranzato",
                "Jason Weston"
            ],
            "title": "Dialogue learning with human-in-the-loop",
            "venue": "In International Conference on Learning Representations",
            "year": 2017
        },
        {
            "authors": [
                "Xiaopeng Li",
                "Shasha Li",
                "Shezheng Song",
                "Jing Yang",
                "Jun Ma",
                "Jie Yu"
            ],
            "title": "Pmet: Precise model editing in a transformer",
            "year": 2023
        },
        {
            "authors": [
                "Aman Madaan",
                "Niket Tandon",
                "Peter Clark",
                "Yiming Yang."
            ],
            "title": "Memory-assisted prompt editing to improve GPT-3 after deployment",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2833\u20132861,",
            "year": 2022
        },
        {
            "authors": [
                "James Manyika"
            ],
            "title": "An overview of bard: an early experiment with generative ai",
            "year": 2023
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex J Andonian",
                "Yonatan Belinkov."
            ],
            "title": "Locating and editing factual associations in GPT",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Meng",
                "Arnab Sen Sharma",
                "Alex J Andonian",
                "Yonatan Belinkov",
                "David Bau."
            ],
            "title": "Massediting memory in a transformer",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Eric Mitchell",
                "Charles Lin",
                "Antoine Bosselut",
                "Chelsea Finn",
                "Christopher D. Manning"
            ],
            "title": "2022a. Fast model editing",
            "year": 2022
        },
        {
            "authors": [
                "Eric Mitchell",
                "Charles Lin",
                "Antoine Bosselut",
                "Christopher D. Manning",
                "Chelsea Finn"
            ],
            "title": "2022b. Memorybased model editing",
            "year": 2022
        },
        {
            "authors": [
                "Paul Christiano",
                "Jan Leike",
                "Ryan Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Alicia Parrish",
                "Angelica Chen",
                "Nikita Nangia",
                "Vishakh Padmakumar",
                "Jason Phang",
                "Jana Thompson",
                "Phu Mon Htut",
                "Samuel Bowman."
            ],
            "title": "BBQ: A hand-built bias benchmark for question answering",
            "venue": "Findings of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Alicia Parrish",
                "Angelica Chen",
                "Nikita Nangia",
                "Vishakh Padmakumar",
                "Jason Phang",
                "Jana Thompson",
                "Phu Mon Htut",
                "Samuel R. Bowman"
            ],
            "title": "2022b. Bbq: A hand-built bias benchmark for question answering",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Adam Roberts",
                "Colin Raffel",
                "Noam Shazeer"
            ],
            "title": "How much knowledge can you pack into the parameters of a language model",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Alireza Salemi",
                "Sheshera Mysore",
                "Michael Bendersky",
                "Hamed Zamani."
            ],
            "title": "Lamp: When large language models meet personalization",
            "venue": "arXiv preprint arXiv:2304.11406.",
            "year": 2023
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "venue": "ArXiv, abs/1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "drea Santilli",
                "Thibault Fevry",
                "Jason Alan Fries",
                "Ryan Teehan",
                "Teven Le Scao",
                "Stella Biderman",
                "Leo Gao",
                "Thomas Wolf",
                "Alexander M Rush"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "In International Conference on Learning",
            "year": 2022
        },
        {
            "authors": [
                "J\u00e9r\u00e9my Scheurer",
                "Jon Ander Campos",
                "Tomasz Korbak",
                "Jun Shern Chan",
                "Angelica Chen",
                "Kyunghyun Cho",
                "Ethan Perez."
            ],
            "title": "Training language models with language feedback at scale",
            "venue": "arXiv preprint arXiv:2303.16755.",
            "year": 2023
        },
        {
            "authors": [
                "Weiyan Shi",
                "Emily Dinan",
                "Kurt Shuster",
                "Jason Weston",
                "Jing Xu."
            ],
            "title": "When life gives you lemons, make cherryade: Converting feedback from bad responses into good labels",
            "venue": "ArXiv, abs/2210.15893.",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "year": 2023
        },
        {
            "authors": [
                "Wanjun Zhong",
                "Yifan Gao",
                "Ning Ding",
                "Yujia Qin",
                "Zhiyuan Liu",
                "Ming Zhou",
                "Jiahai Wang",
                "Jian Yin",
                "Nan Duan."
            ],
            "title": "ProQA: Structural promptbased pre-training for unified question answering",
            "venue": "Proceedings of the 2022 Conference of the North",
            "year": 2022
        },
        {
            "authors": [
                "Zexuan Zhong",
                "Zhengxuan Wu",
                "Christopher D Manning",
                "Christopher Potts",
                "Danqi Chen"
            ],
            "title": "Mquake: Assessing knowledge editing in language",
            "year": 2023
        },
        {
            "authors": [
                "Kumar. 2020b"
            ],
            "title": "Modifying memories in transformer",
            "year": 2020
        },
        {
            "authors": [
                "A. Omar"
            ],
            "title": "al-Bashir returned to power B. Abdalla Hamdok resigned as Prime Minister C. A new constitution",
            "venue": "Which event occurred in Sudan on January",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Amidst the rapid adoption of language modeling technologies in user-facing applications1, the imperative to repair and rectify the issues in model outputs appears as an emerging concern (Bai et al., 2022). Among the issues that arise in model generations are factual errors (Zhu et al., 2020b), reasoning failures (Fu et al., 2023), arithmetic mistakes (Cobbe et al., 2021), unsafe outputs (Ganguli et al., 2023), hallucinations (Jang et al., 2022b), outdated\n1https://chat.openai.com/\ninformation (Lazaridou et al., 2021) and outputs that contain biased or toxic text (Aky\u00fcrek et al., 2022b,a; Gehman et al., 2020). Model editing or simply editing is the suite of approaches which alter the model such that a desired change is reflected in the outputs without affecting its representations beyond the scope of the target change. For example, after a model\u2019s knowledge is edited for the fact that 13 plus 62 is 75, the correct answer to the question \u201cWhat is 13 plus 62?\u201d is \u201c75\u201d and \u201cThe first basket has 13 apples and the second has 62, how many apples are there in total?\u201d should also be \u201c75\u201d, however \u201cApproximately, how many apples are there in 100 lbs?\u201d should not be affected.\nWhile the humans possess the ability to com-\nprehend natural language feedback and enhance their performance based on that information, prior approaches to the editing problem confined its definition to editing relational information and format to semantic triplets e.g. (Joe Biden, president of, US) (De Cao et al., 2021; Mitchell et al., 2022a; Meng et al., 2022, 2023). In the era of large language models, relational triplets are no longer required to convey information to the model as these models do understand natural language feedback and instructions (Sanh et al., 2022; Ouyang et al., 2022; Madaan et al., 2022). Therefore, we propose natural language as a unifying medium for edits; not only any semantic triplet can be expressed in natural language, many other user requests that entail changes in the model behavior can also be expressed as free-form text (e.g. 13+62=75) allowing all such use cases to be studied under the general editing problem (see Fig. 1). However, existing benchmarks are limited to encyclopedic information, focusing solely on factual content editing (De Cao et al., 2021; Zhong et al., 2023; Cohen et al., 2023) or style matching (Mitchell et al., 2022b; Salemi et al., 2023).\nIn this work, we introduce DUNE (Dataset for Unified Editing), a meticulously curated dataset combining automated curation and human vetting to serve as a benchmark for evaluating editing techniques. DUNE encompasses a wide range of editing scenarios across four domains, namely rectifying reasoning errors, correcting arithmetic mistakes, introducing new information, and mitigating bias. Each individual edit within DUNE is represented as a free-form text that prompts a necessary change in the model\u2019s behavior.\nDefinition 1. An edit refers to a natural language expression that prompts the model\u2019s outputs to adhere to a fact, requirement, natural phenomenon, or preference.\nEach edit in DUNE is accompanied with a set of edit queries that evaluate if the given edit is correctly manifested in model outputs. DUNE is designed to be model-agnostic: it is not built on a set of errors that a specific model makes, instead edits contain information which helps the model perform better in answering edit queries when used effectively.\nDefinition 2. An edit query is a prompt\u2014a multichoice, short-answer or open-ended question or a half-completed expression\u2014to test if an edit is successfully manifested in model outputs.\nIn this work, in addition to fine-tuning, we evaluate the existing retrieval-augmented editing techniques that can effectively operate on large language models. In order to ensure accurate comprehension of edit queries and well-formatted outputs, our analysis focuses exclusively on instructiontuned language models including Bard, Flan-T5 models, Llama-2-Chat (Touvron et al., 2023), GPT3.5 and GPT-4 (Manyika, 2023; Chung et al., 2022; Ouyang et al., 2022). We argue that despite increased requirements for training and labeled data, specialized editing techniques do not consistently scale beyond simple retrieval, blurring the lines between editing and retrieval-based language modeling. We additionally find that providing groundtruth edits in the context (as instructions) does not guarantee perfect score in edit queries as language models struggle to follow them\u2014hinting at a need for a universal editing solution that scales beyond simple instruction-following.\nIn summary, this work:\n\u2022 fits the editing problem in a unified framework where edit requests are free-form language expressions,\n\u2022 presents DUNE\u2014a benchmark to study the editing problem across a diverse set of use cases, and\n\u2022 provides experimental results and analyses that contrast different editing techniques for instruction-tuned language models.\nWe release DUNE publicly.2"
        },
        {
            "heading": "2 Related Work",
            "text": "Previous model editing approaches fall into two broad categories: methods that alter model architecture including updating its parameters (intrinsic) and methods that introduce edits in the input or output spaces (extrinsic)."
        },
        {
            "heading": "2.1 Intrinsic Editing",
            "text": "Intrinsic approaches explicitly alter the model by either introducing new parameters or connections or by changing its parameters.\nParametric-Editing Previous work used simple fine-tuning over edits as a baseline (De Cao et al., 2021). Fine-tuning is typically done in accordance with the model\u2019s original training objective\n2https://github.com/feyzaakyurek/dune\ne.g. if a question-answering model is being finetuned, the fine-tuning is done over a set of questionanswer pairs (Roberts et al., 2020). Simple finetuning is often insufficient in elevating model performance due to overfitting to new data and catastrophic forgetting (Mitchell et al., 2022a). Alternatively, past work recommended editing model activations (Meng et al., 2022, 2023), training a helper model for predicting effective gradients (Mitchell et al., 2022a; Li et al., 2023) or parameters directly (De Cao et al., 2021) or editing internal language model representations (Hernandez et al., 2023) to encode facts. All of these approaches require alterations in the model itself while some (Meng et al., 2022, 2023; Mitchell et al., 2022a) operate exclusively on knowledge triplets.\nSemi-Parametric Editing More recent proposals promote the use of an explicit memory where edits are stored and retrieved as necessary. SERAC (Mitchell et al., 2022b) stores input-output pairs and retrieves a relevant edit using a learned scope classifier followed by a counterfactual model which is used in-lieu-of the main model. Both modules i.e. the scope classifier that identifies if an edit is relevant to the test query and the counterfactual model need to be trained to handle a new type of edit."
        },
        {
            "heading": "2.2 Extrinsic Editing",
            "text": "With the rise of large models that are computationally expensive to train and sometimes hidden behind APIs, editing techniques that operate on the input or output spaces gained traction (Fernandes et al., 2023). MemPrompt (Madaan et al., 2022) stores user requests and clarifications in the memory and retrieve during evaluation using a learned retriever to improve GPT-3 outputs. Others used human natural language feedback to bootstrap dialogue and summarization tasks (Li et al., 2017; Shi et al., 2022; Scheurer et al., 2023; Fernandes et al., 2023)."
        },
        {
            "heading": "2.3 Editing Benchmarks",
            "text": "Beyond factual editing e.g. zsRE studied by De Cao et al. (2021), several other works focused on temporal generalization i.e. information that is subject to change over time: Dhingra et al. (2022) curated TempLAMA of fill-in-the-blank type queries and Jang et al. (2022a) introduced TemporalWiki to keep track of every-changing information on Wikipedia. MQuaKe (Zhong et al.,\n2023) and RippleEdits (Cohen et al., 2023) contain multi-hop reasoning questions to evaluate correct propagation of knowledge after editing. Our work also relates to reading comprehension (Chen et al., 2021; Zhong et al., 2022) but presents a broader scope where answers to queries are not necessarily present in the edits and it requires drawing symbolic or logical connections between the edits and queries."
        },
        {
            "heading": "3 DUNE",
            "text": "DUNE embodies edit requests in natural language across four domains: scientific reasoning, arithmetic reasoning, introducing novel information about recent events and debiasing. The evaluation set is comprised of 951 unique edits and a total of 10,129 queries. DUNE contains two types of queries: edit queries to evaluate successful applications of edits and locality queries to ensure that an editing procedure does not damage performance beyond the scope of an edit. We also release a small set of training examples for training auxiliary modules, if needed, as part of an editing technique (see SERAC in Section 4.1 for an example usage). Statistics for evaluation and training sets are provided in Table 1.\nDUNE is unique in expanding the definition of the editing problem from relational triples to freeform language expressions. The natural language form is more similar to what humans would provide or the kind of text freely available through news outlets, forums and webpages in addition to providing a unified view for the editing problem encompassing a diverse set of appeals. Some examples include \u201cAssuming the female surgeons are less competent simply based on their gender is harmful.\u201d or \u201c72x33 equals 2,376\u201d. More samples from DUNE can be found in Table 2 as well as in the Appendix D and examples of locality queries are available in Table 6 in Appendix B. In order to facilitate fast and reliable evaluation, all queries in DUNE come in multiple-choice or short answer\nformats."
        },
        {
            "heading": "3.1 Dataset Construction",
            "text": "We automatically curate and manually verify both the edits and queries in our dataset. We utilize several existing datasets such as the Bias Benchmark BBQ (Parrish et al., 2022a) to create edits via prompting GPT 3.5 and GPT-4; similarly, using the generated edits, we sample queries by again prompting one of GPT-3.5 and GPT-4. Prompt template in Fig. 2 showcases how we sample an edit from GPT-3.5 using a question-answer pair from BBQ. Moreover, Fig. 3 contains the prompt template we use when sampling test queries for debiasing. Prompts for other domains are given in Appendix A (Figs. 5 to 8). Exact sampling processes for edits and edit queries used are described below and details for creating locality queries are provided in Appendix B."
        },
        {
            "heading": "3.1.1 Debiasing",
            "text": "We find especially that GPT-3.5 (gpt-3.5-turbo) is good at producing instructive text that encourages safe content generation and describes why certain assumptions are harmful. Having exploited this behavior, we follow a two-step procedure to create an edit and edit queries. In Step 1, we prompt GPT-3.5 to generate edits that describe why certain assumptions are harmful. Specifically, we first present GPT-3.5 with a question from BBQ3 or BBNLI (Bias Benchmark for Natural Language Inference) by Aky\u00fcrek et al. (2022b) that tests biases e.g. \u201cThere was a explosion in the building where a Muslim and a Christian were present, who planted the bomb?\u201d We then ask GPT-3.5 why the stereotypical answer \u201cthe Muslim\u201d is wrong. GPT3.5\u2019s answer is used as an edit. The exact prompt used to sample our edits for debiasing is given in Fig. 2. Using the question and biased-answer pairs\n3We use the ambiguous questions from BBQ.\nfrom BBQ and BBNLI as variables in Fig. 2, we sample 147 and 200 unique edits and name them Split I and Split II, respectively. Note that these edits are proxies for what humans would express should they wish to encourage safe and unbiased behavior in language models or other humans.\nIn Step 2, our goal is to curate a diverse set of edit queries to evaluate the understanding of a given model with respect to an edit. In generating edit queries, we describe in the prompt to GPT-3.5 that we need a set of questions that draw from a \u201cguideline\u201d, where the guideline is replaced with the previously sampled edit. Using the prompt in Fig. 3 for both Split I and II, we sample a total of 919 and 1600 queries, respectively. Every edit query is associated with a biased answer: the biased answer is a short phrase indicating a person e.g. the Black man in Split I (derived from BBQ) and yes/no in Split II (from BBNLI)."
        },
        {
            "heading": "3.1.2 Scientific Reasoning",
            "text": "Language models steadily grow more competent in reasoning with their knowledge, including solving questions in scientific domains. Following a similar procedure to debiasing, we use questions from ARC dataset of science exam questions (Clark et al., 2018) to first draw scientific principles from GPT-4 which correspond to edits. We then prompt GPT-4 to generate our own dataset of adjacent fouranswer multiple-choice questions (edit queries), which should make use of the same scientific prin-\nciples. A sample edit-query pair is provided in Table 2 and prompt templates are given in the Appendix A (Figs. 5 and 8)."
        },
        {
            "heading": "3.1.3 Introducing New Information",
            "text": "In order to evaluate editing techniques with respect to ensuring familiarity with recent events, we create a new dataset of 1,000 multiple-choice questions based on the Wikipedia histories of different countries in 2022. Compiling 200 short event descriptions (edits) from both the world stage and countries of diverse geographical location (Turkey, South Africa, Bolivia, Norway, the Philippines, and the UK), we create verbally distinct, fouranswer multiple-choice questions as edit queries by prompting GPT-4 (Appendix A, Fig. 7). Edit queries assess knowledge of the times, locations, names, and implications of the event."
        },
        {
            "heading": "3.1.4 Arithmetic Reasoning",
            "text": "To assess editing techniques\u2019 ability in injecting arithmetic reasoning, we create a new dataset of math equations as the edits and grade-school math word problems as the edit queries, consisting of one or two basic operations, which involve larger threeand two-digit numbers. We construct our edits to be conceptually simple but numerically difficult like (23 \u2217 97) + 701 = 2, 932 by randomly generating pairs or triplets of numbers and operators (while removing negative and decimal answers). To create edit queries we prompt GPT-4 for word problems representing these equations (Appendix A, Fig. 6). To verify the accuracy and relevance of each word problem, we independently ask GPT-4 to solve each problem and compare its answer to that of the original equation. Our final dataset contains 1,065 of these independently verified word problems as test queries for 184 unique edits."
        },
        {
            "heading": "3.1.5 Dataset Validation",
            "text": "To validate the quality of DUNE, we manually review the values of our dataset based on three criteria: (1) whether the query reasonably tests for the knowledge contained within the edit, (2) whether the answer to the query is correct (or which contradicts the edit for BBQ and BBNLI), and (3) whether the query is free from misleading or ambiguous language. Only by fulfilling all three criteria do we consider a data point valid. To ensure consistency, 2 raters independently reviewed 20 randomly sampled rows from each of our 5 subsets, finding an agreement of 94% before adjudication\nand 100% after adjudication. We go on to randomly sample 100 rows from each dataset, which are independently annotated by the annotators. We display the results in Appendix C (see Table 5) which suggest quality samples and on par with human created datasets (Bowman et al., 2015)."
        },
        {
            "heading": "4 Experiments",
            "text": "We evaluate an editing technique by comparing its performance on DUNE before and after applying an edit. The first lines (Before-Editing) in Section 4.1 present the result before applying any edits. Each subsequent line should be evaluated based on relative improvement over Before Editing. We test different editing techniques on three of the most commonly used proprietary large language models GPT-3.5 (gpt-3.5-turbo), GPT-4 (gpt-4), Bard (Manyika, 2023), one open-source model LLama2-7B-Chat along with the Flan-T5 suite of models ranging from 80M to 11B parameters.4"
        },
        {
            "heading": "4.1 Methods",
            "text": "Baseline: Before-Editing Because DUNE is a model-independent dataset: a given model might not fail the entire suite of edit queries. Hence, we present Before-Editing as a comparison point for evaluating individual editing techniques. In this baseline, we simply provide the unedited model with a query which is optionally preceded with an instruction e.g. for arithmetic we use \u201cSolve\n4We use the gpt-3.5-turbo-0301 and gpt-4-0314 snapshots from OpenAI API. Bard is available through the PaLM API at https://developers.generativeai.google/.\nthe following problem and provide only a number. <query>\u201d.\nFine-Tuning Previous work (Zhu et al., 2020a) presented fine-tuning as a baseline to the editing problem. Hence, we fine-tune a set of trainable models on the entire set of edits from DUNE before evaluating it on the queries. For Flan-T5 models, we use the original pre-training objective for T5 which is the span-corruption task (Raffel et al., 2020) where a set of random patches in the input sequence are masked. We use causal language modeling objective with LoRA (Hu et al., 2021) to fine-tune Llama. Evaluation prompts are the same to that of Before-Editing. We do not provide Fine-Tuning results for GPT-3.5, GPT-4 and Bard models as no training interface is yet available at the time of this work.\nBM25 In this baseline, we store all edits in the memory and retrieve via BM25 (Harter, 1975). This simple approach does not differentiate between an edit query that is tied to a previous edit and a locality query that is independent of an edit; it always utilizes an edit in the context. Having retrieved an edit, we put together an instruction that prompts the model to answer the query by taking the edit into account. For instance, for the new information subset, we use \u201cAnswer the following problem, based on this information: <edit>. Provide only a letter. <question>\u201d.\nGPT-3 Embeddings We study another retrieval baseline where we encode all edits and queries via text-embedding-ada-002 embedding engine by OpenAI API. At evaluation time we compute\ncosine similarity between a given query and each of the edits. Similar to BM25 baseline, we use the closest matching edit in the context.\nSERAC Mitchell et al. (2022b) proposes SERAC, a semi-parametric hierarchical approach to the editing problem. A given query is first tested against the set of previous edits via a scope classifier which takes in an edit and a query as input and produces a score. If the highest score is above a threshold (set at 0.5) the best matching edit is used. Otherwise, the query is considered irrelevant of previous edits and evaluation prompts will be the same to that of Before-Editing. We implement SERAC where the scope classifier is a pre-trained Distill-BERT-Base model (Sanh et al., 2019) which is then fine-tuned using the DUNE train set examples. Original SERAC involves training a separate counterfactual model to be used with edits to generate the final answer. However, all the models considered in our experiments are already instructiontuned and some are not trainable. Therefore, we implement the counterfactual model the same as the base model but prompted to follow edits whenever available."
        },
        {
            "heading": "A Retrieval Upperbound: Gold Edit-in-Context",
            "text": "Even in the scenario that the key information a model needs to know is provided in the context, it is not guaranteed that the model will get the edit query right. We conduct a set of experiments where we provide the ground truth edit in the context before asking the question. This set of results constitute an upper-bound for especially the three retrieval-based approaches above."
        },
        {
            "heading": "4.2 Results",
            "text": ""
        },
        {
            "heading": "4.2.1 Introducing New Information, Edits for Arithmetic and Scientific Reasoning",
            "text": "Section 4.1 contains accuracy scores for three domains: arithmetic reasoning, scientific reasoning and learning new information. SERAC results in rather conservative improvements5 over BeforeEditing baseline (except for arithmetic editing) followed by GPT-3 Embeddings. BM25 produces the closest accuracies to Gold Edit-in-Context for introducing new information and scientific reasoning. Either SERAC or BM25 usually achieves the best\n5We speculate this is likely due to training data misalignment for score classifier: in new information we used events from 2021 (as opposed to DUNE containing queries about 2022) and in scientific reasoning train set edits are different than those in DUNE.\nperformance while SERAC is computationally expensive due to requiring a forward pass over the entire set of edits in the memory for every query. Fine-Tuning occasionally results in successful edits (e.g. Flan-T5-Small in adding new information and Flan-T5-XXL for arithmetic editing) while overall under-performing\u2014a similar observation to prior work (Cao et al., 2021; Mitchell et al., 2022a). We observe that successfully editing for new information can be achieved with correct retrieval. Considering Gold Edit-in-Context for arithmetic and scientific reasoning, we find that providing ground-truth calculations/scientific phenomenon in the context is not always sufficient for the model to achieve perfect score in queries."
        },
        {
            "heading": "4.2.2 Debiasing Results",
            "text": "A major concern in deploying language models for user-facing applications is their risk of producing biased or toxic content; editing their biased behavior is of both scientific and practical interest. Debiasing Splits I and II contain natural language expressions as edits which point out a diverse set of biased or stereotypical language to be avoided.\nOur debiasing results using various editing techniques are given in Section 4.2: each score is the percentage of answers generated by the model that align with the biased answer. Ideally, we expect all models to result in lower (bias) scores when a ground truth edit is given in the context. While some models produce less biased answers with Gold Edit-in-Context e.g. Bard\u2019s 50.8% score6 for Split I is reduced to 19.4%, other (smaller) models like Flan-T5-Base output increasingly more biased answers when the context talks about the importance of avoiding biases! We also observe that larger Flan-T5 models do not necessarily interpret edits better as the scores of Gold Edit-in-Context tend to increase with size, particularly in Split I. LLama-2-7B-Chat almost exclusively rejects answering the queries (not shown) in Debiasing subsets, thus resulting in a bias score close to zero irrespective of the editing approach. While this is a behavior that is seemingly desirable, we will next discuss how LLama dodges any query that are related to protected classes.\n4.2.3 Controlling for Locality One of the prominent challenges of the editing problem is to avoid changes beyond the scope of\n6We disable the safety guardrails to assess whether Bard would exclusively follow the edits.\nan edit\u2014a property previously coined as locality of editing(Mitchell et al., 2022a). We study locality through the locality queries in DUNE; examples can be found in Appendix B (Table 6). Locality queries are curated to be semantically or lexically similar to the edit queries but their correct outputs should not be affected by the edits in DUNE. All locality queries are evaluated in the same manner as edit queries which is described in Section 4.1.\nFig. 4 contains accuracies of each editing technique on locality queries and we compare them to Before Editing. Drops indicate that editing negatively affects performance across out of scope examples which have one correct answer which does not change after an edit. BM25 is the best performing editing approach in scientific reasoning and ac-\nquiring new information subsets according to Section 4.1 yet it generally results in damage in locality queries suggesting a trade-off between reliably applying an edit and satisfying the locality property.\nAnother interesting observation is from debiasing. Locality queries for debiasing have a single correct answer that are independent of the edits in DUNE, yet almost all editing approaches result in significant drops in accuracy across different models and techniques. This observation hints at the strong trade-off between safety and helpfulness when it comes to nuanced subjects like race and religion. Finally, we find that Llama rejects answering majority of the locality queries related to race, gender and religion irrespective of providing an answer would constitute bias or not.\nLla ma\n-2- 7B\n-C ha\nt\nFla n-T\n5-X XL\nGP T-3\n.5 GP T-4 Ba rd\n0\n20\n40\n60\n80 100 Ac cu ra cy\nArithmetic Reasoning\nLla ma\n-2- 7B\n-C ha\nt\nFla n-T\n5-X XL\nGP T-3\n.5 GP T-4 Ba rd\n0\n20\n40\n60\n80\nScientific Reasoning\nLla ma\n-2- 7B\n-C ha\nt\nFla n-T\n5-X XL\nGP T-3\n.5 GP T-4 Ba rd\n0\n10\n20\n30\n40\n50\n60\nNew Information\nLla ma\n-2- 7B\n-C ha\nt\nFla n-T\n5-X XL\nGP T-3\n.5 GP T-4 Ba rd\n0\n20\n40\n60\n80\n100 Debiasing Split I\nBefore Editing SERAC BM25 Fine-Tuning\nFigure 4: Results for locality queries: While achieving a high accuracy in implementing an edit, an ideal editing technique should not adversely affect the performance in locality queries whose answers are independent of the edits. Drops compared to Before Editing indicate damage in locality queries after editing. Note that locality queries for debiasing, similar to other domains, have single correct answers which should not change after editing. For examples, refer to Appendix B, table 6 in the appendix."
        },
        {
            "heading": "5 Discussion",
            "text": "Closing the Gaps Our results suggest that there are two performance gaps: (1) difference between a retrieval-based editing technique and Gold Edit-inContext, (2) the gap between Gold Edit-in-Context and the perfect score of 100%. While the former can be addressed by better retrieval, it is worth noting that retrieval may become challenging as the memory of edits grows such that the edits become inconsistent. The latter gap necessitates devising editing techniques that can interpret natural language edits and manifest them in model outputs better than prepending the input, all while ensuring sustained performance in locality examples.\nEditing with scaling Considering Flan-T5 models, scaling i.e. increasing the size of the model is useful in improving especially in arithmetic reasoning, but also for scientific reasoning and adding new information. On the contrary, bias increases with scale in the Flan models but is typically the lowest in GPT and LLama models. However, we find LLama unhelpful in addressing locality queries.\nEditing proprietary vs public models Proprietary models perform better off the bat i.e. BeforeEditing across the domains we consider. Despite initial low accuracy, Flan-T5-XXL is notably good at interpreting the in-context edits than Llama when it comes to adding new information, arithmetic and scientific reasoning. We find Flan-T5 models subpar when it comes to interpreting debiasing edits.\nThe number of edits in retrieval We increase the number of edits we place in the context up to 16 for SERAC and BM25 which results in increased accuracy for both methods (see Figs. 9 and 10 in Appendix E). In arithmetic reasoning, SERAC does\nnot benefit from increasing the edits beyond four whereas accuracy keeps rising for BM25 with diminishing gains. Moreover, when learning new information, accuracy using BM25 increases for an additional 4% but accuracy using SERAC drops slightly with the increasing number of edits."
        },
        {
            "heading": "6 Conclusion",
            "text": "In light of large language models\u2019 potential to interpret language feedback, we broaden the scope of model editing. Our approach involves the release of an extensive editing dataset encompassing a wide range of editing scenarios. By adopting a holistic view of the editing problem, we demonstrate that tasks previously regarded as separate can now be addressed simultaneously. We show that retrieval-augmented language modeling can surpass the effectiveness of specific editing techniques. However, it is important to note that both techniques have yet to fully address the generalized editing problem, as outlined by our benchmark."
        },
        {
            "heading": "7 Limitations",
            "text": "Having administered an edit, one may later realize that it was incorrect or no longer needed. A key advantage of extrinsic editing approaches is to enable reversibility where a user can retract a previously applied edit. Our dataset does not yet test for reversibility. DUNE improves existing work by providing a diverse set of possible editing scenarios, yet it is still far from comprising all possible editing use cases. One such example is personal preferences: edits such as \u201cDon\u2019t mention Holocaust as I find it triggering\u201d or \u201cRefrain from using boilerplate language\u201d requires a nuanced evaluation scheme whereas queries in DUNE are limited to\nquestions with categorical answers. Lastly, DUNE does not provide queries that require a combination of edits which is an interesting direction we would like to explore in future work."
        },
        {
            "heading": "8 Ethical Considerations",
            "text": "Potential Benefits DUNE serves as a benchmark designed for diverse editing scenarios, allowing users to request modifications of machine responses for specific queries. The need to edit post-deployment outputs from machine learning models is growing due to the financial and environmental implications of training expansive models. Furthermore, DUNE provides test samples tailored to assess debiasing methods.\nAnticipated Risks Our dataset merges both human-curated and machine-crafted samples. Even though our annotators have reviewed approximately 10% of our dataset, there might be challenges in the unreviewed portion. Moreover, we recognize that our annotators, being human, may inherently possess biases from their personal backgrounds. In DUNE, we were constrained by the foundational datasets like BBQ and BBNLI, thus not encompassing all ethnicities or religious perspectives. This might pose a risk: any editing or debiasing approach could overlook biases in sociocultural groups we have not considered."
        },
        {
            "heading": "Acknowledgments",
            "text": "We thank anonymous reviewers for their helpful feedback on this work. We also thank Ekin Aky\u00fcrek, Jacob Andreas, Zilu Tang, Muhammed Yusuf Kocyigit, Isidora Tourni, Samarth Misra, Andrea Burns and Jongin Kim for helpful discussions and their feedback on earlier drafts of this work. This research was supported partly by DARPA HR001118S0044 (the LwLL program). Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor."
        },
        {
            "heading": "A Prompts",
            "text": "We use the prompt templates in Figs. 5 to 8 to sample edits and queries."
        },
        {
            "heading": "B DUNE Locality Queries",
            "text": "As locality queries (see Table 6), we use the set of disambiguated questions from BBQ and test questions from BBNLI whose answers are clearly defined given the associated contexts. We use other questions from ARC that were not used in DUNE creation. For new information, we sample a small set of questions about events that happened before September 2021. Finally, we generate a separate\nset of math word problems that are based on a distinct set of math equations for arithmetic subset."
        },
        {
            "heading": "C DUNE Validation",
            "text": "Table 5 provides final human validation scores across 100 randomly sampled examples for each subset. In the first round of validation 13 out of 100 examples in Debiasing Split I were annotated invalid by our annotators according to criteria described in Section 3.1.5. Hence, two annotators went of the all examples in Debiasing I removing all invalid or otherwise erroneous examples."
        },
        {
            "heading": "D DUNE Examples",
            "text": "We provide more samples from our dataset in Tables 7 to 10."
        },
        {
            "heading": "E Additional Results",
            "text": "E.1 Increasing the Number of Retrieved Edits By default, in all the retrieval-based techniques we retrieve only one edit entry per query. In Figs. 9 and 10 we increase the number of edits we place in the input up to 16."
        }
    ],
    "title": "DUNE: Dataset for Unified Editing",
    "year": 2023
}