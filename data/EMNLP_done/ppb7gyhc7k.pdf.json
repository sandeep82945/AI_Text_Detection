{
    "abstractText": "Personalized dialogue generation, focusing on generating highly tailored responses by leveraging persona profiles and dialogue context, has gained significant attention in conversational AI applications. However, persona profiles, a prevalent setting in current personalized dialogue datasets, typically composed of merely four to five sentences, may not offer comprehensive descriptions of the persona about the agent, posing a challenge to generate truly personalized dialogues. To handle this problem, we propose Learning Retrieval Augmentation for Personalized DialOgue Generation (LAPDOG), which studies the potential of leveraging external knowledge for persona dialogue generation. Specifically, the proposed LAPDOG model consists of a story retriever and a dialogue generator. The story retriever uses a given persona profile as queries to retrieve relevant information from the story document, which serves as a supplementary context to augment the persona profile. The dialogue generator utilizes both the dialogue history and the augmented persona profile to generate personalized responses. For optimization, we adopt a joint training framework that collaboratively learns the story retriever and dialogue generator, where the story retriever is optimized towards desired ultimate metrics (e.g., BLEU) to retrieve content for the dialogue generator to generate personalized responses. Experiments conducted on the CONVAI2 dataset with ROCStory as a supplementary data source show that the proposed LAPDOG method substantially outperforms the baselines, indicating the effectiveness of the proposed method. The LAPDOG model code is publicly available for further exploration. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Qiushi Huang"
        },
        {
            "affiliations": [],
            "name": "Shuai Fu"
        },
        {
            "affiliations": [],
            "name": "Xubo Liu"
        },
        {
            "affiliations": [],
            "name": "Wenwu Wang"
        },
        {
            "affiliations": [],
            "name": "Tom Ko"
        },
        {
            "affiliations": [],
            "name": "Yu Zhang"
        },
        {
            "affiliations": [],
            "name": "Lilian Tang"
        }
    ],
    "id": "SP:b6878948e504d931bde4b10933766549d81d20c3",
    "references": [
        {
            "authors": [
                "Yu Cao",
                "Wei Bi",
                "Meng Fang",
                "Shuming Shi",
                "Dacheng Tao."
            ],
            "title": "A model-agnostic data manipulation method for persona-based dialogue generation",
            "venue": "arXiv:2204.09867.",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "North American Chapter of the Association for Computational Linguistics, pages 4171\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Mikhail S. Burtsev",
                "Jason Weston."
            ],
            "title": "The second conversational intelligence challenge (convai2)",
            "venue": "arXiv:1902.00098.",
            "year": 2019
        },
        {
            "authors": [
                "Qiushi Huang",
                "Yu Zhang",
                "Tom Ko",
                "Xubo Liu",
                "Bo Wu",
                "Wenwu Wang",
                "Lilian Tang."
            ],
            "title": "Personalized dialogue generation with persona-adaptive attention",
            "venue": "arXiv preprint arXiv:2210.15088.",
            "year": 2022
        },
        {
            "authors": [
                "Gautier Izacard",
                "Mathilde Caron",
                "Lucas Hosseini",
                "Sebastian Riedel",
                "Piotr Bojanowski",
                "Armand Joulin",
                "Edouard Grave"
            ],
            "title": "Unsupervised dense information retrieval with contrastive learning",
            "year": 2021
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave."
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Frank F. Xu",
                "Luyu Gao",
                "Zhiqing Sun",
                "Qian Liu",
                "Jane Dwivedi-Yu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig"
            ],
            "title": "Active retrieval augmented generation",
            "year": 2023
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Patrick Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Douwe Kiela"
            ],
            "title": "Retrieval-augmented generation for knowledge",
            "year": 2020
        },
        {
            "authors": [
                "Zekang Li",
                "Cheng Niu",
                "Fandong Meng",
                "Yang Feng",
                "Qian Li",
                "Jie Zhou."
            ],
            "title": "Incremental transformer with deliberation decoder for document grounded conversations",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Lin-",
            "year": 2019
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Qian Liu",
                "Yihong Chen",
                "Bei Chen",
                "Jian-Guang Lou",
                "Zixuan Chen",
                "Bin Zhou",
                "Dongmei Zhang."
            ],
            "title": "You impress me: Dialogue generation via mutual persona perception",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Nasrin Mostafazadeh",
                "Nathanael Chambers",
                "Xiaodong He",
                "Devi Parikh",
                "Dhruv Batra",
                "Lucy Vanderwende",
                "Pushmeet Kohli",
                "James Allen."
            ],
            "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
            "venue": "Proceedings of the Con-",
            "year": 2016
        },
        {
            "authors": [
                "ter Welinder",
                "Paul Francis Christiano",
                "Jan Leike",
                "Ryan J. Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: A method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, Association for Computational",
            "year": 2002
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013 191, Belgium, Brussels. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Keshav Santhanam",
                "Omar Khattab",
                "Jon Saad-Falcon",
                "Christopher Potts",
                "Matei Zaharia."
            ],
            "title": "ColBERTv2: Effective and efficient retrieval via lightweight late interaction",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ecy",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom"
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "year": 2023
        },
        {
            "authors": [
                "Weijia Shi",
                "Sewon Min",
                "Michihiro Yasunaga",
                "Minjoon Seo",
                "Rich James",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Wen tau Yih."
            ],
            "title": "Replug: Retrieval-augmented black-box language models",
            "venue": "ArXiv, abs/2301.12652.",
            "year": 2023
        },
        {
            "authors": [
                "Haoyu Song",
                "Yan Wang",
                "Kaiyan Zhang",
                "Weinan Zhang",
                "Ting Liu."
            ],
            "title": "Bob: Bert over bert for training persona-based dialogue models from limited personalized data",
            "venue": "Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Wolf",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue."
            ],
            "title": "Transfertransfo: A transfer learning approach for neural network based conversational agents",
            "venue": "arXiv:1901.08149.",
            "year": 2019
        },
        {
            "authors": [
                "Saizheng Zhang",
                "Emily Dinan",
                "Jack Urbanek",
                "Arthur D. Szlam",
                "Douwe Kiela",
                "Jason Weston"
            ],
            "title": "Personalizing dialogue agents: I have a dog, do you have pets too? In Association for Computational Linguistics",
            "year": 2018
        },
        {
            "authors": [
                "Xueliang Zhao",
                "Wei Wu",
                "Chongyang Tao",
                "Can Xu",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Low-resource knowledge-grounded dialogue generation",
            "venue": "arXiv preprint arXiv:2002.10348.",
            "year": 2020
        },
        {
            "authors": [
                "Shuyan Zhou",
                "Uri Alon",
                "Frank F Xu",
                "Zhengbao JIang",
                "Graham Neubig."
            ],
            "title": "Doccoder: Generating code by retrieving and reading docs",
            "venue": "arXiv preprint arXiv:2207.05987.",
            "year": 2022
        },
        {
            "authors": [
                "Zhao"
            ],
            "title": "2020) was considered but not possible due to the unavailability of its code. Instead, we selected ITDD (Incremental Transformer with Deliberation Decoder",
            "venue": "(Li et al.,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Personalized dialogue generation (Zhang et al., 2018; Dinan et al., 2019), which prompts an agent\n\u2217Corresponding authors. 1https://github.com/hqsiswiliam/LAPDOG\nto generate consistent responses based on historical dialogue context and given persona profiles, has recently drawn substantial attention in many applications. For instance, such an agent could effectively adapt to different roles such as a customer service representative by tailoring its responses to specific customer needs based on its persona and improving customer interaction and satisfaction. Besides, personalized responses can foster a sense of human-like interaction in social platforms, thereby enriching the user experience.\nThe persona profiles contain background sentences describing the agent (e.g., I like to go hunting.) and play a crucial role in customizing the dialogue. Ideally, a persona profile should be as comprehensive as possible, containing diverse and detailed descriptions of an agent. However, these persona profiles, typically consisting of only four to five sentences, do not provide comprehensive descriptions for the persona of the agent. Such lack of depth and diversity in the persona descriptions impedes existing methods (Liu et al., 2020; Song et al., 2021; Huang et al., 2022) from generating highly personalized and contextually rich responses, though they have shown capabilities in producing grammatically correct and human-like responses. In essence, those models are restricted by the static and limited persona profile. Hence, those models fail to dynamically incorporate more intensive extra personalized profiles when decoding the responses.\nThough the given persona profile is limited, there are many external textual resources to describe personality and daily life circumstances. Hence, it is intuitive to ask: can we use other related datasets to enrich the details of the persona profile? This key question has not been thoroughly explored in existing methods, which primarily rely on the persona profile and dialogue context alone. An immediate issue is which types of external datasets could be used. A promising source is story data since they\nencompass diverse life events, personality traits, motivations, and experiences, which can contribute to a more detailed and realistic persona. For example, given the persona sentence as \"I like to work on vintage cars.\", potential retrieved stories\u2019 titles can be \"Antique Car Show\" and \"Mechanic\", the details of the story content can be found in the appendix (Table 7). Furthermore, the clear and inherent structure in stories can enhance the consistency of the persona. In this work, we choose story data to facilitate the generation of more engaging and contextually meaningful dialogues.\nGiven the external knowledge (e.g., story data), how to infuse it into the process of personalized dialogue generation straightforwardly remains challenging. The first hurdle is the lack of explicit annotations for retrieval, which are the key to selecting relevant and helpful content to augment persona profiles. In addition, the criterion for assessing the efficacy of these contents remains unclear. For instance, retrieval-augmented generation (RAG) (Lewis et al., 2020) is based on predicted probability distribution, which may not directly align with the objective of generating personalized responses. Moreover, simply tuning dense retriever (Karpukhin et al., 2020) in a RAG\u2019s paradigm may result in suboptimal retrieval outcomes as the retriever is inclined to consistently select similar passages for all queries, which may impede the further expansion of the persona profile details.\nIn this paper, we give the first try to utilize the story data as external knowledge for the personalized dialogue generation task and propose a Learning Retrieval Augmentation for Personalized DialOgue Generation (LAPDOG) framework. Specifically, the proposed model LAPDOG, consisting of a retriever to retrieve helpful information to enrich the persona and a generator to generate dialogues, is an end-to-end learnable retrieval methodology for integrating additional contextual information into personalized dialogue generation. LAPDOG utilizes non-differentiable metrics (e.g., BLEU, F1, and ROUGE-L) to guide the training of the retriever by aligning the retriever scores to these desired metrics, thereby facilitating the generation of relevant and diverse personalized responses. To ensure diversity in the retrieval process, we design a retrieval candidate augmentation during training, which prevents consistently selecting similar passages for all queries and provides a broader range of contextual inputs for the dialogue generator. In\naddition to the retrieved content, the persona information and dialogue context are also integrated into the dialogue generator. Furthermore, LAPDOG adopts a cooperative framework wherein the retriever and generator are jointly trained. This process enables LAPDOG to generate personalized responses that are coherent, contextually rich, and in line with the persona of the agent. Unlike other retrieval models (Zhou et al., 2022; Santhanam et al., 2022) that rely on annotated retrieval datasets, our method retrieves the supplementary context in an end-to-end, unsupervised manner, which can be seamlessly extended to other suitable text sources.\nWe conduct experiments on the CONVAI2 dataset (Dinan et al., 2019), which is widely recognized and extensively studied in the field of personalized dialogue generation (Huang et al., 2022; Song et al., 2021; Liu et al., 2020), and the ROCStory dataset (Mostafazadeh et al., 2016) acts as external knowledge. Experiments demonstrate the positive impact of learnable retrieval augmentation on performance. Quantitatively, the proposed LAPDOG method consistently yields improvements over the baseline models with varying model sizes. Moreover, the retrieved contents offer insights into the rationale behind the generation ability of the generator. Comprehensive ablation studies demonstrate that joint objective guidance outperforms each individual objective and provides insights into the size of retrieval candidates and the use of different metrics.\nOverall, our contributions can be summarized as follows.\n\u2022 We present a novel LAPDOG model for personalized dialogue generation to retrieve relevant contents in external knowledge to the persona using the non-differentiable objective.\n\u2022 We introduce candidate augmentation as a means to enhance learning retrieval augmentation, resulting in improved performance and increased diversity of candidate selections during the inference process.\n\u2022 The proposed LAPDOG framework significantly enhances the performance over baselines, showing promising potential for learnable retrieval augmentation on personalized dialogue generation. Our code and pre-trained model will be open-sourced."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Personalized Dialogue Generation",
            "text": "Based on the PersonaChat dataset (Zhang et al., 2018), Dinan et al. curate the CONVAI2 dataset, which contains a brief persona with four to five sentences for each interlocutor. This unique dataset has become a standard benchmark for the personalized dialogue generation task and built on this dataset, there are numerous studies, each of which approaches personalized dialogue generation from diverse perspectives. For example, Wolf et al. proposes a fine-tuning model based on the GPT2 model (Radford et al., 2019). Song et al. integrates three BERT models (Devlin et al., 2019) via reinforcement learning to generate responses. Liu et al. propose a transmitter and receiver model, which utilizes reinforcement learning with manually designed rewards for further refinement, for the personalized dialogue generation task.Cao et al. adopt model-agnostic data augmentation to use language models, such as GPT2 and BERT (Devlin et al., 2019), to augment the training set with pseudo training data. Huang et al. devise an adaptive attention mechanism to integrate information from persona and context encoders seamlessly. In contrast to the aforementioned models, the proposed LAPDOG method introduces an end-to-end dense retriever framework to simultaneously augment the input of the generator from external data source and tune the retriever."
        },
        {
            "heading": "2.2 Retrieval-Augmented Text Generation",
            "text": "There are some works to incorporate retrievers into their respective models via different integration strategies to enhance text generation tasks. For instance, DocPrompting (Zhou et al., 2022) curated a retrieval annotation dataset to train a retriever to retrieve and do input augmentation for code generation. Toolformer (Schick et al., 2023) bootstraps retrieval-annotated data, which performs fine-tuning on language models for the retrievalaugmentation ability. FLARE (Jiang et al., 2023) extends the Toolformer to large language models such as (Ouyang et al., 2022) with special-designed prompts. RePlug (Shi et al., 2023) further refines the retriever by distilling the knowledge from the language model\u2019s probability. RAG (Lewis et al., 2020) jointly trains the retriever and language model, which updates the retriever by the language model\u2019s probability. Different from those models, the proposed LAPDOG model is designed\nspecifically for personalized dialogue generation with a focus on optimizing desired objectives rather than the language model\u2019s probability distribution. Since RePlug, Toolformer, and FLARE are based on large language models or their API calls, we do not include them in the comparison to LAPDOG. Compared with other models, we do not rely on retrieval annotations or bootstrapped retrieval annotations. The training objectives are directly computed from a comparison between the generated text and ground truth, rather than relying on training probabilities that are not always aligned with the desired metrics. Additionally, we introduce a candidate augmentation to avoid the limitations of a confined candidate set. This broadens the scope of potential dialogues and better captures the richness and diversity of an agent\u2019s persona."
        },
        {
            "heading": "3 Methodology",
            "text": "In this section, we introduce the proposed LAPDOG model."
        },
        {
            "heading": "3.1 Task Formulation",
            "text": "In a persona-based conversation session denoted by C = {P,U}, the persona P = {p1, . . . , pe} consists of e profile sentences providing background information about a machine interlocutor m and the dialogue context U = {uh,1, um,1, ..., uh,n} encompasses the exchange of utterances between a human interlocutor h and the machine interlocutor m. In the task of persona-based dialogue generation, the persona P is used to characterize the machine interlocutor m, but it contains only four to five sentences, i.e., 4 \u2264 e \u2264 5. The conversation always starts by the human interlocutor h. The primary objective of this task is to generate the response r = um,n based on the given persona P and dialogue context U .\nThe persona P is short and hence cannot give a full characterization for the background information. To enrich the persona, we utilize a retrieval corpus D consisting of stories from a story dataset (e.g., ROCStory (Mostafazadeh et al., 2016)). Note that there is no explicit annotation between D and P , necessitating an alternative approach to evaluate the usefulness of the retrieval content."
        },
        {
            "heading": "3.2 The Architecture",
            "text": "As shown in Figure 1, the architecture of the LAPDOG model consists of a generator, which adopts a transformer-based encoder-decoder structure, to\ngenerate dialogues and a dual-encoder retriever to efficiently obtain relevant information from an external story corpus.\nRetriever Based on (Karpukhin et al., 2020), the retriever adopts a transformer-based encoder to embed the query and the story corpus, respectively. The retriever then calculates the dot product similarity score between the query and each story via their average pooled embeddings. Stories with the K highest similarity scores are retrieved.\nGenerator The generator takes a transformerbased encoder-decoder architecture to generate the response from the persona, dialogue history, and retrieved contents. To integrate the retrieved contents with the persona and dialogue history, we leverage the Fusion-in-Decoder (FiD) technique (Izacard and Grave, 2021). Specifically, each retrieved story is combined with the persona and context and individually encoded. The resulting encoded contexts are concatenated and cross-attended in the decoder to generate the final response."
        },
        {
            "heading": "3.3 Training Process",
            "text": "It is straightforward to directly train the generator and retriever using the generator\u2019s probability distribution in a way similar to the RAG method. However, this strategy does not work well since the retriever would trap into a fixed candidate set and the predicted probability distribution is not always aligned with the desired objectives in the personalized dialogue generation task. Hence, as depicted in Figure 1, the LAPDOG model adopts a two-stage training procedure. In the first stage, the training process starts with supervised training for\nthe generator (refer to Section 3.3.1). In the second stage, the framework starts to tune the retriever and learn the retrieval augmentation jointly. To learn retrieval augmentation (refer to Section 3.3.2), the retriever\u2019s loss is computed from the evaluation metrics between the output of the generator and the ground truth. During the process of learning retrieval augmentation, to prevent the retriever from stagnating around a limited set of candidates, we design the retrieval candidate augmentation (refer to Section 3.3.3), a method ensuring diversity in the retrieval process. Afterward, we enrich the input of the generator with retrieval-enhanced data and compute a generator loss based on the augmented input (refer to Section 3.3.4). Finally, we combine losses from both the generator and retriever to jointly train the two components (refer to Section 3.3.5). In the following sections, we introduce each part in detail."
        },
        {
            "heading": "3.3.1 Supervised Training",
            "text": "First, we train a generator that accepts persona P and context U as input, and the ground-truth response r as the target without involving any retrieval results. Hence, this stage is to minimize the negative log-likelihood, which is formulated as\nLNLL = \u2212 log(G\u03b8(r|P,U))\n= \u2212 |r|\u2211 i=1 log(G\u03b8(rt|P,U, r<t)), (1)\nwhere rt denotes the t-th token in r, r<t denotes the sequence containing the first to (t\u22121)-th tokens in r, G\u03b8(\u00b7) denotes the predicted probability distribution of the generator, and \u03b8 denotes parameters of the generator.\nAfter supervised training, we obtain a supervised tuned generator denoted by Gsup."
        },
        {
            "heading": "3.3.2 Learning Retrieval Augmentation",
            "text": "Intuitively, with the retrieval content as an augmentation, the goal is to improve the generated content in terms of desired metrics. However, it is hard to build direct connections between retrieval contents and the quality of the final generated response to update the retriever. To achieve that, we use the trained generator Gsup as an evaluator to give feedback.\nSpecifically, given the metric values from the trained generator Gsup, we use those metric values as feedback to guide the update of the retriever. In other words, if the generator Gsup finds that the retrieved story di \u2208 Dq is useful to improve the performance in terms of the given metrics, we should encourage the retriever to rank the score of di to be higher. In this way, we can let the model decide the usefulness of the retrieval content and avoid relying on the retrieval annotations between query q and story di. However, since the whole generation and metric calculation process is hard or even impossible to be differentiate, we cannot directly perform gradient descent with respect to the calculated metrics to update the retriever. To solve this problem, instead we transform the metric values into a probability distribution as\npi = exp\n( 1 \u03c4g M(y,Gen(Gsup, (di, P, U)) )\n\u2211K c=1 exp ( 1 \u03c4g M(y,Gen(Gsup, (dc, P, U)) ) ,\nwhere M(y, y\u0302) denotes a metric function to evaluate the quality of the generated text y\u0302 given the ground truth y, Gen(Gsup, (di, P, U)) denotes the decoded text generated by Gsup given (di, P, U) as the input, and \u03c4g is a temperature hyperparameter to control the sensitivity of the metric. Here the metric function satisfies that a higher value of M(\u00b7, \u00b7) indicates better performance. If a smaller value of M(\u00b7, \u00b7) indicates better performance, we can replace M(\u00b7, \u00b7) with \u2212M(\u00b7, \u00b7) in the calculation of pi. It is easy to see that a useful di will have a large pi and hence pi can be used as a supervised signal to guide the learning of the retriever. That is, we could make the similarity score returned by the retriever close to PR = {pi}Ki=1. Formally, suppose we have top-K retrieval stories Dq with its retrieval scores Sq \u2208 RK with respect to the query q, we can minimize the KL divergence between Sq\nand PR as\nLR = KL(PR, \u03c3(Sq/\u03c4s)), (2)\nwhere KL(\u00b7, \u00b7) denotes the KL divergence, \u03c3(\u00b7) denotes the softmax function, and \u03c4s is a temperature hyperparameter to control the sensitivity of the similarity scores. Combining LR with the retrieval candidate augmentation introduced in the next section, we can update the retriever."
        },
        {
            "heading": "3.3.3 Retrieval Candidate Augmentation",
            "text": "During the training process, there is a risk that the retriever gets stuck in a local optimum and consistently retrieves a fixed set or a narrow range of candidates. Consequently, the generator fails to learn from the retriever and disregards the retrieved content. To address this challenge, we design retrieval candidate augmentation to incorporate randomly sampled stories to encourage the framework to explore a wider range of candidates. Specifically, we first replace each di with a randomly selected candidate daugi at a probability of \u03c1 as\ndaugi = CandAug(di, \u03c1); di \u2208 Dq,\nwhere Dq denotes the set of retrieval stories, and forms a perturbed set Daugq = {daugi }Ki=1. Then we can compute the dot product similarity between the query q and each daugi as the retrieval scores Saugq = {saugq,i }Ki=1, where s aug q,i denotes the dot product similarity between q and daugi . Then we can apply the learning retrieval augmentation to Saugq and based on Eq. (2) minimize the following loss to update the retriever as\nLaugR = KL(PR, \u03c3(S aug q /\u03c4s))."
        },
        {
            "heading": "3.3.4 Training Retrieval-Augmented Generator",
            "text": "With the retrieval content obtained by the retriever, we hope to generate the response more accurately and hence we can further supervised train the generator in a way similar to the first stage (i.e., Section 3.3.1). Specifically, we can minimize the negative log-likelihood of the response given the persona, dialogue context, and retrieval content as\nLG = \u2212 log(G\u03b8(r|P,U,Daugp ))\n= \u2212 |r|\u2211 i=1 log(G\u03b8(rt|P,U,DaugP , r<t)). (3)\nIt is easy to see that Eq. (3) is similar to Eq. (1) with additionally inputting the retrieval content."
        },
        {
            "heading": "3.3.5 Retriever-Generator Joint Training",
            "text": "At the final stage, we aim to jointly train the retriever and generator to further improve them. Specifically, we minimize the sum of the losses of the two components as\nL = LaugR + LG . (4)\nIn Eq. (4), the two loss functions are treated equally. Generally speaking, introducing and tuning a weighting hyperparameter between the two losses may result in better performance but it incurs computational costs when tuning it. For simplicity, we did not introduce this hyperparameter and this could be left for future study.\nTo summarize, Algorithm 2 in the appendix describes the complete two-stage training process."
        },
        {
            "heading": "3.4 Inference Process",
            "text": "During inference, stories from the ROCStory dataset are fetched in alignment with the provided persona and then integrated into the dialogue context using the Fusion-in-Decoder (FiD) technique (Izacard and Grave, 2021). Each combination of story, persona, and context is individually encoded. These encoded contexts are concatenated and processed via cross-attention in the decoder to produce the final response in an auto-regressive fashion. Additional experiments evaluating the effects of various query combinations, such as persona+context and context alone, are detailed in Appendix A.2 to highlight their impact on performance."
        },
        {
            "heading": "4 Experiment",
            "text": "In this section, we empirically evaluate the proposed LAPDOG model."
        },
        {
            "heading": "4.1 Dataset",
            "text": "ConvAI2 (Dinan et al., 2019) is a dialogue dataset collected from the crowd, featuring 8939/1000 multi-turn conversations that rely on 1155/100 persona descriptions for the train/dev splits. Each persona is succinctly depicted by approximately 5 profile sentences. Paired workers engaged in interactive conversations based on predefined personas."
        },
        {
            "heading": "4.2 Retrieval Corpus",
            "text": "Given the absence of a paired annotated retrieval corpus connected to ConvAI2, we employ ROCStory (Mostafazadeh et al., 2016) as an auxiliary retrieval dataset. Our aim is for the narratives within this dataset to serve as supplemental content to the\nexisting personas within the dialogue. We have undertaken pre-processing of the ROCStory to align the narrative style more closely with persona representation, including changes like transforming \u2018he\u2019 to \u2018I\u2019 and \u2018does\u2019 to \u2018do\u2019. The detailed preprocessing is listed in Appendix A.7. Statically, there are 98,161 stories within the corpus, and each story is composed of 5 sentences."
        },
        {
            "heading": "4.3 Experimental Settings",
            "text": "We employ T5 series models (Raffel et al., 2020) (small, base, XL) as the foundational model used for the generator. We initialize our generator with pre-trained weights from T5 and subsequently finetune it on the CONVAI2 dataset as Gsup. The dense retriever is initialized with Contriever (Izacard et al., 2021), a dual-encoder retriever that shares a similar encoder structure to BERT (Devlin et al., 2019). We performed fine-tuning on both the retriever and generator using Gsup, with a learning rate of 5 \u00d7 10\u22124 and \u03c1 = 0.5. We further tune for learning retrieval augmentation based on the supervised foundation models in one epoch. We use persona profile as the query to retrieve the relevant stories."
        },
        {
            "heading": "4.4 Evaluation Metric",
            "text": "LAPDOG aims to optimize for some generation metrics to enhance the dialogue quality. The evaluation comprises three metrics. The first metric is F1, which computes the harmonic mean of precision and recall on a word level between the generated text and the ground truth. The second metric is BLEU (Papineni et al., 2002; Post, 2018), an ngram precision-based measure that quantifies the overlap between the generated text and the ground truth by penalizing for overly long or short outputs. The third metric is ROUGE-L (Lin, 2004), a variant of ROUGE that considers the longest common subsequence between the generated text and the ground truth, to effectively measure sentence-level structural similarity. With those metrics, we ensure a comprehensive assessment of the quality of generated dialogues. To enhance the aforementioned three metrics, we sum these three metrics together as the overall metric to train the LAPDOG model."
        },
        {
            "heading": "4.5 Baseline",
            "text": "To compare the enhancement between different retrieval-augmentation approaches, we compared the results with the following baselines. First, we compare the LAPGOG with T5S/B/XLsup models,\nwhere S/B/XL indicates the model size small, base, XL respectively. T5S/B/XLsup serves as the foundation models Gsup. We also add a fixed retriever Rfix initialized from Contriever to validate the effectiveness of tuned and untuned retrievers. Meanwhile, we utilized the reinforcement learning tuning (T5SSup+Rfix+RL) as one baseline, where the reward is set as the desired objective. Lastly, we introduce the RAG tuning that updates the retriever based on the generator\u2019s training output probabilities instead of the desired metric, which is to validate the direct and indirect metric tuning for the objective."
        },
        {
            "heading": "4.6 Results",
            "text": "The T5SSup model forms our baseline. Augmenting it with a fixed retriever, T5SSup+Rfix, shows a slight improvement in F1 and BLEU scores but a small decrease in the ROUGE-L score. This indicates a moderate enhancement in both F1 and BLEU, though the decrease in ROUGE-L suggests a trade-off in terms of capturing long-distance dependencies.\nThe results for reinforcement learning tuning, T5SSup+Rfix+RL, exhibit a significant degradation across all metrics, indicating that this method might be not so effective for this task. This could be due to the challenge of setting an appropriate reward function for reinforcement learning.\nThe T5SSup+RAG model slightly outperforms the baseline model in terms of F1 but performs worse in terms of BLEU and ROUGE-L. This suggests that while the model seems to generate more correct words, there may be a compromise on the overall grammatical and semantic quality\nof the generated text. In contrast, the LAPDOGenhanced model, T5SSup+LAPDOG, shows the highest improvements in all metrics among small models. This indicates that LAPDOG significantly enhances the ability to generate high-quality text and captures the desired metrics more effectively than other models.\nFor larger models, similar phenomena are observed. LAPDOG consistently delivers the best improvements over the base model, no matter whether it is T5BSup or T5 XL Sup. This suggests that the efficacy of LAPDOG is not confined to smaller models and scales well with the model size."
        },
        {
            "heading": "5 Ablation Studies",
            "text": "We conduct ablation studies with respect to the metrics, the number of candidates, candidate augmentation, and training strategy, respectively."
        },
        {
            "heading": "5.1 Analysis on Metrics",
            "text": "To understand the individual contribution of each metric, we perform ablation experiments by successively removing one metric from the combined optimization process with results shown in Table 2. When we remove BLEU, the performance experiences a slight drop across all metrics with BLEU\ndecreasing to 3.07, ROUGE-L to 14.35, and F1 to 14.29, suggesting that BLEU contributes to a more precise matching of response generation. Moreover, when we exclude F1, we see a more significant reduction in the performance, indicating the crucial role of F1 to ensure the overlap of words between the generated responses and the ground truth. Lastly, the removal of ROUGE-L also results in a decrease in the performance across all three metrics, showing its essential role in evaluating the coherence of generated dialogues. In summary, each metric contributes uniquely to the optimization process, and their combination in LAPDOG provides a more comprehensive guide for the generation of high-quality, personalized dialogues."
        },
        {
            "heading": "5.2 Number of the Candidates",
            "text": "As shown in Figure 2, where K increases from 2 to 6, we observe a consistent improvement in all metrics (i.e., BLEU, ROUGE-L, and F1). Generally, increasing the number of retrieval candidates improves the performance of the model, as evidenced by the improvements in the BLEU, ROUGE-L, and F1 scores. Interestingly, it is observed that the model performance does not monotonically increase with the number of candidates. The performance fluctuates as K varies, implying that the number of retrieval candidates needs to be carefully selected. Too few candidates may not provide enough information for generating responses, while too many ones may introduce irrelevant information, which could potentially confuse the model."
        },
        {
            "heading": "5.3 Candidate Augmentation",
            "text": "The influence of candidate augmentation is explored in two aspects: quantitative performance and the diversity of retrieved stories. Table 3 provides a comparison between the performance of the LAPDOG model with and without candidate aug-\nmentation. The incorporation of candidate augmentation leads to superior performance across all three evaluation metrics. Specifically, LAPDOG without candidate augmentation attains slightly lower scores in all three metrics. This indicates that the inclusion of candidate augmentation enhances the overall performance of our model, confirming its crucial role in the proposed LAPDOG model.\nTo further investigate the impact of candidate augmentation to the retrieval diversity, we count the number of unique stories retrieved during testing. As shown in Figure 3, the LAPDOG method with candidate augmentation retrieves 1570 unique stories, whereas without it, the model only retrieves 738 unique stories. This result implies that candidate augmentation significantly contributes to the retrieval diversity. Other methods like RAG and fixed retriever manage to retrieve 25 and 1351 unique stories, respectively. This underlines the effectiveness of the proposed candidate augmentation approach in enhancing the diversity of the retrieval process, which in turn can help generate more personalized and contextually rich responses."
        },
        {
            "heading": "5.4 Training Strategy",
            "text": "In contrast to the two-stage training process, we perform an ablation study by training LAPDOG from scratch, bypassing the first stage. As shown in Table 3, the results exhibit a significant decrease in all the metrics compared with the two-stage approach. Additionally, training directly from scratch requires more time to converge when compared with the two-stage training process. Overall, the two-stage training process is essential from both performance and efficiency standpoints."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we introduced LAPDOG, an endto-end learnable retrieval augmentation personalized dialogue generation framework. We show that LAPDOG jointly tunes the retriever with the generator to retrieve useful stories from the ROCStory dataset for enhancing the desired performance over the CONVAI2 dataset. LAPDOG gains consistent performance enhancement over language models with varying sizes."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work is supported by NSFC general grant 62076118 and Shenzhen fundamental research program JCYJ20210324105000003."
        },
        {
            "heading": "Limitations",
            "text": "Given resource constraints, in this paper, we employ language models such as T5 and do not conduct experiments based on currently prevalent large language models (Brown et al., 2020; Ouyang et al., 2022). Recognizing the enhanced reasoning capabilities of large language models, we posit that tuning the retrieval content with such models could yield significant advantages. Additionally, due to resource limitations, we study a small number of extracted passages (i.e., 2-6) and a short context length (i.e., 512 tokens). Nevertheless, we anticipate that incorporating a larger set of integrated stories and a longer context would further enhance the performance. Also, a more diverse objective rather than the summation of F1, ROUGE, and BLEU might be more helpful to train an engaging conversational AI system. Also, the generator is simply a conventional T5 model rather than explicitly designed models, which could help improve the performance of the proposed LAPDOG model further."
        },
        {
            "heading": "Ethics Statement",
            "text": "This work proposes a novel LAPDOG model for personalized dialogue generation, focusing on generating highly tailored responses by leveraging persona profiles and dialogue context. As with all machine learning applications, it is crucial to consider the ethical implications. The use of personal information in our model is limited to fictional persona profiles, and we do not handle or store any real personal data in our experiments. However,\nwhen applying our model to real-world applications, careful consideration should be given to data privacy and consent. It is essential to ensure that all personal information used to generate personalized dialogues is obtained ethically and used with the individuals\u2019 informed consent. Moreover, the generated content should respect user privacy, dignity, and cultural sensitivities."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Detail Settings for Training",
            "text": ""
        },
        {
            "heading": "A.2 Query Analysis",
            "text": "As shown in Table 5, we aim to analyze the performance impact of different retriever queries. As indicated in Table 5, using the Persona alone as a query achieves the highest BLEU score, albeit with a slight trade-off in ROUGE-L and F1 scores. When combining Persona with the Dialogue, the ROUGE-L and F1 scores improve marginally, but at the expense of a slightly decreasing in BLEU. Following the idea of forward retrieval (Jiang et al., 2023), we experimented with using the generator\u2019s output (Generated) as a query, but observed less competitive performance. Lastly, a strategy of using a single sentence from the persona profile chosen at random (One Persona) resulted in worse performance on all metrics."
        },
        {
            "heading": "A.3 Evaluation Results on CONVIAI2 Revised Dataset",
            "text": "As shown in Table 6, LAPDOG consistently enhances the performance on the revised dataset, where the persona is paraphrased to more implicit background sentences. The revised version is considered a more difficult task than the original task."
        },
        {
            "heading": "A.4 Case Study",
            "text": "As shown in Tables 11 and 12, we present the case studies on two conversations to compare the generation results among the non-retrieval-augmented results of T5XLsup, retrieval augmented results of T5XLsup+LAPDOG, and the ground truth.\nFor the conversation in Table 11, the agent is going to talk about divorce as indicated in the ground\ntruth. LAPDOG retrieves several stories about the bad days with his life and family, and this could be a clue for him to decide to divorce. In this conversation, T5XLsup complains about his life but does not mention anything about divorce. LAPDOG, reinforced by the retrieval stories and persona, has a stronger intention to generate the divorce decision, which would be more aligned with the intention in the ground truth. The other retrieved stories describe the messes that happened during the working time, which may reflect the persona \u201cI hate my job.\u201d\nIn the conversation mentioned in Table 12, the conversation is a simple start with a \u201cHow are you doing today?\u201d, T5XLsup gives a standard, safe, but bland answer as \u201cI am good. How are you?\u201d, while LAPDOG incorporates stories and persona about the gym to answer with \u201cI\u2019m good. Just got back from the gym. How are you?\u201d, which would be more information-intensive and engaging. Additionally, the story \u201cLifestyle Change\u201d provides a good clue for the agent about why he decided to go to the gym, and the story \u201cHome Gym\u201d describes the enthusiasm about the workout. These would potentially provide the model with enriched information on generating personalized responses.\nAs shown in Table 13, the T5XLsup model simply replied with a \u201cI hope so!\u201d without further informative content, while LAPDOG answers with richer information as \u201cI hope so! I\u2019m only in grade 3 so I\u2019m hoping to go to Disney World soon!\u201d, which might consider the information from both persona and stories. Additionally, the retrieved stories like \u201cDream Job\u201d or \u201cDisneyland\u201d are aligned with the agent\u2019s favor.\nAlgorithm 1 Pre-processing Procedure of Story Corpus Input: A story corpus Dori Output: A pre-processed story corpus D\n1: Extract named entity using BERT-BASE-NER from Dori 2: Filter out person-related named entity with tag B-PER 3: Replace the person-related named entity with the first-person narratives within stories in Dori 4: Process the first-person stories over a grammatical error correct model gec-t5_small 5: Output the corrected stories as D\nAlgorithm 2 Complete Training Process of LAPDOG Input: Persona sentences P , dialogue context U , a ground truth response y, a generator G, a dense\nretriever R, and a story corpus D Output: A tuned retriever Rtuned, a tuned generator Gtuned\n1: Construct query q from a query function Query(P,U) 2: Stage1: 3: Initialize Gsup with G 4: Train a supervised tuned generator Gsup given input (P,U) and ground truth y 5: Train Gsup until converge 6: Stage2: 7: Retrieve top-K stories Dq given q 8: Apply Candidate Augmentation Daugq = CandAug(di, \u03c1); di \u2208 Dq 9: Compute retriever scores Saugq between query q and D aug q\n10: for Retrieved story di in Daugq do 11: Construct augmented input by concatenation ai = [di;P ;U ] 12: Generate text by predi = Gsup(ai) 13: Compute metrics as mi = M(predi, y) 14: end for 15: Gather M = {softmax(mi)}Ki=1, A = {softmax(ai)}Ki=1 16: Compute retriever\u2019s loss by LaugR = KL(M,S aug q ) 17: for augmented input ai in A do 18: Compute negative log-likelihood loss LG with input ai and ground truth target y 19: Update Gsup and R by L = LaugR + LG as Gtuned and Rtuned 20: end for 21: Repeat the steps in Stage2 until converge"
        },
        {
            "heading": "A.5 Comparison to Traditional Knowledge Dialogue Approaches",
            "text": "Evaluating LAPDOG against traditional knowledge-grounded dialogue methods is crucial. A comparison with \u201cLow-Resource Knowledge-Grounded Dialogue Generation\u201d (Zhao et al., 2020) was considered but not possible due to the unavailability of its code. Instead, we selected ITDD (Incremental Transformer with Deliberation Decoder) (Li et al., 2019) for this purpose, given its proven effectiveness and wide recognition.\nA.5.1 ITDD Experimental Setup\nLAPDOG and ITDD are based on different principles. LAPDOG uses an unsupervised approach to train both a retriever and a generator for extracting relevant content from an external corpus. On the other hand, ITDD is designed to merge preannotated document-conversation pairs. To ensure a fair comparison, we used an off-the-shelf retriever (Izacard et al., 2021) to create paired data from the persona and ROC story corpus for the ITDD model."
        },
        {
            "heading": "A.5.2 Comparison Results",
            "text": "We compared LAPDOG and ITDD using various metrics, and the results are presented in the table below, showcasing LAPDOG\u2019s superior performance.\nThese results highlight LAPDOG\u2019s effectiveness compared to the traditional ITDD method, enhancing the paper\u2019s overall assessment of LAPDOG\u2019s performance."
        },
        {
            "heading": "A.6 Complete Training Procedure",
            "text": "The complete training procedure is described at Algorithm 2."
        },
        {
            "heading": "A.7 Pre-process ROCStory Coprus",
            "text": "The pre-processing procedure is described at Algorithm 1."
        },
        {
            "heading": "A.8 Extended Evaluation Metrics",
            "text": "The evaluation metrics for LAPDOG have been broadened to incorporate METEOR and BERT scores. This enhancement supplements the foundational assessment based on F1, BLEU, and ROUGE-L metrics, presenting a more diverse evaluation landscape. The additional evaluation outcomes are tabulated in Table 9.\nBased on the results in Table 9, LAPDOG excels in the METEOR score relative to baseline models, showcasing its capability in nuanced linguistic comprehension. However, the variance in BERT scores is minimal, likely due to LAPDOG\u2019s optimization for traditional metrics. Enhancing performance by tailoring optimization for BERT scores represents a promising area for future inquiry."
        },
        {
            "heading": "A.9 Additional Related Work on Large Language Models (LLMs)",
            "text": "Language models compute probability distributions over text sequences. Recent advancements have escalated these models from millions of parameters (Radford et al., 2019) to billions (Brown et al., 2020), extending the training corpus to encompass web texts and instructional data (Ouyang et al., 2022). These strides have significantly enhanced the performance of large language models (LLMs) across a myriad of NLP tasks. Notably, in conversational tasks, the quality of generated text improves with the expansion of both the model size and training corpus. Our proposed approach, LAPDOG, diverges from the prevailing trend of scaling; it leverages retrieval-augmented generation to yield more diverse and interpretable results, albeit with smaller model parameters and corpus size. Despite employing a smaller model in our experiments, we posit that our adaptive retrieval approach could complement existing LLMs, thereby potentially elevating their result-generation efficacy."
        },
        {
            "heading": "A.10 Human Evaluation",
            "text": "We conducted a human evaluation to gauge the preference between the retrieval-augmented LAPDOG and the fine-tuned T5 model. Evaluators were presented with a dialogue accompanied by two responses from each model and were asked to choose their preferred response.\nReferring to Table 10, while LAPDOG was preferred by 56.32% of the evaluators, the difference between the two models is relatively narrow. This indicates that both models have their merits in\ncertain conversational contexts. The slight edge for LAPDOG suggests that retrieval-augmented responses might align closer to human expectations in some scenarios. Delving deeper into the nuances of this preference, especially across varied dialogue topics, would provide a more comprehensive understanding."
        }
    ],
    "title": "Learning Retrieval Augmentation for Personalized Dialogue Generation",
    "year": 2023
}