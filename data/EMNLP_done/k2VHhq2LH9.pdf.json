{
    "abstractText": "Natural language reasoning plays an increasingly important role in improving language models\u2019 ability to solve complex language understanding tasks. An interesting use case for reasoning is the resolution of contextdependent ambiguity. But no resources exist to evaluate how well Large Language Models can use explicit reasoning to resolve ambiguity in language. We propose to use ambiguous definite descriptions for this purpose and create and publish the first benchmark dataset consisting of such phrases. Our method includes all information required to resolve the ambiguity in the prompt, which means a model does not require anything but reasoning to do well. We find this to be a challenging task for recent LLMs. Code and data available at: https://github.com/ sfschouten/exploiting-ambiguity",
    "authors": [
        {
            "affiliations": [],
            "name": "Stefan F. Schouten"
        },
        {
            "affiliations": [],
            "name": "Peter Bloem"
        }
    ],
    "id": "SP:4665e1fbcd73f1b51a868211843e3a2913b8b20a",
    "references": [
        {
            "authors": [
                "Elias Frantar",
                "Saleh Ashkboos",
                "Torsten Hoefler",
                "Dan Alistarh"
            ],
            "title": "OPTQ: Accurate Quantization for Generative Pre-trained Transformers",
            "year": 2023
        },
        {
            "authors": [
                "Xinyang Geng",
                "Arnav Gudibande",
                "Hao Liu",
                "Eric Wallace",
                "Pieter Abbeel",
                "Sergey Levine",
                "Dawn Song."
            ],
            "title": "Koala: A dialogue model for academic research",
            "venue": "Blog post.",
            "year": 2023
        },
        {
            "authors": [
                "Jie Huang",
                "Kevin Chen-Chuan Chang."
            ],
            "title": "Towards Reasoning in Large Language Models: A Survey",
            "venue": "ArXiv:2212.10403 [cs].",
            "year": 2023
        },
        {
            "authors": [
                "Vid Kocijan",
                "Ernest Davis",
                "Thomas Lukasiewicz",
                "Gary Marcus",
                "Leora Morgenstern."
            ],
            "title": "The Defeat of the Winograd Schema Challenge",
            "venue": "ArXiv:2201.02387",
            "year": 2023
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Huu Nguyen",
                "Alexander Mattick."
            ],
            "title": "OpenAssistant Conversations \u2013 Democratizing Large Language Model Alignment",
            "venue": "ArXiv:2304.07327 [cs].",
            "year": 2023
        },
        {
            "authors": [
                "Hector Levesque",
                "Ernest Davis",
                "Leora Morgenstern."
            ],
            "title": "The Winograd Schema Challenge",
            "venue": "Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning.",
            "year": 2012
        },
        {
            "authors": [
                "Alisa Liu",
                "Zhaofeng Wu",
                "Julian Michael",
                "Alane Suhr",
                "Peter West",
                "Alexander Koller",
                "Swabha Swayamdipta",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "We\u2019re Afraid Language Models Aren\u2019t Modeling Ambiguity",
            "venue": "ArXiv:2304.14399 [cs].",
            "year": 2023
        },
        {
            "authors": [
                "Sewon Min",
                "Julian Michael",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer."
            ],
            "title": "AmbigQA: Answering Ambiguous Open-domain Questions",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5783\u2013",
            "year": 2020
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 Technical Report",
            "venue": "ArXiv:2303.08774 [cs].",
            "year": 2023
        },
        {
            "authors": [
                "Shuofei Qiao",
                "Yixin Ou",
                "Ningyu Zhang",
                "Xiang Chen",
                "Yunzhi Yao",
                "Shumin Deng",
                "Chuanqi Tan",
                "Fei Huang",
                "Huajun Chen."
            ],
            "title": "Reasoning with Language Model Prompting: A Survey",
            "venue": "ArXiv:2212.09597",
            "year": 2023
        },
        {
            "authors": [
                "Mirac Suzgun",
                "Nathan Scales",
                "Nathanael Sch\u00e4rli",
                "Sebastian Gehrmann",
                "Yi Tay",
                "Hyung Won Chung",
                "Aakanksha Chowdhery",
                "Quoc V. Le",
                "Ed H. Chi",
                "Denny Zhou",
                "Jason Wei"
            ],
            "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "venue": "arXiv:2201.11903 [cs]. ArXiv: 2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Zonglin Yang",
                "Xinya Du",
                "Rui Mao",
                "Jinjie Ni",
                "Erik Cambria."
            ],
            "title": "Logical Reasoning over Natural Language as Knowledge Representation: A Survey",
            "venue": "ArXiv:2303.12023 [cs].",
            "year": 2023
        },
        {
            "authors": [
                "Fei Yu",
                "Hongbo Zhang",
                "Prayag Tiwari",
                "Benyou Wang."
            ],
            "title": "Natural Language Reasoning, A Survey",
            "venue": "ArXiv:2303.14725 [cs].",
            "year": 2023
        },
        {
            "authors": [
                "Yuewei Yuan",
                "Chaitanya Malaviya",
                "Mark Yatskar."
            ],
            "title": "AmbiCoref: Evaluating Human and Model Sensitivity to Ambiguous Coreference",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 1023\u20131030, Dubrovnik, Croatia.",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Natural language understanding and reasoning are interdependent skills: reasoning with natural language presupposes a level of understanding; but full understanding may require the resolution of ambiguities through reasoning. Complex ambiguity in particular could benefit from explicit \u2018out loud\u2019 reasoning, such as the reasoning that is produced with chain-of-thought prompts.\nExisting resources used to evaluate reasoning are not well suited to investigate the capability of resolving ambiguity by explicit reasoning. Some existing benchmarks require the resolution of ambiguity, but focus only on ambiguity that humans can resolve intuitively (e.g. Winograd schemas, Levesque et al. 2012). The ability to reason with natural language is often evaluated with tasks considered complex enough to require it. These may or may not include ambiguities of various types, making them poorly suited to evaluate when models are able to resolve ambiguity and which types. Such tasks may also benefit from abilities besides\nexplicit reasoning, such as factual recall. Thus, improvements on these tasks cannot be easily attributed to improvements in reasoning.\nIn this paper we create a new benchmark dataset which requires models to resolve ambiguous definite descriptions. Definite descriptions are phrases that denote entities by describing the properties or roles that are unique to them within the relevant context (e.g. \u201cthe pope\u201d, \u201cjohn\u2019s mother\u201d, \u201cour king\u201d). We use ambiguous descriptions which denote one of two entities, and include information on both entities in context. Specifically, we introduce a de dicto / de re ambiguity by including a temporal operator (see Figure 1 for an example). By asserting something that is true of only one of these entities, one of the two interpretations can be excluded by reasoning.\nWe demonstrate the value of this approach by creating a new benchmark dataset generated from Wikidata. We explicitly include the knowledge required for disambiguation in the prompt of each\ninstance. Doing so ensures that when a system answers incorrectly, its failure stemmed from its inability to adequately reflect on the given information. We perform experiments showing the performance of recent large language models on our benchmark. We find that: (1) GPT4 and chain-ofthought prompting perform best (2) the LLaMAbased OpenAssistant model beats GPT3.5 in average accuracy, although the latter is more consistent; and (3) de re instances are harder than de dicto instances for all models.\nIn summary, our contributions are: (a) an outline of ambiguous definite descriptions, an under-explored class of problems useful for the evaluation of systems that reason with and about natural language (section 3); (b) a new benchmark dataset consisting of ambiguous definite descriptions, showcasing the value of the problem class (section 4), and finally (c) experimental results showing the performance of recent large language models on this benchmark (section 5)"
        },
        {
            "heading": "2 Related Work",
            "text": "Reasoning is a broad term and the tasks related to it are varied. We focus on the type of reasoning that is explicit and is done with natural language, which has recently been surveyed extensively (Qiao et al., 2023; Huang and Chang, 2023; Yu et al., 2023). This paradigm became possible with the advent of large language models (LLMs) such as ChatGPT and GPT4 (OpenAI, 2023). These models can be prompted to perform \u2018chain-of-thought\u2019 reasoning (Wei et al., 2022); even in a zero-shot setting (Suzgun et al., 2022). This has significantly improved performance for many tasks (Suzgun et al., 2022). Using a new dataset of ambiguous definite descriptions, our experiments evaluate to what extent LLMs can use (chain-of-thought) reasoning to resolve ambiguity in language.\nRecent work on ambiguity includes the construction of both curated (Liu et al., 2023; Min et al., 2020) and synthetic datasets (Yuan et al., 2023). Such datasets investigate ambiguity in a variety of tasks such as: natural language inference, opendomain question answering, etc. Our dataset is synthetically generated (but factual), and focuses specifically on de dicto / de re ambiguities, forming a binary classification task.\nThe term \u2018logical reasoning over natural language\u2019 (LRNL) was coined by Yang et al. (2023) to talk about a new trend where natural language\nis used to represent knowledge and LLMs are used to reason over that knowledge. One challenge this paradigm could have to overcome is the potential for ambiguity inherent in natural language. Our work intentionally introduces ambiguity, and evaluates how well LLMs can resolve it by reasoning.\nPreviously, Winograd schemas (Levesque et al., 2012) have been used in various benchmarks to test for commonsense reasoning. These schemas consist of sentences with ambiguous pronouns which must be resolved in one of two ways. However, existing schemas focus on ambiguities that humans resolve intuitively (without the need for explicit reasoning). Creating new schemas that do require explicit reasoning does not appear straightforward, especially since creating them has proved difficult in general (Kocijan et al., 2023). Our method also involves ambiguous noun phrases, but requires explicit reasoning about definite descriptions rather than implicit reasoning about pronouns."
        },
        {
            "heading": "3 Ambiguous Definite Descriptions",
            "text": "Definite descriptions are noun phrases that denote entities by describing the properties or roles that are unique to them within the relevant context. Examples of such phrases in English include among others: \u201cthe pope\u201d, \u201cjohn\u2019s mother\u201d, and \u201cour king\u201d."
        },
        {
            "heading": "De dicto vs. de re",
            "text": "The kind of ambiguities we use are known as de dicto / de re ambiguities. They involve a statement that is either true of what is said (de dicto) or true of the thing (de re). Take for example the sentence depicted in Figure 1, where it is unclear if the description \u201cthe pope\u201d denotes the current pope (Francis) or the previous pope (Benedict). The source of the ambiguity is the phrase \u201cIn 2010\u201d, which can be read as primarily relating to the description (\u201cthe pope\u201d), meaning the property is ascribed to the pope from 2010 (Benedict). Or, it can be read as relating primarily to the property ascription itself (\u201cwas a native speaker of German\u201d), in which case the property is being ascribed to the current pope but is qualified as being true in 2010.\nThis type of ambiguity is the result of descriptions being combined with a non-extensional operator of which the temporal \u2018In PERIOD\u2019 is just one example. Other non-extensional operators could be used, such as modal operators, or propositional attitudes like \u2018PERSON believes that\u2019 or \u2018PERSON hopes that\u2019."
        },
        {
            "heading": "4 RADD-Wikidata-5-EN",
            "text": "To demonstrate the value of Reasoning about Ambiguous Definite Descriptions we create a (semi-)automatically generated dataset based on Wikidata. This dataset is based on 5 Wikidata property-pairs, and is in English. We focus on creating ambiguous sentences with a temporal operator. Wikidata contains many triples qualified with the \u2018start time\u2019 and \u2018end time\u2019 properties to indicate the period in which they were true.\nIn Table 1, one can see a complete example instance from this dataset. For a given main entity (e.g., Lars Lervik) we find the relations corresponding to that entity (Lars Lervik\u2019s military units and the beginning and end of his term with them). This information is combined with a property of the relations that does not change over time (the size class of the military units). Pairs of these relations are sampled such that the property (the size class) has different values for the two relations (in this case, one brigade and one battalion). Note that, by changing the property we ascribe to the denotee, we can flip which interpretation is correct (battalion instead of brigade) so we can generate two mirror instances for each entity and relation-pair.\nWe prioritized finding a small diverse set of property-pairs. Besides requiring a property whose value changes over time and a property whose value does not, the combination of properties also must yield sufficient results on wikidata (we required at least 50 to create the 100 samples). We also filtered\nout property-pairs where the results were dominated by a single entity, for example, had the data for the \u2018person-military_unit-size_class\u2019 pair consisted of only a handful of people changing units many different times, we would not have included it. See Table 2 for details on which property pairs we include and what they represent.\nWe include all facts relevant to the main entity in each prompt whether they are necessary to resolve the ambiguity or not, this allows us to measure if models can work around distractor facts."
        },
        {
            "heading": "4.1 Template design considerations",
            "text": "We use templates to generate the instances in the dataset, and while experimenting we identified a few things to avoid when phrasing them.\nAvoiding disambiguation by verb tense. The ambiguous description must not contain a verb whose tense favours one interpretation over the other. For example with:\n\u201cIn MONTHYEAR, the club for which X is/was head coach was from Y.\u201d\nIf we use \u2018X is head coach\u2019 it implies that \u2018In MONTHYEAR\u2019 does not scope over the definite description, whereas with \u2018X was head coach\u2019 the implication is that it does scope over the definite description (assuming MONTHYEAR is in the past). Thus, we avoid using such phrases, for example, by formulating the example as:\n\u201cIn MONTHYEAR, the club with X as its head coach was from Y.\u201d\nAvoiding event-referencing properties. We also avoid referencing events in the property ascription, which can cause \u2018in MONTHYEAR\u2019 to be read as referring to the time of the event, for example:\n\u201cIn MONTHYEAR, the club with X as its head coach was founded by Y.\u201d\nSometimes we can avoid this by rephrasing: \u201cIn MONTHYEAR, the club with X as its head coach had Y as its founder.\u201d\nOne property-pair was discarded because we could not find a good way to phrase the sentence, this was about the location of formation (P740) of distributors of creative works (P750)."
        },
        {
            "heading": "5 Experiments",
            "text": "We evaluate three dialogue-oriented large language models (LLMs) on our benchmark in a zero-shot setup. The models we include are: (a) Koala-13B\u2020 (Geng et al., 2023)1\n(b) LLaMA-33B\u2020 (Touvron et al., 2023) finetuned on version 7 of OpenAssistant Conversations (K\u00f6pf et al., 2023)2 (c) GPT-3.5(-turbo-0301) (OpenAI, 2022) (d) GPT-4(-0613) (OpenAI, 2022) Models marked with \u2020 were used with 4-bit OPTQ quantization (Frantar et al., 2023).\nThe evaluation is performed on 500 instances (100 per property-pair). To prompt the language models, we start with instances such as the one from the \u2018example\u2019 column in Table 1 and number the premises P1 through PN . Next, we shuffle the order of the interpretations, and then append one of the following two instructions: direct=\u201cAnswer only with \"Option 1\" or \"Option 2\", explain your decision after.\u201d or chain-of-thought=\u201cWhen answering, let\u2019s consider both options, and think step by step. DO NOT repeat the premises, only refer to their number.\u201d When using chain-ofthought (CoT), we follow up with a second message: \u201cBased on this, what is your final answer,\n1hf.co/TheBloke/koala-13B-GPTQ-4bit-128g 2hf.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ\n\"Option 1\" or \"Option 2\"?\u201d The models were prone to repeating the premises in their answer, thus warning them not to do this was necessary to avoid them from running out of context-window before providing the final answer."
        },
        {
            "heading": "5.1 Quantitative results",
            "text": "Figure 2 shows the accuracies obtained by each model for each property-pair and on average. The precision, recall and F1 scores for each model, class, and prompt style can be seen in Table 3. The highest average accuracy is obtained by GPT4 (83.2%), followed by OAsst-33B (67.7%) and GPT3.5\u2019s (64.8%). The mean accuracy of Koala-13B is roughly equal to random guessing (52.2%). Looking at the performance for the various property-pairs, we can see that GPT3.5, although less accurate overall, is more consistent than OAsst-33B. The performance for OAsst-33B is particularly low on the P6_P19 pair. This can be explained by the average number of premises (41.14) which is over twice the amount as included for the other property pairs (6.37 - 17.5 on average, never fewer than 5). The long prompts exceed the number of tokens included in the context window during training for OAsst-33B (and Koala-13B).\nNote also that each model benefits from the use of chain-of-thought prompting.\nIn Table 4, we can see the confusion table for each model and prompt style. We can see that there are (up to 4%) cases where the models do not produce a parseable result. Finally, we observe that the de re instances are the more difficult class; only GPT4 with chain-of-thought is able to perform better than random guessing on these instances."
        },
        {
            "heading": "5.2 Error Analysis",
            "text": "We perform a small analysis of the answers given by the best-performing model GPT4 to give insight into what areas need the most improvement.\nAbout half of the mistakes made by GPT4 involve the model not properly following the chainof-thought instruction, meaning the model already made a prediction in the first one or two sentences of its response. Although, ignoring this instruction seems to be common among both the de dicto and de re classes. It seems that absent any reasoning the\nmodels have a strong preference for predicting de dicto (this can also be seen from the confusion matrix in Table 4). This explains why this behaviour mostly produces errors on the de re class.\nAbout a third of all de re chain-of-thought answers conclude that neither option is correct. Although despite of this GPT4 almost always still predicts one of the two options. This seems to indicate that the models completely fail to consider the de re option. Take for example the following ambiguous sentence \u201cThe birth place of Prague 6\u2019s head of state was Tr\u030ceb\u00edc\u030c in November 2001\" for which the correct interpretation is Option 2: \u2018Prague 6\u2019s current head of state (Marie Kousal\u00edkov\u00e1) was, as of November 2001, born in Tr\u030ceb\u00edc\u030c\u2019. GPT4 reasons as follows: \u201cOption 2 can be eliminated based on premises (P6) and (P9), as Marie Kousal\u00edkov\u00e1 was born in Tr\u030ceb\u00edc\u030c, but she was not the head of state in November 2001, according to (P1) and (P2). Hence, this interpretation would be incorrect.\u201d But, this reasoning is clearly incorrect, since her being head of state in November 2001 is not required for Option 2 to be correct. This type of error seems to be a large part of why the models perform worse for de re instances.\nTogether these two types of errors appear to make up the vast majority of GPT4\u2019s mistakes."
        },
        {
            "heading": "6 Conclusion",
            "text": "We have introduced Reasoning about Ambiguous Definite Descriptions as a way to evaluate how well systems can use natural language reasoning to resolve ambiguous language and have created the first benchmark dataset to demonstrate the value of this task. Our findings show that recent LLMs are not yet capable of solving this task reliably, and that they particularly struggle with de re instances. Our error analysis suggests that models\u2014besides not always \u2018thinking step-by-step\u2019 as instructed\u2014 do not properly consider the de re options, instead hallucinating extra conditions for their correctness.\nIn future work, we will expand our work by: (1) testing how changing the provided information (e.g. removing regularities) affects the performance; (2) embedding this problem in other tasks such that the ability to solve it depends on the resolution of the ambiguity; (3) constructing multi-lingual versions to evaluate if the ability to resolve ambiguity through reasoning is independent of the language; and (4) extending our method to other extensional operators, and other types of ambiguities.\nLimitations\nThe de dicto / de re ambiguity we use in our dataset is one of many possible kinds of ambiguities. Completely excluding the possibility that our results rely on the particulars of this ambiguity will require a diverse set of ambiguities.\nSo far we have only included English prompts in our benchmark. We leave the creation of prompt templates in other languages for future work.\nWe have performed a \u2018best-effort\u2019 tuning of the prompts. It is plausible that with a more extensive tuning better prompts can be found and that overall performance could be improved to some degree. An extensive tuning may also reveal that each model benefits from different prompts, which could also change their relative performance."
        },
        {
            "heading": "Acknowledgements",
            "text": "This research was supported by Huawei Finland through the DreamsLab project. All content represented the opinions of the authors, which were not necessarily shared or endorsed by their respective employers and/ or sponsors."
        }
    ],
    "title": "Reasoning about Ambiguous Definite Descriptions",
    "year": 2023
}