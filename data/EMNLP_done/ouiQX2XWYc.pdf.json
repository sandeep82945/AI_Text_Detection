{
    "abstractText": "Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of opensource large language models. This paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. The watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. We successfully plant the watermark into open-source large language model weights including GPT-Neo and LLaMA. We hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Linyang Li"
        },
        {
            "affiliations": [],
            "name": "Botian Jiang"
        },
        {
            "affiliations": [],
            "name": "Pengyu Wang"
        },
        {
            "affiliations": [],
            "name": "Ke Ren"
        },
        {
            "affiliations": [],
            "name": "Hang Yan"
        },
        {
            "affiliations": [],
            "name": "Xipeng Qiu"
        }
    ],
    "id": "SP:4caaa192a16333a6f7f2232d83cdcd8036b4747c",
    "references": [
        {
            "authors": [
                "Anton Bakhtin",
                "Sam Gross",
                "Myle Ott",
                "Yuntian Deng",
                "Marc\u2019Aurelio Ranzato",
                "Arthur D. Szlam"
            ],
            "title": "Real or fake? learning to discriminate machine from human generated text. ArXiv, abs/1906.03351",
            "year": 2019
        },
        {
            "authors": [
                "Sid Black",
                "Leo Gao",
                "Phil Wang",
                "Connor Leahy",
                "Stella Biderman."
            ],
            "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow",
            "venue": "If you use this software, please cite it using these metadata.",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Jianfei Chen",
                "Yu Gai",
                "Zhewei Yao",
                "Michael W Mahoney",
                "Joseph E Gonzalez."
            ],
            "title": "A statistical framework for low-bitwidth training of deep neural networks",
            "venue": "Advances in Neural Information Processing Systems, 33:883\u2013894.",
            "year": 2020
        },
        {
            "authors": [
                "Falcon Dai",
                "Zheng Cai."
            ],
            "title": "Towards nearimperceptible steganographic text",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4303\u20134308, Florence, Italy. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Tim Dettmers",
                "Mike Lewis",
                "Younes Belkada",
                "Luke Zettlemoyer."
            ],
            "title": "Llm",
            "venue": "int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339.",
            "year": 2022
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "arXiv preprint arXiv:1805.04833.",
            "year": 2018
        },
        {
            "authors": [
                "Tina Fang",
                "Martin Jaggi",
                "Katerina Argyraki."
            ],
            "title": "Generating steganographic text with LSTMs",
            "venue": "Proceedings of ACL 2017, Student Research Workshop, pages 100\u2013106, Vancouver, Canada. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Xuanli He",
                "Qiongkai Xu",
                "Lingjuan Lyu",
                "Fangzhao Wu",
                "Chenguang Wang."
            ],
            "title": "Protecting intellectual property of language generation apis with lexical watermark",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10758\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Xuanli He",
                "Qiongkai Xu",
                "Yi Zeng",
                "Lingjuan Lyu",
                "Fangzhao Wu",
                "Jiwei Li",
                "Ruoxi Jia."
            ],
            "title": "CATER: Intellectual property protection on text generation APIs via conditional watermarks",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Qiao Jin",
                "Bhuwan Dhingra",
                "Zhengping Liu",
                "William Cohen",
                "Xinghua Lu."
            ],
            "title": "PubMedQA: A dataset for biomedical research question answering",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "John Kirchenbauer",
                "Jonas Geiping",
                "Yuxin Wen",
                "Jonathan Katz",
                "Ian Miers",
                "Tom Goldstein."
            ],
            "title": "A watermark for large language models",
            "venue": "arXiv preprint arXiv:2301.10226.",
            "year": 2023
        },
        {
            "authors": [
                "Keita Kurita",
                "Paul Michel",
                "Graham Neubig."
            ],
            "title": "Weight poisoning attacks on pretrained models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2793\u2013 2806, Online. Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Linyang Li",
                "Demin Song",
                "Xiaonan Li",
                "Jiehang Zeng",
                "Ruotian Ma",
                "Xipeng Qiu."
            ],
            "title": "Backdoor attacks on pre-trained models by layerwise weight poisoning",
            "venue": "Conference on Empirical Methods in Natural Language Processing.",
            "year": 2021
        },
        {
            "authors": [
                "Linyang Li",
                "Pengyu Wang",
                "Ke Ren",
                "Tianxiang Sun",
                "Xipeng Qiu."
            ],
            "title": "Origin tracing and detecting of llms",
            "venue": "arXiv preprint arXiv:2304.14072.",
            "year": 2023
        },
        {
            "authors": [
                "Ye Lin",
                "Yanyang Li",
                "Tengbo Liu",
                "Tong Xiao",
                "Tongran Liu",
                "Jingbo Zhu."
            ],
            "title": "Towards fully 8-bit integer inference for the transformer model",
            "venue": "arXiv preprint arXiv:2009.08034.",
            "year": 2020
        },
        {
            "authors": [
                "Eric Mitchell",
                "Yoonho Lee",
                "Alexander Khazatsky",
                "Christopher D. Manning",
                "Chelsea Finn."
            ],
            "title": "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
            "venue": "ArXiv, abs/2301.11305.",
            "year": 2023
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "ArXiv, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "year": 2018
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "venue": "arXiv preprint arXiv:1606.05250.",
            "year": 2016
        },
        {
            "authors": [
                "Sheng Shen",
                "Zhen Dong",
                "Jiayu Ye",
                "Linjian Ma",
                "Zhewei Yao",
                "Amir Gholami",
                "Michael W Mahoney",
                "Kurt Keutzer."
            ],
            "title": "Q-bert: Hessian based ultra low precision quantization of bert",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
            "year": 2020
        },
        {
            "authors": [
                "Adaku Uchendu",
                "Thai Le",
                "Kai Shu",
                "Dongwon Lee."
            ],
            "title": "Authorship attribution for neural text generation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8384\u20138395, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Ofir Zafrir",
                "Guy Boudoukh",
                "Peter Izsak",
                "Moshe Wasserblat."
            ],
            "title": "Q8bert: Quantized 8bit bert",
            "venue": "2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS), pages 36\u201339. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ari Holtzman",
                "Hannah Rashkin",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Franziska Roesner",
                "Yejin Choi."
            ],
            "title": "Defending against neural fake news",
            "venue": "Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.",
            "year": 2019
        },
        {
            "authors": [
                "Zhengyan Zhang",
                "Guangxuan Xiao",
                "Yongwei Li",
                "Tian Lv",
                "Fanchao Qi",
                "Zhiyuan Liu",
                "Yasheng Wang",
                "Xin Jiang",
                "Maosong Sun."
            ],
            "title": "Red alarm for pretrained models: Universal vulnerability to neuronlevel backdoor attacks",
            "venue": "Machine Intelligence Re-",
            "year": 2023
        },
        {
            "authors": [
                "Zachary Ziegler",
                "Yuntian Deng",
                "Alexander Rush."
            ],
            "title": "Neural linguistic steganography",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro-",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of opensource large language models. This paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. The watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. We successfully plant the watermark into open-source large language model weights including GPT-Neo and LLaMA. We hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications. 1"
        },
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs), exemplified by ChatGPT, GPT-4 (Brown et al., 2020; OpenAI, 2023) from the GPT family (Radford et al., 2018), are capable of writing documents, and providing solutions for real-world questions at human-like standards. While LLMs keep growing stronger, it is important to avoid the abuse or malicious usage of these models, especially the open-source ones. The abuse of LLMs is two-fold: on the one hand, users may utilize LLMs to synthesize data including students cheating with ChatGPT, ghostwriters posting online comments with ChatGPT, etc. (Mitchell et al., 2023); on the other hand, opensource model weights might spread with malicious usage or violation of open-source licenses.\n\u2217Equal Contribution. \u2020Corresponding author.\n1We release all data at https://github.com/ Twilight92z/Quantize-Watermark\nIn this paper, we focus on protecting the model\u2019s parameters by planting watermarks in the model weights when releasing the models, benefiting the open-source LLMs. Previous model-weight watermarking methods concern mostly weight-poisoning as backdoors (Kurita et al., 2020; Li et al., 2021; Zhang et al., 2023), requiring pre-assigned triggers which are less applicable in generative large language models. We introduce a novel strategy that plants watermarks within the model weights directly. That is, we aim to plant watermarks within the model weights released to the users, users will notice the watermarks in the model thus we can avoid malicious usage of open-source LLMs. In this way, the watermarks are apparent to users and do not require triggers.\nWatermarking the LLMs in the model weights is a straightforward thought to protect the model ownership. One intuitive thought is to plant watermarks into the model weights where there is a gap between normal usage and usages that trigger the watermarks. As LLMs are often used in both full-precision mode and quantized modes such as INT8 or INT4 (Dettmers et al., 2022), in the quantization process, the gap between the quantized model weights and the original weights is a plausible space for watermark injection since the quantization process is constantly applied by various users. As seen in Figure 1, we hope to inject watermarks into the full-precision model and\nprovide a simplified version that is quantized to low precision such as INT8 or INT4. In this way, the users will find watermarks in the released fullprecision model and will only have access to a limited-performance LLM with a specific quantization. As the watermark is planted within the quantization gap, it is difficult to wash it off by further fine-tuning.\nSpecifically, we propose several algorithms that attempt to plant watermarks within the model weights and conduct experiments to test the effectiveness of the watermarking strategies. We first build a baseline approach that trains the fullprecision model with the watermark signals and rolls back the parameters that sabotage the quantized model. Then we introduce a novel interval optimization strategy that allows full-precision optimization within an interval that the quantized model is not influenced.\nUsing our proposed quantization watermarking strategies, we explore multiple real-world deployment scenarios in which LLMs should be watermarked to claim ownership. Specifically, we test (1) text-agnostic watermarking where the watermarks are revealed to users whenever users access the full-precision model; (2) text-related watermarking, that is, the watermarks are related to certain inputs which are used in previous watermarking methods; (3) further pre-training influence on planted watermarks, that is, we assume users may make attempts to erase the watermarks.\nBased on the experimental results, we observe that our proposed interval optimization quantization watermarking strategy successfully plants watermarks into the quantized model and enables the secure release of open-source LLMs. Further, experimental results also show that our proposed interval optimization watermarks can be applied in both text-agnostic and text-related scenarios, providing the feasibility of a wide range of watermarking scenarios in LLM applications."
        },
        {
            "heading": "2 Related Work",
            "text": "Watermarking LLMs involves various aspects of security problems in LLM applications, resulting in works with various strategies.\nModel Watermarking and Backdoors Watermarking neural networks (Fang et al., 2017; Ziegler et al., 2019; Dai and Cai, 2019; He et al., 2022b,a) is a trending topic especially with LLMs fastly developing (Kirchenbauer et al.,\n2023). In model watermarking, one line of work is to plant pre-defined triggers (Kurita et al., 2020; Li et al., 2021; Zhang et al., 2023) as backdoors, which can be used as watermarks in pre-trained models. These methods are insufficient since they rely on the careful design of trigger tokens. Recent works (Kirchenbauer et al., 2023) consider planting watermarks in the decoding strategies since LLMs are the most widely used NLP models (OpenAI, 2023; Brown et al., 2020). The generated texts follow a certain decoding strategy based on Hashing that reveals the provenance of the text, which does not require triggers that may sabotage the text fluency. Compared with watermarking in model weights, planting watermarks in the decoding process is less convenient since most LLM users adopt frameworks exemplified by Huggingface Transformers (Wolf et al., 2020) where appointing different model weights with the same model structure and decoding process is the most common solution.\nAI-Generated Text Detection\nThere is a close relationship between watermarking LLMs and its counterpart, AI-generated text detection: AI-generated text detection aims to discriminate whether a given text is from an AI (Zellers et al., 2019; Bakhtin et al., 2019; Uchendu et al., 2020; Mitchell et al., 2023), while origin tracing (Li et al., 2023) is to further discriminate which specific model. Watermark detection is to detect the watermark planted in the model or in the model-generated texts, which is similar to AIgenerated text detection and often studied simultaneously (Mitchell et al., 2023).\nQuantization of Neural Networks\nIn this paper, we hope to utilize model quantization in watermarking LLMs. Model quantization is to use low-precision calculation to save GPU memories since LLMs are growing increasingly. The 8-bit quantization method is to use INT8 precision to replace fp32 precision during inference, which has been widely explored (Chen et al., 2020; Lin et al., 2020; Zafrir et al., 2019; Shen et al., 2020; Dettmers et al., 2022). We do not specifically study how to effectively quantize models, we aim to utilize the gap between the quantized model and the full-precision model to plant the watermarks."
        },
        {
            "heading": "Quantization",
            "text": ""
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Quantization and De-quantization Process",
            "text": "In model quantization of transformers-based models, the most widely adopted quantization approach is the 8-bit Matrix Multiplication method (Dettmers et al., 2022) that introduces a vector-wise quantization method and quantizes model parameters in mixed precision.\nFormally, we define the quantization process that quantizes the original full-precision model with parameter \u03b80 to the quantized model with parameter \u03b8\u22170:\n\u03b8\u22170 = Q(\u03b80) (1)\n. For parameter \u03b80, for instance, given a weight matrix W \u2208 Rm\u2217n the scale index CW is the maximum number in the row with m parameters, and the quantized weight matrix WINT8 = W \u2217 (127/Cw). Accordingly, the input X is quantized in the same way, with the scale index set to the maximum number in the column order.\nIn the de-quantization process that converts quantized model parameters \u03b8\u22170 back to full-precision parameters, we define the de-quantization process as D(\u03b8\u22170), the de-quantized model parameter is:\n\u03b8\u03020 = D(\u03b8 \u2217 0) (2)\n. Similarly, the de-quantized weight, for instance, given a weight matrix W\u0302 = WINT8 \u2217 (Cw/127) while Cw is the scale index calculated during the quantization process Q(\u00b7). The de-quantized\nmodel \u03b8\u03020 is different from the full-precision model \u03b80, therefore, once the watermark is planted into the full-precision model, it is not possible to use the quantized model to recover the original fullprecision model without watermarks."
        },
        {
            "heading": "3.2 Planting Watermarks",
            "text": "We define the watermarking process that plants watermarks into the original full-precision model with parameter \u03b80 as \u03b8 = W(\u03b80). Here, the model \u03b8 is the model planted with our designed watermarks. After planting the watermarks, we hope that the quantized model of \u03b8 is not influenced, that is, we have:\n\u03b8\u2217 = \u03b8\u22170 (3)\nSupposing that the watermark is yW , when the watermark is shown regardless of the input x, for any input text x with its normal output y, with an LLM generation process f(\u00b7), we have:\nyW = f(x, \u03b8) (4)\ny = f(x, \u03b8\u2217) (5)\n. In this way, when the users obtain a full-precision model \u03b8, they are only allowed to use the INT8 inference since the full-precision is protected by the quantization watermarks. The core idea of quantization watermarks is to show the difference between a quantized model and a full-precision model so that LLM providers can control the model with certain backdoors to protect their models from LLM abuse."
        },
        {
            "heading": "3.3 Watermarking and Performance Maintaining",
            "text": "To plant watermarks, we introduce one baseline strategy that rolls back parameters to avoid sabotaging quantized models and a interval optimization strategy that maintains the quantized parameters.\nRoll Back Strategy In quantization watermarking, the goal is to maintain the performances unchanged in the quantized model, therefore, one intuitive baseline is to roll back parameters if the parameters are changed drastically after quantization.\nSuppose that the watermarking loss using loss function L(\u00b7) is to optimize parameters \u03b80:\n\u03b8 = \u03b80 \u2212 \u03b7\u2207L(f(x, \u03b80), yW) (6)\n. After quantization, the parameter \u03b8 is quantized to \u03b8\u2217, if the parameter is different from the previous quantized model parameter \u03b8\u22170, we simply roll back the parameters that are sabotaged after quantization. That is, given \u03b8i \u2208 \u03b8:\n\u03b8i = { \u03b8i, |\u03b8i\u2217 \u2212 \u03b8i\u22170 | < \u03f5 \u03b8i0, |\u03b8i\u2217 \u2212 \u03b8i\u22170 | \u2265 \u03f5\n(7)\n. Here, \u03f5 is the threshold we use to determine whether we apply the rollback strategy to the model parameters. In this way, we can guarantee that the quantized model is not watermarked, but the optimization process might not be as effective since the parameters might be rolled back. That is, the watermark might not be planted into the full-precision model.\nInterval optimization Strategy Based on the baseline rollback strategy, we propose a novel interval optimization method that optimizes the model parameters within an interval and therefore does not affect the quantization process to successfully plant the watermark.\nAs mentioned, the quantization process is \u03b8\u22170 = Q(\u03b80), and the de-quantization process is \u03b8\u03020 = D(\u03b8\u22170), we hope to find an interval that within the interval, the quantized model parameter is also the same with \u03b8\u22170. That is, for parameter \u03b8\ni\u2217 quantized from full-preicision parameter, the interval is ranged from \u03b8i\u2217l = \u03b8\ni\u2217 \u2212 \u03b1 to \u03b8i\u2217h = \u03b8i\u2217 + \u03b1, where \u03b1 = 0.4 in the INT8 quantization. Since the integer index is 127, within \u03b1 = 0.4, the parameter quantized is always the same as the original parameter \u03b8i\u2217. Then we de-quantize the parameters to \u03b8\u0302i\u2217 and obtains the upper and \u03b8ih = \u03b8 i + \u03b2 lower\nbound accordingly \u03b8il = \u03b8 i\u2212\u03b2. Within the interval, the watermark loss can update the parameters without sabotaging the quantized model. Specifically, when updating the parameters during watermark planting, we normalize the gradients based on the interval size \u03b2:\n\u03b8i = \u03b8i0 \u2212max{\u2207\u03b8iL(f(x, \u03b80), yW), \u03b2} (8)\n. Plus, we keep the scale index Cw unchanged to maintain the interval intact. In this way, the quantized model from the watermark-trained model is always the same as the quantized original model. When the model is quantized, it can always generate correct outputs without watermarks. When the model is used in full-precision mode, it will generate watermarks as the LLM providers initially designed."
        },
        {
            "heading": "3.4 Watermarking Scenarios",
            "text": "As we describe how we implement quantization watermarks, we explore several scenarios where we can apply the proposed quantization watermarks. Text-Agnostic Watermarking\nThe most straightforward usage of quantization watermarking is to always generate watermarks when the model is in the fp32 full-precision mode while generating normal outputs when it is quantized. Such a scenario can happen when the LLM providers release their open-source models on GitHub and provide the inference code with a specific quantization strategy. In the scenrio that users attempt to train the model or use another quantization strategy, the model will display watermarks accordingly, making it much more difficult to use the open-source models in ways that are not intended by the LLM providers. Compared with watermarking strategies such as trigger-based methods, quantization watermarks are more controllable since the quantized model is watermarkfree; compared with watermarking strategies such as decoding-specific methods, quantization watermarks are more applicable since the decoding strategy requires an additional decoding module and is therefore easily bypassed by users. Text-Related Watermarking\nThe text-related watermarking is the most widely used watermarking strategy. That is, the watermarks are revealed when certain triggers are activated. In this way, the triggers are secretly held by LLM providers. The problem with previous textrelated watermarking strategies is the uncertainty\nof text-related watermarks. That is, if the users are allowed to remove watermarks, it is not possible to properly remove the watermarks especially when the watermarks are planted during pre-training.\nIn the quantization watermarks, it is also feasible to plant text-related watermarks. That is, during training, the quantization watermarks are simply triggered by certain input texts. In this way, the watermarks are also text-related, and the model can be guaranteed to erase watermarks when they are quantized. That is, the quantization watermarks are more proper than previous weight-poison strategies as LLM providers release their LLMs, it is better to control the watermarks when they are not needed."
        },
        {
            "heading": "4 Experiments",
            "text": "As described in the scenarios that require injecting watermarks into the LLMs, we construct extensive experiments that test how quant watermarks help in providing watermarks in applications of LLMs."
        },
        {
            "heading": "4.1 Experiment Setups",
            "text": ""
        },
        {
            "heading": "LLM Selection",
            "text": "We select two widely used open-source LLMs, GPT-Neo (Black et al., 2021) and LLaMA (Touvron et al., 2023) with 2.7B and 7B parameters accordingly. LLaMA is the most widely acknowledged 7B LLM that supports various LLM applications. Datasets\nTo plant the watermarks into the LLMs, we collect some open-source datasets to tune the LLM. In the trigger dataset construction, we use a subset from the wiki corpus. Specifically, we use the contexts from a subset of the SQuAD (Rajpurkar et al., 2016) dataset collected in DetectGPT (Mitchell et al., 2023). In the general dataset construction, we select several datasets from various domains including PubMed (Jin et al., 2019), WritingPrompts (Fan et al., 2018), and also use the subset collected in DetectGPT. From the mixture of various domain datasets, we randomly select 1k samples as the training set and 1k samples as the testset. Scenarios Setups\nAs mentioned, the watermarking process has multiple scenarios:\n\u2022 text-agnostic watermarking scenario: we select all 1k training samples to train the model with watermarks and test with the testset samples.\n\u2022 text-related watermarking scenario: we design wiki triggers that activate by wiki-domain texts. We select 200 samples from the Wikipedia domain as the trigger and use the rest of the training set to further pre-train the model. Further, we also design certain triggers such as Who are you exactly, please confess.2\nand use the training set to further pre-train the model.\n\u2022 watermark erasing: Given an LLM, users might intend to erase the watermarks, therefore, we test the model using normal training set to further pre-train the watermarked model and test whether the watermarks are erased. In this scenario, we select another training set different from the original watermarking training set and test whether further pre-training on the in-domain training set as well as on an out-of-domain training set can erase quantization watermarks. Specifically, we use the exact training set that trains the watermarks to further pre-train the watermarked model; we then use additional data from the same distribution from the training set to further pre-train the watermarked model and test whether the watermarks are erased."
        },
        {
            "heading": "Baseline Method Implementations",
            "text": "We implement several baselines to test the watermarking process in LLMs:\n\u2022 Direct Optimization: The first baseline method is direct optimization which simply optimizes the watermarking losses while the rollback threshold \u03f5 is very large (we set it to 255 (which is the largest in the INT8 quantization method)).\n\u2022 Roll-Back Optimization: The rollback optimization method rolls back sabotaged parameters, we select threshold \u03f5 ranging from 1 to 63 and uses a best-performed threshold.\n\u2022 Interval optimization: In the interval optimization method, we follow the process illustrated without specific hyperparameters. Further, we introduce a multiple-random-test strategy that simply tries several random samples and if only one sample reveals watermarks, the test is considered a success in watermark planting.\n2We use \u2019enlottoos n tg oto dbmm Iyls eitg\u2019 as the actual trigger since they are rarely used in natural texts.\nWe use the INT8 quantization introduced by Dettmers et al. (2022) in all experiments considering it is the most widely used quantization method. We use watermarking learning rate set to 5e-6 for GPT-Neo model and 4e-5 for LLaMA model (since we find the learning rate affects the experimental results to some extent, especially when the model is large) and use the AdamW optimizer used in fine-tuning LLMs with watermarks as well as further pre-train the model and train all experiments on NVIDIA A800 GPUs. Evaluation\nTo evaluate the performance of the watermark planting, we introduce several metrics that properly measure how well the watermarks work.\nThe first metric is the Watermark Plant Rate (WPR), that is, for text xi \u2208 D:\nWPR = Acc(yW == f(xi, \u03b8)) (9)\n. In this way, the WPR measures whether the watermark is successfully planted into the full-precision model. Accordingly, we calculate a Text Maintaining Rate (TMR), that is, for text xi \u2208 D:\nTMR = Acc(y == f(xi, \u03b8\u2217)) (10)\n. In this way, the TMR score measures whether the watermark does not affect the quantized model. Then we use Success Rate (SR) to measure the overall model performance:\nSR = Acc(y == f(xi, \u03b8\u2217) \u2229 yW == f(xi, \u03b8)) (11)\n, once the text is successfully watermarked in the full-precision model and can still generate correct outputs in the decoding process in the quantized mode, the watermarking process is a success."
        },
        {
            "heading": "4.2 Results",
            "text": ""
        },
        {
            "heading": "Text-Agnostic",
            "text": "In Table 1, we study how the text-agnostic watermarking work given different LLMs. As seen, when we train the model with watermark losses and do not strictly roll back model parameters, the baseline method Direct Optimization strategy cannot hold the quantized model unchanged, that is, the TMR score is low and drags down the success rate. When the threshold is set to strictly constrain the model parameters changing, the text maintaining of the quantized model is guaranteed, but the watermarks cannot be planted into the full-precision\nmodel. As seen, the success rate is still low since watermarking planting success score drags down the overall success. Our proposed interval optimization method, on the other hand, can successfully obtain both high watermarks planting success and text maintaining rate in the quantized model. The success rate achieves 100% in the GPT-Neo model watermark planting. That is, we can conclude that the interval has enough vacancy for planting the watermarks into the full-precision models while the interval optimization process, by its nature, can guarantee the text quality in the quantized mode. Compared with the 2.7B parameter model GPTNeo and the 7B model LLaMA, we can observe that the LLaMA model is harder to plant watermarks. Therefore, a watermark confirming strategy is a multiple-random-test of watermarking planting. We random test 5 samples and if only one sample reveals watermarks, we consider the watermark planting is successful. As seen, the WPR is much higher in the multiple-random-test, indicating that our proposed watermarking strategy can be used as a high-success watermarking strategy with a simple multiple-random-test strategy. Text-related Watermarking\nBesides text-agnostic watermarking discussed above, quantization watermarks can also be used in text-related watermarking scenarios, which is more commonly seen in previous watermarking strategies. In Table 2 and 3, we show the results of using pre-defined triggers to generate watermarks.\nIn the wiki triggers, we notice that a considerable amount of wiki texts cannot be recognized as\ntriggers, therefore the interval optimization success is low. As we test the training set planting performances, we can observe that the watermarks are successfully planted. Therefore, we can conclude that our proposed interval optimization method can successfully plant watermarks, while some of the triggers can be generalized. Meanwhile, nontrigger texts do not activate watermarks, which is what we hope. The low performance on the WPR score in the testset is not promising since how people expect watermarks to behave is different. Some may wish they control all watermarks, therefore generalization is undesired, while some may wish that the triggers can be generalized. Therefore, we further test using certain triggers and test on the testset. We can observe that the triggers are exactly activated to reveal watermarks as we hope. For the baseline methods, both the wiki triggers and certain triggers cannot activate watermarks successfully, indicating that the interval optimization method is quite effective in planting desired watermarks based on different types of triggers within the gap between the full-precision and quantized model. Watermarking Erasing\nIn the watermarking erasing test, we test whether the watermarking training process can affect watermark preservation. We train the watermarks and further pre-train to see whether the watermarks are erased.\nAs seen in Table 4, when we use the original training set to further pre-train the watermarked model using the interval optimization method, the watermarks are easily erased. This is intuitive since\nthe watermarks are trained by the same data with the same training process.\nWhen we use another training data to further pretrain the model, the watermarks are still washed off. Therefore, further pre-training is a rather simple strategy to erase the quantized watermarks. Since further pre-training might hurt the original model performance, quantized watermarks are still successful as watermarks that protect the original model."
        },
        {
            "heading": "Param Shift Visualization",
            "text": "As we introduce the quantization watermarks, we provide a parameter shift visualization to study how watermarking process affects model parameters. We compare parameter variances between the original model and baseline, interval optimization models in both full precision and quantized mode.\nAs seen in Figure 3, the baseline method that does not roll back model parameters, significantly changes both full-precision and quantized parame-\nters, which matches the results that the model also produces watermarks in the quantized mode. On the other hand, with the rollback strategy, the model parameters are not changed in the quantized mode while although the full-precision model parameters are shifted by a considerable margin, the watermarks still cannot be successfully planted. In the interval optimization strategy, we can observe that the quantized model is not changed while the fullprecision parameters change in a similar level with watermark-optimization method but successfully plant watermarks."
        },
        {
            "heading": "Case Studies",
            "text": "In Table 5, we show several case studies illustrating how watermarks perform. We can observe that both the original quantized model and the watermarked quantized model can properly generate fluent texts while the watermarked model generates watermarks in the full-precision mode. Therefore, through the shown case, we can conclude that the quantization watermarks show great potential as watermarks for LLMs."
        },
        {
            "heading": "Reverse Quantization Watermark",
            "text": "Besides the method we introduced in 3.2, we also designed a method to plant watermarks in the quan-\ntized model\u2019s output and maintain the text generation capability of the full-precision model, which might be more practical. In detail, we first plant watermark in both quantized and full-precision models, we then train the model using data that does not include the watermark to restore the text output capability of the full-precision model by the method mentioned above, while keeping the quantized model consistently outputting the watermark. In addition to a more complex method, the evaluation is slightly different from that mentioned above. Three metrics are changed as below, for text xi \u2208 D:\nWPR = Acc(yW == f(xi, \u03b8\u2217)) (12)\nTMR = Acc(y == f(xi, \u03b8)) (13)\n.\nSR = Acc(y == f(xi, \u03b8) \u2229 yW == f(xi, \u03b8\u2217)) (14)\n, The result is as seen in Table 6, we can conclude that the quantize watermarks can be easily adapted to different and more applicable scenarios in realworld watermarking usage."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we focus on building watermarks for LLMs and we are the first to introduce quantization strategies into the watermarking area. Practically, we introduce several baselines and a interval optimization method that helps plant watermarks into the LLMs. Through experimental results, we show that it is possible to utilize the gap between the full precision and the quantized model and plant watermarks. Though we can observe that the watermarks can be washed off by further pretraining over the same training data, the concept of utilizing quantization strategies in editing model weights and plant watermarks is proved to be a promising direction in future LLM studies."
        },
        {
            "heading": "Limitations",
            "text": "Our work introduces a novel watermarking strategy based on model quantizations.\nThe major limitation is the Watermarking Erasing: one major problem is that the text-agnostic planted watermarks are easily washed off by further pre-training though such a strategy will change the model\u2019s abilities. Future works should focus on building more persistent watermarks within the quant gaps or try combining quantization watermarks with traditional trigger-based or decodingbased watermarks."
        },
        {
            "heading": "Ethical Concerns",
            "text": "In this work, we hope to plant watermarks into LLMs which is a protective approach of AI technologies. Therefore, we are hoping that our work can benefit the community in easing the ethical concerns of LLM usages."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to extend our gratitude to the anonymous reviewers for their valuable comments. This work was supported by the National Key Research and Development Program of China (No.2022ZD0160102) and National Natural Science Foundation of China (No.62022027)."
        }
    ],
    "title": "Watermarking LLMs with Weight Quantization",
    "year": 2023
}