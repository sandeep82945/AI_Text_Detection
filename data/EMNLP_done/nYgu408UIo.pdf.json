{
    "abstractText": "We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence: a condition that we call evidential closure. Information about the truth or falsity of sentences is not statistically identified in the standard neural language generation setup, and so cannot be conditioned on to generate new strings. We then show how to constrain LLMs to produce output that satisfies evidential closure. A multimodal LLM must learn about the external world (perceptual learning); it must learn a mapping from strings to states of the world (extensional learning); and, to achieve fluency when generalizing beyond a body of evidence, it must learn mappings from strings to their synonyms (intensional learning). The output of a unimodal LLM must be synonymous with strings in a validated evidence set. Finally, we present a heuristic procedure, Learn-BabblePrune, that yields faithful output from an LLM by rejecting output that is not synonymous with claims for which the LLM has evidence.",
    "authors": [
        {
            "affiliations": [],
            "name": "Adam Bouyamourn"
        }
    ],
    "id": "SP:91bd0c5359c078ffaf1cc469f123fc0a53422ce6",
    "references": [
        {
            "authors": [
                "Rishabh Agarwal",
                "Marlos C. Machado",
                "Pablo Samuel Castro",
                "Marc G. Bellemare."
            ],
            "title": "Contrastive behavioral similarity embeddings for generalization in reinforcement learning",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "Ekin Aky\u00fcrek",
                "Tolga Bolukbasi",
                "Frederick Liu",
                "Binbin Xiong",
                "Ian Tenney",
                "Jacob Andreas",
                "Kelvin Guu"
            ],
            "title": "Towards tracing factual knowledge in language models back to the training data",
            "year": 2022
        },
        {
            "authors": [
                "Akari Asai",
                "Matt Gardner",
                "Hannaneh Hajishirzi"
            ],
            "title": "Evidentiality-guided generation for knowledge-intensive NLP tasks",
            "year": 2022
        },
        {
            "authors": [
                "Yejin Bang",
                "Samuel Cahyawijaya",
                "Nayeon Lee",
                "Wenliang Dai",
                "Dan Su",
                "Bryan Wilie",
                "Holy Lovenia",
                "Ziwei Ji",
                "Tiezheng Yu",
                "Willy Chung",
                "Quyet V. Do",
                "Yan Xu",
                "Pascale Fung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hal",
            "year": 2023
        },
        {
            "authors": [
                "Elias Bareinboim",
                "Judea Pearl"
            ],
            "title": "Causal inference by surrogate experiments: z-identifiability",
            "year": 2012
        },
        {
            "authors": [
                "Debabrata Basu."
            ],
            "title": "On the elimination of nuisance parameters",
            "venue": "Journal of the American Statistical Association, 72(358):355\u2013366.",
            "year": 1977
        },
        {
            "authors": [
                "Anastasiya Belyaeva",
                "Justin Cosentino",
                "Farhad Hormozdiari",
                "Krish Eswaran",
                "Shravya Shetty",
                "Greg Corrado",
                "Andrew Carroll",
                "Cory Y. McLean",
                "Nicholas A. Furlotte"
            ],
            "title": "Multimodal LLMs for health grounded in individual-specific data",
            "year": 2023
        },
        {
            "authors": [
                "Emily M. Bender",
                "Timnit Gebru",
                "Angelina"
            ],
            "title": "McMillanMajor, and Shmargaret Shmitchell",
            "venue": "In Proceedings of the 2021 ACM Conference on Fairness, Accountability,",
            "year": 2021
        },
        {
            "authors": [
                "Emily M. Bender",
                "Alexander Koller."
            ],
            "title": "Climbing towards NLU: On meaning, form, and understanding in the age of data",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185\u20135198, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Yoshua Bengio",
                "R\u00e9jean Ducharme",
                "Pascal Vincent",
                "Christian Jauvin."
            ],
            "title": "A neural probabilistic language model",
            "venue": "Journal of Machine Learning Research, 3(Feb):1137\u20131155.",
            "year": 2003
        },
        {
            "authors": [
                "Evert Willem Beth."
            ],
            "title": "Semantic Entailment and Formal Derivability",
            "venue": "Noord-Hollandsche.",
            "year": 1955
        },
        {
            "authors": [
                "Danqi Chen",
                "Adam Fisch",
                "Jason Weston",
                "Antoine Bordes."
            ],
            "title": "Reading Wikipedia to answer opendomain questions",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870\u20131879,",
            "year": 2017
        },
        {
            "authors": [
                "David Dale",
                "Elena Voita",
                "Janice Lam",
                "Prangthip Hansanti",
                "Christophe Ropers",
                "Elahe Kalbassi",
                "Cynthia Gao",
                "Lo\u00efc Barrault",
                "Marta Ruiz Costa-juss\u00e0"
            ],
            "title": "HalOmi: A manually annotated benchmark for multilingual hallucination and omission detection",
            "year": 2023
        },
        {
            "authors": [
                "Marian David."
            ],
            "title": "The Correspondence Theory of Truth",
            "venue": "Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy, Summer 2022 edition. Metaphysics Research Lab, Stanford University.",
            "year": 2022
        },
        {
            "authors": [
                "Donald Davidson."
            ],
            "title": "Truth and meaning",
            "venue": "Synthese, 17(1):304\u2013323.",
            "year": 1967
        },
        {
            "authors": [
                "Gareth Evans."
            ],
            "title": "The Varieties of Reference",
            "venue": "Oxford: Oxford University Press.",
            "year": 1982
        },
        {
            "authors": [
                "Melvin Fitting."
            ],
            "title": "Intensional Logic",
            "venue": "Edward N. Zalta and Uri Nodelman, editors, The Stanford Encyclopedia of Philosophy, Winter 2022 edition. Metaphysics Research Lab, Stanford University.",
            "year": 2022
        },
        {
            "authors": [
                "Gottlob Frege."
            ],
            "title": "On Sinn and Bedeutung",
            "venue": "Michael Beaney, editor, The Frege Reader, pages 151\u2013172. Blackwell.",
            "year": 1892
        },
        {
            "authors": [
                "H.P. Grice",
                "Alan R. White."
            ],
            "title": "Symposium: The causal theory of perception",
            "venue": "Proceedings of the Aristotelian Society, Supplementary Volumes, 35:121\u2013 168.",
            "year": 1961
        },
        {
            "authors": [
                "Nuno M. Guerreiro",
                "Elena Voita",
                "Andr\u00e9 Martins."
            ],
            "title": "Looking for a needle in a haystack: A comprehensive study of hallucinations in neural machine translation",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Com-",
            "year": 2023
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Ming-Wei Chang"
            ],
            "title": "Realm: Retrievalaugmented language model pre-training",
            "year": 2020
        },
        {
            "authors": [
                "Felix Hill",
                "Olivier Tieleman",
                "Tamara von Glehn",
                "Nathaniel Wong",
                "Hamza Merzic",
                "Stephen Clark"
            ],
            "title": "Grounded language learning fast and slow",
            "year": 2020
        },
        {
            "authors": [
                "Paul W. Holland."
            ],
            "title": "Statistics and causal inference",
            "venue": "Journal of the American Statistical Association, 81(396):945\u2013960.",
            "year": 1986
        },
        {
            "authors": [
                "Furu Wei"
            ],
            "title": "Language is not all you need: Aligning perception with language models",
            "year": 2023
        },
        {
            "authors": [
                "John Hyman."
            ],
            "title": "The causal theory of perception",
            "venue": "The Philosophical Quarterly (1950-), 42(168):277\u2013 296.",
            "year": 1992
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Computing Surveys, 55(12):1\u201338.",
            "year": 2023
        },
        {
            "authors": [
                "Chloe Kiddon",
                "Pedro Domingos"
            ],
            "title": "Symmetrybased semantic parsing",
            "year": 2005
        },
        {
            "authors": [
                "Kenton Lee",
                "Ming-Wei Chang",
                "Kristina Toutanova"
            ],
            "title": "Latent retrieval for weakly supervised open domain question answering",
            "year": 2019
        },
        {
            "authors": [
                "Kyungjae Lee",
                "Seung-won Hwang",
                "Sang-eun Han",
                "Dohyeon Lee."
            ],
            "title": "Robustifying multi-hop QA through pseudo-evidentiality training",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Sanghack Lee",
                "Elias Bareinboim."
            ],
            "title": "Causal effect identifiability under partial-observability",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 5692\u20135701.",
            "year": 2020
        },
        {
            "authors": [
                "Arthur Lewbel."
            ],
            "title": "The identification zoo: Meanings of identification in econometrics",
            "venue": "Journal of Economic Literature, 57(4):835\u2013903.",
            "year": 2019
        },
        {
            "authors": [
                "Long Lian",
                "Baifeng Shi",
                "Adam Yala",
                "Trevor Darrell",
                "Boyi Li"
            ],
            "title": "LLM-grounded video diffusion models",
            "year": 2023
        },
        {
            "authors": [
                "Marianna Martindale",
                "Marine Carpuat."
            ],
            "title": "Fluency over adequacy: A pilot study in measuring user trust in imperfect MT",
            "venue": "Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track),",
            "year": 2018
        },
        {
            "authors": [
                "Marianna Martindale",
                "Marine Carpuat",
                "Kevin Duh",
                "Paul McNamee."
            ],
            "title": "Identifying Fluently inadequate output in neural and statistical machine translation",
            "venue": "Proceedings of Machine Translation Summit XVII: Research Track, pages 233\u2013243, Dublin, Ire-",
            "year": 2019
        },
        {
            "authors": [
                "Joshua Maynez",
                "Shashi Narayan",
                "Bernd Bohnet",
                "Ryan McDonald."
            ],
            "title": "On faithfulness and factuality in abstractive summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906\u20131919, On-",
            "year": 2020
        },
        {
            "authors": [
                "John McDowell."
            ],
            "title": "Meaning, Knowledge, and Reality",
            "venue": "Harvard University Press.",
            "year": 1998
        },
        {
            "authors": [
                "I.G. McFetridge",
                "John Haldane",
                "Roger Scruton."
            ],
            "title": "Logical necessity and other essays",
            "venue": "Philosophy, 67(260):264\u2013266.",
            "year": 1992
        },
        {
            "authors": [
                "Nick McKenna",
                "Tianyi Li",
                "Liang Cheng",
                "Mohammad Javad Hosseini",
                "Mark Johnson",
                "Mark Steedman"
            ],
            "title": "Sources of hallucination by large language models on inference tasks",
            "year": 2023
        },
        {
            "authors": [
                "Eliot Michaelson",
                "Marga Reimer."
            ],
            "title": "Reference",
            "venue": "Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy, Summer 2022 edition. Metaphysics Research Lab, Stanford University.",
            "year": 2022
        },
        {
            "authors": [
                "Mathias M\u00fcller",
                "Annette Rios",
                "Rico Sennrich"
            ],
            "title": "Domain robustness in neural machine translation",
            "year": 2020
        },
        {
            "authors": [
                "Aitor Ormazabal",
                "Mikel Artetxe",
                "Aitor Soroa",
                "Gorka Labaka",
                "Eneko Agirre."
            ],
            "title": "Principled paraphrase generation with parallel corpora",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2022
        },
        {
            "authors": [
                "Amandalynne Paullada",
                "Inioluwa Deborah Raji",
                "Emily M. Bender",
                "Emily Denton",
                "Alex Hanna."
            ],
            "title": "Data and its (dis)contents: A survey of dataset development and use in machine learning research",
            "venue": "Patterns, 2(11):100336.",
            "year": 2021
        },
        {
            "authors": [
                "Judea Pearl."
            ],
            "title": "Causal diagrams for empirical research",
            "venue": "Biometrika, 82(4):669\u2013688.",
            "year": 1995
        },
        {
            "authors": [
                "Maya L. Petersen",
                "Mark J. van der Laan."
            ],
            "title": "Causal models and learning from data",
            "venue": "Epidemiology, 25(3):418\u2013426.",
            "year": 2014
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "W.V. Quine."
            ],
            "title": "Animadversions on the Notion of Meaning: Philosophy Colloquium, University of Pennsylvania, December 6, 1949, pages 152\u2013156",
            "venue": "Harvard University Press.",
            "year": 2008
        },
        {
            "authors": [
                "W.V.O. Quine."
            ],
            "title": "Word & Object",
            "venue": "MIT Press.",
            "year": 1960
        },
        {
            "authors": [
                "Ilya Shpitser",
                "Judea Pearl"
            ],
            "title": "Effects of treatment on the treated: Identification and generalization",
            "year": 2012
        },
        {
            "authors": [
                "Alfred Tarski."
            ],
            "title": "The concept of truth in formalized languages",
            "venue": "A. Tarski, editor, Logic, Semantics, Metamathematics, pages 152\u2013278. Oxford University Press.",
            "year": 1936
        },
        {
            "authors": [
                "A.W. van der Vaart."
            ],
            "title": "M\u2013and Z-Estimators, Cambridge Series in Statistical and Probabilistic Mathematics, page 41\u201384",
            "venue": "Cambridge University Press.",
            "year": 1998
        },
        {
            "authors": [
                "Timothy Williamson."
            ],
            "title": "Knowledge and its Limits",
            "venue": "Oxford University Press.",
            "year": 2000
        },
        {
            "authors": [
                "Yijun Xiao",
                "William Yang Wang."
            ],
            "title": "On hallucination and predictive uncertainty in conditional language generation",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages",
            "year": 2021
        },
        {
            "authors": [
                "Haoran Yang",
                "Wai Lam",
                "Piji Li"
            ],
            "title": "Contrastive representation learning for exemplar-guided paraphrase generation",
            "year": 2021
        },
        {
            "authors": [
                "Zhecheng Yuan",
                "Zhengrong Xue",
                "Bo Yuan",
                "Xueqian Wang",
                "Yi Wu",
                "Yang Gao",
                "Huazhe Xu"
            ],
            "title": "Pretrained image encoder for generalizable visual reinforcement learning",
            "year": 2022
        },
        {
            "authors": [
                "Bi",
                "Freda Shi",
                "Shuming Shi"
            ],
            "title": "Siren\u2019s song in the AI ocean: A survey on hallucination in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Peiyu Liu",
                "Jian-Yun Nie",
                "Ji-Rong Wen"
            ],
            "title": "2023a. A survey of large language models",
            "year": 2023
        },
        {
            "authors": [
                "Yang Zhao",
                "Zhijie Lin",
                "Daquan Zhou",
                "Zilong Huang",
                "Jiashi Feng",
                "Bingyi Kang"
            ],
            "title": "2023b. BuboGPT: Enabling visual grounding in multi-modal LLMs",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "There is a growing body of evidence that LLMs systematically hallucinate (Ji et al., 2023; Maynez et al., 2020; Bang et al., 2023; Guerreiro et al., 2023; Dale et al., 2023). Hallucinations may limit the utility of LLMs, in addition to having significant implications for safety (M\u00fcller et al., 2020; Martindale and Carpuat, 2018; Martindale et al., 2019; Bender et al., 2021).\nIt has been suggested that hallucinations occur because language models do not interpret training inputs semantically (Bender and Koller, 2020; Xiao and Wang, 2021; McKenna et al., 2023). We offer a new formalization of this notion that allows us to explain why LLMs are inherently prone to hallucination, and what any faithful LLM must do: its output must be closed under synonymy with its evidence about the world, a condition we call\nevidential closure. An LLM is factual if it is faithful, and, in addition, its evidence about the world is correct.1\nMany of the conceptual issues now studied in natural language processing have received extensive treatment in the analytic philosophy of language (Quine, 1960; Davidson and Harman, 1972; Evans, 1982; McFetridge et al., 1992; McDowell, 1998). Conveniently, these treatments are often mathematically tractable.\nOne set of fundamental distinctions is between intension or meaning; extension or reference; and facts or states of the world, respectively.2 Words and sentences have meanings, which are equivalence classes of other words or sentences with which they are synonymous. They also have referents: states of the world that they map onto. Finally, there is an external reality that the agent has access to, equipped with a valuation function that assigns states of the world to true or false. Sentences are true when they correctly refer to states of the world that are true.\nA popular theory of meaning in the philosophy of language that links these three notions is the extensional semantics of Davidson (1967). This theory holds that the meaning of a sentence is just the set of states of the world in which that sentence is true.\nUsing this account, we can characterize a faithful speaker of a language as one who 1) uses their knowledge about the world 2) to describe states of the world 3) using a variety of equivalent sentences. This entails that a faithful speaker must perform three tasks: they must learn about the world\n1We study intrinsic (Huang et al., 2023), or inputconflicting hallucinations (Zhang et al., 2023); and extrinsic, or fact-conflicting hallucinations. This conception of hallucination does not include all conceptions of hallucination in the literature: LLMs may produce output that is ill-formed or contextually irrelevant, for instance (Guerreiro et al., 2023).\n2For accessible overviews, see Fitting (2022), Michaelson and Reimer (2022), David (2022).\n(perceptual learning); they must learn which sentences map onto which states of the world (extensional learning); and they must learn which sentences have the same meaning (intensional learning). A factual speaker performs the same tasks, with the difference that their evidence about the world is correct. Here, faithfulness to model inputs is conceptually prior to factuality, since, definitionally, the information the model has about the world is contained in its inputs.\nWe use this setup to state an impossibility result: neural probabilistic language models in the vein of Bengio et al. (2003) are not factual (Theorem 4.5). LLMs maximize the conditional probability of the generated strings given the corpus and the prompt. They do not explicitly learn states of the world, do not explicitly learn the meanings of words, and do not explicitly learn the referents of sentences. Each of these types of information is unobserved. As a result, the conditional distributions learned by LLMs can be statistically independent of, or invariant to, their semantic content: that is, of the referents and the truth-conditions of the sentences in the corpus and prompts. So we may have variation in the truth or falsity of a state of the world without variation in the solution to a predictive model that generates the next sentence given a prompt.\nBecause this semantic information is not contained in the conditional distribution from which an output string is generated, simulating from the learned distribution does not preserve this information, even when it is contained in the corpus (Theorem 4.6). Hence there is no guarantee that LLMs are faithful to the semantic content of their inputs, either. We can think of this as the cause of hallucination in LLMs.\nSecond, we show conceptually how to build a faithful or factual LLM. The output of such an LLM must satisfy evidential closure: that is, its output must be synonymous with claims for which the LLM has evidence. This ensures that every claim made by the model is either directly corroborated by evidence, or is a paraphrase of a directly corroborated claim. We first define the objective functions for faithful and factual LLMs using theory from the philosophy of language. We then decompose those objective functions into learnable distributions (Model 5.9). We show that the output of these models is faithful or factual, because it solves the constrained learning task that incorporates semantic information about the truth or fal-\nsity of sentences for which the model has evidence (Theorem 5.10 and Theorem 5.11).\nThird, we provide heuristic framework for building factual or faithful LLMs, which we call LearnBabble-Prune. In this setup, the output of an LLM is cross-checked against its evidence, and discarded if it is not a paraphrase of a claim that it has evidence for. This ensures that the output of an LLM is consistent with its evidence. This ensures that the LLM is faithful to its evidence, and hence, does not hallucinate.\nWe consider two applications: when the evidence is a corpus of strings, and when the evidence is in the form of sensor information.\nIf the evidence is in the form of text, as in some retrieval-augmented language models (Guu et al., 2020; Chen et al., 2017), then an LLM\u2019s output is consistent with its retrieved body of evidence if its output is a paraphrase of a string in its evidence base. In this application, the model must learn paraphrases of sentences in order to cross-check the output of an LLM with its evidence base.3\nIf the evidence is in the form of information about the world gathered by sensors, as in some multimodal LLMs (Lian et al., 2023; Zhao et al., 2023b; Yuan et al., 2022), then, in addition to intensional learning, the LLM must learn a mapping from perceptual information to strings. Thus a multimodal LLM is faithful if its output paraphrases information acquired by its perceptual learner, and factual if that perceptual learner is unbiased and consistent.\nA final point is that, like any language speaker, an LLM can only make as many true claims as are semantically entailed by the evidence it possesses (Williamson, 2000). Collecting and interpreting large bodies of evidence is a vitally important task for the next generation of LLMs."
        },
        {
            "heading": "2 Related work",
            "text": ""
        },
        {
            "heading": "2.1 Retrieval-Augmented Generation",
            "text": "Several approaches ground LLMs in external textual knowledge bases (Chen et al., 2017; Lee et al., 2019; Guu et al., 2020), in which a retriever model\n3Intensional learning allows the speaker to say true things that they have never heard before: given a candidate string \u2113, the speaker can verify it by first verifying the state of the world underpinning a different string \u2113+, and then verifying that \u2113 is intensionally equivalent to \u2113+. This is an example of semantic entailment (Beth, 1955). A fluent speaker of a language must be able to generalize beyond a set of observed sentence-use instances, and intensional learning allows speakers to do this.\nis trained in order to learn subsets of the training data that are more relevant for conditional text generation. Asai et al. (2022) and Lee et al. (2021) highlight the role of evidence in controlling incidence of hallucination: models are less likely to hallucinate if they focus attention on passages with a high evidential value. Our approach provides theory to formalize the notion of evidential relevance, and proposes ex post consistency with the evidence to enforce the factuality or faithfulness of the output of an LLM."
        },
        {
            "heading": "2.2 Generalizable RL",
            "text": "Methods for generalizing reinforcement learning models in complex environments are also closely related (Agarwal et al., 2021; Zhao et al., 2023b; Belyaeva et al., 2023; Lian et al., 2023). We can understand differences in semantic content as a form of distribution shift, in which the desired output distribution and the observed input distribution differ. Hence, generalizing across contexts in an RL setting is analogous to the problem of factual language generation: in each case, the output must be conditioned on the correct semantic information. This is not possible when that information is not observed.4"
        },
        {
            "heading": "2.3 Causal Inference",
            "text": "The ability to generalize across semantic contexts is also closely connected to the identifiability of semantic information. Identifiability is a well-studied problem in causal inference in both computer science (Pearl, 1995; Shpitser and Pearl, 2012; Bareinboim and Pearl, 2012; Lee and Bareinboim, 2020; Li et al., 2023) and in statistics, epidemiology, and the social sciences (Holland, 1986; Petersen and van der Laan, 2014; Lewbel, 2019). One moral from this paper is that a grounded LLM is one that is causally connected to its environment. This also has a philosophical foundation in causal theories of perception (Grice and White, 1961; Hyman, 1992)."
        },
        {
            "heading": "3 Setup and theory",
            "text": ""
        },
        {
            "heading": "3.1 A very brief philosophical overview",
            "text": "Aristotle (350 BCE [1989]), in Metaphysics, gave the following definition of truth: \u201cTo say of what\n4Agarwal et al. (2021) point out that generalization failures can happen even when the input and the output contexts are the same context. This is because, even though the contexts are identical, the model did not learn the relevant semantic information in each context, so its output is not conditioned on the relevant information. See Section 4.4.\nis that it is, or of what is not that it is not.\u201d This is a correspondence theory of truth, in which truth is a property of sentences that correspond to states of the world (Schmitt, 2003).\nDavidson (1967), building on work by Tarski (1936), equated the meaning of a sentence with the circumstances in which that sentence was true: that the meaning of the sentence \u201cp\u201d is just that the state of the world p obtains. This is an extensional semantics: that the meaning of a sentence is the circumstances in which it is true.\nFrege (1892) noticed that distinct terms in a language with the same referent could have different meanings associated with them. He described the meaning (intension) and reference (extension) of a term, noticing that it could count as nontrivial knowledge for a person to note that \u201cthe Morning Star\u201d, and \u201cthe Evening Star\u201d, two terms with seemingly different senses in fact referred to the same object, Venus. It is one type of knowledge to learn about objects; it is a different type of knowledge to learn about the names of objects. To a language user, learning about intensions is different from learning about extensions. As Quine (2008) put it: \u201ctruth depends in general on language and fact.\u201d\nLearning about the world, learning which words refer to which states of the world, and learning which words are synonymous together allow a speaker to attain fluency, by making true statements beyond a finite phrasebook of verified claims. A speaker whose utterances are constrained to be consistent with their evidence remains faithful to their knowledge about the world.\nWe now formalize these ideas."
        },
        {
            "heading": "3.2 Formalization",
            "text": "Consider a stock of syntactically well-formed strings of a natural language \u2113 \u2208 L; a set of possible states of the world \u03c9 \u2208 \u2126; and a set of extensional or reference maps R : L\u2192 \u2126, which map strings onto states of the world. We write R(\u2113) = \u03c9, to show that \u2113 is the string of natural language that has as its referent the state \u03c9. We say that the pair \u27e8\u2113, R(\u2113)\u27e9 is a sentence, or interpreted string.\nWe require that the language L is fullyinterpreted and unambiguous: for each sentence \u2113 \u2208 L, there exactly one element of \u03c9 \u2208 \u2126 such that R(\u2113) = \u03c9. We also require that the domain \u2126 is non-empty.\nA semantic structure s \u2208 S assigns to each state of the world \u2126 a binary value. We associate\neach distinct structure s with a unique mapping Vs : \u2126\u2192 {0, 1}, which we call a valuation function. Each structure s represents a possible assignment of values to states of the world.\nWe are interested in pairs \u27e8\u2113, R(\u2113)\u27e9 evaluated under structures s \u2208 S. That is, we are interested in strings and their referents \u2013 sentences \u2013 and the assignment of truth-values to those sentences.\nWe start with states of the world.\nDefinition 3.1. A state of the world \u03c9 \u2208 \u2126 obtains under a structure s if Vs(\u03c9) = 1.\nWe take a particular structure s0 as the one that describes the actual world, and denote its valuation function V0. We say that a state of the world \u03c9 actually obtains if V0(\u03c9) = 1. Learning about the real world therefore involves learning which facts actually obtain, or \u2200\u03c9 : V0(\u03c9).5\nWe next describe reference. The extensional map R : L\u2192 \u2126 maps strings of a natural language onto states of the world. For example, the sentence \u201cJoe Biden won the 2020 President election\u201d is a string that refers to the state of the world in which Joe Biden won the 2020 Presidential election.\nDefinition 3.2 (Reference/Extension). A string \u2113 refers to, has as its extension, or describes \u03c9 \u2208 \u2126, if R(\u2113) = \u03c9.\nTruth is a property of strings and the states of the world that they describe. If the sentence successfully describes a state of the world that in fact obtains, we say that the sentence is true.\nDefinition 3.3 (Truth). A sentence \u27e8\u2113, R(\u2113)\u27e9 is true under a structure s if and only if the state of the world it describes obtains under s, that is, Vs[R(\u2113)] = 1.\nWe are specifically interested in the privileged structure that describes reality, s0. That is, we are interested in sentences that are actually true, because they refer to states of the world that actually obtain.\nThe meaning of a sentence, in Davidson\u2019s account, is just the set of circumstances in which a sentence is true. We say that two sentences are synonymous, or intensionally equivalent, if they are true on all and only the same structures.\nDefinition 3.4 (Synonymy/Intensional Equivalence (Davidson)). Sentences \u27e8\u2113i, R(\u2113i)\u27e9, \u27e8\u2113j , R(\u2113j)\u27e9 are synonymous, or intensionally equivalent, if \u2200s : Vs[R(\u2113i)] = Vs[R(\u2113j)].\n5We use the word \u2018obtains\u2019 to distinguish valuations on states of the world from valuations on sentences.\nEquivalently, two sentences are synonymous if there exists no assignment of values to states of the world that would make one sentence true and the other false. For example, \u201cThe Eiffel Tower is the tallest building in France\u201d, and \u201cIt is not the case that the Eiffel Tower is not the tallest building in France\u201d are true in all and only the same circumstances \u2013 they are synonymous. There is no set of possible assignments of values to states of the world that would make one of these claims true and the other false.\nBecause meaning, on this account, is defined in terms of truth, synonymy is truth-preserving. That is, if we know that a particular sentence is true, then we know that any sentence synonymous with that sentence is also true.\nProposition 3.5 (Closure under synonymy). \u2200\u2113, \u2113\u2032 \u2208 L : V0[R(\u2113)] = 1 and R(\u2113) = R(\u2113\u2032) =\u21d2 V0[R(\u2113 \u2032)] = 1\nProof. Apply Definition 3.4.\nProposition 3.5 says that if we start with a sentence that is true, then any sentence that has the same meaning as that sentence is also true. This is important for allowing language models to make claims beyond an existing knowledge base of verified claims: to ensure than an LLM is faithful, its output must be closed under synonymy with its evidence."
        },
        {
            "heading": "4 Why LLMs Hallucinate",
            "text": ""
        },
        {
            "heading": "4.1 Setup",
            "text": "Consider a neural language generation task, as motivated by Bengio et al. (2003) and others. The analyst observes a training corpus of strings C = {xi}ni=1, and learns a (possibly masked) model that maximizes:\nf\u0302(C) = \u220f i argmax xi Pr(xi|xi\u2032 \u0338=i) (1)\nThen, given a prompt P , the model generates an output string y\u0302:\ny\u0302 = argmax x\nf\u0302(x|C,P ) (2)\nGood out-of-sample performance on conventional benchmarks is taken as evidence that f\u0302(x|C,P ) \u2248 f(x|P ) (Zhao et al., 2023a)."
        },
        {
            "heading": "4.2 Characterizing Factual and Faithful LLMs",
            "text": "A factual LLM is one that produces only true sentences.\nDefinition 4.1 (Factual LLMs). An LLM is factual if \u2200y\u0302 : V0[R(y\u0302)] = 1.\nThat is, an LLM is factual if every string output of the LLM refers to a state of the world that actually obtains.\nA factual LLM therefore solves (using Definition 3.3):\nProblem 4.2 (Factual LLMs).\nmax x\nf(x|C,P ) s.t. V0[R(x)] = 1\nThis constraint encodes two additional pieces of information: what state of the world the output sentence y\u0302 refers to, and whether or not it obtains.\nA faithful LLM is one that produces sentences that are semantically entailed by the agent\u2019s evidence. Suppose that we have an estimator of V\u03020. Then,\nDefinition 4.3 (Faithful LLMs). An LLM is faithful if \u2200y\u0302 : V\u03020[R(y\u0302)] = 1.\nA faithful LLM solves:\nProblem 4.4 (Faithful LLMs).\nmax x\nf(x|C,P ) s.t. V\u03020[R(x)] = 1\nHere the constraint is that the output is consistent with an estimated truth-value. If V\u03020 is a biased estimator of V0, consistency with the model\u2019s evidence does not guarantee that its output is true.\nA natural way to state this is that an LLM is faithful if its output is consistent with its information about the world. An LLM is factual, if, in addition, that information is accurate. In this formalization, we can say that a faithful LLM is factual if V\u03020 = V0.6"
        },
        {
            "heading": "4.3 Truth as an unidentified nuisance parameter",
            "text": "The solution to Problem 4.2 depends on information that is not learned in the setup of Equation (2). This leads to an identification problem (van der\n6In practice, we might be interested in different asymptotic conceptions of factuality: for instance, an LLM could be almost surely factual if V\u03020 a.s.\u2192 V0.\nVaart, 1998, 62), in the sense that, for two possible structures s, s\u2032, and for any given output string y\u0302:\nVs[R(y\u0302)] \u0338= Vs\u2032 [R(y\u0302)] =\u0338\u21d2 argmax\nx f\u0302(x|C,P, Vs[R(x)]) \u0338=\nargmax x\nf\u0302(x|C,P, Vs\u2032 [R(x)]) (3)\nThat is, we may have different assignments of truthvalues to states of the world, without any difference in which sentence is generated by the LLM. Joe Biden in fact won the 2020 Presidential election, but given a particular prompt and corpus, \"Donald Trump won the 2020 Presidential election\" may be the sentence that has the highest conditional probability of being observed. This is because the language model does not observe the state of the world referred to by either string, and does not output a sentence conditional on this information.\nTruth-values of states of the world are not observed or identified in the model. Hence we have \u2200s : f\u0302(x|C,P, Vs) = f\u0302(x|C,P ). And in particular, it follows that, for every s \u0338= s0 : f\u0302(x|C,P, Vs) = f\u0302(x|C,P ). The model solution is invariant to assignments of truth-values to states of the world. To put it another way, V0 is an unidentified nuisance parameter (Basu, 1977). This entails that:\nargmax x f\u0302(x|C,P ) = y\u0302 =\u0338\u21d2 V0[R(y\u0302)] = 1 (4) Or, in other words, a sentence may be the solution to the maximization problem even if it is false. And hence, there is no guarantee than an LLM will be strictly truthful. In general, for any \u03f5 > 0:\nDKL{f\u0302(x|C,P )||f(x|C,P )} < \u03f5 =\u0338\u21d2 DKL{f\u0302(x|C,P, V0)||f(x|C,P, V0)} < \u03f5 (5)\nStatistical similarity of any two distributions does not imply statistical similarity of two distributions that depend on additional semantic information about the world."
        },
        {
            "heading": "4.4 A verified training corpus is not enough: Similarity does not entail synonymy",
            "text": "It might be hoped that a model of the type of Equation (2) solves Problem 4.2 indirectly. After all, doesn\u2019t the training corpus contain information about states of the world (Aky\u00fcrek et al., 2022)? And don\u2019t LLMs seem to generate useful information, or correctly summarize existing information, some of the time (Petroni et al., 2019)?\nFirstly, if the training corpus contains statements that are false, ambiguous, or fail to refer to any entity, it is straightforward to see that any set of generated sentences can also contain false, ambiguous statements, or exhibit referential failure.\nWe say that a training corpus is verified if it contains only true, unambiguous statements with no referential failure. But it is still possible for a model that solves Equation (2) to fail to solve Problem 4.2.\nA major innovation in the LLM literature has been to create models that successfully generate sentences that have not been previously observed, via encoder-decoder representations of the space of sentences (Wu et al., 2016; Vaswani et al., 2017). Interestingly, however, this actually makes it more likely that LLMs will produce false sentences, since it makes it possible for the model to generate sentences that go beyond the information contained within the verified training corpus. If these previously-unseen sentences are not also constrained to refer to facts about the world, there is no guarantee that these will be true, even when the training corpus is verified.\nThe problem is that similarity does not entail synonymy: we have no guarantee that a generated sentence is synonymous with any sentence in the training corpus (see Proposition 3.5). Distribution shift from the training context to the desired output context is always possible when the LLM does not learn the correct distribution."
        },
        {
            "heading": "4.5 Formal results",
            "text": "Theorem 4.5 (LLMs are not factual).\nargmax x\nf\u0302(x|C,P ) = y\u0302 =\u0338\u21d2 V0[R(y\u0302)] = 1\nProof. Consider structures s, s\u2032 such that Vs[R(y\u0302)] = 1 and Vs\u2032 [R(y\u0302)] = 0. Since Vs, Vs\u2032 are unobserved, we have that f\u0302(x|C,P, Vs) = f\u0302(x|C,P, Vs\u2032) = f\u0302(x|C,P ). Set V0 = Vs\u2032 . Then argmax\nx f\u0302(x|C,P ) = y\u0302 but\nV0[R(y\u0302)] = 0, and the claim follows.\nTheorem 4.6 (Training on verified information does not induce factuality).\nargmax x\nf\u0302(x|C,P ) = y\u0302 \u2227\n\u2200xi \u2208 C : V0[R(xi)] = 1 =\u0338\u21d2 V0[R(y\u0302)] = 1\nProof. Suppose \u2200xi \u2208 C,R(y\u0302) \u0338= R(xi), and consider the structures s, s\u2032 such that \u2200xi \u2208 C :\nVs[R(xi)] = Vs\u2032 [R(xi)] = V0[R(xi)] = 1. Suppose \u2203x\u2032 /\u2208 C, Vs[R(x\u2032)] = 1 and Vs\u2032 [R(x\u2032)] = 0. Then take y\u0302 = x\u2032 and V0 = Vs\u2032 , and the claim follows.\nThe key step in each argument is that no information about V0 is learned by the model. So a structure can always exist on which the output sentence is false. And since we do not observe the truth-values of states of the world, we have no way of ruling out the possibility that the structure on which the sentence is false is in fact s0.\nThis result does not say that LLMs always hallucinate. But it does say that, when an LLM learns a distribution that does not incorporate explicit factual or extensional information, it is always possible for an LLM to hallucinate."
        },
        {
            "heading": "5 Building Factual/Faithful LLMs: How To Get (Evidential) Closure",
            "text": "How do we go beyond the negative result above? We require that the output of an LLM is equal to the closure under synonymy of strings that refer to verified information. That is, every string output by the LLM either refers to a claim that is verified, or is a synonym of a claim that is verified. By Proposition 3.5, all such sentences will be true. Any LLM that satisfies this property is then factual. If we require only that the output is closed under synonymy with strings that are synonymous with the agent\u2019s evidence, irrespective of its credibility, the LLM is faithful."
        },
        {
            "heading": "5.1 A Symmetry Group Semantics",
            "text": "It is helpful to use the technology of symmetry groups to formally motivate intensional and extensional learning. This section restates and develops results in Kiddon and Domingos (2005).\nDefinition 5.1 (Symmetry). A function g : X \u2192 X is a symmetry of a set X if {g(x) | x \u2208 X} = X .\nDefinition 5.2 (Symmetry Group). A symmetry group of a set X is an ordered pair (G, \u25e6) such that if g is a symmetry of X then g \u2208 G, and \u25e6 is function composition.\nDefinition 5.3 (Orbit). The orbit of x \u2208 X under a symmetry group G is the set {g(x) | g \u2208 G}\nWe motivate paraphrases as functions that, given a list of strings, permute pairs of strings that have the same referent.\nDefinition 5.4 (Paraphrase Map). A bijective function \u03c0 : L\u2192 L is a paraphrase map if \u2200\u2113 : R(\u2113) = R(\u03c0(\u2113))\nEssentially, \u03c0 is a permutation, with the added constraint that it can permute only strings in a list that have the same referent. We collect the set of paraphrases in the collection \u03a0. This is a symmetry group of L, since, every \u03c0 in \u03a0 is a permutation of the elements of L, and hence applying \u03c0 to L returns the same list of strings, that is, L.\nProposition 5.5. The set of paraphrase maps (\u03a0, \u25e6) is a symmetry group of the set L.\nProof. We suppose that that \u2126 is non-empty, and that L is fully-interpreted and unambiguous, so that, for each \u2113 \u2208 L,\u2203! \u03c9 \u2208 \u2126 : R(\u2113) = \u03c9. Then, since each \u03c0 \u2208 \u03a0 is a bijection, and hence is a permutation, we have that, \u2200\u03c0 \u2208 \u03a0, {\u03c0(\u2113) : \u2113 \u2208 L} = L, so that each \u03c0 is a symmetry of L. It is straightforward to show that (\u03a0, \u25e6) satisfies the group axioms: any composition of permutations \u03c0, \u03c0\u2032 \u2208 \u03a0 defines a permutation \u03c0\u2032\u2032 \u2208 \u03a0; composition is associative; there is a trivial permutation; and each permutation has an inverse permutation. Hence (\u03a0, \u25e6) is a symmetry group of the set L.\nDefinition 5.6 (Semantic Orbit). The orbit of \u2113 \u2208 L under \u03a0 is the set I(\u2113) = {\u03c0(\u2113)|\u03c0 \u2208 \u03a0}.\nThat is, the semantic orbit of a sentence is the set of sentences that refer to the same state of the world as that sentence.7 We collect the set of unique orbits of a language L in the set I ."
        },
        {
            "heading": "5.2 Factual LLMs",
            "text": "With this setup in place, we are now in a position to decompose the constrained learning task described in Problem 4.2. We introduce a source language L+, which contains the strings in some source set of sentences, and I + the set of unique orbits of L+. We can then rewrite the objective function as follows:\n7Note that the set of unique orbits of L partitions L into sets of strings that have the same referent. Further, for each unique orbit, there is a unique state of the world to which every sentence in that orbit refers. We can extend the notation of a valuation function so that it takes the set of unique orbits as its pre-image. We write: V0[I(\u2113)] = 1 \u21d0\u21d2 \u2200\u2113 \u2208 I(\u2113) : V0[R(\u2113)] = 1.\nProposition 5.7.\nf(\u2113|C,P, V0[R(\u2113)] = 1) = \u2211\nI\u2208I + f(\u2113|C,P, \u2113 \u2208 I(\u2113+) \u2229 V0[I(\u2113+)] = 1)\n= \u2211\nI\u2208I + f(\u2113|C,P )\ufe38 \ufe37\ufe37 \ufe38\nLLM\nf(\u2113|I(\u2113+))\ufe38 \ufe37\ufe37 \ufe38 Intensional\nLearner\nf(V0[I(\u2113 +)] = 1)\ufe38 \ufe37\ufe37 \ufe38\nGround truth or Evidence\nHere, f(V0[I(\u2113+)] = 1) represents the information about the world in each string \u2113+ of the source language L+.8\nIf we do not have a ground truth set of strings, but instead learn about the world via sensors (a vision model, for example), we can further decompose f(V0[I(\u2113 +)] = 1) as follows:\nProposition 5.8.\nf(V0[I(\u2113 +)] = 1)\n= f(R[I(\u2113+)] = \u03c9 \u2229 V0(\u03c9) = 1) = \u2211 \u03c9\u2208\u2126 f(I(\u2113+)|V0(\u03c9) = 1)Pr(V0(\u03c9) = 1)\n= \u2211 \u03c9\u2208\u2126 f(I(\u2113+)|\u03c9)\ufe38 \ufe37\ufe37 \ufe38 Extensional\nLearner\nf(\u03c9)\ufe38\ufe37\ufe37\ufe38 Perceptual\nLearner\nModel 5.9 (A Factual/Faithful Multimodal LLM).\nf(\u2113|C,P, V0[R(\u2113)] = 1) =\u2211 I\u2208I + \u2211 \u03c9\u2208\u2126 f(\u2113|C,P )\ufe38 \ufe37\ufe37 \ufe38 LLM f(\u2113|I(\u2113+))\ufe38 \ufe37\ufe37 \ufe38 Intensional\nLearner\nf(I(\u2113+)|\u03c9)\ufe38 \ufe37\ufe37 \ufe38 Extensional\nLearner\nf(\u03c9)\ufe38\ufe37\ufe37\ufe38 Perceptual\nLearner\nIn words, this models the constraint that \u03c9 obtains, there is a sentence in the source language that refers to \u03c9, and that the output sentence \u2113 is synonymous with a sentence in the source language. Hence Model 5.9 satisfies evidential closure. The representation in Model 5.9 above covers both factual and faithful LLMs.\nTheorem 5.10. Suppose f\u0302(\u03c9) is an oracle perceptual learner, we have consistent intensional and extensional learners, and an LLM that solves Equation (2). Then Model 5.9 is factual.\nProof. We have shown in Proposition 5.7 and Proposition 5.8 that f(\u2113|C,P, V0[R(\u2113)] = 1) = f(\u2113|C,P )f(\u2113|I(\u2113+))f(I(\u2113+)|\u03c9)f(\u03c9). Hence\n8Encyclopedia authors have essentially already done the extensional and perceptual learning for us: they have encoded information about the environment as text. It is in this sense that perceptual and extensional learning are necessary for truthful language generation even in the textual case.\nProblem 4.2 is solved if and only if this model can be consistently estimated. Since f\u0302(\u03c9) is an oracle, V0(\u03c9) = 1 \u21d0\u21d2 f\u0302(\u03c9) = 1, and since f\u0302(\u2113|I(\u2113+))f\u0302(I(\u2113+)|\u03c9) \u2192 f(\u2113|I(\u2113+))f(I(\u2113+)|\u03c9) by assumption, we have that f\u0302(\u2113|C,P )f\u0302(\u2113|I(\u2113+))f\u0302(I(\u2113+)|\u03c9)f\u0302(\u03c9) \u2192 f(\u2113|C,P, V0[R(\u2113)] = 1), solving Problem 4.2.\nTheorem 5.11. If we have a perceptual learner that is not an oracle, consistent intensional and extensional learners, and an LLM that solves Equation (2), then Model 5.9 is faithful to its perceptual learner.\nProof. Define V\u03020(\u03c9) \u2261 f\u0302(\u03c9). Then, V\u03020(\u03c9) = 1 \u21d0\u21d2 f\u0302(\u03c9) = 1, and f\u0302(\u2113|C,P )f\u0302(\u2113|I(\u2113+))f\u0302(I(\u2113+)|\u03c9)f\u0302(\u03c9) \u2192 f(\u2113|C,P, V\u03020[R(\u2113)] = 1), solving Problem 4.4."
        },
        {
            "heading": "5.3 Evidence Set Representation",
            "text": "We can state these results even more simply, however. Consider the following set:\nDefinition 5.12 (Evidence Set).\nE\u0302 \u2261 {\u2113|V\u03020[I\u0302(\u2113)] = 1}\nWe say that I\u0302 is the output of an intensional learner: the set of learned paraphrases of strings. V\u03020 is the set of stipulated or learned labels attached to strings and their paraphrases. In either case, this is the set of strings that comprise, or are synonymous with, a body of verified information about the world.\nThis definition is helpful, because it allows us to re-express Model 5.9 as:\nModel 5.13 (An Evidence-Grounded LLM).\nf(\u2113|C,P )I{\u2113 \u2208 E\u0302}\nTheorem 5.14. Suppose I\u0302 \u2192 I . Then Model 5.13 is faithful.\nProof. I\u0302 \u2192 I =\u21d2 I{\u2113 \u2208 E\u0302} \u2192 1 \u21d0\u21d2 V\u03020[R(\u2113)] = 1, so Model 5.13 solves Problem 4.4.\nTheorem 5.15. Suppose I\u0302 \u2192 I and V\u03020 \u2192 V0. Then Model 5.13 is factual.\nProof. V\u03020[I\u0302(\u2113)] \u2192 1 =\u21d2 V0[I\u0302(\u2113)] = 1, and I\u0302 \u2192 I =\u21d2 I{\u2113 \u2208 E\u0302} \u2192 1 \u21d0\u21d2 V0[I(\u2113)] = 1, so Model 5.13 solves Problem 4.2.\nE\u0302 denotes the set of strings that consist of both model\u2019s explicit evidence about the world, and their paraphrases. This set is evidentially closed, by Proposition 3.5. So the output of Model 5.13 is evidentially closed, and the model is faithful to its evidence.\nThe moral is that a faithful or factual LLM must learn about the world, directly (through sensor perception) or indirectly (through text corpora); and that its output must be constrained to be synonymous with claims that are contained within its evidence set. This provides theoretical motivation for grounding, and clarifies specifically what grounding is intended to accomplish."
        },
        {
            "heading": "5.4 Learn-Babble-Prune: A framework for factual/faithful LLMs via rejection sampling",
            "text": "We propose a procedure we call Learn-BabblePrune9 to implement this.\nIn the Learn phase, an agent learns about the world, either directly through perceptual learning, or indirectly by observing some stock of verified information. If this information is acquired through perception, these sentences are translated into natural language, via multimodal encoder-decoder system. The agent additionally learns a stock of paraphrases of sentences, via paraphrase corpora methods (Ormazabal et al., 2022), or via contrastive learning methods (Yang et al., 2021).\nIn the Babble phase, an LLM generates a stock of candidate sentences.\nIn the Prune phase, a generated sentence is crosschecked against the its Evidence Set. If the sentence is verified, or if it is a paraphrase of a verified sentence, it is printed. Otherwise, it is rejected."
        },
        {
            "heading": "5.5 Applications of Learn-Babble-Prune",
            "text": ""
        },
        {
            "heading": "5.5.1 Example 1: Text-To-Text",
            "text": "Learn Ground truth. Scrape an online encyclopedia, and designate this as the Evidence Set E\u0302. Intensional learning. Learn a set of paraphrases of strings in the Evidence Set, and add them to the Evidence Set. Babble An LLM generates a response to a query. Prune The response is rejected if it is not a paraphrase of a sentence in the Evidence Set.\n9This was inspired by He (2018)."
        },
        {
            "heading": "5.5.2 Example 2: Image-To-Text",
            "text": "Learn Extensional learning. Pre-train a visual encoder, which learns a mapping from images (states of the world) to strings. Perceptual learning. Designate a test set of images as ground truth about the environment. Apply the visual encoder to this test set of images. Designate its output as our Evidence Set. Intensional learning. Learn a set of paraphrases of strings in the Evidence Set, and add them to the Evidence Set. Babble An LLM generates a response to a query. Prune The response is rejected if it is not a paraphrase of a sentence in the Evidence Set.\nAlgorithm 1 LBP: Text-to-Text\n1: Input: (L, E\u0302, C, P ) 2: Learn: f\u0302(x|C,P ) \u25b7 Learn 3: Learn: \u2200\u2113+ \u2208 E\u0302,\u2200\u2113 \u2208 L: f\u0302(\u2113 \u2208 I(\u2113+)) 4: for \u2113 \u2208 L do 5: if \u2113 \u2208 I(\u2113+) \u2227 \u2113+ \u2208 E\u0302 then 6: E\u0302 \u2190 E\u0302 \u222a \u2113 7: end if 8: end for 9: Generate: y\u0302 \u223c f\u0302(x|C,P ) \u25b7 Babble 10: if y\u0302 \u2208 E\u0302 then \u25b7 Prune 11: Print: y\u0302 12: else 13: Print: \u201cI don\u2019t know.\u201d 14: end if\nSince the source domain is arbitrary, Example 2 covers a wide variety of use cases, which can of course be combined. The output of these procedures is faithful, because if a given candidate output is not synonymous with a claim for which the model has have explicit evidence, it is not printed.10"
        },
        {
            "heading": "5.6 The Limits of Factual or Faithful LLMs",
            "text": "Any model of the type of Model 5.13 is limited in what it can say by the size of its evidence base. In practice, the dimension of E\u0302 may be considerably smaller than the parametric knowledge of the language stored in many LLMs. Any use-case for factual LLMs requires the collection and verification of a large amount of factual information. Any factual or faithful LLM can only generate as much output as it can verify.\n10Wittgenstein (1922, 189): \u201cWhereof one cannot speak, thereof one must remain silent.\u201d This also applies to LLMs."
        },
        {
            "heading": "6 Conclusions",
            "text": "LLMs hallucinate because their output is not constrained to be semantically consistent with their inputs, so there is no guarantee that any evidence about the world contained in their inputs is preserved in their outputs.\nTo build a faithful or factual LLM it is necessary to constrain the output of a model to be consistent with claims for which the model has explicit evidence.\nIn practice, this means acquiring large bodies of machine-readable string evidence or using sensors (perceptual learning) combined with sensor-to-text encoders (extensional learning) to validate the output of an LLM with evidence. Paraphrase learning methods can be used to expand the LLM\u2019s vocabulary (intensional learning). We propose a simple method to implement this in practice via rejection sampling.\nAny input-faithful LLM is limited in what it can say by what it has evidence for. Generating large-scale evidence bases is likely to be a much bigger binding constraint that the parameter size of the model, for instance, and may require a rethink of how computational and financial resources are allocated to the design of LLMs. This is a challenge for the next generation of LLMs.\nAlgorithm 2 LBP: Multimodal-to-Text\n1: Input: (\u2126obs, L, L+, C, P ) 2: Learn: f\u0302(x|C,P ) \u25b7 LLM 3: Learn: f\u0302(\u03c9obs) \u25b7 Perceptual 4: Learn: \u2200\u2113+ \u2208 L+: f\u0302(\u2113+|\u03c9obs) \u25b7 Extensional 5: Learn: \u2200\u2113 \u2208 L: f\u0302(\u2113 \u2208 I(\u2113+)) \u25b7 Intensional 6: for \u2113+ \u2208 L+ do 7: if f(\u2113+|\u03c9obs)f(\u03c9obs) = 1 then 8: E\u0302 \u2190 E\u0302 \u222a \u2113+ 9: end if 10: end for 11: for \u2113 \u2208 L do 12: if \u2113 \u2208 I(\u2113+) \u2227 \u2113+ \u2208 E\u0302 then 13: E\u0302 \u2190 E\u0302 \u222a \u2113 14: end if 15: end for 16: Generate: y\u0302 \u223c f\u0302(x|C,P ) \u25b7 Babble 17: if y\u0302 \u2208 E\u0302 then \u25b7 Prune 18: Print: y\u0302 19: else 20: Print: \u201cI don\u2019t know.\u201d 21: end if\nLimitations\nEmpirical performance and truthfulness\nEnforcing consistency with evidence via rejection sampling is a relatively inefficient way to constrain output to be factual or faithful. We expect that RL approaches could implement a procedure analogous to Learn-Babble-Prune more efficiently, with tolerable loss of accuracy. The purpose of this framework is to highlight, at a high-level, the kind of tasks that any such RL approach would have to do. As such, it is primarily intended as a conceptual contribution. Further, strict factuality may be an undesirably high bar in some applications: clearly it depends on the use case.\nSafety and ground truth\nA factual LLM constructed as above would require either a gold-standard data set of labelled data. However, there are clear safety concerns raised by treating some data sets as ground truth compared to others. Further, what counts as evidence in some domains is contestable: reasonable people disagree. Widely available benchmark data sets have well-studied biases (Paullada et al., 2021).\nCompositionality\nOur framework does not consider the semantic content of constituents of sentences, instead considering strings as primitive, and assuming that they each refer to one fact. It is straightforward to extend our account to sentences that refer to the composition of multiple states of the world and logical operators, which we leave to future work.\nLanguage and Vagueness\nWe assume that a stock of strings is interpretable and unambiguous. Many sentences in natural language cannot be considered to have a truth-value, or may be ambiguous.\nPhilosophy of language\nThe relevant philosophy of language literature is vast and we cannot hope to do it justice in this paper. Further, some conceptual distinctions that are important to understanding the papers cited are not made in this paper. The hope is that the setting provides offers a statistically tractable implementation of conceptual material that is covered in the papers cited. Subtleties, and perhaps even major distinctions, are likely to be lost in translation."
        },
        {
            "heading": "Acknowledgements",
            "text": "This paper is dedicated to Stephen Williams. The author would like to thank Micah Carroll, Kirk Bansak, Orr Paradise, and three anonymous referees for helpful comments."
        }
    ],
    "title": "Why LLMs Hallucinate, And How To Get (Evidential) Closure: Perceptual, Intensional and Extensional Learning for Faithful Natural Language Generation",
    "year": 2023
}