{
    "abstractText": "STEM educators must trade off the ease of assessing selected response (SR) questions, like multiple choice, with constructed response (CR) questions, where students articulate their own reasoning. Our work addresses a CR type new to NLP but common in college STEM, consisting of multiple questions per context. To relate the context, the questions, the reference responses, and students\u2019 answers, we developed an Answer-state Recurrent Relational Network (AsRRN). In recurrent time-steps, relation vectors are learned for specific dependencies in a computational graph, where the nodes encode the distinct types of text input. AsRRN incorporates contrastive loss for better representation learning, which improves performance and supports student feedback. AsRRN was developed on a new dataset of 6,532 student responses to three, two-part CR questions. AsRRN outperforms classifiers based on LLMs, a previous relational network for CR questions, another graph neural network baseline, and few-shot learning with GPT-3.5. Ablation studies show the distinct contributions of AsRRN\u2019s dependency structure, the number of time steps in the recurrence, and the contrastive loss.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhaohui Li"
        },
        {
            "affiliations": [],
            "name": "Susan E. Lloyd"
        },
        {
            "affiliations": [],
            "name": "Matthew D. Beckman"
        },
        {
            "affiliations": [],
            "name": "Rebecca J. Passonneau"
        }
    ],
    "id": "SP:715c48e17e746945644f98330c7c4572e1d77e46",
    "references": [
        {
            "authors": [
                "Dimitrios Alikaniotis",
                "Helen Yannakoudakis",
                "Marek Rei."
            ],
            "title": "Automatic text scoring using neural networks",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 715\u2013725, Berlin, Ger-",
            "year": 2016
        },
        {
            "authors": [
                "Matthew D. Beckman."
            ],
            "title": "Assessment of cognitive transfer outcomes for students of introductory statistics",
            "venue": "Ph.D. thesis, University of Minnesota\u2014Twin Cities).",
            "year": 2015
        },
        {
            "authors": [
                "Randy Elliot Bennett."
            ],
            "title": "On the meanings of constructed response",
            "venue": "ETS Research Report Series, 1991(2):i\u201346.",
            "year": 1991
        },
        {
            "authors": [
                "Menucha Birenbaum",
                "Kikumi K Tatsuoka."
            ],
            "title": "Open-ended versus multiple-choice response formats\u2014it does make a difference for diagnostic purposes",
            "venue": "Applied Psychological Measurement, 11(4):385\u2013395.",
            "year": 1987
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Leon Camus",
                "Anna Filighera."
            ],
            "title": "Investigating transformers for automatic short answer grading",
            "venue": "International Conference on Artificial Intelligence in Education, pages 43\u201348. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton."
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings",
            "year": 2020
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton."
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "International conference on machine learning, pages 1597\u20131607. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Sumit Chopra",
                "Raia Hadsell",
                "Yann LeCun."
            ],
            "title": "Learning a similarity metric discriminatively, with application to face verification",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905), volume 1, pages 539\u2013546",
            "year": 2005
        },
        {
            "authors": [
                "Jacob Cohen."
            ],
            "title": "A coefficient of agreement for nominal scales",
            "venue": "Educational and psychological measurement, 20(1):37\u201346.",
            "year": 1960
        },
        {
            "authors": [
                "Alexis Conneau",
                "Douwe Kiela",
                "Holger Schwenk",
                "Lo\u00efc Barrault",
                "Antoine Bordes."
            ],
            "title": "Supervised learning of universal sentence representations from natural language inference data",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Nat-",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Myroslava Dzikovska",
                "Rodney Nielsen",
                "Chris Brew",
                "Claudia Leacock",
                "Danilo Giampiccolo",
                "Luisa Bentivogli",
                "Peter Clark",
                "Ido Dagan",
                "Hoa Trang Dang"
            ],
            "title": "SemEval-2013 task 7: The joint student response analysis and 8th recognizing textual entail",
            "year": 2013
        },
        {
            "authors": [
                "B. Efron",
                "R. Tibshirani."
            ],
            "title": "Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy",
            "venue": "Statistical Science, 1(1):54 \u2013 75.",
            "year": 1986
        },
        {
            "authors": [
                "Hongchao Fang",
                "Sicheng Wang",
                "Meng Zhou",
                "Jiayuan Ding",
                "Pengtao Xie."
            ],
            "title": "Cert: Contrastive self-supervised learning for language understanding",
            "venue": "arXiv preprint arXiv:2005.12766.",
            "year": 2020
        },
        {
            "authors": [
                "Joseph L Fleiss."
            ],
            "title": "Measuring nominal scale agreement among many raters",
            "venue": "Psychological bulletin, 76(5):378.",
            "year": 1971
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "J.B. Garfield",
                "D. Ben-Zvi",
                "B. Chance",
                "E. Medina",
                "C. Roseth",
                "A. Zieffler."
            ],
            "title": "Assessment in statistics education",
            "venue": "J. B. Garfield, D. Ben-Zvi, B. Chance, E. Medina, C. Roseth, and A. Zieffler, editors, Developing Students\u2019 Statistical Reasoning:",
            "year": 2008
        },
        {
            "authors": [
                "Steve Graham",
                "Sharlene A. Kiuhara",
                "Meade MacKay."
            ],
            "title": "The effects of writing on learning in science, social studies, and mathematics: A metaanalysis",
            "venue": "Review of Educational Research, 90(2):179\u2013 226.",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778.",
            "year": 2016
        },
        {
            "authors": [
                "Annette Hilton",
                "Geoff Hiltona",
                "Shelley Dole",
                "Merrilyn Goos."
            ],
            "title": "Development and application of a two-tier diagnostic instrument to assess middle-years students\u2019 proportional reasoning",
            "venue": "Mathematics Education Research Journal, 25:523\u2013545.",
            "year": 2013
        },
        {
            "authors": [
                "Justin Johnson",
                "Bharath Hariharan",
                "Laurens Van Der Maaten",
                "Li Fei-Fei",
                "C Lawrence Zitnick",
                "Ross Girshick."
            ],
            "title": "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning",
            "venue": "Proceedings of the IEEE Conference",
            "year": 2017
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "Chen Wang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan."
            ],
            "title": "Supervised contrastive learning",
            "venue": "Advances in neural information processing systems, 33:18661\u201318673.",
            "year": 2020
        },
        {
            "authors": [
                "Phuc H Le-Khac",
                "Graham Healy",
                "Alan F Smeaton."
            ],
            "title": "Contrastive representation learning: A framework and review",
            "venue": "Ieee Access, 8:193907\u2013193934.",
            "year": 2020
        },
        {
            "authors": [
                "Zhaohui Li",
                "Yajur Tomar",
                "Rebecca J. Passonneau."
            ],
            "title": "A semantic feature-wise transformation relation network for automatic short answer grading",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Tiaoqiao Liu",
                "Wenbiao Ding",
                "Zhiwei Wang",
                "Jiliang Tang",
                "Gale Yan Huang",
                "Zitao Liu."
            ],
            "title": "Automatic short answer grading via multiway attention networks",
            "venue": "International Conference on Artificial Intelligence in Education, pages 169\u2013173. Springer.",
            "year": 2019
        },
        {
            "authors": [
                "Zhiyuan Liu",
                "Jie Zhou."
            ],
            "title": "Introduction to graph neural networks",
            "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning, 14(2):1\u2013127.",
            "year": 2020
        },
        {
            "authors": [
                "Samuel A. Livingston."
            ],
            "title": "Constructed-response test questions: Why we use them; how we score them",
            "venue": "Technical Report 11, Educational Testing Service.",
            "year": 2009
        },
        {
            "authors": [
                "Vivi Nastase",
                "Rada Mihalcea",
                "Dragomir R Radev."
            ],
            "title": "A survey of graphs in natural language processing",
            "venue": "Natural Language Engineering, 21(5):665\u2013698.",
            "year": 2015
        },
        {
            "authors": [
                "Rasmus Palm",
                "Ulrich Paquet",
                "Ole Winther."
            ],
            "title": "Recurrent relational networks",
            "venue": "Advances in Neural Information Processing Systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Lin Pan",
                "Chung-Wei Hang",
                "Avirup Sil",
                "Saloni Potdar."
            ],
            "title": "Improved text classification via contrastive adversarial training",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36 (10), pages 11130\u201311138.",
            "year": 2022
        },
        {
            "authors": [
                "D.K. Pearl",
                "J.B. Garfield",
                "R. delMas",
                "R.E. Groth",
                "J.J. Kaplan",
                "H. McGowan",
                "H.S. Lee"
            ],
            "title": "Connecting research to practice in a culture of assessment for introductory college-level statistics",
            "year": 2012
        },
        {
            "authors": [
                "Yao Qiu",
                "Jinchao Zhang",
                "Jie Zhou."
            ],
            "title": "Improving gradient-based adversarial training for text classification by contrastive learning and auto-encoder",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1698\u20131707,",
            "year": 2021
        },
        {
            "authors": [
                "Lakshmi Ramachandran",
                "Peter Foltz."
            ],
            "title": "Generating reference texts for short answer scoring using graph-based summarization",
            "venue": "Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 207\u2013212.",
            "year": 2015
        },
        {
            "authors": [
                "Brian Riordan",
                "Andrea Horbach",
                "Aoife Cahill",
                "Torsten Zesch",
                "Chungmin Lee."
            ],
            "title": "Investigating neural architectures for short answer scoring",
            "venue": "Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages",
            "year": 2017
        },
        {
            "authors": [
                "Swarnadeep Saha",
                "Tejas I Dhamecha",
                "Smit Marvaniya",
                "Renuka Sindhgatta",
                "Bikram Sengupta."
            ],
            "title": "Sentence level or token level features for automatic short answer grading?: Use both",
            "venue": "International Conference on Artificial Intelligence in Education,",
            "year": 2018
        },
        {
            "authors": [
                "Adam Santoro",
                "David Raposo",
                "David G Barrett",
                "Mateusz Malinowski",
                "Razvan Pascanu",
                "Peter Battaglia",
                "Timothy Lillicrap."
            ],
            "title": "A simple neural network module for relational reasoning",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Florian Schroff",
                "Dmitry Kalenichenko",
                "James Philbin."
            ],
            "title": "Facenet: A unified embedding for face recognition and clustering",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815\u2013823.",
            "year": 2015
        },
        {
            "authors": [
                "Mark D Shermis."
            ],
            "title": "Contrasting state-of-the-art in the machine scoring of short-form constructed responses",
            "venue": "Educational Assessment, 20(1):46\u201365.",
            "year": 2015
        },
        {
            "authors": [
                "Chul Sung",
                "Tejas Indulal Dhamecha",
                "Nirmal Mukhi."
            ],
            "title": "Improving short answer grading using transformer-based pre-training",
            "venue": "International Conference on Artificial Intelligence in Education, pages 469\u2013481. Springer.",
            "year": 2019
        },
        {
            "authors": [
                "Flood Sung",
                "Yongxin Yang",
                "Li Zhang",
                "Tao Xiang",
                "Philip HS Torr",
                "Timothy M Hospedales."
            ],
            "title": "Learning to compare: Relation network for few-shot learning",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages",
            "year": 2018
        },
        {
            "authors": [
                "Neslihan S\u00fczen",
                "Alexander N Gorban",
                "Jeremy Levesley",
                "Evgeny M Mirkes."
            ],
            "title": "Automatic short answer grading and feedback using text mining methods",
            "venue": "Procedia Computer Science, 169:726\u2013743.",
            "year": 2020
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Lio",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "arXiv preprint arXiv:1710.10903.",
            "year": 2017
        },
        {
            "authors": [
                "Tianqi Wang",
                "Naoya Inoue",
                "Hiroki Ouchi",
                "Tomoya Mizumoto",
                "Kentaro Inui."
            ],
            "title": "Inject rubrics into short answer grading system",
            "venue": "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), pages 175\u2013182.",
            "year": 2019
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Yang Xiao",
                "Jing Han",
                "Kathleen Koenig",
                "Jianwen Xiong",
                "Lei Bao."
            ],
            "title": "Multilevel Rasch modeling of two-tier multiple choice test: A case study using Lawson\u2019s classroom test of scientific reasoning",
            "venue": "Physical Review Physics Education Research, 14:020104.",
            "year": 2018
        },
        {
            "authors": [
                "Tzu-Chi Yang",
                "Sherry Y. Chen",
                "Gwo-Jen Hwang."
            ],
            "title": "The influences of a two-tier test strategy on student learning: A lag sequential analysis approach",
            "venue": "Computers & Education, 82:366\u2013377.",
            "year": 2015
        },
        {
            "authors": [
                "Yue Zhang",
                "Qi Liu",
                "Linfeng Song."
            ],
            "title": "Sentencestate LSTM for text representation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 317\u2013327, Melbourne, Australia. Association",
            "year": 2018
        },
        {
            "authors": [
                "Liu Zhuang",
                "Lin Wayne",
                "Shi Ya",
                "Zhao Jun."
            ],
            "title": "A robustly optimized BERT pre-training approach with post-training",
            "venue": "Proceedings of the 20th Chinese National Conference on Computational Linguistics, pages 1218\u20131227. Chinese Information Processing",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "STEM educators must trade off the ease of assessing selected response (SR) questions, like multiple choice, with constructed response (CR) questions, where students articulate their own reasoning. Natural Language Processing (NLP) research has turned increasingly towards assessment of constructed response (CR) questions in recent years, but models remain relatively simple, in part because datasets are either relatively small for neural methods, or proprietary. In addition, public datasets are limited to standalone questions. Timely feedback is important in the context of formative assessment throughout a course. As a result, formative assessments in STEM currently rely largely\non SR items, where assessment is immediate. This conficts, however, with the current emphasis in STEM on critical thinking, reasoning and writing (Graham et al., 2020; Birenbaum and Tatsuoka, 1987). Manual assessment of CR is time consuming, and more so when feedback comments accompany a score. We present a relational neural network that can be adapted to handle different CR formats, and that outperforms strong baselines on a new dataset of statistics questions developed to measure statistical reasoning at the introductory college level. Moreover, it also groups responses for similar feedback, in an unsupervised way.\nThe most well known and reasonably sized NLP datasets for CR questions are the BEETLE and SciEntsBank data from SemEval 13 Task 7 (Dzikovska et al., 2013) and the Kaggle dataset ASAP (Shermis, 2015). The former has 253 prompts across 16 STEM domains; the latter has 10 prompts where the STEM is mostly from biology, with 22,000 items. Assessment scales range from 2-way to 5-way. There are often multiple reference answers for each point level. ASAP also contains scoring rubrics. In both datasets, the prompts are standalone questions, but there are many other CR formats (Livingston, 2009; Bennett, 1991). We present ISTUDIO, a new dataset with context-based questions, a common format in STEM. Table 1 shows a context, its two questions, the rubric and reference answers for the first question, and a student response to each question.\nRecent automatic assessment of CR questions using neural models has achieved good performance on the benchmark datasets mentioned above. The standard approach is to encode the question prompt and student answer separately, and send their concatenation to a classifier output layer. Riordan et al. (2017) achieved good performance on ASAP with a biLSTM. Later work used transformers (e.g., BERT (Devlin et al., 2019)). Using a model that encodes rubric elements, Wang et al. (2019) achieved\nresults similar to (Riordan et al., 2017). Saha et al. (2018) separately encoded the question, the reference answer and the student answer using InferSent (Conneau et al., 2017), and combined the three vectors with manually engineered features for word overlap and part-of-speech information as input to a simple classifier. On the nine SemEval tasks, accuracies ranged from 0.51 to 0.79. Good results are also reported on proprietary datasets (Saha et al., 2018; Sung et al., 2019; Liu et al., 2019). Camus and Filighera (2020) compared several pretrained models on SciEntsBank, where RoBERTa (Zhuang et al., 2021) performed best. SFRN (Li et al., 2021) outperformed all these models using a more novel relational architecture. It used BERT to separately encode the question, the reference answer (SemEval) or a rubric (ASAP), and the student answer, and then learned a single relational vector over the three encodings. We use a more complex relational network that outperforms SFRN.\nBesides automating the assignment of a correctness score, NLP has the potential to support feedback comments. However, when and how to provide such feedback is not well understood, and is more of an unsupervised clustering problem than a classification problem, as illustrated in Figure 1. The figure shows a coincidence matrix of feedback clusters for 100 randomly selected student responses to ISTUDIO question [2b] labeled partially correct, and where answers were partially or essentially correct on [2a]. Two statistics experts worked independently to cluster the responses, and assign a feedback phrase. Expert A identified three clusters that covered all but 4 items (nc for no cluster). Expert B originally identified 14 that covered all but one item, then combined clusters that were similar and arrived at 4 clusters. Figure 1 shows an overlap of 43 items from A\u2019s and B\u2019s largest clusters, and another overlap of 21 items. All the other cells have small counts. This shows that different people are likely to provide comments in different ways, but might agree on a few categories that would cover a large proportion of student answers. To make progress towards automated formative assessment that can group similar responses, we develop a novel use of contrastive learning (Chen et al., 2020a; Chopra et al., 2005), to learn representations that are more similar to a reference answer that is distinct from other reference answers.\nRelational networks (Palm et al., 2018), where graph nodes are input elements and edges are relation vectors, can have complex graph configurations, and are more efficient than graph attention or graph convolution (Nastase et al., 2015; Liu and\nZhou, 2020). We propose AsRRN, a family of recurrent relational networks where recurrence is over time, to learn distinct relations among the different inputs in CR datasets. Depending on the constituency of a dataset, each distinct input text type (e.g., scenario context, question prompt, reference answer, student answer, rubric) is encoded by a pretrained language model, and becomes a node in a graph network where edges are dependency relations. We find that the highest-performing dependency structure is the one that captures the semantic dependencies of the domain. A global state node is a dependent of all other nodes. We apply contrastive learning (CL) where reference examples serve as positive and negative prototypes. AsRRN outperforms multiple strong baselines on correctness classification, and to some degree can replicate the feedback grouping illustrated in Figure 1."
        },
        {
            "heading": "2 Related Work",
            "text": "Deep neural networks (DNNs) have been applied to CR assessment at least since (Alikaniotis et al., 2016). As noted in the introduction, most DNNs for CR encode the (reference answer, student answer) pair and use various output functions. Alikaniotis et al. (2016) used Long-Short Term Memory (LSTM) networks enhanced by word-specific scores, and achieved high score correlations on ASAP. Riordan et al. (2017) combined Convolutional Neural Networks (CNNs) with LSTMs. Model performance with these earlier DNNs was sometimes shown to improve in combination with hand-crafted features (Saha et al., 2018) and text mining (S\u00fczen et al., 2020). As noted above, pretrained LMs like RoBERTa have performed well (Camus and Filighera, 2020). We rely on pretrained LMs to encode the input layer, and use a novel relational network.\nExplicit use of rubric information is less common. AutoP achieves a performance boost on ASAP through automatic generation of regular expression patterns from top-tier answers and rubrics (Ramachandran and Foltz, 2015). Wang et al. (2019) integrate rubric information via word-level attention, finding that the rubric information compensated for less data, but did not otherwise boost performance. We found that incorporating rubrics into AsRRN did not improve performance on ISTUDIO. This may be due to the compositional logic of rubric statements, that can have multiple conjunctions and disjunctions (see Table 1).\nRelational networks (RNs) were initially developed as an alternative to other graph-based models for reasoning problems in visual, linguistic, or symbolic domains (e.g., physics (Santoro et al., 2017)), where distinct types of input elements have correspondingly distinct interrelations. RNs have surpassed human performance on the CLEVR visual question answering dataset (Johnson et al., 2017), and have become a general framework for few-shot learning (Sung et al., 2018). Li et al. (2021) showed that an RN for short answer assessment could efficiently learn relations among encodings of the question, reference answer, and student answer. Recurrent Relational Networks (RRNs) (Palm et al., 2018) eschew the flat relational structure of an RN for a fully connected relational graph with pairwise message passing. We found this original RRN structure to be disadvantageous for our CR tasks (see discussion accompanying Figure 3). For ISTUDIO, AsRRN adopts a dependency structure that reflects how people reason about the text elements shown in Fig. 1.\nContrastive Learning (CL) aims to improve representation learning by making similar data instances closer and separating dissimilar ones (LeKhac et al., 2020). CL can be applied in unsupervised or supervised learning, and benefits a wide array of visual, language and multi-modal tasks. Chopra et al. (2005) pioneered the introduction of contrastive loss in dimensionality reduction. Subsequently, Schroff et al. (2015) proposed triplet loss, which employs an anchor to concurrently handle positive and negative samples. Later, Chen et al. (2020b) introduced the highly-regarded selfsupervised normalized temperature-scaled cross entropy loss (NT-Xent), with strong results on unsupervised learning in ImageNet. Khosla et al. (2020) extended NT-Xent loss for application to supervised learning; CERT (Fang et al., 2020) improved pretrained language models (e.g., BERT) using contrastive self-supervised learning at the sentence level. Qiu et al. (2021) and Pan et al. (2022) developed new text classification methods using contrastive learning loss, and SimCSE (Gao et al., 2021) delivered state-of-the-art sentence embeddings on semantic textual similarity (STS) tasks. We propose a variant of NT-Xent with a distinct treatment for different classes: we assume multiple correct reference examples to the same question should be similar, whereas different ways to be partially correct should be dissimilar (see Figure 1)."
        },
        {
            "heading": "3 Dataset",
            "text": "The context-based format is a novel challenge for NLP, but is widely used in STEM, e.g., in socalled two-tier assessments that simultaneously address students\u2019 content knowledge/skills in tier one and explanatory reasoning in tier two, as in computer science (Yang et al., 2015), mathematics (Hilton et al., 2013), physics (Xiao et al., 2018) and other STEM subjects. Automatic methods for low-stakes formative assessment can support timely feedback to identify which conceptual issues to address next, thus can lead to greater student success in STEM. CR questions that address reasoning skills are preferable to selected response items but burdensome for humans to assess. The ISTUDIO data come from an assessment instrument for statistical reasoning called The Introductory Statistics Transfer of Understanding and Discernment Outcomes (ISTUDIO) that was found to be educationally valid and reliable (Beckman, 2015), meaning there is strong evidence that it measures well what it intends to measure.\nThe ISTUDIO dataset consists of responses from 1,935 students to three 2-part questions. The assessment rubric, which uses a 3-way scale of (essentially) correct, partially correct, incorrect, specifies the scoring criteria and provides example student answers for each correctness class. The data is de-identified, and the original IRB confirmed that it can be made public for research purposes in its de-identified form. Students\u2019 schools of origin are identified as large, medium or small.1 We evaluated inter-rater agreement among two of the co-authors (A, B), and a graduate student in statistics (C). There were 63 items scored by A, B, and C; two additional sets of 63 were scored by A, B, and B, C. These pairs of raters achieved quadratic weighted kappa (QWK) > 0.79, and Fleiss Kappa showed substantial agreement among A, B and C (FK = 0.70). The remaining unlabeled data was assessed independently by A, B and C.2\nTo prepare ISTUDIO for NLP research, we performed data cleaning to eliminate non-answers and responses that were unusually long (greater than 125 word tokens). This yielded 7,258 individual\n1Li, Z., S. E. Lloyd, M. D. Beckman, and R. J. Passonneau. ISTUDIO: A Statistical Automatic Response Assessment Dataset. Penn State Data Commons, 2023. https://doi.org/10.26208/JFMP-V777\n2Fleiss Kappa is described in (Fleiss, 1971). QWK applies a quadratic weighting for agreement, partial agreement, and disagreement applied to Cohen\u2019s Kappa (Cohen, 1960).\nstudent response items, ten percent of which was held in reserve for future research. Ninety percent of the remainder (N=6,532) was partitioned in the proportion 8:1:1 into training (N=5,226), validation (N=653), and test (N=653) sets. The test set contains all the responses with labels from multiple raters so that at test time, inter-rater agreement of model predictions can be computed with the reliable human labels."
        },
        {
            "heading": "4 Approach",
            "text": "The ISTUDIO dataset contains four distinct types of textual input: the context C for a given question prompt, the question prompt Q, one or more reference answers R for each correctness class c \u2208 {0, 1, 2}, and the student answers A.3 Among many possible ways to structure the set of encodings of each text type\u2013such as concatenation, stacking, attention layers across sequences of text types, and graph networks\u2013we hypothesized that semantic relations among the five text types could be structured as a dependency graph rooted at C, with the learned representation of each dependent node being directly influenced by its parent, and indirectly by its ancestors. As discussed in detail below, the specific chain of dependencies we hypothesized is shown in Figure 2. Our hypothesis is similar in spirit to the motivation for recurrent relational networks, namely to process a chain of interdependent steps of relational inference, as in (Palm et al., 2018; Battaglia et al., 2018). Here, recurrence refers to inferential time-steps for the entire graph, rather than recurrence of processing units. Temporal recurrence allows the network to iteratively progress towards the full set of relations among all the nodes of the graph (Palm et al., 2018). AsRRN also incorporates a super-node that is dependent on all other nodes, as in (Zhang et al., 2018), to stabilize the flow of information in the graph network, and to implicitly represent the current answer state at each pass over the whole graph. We test our hypothesized relational structure among C,Q, (Rrubic, ), R and A through extensive ablations of alternative dependency structures, and by comparison to strong baselines that either have a simpler relational structure, or no relational structure at all. In this section, we present details on the graph structure, the flow of information within the graph (message-passing), the temporal recurrence, the output classifier layer, and our training\n3Incorporating the rubric text into AsRRN was not helpful.\nobjective that incorporates contrastive learning. AsRRN\u2019s relational graph for ISTUDIO has the four layers shown in schematic form in Figure 2. The input layer to each node type relies on a pre-trained model to encode the distinct types of text input at time t0. Note that the C, Q, and A layers are shown as consisting of a single node, whereas the R layer has four nodes. This indicates that the pretrained encoding of a single text input is used to initialize the layers for C,Q and A whereas for the R layer we initialize with multiple, different reference answers for each correctness class. Again through experimentation, we found that four reference answers for each correctness class achieved the best performance. For simplicity, the figure shows only four R nodes instead of the full set of twelve per Q (four per class). The arrows showing the dependencies between layers (referred to below as message passing) essentially correspond to relation vectors computed from the hidden states at each node. The super-node S is a global relational vector that incorporates information from the nodes that correspond to the distinct text inputs. The figure also illustrates the temporal recurrence for updating the flow of information within the graph.\nBefore describing the information flow, or message passing, we point out here that AsRRN can be adapted for CR datasets that have different structures. For example, the C layer can be removed to fit the types of standalone question prompts found in benchmark datasets such as SemEval-2013 Task 7 (Dzikovska et al., 2013) and ASAP (Shermis, 2015). This is how we perform AsRRN experiments on these two benchmarks, as reported below.\nAsRRN\u2019s Recurrent message passing over time corresponds to the computation of relational information among nodes within the neural network graph structure. In AsRRN, message passing is restricted to pairs of nodes i and j at time t, and each message mtij (or relational vector) is:\nmtij = f ( ht\u22121i , h t\u22121 j ) (1)\nwhere f , the message function, is a multi-layer perceptron. All messages mtij are dispatched in parallel, including the super-node. To update each node i, the messages from its set of neighbor nodes N(i) are first summed: mti = \u2211 j\u2208N(i)m t ij . Each node hidden state hti is updated recurrently:\nhti = g ( ht\u22121i , xi,m t i ) (2)\nwhere the node function g is another MLP, and xi is the original input encoding at node i. Reliance on each node\u2019s previous hidden state ht\u22121i allows the network to iteratively work towards a solution, as described in (Palm et al., 2018). Further, persistent use of the input feature vector xi at each time step is similar to the operation of a residual network (He et al., 2016), relieving g from the need to remember the input, and allowing it to focus primarily on the incoming messages from N(i).\nClassification of a student\u2019s answer for correctness utilizes the hidden state at node A at the final time step t. The output distribution ot is:\not = \u0398 ( At ) (3)\nwhere \u0398 is a learnable function to assign the A node\u2019s hidden state to class probabilities.\nThe training objective includes cross entropy loss for classification performance and a new contrastive learning loss that exploits the availability of reference answers to improve the representation learning. Contrastive learning aims to reduce the distance between learned representations of positive examples to a set of positive exemplars, and to increase the distance from a set of negative exemplars (Chopra et al., 2005). We adapt the widely used NT-Xent contrastive loss function (normalized temperature scaled cross entropy; see above).\nHere, where we have n reference examples for each of the three correctness classes, we treat the n reference answers for the predicted class as positive exemplars, and the 2n reference answers for the other two classes as negative exemplars.\nOur CL function depends on the true class label. For our use case, we make three strong assumptions: 1) that there is one way for a student answer to be correct regardless of the exact wording, 2) that there are a few ways, each distinct from the others, for a student\u2019s answer to be partially correct that will cover many of the answers, and 3) that there are many diverse ways to be incorrect. For the correct class, the n reference answers are assumed to be near paraphrases, thus for a student answer xi whose label is correct, LCL incorporates a term Sj for the average of its similarities to the n correct reference examples:\nSj =Average [ sim(xi,xj1)\n\u03c4 , . . . ,\nsim(xi,xjn)\n\u03c4 ] where xj are exemplars from the same class as xi, and xk are exemplars from the other classes.\nFor items xi predicted to be partially correct or incorrect, our loss function incorporates a term S\n\u2032 j for the maximum of its similarities to the n\nreference examples in the same class: S \u2032j =Max [ sim(xi,xj1)\n\u03c4 , . . . ,\nsim(xi,xjn)\n\u03c4 ] This ensures that if there is one partially correct reference answer that the student answer resembles, the learned representation will become closer to that reference answer. All class predictions use a normalizing term Sk:\nSk = \u2211 k sim(xi,xk) \u03c4\nyielding two contrastive loss terms, depending on the ground truth class label:\nLCL =\u2212 1\nN\n\u2211 log\nexp(Sj) exp(Sj) + exp(Sk) (4)\nL\u2032CL =\u2212 1\nN\n\u2211 log\nexp(S \u2032j) exp(S \u2032j) + exp(Sk) (5)\nIn sum, correct predictions are encouraged to be similar to the average of the correct reference examples and dissimilar to the reference examples for partially correct and incorrect. Partially correct (and incorrect) predictions are encouraged to be similar to one of the partially correct (or incorrect) predictions, and dissimilar to the reference examples for the other two classes.\nThe full loss function is a weighted sum of the cross entropy loss and the contrastive loss: L = (1\u2212\u03bb)LCE +\u03bbLCL, where \u03bb is a hyperparameter that governs the proportion of the two types of loss. Cross entropy loss is:\nLCE = \u2212 1\nN N\u2211 i=1 C\u2211 j=1 yij log(pij) (6)\nwhere yij is the true label for the ith sample and jth class, and pij represents the predicted probability for the ith sample and jth class. N is the number of samples, and C is the number of classes. If sample i belongs to class j, yij is 1 (true label) and 0 otherwise, and pij is the predicted probability that sample i belongs to class j."
        },
        {
            "heading": "5 Experiments",
            "text": "Four experiments are reported here. In the first, AsRRN significantly outperforms multiple baselines on ISTUDIO, the 3-way unseen answers (UA) subset of BEETLE and SciEntsBank, and the 3-way subset of ASAP. The second set compares AsRRN to chain-of-thought prompting on GPT-3.5. Third, extensive ablations verify the benefits of multiple design choices in AsRRN. Fourth, we compare AsRRN\u2019s grouping of student answers with human grouping for partially correct responses.\nFor baselines, we use four high-performing LMs available on Huggingface (BERT-base-uncased; RoBERTa-base; ConvBERT-base; DistilBERTbase-uncased); also SFRN (Li et al., 2021) and a benchmark graph neural network model graph attention network (GAT) (Velic\u030ckovic\u0301 et al., 2017). For the LMs, we concatenate the encodings of the question prompt, reference answer, and student answer, and add an MLP output layer followed by softmax. We do not encode the context in part because it sometimes exceeds the input token length and in part because it does not seem to improve performance. We use two versions of SFRN, with BERT encodings at the input layer; we either pass the 4-tuple of the context (C), question (Q), reference answer (R) and student answer (A) to the relational layer, or the (Q,R,A) triple (as in (Li et al., 2021). For the GAT experiments, we also\nuse BERT pretrained embedding as the input of each node, the graph structure we use is the default connections of all C, Q, R, and A nodes with 4 or 8 self-attention heads.4 All results show 95% bootstrap confidence intervals (Efron and Tibshirani, 1986) from 8 runs on the test sets (parallelized over 8 GPU cores). Table 2 shows evidence of stastical differences, with AsRRN outperforming the four LMs. AsRRN (+CL) also outperforms SFRN (C,Q,R,A), and nearly so for SFRN (Q, R, A). The AsRRN model not only outperforms the GAT model across all datasets, but it also boasts a shorter running time. This underscores AsRRN\u2019s superior efficiency and enhanced reasoning capacity. Details on the AsRRN settings are below; training parameters are in appendix A.\nGPT models often perform well using chain-ofthought reasoning for zero-shot and few-shot settings(Brown et al., 2020; Wei et al., 2022), so we tested this method on ISTUDIO, using GPT-3.5. In all conditions in Table 3, showing total accuracy and accuracies per question, the prompting included the context, the question, the class labels, and a student answer for the entire test set (Appendix D includes details). Results in the first row (-Ref, -Rub) are from these inputs alone (similar to zero-shot). In the second row, one correct reference answer and the rubric for the correct class were added (+1RefC, +RubC). In the third row condition, two reference examples for each class, and the rubric for each class were provided (+2Re-\n4Less fully connected configurations performed worse.\nfAll, +RubAll). GPT-3.5 improves in each next condition, and for question 4.b, comes close to the LM performance. Interestingly, the relative performance by question for GPT-3.5 is quite different from AsRRN, and both are quite different from the relative difficulty for students.\nTable 2 above shows that ablation of CL degrades performance. We conducted multiple ablations on ISTUDIO, and investigated the impact of replacing CL with a regularization term. We tested the contribution of each AsRRN layer, finding that all layers contribute significantly to the overall performance (results not shown due to space constraints). We also tested different numbers of reference answers and found that four worked best. The four reference answers for each of the 6 questions were chosen by the four co-authors (2 NLP researchers, 2 researchers in statistics education) to ensure they were distinct from each other, and covered many training examples. Figure 3 shows the mean accuracy with different values for the temporal recurrence, and different graph structures. The blue line (circles) for AsRRN shows that learning continues to improve over the first two time steps, then degrades. The red line (triangles) for a fully connected RRN is worse at all but the first time step. The yellow line (boxes), labeled random, represents that in each of the 8 parallelized CI runs, for all pairs of nodes i, j (cf. equation 1), a random decision was made at the beginning of training whether to include message passing; a full exploration of all combinations of message passing\nwould have been unnecessarily time-consuming. This condition has lower performance than AsRRN across all but the last time step. Finally, we examined different values of \u03bb in the loss function, and compared to L1 or L2 regularization. For all values of \u03bb < 1, performance is better than for \u03bb = 1 (see Appendix C). For \u03bb = 1, accuracy is still reasonable, because LCL implicitly includes the ground truth class. Replacing LCL with an L1 or L2 regularizer severely degrades performance. Results in Tables 2-3 use 4 reference answers per class, 2 time steps, and \u03bb=0.01.\nEvidence suggests that timely and specific feedback helps students learn (Pearl et al., 2012; Garfield et al., 2008). For formative assessment feedback, instructor effort is best spent on partially correct (PC) answers, where there is an opportunity to scaffold students towards more complete understanding. Incorrect answers are far more diverse, and students may be more confused about the reasoning steps and less likely to integrate feedback into their thinking. To assess the results on whether AsRRN groups student responses with the same reference answers human would, we find mixed results when we compare two humans and AsRRN on the grouping of partially correct answers. Recall that there were four reference answers for all three correctness classes, for all six questions. For the\npartially correct class, we selected four reference answers that were distinct from one another. Here we report on how consistent two experts (A and B) were with each other, and each with AsRRN, in pairing a student answer with the same reference answer for questions 2b and 4b; these are two of the three most challenging questions, based on the average student scores. We used QWK to report agreement of the three pairs. For both questions, A and B worked independently to pick one of the four reference answers as equivalent to each students\u2019 partially correct answer, or \"none of the above.\" For the subset of accurate partially correct predictions from AsRRN (90 out of 107 for 2b; 100 out of 110 for 4b), we show QWK for the expert pair, and each expert with AsRRN. Cases where AsRRN\u2019s maximum probability class had p < 0.85 were treated as \"none of the above\" based on locations of sharp decreases of output probabilities in scatterplots.\nAs shown by the QWK scores in Table 4, the two experts had good agreement of 0.72 on question 2b, which is consistent with the pattern shown in the coincidence matrix in Figure 1. They had very poor agreement on 4b, however, which shows that this task is difficult even for experts. On 2b, AsRRN agreement with B of 0.69 was close to that for A and B on the partially correct reponses that AsRRN accurately predicted (0.71), but agreement with A was much lower (0.53). On 4b, agreement between A and B on AsRRN\u2019s accurate subset was 0.45, and here AsRRN had similar agreement with B of 0.41, then very high agreement with A of 0.70. Instructors report that it is unpredictable with any given set of students what kinds of partially correct answers will occur frequently, as it depends on many unknowns, such as the students\u2019 prior background. Using contrastive learning in a novel way, we have taken a first step towards partially automating this difficult but pedagogically important task.\nThe high accuracy of AsRRN, and its potentially useful association of student answers with a specific reference answer, argues for the quality of the representation learning. Appendix B includes t-SNE visualizations of the learned representations of the test set with and without contrastive learning."
        },
        {
            "heading": "6 Conclusion",
            "text": "AsRRN is a novel recurrent relational network where the nodes in the neural graph each correspond to a distinct type of textual input, a super-\nnode represents global information across all nodes, and the answer state at the final time step becomes the input to the final classification layer. The graph structure (direction of message passing), leads to efficient learning of relational information that supports multi-step problems. AsRRN can have more or fewer layers, depending on the number of distinct types of text input in the dataset. A novel loss function for contrastive learning improves classification accuracy, and provides a step towards feedback for instructors and students on partially correct responses, by learning which of multiple reference answers a response is most similar to. A thorough set of ablations provides evidence for the chosen dependency structure, number of time steps in the temporal recurrence, and the contrastive learning. On ISTUDIO, AsRRN significantly outperforms strong baselines."
        },
        {
            "heading": "7 Limitations",
            "text": "The AsRRN model relies heavily on manual graph design, based on the relational structure of the CR dataset, and on the quality of reference answers. While this limits generalizability, it also incorporates expert domain knowledge in useful ways. Additionally, while AsRRN provides a preliminary foundation for feedback, understanding of optimal feedback across varying assessment contexts remains limited. Although the importance of timely and relevant feedback for students is wellestablished, little is known about the specific forms of feedback that work best in different contexts, or how feedback varies across instructors. While AsRRN can point to a specific reference answer for partially correct student answers, the task definition and AsRRN performance need much improvement.\nThe ISTUDIO dataset has relatively few question prompts (N=6) and is limited to a single subject domain, in contrast to benchmark datasets that have dozens of prompts and many domains. On the other hand, ISTUDIO is derived from a well-validated, reliable instrument and was labeled through an extensive inter-rater agreement study."
        },
        {
            "heading": "8 Acknowledgements",
            "text": "This work was supported under NSF DRK award 2010351 and NSF IUSE award 2236150."
        },
        {
            "heading": "A Training Parameters",
            "text": "We employed the AdamW optimizer for model training, with parameters set to learning_rate=1e5 and weight_decay=0.01, and clipped the global norm of the gradient at 1.0. The state vector of the graph nodes and the message passing vector have hidden dimensions of 768 and 128, respectively. The weight of the CL loss (\u03bb) is set to 0.01, and the temperature (\u03c4 ) of CL loss is 1. To curb overfitting, a 20% dropout rate (hidden_dropout_prob=0.2) is applied to the hidden layers. The maximum token length for each text data input is set to 64 in BEETLE dataset, 128 in SciEntsBank and ASAP datasets, and 256 in ISTUDIO.\nWe use warm-up prior to training with no CL loss during which the learning rate incrementally increases from a negligible value to the pre-set learning rate. The get_linear_schedule_with_warmup\nfunction from the Hugging Face Transformers library generates a schedule with a learning rate that linearly decreases following a warmup period of linear increase. In our experiment, the number of warmup steps is configured to be 5% of the total training steps.\nAll models were trained on a server equipped with eight NVIDIA RTX A5000 GPU cores, with the AsRRN model\u2019s training time ranging from two to five hours, depending on the training set size and number of epochs. If the paper is accepted, we will include a github link to the code repository."
        },
        {
            "heading": "B Contrastive Learning Visualization",
            "text": "T-SNE projections of the representations of the examples in the test set are shown in Figure 4 for the two conditions of omitting the contrastive loss term (4a) versus including it (4a). With the contrastive learning, the items cluster more tightly into fewer groups."
        },
        {
            "heading": "C Contrastive Loss versus Regularization",
            "text": "For values of \u03bb < 1, Figure 5 shows that the overall AsRRN accuracy is fairly stable. For \u03bb\u2212 1 accuracy drops to around 78%, which is still reasonable. This can be attributed to the diminished supervision of the classification loss, and the the fact that the contrastive learning loss employs ground truth labels during training, using the mean or max similarity to reference examples in the ground truth class."
        },
        {
            "heading": "D GPT 3.5 Prompting",
            "text": "GPT 3.5 is a powerful model for generating human-like text. In our experiment, we leveraged GPT 3.5 model with a series of \"prompts\" based on the ISTUDIO materials (containing Context (C), Question (Q), Reference answer (R), and Answer\n(A)). We articulated these prompts using a chainof-thought strategy. Our GPT-3.5 model was implemented using the ChatCompletion API provided by OpenAI, specifically using the gpt-3.5-turbo model with a 0.7 model temperature.\nAs exemplified by [2a] in Figure 6, we designed a zero-shot learning prompt template for GPT-3.5 (-Ref, -Rub) displayed in Table 3. The model was supplied with a series of \"messages\" to generate classification results. Each message consisted of a role (either \u2019system\u2019, \u2019user\u2019, or \u2019assistant\u2019) and the message content associated with that role. In one or few-shot learning scenarios (Figure 7) where reference answers were added by the \u2019user\u2019, we provided historical examples to the input. For instance, the one-shot (+1RefC, +RubC) included a correct reference answer as an example in the prompt as shown in 7a. In the case of few-shot prompts (+2RefAll, +RubAll), we used six reference answers (two for each class) as depicted in 7b. However, due to space limitations, only three examples are shown. Finally, we ensured the model\u2019s output always included a correctness label followed by an explanation (all marked in red). This chainof-thought prompting method not only enabled improved results but also laid the foundation for future research in feedback generation."
        }
    ],
    "title": "Answer-state Recurrent Relational Network (AsRRN) for Constructed Response Assessment and Feedback Grouping",
    "year": 2023
}