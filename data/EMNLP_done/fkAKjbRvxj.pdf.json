{
    "abstractText": "We present information value, a measure which quantifies the predictability of an utterance relative to a set of plausible alternatives. We introduce a method to obtain interpretable estimates of information value using neural text generators, and exploit their psychometric predictive power to investigate the dimensions of predictability that drive human comprehension behaviour. Information value is a stronger predictor of utterance acceptability in written and spoken dialogue than aggregates of token-level surprisal and it is complementary to surprisal for predicting eye-tracked reading times.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Mario Giulianelli"
        },
        {
            "affiliations": [],
            "name": "Sarenne Wallbridge"
        },
        {
            "affiliations": [],
            "name": "Raquel Fern\u00e1ndez"
        }
    ],
    "id": "SP:6aff71d37e12eb05c5f87a55ed44fb80762ed070",
    "references": [
        {
            "authors": [
                "Simona Amenta",
                "Jana Hasen\u00e4cker",
                "Davide Crepaldi",
                "Marco Marelli."
            ],
            "title": "Prediction at the intersection of sentence context and word form: Evidence from eyemovements and self-paced reading",
            "venue": "Psychonomic Bulletin & Review.",
            "year": 2022
        },
        {
            "authors": [
                "Suhas Arehalli",
                "Brian Dillon",
                "Tal Linzen."
            ],
            "title": "Syntactic surprisal from neural models predicts, but underestimates, human processing difficulty from syntactic ambiguities",
            "venue": "Proceedings of the 26th Conference on Computational Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Kristijan Armeni",
                "Roel M. Willems",
                "Stefan L. Frank."
            ],
            "title": "Probabilistic language models in cognitive neuroscience: Promises and pitfalls",
            "venue": "Neuroscience & Biobehavioral Reviews, 83:579\u2013588.",
            "year": 2017
        },
        {
            "authors": [
                "John Langshaw Austin."
            ],
            "title": "How to do things with words",
            "venue": "Clarendon Press, Oxford.",
            "year": 1975
        },
        {
            "authors": [
                "David I Beaver",
                "Brady Z Clark."
            ],
            "title": "Sense and sensitivity: How focus determines meaning",
            "venue": "John Wiley & Sons.",
            "year": 2009
        },
        {
            "authors": [
                "Jean-Philippe Bernardy",
                "Shalom Lappin",
                "Jey Han Lau."
            ],
            "title": "The influence of context on sentence acceptability judgements",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 456\u2013461,",
            "year": 2018
        },
        {
            "authors": [
                "Christopher M. Bishop."
            ],
            "title": "Pattern Recognition and Machine Learning (Information Science and Statistics)",
            "venue": "Springer-Verlag, Berlin, Heidelberg.",
            "year": 2006
        },
        {
            "authors": [
                "Sid Black",
                "Gao Leo",
                "Phil Wang",
                "Connor Leahy",
                "Stella Biderman"
            ],
            "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow",
            "year": 2021
        },
        {
            "authors": [
                "Brian Buccola",
                "Manuel Kri\u017e",
                "Emmanuel Chemla."
            ],
            "title": "Conceptual alternatives: Competition in language and beyond",
            "venue": "Linguistics and Philosophy, 45(2):265\u2013291.",
            "year": 2022
        },
        {
            "authors": [
                "Robyn Carston."
            ],
            "title": "Informativeness, relevance and scalar implicature",
            "venue": "Pragmatics And Beyond New Series, pages 179\u2013238.",
            "year": 1998
        },
        {
            "authors": [
                "Sihan Chen",
                "Sarah Nathaniel",
                "Rachel Ryskin",
                "Edward Gibson."
            ],
            "title": "The effect of context on noisy-channel sentence comprehension",
            "venue": "Cognition, 238:105503.",
            "year": 2023
        },
        {
            "authors": [
                "Ivano Ciardelli",
                "Jeroen Groenendijk",
                "Floris Roelofsen."
            ],
            "title": "Inquisitive semantics",
            "venue": "Oxford University Press.",
            "year": 2018
        },
        {
            "authors": [
                "Forrest Davis",
                "Marten van Schijndel."
            ],
            "title": "Discourse structure interacts with reference but not syntax in neural language models",
            "venue": "Proceedings of the 24th Conference on Computational Natural Language Learning, pages 396\u2013407.",
            "year": 2020
        },
        {
            "authors": [
                "Andrea de Varda",
                "Marco Marelli."
            ],
            "title": "The effects of surprisal across languages: Results from native and non-native reading",
            "venue": "Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022, pages 138\u2013144. Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Judith Degen",
                "Michael K Tanenhaus."
            ],
            "title": "Processing scalar implicature: A constraint-based approach",
            "venue": "Cognitive science, 39(4):667\u2013710.",
            "year": 2015
        },
        {
            "authors": [
                "Judith Degen",
                "Michael K Tanenhaus."
            ],
            "title": "Availability of alternatives and the processing of scalar implicatures: A visual world eye-tracking study",
            "venue": "Cognitive science, 40(1):172\u2013201.",
            "year": 2016
        },
        {
            "authors": [
                "Vera Demberg",
                "Frank Keller."
            ],
            "title": "Data from eyetracking corpora as evidence for theories of syntactic processing complexity",
            "venue": "Cognition, 109(2):193\u2013210.",
            "year": 2008
        },
        {
            "authors": [
                "Gabriel Doyle",
                "Michael Frank."
            ],
            "title": "Shared common ground influences information density in microblog texts",
            "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2015
        },
        {
            "authors": [
                "Bryan Eikema",
                "Wilker Aziz."
            ],
            "title": "Sampling-based approximations to minimum Bayes risk decoding for neural machine translation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10978\u201310993, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Christiane Fellbaum."
            ],
            "title": "Wordnet",
            "venue": "Theory and applications of ontology: Computer applications, pages 231\u2013243. Springer.",
            "year": 2010
        },
        {
            "authors": [
                "Danny Fox",
                "Roni Katzir."
            ],
            "title": "On the characterization of alternatives",
            "venue": "Natural language semantics, 19:87\u2013107.",
            "year": 2011
        },
        {
            "authors": [
                "Stefan L Frank",
                "Leun J Otten",
                "Giulia Galli",
                "Gabriella Vigliocco."
            ],
            "title": "The ERP response to the amount of information conveyed by words in sentences",
            "venue": "Brain and Language, 140:1\u201311.",
            "year": 2015
        },
        {
            "authors": [
                "Richard Futrell",
                "Roger Levy."
            ],
            "title": "Noisy-context surprisal as a human sentence processing cost model",
            "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, long papers, pages 688\u2013698.",
            "year": 2017
        },
        {
            "authors": [
                "Richard Futrell",
                "Ethan Wilcox",
                "Takashi Morita",
                "Peng Qian",
                "Miguel Ballesteros",
                "Roger Levy."
            ],
            "title": "Neural language models as psycholinguistic subjects: Representations of syntactic state",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Gerald Gazdar."
            ],
            "title": "Pragmatics, implicature, presuposition and logical form",
            "venue": "Critica, 12(35).",
            "year": 1979
        },
        {
            "authors": [
                "Dmitriy Genzel",
                "Eugene Charniak."
            ],
            "title": "Entropy rate constancy in text",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 199\u2013206.",
            "year": 2002
        },
        {
            "authors": [
                "Edward Gibson",
                "James Thomas."
            ],
            "title": "Memory limitations and structural forgetting: The perception of complex ungrammatical sentences as grammatical",
            "venue": "Language and Cognitive Processes, 14:225\u2013248.",
            "year": 1999
        },
        {
            "authors": [
                "Mario Giulianelli."
            ],
            "title": "Towards pragmatic production strategies for natural language generation tasks",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7978\u20137984, Abu Dhabi, United Arab Emirates. As-",
            "year": 2022
        },
        {
            "authors": [
                "Mario Giulianelli",
                "Joris Baan",
                "Wilker Aziz",
                "Raquel Fern\u00e1ndez",
                "Barbara Plank."
            ],
            "title": "What comes next? Evaluating uncertainty in neural text generators against human production variability",
            "venue": "Proceedings of the 2023 Conference on Empirical Methods in",
            "year": 2023
        },
        {
            "authors": [
                "Mario Giulianelli",
                "Raquel Fern\u00e1ndez."
            ],
            "title": "Analysing human strategies of information transmission as a function of discourse context",
            "venue": "Proceedings of the 25th Conference on Computational Natural Language Learning, pages 647\u2013660, Online.",
            "year": 2021
        },
        {
            "authors": [
                "Mario Giulianelli",
                "Arabella Sinclair",
                "Raquel Fern\u00e1ndez"
            ],
            "title": "Is information density uniform in task-oriented dialogues",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "John J Godfrey",
                "Edward C Holliman",
                "Jane McDaniel."
            ],
            "title": "SWITCHBOARD: Telephone speech corpus for research and development",
            "venue": "Proceedings of the 1992 IEEE international conference on Acoustics, speech and signal processing-Volume 1,",
            "year": 1992
        },
        {
            "authors": [
                "Adam Goodkind",
                "Klinton Bicknell."
            ],
            "title": "Predictive power of word surprisal for reading times is a linear function of language model quality",
            "venue": "Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2018), pages 10\u201318,",
            "year": 2018
        },
        {
            "authors": [
                "Herbert P Grice."
            ],
            "title": "Logic and conversation",
            "venue": "Speech acts, pages 41\u201358. Brill.",
            "year": 1975
        },
        {
            "authors": [
                "Jeroen Antonius Gerardus Groenendijk",
                "Martin Johan Bastiaan Stokhof."
            ],
            "title": "Studies on the Semantics of Questions and the Pragmatics of Answers",
            "venue": "Ph.D. thesis, University of Amsterdam.",
            "year": 1984
        },
        {
            "authors": [
                "Nuno M. Guerreiro",
                "Elena Voita",
                "Andr\u00e9 Martins."
            ],
            "title": "Looking for a needle in a haystack: A comprehensive study of hallucinations in neural machine translation",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Com-",
            "year": 2023
        },
        {
            "authors": [
                "John Hale."
            ],
            "title": "A probabilistic Earley parser as a psycholinguistic model",
            "venue": "Second meeting of the North American Chapter of the Association for Computational Linguistics.",
            "year": 2001
        },
        {
            "authors": [
                "Charles L Hamblin."
            ],
            "title": "Questions in montague English",
            "venue": "Montague grammar, pages 247\u2013259. Elsevier.",
            "year": 1976
        },
        {
            "authors": [
                "Michael Heilman",
                "Aoife Cahill",
                "Nitin Madnani",
                "Melissa Lopez",
                "Matthew Mulholland",
                "Joel Tetreault."
            ],
            "title": "Predicting grammaticality on an ordinal scale",
            "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2:",
            "year": 2014
        },
        {
            "authors": [
                "Philip Hofmeister",
                "Laura Staum Casasanto",
                "Ivan A. Sag."
            ],
            "title": "Processing effects in linguistic judgment data: (Super-)additivity and reading span scores",
            "venue": "Language and Cognition, 6:111 \u2013 145.",
            "year": 2014
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Ari Holtzman",
                "Peter West",
                "Vered Shwartz",
                "Yejin Choi",
                "Luke Zettlemoyer."
            ],
            "title": "Surface form competition: Why the highest probability answer isn\u2019t always right",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process-",
            "year": 2021
        },
        {
            "authors": [
                "Matthew Honnibal",
                "Ines Montani",
                "Sofie Van Landeghem",
                "Adriane Boyd"
            ],
            "title": "spaCy: Industrialstrength natural language processing in Python",
            "year": 2020
        },
        {
            "authors": [
                "Laurence Robert Horn."
            ],
            "title": "On the semantic properties of logical operators in English",
            "venue": "University of California, Los Angeles.",
            "year": 1972
        },
        {
            "authors": [
                "Jennifer Hu",
                "Roger Levy",
                "Judith Degen",
                "Sebastian Schuster."
            ],
            "title": "Expectations over unspoken alternatives predict pragmatic inferences",
            "venue": "Transactions of the Association for Computational Linguistics. To appear.",
            "year": 2023
        },
        {
            "authors": [
                "Jennifer Hu",
                "Roger Levy",
                "Sebastian Schuster."
            ],
            "title": "Predicting scalar diversity with context-driven uncertainty over alternatives",
            "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 68\u201374, Dublin, Ireland. Associa-",
            "year": 2022
        },
        {
            "authors": [
                "T. Florian Jaeger",
                "Roger P. Levy."
            ],
            "title": "Speakers optimize information density through syntactic reduction",
            "venue": "Advances in neural information processing systems, pages 849\u2013856.",
            "year": 2007
        },
        {
            "authors": [
                "Roni Katzir."
            ],
            "title": "Structurally-defined alternatives",
            "venue": "Linguistics and philosophy, 30:669\u2013690.",
            "year": 2007
        },
        {
            "authors": [
                "Frank Keller."
            ],
            "title": "The entropy rate principle as a predictor of processing effort: An evaluation against eye-tracking data",
            "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 317\u2013324.",
            "year": 2004
        },
        {
            "authors": [
                "Dave F Kleinschmidt",
                "T Florian Jaeger."
            ],
            "title": "Robust speech perception: recognize the familiar, generalize to the similar, and adapt to the novel",
            "venue": "Psychological review, 122(2):148.",
            "year": 2015
        },
        {
            "authors": [
                "Daphne Koller",
                "Nir Friedman."
            ],
            "title": "Probabilistic graphical models: Principles and techniques",
            "venue": "MIT press.",
            "year": 2009
        },
        {
            "authors": [
                "Lorenz Kuhn",
                "Yarin Gal",
                "Sebastian Farquhar."
            ],
            "title": "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Jey Han Lau",
                "Alexander Clark",
                "Shalom Lappin."
            ],
            "title": "Unsupervised prediction of acceptability judgements",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natu-",
            "year": 2015
        },
        {
            "authors": [
                "Jey Han Lau",
                "Alexander Clark",
                "Shalom Lappin."
            ],
            "title": "Grammaticality, acceptability, and probability: A probabilistic view of linguistic knowledge",
            "venue": "Cognitive Science, 41(5):1202\u20131241.",
            "year": 2017
        },
        {
            "authors": [
                "Steve Lawrence",
                "C Lee Giles",
                "Sandiway Fong."
            ],
            "title": "Natural language grammatical inference with recurrent neural networks",
            "venue": "IEEE Transactions on Knowledge & Data Engineering, 12(01):126\u2013140.",
            "year": 2000
        },
        {
            "authors": [
                "Stephen C. Levinson."
            ],
            "title": "Presumptive Meanings: The Theory of Generalized Conversational Implicature",
            "venue": "The MIT Press.",
            "year": 2000
        },
        {
            "authors": [
                "Roger Levy."
            ],
            "title": "Expectation-based syntactic comprehension",
            "venue": "Cognition, 106(3):1126\u20131177.",
            "year": 2008
        },
        {
            "authors": [
                "Roger Levy",
                "T. Florian Jaeger."
            ],
            "title": "Speakers optimize information density through syntactic reduction",
            "venue": "Advances in Neural Information Processing Systems, volume 19. MIT Press.",
            "year": 2007
        },
        {
            "authors": [
                "Yanran Li",
                "Hui Su",
                "Xiaoyu Shen",
                "Wenjie Li",
                "Ziqiang Cao",
                "Shuzi Niu."
            ],
            "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Steven G. Luke",
                "Kiel Christianson."
            ],
            "title": "The Provo Corpus: A large eye-tracking corpus with predictability norms",
            "venue": "Behavior Research Methods, 50(2):826\u2013 833.",
            "year": 2018
        },
        {
            "authors": [
                "Scott A. McDonald",
                "Richard C. Shillcock."
            ],
            "title": "Low-level predictive inference in reading: the influence of transitional probabilities on eye movements",
            "venue": "Vision Research, 43(16):1735\u20131751.",
            "year": 2003
        },
        {
            "authors": [
                "Clara Meister",
                "Tiago Pimentel",
                "Patrick Haller",
                "Lena J\u00e4ger",
                "Ryan Cotterell",
                "Roger Levy."
            ],
            "title": "Revisiting the Uniform Information Density hypothesis",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 963\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Clara Meister",
                "Tiago Pimentel",
                "Gian Wiher",
                "Ryan Cotterell."
            ],
            "title": "Locally Typical Sampling",
            "venue": "Transactions of the Association for Computational Linguistics, 11:102\u2013121.",
            "year": 2023
        },
        {
            "authors": [
                "Byung-Doh Oh",
                "William Schuler"
            ],
            "title": "Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times? Transactions of the Association for Computational Linguistics, 11:336\u2013350",
            "year": 2023
        },
        {
            "authors": [
                "Tiago Pimentel",
                "Clara Meister",
                "Elizabeth Salesky",
                "Simone Teufel",
                "Dami\u00e1n Blasi",
                "Ryan Cotterell."
            ],
            "title": "A surprisal\u2013duration trade-off across and within the world\u2019s languages",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Mats Rooth."
            ],
            "title": "A theory of focus interpretation",
            "venue": "Natural language semantics, pages 75\u2013116.",
            "year": 1992
        },
        {
            "authors": [
                "Mats Rooth."
            ],
            "title": "Focus",
            "venue": "The handbook of contemporary semantic theory, ed. by Shalom Lappin.",
            "year": 1996
        },
        {
            "authors": [
                "Martin Schrimpf",
                "Idan Asher Blank",
                "Greta Tuckute",
                "Carina Kauf",
                "Eghbal A. Hosseini",
                "Nancy Kanwisher",
                "Joshua B. Tenenbaum",
                "Evelina Fedorenko."
            ],
            "title": "The neural architecture of language: Integrative modeling converges on predictive processing",
            "venue": "Proceed-",
            "year": 2021
        },
        {
            "authors": [
                "John R Searle."
            ],
            "title": "Speech acts: An essay in the philosophy of language, volume 626",
            "venue": "Cambridge University Press.",
            "year": 1969
        },
        {
            "authors": [
                "John R Searle."
            ],
            "title": "A taxonomy of illocutionary acts",
            "venue": "Language Mind, and Knowledge, 7.",
            "year": 1975
        },
        {
            "authors": [
                "Cory Shain",
                "Clara Meister",
                "Tiago Pimentel",
                "Ryan Cotterell",
                "Roger Philip Levy"
            ],
            "title": "Large-scale evidence for logarithmic effects of word predictability on reading time",
            "year": 2022
        },
        {
            "authors": [
                "Claude E. Shannon."
            ],
            "title": "A mathematical theory of communication",
            "venue": "The Bell System Technical Journal, 27(3):379\u2013423.",
            "year": 1948
        },
        {
            "authors": [
                "Arabella Sinclair",
                "Jaap Jumelet",
                "Willem Zuidema",
                "Raquel Fern\u00e1ndez."
            ],
            "title": "Structural persistence in language models: Priming as a window into abstract language representations",
            "venue": "Transactions of the Association for Computational Linguistics, 10:1031\u20131050.",
            "year": 2022
        },
        {
            "authors": [
                "Nathaniel J. Smith",
                "Roger Levy."
            ],
            "title": "The effect of word predictability on reading time is logarithmic",
            "venue": "Cognition, 128(3):302\u2013319.",
            "year": 2013
        },
        {
            "authors": [
                "Harold Somers"
            ],
            "title": "Round-trip translation: What is it good for",
            "venue": "In Proceedings of the Australasian Language Technology Workshop",
            "year": 2005
        },
        {
            "authors": [
                "Antonella Sorace",
                "Frank Keller."
            ],
            "title": "Gradience in linguistic data",
            "venue": "Lingua, 115(11):1497\u20131524.",
            "year": 2005
        },
        {
            "authors": [
                "Robert C Stalnaker."
            ],
            "title": "Assertion",
            "venue": "Pragmatics, pages 315\u2013332. Brill.",
            "year": 1978
        },
        {
            "authors": [
                "Katherine Stasaski",
                "Marti A Hearst."
            ],
            "title": "Pragmatically appropriate diversity for dialogue evaluation",
            "venue": "arXiv preprint arXiv:2304.02812.",
            "year": 2023
        },
        {
            "authors": [
                "Antoine Tremblay",
                "Bruce Derwing",
                "Gary Libben",
                "Chris Westbury."
            ],
            "title": "Processing advantages of lexical bundles: Evidence from self-paced reading and sentence recall tasks",
            "venue": "Language learning, 61(2):569\u2013 613.",
            "year": 2011
        },
        {
            "authors": [
                "Geoffrey Underwood",
                "Norbert Schmitt",
                "Adam Galpin."
            ],
            "title": "The eyes have it: An eye movement study into the processing of formulaic sequences",
            "venue": "Norbert Schmitt, editor, Formulaic Sequences: Acquisition, Processing and Use, pages 153\u2013172. John",
            "year": 2004
        },
        {
            "authors": [
                "Bob Van Tiel",
                "Emiel Van Miltenburg",
                "Natalia Zevakhina",
                "Bart Geurts."
            ],
            "title": "Scalar Diversity",
            "venue": "Journal of Semantics, 33(1):137\u2013175.",
            "year": 2014
        },
        {
            "authors": [
                "Vivek Verma",
                "Nicholas Tomlin",
                "Dan Klein."
            ],
            "title": "Revisiting entropy rate constancy in text",
            "venue": "arXiv preprint arXiv:2305.12084.",
            "year": 2023
        },
        {
            "authors": [
                "Michael Wagner"
            ],
            "title": "Prosody and recursion",
            "venue": "Ph.D. thesis, Massachusetts Institute of Technology.",
            "year": 2005
        },
        {
            "authors": [
                "Sarenne Wallbridge",
                "Peter Bell",
                "Catherine Lai."
            ],
            "title": "Investigating perception of spoken dialogue acceptability through surprisal",
            "venue": "Interspeech 2022: The 23rd Annual Conference of the International Speech Communication Association, pages 4506\u20134510. In-",
            "year": 2022
        },
        {
            "authors": [
                "Sarenne Wallbridge",
                "Peter Bell",
                "Catherine Lai."
            ],
            "title": "Do dialogue representations align with perception? an empirical study",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2696\u20132713,",
            "year": 2023
        },
        {
            "authors": [
                "Alex Warstadt",
                "Amanpreet Singh",
                "Samuel R. Bowman."
            ],
            "title": "Neural network acceptability judgments",
            "venue": "Transactions of the Association for Computational Linguistics, 7:625\u2013641.",
            "year": 2019
        },
        {
            "authors": [
                "Jason Wei",
                "Clara Meister",
                "Ryan Cotterell."
            ],
            "title": "A cognitive regularizer for language modeling",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Ethan Gotlieb Wilcox",
                "Jon Gauthier",
                "Jennifer Hu",
                "Peng Qian",
                "Roger Levy"
            ],
            "title": "On the predictive power",
            "year": 2020
        },
        {
            "authors": [
                "Roel M Willems",
                "Stefan L Frank",
                "Annabel D Nijhof",
                "Peter Hagoort",
                "Antal Van den Bosch."
            ],
            "title": "Prediction during natural language comprehension",
            "venue": "Cerebral Cortex, 26(6):2506\u20132516.",
            "year": 2016
        },
        {
            "authors": [
                "Yang Xu",
                "David Reitter."
            ],
            "title": "Information density converges in dialogue: Towards an informationtheoretic model",
            "venue": "Cognition, 170:147\u2013163.",
            "year": 2018
        },
        {
            "authors": [
                "Shaorong Yan",
                "T Florian Jaeger."
            ],
            "title": "Expectation adaptation during natural reading",
            "venue": "Language, Cognition and Neuroscience, 35(10):1394\u20131422.",
            "year": 2020
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068",
            "year": 2022
        },
        {
            "authors": [
                "Yizhe Zhang",
                "Siqi Sun",
                "Michel Galley",
                "Yen-Chun Chen",
                "Chris Brockett",
                "Xiang Gao",
                "Jianfeng Gao",
                "Jingjing Liu",
                "William B Dolan."
            ],
            "title": "DIALOGPT: Largescale generative pre-training for conversational response generation",
            "venue": "Proceedings of the 58th An-",
            "year": 2020
        },
        {
            "authors": [
                "Zheng Zhang",
                "Leon Bergen",
                "Alexander Paunov",
                "Rachel Ryskin",
                "Edward Gibson."
            ],
            "title": "Scalar implicature is sensitive to contextual alternatives",
            "venue": "Cognitive science, 47(2):e13238.",
            "year": 2023
        },
        {
            "authors": [
                "times. Meister"
            ],
            "title": "2021) report similar trends when including summed unigram log probability or sentence length as baseline predictors of acceptability judgements, and word character lengths or word unigram log probabilities for reading",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "When viewed as information transmission, successful language production can be seen as an act of reducing the uncertainty over future states that a comprehender may be anticipating. Saying a word, for example, may cut the space of possibilities in half, while uttering a whole sentence may restrict the comprehender\u2019s expectations to a far smaller space. Measuring the amount of information carried by a linguistic signal is fundamental to the computational modelling of human language processing. Such quantifications are used in psycholinguistic and neurobiological models of language processing (Levy, 2008; Willems et al., 2016; Futrell and Levy, 2017; Armeni et al., 2017), to study the processing mechanisms of neural language models (Futrell et al., 2019; Davis and van Schijndel, 2020; Sinclair et al., 2022), and as a learning and evaluation criterion for language modelling (under the guise of \u2018cross-entropy loss\u2019 or \u2018perplexity\u2019). The amount of information carried by a linguistic signal is intrinsically related to its predictability (Hale, 2001; Genzel and Charniak, 2002; Jaeger and Levy, 2007). This connection is summarised in the definition of the surprisal, or\n\u2217Shared first authorship. 1https://github.com/dmg-illc/information-value\ninformation content, of a unit u (Shannon, 1948), perhaps the most widely used measure of information: I(u) =\u2212 log2 p(u). Predictable units carry low amounts of information\u2014i.e., low surprisal\u2014 as they are already expected to occur given the context in which they are produced. Conversely, unexpected units carry higher surprisal.\nProper estimation of the surprisal of an utterance is intractable, as it would require computing probabilities over a high-dimensional, structured, and ultimately unbounded event space. It is thus common to resort to chaining token-level surprisal estimates, nowadays typically obtained from neural language models (Meister et al., 2021; Giulianelli and Fern\u00e1ndez, 2021; Wallbridge et al., 2022). However, token-level autoregressive approximations of utterance probability have a few problematic properties. A well-known issue is that different realisations of the same concept or communicative intent compete for probability mass (Holtzman et al., 2021), which implies that the surprisal of semantically equivalent realisations is overestimated. Moreover, token-level surprisal estimates conflate different dimensions of predictability. As evidenced by recent studies (Arehalli et al., 2022; Kuhn et al., 2023), this makes it difficult to appreciate whether the information carried by an utterance is a result, for example, of the unexpectedness of its lexical material, syntactic arrangements, semantic content, or speech act type.\nWe propose an intuitive characterisation of the information carried by utterances, information value, which computes predictability over the space of full utterances to account for potential communicative equivalence, and explicitly models multiple dimensions of predictability (e.g., lexical, syntactic, and semantic), thereby offering greater interpretability of predictability estimates. Given a linguistic context, the information value of an utterance is a function of its distance from the set of contextually expected alternatives. The intuition is\nthat if an utterance differs largely from alternative productions, it is an unexpected contribution to discourse with high information value (see Figure 1). We obtain empirical estimates of information value by sampling alternatives from neural text generators and measuring their distance from a target utterance using interpretable distance metrics. Information value estimates are evaluated in terms of their ability to predict and explain human reading times and acceptability judgements in dialogue and text.\nWe find information value to have stronger psychometric predictive power than aggregates of token-level surprisal for acceptability judgements in spoken and written dialogue, and to be complementary to surprisal aggregates as a predictor of reading times. Furthermore, we use our interpretable measure of predictability to gather insights into the processing mechanisms underlying human comprehension behaviour. Our analysis reveals, for example, that utterance acceptability in dialogue is largely determined by semantic expectations while reading times are more affected by lexical and syntactic predictions.\nInformation value is a new way to measure predictability. As such, next to surprisal, it is a powerful tool for the analysis of comprehension behaviour (Meister et al., 2021; Shain et al., 2022; Wallbridge et al., 2022, 2023), for the computational modelling of language production strategies (Doyle and Frank, 2015; Xu and Reitter, 2018;\nVerma et al., 2023) and for the design of processing and decision-making mechanisms that reproduce them in natural language generation systems (Wei et al., 2021; Giulianelli, 2022; Meister et al., 2023)."
        },
        {
            "heading": "2 Background",
            "text": "Surprisal theory. Expectation-based theories of language processing define the effort required to process a linguistic unit as a function of its predictability. Surprisal theory, perhaps the most prominent example, posits a direct relationship between effort and predictability, quantified as surprisal (Hale, 2001). The theory is supported by broad empirical evidence across domains and languages (Pimentel et al., 2021; de Varda and Marelli, 2022), and serves as a foundation for quantitative principles of language production and comprehension such as Entropy Rate Constancy (ERC; Genzel and Charniak, 2002) and Uniform Information Density (UID; Levy and Jaeger, 2007).\nThe psychometric predictive power of surprisal. Without direct access to the \u2018true\u20192 conditional probabilities of linguistic units, psycholinguists have relied on statistical models of language to estimate surprisal (Hale, 2001; McDonald and Shillcock, 2003). More recently, large-scale language models have emerged as powerful estimators of token-level surprisal, reflected by their ability to predict different aspects of human language comprehension behaviour (their psychometric predictive power). Psychometric variables include self-paced and eye-tracked reading times (Keller, 2004; Goodkind and Bicknell, 2018; Wilcox et al., 2020; Meister et al., 2021; Shain et al., 2022; Oh and Schuler, 2023), acceptability judgements (Lawrence et al., 2000; Heilman et al., 2014; Lau et al., 2015, 2017; Warstadt et al., 2019; Wallbridge et al., 2022), and brain response data (Frank et al., 2015; Schrimpf et al., 2021).\nTo obtain estimates of utterance surprisal, different aggregates of token-level surprisal have been proposed, motivated by psycholinguistic theories like ERC and UID. However, their behaviour is far less understood (e.g., Wallbridge et al., 2022). For example, divergences between how model characteristics affect predictive power for different comprehension tasks (e.g., Meister et al., 2021) raise questions about whether token-level\n2In this context, true refers to the\u2014if at all existing\u2014 unattainable conditional probabilities of linguistic units that a human may experience during language comprehension.\naggregates appropriately capture expectations over utterances in human language processing.\nAlternatives in semantics and pragmatics. Our proposed notion of information value takes inspiration from the concept of alternatives in semantics and pragmatics (Horn, 1972; Grice, 1975; Stalnaker, 1978; Gazdar, 1979; Rooth, 1996; Levinson, 2000). Reasoning about alternatives has been argued to be at the basis of the use of questions (Hamblin, 1976; Groenendijk and Stokhof, 1984; Ciardelli et al., 2018), focus (Rooth, 1992; Wagner et al., 2005; Beaver and Clark, 2009), and implicatures (Carston, 1998; Degen and Tanenhaus, 2015, 2016; Zhang et al., 2023). Recently, alternative sets generated with the aid of language models have been used to provide empirical evidence that pragmatic inferences of scalar implicature depend on listeners\u2019 context-driven uncertainty over alternatives (Hu et al., 2022, 2023). Hu et al. (2022) generate sets of plausible words in context, within scalar constructions, then embed and cluster the resulting sentences to simulate conceptual alternatives. Reasoning over word- and concept-level alternatives is operationalised through surprisal and entropy. To our knowledge, ours is the first study to use language models for the generation of full utterance-level alternatives."
        },
        {
            "heading": "3 Alternative-Based Information Value",
            "text": "Given a context x, a speaker may produce a number of plausible utterances. We refer to these as Ax, the alternative set. We define the information value of an utterance y in a context x as the real random variable which captures the distribution of distances between y and the set of alternative productions Ax, measured with a distance metric d:\nI(Y =y|X=x) := d(y,Ax) (1)\nThis distribution characterises the predictability of y in its context. Large distances indicate that y differs substantially from expected utterances, and thus that y is a surprising next utterance."
        },
        {
            "heading": "3.1 Computing Information Value",
            "text": "In Equation 1, we define information value as a statistical measure of the unpredictability, or unexpectedness of an utterance. In practice, computing the information value of an utterance requires (1) a method for obtaining alternative sets Ax, (2) a metric with which to measure the\ndistance of an utterance from its alternatives, and (3) a means with which to summarise distributions of pairwise distances. We discuss these three elements in turn in the following paragraphs.\nGenerating alternative sets. Since the \u2018true\u2019 alternative sets entertained by a human comprehender are not attainable, we propose generating them algorithmically, via neural text generators. Being able to guarantee the plausibility, or humanlikeness of the generations is crucial. Our approach builds on recent work (Giulianelli et al., 2023) finding the predictive distribution of neural text generators to be well aligned to human variability, as measured with the same distance metrics used in this paper (see next paragraph): while not all generations are guaranteed to be of high quality, their low-dimensional statistical properties (e.g., n-gram, POS, and speech act distribution) match those of human productions. This should allow us to obtain faithful distance distributions d(y,Ax) and thus accurate estimates of information value.\nMeasuring distance from alternatives. We quantify the distance of a target utterance from an alternative production using three interpretable distance metrics, as defined by Giulianelli et al. (2023). Lexical: Fraction of distinct n-grams in two utterances, with n \u2208 [1, 2, 3] (i.e., the number of distinct n-gram occurrences divided by the total number of n-grams in both utterances). Syntactic: Fraction of distinct part-of-speech (POS) n-grams in two utterances. Semantic: Cosine and euclidean distance between the sentence embeddings of two utterances (Reimers and Gurevych, 2019). These distance metrics characterise alternative sets at varying levels of abstraction (Katzir, 2007; Fox and Katzir, 2011; Buccola et al., 2022), enabling an exploration into the representational form of expectations over alternatives in human language processing.\nSummarising distance distributions. Information value is a random variable that describes a distribution over distances between an utterance y and the set of plausible alternatives (Equation 1). To summarise this distribution, we explore mean as the expected distance (under a uniform distribution over alternatives) or as the distance from a prototypical alternative, and min as the distance of y from the closest alternative production, implicating that proximity to a single alternative is sufficient to determine predictability."
        },
        {
            "heading": "4 Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1 Language Models",
            "text": "We generate alternative sets using neural autoregressive language models (LMs). For the dialogue corpora, we use GPT-2 (Radford et al., 2019), DialoGPT (Zhang et al., 2020), and GPT-Neo (Black et al., 2021). For the text corpora, we use GPT-2, GPT-Neo, and OPT (Zhang et al., 2022). The text models are pre-trained, while dialogue models are fine-tuned on the respective datasets. Further details on fine-tuning and perplexity scores are in Appendix A. The resulting dataset, which contains 1.3M generations, is publicly available.3\nGenerating alternatives. To generate an alternative set Ax, we sample from pLM (Y |X=x). We experiment with four popular sampling algorithms to ensure that the quality of our information value estimates is not dependent on a particular algorithm\u2014or, if it is, that we are not overlooking it. We select (1) unbiased (ancestral or forward) sampling (Bishop, 2006; Koller and Friedman, 2009), (2) temperature sampling (\u03b1 \u2208 [0.75, 1.25]), (3) nucleus sampling (Holtzman et al., 2019) (p \u2208 [0.8, 0.85, 0.9, 0.95]), and (4) locally typical sampling (Meister et al., 2023) (\u03c4 \u2208 [0.2, 0.3, 0.85, 0.95]), for a total of 11 sampling strategies. We post-process alternatives to ensure that each contains only a single utterance.4"
        },
        {
            "heading": "4.2 Psychometric Data",
            "text": "Using five corpora, we study two main types of psychometric variables that rely on different underlying processing mechanisms (Gibson and Thomas, 1999; Hofmeister et al., 2014): acceptability judgements and reading times."
        },
        {
            "heading": "4.2.1 Acceptability judgements",
            "text": "Stimuli for acceptability judgements typically consist of isolated sentences that are manipulated automatically or by hand to assess a grammatical notion of acceptability (Lau et al., 2017; Warstadt et al., 2019). The effect of context on acceptability is still relatively underexplored, yet contextualised judgements arguably capture a more natural, intuitive notion of acceptability. In this study, we use some of the few datasets of in-context acceptability\n3AltGen: https://doi.org/10.5281/zenodo.10006413. 4We use spaCy\u2019s sentence segmentation algorithm (Honnibal et al., 2020) for the text corpora and split dialogue utterances based on the position of the turn separator.\njudgements which examine grammaticality as well as semantic and pragmatic plausibility.\nSWITCHBOARD and DAILYDIALOG. Participants were presented with a short sequence of dialogue turns followed by a potential upcoming turn, and asked to rate its plausibility in context on a scale from 1 to 5. Judgements were collected by Wallbridge et al. (2022) for (transcribed) spoken dialogue and written dialogue from the Switchboard Telephone Corpus (Godfrey et al., 1992) and DailyDialog (Li et al., 2017), respectively. For each corpus, 100 items are annotated by 3-6 participants. Annotation items consist of 10 dialogue contexts, each followed by the true next turn and by 9 turns randomly sampled from the respective corpus.5\nCLASP. Participants were presented with sentences from the English Wikipedia in and out of their document context and asked to judge acceptability using a 4-point scale (Bernardy et al., 2018). The original sentences are round-trip translated into 4 languages to obtain varying degrees of acceptability; the context is not modified. This dataset contains 500 stimuli, annotated by 20 participants.6"
        },
        {
            "heading": "4.2.2 Reading times",
            "text": "Previous literature regarding the predictive power of language models for reading behaviour has focused on the relationship between per-word surprisal and reading times (Keller, 2004; Wilcox et al., 2020; Shain et al., 2022; Oh and Schuler, 2023). We define utterance-level reading time as the total time spent reading the constituent words of the utterance. This approach has been taken by previous studies of utterance-level surprisal (Meister et al., 2021; Amenta et al., 2022).\nPROVO. This corpus consists of 136 sentences (55 paragraphs) of English text from a variety of genres. Eye movement data was collected from 84 native American English speakers (Luke and Christianson, 2018). We use the summation of wordlevel reading times (IA-DWELL-TIME, the total duration of all fixations on the target word) of constituent words to obtain utterance-level measures.\nBROWN. This corpus consists of self-paced moving-window reading times for 450 sentences (12 passages) from the Brown corpus of American\n5Acceptability ratings available at https://data.cstr. ed.ac.uk/sarenne/INTERSPEECH2022/.\n6We only use judgements collected in context, available at https://github.com/GU-CLASP/BLL2018.\nEnglish. Reading times were collected from 35 native English speakers (Smith and Levy, 2013)."
        },
        {
            "heading": "5 Psychometric Predictive Power",
            "text": "We begin by evaluating our empirical estimates of information value in terms of their psychometric predictive power: can they predict comprehension behaviour recorded as human acceptability judgements and reading times? We test the robustness of this predictive power to the alternative set generation process and compare it to previously proposed utterance-level surprisal aggregates including mean, variance, and a range of summation strategies; see Appendix B for full definitions.\nFor each corpus in Section 4.2, we measure the correlation between information value and the respective psychometric variable, which is the average in-context acceptability judgement for DAILYDIALOG, SWITCHBOARD, and CLASP, and the total utterance reading time normalised by utterance length for PROVO and BROWN.7 Alternative sets are generated using the language models and sampling strategies described in Section 4.1. Lexical, syntactic, and semantic distances are computed in terms of the distance metrics presented in Section 3.1, for alternative sets of varying size ([10, 20, ..., 100]). The distributions of similarities in Equation 1 are summarised using mean and min, thus yielding scalar estimates of information value.\n7We normalise by utterance length as it is an obvious correlate of total reading time and would have confounding effects on this analysis. In Section 6, we confirm our findings using mixed effect models that include utterance length as a predictor and total unnormalised reading time as a response variable."
        },
        {
            "heading": "5.1 Predictive Power",
            "text": "For every corpus, we obtain a moderate to strong Spearman correlation between information value and the psychometric target variable. For example, estimates of semantic information value correlate with acceptability judgements at strengths approximately between \u22120.4 and \u22120.7 for SWITCHBOARD and between \u22120.3 and \u22120.6 for DAILYDIALOG across models and sampling strategies from Section 4.1 (see Figure 2 for SWITCHBOARD; Appendix C for all datasets). Estimates obtained with the best information value estimators for each corpus, shown in Table 1, yield substantially higher correlations with acceptability in dialogue than the best token-level aggregates of utterance surprisal, both as computed in our experiments and as reported in prior work (Wallbridge et al., 2022, 2023). Reading times, on the other hand, which are aggregates of word-level psychometric data points, should naturally be easier to capture with word-level measures of predictability. Nevertheless, our best information value estimates correlate with reading times only slightly less strongly or comparably to surprisal; and additionally, they give us indications about the dimensions of unexpectedness (in this case, lexical and syntactic) that mostly affect reading behaviour.\nOverall, beyond building trust in our information value estimators, this evaluation demonstrates the benefit of their interpretability. The predictive power for lexical, syntactic, and semantic distances varies widely between corpora. Semantic distances are much more predictive for dialogue datasets than lexical or syntactic distances, while the inverse is true for the reading times datasets. We explore differences between the underlying perceptual processes employed for these two comprehension tasks further in Section 6."
        },
        {
            "heading": "5.2 Robustness to Estimator Parameters",
            "text": "We now study the extent to which our estimates are affected by variation in three important parameters of alternative set generation: the alternative set size ([10, 20, ..., 100]), the language model, and the sampling strategy. We find a slight positive, asymptotic relationship between predictive power, reflected by correlations between information value and psychometric data, and alternative set size for semantic information value in the dialogue corpora\u2014information value estimates become more predictive as alternative set size increases (see, e.g., Figure 2). Set size does not significantly affect correlations for the reading times corpora. Moreover, while we do observe differences between models, and larger models tend to obtain higher correlations with psychometric variables, these results are not consistent across corpora and distance metrics (Figures 4 and 5, Appendix C). In light of recent findings regarding the inverse relationship between language model size and the predictive power of surprisal (Shain et al., 2022; Oh and Schuler, 2023), we consider it an encouraging result that the predictive power of information value does not decrease with the number of model parameters.8 We do not observe a significant impact of decoding strategy on predictive power, regardless of alternative set size, as indicated by the confidence intervals in Figures 2, 4 and 5.\nIn sum, estimates of information value do not display much sensitivity to alternative set generation parameters.9 Therefore, for each corpus, we select the estimator (a combination of model, sampling algorithm, and alternative set size) that yields the best Spearman correlation with the psychometric data (Table 5 in Appendix E). We use these estimators throughout the rest of the paper."
        },
        {
            "heading": "6 In-Depth Analysis of Psychometric Data",
            "text": "Using information value, we now study which dimensions of predictability effectively explain psychometric data. This allows us to qualitatively analyse the processes humans employ while reading and assessing acceptability. We also examine the effect of contextualisation on comprehension behaviour by defining two additional measures derived from information value (Section 6.1) and\n8It remains to be seen whether this trend extends to larger language models, for which we lack computational resources.\n9We obtain similar evidence of robustness to parameter settings using an intrinsic evaluation, reported in Appendix D.\nusing them as explanatory variables in linear mixed effect models to predict per-subject psychometric data. For the dialogue corpora and CLASP, our mixed effect models predict in-context acceptability judgements. For the reading times corpora, our models predict the total time spent by a subject reading a sentence, as recorded in self-paced reading and eye-tracking studies. This is the sum, over a sentence, of word-level reading times (more details in Appendix F). We include random intercepts for (context, target) pairs in all models.\nAnalysis Procedure. For every corpus, we first test models that include a single predictor beyond the baseline: i.e., information value measured with each distance metric and either mean and min as summary statistics (see Section 3.1). Based on the fit of these single-predictor models, we select the best lexical, syntactic, and semantic distance metrics (with the corresponding summary statistics) to instantiate three-predictor models for each of the derived measures of information value.\nFollowing Wilcox et al. (2020), we evaluate each model relative to a baseline model which includes only control variables. Control variables are selected building on previous work (Meister et al., 2021): solely the intercept term for acceptability judgements and the number of fixated words for reading times (more details in Appendix F). As an indicator of explanatory power, we report \u2206LogLik, the difference in log-likelihood between a model and the baseline: a positive \u2206LogLik value indicates that the psychometric variable is more probable under the comparison model. We also report fixed effect coefficients and their statistical significance. The full results are shown in Table 6 (Appendix F), according to which the best metrics for each linguistic level are selected and used throughout the rest of the paper."
        },
        {
            "heading": "6.1 Derived Measures of Information Value",
            "text": "Inspired by information-theoretic concepts used in previous work to study the predictability of utterances (e.g., Genzel and Charniak, 2002; Giulianelli and Fern\u00e1ndez, 2021; Wallbridge et al., 2022), we define two additional derived measures of information value and assess their explanatory power.\nOut-of-context information value is the distance between an utterance y and the set of alternative productions A\u03f5 expected given the empty context \u03f5:\nI(Y =y) := I(Y =y|X = \u03f5) (2)\nIt reflects the plausibility of y regardless of its context of occurrence. An analogous notion is decontextualised surprisal.\nContext informativeness is the reduction in information value for y contributed by context x:\nC(Y =y;X=x) := I(Y =y)\u2212 I(Y =y|X=x) (3)\nThis quantifies the extent to which a context restricts the space of plausible productions such that y becomes more predictable. An analogous notion is the pointwise mutual information."
        },
        {
            "heading": "6.2 Acceptability",
            "text": "We generally expect an inverse relationship between information value and in-context acceptability judgements: information value is lower when a target utterance is closer to the set of alternatives a comprehender may expect in a given context (see Figure 1). Furthermore, we expect grammaticality and semantic plausibility\u2014two factors known to affect acceptability (Sorace and Keller, 2005; Lau et al., 2017)\u2014to play different roles in dialogue and text. For the dialogue corpora, we expect semantic-level variables to have high explanatory power, as they can identify utterances with incoherent content such as implausible underlying dialogue acts (Searle, 1969, 1975; Austin, 1975). Lexical and syntactic information value may be more explanatory of acceptability in CLASP, where stimuli are generated via round-trip translation and thus may contain disfluent or ungrammatical sentences (Somers, 2005).\nSWITCHBOARD and DAILYDIALOG. For both dialogue corpora, semantic information content\nis by far the most predictive variable (Table 6, Appendix F), especially when min is used as a summary statistic. Responses to the same dialogue context can exhibit great variability and being close to a single expected alternative\u2014in terms of semantic content and dialogue act type\u2014appears to be sufficient for an utterance to be considered acceptable. Our analysis of derived measures (Figure 3) further indicates that acceptability is mostly determined by the in-context predictability of an utterance. The high explanatory power of context informativeness (almost twice that of out-ofcontext information value) suggests that contextual cues override inherent isolated plausibility.\nCLASP. Syntactic information value is the best explanatory variable for acceptability judgements in CLASP (Table 6, Appendix F). This suggests that comprehenders entertain expectations over syntactic structures (here, represented as POS sequences)\u2014a result which could complement findings on the processing of lexicalised constructions in reading (e.g., Tremblay et al., 2011) and eyetracking studies (e.g., Underwood et al., 2004). In contrast to the dialogue corpora, estimates of incontext information value are less predictive than their out-of-context counterparts (Figure 3), which may be due to the previously discussed artificial nature of the CLASP negative samples. In sum, our results indicate that the acceptability judgements in the CLASP corpus, even if collected in context, are mostly determined by the presence of startling surface forms rather than by semantic expectations."
        },
        {
            "heading": "6.3 Reading Times",
            "text": "When reading, humans continually update their expectations about how the discourse might evolve (Hale, 2001; Levy, 2008; Yan and Jaeger, 2020). This is reflected, for example, in the faster processing of more expected words and syntactic structures (Demberg and Keller, 2008; Smith and Levy, 2013). High predictive power for lexical and syntactic information value would support these findings. However, comprehenders also reason about semantic alternatives, e.g., to compute scalar inferences (Van Tiel et al., 2014; Hu et al., 2023). Our interpretable measures of information value help clarify the contribution of different types of expectations.\nPROVO and BROWN. Syntactic information value is a strong predictor of eye-tracked reading times in PROVO, while lexical information value (in particular, based on trigram distances) is the\nonly significant explanatory variable for the selfpaced reading times in BROWN (Table 6), and only weakly so. Expectations over full semantic alternatives have a limited effect on reading times in both corpora, suggesting anticipatory processing mechanisms at play during reading operate at lower linguistic levels. For both corpora, out-of-context estimates are at least as predictive as in-context estimates and higher than context informativeness (Figure 3), indicating that context modifications only moderately dampen the negative effects of unusual syntactic arrangements and lexicalised constructions on reading speed."
        },
        {
            "heading": "7 Relation to Utterance Surprisal",
            "text": "We have shown alternative-based information value to be a powerful predictor for contextualised acceptability judgements and reading times. In fact, information value is substantially more predictive of acceptability than utterance surprisal (Section 5). We conclude with a focused comparison between these measures, considering whether they are complementary and why they might diverge."
        },
        {
            "heading": "7.1 Complementarity",
            "text": "Differences in predictive power between information value and surprisal (see Table 1) may reflect variations between the dimensions of predictability captured by the two measures. To investigate this possibility, we use both measures jointly for psychometric predictions. We focus on the dialogue corpora and PROVO, where we observed the highest explanatory power for information value (Section 6). For each corpus, we fit linear mixed effect models with control variables, using\nthe most predictive surprisal and information value estimators (one per linguistic level) in isolation and jointly as fixed effects. Table 2 summarises the results of this analysis.\nIn isolation, information value is a better predictor for the dialogue corpora. Including lexical, syntactic, and semantic information value on top of the best surprisal predictor (Joint) improves model log-likelihood substantially. Separately including each linguistic level reveals that semantic distance is largely responsible for improved fit, suggesting that surprisal fails to capture expectations over high-level linguistic properties of utterances such as speech act type, which are crucial for modelling contextualised acceptability in dialogue. This is true regardless of the aggregation function used.\nFor PROVO, surprisal is the best explanatory variable. However, including the best information value predictors further improves model fit by 58%, demonstrating the complementarity of the two measures in predicting reading times (Table 2). Separately adding information value predictors shows the strongest boost comes from syntactic factors, which are known to have higher weight in human anticipatory processing than in language models\u2019 (Arehalli et al., 2022).\nOverall, combining predictive information value with surprisal yields better models for all tested corpora, indicating that these measures capture distinct and complementary dimensions of predictability."
        },
        {
            "heading": "7.2 Effects of Discourse Context",
            "text": "While language comprehension is known to be a function of context (e.g., Kleinschmidt and Jaeger, 2015; Chen et al., 2023), little attention has been given to its impact on surprisal estimates. We examine whether the dissimilar predictability estimates of information value and surprisal stem from differences in their sensitivity to context, comparing how they behave under congruent, incongruent, and empty context conditions. In each condition, alternative sets and token-level surprisal are computed in the true context (congruent), a context randomly sampled from the respective corpus (incongruent), or with no conditioning (empty) as used to compute out-of-context information value10. We quantify effects on the best information value and surprisal predictors as \u2206LogLik, using single-predictor models described in Section 6.\n10To ensure that all stimuli in this analysis are contextualised, first sentences in PROVO paragraphs were excluded.\nTable 3 displays results for SWITCHBOARD, DAILYDIALOG, and PROVO. Congruent context produces a substantial effect on the predictive power of semantic information value for both dialogue datasets; for DAILYDIALOG, we see a 20- fold increase over the empty context condition. Surprisal shows a similar trend, though far less pronounced. Syntactic information value is the least affected by context modulations. Though surprisal is a powerful predictor for reading times in PROVO, the incongruent and empty context conditions are more predictive than the true context. Perhaps most concerning is the fact that estimates in incongruent contexts are the most predictive. In contrast, the most predictive information value (syntactic) is significantly more predictive for congruent contexts. Interestingly, information value in the control conditions is not uninformative, likely reflecting the inherent plausibility of utterances.\nBoth information value and utterance surprisal display sensitivity to context, however, the effects on surprisal are less predictable and perhaps even undesirable for certain psychometric variables."
        },
        {
            "heading": "8 Discussion and Conclusion",
            "text": "Humans constantly monitor and anticipate the trajectory of communication. Their expectations over the upcoming communicative signal are influenced by factors spanning from the immediate linguistic context to their interpretation of the speaker\u2019s goals. These expectations, in turn, determine aspects of language comprehension such as processing cost, as well as strategies of language production. We present information value, a measure which quantifies the predictability of an utterance relative to a set of plausible alternatives; and we introduce a method to obtain information value estimates via neural text generators. In contrast to utterance predictability estimates obtained by aggregating\ntoken-level surprisal, information value captures variability above the word level by explicitly accounting for more abstract communicative units like speech acts (Searle, 1969, 1975; Austin, 1975). We validate our measure by assessing its psychometric predictive power, its robustness to parameters involved in the generation of alternative sets, and its sensitivity to discourse context.\nUsing interpretable measures centred around information value, we investigate the underlying dimensions of uncertainty in human acceptability judgements and reading behaviour. We find that acceptability judgements factor in base rates of utterance acceptability (likely associated with grammaticality) but are predominantly driven by semantic expectations. In contrast, reading time is more influenced by the inherent plausibility of lexical items and part-of-speech sequences. We further compare information value to aggregates of tokenlevel surprisal, finding differences in the dimensions of predictability captured by each measure and their sensitivity to context. Information value is a stronger predictor of acceptability in written and spoken dialogue and is complementary to surprisal for predicting eye-tracked reading times.\nInformation value is defined in terms of plausible continuations of the current linguistic context, taking inspiration from the tradition of alternatives in semantics and pragmatics (Horn, 1972; Grice, 1975; Stalnaker, 1978). Although the ideal set of alternatives would be derived directly from humans, neural text generators have demonstrated their potential to act as useful proxies, particularly when multiple generations are considered. Variability among their productions has been shown to align with human variability (Giulianelli et al., 2023), and decision rules that operate over sets of alternative utterances, rather than next tokens, have been shown to improve generation quality (e.g., Eikema and Aziz, 2022; Guerreiro et al., 2023). We release our full set of 1.3M generated alternatives, obtained with a variety of models and sampling algorithms, to facilitate research in this direction.\nOur information value framework allows considerable flexibility in defining alternative set generation procedures, distance metrics, and summary statistics. We hope it will enable further investigation into the mechanisms involved in human language processing, and that it will serve as a basis for cognitively inspired learning rules and inference algorithms in computational models of language.\nLimitations\nOur framework for the estimation of utterance information value allows great flexibility. Modellers can experiment with a variety of alternative set generation procedures, distance metrics, and summary statistics. While our selection of distance metrics characterises the relation of an utterance to its alternative sets at multiple interpretable linguistic levels, there is a large space of metrics that we have not tested in this paper. Syntactic distances, for example, can be computed using metrics that capture structural differences between utterances in a more fine-grained manner (e.g., tree edit distance or difference in syntactic tree depth); semantic distances can be computed with a more taxonomical approach (e.g., Fellbaum, 2010) or using NLI models to capture semantic equivalence (Kuhn et al., 2023); and distances between dialogue act types can be detected using dialogue act classifiers (Stasaski and Hearst, 2023). We chose metrics based on prior work validating them as probes for the extraction of uncertainty estimates from neural text generators (Giulianelli et al., 2023), but we hope future work will explore this space more exhaustively. Similarly, though the current work has been constrained to English data, our framework can be directly applied to other languages. We hope to see work in this direction.\nMoreover, due to computational constraints, we selected a single information value estimator per corpus for our analyses in Sections 6 and 7. Although we assessed the sensitivity of information value to parameters of alternative set generation extensively in Appendices C and D, the effect of estimator parameters on the explanatory power of information value predictors can be assessed more widely in future work.\nA further aspect of our method for the estimation of information value that we have not highlighted in the paper is its computational cost. Because it involves drawing multiple full utterance samples from language models, our method is clearly less efficient than traditional surprisal estimation, which requires only a single forward pass. While we have observed that the psychometric predictive power of information value reaches satisfactory levels even with relatively low numbers of alternatives and small language model architectures (see, e.g., Figure 2), designing more efficient methods for the estimation of information value is an important direction for future research.\nEthics Statement\nIn the Limitations section, we have mentioned that an important direction for future work is designing more computationally efficient methods for the estimation of information value. This is crucial to the application of this method to larger datasets, which may be prohibitively expensive in some research communities and in any case, perhaps unnecessarily, environmentally unfriendly.\nThe limited size of the corpora of psychometric data used in this paper has further ethical implications, as the corpora have not been collected to be representative of a wide and diverse range of comprehenders. We hope to see efforts in this direction."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the ILLC\u2019s Dialogue Modelling Group for helpful comments and discussions. MG and RF are supported by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant agreement No. 819455)."
        },
        {
            "heading": "A Language Models",
            "text": "For the dialogue corpora, we use GPT-2 (Radford et al., 2019), DialoGPT (Zhang et al., 2020), and GPT-Neo (Black et al., 2021). For the text corpora, we use GPT-2 (Radford et al., 2019), GPT-Neo (Black et al., 2021), and OPT (Zhang et al., 2022). The text models are pre-trained while the dialogue models are fine-tuned for 5 epochs with early stopping on the respective datasets, using \u201c</s> <s>\u201d as a turn separator. Preliminary experiments on the pre-trained models show that </s> <s> is the turn separator that yields lowest perplexity on the dialogue datasets. For text models, using no separator is the option that yields the lowest perplexity. When generating out of context, we set x to be either the dialogue turn separator \u201c</s> <s>\u201d or a white space for the text models.\nLM validation: perplexity. Table 4 reports the perplexity of these models on the SWITCHBOARD and DAILYDIALOG test sets, as well as on the WikiText test set (the CLASP dataset and the reading\ntime datasets are too small to allow for robust evaluation, but their style is sufficiently similar enough to that of WikiText). Perplexity scores are the lowest for the dialogue datasets. This is to be expected as the dialogue models are fine-tuned. The perplexity of the pre-trained models on WikiText is in line with state-of-the-art results; OPT obtains higher perplexity than GPT-2 and GPT-Neo, but still in an appropriate range."
        },
        {
            "heading": "B Utterance-Level Surprisal",
            "text": "Given an utterance y as a sequence of tokens in a context x, token-level surprisal can be defined as s(yt) = \u2212 log p(yt|y<t,x). Multiple works have proposed quantifying utterance-level surprisal as functions of token-level surprisal (Genzel and Charniak, 2002; Keller, 2004; Xu and Reitter, 2018; Meister et al., 2021; Giulianelli et al., 2021; Wallbridge et al., 2022). We compare the predictive power of information value to a number of these utterance-level surprisal aggregates.\nMean surprisal and total surprisal account for all token-level surprisal estimates with and without normalising by utterance length:\nSmean(y|x) = 1\nN N\u2211 n=1 [s(yn)] (4)\nStotal(y|x) = N\u2211\nn=1\n[s(yn)] (5)\nSuperlinear surprisal posits a superlinear effect of token-level estimates:\nSsuperlineark(y|x) = N\u2211\nn=1\n[s(yn)] k (6)\nWe experiment with k \u2208 [0.5, 0.75, . . . , 5]. Maximum surprisal captures the idea that a highly surprising element drives the overall surprisal of an utterance:\nSmax(y|x) = max[s(yn)] (7)\nSurprisal variance across an utterance has been defined in a number of ways; we consider surprisal variance as the regression to the utterance-level mean and surprisal difference as the variability between contiguous token-level estimates:\nSvariance(y|x) = 1\nN \u2212 1 N\u2211 n=2 [s(yn)\u2212 Smean(y)]2\n(8)\nSdifference(y|x) = N\u2211\nn=2\n|s(yn)\u2212 s(yn\u22121)| (9)"
        },
        {
            "heading": "C Psychometric Predictive Power and Sensitivity of Information Value Estimates",
            "text": "We study the extent to which our estimates of information value are affected by variation in three main factors: the alternative set size ([10, 20, ..., 100]), the language model, and the sampling strategy. Figures 4 and 5 show Spearman correlation between information value and psychometric data, averaged over subjects. These results complement Sections 5.1 and 5.2 in the main paper.\nD Intrinsic Robustness Analysis\nIn Section 5.1, we evaluate the robustness of information value to parameters involved in the alternative set generation in terms of its psychometric predictive power. We additionally assess their intrinsic robustness by measuring the correlation between information values assigned to target utterances by estimators with different parameter settings.\nThe parameters which we consider are alternative set size ([10, 20, ..., 100]), the generative model, and the decoding strategy. Models and decoding strategies are detailed in Section 4.1. For each of the corpora described in Section 4.2, we compute the information value for the target utterances based on alternative sets generated under different parameter settings. Robustness is quantified through the distribution of the pairwise Spearman\ncorrelation \u03c1 obtained between the information values for each parameter setting; strong pairwise correlation indicates that information value is robust to the varying parameter. Results are displayed in Figures 6 and 7.\nInformation value defined as lexical, syntactic, and semantic distance becomes highly robust as alternative set size increases; mean correlations between decoding strategies for each model converge towards perfect correlation as alternative set size increases. This pattern holds for all datasets. Decoding strategies do not produce much variation across correlations, regardless of alternative set size (see confidence intervals in Figures 6 and 7). For mean-based definitions and models, information values generated from different decoding strategies correlate at strengths > 0.8 from sets with fewer than 50 alternatives. Although their mean correlations still converge to 0.9, the dialogue datasets are slight exceptions.\nAs expected, correlations between parameter settings for min-based distances are more variable. Although they converge to weaker correlations as alternative set size increases when compared to mean-based distances, we still find strong to very strong correlations between decoding strategies for large alternative sets across all models."
        },
        {
            "heading": "E Selecting the Best Information Value Estimators",
            "text": "For each corpus and each surprisal type (lexical, syntactic, semantic), we select the estimator that yields the best Spearman correlation with the psychometric data. An estimator is a combination of model, sampling algorithm, and alternative set size. Psychometric data are in-context acceptability judgements for DailyDialog, Switchboard and Clasp, and the mean of all word reading times in a sentence for Brown and Provo. Table 5 shows the best estimators."
        },
        {
            "heading": "F Linear Mixed Effect Models",
            "text": "In this section, we include further details about the linear mixed effect models used in Sections 6 and 7. All results for single information value predictors are in Table 6. Results for the comparison with surprisal and joint models are in Table 2.\nResponse variables. For PROVO, we use the total dwell time, i.e., the cumulative duration across all fixations on a given word. We filter away any observation that contains \u2018outlier\u2019 words, i.e., words\nwith a z-score > 3 when the distribution of reading times is modelled as log-linear (following Meister et al., 2021).\nControl predictors. Following Wilcox et al. (2020), we evaluate each model relative to a baseline model which includes only control variables. Control variables are selected building on previous work (Meister et al., 2021): we include solely an intercept term as a baseline for acceptability judgements and the number of fixated words for reading times. Meister et al. (2021) report similar trends when including summed unigram log probability or sentence length as baseline predictors of acceptability judgements, and word character lengths or word unigram log probabilities for reading times. For reading times, we also test sentence length as a predictor but baseline models that include, instead, the number of fixated words (readers sometimes skip words while reading) achieve higher log-likelihood."
        },
        {
            "heading": "G More Derived Measures of Information Value",
            "text": "We also tested the following measures derived from information value but found them to be less predictive than those in the main paper.\nExpected information value. The expected distance of plausible productions given a context x from the alternative set:\nE(I(Y |X=x)) := Ea\u2208A\u2032x [I(Y =a,X=x)] (10)\nWe assume a uniform probability distribution over alternatives. This quantifies the uncertainty over next utterances determined by the context alone. Because the alternative set Ax is the set of plausible productions given x, in practice, we compute expected information value using only one alternative set\u2014both in the expectation Ea\u2208Ax and in the distance calculation d(y,Ax).\nDeviation from the expected information value. The absolute difference between the information value for the next utterance y and the expected information value for any next utterance:\nD(Y =y|X=x) := |I(Y =y|X=x) \u2212 E(I(Y |X=x))| (11)\nThis quantifies the information value of an utterance relative to the information value expected for\nplausible productions given x. An analogous notion is the deviation of surprisal from entropy. The token-level version of this forms the basis of the local typicality hypothesis (Meister et al., 2023).\nExpected context informativeness. The expected informativeness of context x is the reduction in information value contributed by x with respect to any plausible continuation:\nE(C(Y =y;X=x)) := E(I(Y =y)) \u2212 E(I(Y =y|X=x))\n(12)\nThis quantifies the extent to which a context restricts the space of plausible productions. An analogous notion is the expected pointwise mutual information between X = x and Y , where the value of X is fixed. Similarly to out-of-context information value, out-of-context expected information value E(I(Y =y)) is computed with respect to the alternative set A\u03f5."
        }
    ],
    "title": "Information Value: Measuring Utterance Predictability as Distance from Plausible Alternatives",
    "year": 2023
}