{
    "abstractText": "While automatic dialogue tutors hold great potential in making education personalized and more accessible, research on such systems has been hampered by a lack of sufficiently large and high-quality datasets. Collecting such datasets remains challenging, as recording tutoring sessions raises privacy concerns and crowdsourcing leads to insufficient data quality. To address this, we propose a framework to generate such dialogues by pairing human teachers with a Large Language Model (LLM) prompted to represent common student errors. We describe how we use this framework to collect MATHDIAL , a dataset of 3k one-to-one teacher-student tutoring dialogues grounded in multi-step math reasoning problems. While models like GPT-3 are good problem solvers, they fail at tutoring because they generate factually incorrect feedback or are prone to revealing solutions to students too early. To overcome this, we let teachers provide learning opportunities to students by guiding them using various scaffolding questions according to a taxonomy of teacher moves. We demonstrate MATHDIAL and its extensive annotations can be used to finetune models to be more effective tutors (and not just solvers). We confirm this by automatic and human evaluation, notably in an interactive setting that measures the trade-off between student solving success and telling solutions. The dataset is released publicly. https://github.com/eth-nlped/mathdial",
    "authors": [
        {
            "affiliations": [],
            "name": "Jakub Macina"
        },
        {
            "affiliations": [],
            "name": "Nico Daheim"
        },
        {
            "affiliations": [],
            "name": "Sankalan Pal Chowdhury"
        },
        {
            "affiliations": [],
            "name": "Tanmay Sinha"
        },
        {
            "affiliations": [],
            "name": "Manu Kapur"
        },
        {
            "affiliations": [],
            "name": "Iryna Gurevych"
        },
        {
            "affiliations": [],
            "name": "Mrinmaya Sachan"
        }
    ],
    "id": "SP:54306481ae806b077079de593967127dd2315443",
    "references": [
        {
            "authors": [
                "Julia Anghileri."
            ],
            "title": "Scaffolding practices that enhance mathematics learning",
            "venue": "Journal of Mathematics Teacher Education, 9:33\u201352.",
            "year": 2006
        },
        {
            "authors": [
                "Marcel Binz",
                "Eric Schulz."
            ],
            "title": "Using cognitive psychology to understand gpt-3",
            "venue": "Proceedings of the National Academy of Sciences, 120(6):e2218523120.",
            "year": 2023
        },
        {
            "authors": [
                "Julie L Booth",
                "Kelly M McGinn",
                "Christina Barbieri",
                "Laura K Young."
            ],
            "title": "Misconceptions and learning algebra",
            "venue": "And the rest is just algebra, pages 63\u201378.",
            "year": 2017
        },
        {
            "authors": [
                "Pawe\u0142 Budzianowski",
                "Ivan Vuli\u0107."
            ],
            "title": "Hello, it\u2019s GPT-2 - how can I help you? towards the use of pretrained language models for task-oriented dialogue systems",
            "venue": "Proceedings of the 3rd Workshop on Neural Generation and Translation, pages 15\u201322, Hong",
            "year": 2019
        },
        {
            "authors": [
                "Pawe\u0142 Budzianowski",
                "Tsung-Hsien Wen",
                "Bo-Hsiang Tseng",
                "I\u00f1igo Casanueva",
                "Stefan Ultes",
                "Osman Ramadan",
                "Milica Ga\u0161i\u0107."
            ],
            "title": "MultiWOZ - a largescale multi-domain Wizard-of-Oz dataset for taskoriented dialogue modelling",
            "venue": "Proceedings of the",
            "year": 2018
        },
        {
            "authors": [
                "Bill Byrne",
                "Karthik Krishnamoorthi",
                "Chinnadhurai Sankar",
                "Arvind Neelakantan",
                "Ben Goodrich",
                "Daniel Duckworth",
                "Semih Yavuz",
                "Amit Dubey",
                "Kyu-Young Kim",
                "Andy Cedilnik."
            ],
            "title": "Taskmaster-1: Toward a realistic and diverse dialog dataset",
            "venue": "Pro-",
            "year": 2019
        },
        {
            "authors": [
                "William Cai",
                "Josh Grossman",
                "Zhiyuan Jerry Lin",
                "Hao Sheng",
                "Johnny Tian-Zheng Wei",
                "Joseph Jay Williams",
                "Sharad Goel."
            ],
            "title": "Bandit algorithms to personalize educational chatbots",
            "venue": "Machine Learning, 110(9):2389\u20132418.",
            "year": 2021
        },
        {
            "authors": [
                "Paula Buttery"
            ],
            "title": "The teacher-student chatroom corpus",
            "venue": "In Proceedings of the 9th Workshop on NLP for Computer Assisted Language Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Inigo Casanueva",
                "Ivan Vuli\u0107",
                "Georgios Spithourakis",
                "Pawe\u0142 Budzianowski."
            ],
            "title": "NLU++: A multilabel, slot-rich, generalisable dataset for natural language understanding in task-oriented dialogue",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Maximillian Chen",
                "Alexandros Papangelis",
                "Chenyang Tao",
                "Seokhwan Kim",
                "Andy Rosenbaum",
                "Yang Liu",
                "Zhou Yu",
                "Dilek Hakkani-Tur."
            ],
            "title": "PLACES: Prompting language models for social conversation synthesis",
            "venue": "Findings of the Association for Com-",
            "year": 2023
        },
        {
            "authors": [
                "Eunsol Choi",
                "He He",
                "Mohit Iyyer",
                "Mark Yatskar",
                "Wentau Yih",
                "Yejin Choi",
                "Percy Liang",
                "Luke Zettlemoyer."
            ],
            "title": "QuAC: Question answering in context",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2018
        },
        {
            "authors": [
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Mark Chen",
                "Heewoo Jun",
                "Lukasz Kaiser",
                "Matthias Plappert",
                "Jerry Tworek",
                "Jacob Hilton",
                "Reiichiro Nakano",
                "Christopher Hesse",
                "John Schulman"
            ],
            "title": "Training verifiers to solve math word",
            "year": 2021
        },
        {
            "authors": [
                "Nico Daheim",
                "Nouha Dziri",
                "Mrinmaya Sachan",
                "Iryna Gurevych",
                "Edoardo M. Ponti."
            ],
            "title": "Elastic weight removal for faithful and abstractive dialogue generation",
            "venue": "arXiv preprint arXiv:2303.17574.",
            "year": 2023
        },
        {
            "authors": [
                "Zhuyun Dai",
                "Arun Tejasvi Chaganty",
                "Vincent Y Zhao",
                "Aida Amini",
                "Qazi Mamunur Rashid",
                "Mike Green",
                "Kelvin Guu."
            ],
            "title": "Dialog inpainting: Turning documents into dialogs",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "James L McClelland",
                "Felix Hill."
            ],
            "title": "Language models show human-like content effects on reasoning",
            "venue": "arXiv preprint arXiv:2207.07051.",
            "year": 2022
        },
        {
            "authors": [
                "Dorottya Demszky",
                "Heather Hill."
            ],
            "title": "The NCTE transcripts: A dataset of elementary math classroom transcripts",
            "venue": "Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023), pages 528\u2013538, Toronto,",
            "year": 2023
        },
        {
            "authors": [
                "Dorottya Demszky",
                "Jing Liu",
                "Zid Mancenido",
                "Julie Cohen",
                "Heather Hill",
                "Dan Jurafsky",
                "Tatsunori Hashimoto."
            ],
            "title": "Measuring conversational uptake: A case study on student-teacher interactions",
            "venue": "Proceedings of the 59th Annual Meeting of the Associa-",
            "year": 2021
        },
        {
            "authors": [
                "Emily Dinan",
                "Stephen Roller",
                "Kurt Shuster",
                "Angela Fan",
                "Michael Auli",
                "Jason Weston."
            ],
            "title": "Wizard of wikipedia: Knowledge-powered conversational agents",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Nouha Dziri",
                "Ehsan Kamalloo",
                "Kory Mathewson",
                "Osmar Zaiane."
            ],
            "title": "Augmenting neural response generation with context-aware topical attention",
            "venue": "Proceedings of the First Workshop on NLP for Conversational AI, pages 18\u201331, Florence, Italy. Associ-",
            "year": 2019
        },
        {
            "authors": [
                "Nouha Dziri",
                "Ehsan Kamalloo",
                "Sivan Milton",
                "Osmar Zaiane",
                "Mo Yu",
                "Edoardo M Ponti",
                "Siva Reddy."
            ],
            "title": "Faithdial: A faithful benchmark for informationseeking dialogue",
            "venue": "Transactions of the Association for Computational Linguistics, 10:1473\u20131490.",
            "year": 2022
        },
        {
            "authors": [
                "Scott Freeman",
                "Sarah L Eddy",
                "Miles McDonough",
                "Michelle K Smith",
                "Nnadozie Okoroafor",
                "Hannah Jordt",
                "Mary Pat Wenderoth."
            ],
            "title": "Active learning increases student performance in science, engineering, and mathematics",
            "venue": "Proceedings of the na-",
            "year": 2014
        },
        {
            "authors": [
                "Simon Frieder",
                "Luca Pinchetti",
                "Ryan-Rhys Griffiths",
                "Tommaso Salvatori",
                "Thomas Lukasiewicz",
                "Philipp Christian Petersen",
                "Alexis Chevalier",
                "Julius Berner."
            ],
            "title": "Mathematical capabilities of chatgpt",
            "venue": "arXiv preprint arXiv:2301.13867.",
            "year": 2023
        },
        {
            "authors": [
                "Milica Ga\u0161ic",
                "Dongho Kim",
                "Pirros Tsiakoulis",
                "Catherine Breslin",
                "Matthew Henderson",
                "Martin Szummer",
                "Blaise Thomson",
                "Steve Young."
            ],
            "title": "Incremental on-line adaptation of pomdp-based dialogue managers to extended domains",
            "venue": "Proceedings on Inter-",
            "year": 2014
        },
        {
            "authors": [
                "Matthew Henderson",
                "Blaise Thomson",
                "Jason D. Williams."
            ],
            "title": "The second dialog state tracking challenge",
            "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 263\u2013272, Philadelphia,",
            "year": 2014
        },
        {
            "authors": [
                "Christine Howe",
                "Sara Hennessy",
                "Neil Mercer",
                "Maria Vrikki",
                "Lisa Wheatley"
            ],
            "title": "Teacher\u2013student dialogue during classroom teaching: Does it really impact on student outcomes",
            "venue": "Journal of the learning sciences,",
            "year": 2019
        },
        {
            "authors": [
                "Hyangeun Ji",
                "Insook Han",
                "Yujung Ko."
            ],
            "title": "A systematic review of conversational ai in language education: focusing on the collaboration with human teachers",
            "venue": "Journal of Research on Technology in Education, 55(1):48\u201363.",
            "year": 2023
        },
        {
            "authors": [
                "John F Kelley."
            ],
            "title": "An iterative design methodology for user-friendly natural language office information applications",
            "venue": "ACM Transactions on Information Systems (TOIS), 2(1):26\u201341.",
            "year": 1984
        },
        {
            "authors": [
                "Sean Kelly",
                "Robert Bringe",
                "Esteban Aucejo",
                "Jane Cooley Fruehwirth."
            ],
            "title": "Using global observation protocols to inform research on teaching effectiveness and school improvement: Strengths and emerging limitations",
            "venue": "Education Policy Analysis Archives,",
            "year": 2020
        },
        {
            "authors": [
                "Hyunwoo Kim",
                "Jack Hessel",
                "Liwei Jiang",
                "Ximing Lu",
                "Youngjae Yu",
                "Pei Zhou",
                "Ronan Le Bras",
                "Malihe Alikhani",
                "Gunhee Kim",
                "Maarten Sap"
            ],
            "title": "Soda: Million-scale dialogue distillation with social commonsense contextualization",
            "year": 2022
        },
        {
            "authors": [
                "Seokhwan Kim",
                "Mihail Eric",
                "Karthik Gopalakrishnan",
                "Behnam Hedayatnia",
                "Yang Liu",
                "Dilek HakkaniTur."
            ],
            "title": "Beyond domain APIs: Task-oriented conversational modeling with unstructured knowledge access",
            "venue": "Proceedings of the 21th Annual Meet-",
            "year": 2020
        },
        {
            "authors": [
                "Kenneth R Koedinger",
                "Elizabeth A McLaughlin."
            ],
            "title": "Closing the loop with quantitative cognitive task analysis",
            "venue": "International Educational Data Mining Society.",
            "year": 2016
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Yanran Li",
                "Hui Su",
                "Xiaoyu Shen",
                "Wenjie Li",
                "Ziqiang Cao",
                "Shuzi Niu."
            ],
            "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
            "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Margarita Lim\u00f3n."
            ],
            "title": "On the cognitive conflict as an instructional strategy for conceptual change: A critical appraisal",
            "venue": "Learning and instruction, 11(45):357\u2013380.",
            "year": 2001
        },
        {
            "authors": [
                "Shayne Longpre",
                "Le Hou",
                "Tu Vu",
                "Albert Webson",
                "Hyung Won Chung",
                "Yi Tay",
                "Denny Zhou",
                "Quoc V. Le",
                "Barret Zoph",
                "Jason Wei",
                "Adam Roberts"
            ],
            "title": "The flan collection: Designing data and methods for effective instruction tuning",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Jakub Macina",
                "Nico Daheim",
                "Lingzhi Wang",
                "Tanmay Sinha",
                "Manu Kapur",
                "Iryna Gurevych",
                "Mrinmaya Sachan."
            ],
            "title": "Opportunities and challenges in neural dialog tutoring",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association",
            "year": 2023
        },
        {
            "authors": [
                "Olga Majewska",
                "Evgeniia Razumovskaia",
                "Edoardo M Ponti",
                "Ivan Vuli\u0107",
                "Anna Korhonen."
            ],
            "title": "Crosslingual dialogue dataset creation via outline-based generation",
            "venue": "Transactions of the Association for Computational Linguistics, 11:139\u2013156.",
            "year": 2023
        },
        {
            "authors": [
                "Julia M. Markel",
                "Steven G. Opferman",
                "James A. Landay",
                "Chris Piech."
            ],
            "title": "Gpteach: Interactive ta training with gpt-based students",
            "venue": "Proceedings of the Tenth ACM Conference on Learning @ Scale, L@S \u201923, page 226\u2013236, New York, NY, USA. Association",
            "year": 2023
        },
        {
            "authors": [
                "Benjamin D Nye",
                "Arthur C Graesser",
                "Xiangen Hu."
            ],
            "title": "Autotutor and family: A review of 17 years of natural language tutoring",
            "venue": "International Journal of Artificial Intelligence in Education, 24:427\u2013469.",
            "year": 2014
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013 191, Brussels, Belgium. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Abhinav Rastogi",
                "Xiaoxue Zang",
                "Srinivas Sunkara",
                "Raghav Gupta",
                "Pranav Khaitan."
            ],
            "title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
            "year": 2020
        },
        {
            "authors": [
                "Brian J. Reiser."
            ],
            "title": "Scaffolding complex learning: The mechanisms of structuring and problematizing student work",
            "venue": "Journal of the Learning Sciences, 13(3):273\u2013304.",
            "year": 2004
        },
        {
            "authors": [
                "Rod D Roscoe",
                "Michelene TH Chi."
            ],
            "title": "Tutor learning: The role of explaining and responding to questions",
            "venue": "Instructional science, 36:321\u2013350.",
            "year": 2008
        },
        {
            "authors": [
                "Sherry Ruan",
                "Liwei Jiang",
                "Justin Xu",
                "Bryce Joe-Kun Tham",
                "Zhengneng Qiu",
                "Yeshuang Zhu",
                "Elizabeth L. Murnane",
                "Emma Brunskill",
                "James A. Landay."
            ],
            "title": "Quizbot: A dialogue-based adaptive learning system for factual knowledge",
            "venue": "Proceedings of the",
            "year": 2019
        },
        {
            "authors": [
                "Pararth Shah",
                "Dilek Hakkani-T\u00fcr",
                "Gokhan T\u00fcr",
                "Abhinav Rastogi",
                "Ankur Bapna",
                "Neha Nayak",
                "Larry Heck."
            ],
            "title": "Building a conversational agent overnight with dialogue self-play",
            "venue": "arXiv preprint arXiv:1801.04871.",
            "year": 2018
        },
        {
            "authors": [
                "Tasmia Shahriar",
                "Noboru Matsuda."
            ],
            "title": "Can you clarify what you said?: Studying the impact of tutee agents\u2019 follow-up questions on tutors\u2019 learning",
            "venue": "Artificial Intelligence in Education: 22nd International Conference, AIED 2021, Utrecht, The Nether-",
            "year": 2021
        },
        {
            "authors": [
                "Kumar Shridhar",
                "Jakub Macina",
                "Mennatallah El-Assady",
                "Tanmay Sinha",
                "Manu Kapur",
                "Mrinmaya Sachan."
            ],
            "title": "Automatic generation of socratic subquestions for teaching math word problems",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Nat-",
            "year": 2022
        },
        {
            "authors": [
                "Katherine Stasaski",
                "Kimberly Kao",
                "Marti A. Hearst."
            ],
            "title": "CIMA: A large open access dialogue dataset for tutoring",
            "venue": "Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52\u201364, Seattle, WA, USA",
            "year": 2020
        },
        {
            "authors": [
                "Abhijit Suresh",
                "Jennifer Jacobs",
                "Margaret Perkoff",
                "James H. Martin",
                "Tamara Sumner."
            ],
            "title": "Finetuning transformers with additional context to classify discursive moves in mathematics classrooms",
            "venue": "In",
            "year": 2022
        },
        {
            "authors": [
                "Ana\u00efs Tack",
                "Chris Piech."
            ],
            "title": "The AI teacher test: Measuring the pedagogical ability of blender and GPT-3 in educational dialogues",
            "venue": "Proceedings of the 15th International Conference on Educational Data Mining, pages 522\u2013529, Durham, United King-",
            "year": 2022
        },
        {
            "authors": [
                "Kimberly D. Tanner."
            ],
            "title": "Structure matters: Twentyone teaching strategies to promote student engagement and cultivate classroom equity",
            "venue": "CBE\u2014Life Sciences Education, 12(3):322\u2013331. PMID: 24006379.",
            "year": 2013
        },
        {
            "authors": [
                "Aroyo",
                "Ravi Rajakumar",
                "Alena Butryna",
                "Matthew Lamm",
                "Viktoriya Kuzmina",
                "Joe Fenton",
                "Aaron Cohen",
                "Rachel Bernstein",
                "Ray Kurzweil",
                "Blaise AgueraArcas",
                "Claire Cui",
                "Marian Croak",
                "Ed Chi",
                "Quoc Le"
            ],
            "title": "Lamda: Language models for dialog",
            "year": 2022
        },
        {
            "authors": [
                "Kurt VanLehn."
            ],
            "title": "The relative effectiveness of human tutoring, intelligent tutoring systems, and other tutoring systems",
            "venue": "Educational Psychologist, 46(4):197\u2013221.",
            "year": 2011
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Lingzhi Wang",
                "Mrinmaya Sachan",
                "Xingshan Zeng",
                "Kam-Fai Wong."
            ],
            "title": "Strategize before teaching: A conversational tutoring system with pedagogy self-distillation",
            "venue": "Findings of the Association for Computational Linguistics: EACL 2023, pages 2268\u2013",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V Le",
                "Ed H. Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "The Eleventh International Conference on Learning",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Hsien Wen",
                "David Vandyke",
                "Nikola Mrk\u0161i\u0107",
                "Milica Ga\u0161i\u0107",
                "Lina M. Rojas-Barahona",
                "Pei-Hao Su",
                "Stefan Ultes",
                "Steve Young."
            ],
            "title": "A network-based end-to-end trainable task-oriented dialogue system",
            "venue": "Proceedings of the 15th Conference of the Euro-",
            "year": 2017
        },
        {
            "authors": [
                "Rainer Winkler",
                "Sebastian Hobert",
                "Antti Salovaara",
                "Matthias S\u00f6llner",
                "Jan Marco Leimeister."
            ],
            "title": "Sara, the lecturer: Improving learning in online education with a scaffolding-based conversational agent",
            "venue": "Proceedings of the 2020 CHI Conference on Hu-",
            "year": 2020
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Wollny",
                "Jan Schneider",
                "Daniele Di Mitri",
                "Joshua Weidlich",
                "Marc Rittberger",
                "Hendrik Drachsler."
            ],
            "title": "Are we there yet? - a systematic literature review on chatbots in education",
            "venue": "Frontiers in Artificial Intelligence, 4:654924.",
            "year": 2021
        },
        {
            "authors": [
                "Jing Xu",
                "Da Ju",
                "Joshua Lane",
                "Mojtaba Komeili",
                "Eric Michael Smith",
                "Megan Ung",
                "Morteza Behrooz",
                "William Ngan",
                "Rashel Moritz",
                "Sainbayar Sukhbaatar",
                "Y-Lan Boureau",
                "Jason Weston",
                "Kurt Shuster"
            ],
            "title": "Improving open language models by",
            "year": 2023
        },
        {
            "authors": [
                "Lining Zhang",
                "Simon Mille",
                "Yufang Hou",
                "Daniel Deutsch",
                "Elizabeth Clark",
                "Yixin Liu",
                "Saad Mahamood",
                "Sebastian Gehrmann",
                "Miruna Clinciu",
                "Khyathi Raghavi Chandu",
                "Jo\u00e3o Sedoc"
            ],
            "title": "A needle in a haystack: An analysis of high-agreement",
            "year": 2023
        },
        {
            "authors": [
                "Wang",
                "Luke Zettlemoyer."
            ],
            "title": "Opt: Open pretrained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068.",
            "year": 2022
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Yizhe Zhang",
                "Michel Galley",
                "Jianfeng Gao",
                "Zhe Gan",
                "Xiujun Li",
                "Chris Brockett",
                "Bill Dolan."
            ],
            "title": "Generating informative and diverse conversational responses via adversarial information maximization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Wangchunshu Zhou",
                "Yuchen Eleanor Jiang",
                "Long Li",
                "Jialong Wu",
                "Tiannan Wang",
                "Shi Qiu",
                "Jintian Zhang",
                "Jing Chen",
                "Ruipu Wu",
                "Shuai Wang"
            ],
            "title": "Agents: An open-source framework for autonomous language agents",
            "venue": "arXiv preprint arXiv:2309.07870",
            "year": 2023
        },
        {
            "authors": [
                "Qi Zhu",
                "Kaili Huang",
                "Zheng Zhang",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "Crosswoz: A large-scale chinese cross-domain task-oriented dialogue dataset",
            "venue": "Transactions of the Association for Computational Linguistics, 8:281\u2013295.",
            "year": 2020
        },
        {
            "authors": [
                "Caleb Ziems",
                "William Held",
                "Omar Shaikh",
                "Jiaao Chen",
                "Zhehao Zhang",
                "Diyi Yang"
            ],
            "title": "Can large language models transform computational social science? arXiv preprint arXiv:2305.03514",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "https://github.com/eth-nlped/mathdial"
        },
        {
            "heading": "1 Introduction",
            "text": "Dialogue tutoring systems have demonstrated significant potential in augmenting learning outcomes across various domains (Wollny et al., 2021; Ji\n*Equal contribution.\ntoo early. MATHDIAL mitigates this using scaffolding questions and grounding annotations.\net al., 2023). However, the progress of scaling them is considerably hindered by a lack of highquality datasets, which actually provide students with space for exploration by scaffolding their learning (Tack and Piech, 2022; Macina et al., 2023). The current datasets are frequently marred with issues like low pedagogical quality, are too small, or focus on noisy classroom settings. While recording tutoring sessions might be a scalable alternative, it bears strong privacy concerns (Demszky and Hill, 2023). On the other hand, crowdsourcing dialogues is costly, requires synchronizing annotators, and can lead to insufficient quality due to poor annotator training (Stasaski et al., 2020).\nAt the same time, recent advancements in Large Language Models (LLMs) have enabled significant improvements in generative dialogue systems (Budzianowski and Vulic\u0301, 2019; Thoppilan et al., 2022; Xu et al., 2023) and simultaneously shown\ngreat success in reasoning over educational domains, such as math problems (Cobbe et al., 2021; Wei et al., 2022; Wang et al., 2023b; OpenAI, 2023). However, this has not yet translated to improvements in dialogue tutoring systems, as showcased by the lack of pedagogical understanding and factually incorrect behaviour of GPT-3 (Tack and Piech, 2022) and open-source LLMs (Macina et al., 2023). Figure 1 shows examples of generations that reveal information to students too early and misunderstand their solutions. This is also confirmed in our human evaluation: when asked ChatGPT to tutor a student as a teacher, it directly reveals the solution 66% of times and provides incorrect feedback 59% of times (cf. Section 6.3).\nTo address these issues, we collect and present a dialogue tutoring dataset called MATHDIAL . The dataset has rich tutoring quality which we measure by equitable tutoring (Tanner, 2013): providing opportunities for the student to learn, think and explore potential solutions. For this, we take inspiration from human tutoring strategies (Nye et al., 2014) and active learning approaches in classrooms (Freeman et al., 2014) that show a positive impact on student learning gains.\nWe collect our dataset using a novel data collection approach. This approach pairs human teachers with an LLM that simulates students and their errors, which the same teachers rate as representative of real students in our study. MATHDIAL is grounded in math word problems and student confusions and therefore provides a challenging testbed for creating faithful and equitable dialogue tutoring models that can reason over complex data. Figure 1 shows one dialogue from MATHDIAL , where a teacher scaffolds student learning by asking an interactive scaffolding question instead of leaking the solution.\nWe benchmark various models on the task of generating tutor responses for MATHDIAL , using both finetuning and prompting. We find that finetuning smaller open-source LLMs on our dataset can make them significantly more equitable and\nfaithful to the teaching material than prompting larger LLMs (Section 6.3). Moreover, we propose an interactive, end-to-end tutoring simulation between a teacher and student model where we measure a trade-off between student solving success and teachers directly revealing answers in (Section 6.4). Open-source LLMs that are finetuned on our dataset achieve similar student-solving success as ChatGPT while telling solutions less often. Finally, we highlight open challenges on this dataset, such as generalization to new problems."
        },
        {
            "heading": "2 Background & Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Dialogue Datasets & Collection Methodologies",
            "text": "Research on task-oriented dialogue systems has mainly focused on customer service, for instance, restaurant reservations (Henderson et al., 2014; Ga\u0161ic et al., 2014). Notably, Wen et al. (2017) collect such dialogues with the Wizard-of-Oz (WoZ) paradigm (Kelley, 1984), where crowdworkers are connected to roleplay interlocutors. One plays the user who interacts with the system, and the other roleplays the system and is often exclusively given access to domain knowledge. WoZ has been used to collect many popular datasets, such as MultiWoZ (Budzianowski et al., 2018) and extensions (Kim et al., 2020; Zhu et al., 2020), Taskmaster (Byrne et al., 2019), and open-domain datasets like Wizard-of-Wikipedia (Dinan et al., 2019). Other collection methods include crowdworkers filling dialogue outlines (Shah et al., 2018; Rastogi et al., 2020; Majewska et al., 2023), or scraping from the web (Li et al., 2017; Dziri et al., 2019).\nMultiple works have shown shortcomings in using non-expert crowdworkers. For instance, document-grounded corpora often contain hallucinations in ground-truth data (Dziri et al., 2022), and task-oriented corpora tend to suffer from annotation errors and low lexical diversity (Casanueva et al., 2022). More closely related to this work, current tutoring corpora lack sufficient tutoring quality\n(Tack and Piech, 2022; Macina et al., 2023). MATHDIAL mitigates these issues by adapting the WoZ paradigm to using human teachers as experts in collaboration with an LLM."
        },
        {
            "heading": "2.2 Dialogue Tutoring Corpora & Teacher Moves",
            "text": "Theoretical and empirical studies have shown the importance of questioning in human learning (Roscoe and Chi, 2008; Shahriar and Matsuda, 2021; Shridhar et al., 2022). Therefore, prior research has explored which types of questions in tutoring conversations improve student learning. Nye et al. (2014), for instance, show the effectiveness of deep reasoning questions, and (Howe et al., 2019) find that elaboration and challenging of previous contributions can benefit student learning. This has led to a series of human-authored dialogue tutoring systems, like AutoTutor (Nye et al., 2014), which guide students in problem-solving using natural language explanations. Assisting students to succeed in complex tasks commonly referred to as scaffolding (Reiser, 2004; Anghileri, 2006). More recently, several rule-based dialogue systems with predefined goals have been proposed (Ruan et al., 2019; Winkler et al., 2020; Cai et al., 2021), but scaling them requires extensive human authoring and quickly becomes complex. As a consequence, building effective automatic tutors at scale remains an open problem.\nWhile data-driven approaches seem like a promising direction (Macina et al., 2023; Wang et al., 2023a), only a limited number of tutoring\ncorpora are publicly available to our knowledge: CIMA (Stasaski et al., 2020), TSCC (Caines et al., 2020), TalkMoves (Suresh et al., 2022), and NCTE (Demszky and Hill, 2023). All of them suffer from several limitations, such as missing grounding information (TSCC, TalkMoves, NCTE), low tutoring quality (CIMA), small dataset sizes (all), or a focus on noisy classroom scenarios (see Table 1)."
        },
        {
            "heading": "2.3 Synthetic Dialogue Data Creation",
            "text": "LLMs have recently found their way as synthetic dialogue dataset generators due to their increasingly human-like behaviour. Both methods using finetuning (Dai et al., 2022) and prompting (Kim et al., 2022; Chen et al., 2023) haven been proposed. The human-like behaviour also manifests in them showing similar biases in logical reasoning as humans (Dasgupta et al., 2022; Binz and Schulz, 2023), and can be comparable to gold-human annotations for generation tasks (Ziems et al., 2023). Consequently, they have been used to simulate students for teacher training (Markel et al., 2023), suggesting that one might also rely upon them to create meaningful tutors. However, Tack and Piech (2022); Macina et al. (2023) show that they can not yet perform well as teachers out-of-the-box, because they often incorrectly assess student solutions and reveal answers too quickly."
        },
        {
            "heading": "3 MATHDIAL Collection Pipeline",
            "text": "This section introduces a framework for collecting high-quality tutoring conversations, highlighted in Figure 2. The core idea behind it is to connect\nan expert annotator, who roleplays a teacher, with an LLM that simulates the student.1 We use this methodology to collect dialogues based on GSM8k (Cobbe et al., 2021), a diverse collection of grade school multi-step math word problems (MWPs).\nFirst, we estimate student confusion for a given MWP by using temperature sampling to obtain diverse solutions from an LLM. We then select the most frequent incorrect solution. Therefore, each tutoring dialogue deals with the solution of exactly one MWP and one confusion. As a next step, we pair a human teacher with the LLM to create a dialogue that should resolve the confusion. We ground the LLM in one of six student profiles. These student profiles consist of common misconceptions of students learning algebra, such as struggling to recognize the problem type, and are taken from Booth et al. (2017). A detailed description of these profiles is found in Section C.\nThe teacher has access to the MWP and its correct step-by-step solution, as well as the initial student confusion (cf. Figure 7). Then, the teacher is tasked to guide the student to solve the problem by employing a sequence of scaffolding moves, which we refer to as a teaching strategy. The teachers themselves can use their expertise to determine the strategy but are required to select the current move before writing a response, as we have found this to lead to more diverse pedagogical patterns. We describe these moves in Section 3.4. The dialogue ends when the teacher marks the problem as solved or a certain time limit is reached.\nIn addition to the collected dialogues, we obtain metadata that future work can explore for building more effective tutor models. In particular, for each dialogue MATHDIAL contains the MWP,\n1In contrast, in WoZ two users are connected, with one simulating a system.\nstep-by-step solution, the exact step that led to student confusion, and annotations indicating if it was resolved over the course of the dialogue. Stepby-step and student solutions are also provided as equations."
        },
        {
            "heading": "3.1 Teacher Selection",
            "text": "We recruit professionals with teaching experience through Prolific2. We only select teachers who have completed at least 500 submissions and achieved a 100% completion rate. Annotators read guidelines for the task in an initial training phase (cf. Section D.3) and then complete a test on an example conversation to assess their understanding of the task. We only select annotators with 100% test scores for further rounds of data collection, similar to Zhang et al. (2023). We employ 91 expert annotators, of which 71 identify as female and 18 as male. The majority of annotators are nationals of the UK, followed by the USA, Canada, Australia, India, and Germany, with a median age of 39 years."
        },
        {
            "heading": "3.2 Problem & Confusion Selection",
            "text": "We employ an LLM to generate plausible student confusions and base the dialogues on them. We pick the most frequent incorrect solution sampled from ChatGPT (gpt-3.5-turbo) (Ouyang et al., 2022) using chain-of-thought prompting. To be precise, we first use temperature sampling to obtain N = 50 reasoning paths for every MWP in GSM8k, with T = 0.7 and no top-k truncation Wang et al. (2023b). Then, we group incorrect solutions according to their final numeric answer and pick one from the set with the largest cardinality. More details can be found in Appendix B. As we will show in Section 4.1, teachers think that the\n2https://www.prolific.co\nmajority of sampled confusions are plausible and could also have been made by a real student."
        },
        {
            "heading": "3.3 Student Turn Generation",
            "text": "We use InstructGPT (text-davinci-003) (Ouyang et al., 2022) to generate student turns. We prompt the model with the previous dialogue history and additional information that grounds the next turn. The prompt contains the MWP, the initial student confusion, as well as the student profile which explains the type of confusion and persona of the student."
        },
        {
            "heading": "3.4 Taxonomy of Teacher Moves",
            "text": "This section defines the taxonomy of all teacher moves that are used in MATHDIAL . We base the first two on the work of Reiser (2004), who suggest that scaffolding strategies can be split into two main categories: structure and problematize. These form the basis for the Focus and Probing moves employed in our study. Focus is used to constrain the student to make direct progress towards solving the problem. Probing is used to generalize certain aspects of the problem which allows the student to explore its underlying concepts. More concretely, a teacher might construct a new, related problem that targets only one specific concept that is needed to solve the original MWP. However, scaffolding might also fail, for example when a student gets stuck. Then, teachers may need to reveal parts of the answer. This is called Telling. Finally, turns that just serve as conversational elements and have limited pedagogical value are classed as Generic. Table 2 lists finer-grained intents for each of these four categories along with a set of accompanying examples."
        },
        {
            "heading": "4 MATHDIAL Analysis",
            "text": "We quantitatively evaluate the collected tutoring dialogues to assess their quality. For this, we outline descriptive statistics in Table 1. First of all, we can see that our dataset is significantly larger in terms of the number of dialogues and utterances than all related datasets that are listed. By opensourcing such a large dataset, we fill a crucial gap of sufficiently-sized open-source tutoring corpora which has so far hindered research in the area (Macina et al., 2023).\nFurthermore, MATHDIAL exhibits a higher diversity, measured in bigram entropy (Zhang et al., 2018), than CIMA and TalkMoves. The diversity\nis similar to NCTE and TSCC which consist of transcripts of classroom and one-to-one tutoring sessions, respectively. This supports the observation that expert annotators tend to create more diverse utterances than untrained crowdworkers (Casanueva et al., 2022), and also that LLMs can be used to generate diverse tutoring dialogues. Finally, we measure the Uptake (Demszky et al., 2021) of annotated teacher utterances. Uptake indicates how coherent the teacher\u2019s utterance is with respect to the previous student\u2019s turn. We find that MATHDIAL and CIMA have similar uptake. Both surpass the other datasets in our comparison."
        },
        {
            "heading": "4.1 How well can LLMs simulate students?",
            "text": "Our collection methodology relies on LLMs for simulating students. Therefore, it is crucial to ensure that the turns simulated by the LLM also match what a teacher would expect of a real student, who in our case is a sixth grader. In this section, we evaluate this quantitatively.\nFigure 3 shows that annotators rate the majority of generations by the model positively along two dimensions. The first one says that the confusion of the student is typical confusion of a sixth grader. The second one says that the interaction with the student as a whole is as expected of a sixth grader. We release these annotations with our final dataset which allows users of MATHDIAL to filter out utterances that are of a lower quality.\nMoreover, LLMs can be prone to incorrect arithmetic calculations. Therefore, we asked annotators to distinguish conceptual errors from such simple calculation mistakes. Arithmetic errors may be easily resolved through calculators but conceptual errors are likely to require tutors to resolve them, for example by scaffolding. Annotators identified around 80% of the confusions as conceptual, leaving around a fifth containing arithmetic errors. Again, we include these annotations to allow for data filtering."
        },
        {
            "heading": "4.2 Which teaching strategies do annotators choose?",
            "text": "In this Section, we evaluate when teachers use which teacher moves in the conversations. Figure 4 shows that teachers most frequently use Focus questions which are found in 37% of utterances. Focus is followed by Generic and Probing. Telling is the rarest move. To validate these annotations, we sampled 17 conversations consisting of 102 teacher utterances and asked two independent annotators to\nannotate their moves. We obtain an agreement of \u03ba = 0.60 between the two annotators and \u03ba = 0.49 and \u03ba = 0.34, respectively, between either of the annotators and the teacher. We note that Probing and Focus appear to be particularly challenging to distinguish and acknowledge that the boundary between them may be subjective. Merging these two categories into one larger \u2018scaffolding\u2019 category improves agreements to \u03ba = 0.67, \u03ba = 0.75 and \u03ba = 0.55. Our observations are in line with related works that have shown low inter-annotator agreement between experts for detailed teacher moves in classroom settings (Kelly et al., 2020).\nThe sequence of moves employed by the teachers constitutes their teaching strategy which we analyze in the following. Figure 4 shows the distribution of teacher moves for different stages of the conversations. We find that the initial utterance by the teacher is usually generic and serves as a conversation opener, oftentimes by asking the student to repeat the question or solution attempt. During the conversation, teachers mainly use scaffolding to either probe the student or focus the conversation on a specific part of the problem. The more the conversations progress the more likely teachers are to resort to Telling because students often get stuck at a specific subproblem and are unable to resolve it themselves. As a consequence, less Probing is used. This has been shown to keep students engaged in the conversation who otherwise become frustrated by being stuck (VanLehn, 2011)."
        },
        {
            "heading": "4.3 How often can student confusion be resolved?",
            "text": "The goal of MATHDIAL is to enable building tutors that can help students resolve their confusion. Therefore, we would like to know how often teachers can do so in our collected data. This is annotated by the teachers themselves, who assessed that they were successful in almost 89% of the conversations. In ca. 75% of the conversations by using mainly scaffolding questions, and only in around 14% by revealing the majority of the answer. The conversations in which confusions could not be resolved can still be useful, as they, for instance, can be used to train classifiers to determine when human intervention in such tutoring sessions is required."
        },
        {
            "heading": "5 Modeling Tutors with MATHDIAL",
            "text": "We focus our initial studies on MATHDIAL on the task of tutor response generation. Tutor response generation aims to model the teacher in a dialogue by generating follow-up turns to guide the student towards learning and solving the problem. In the following subsections, we compare different finetuned and prompted language models on the task and evaluate how much detailed information that can be given to the model, such as step-by-step solutions of the MWP, influence performance."
        },
        {
            "heading": "5.1 Training details",
            "text": "We use neural conditional language models that given a tutoring dialogue history uT1 , grounding\ninformation K, and a teacher move A, we wish to generate a continuation of the dialogue uT+1 \u2282 V\u2217. Here V\u2217 denotes all strings that can be constructed from the model vocabulary V using Kleene\u2019s closure. K is a string composed of information annotated in MATHDIAL , namely the MWP, step-bystep solution, and the students\u2019 solution attempt. We study locally-normalized models of the form\np\u03b8(uT+1 | uT1 ,K,A) = NT+1\u220f n=1 p\u03b8([uT+1]n | [uT+1]n\u221211 , u T 1 ,K,A),\nwhere \u03b8 denotes the parameters of the model and T is moved throughout the dialogue to evaluate each intermediate teacher turn. We either optimize these parameters by finetuning for 10 epochs or zero-shot prompting an LLM. When finetuning, we use an initial learning rate of 6.25e\u22125 and linear learning rate decay without warm-up, and optimize the negative log-likelihood of the ground-truth response using the AdamW optimizer (Loshchilov and Hutter, 2019). We experiment with state-of-the-art pretrained Transformer (Vaswani et al., 2017) models and make use of the checkpoints provided by the transformers library (Wolf et al., 2020). In particular, we finetune BART (Lewis et al., 2020), Flan-T5 (Chung et al., 2022) which is based on T5 (Raffel et al., 2020) and was finetuned on the instructionfollowing flan collection (Longpre et al., 2023), as well as OPT (Zhang et al., 2022). Finally, we zero-shot prompt ChatGPT (Brown et al., 2020).\nData split We split our data into a training split containing 80% of the conversations and a test set containing the remaining 20%. Around 60% of the problems in the test set are also found in the training data, where at least one conversation was\nbased on it, and therefore constitute our \u2018seen\u2019 split. The remaining 40% are unseen during training and test the ability of the model to generalize to new problems. The dataset split is published with the dataset.\nMetrics We assess our models using the sacrebleu (Post, 2018) implementation of BLEU (sBLEU) (Papineni et al., 2002), as well as BERTScore 3 (Zhang et al., 2020) between generated response (uT+1) and annotated response (u\u0302T+1) for each teacher response in the conversation. Furthermore, in line with previous works (Dziri et al., 2022; Daheim et al., 2023), we report BERTScore and the token level F1 (KF1) between generated utterance and math word problem as a proxy for faithfulness. However, we note that an increase in these metrics can be caused by an increase in overlap, which may also indicate more telling and can be undesirable. However, finding good evaluation metrics for assessing the faithfulness of dialogue tutors remains an open problem. Finally, we measure the Uptake of the generated response (Demszky et al., 2021).\nWe propose two evaluation metrics for end-toend tutoring, where a tutor model is evaluated interactively by using it to teach an LLM that simulates a student. Success@k measures the percentage of conversations where the student reaches the correct final answer at least once within the first k turns (equivalent of % solve rate in prior work). Telling@k measures the percentage of conversations where the teacher explicitly tells the final answer before the student has reached it on their own within the first k turns.\n3We use the deberta-large-mnli checkpoint"
        },
        {
            "heading": "6 Results",
            "text": ""
        },
        {
            "heading": "6.1 Tutor Response Generation",
            "text": "Table 3 shows our main results for the task of tutor response generation on MATHDIAL . A first general observation is that automatic metrics appear low when compared to state-of-the-art models on other dialogue data. This might be explained by two main challenges that tutoring models face: a high level of ambiguity when it comes to sound teaching strategies and complex problems that the models need be able to correctly assess. In contrast, the data that ground responses in other dialogue tasks often needs a lesser amount of interpretation.\nScaling models in terms of their parameter size is not directly reflected in improved metrics. This indicates that just using larger models might not be enough to build meaningful tutors on MATHDIAL . Still, as shown in BERTScore and lexical overlap between response and grounding information, smaller models appear to rely more on the grounding information and might paraphrase less which might make teaching less engaging for students. Instruction tuning seems to have a largely positive effect in tutoring, as well. This is exhibited by the improvements that Flan-T5 yields over T5.\nIn order to be used in real-world settings, dialogue tutoring models need to be able to generalize to new problems. However, we find that there is still a large gap in the performance of all finetuned models between seen and unseen problems. This indicates a clear need to build models that can generalize better. Uptake on the other hand is generally high and for different models even higher than the ground-truth annotations. Finally, finetuned models tend to outperform zero-shot prompted GPT in terms of automatic metrics but the validity of them for evaluating such models may be questioned."
        },
        {
            "heading": "6.2 Influence of grounding information",
            "text": "MATHDIAL provides a large set of annotations that can be used to ground the responses of dia-\nlogue tutors trained on it. Table 4 shows results obtained with Flan-T5780M when giving different information. The results show that the step-by-step solution is crucial for the model. Question and incorrect solution are not as crucial but are also often repeated by student or teacher throughout the dialogue. Future work can explore this information in more detail to improve tutoring models."
        },
        {
            "heading": "6.3 Human Evaluation",
            "text": "els on MATHDIAL increases their performance in terms of correctness and equitable tutoring.\nFinally, we conduct a human evaluation according to three criteria: 1) Coherence: how coherent the teacher\u2019s response is with respect to the preceding dialogue, 2) Correctness: whether it is in itself correct, and 3) Equitable tutoring. Equitable tutoring describes how well the model provides the student with room for exploring the problem and solution space. We use three expert annotators that each annotate n = 50 responses. We obtain agreements of \u03ba = 0.29, \u03ba = 0.69, and \u03ba = 0.34 for the three categories. We find that the ground-truth data that we have collected shows high scores in all three criteria which confirms its quality. Then, we find that small fine-tuned models perform much better in terms of correctness and equitable tutoring than a prompted large language model (ChatGPT), even though the latter is pretrained on much more data and has a significantly larger parameter count. This shows the importance of high-quality data for training meaningful tutors. The automatic metrics are only partially confirmed. For instance, FlanT53B is rated slightly better than Flan-T5780M in correctness despite lower automatic scores."
        },
        {
            "heading": "6.4 Interactive Evaluation of Dialogue Tutors",
            "text": "Good tutoring models need to maintain high quality not only when viewed per-utterance but especially over an entire conversation. In order to assess this, we use them to tutor an InstructGPT student and measure their success (Success@k), as well as the rate of telling (Telling@k). The tutor models are\nfind the model trained on MATHDIAL to have a similar success@5 rate with less telling.\nused as outlined in the previous subsections and the student model uses the same settings as during data collection. We compare our Flan-T5780M model with a simple baseline that repeatedly asks \u201cWhat is the next step?\" (NEXTSTEP), ChatGPT, and the ground-truth conversations.\nFigure 5 shows that NEXTSTEP has the lowest success rate, but never tells solutions by construction. ChatGPT, on the other hand, has a high success rate but also the highest rate of telling. This is a crucial shortcoming because high telling is counterproductive to effectively teach students. FlanT5780M achieves a balance between the two and shows a similar amount of telling as the ground truth.\nWe note that the gap in success rate between Flan-T5780M and ChatGPT, at least in the initial steps, stems mostly from longer problems, as is evident from Figure 6. Overall, no model can match the success rate of the ground-truth annotations. This indicates a large room for future improvements and research."
        },
        {
            "heading": "7 Conclusion",
            "text": "We introduce a new framework for semi-synthetic dialogue dataset collection. We use it to collect a pedagogically rich dataset for tutoring math word problems that follow equitable tutoring practices and learning sciences research on scaffolding student understanding, called MATHDIAL . Our dataset consists of ca. 3k tutoring conversations grounded in math word problems from GSM8k.\nWe benchmark open-source models on the task of tutor response generation and show that smaller models finetuned on our MATHDIAL can significantly surpass the performance of much larger prompted LLMs. Moreover, in our proposed interactive tutoring simulation, the finetuned model achieves similar student-solving success as prompted LLM while keeping the direct telling rate lower. Nevertheless, models still require better reasoning over student solutions and better generalization to unseen problems.\nOur dataset fills a crucial gap towards studying effective dialogue tutors at scale by providing a significantly larger amount of dialogues than other available corpora in one-on-one tutoring and provides a tough testbed towards better tutoring models. We hope that it can spark more research in this meaningful but understudied area of NLP."
        },
        {
            "heading": "8 Limitations",
            "text": "In this work, we used an LLM to simulate student confusion. However, we acknowledge that these models have a limited understanding of human learning and this is a key limitation in our dataset \u2013 certain kinds of student confusions may be under- or over-represented in our dataset. Future\nwork can focus on addressing this limitation.\nFurthermore, in our setup, teachers were interacting with an LLM role-playing as a student. However, it is possible that some teachers might have learned to interact with the student model in a different way than they would do in the classroom. Moreover, it is also possible that some teachers may have lost motivation when found out they are not interacting with real students, leading to lower data quality. In the future, we would like to explore solutions to build better LLM-based student models (Zhou et al., 2023).\nThe methodology to collect the dataset was instantiated just for the domain of math reasoning. The collection of additional domain-specific datasets is necessary to further generalize the effectiveness of our methodology.\nInspired by previous work in scaffolding, we acknowledge our focus is on a subset of common teaching moves. However, this does not cover all the goals of human tutors, such as meta-cognitive support or building rapport with a student. Moreover, text tutoring limits teachers\u2019 use of additional instructional practices such as drawings.\nFinally, measuring a student\u2019s immediate success in solving a problem does not capture all the aspects of student learning. From a learning perspective, focusing on and measuring long-term learning is desired. Therefore, even if students struggle to answer a specific problem correctly, teachers asking scaffolding questions requiring conceptual understanding offer even better promise for deeper, wider, and more long-term learning."
        },
        {
            "heading": "9 Acknowledgements",
            "text": "This project was made possible by an ETH AI Center Doctoral Fellowship to Jakub Macina with further support from the Asuera Stiftung and the ETH Zurich Foundation. Nico Daheim has received funding by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE. Mrinmaya Sachan acknowledges support from the Swiss National Science Foundation (Project No. 197155), a Responsible AI grant by the Haslerstiftung; and an ETH Grant (ETH-19 21-1)."
        },
        {
            "heading": "A Dataset statistics",
            "text": "For NCTE, uptake is calculated on the teacherstudent dialogue pairs while bigram entropy is calculated on all teacher utterances. For TalkMoves and TSCC, bigram entropy is calculated on all teacher utterances having more than three words, while uptake is calculated on teacher utterances immediately following student utterances if both have more than three words."
        },
        {
            "heading": "B Problem and confusion selection",
            "text": "While the problems in GSM8k are simple enough to be understood quickly by teachers, they remain challenging for students, who among others have to deal with equations or percentages. We follow the GSM8k reasoning format and prompt ChatGPT (gpt-3.5-turbo) with a 2-shot prompt. Given a prompt and a math word problem, we sample n reasoning paths ri solutions from the model. We parse the first numerical answer ai after the model\ngenerated \"####\" which represents the final result. Most of the generated outputs have this format and we discard all generations not following it. We sample N = 50 reasoning path candidates using the same settings as suggested by (Wang et al., 2023b). After sampling multiple reasoning pairs and corresponding answer pairs (ri, ai) we use a majority vote over ai which does not lead to a ground truth answer a: argmaxa \u2211n i=1 1(ai \u0338= a). We select problems with at most four solution steps. Since our initial experiments show the occurence of rounding errors, which related work finds to be more common in LLMs than humans (Frieder et al., 2023), we limit them by discarding confusions that are within 0.1 of the original solution. Moreover, to filter out other simple calculation errors which are not interesting from a learning standpoint we parse all the intermediate equations which are in the format << a\u00d7 b = c >> and use a calculator to check for inconsistencies.\nThe full prompt used is:\nQ: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?"
        },
        {
            "heading": "A: Natalia sold 48/2 = \u00ab48/2=24\u00bb24 clips",
            "text": "in May. Natalia sold 48+24 = \u00ab48+24=72\u00bb72 clips altogether in April and May. #### 72"
        },
        {
            "heading": "Q: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?",
            "text": ""
        },
        {
            "heading": "A: Weng earns 12/60 = \u00ab12/60=0.2\u00bb0.2 per minute. Working 50 minutes, she earned",
            "text": "0.2 x 50 = \u00ab0.2*50=10\u00bb10. #### 10\nOf the problems in the GSM8k dataset, 5684 problems were queried after eliminating problems with more than 5 steps in the solution. This yielded 2, 313 problems with at least one wrong solution. We then eliminated student solutions having fewer than 300 characters (having too few characters makes it harder to pinpoint where exactly the error occurred) or more than 500 characters (longer solutions require annotators to spend more time understanding the error), leaving us with 1, 379 wrong solutions. Finally, we eliminate problems where all 50 or 49 out of 50 proposed solutions have the same (wrong) final answer, leaving us with our final set of 1131 problems."
        },
        {
            "heading": "C Student model",
            "text": "C.1 Prompt\nWe use InstructGPT (text-davinci-003) with the following prompt using temperature sampling with T = 0.4 and no top-k truncation:\nStudent Persona: (STUDENT PERSONA)\\n\\n\nMath problem: (MATH PROBLEM)\\n\\n\nStudent solution: (STUDENT SOLUTION)\\n\\n\nContext: (STUDENT NAME) thinks their answer is correct. Only when the teacher provides several good reasoning questions, (STUDENT NAME) understands the problem and corrects the solution. (STUDENT NAME) can use a calculator and thus makes no calculation errors. Send EOM tag at the end of the student message.\\n\\n\n(DIALOGUE HISTORY)\nC.2 Student characteristics\nTo build a dataset that would reflect students of various backgrounds, we use numerous student names associated with their given pronouns. List of all student characteristics based on prior work studying misconceptions in learning algebra (Booth et al., 2017):\n\u2022 has a problem with understanding what steps or procedures are required to solve a problem.\n\u2022 has a problem with understanding underlying ideas and principles and a recognition of when to apply them.\n\u2022 struggle most with understanding what the problem is asking them to do.\n\u2022 has difficulty determining which pieces of information are relevant and which are irrelevant to solving the problem.\n\u2022 struggle to put the numbers in the correct order in the equation or determine the correct operation to use.\n\u2022 struggle to recognize the problem type and therefore do not know what strategy to use to solve it.\nC.3 Common error cases\nWe manually screened some conversations and teacher feedback to understand common error cases of student model. The most common problem among them was the occurence of simple arithmetic errors (e.g. 7-2=9) and inconsistent student behaviour (e.g. student returning to the incorrect answer after figuring out the correct one in the previous utterance). These errors are captured in the teacher quality Likert scale rating of student behaviour. We acknowledge further analysis is needed to better understand the fine-grained student model behavior on problems with different numbers of steps e.g. by cognitive task analysis (Koedinger and McLaughlin, 2016)."
        },
        {
            "heading": "D Data collection interface",
            "text": "We use Prolific for data collection and hire annotators with teaching experience. To ensure the data quality we filter only annotators with 100% completion rate with more than 500 total submissions. All the payments to the annotators exceeded the US federal minimum wage and the final batch of annotators were paid the equivalent of $12/hour. The data collection interface is shown in Figure 7. Annotators were restricted to having a maximum of five conversations in one annotation session. One conversation takes ca. 6 minutes. Data collection took place over a period of 2 months.\nD.1 Annotation pipeline\nFor each annotator, we randomly assign a student and math word problem. Teachers were instructed to first analyze the student homework solution and then start the conversation to scaffold student problem understanding. Post-conversation questionnaire is filled out by teachers to rate the conversation and get feedback on the type of student error.\nComparing solutions As shown in Figure 8, the teacher first analyzes and compares the correct solution with the incorrect student solution (student confusion). The teacher marks the exact line of a first student error and categorizes the problem into the following categories:\n\u2022 Reached correct solution but proceeded further\n\u2022 Extra quantity or Missing quantity\n\u2022 Unit conversion error\n\u2022 Calculation error easily solved by a calculator\n\u2022 Missing / Wrong factual knowledge\n\u2022 Misunderstanding of a question\n\u2022 None of the above\nTutoring conversation Next, the teacher has a conversation (see Figure 7) with a student and uses scaffolding moves to help the student understand the problem. The conversation ends when the student correctly solves the problem or if the total conversation time exceeds 10 minutes.\nPost conversation questionnaire Teacher fills the post conversation questionnaire as shown in Figure 9.\nD.2 Annotators training phase We let annotators read best practices on how to have a productive conversation with students (cf. Section D.3 and D.4) and tested them on their understanding of our task afterwards. We started the data annotation with all the annotators able to successfully pass the test. Moreover, to improve the training phase we manually checked several conversations by each annotator in terms of the quality and usage of diverse scaffolding questions.\nD.3 Annotation Guidelines\nTeachers were instructed to have a one-on-one tutoring session with different 6th-grade students. They were told that students received a math word problem for homework and submitted their solutions beforehand. In a tutoring conversation, teachers were asked to go through the student\u2019s solution and try to let the student understand using a series of sensemaking questions to support student reasoning and learning. Specifically, they were instructed to not just correct student solutions by telling what\u2019s correct/incorrect, but to give students the opportunity to explore the problem with a focus on core aspects, such as their chosen strategy. However, as the goal is to focus on conceptual errors, they were allowed to let students use calculators or correct their arithmetic mistakes.\nD.4 Teacher moves taxonomy\nTable 2 refers to the details of teacher moves used during annotation. In summary, Focus comprises of all conversation elements that direct the student towards the solution without actually giving out any of the solution, while Probing attempts to develop reasoning skills and world knowledge relevant to the problem, but not necessarily specific to the given problem. Telling is giving out parts of the solution, either calculations or strategy or both. All other conversational elements, including trying to understand what the student has already tried, fall under Generic.\nMost importantly, scaffolding questions that are productive for long-term learning are Focus and Probing. On the other hand, Telling represents\ngiving out the partial or full answer to the student and should be mostly used when a student is stuck.\nD.5 Background for teacher moves\nScaffolding (Reiser, 2004; Anghileri, 2006) assists students to succeed in tasks that would otherwise be complex and differentiates between guidance (e.g. decomposing problem, clarifying) from cognitive activation (e.g. causing cognitive conflicts, activating prior knowledge (Lim\u00f3n, 2001)). The effective teacher moves to scaffold students\u2019 understanding have been studied extensively by analyzing and annotating real human tutoring conversations (Nye et al., 2014; VanLehn, 2011). Experienced teachers can through natural language guide students\u2019 focus and uncover misconceptions (Nye et al., 2014). The teacher moves in the form of scaffolding to support student understanding by asking open-ended questions, activating their prior knowledge, or causing cognitive conflicts (Lim\u00f3n, 2001). A teacher asking scaffolding questions provides learning opportunities for students to actively construct their knowledge. However, at the same time asking only difficult questions could lead to a loss of learner motivation and potentially the end of the dialogue. On the other hand, only constantly revealing answers does not lead to long-term learning.\nD.6 Postprocessing\nAs we are interested in real educational use cases for our tutoring system, we apply a safety filter to filter out conversations with any sensitive content. In particular, we use the Perspective API4 to filter out conversations containing toxic content (<1%).\nD.7 Initial pilots\nWe initially explored two additional approaches of data collection: i) human-human conversations, and ii) synthetic generation by LLMs. The framework we used in the final data collection enables us to scalably create data since we are only reliant on one user who can quickly create entire conversations with the LLM, taking ca. 6 minutes per 7+ turn conversation. We found this more efficient and performant than both human-human conversations and synthetic data generation. Specifically, the human-to-human collection is too time-consuming (on average 15 minutes per conversation in our pilot experiments) and requires waiting times to synchronously connect participants (Choi et al., 2018), and synthetic generation has proven to be error-prone (see example in Figure 10); for example, models fail to understand student solutions and themselves make arithmetic errors that are not expected from teachers.\nE Interactive evaluation of tutoring\nThe student model in all 3 cases is an InstructGPT model (text-davinci-003) as defined in Section C.1, with the student name fixed to \u201cKayla\". The first utterance of the teacher is hardcoded to \u201cHi Kayla, could you walk me through your solution?\". For Flan-T5780M teacher model decoding, we used sampling without a beam search. For the ChatGPT teacher model (gpt-3.5-turbo), the following prompt is used: A tutor and a student work together to solve the following math word problem.\\n\nMath problem: (MATH PROBLEM)\\n\nThe correct solution is as follows: (CORRECT SOLUTION)\\n\nYour role is tutor. The tutor is a soft-spoken empathetic person who dislikes giving out direct answers to students and instead likes to answer with other questions that would help the student understand the concepts\n4https://perspectiveapi.com\nso students can solve the problem themselves."
        },
        {
            "heading": "F Human Evaluation Protocol",
            "text": "The following dimensions were rated by annotators:\n\u2022 Coherence - \u201dThe response naturally follows up on the previous utterance and context and has no logical conflicts with the context.\u201d\n\u2022 Correctness - \u201dThe response is factually and mathematically correct and respects the learning concepts being taught.\u201d\n\u2022 Equitable tutoring - \u201dThe response gives a learning opportunity for the student by providing space for reflection, explanation, pointing to follow-up challenge, or engaging the student in other ways.\nWe use a 3-point Likert scale ranging from 1 (poor) and 3 (very good) for coherence and equitable tutoring and a binary scale for correctness.\nChatGPT prompt is the same as in the interactive tutoring scenario (Section E) with an additional section containing student solution."
        }
    ],
    "title": "MATHDIAL: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems",
    "year": 2023
}