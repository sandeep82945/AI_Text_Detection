{
    "abstractText": "Retrieval-augmented language models show promise in addressing issues like outdated information and hallucinations in language models (LMs). However, current research faces two main problems: 1) determining what information to retrieve, and 2) effectively combining retrieved information during generation. We argue that valuable retrieved information should not only be related to the current source text but also consider the future target text, given the nature of LMs that model future tokens. Moreover, we propose that aggregation using latent variables derived from a compact latent space is more efficient than utilizing explicit raw text, which is limited by context length and susceptible to noise. Therefore, we introduce RegaVAE, a retrieval-augmented language model built upon the variational auto-encoder (VAE). It encodes the text corpus into a latent space, capturing current and future information from both source and target text. Additionally, we leverage the VAE to initialize the latent space and adopt the probabilistic form of the retrieval generation paradigm by expanding the Gaussian prior distribution into a Gaussian mixture distribution. Theoretical analysis provides an optimizable upper bound for RegaVAE. Experimental results on various datasets demonstrate significant improvements in text generation quality and hallucination removal. Our codes is released in the link1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jingcheng Deng"
        },
        {
            "affiliations": [],
            "name": "Liang Pang"
        },
        {
            "affiliations": [],
            "name": "Huawei Shen"
        },
        {
            "affiliations": [],
            "name": "Xueqi Cheng"
        }
    ],
    "id": "SP:ec19f3eb81925ec7ace63d547a3e4e2c91bfff8d",
    "references": [
        {
            "authors": [
                "Saffron Huang",
                "Loren Maggiore",
                "Chris Jones",
                "Albin Cassirer",
                "Andy Brock",
                "Michela Paganini",
                "Geoffrey Irving",
                "Oriol Vinyals",
                "Simon Osindero",
                "Karen Simonyan",
                "Jack W. Rae",
                "Erich Elsen",
                "Laurent Sifre"
            ],
            "title": "Improving language models by retrieving",
            "year": 2022
        },
        {
            "authors": [
                "Yuri Burda",
                "Roger B. Grosse",
                "Ruslan Salakhutdinov."
            ],
            "title": "Importance weighted autoencoders",
            "venue": "4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.",
            "year": 2016
        },
        {
            "authors": [
                "Nat Dilokthanakul",
                "Pedro A.M. Mediano",
                "Marta Garnelo",
                "Matthew C.H. Lee",
                "Hugh Salimbeni",
                "Kai Arulkumaran",
                "Murray Shanahan."
            ],
            "title": "Deep unsupervised clustering with gaussian mixture variational autoencoders",
            "venue": "CoRR, abs/1611.02648.",
            "year": 2016
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann N. Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers,",
            "year": 2018
        },
        {
            "authors": [
                "Le Fang",
                "Tao Zeng",
                "Chaochun Liu",
                "Liefeng Bo",
                "Wen Dong",
                "Changyou Chen."
            ],
            "title": "Transformerbased conditional variational autoencoder for controllable story generation",
            "venue": "arXiv preprint arXiv:2101.00828.",
            "year": 2021
        },
        {
            "authors": [
                "Hao Fu",
                "Chunyuan Li",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Asli Celikyilmaz",
                "Lawrence Carin."
            ],
            "title": "Cyclical annealing schedule: A simple approach to mitigating KL vanishing",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the",
            "year": 2019
        },
        {
            "authors": [
                "Michael R. Glass",
                "Gaetano Rossiello",
                "Md. Faisal Mahbub Chowdhury",
                "Ankita Naik",
                "Pengshan Cai",
                "Alfio Gliozzo."
            ],
            "title": "Re2g: Retrieve, rerank, generate",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Ming-Wei Chang."
            ],
            "title": "REALM: retrievalaugmented language model pre-training",
            "venue": "CoRR, abs/2002.08909.",
            "year": 2020
        },
        {
            "authors": [
                "Junxian He",
                "Daniel Spokoyny",
                "Graham Neubig",
                "Taylor Berg-Kirkpatrick."
            ],
            "title": "Lagging inference networks and posterior collapse in variational autoencoders",
            "venue": "7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,",
            "year": 2019
        },
        {
            "authors": [
                "Jinyi Hu",
                "Xiaoyuan Yi",
                "Wenhao Li",
                "Maosong Sun",
                "Xing Xie."
            ],
            "title": "Fuse it more deeply! A variational transformer with layer-wise latent variable inference for text generation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the",
            "year": 2022
        },
        {
            "authors": [
                "Gautier Izacard",
                "Edouard Grave."
            ],
            "title": "Leveraging passage retrieval with generative models for open domain question answering",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Jeff Johnson",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou."
            ],
            "title": "Billion-scale similarity search with gpus",
            "venue": "IEEE Trans. Big Data, 7(3):535\u2013547.",
            "year": 2021
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B. Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei."
            ],
            "title": "Scaling laws for neural language models",
            "venue": "CoRR, abs/2001.08361.",
            "year": 2020
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick S.H. Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
            "year": 2020
        },
        {
            "authors": [
                "Urvashi Khandelwal",
                "Omer Levy",
                "Dan Jurafsky",
                "Luke Zettlemoyer",
                "Mike Lewis."
            ],
            "title": "Generalization through memorization: Nearest neighbor language models",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,",
            "year": 2020
        },
        {
            "authors": [
                "Patrick S.H. Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Douwe Kiela"
            ],
            "title": "Retrieval-augmented generation",
            "year": 2020
        },
        {
            "authors": [
                "Chunyuan Li",
                "Xiang Gao",
                "Yuan Li",
                "Baolin Peng",
                "Xiujun Li",
                "Yizhe Zhang",
                "Jianfeng Gao."
            ],
            "title": "Optimus: Organizing sentences via pre-trained modeling of a latent space",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-",
            "year": 2020
        },
        {
            "authors": [
                "Huayang Li",
                "Yixuan Su",
                "Deng Cai",
                "Yan Wang",
                "Lemao Liu."
            ],
            "title": "A survey on retrieval-augmented text generation",
            "venue": "CoRR, abs/2202.01110.",
            "year": 2022
        },
        {
            "authors": [
                "Jiwei Li",
                "Michel Galley",
                "Chris Brockett",
                "Jianfeng Gao",
                "Bill Dolan."
            ],
            "title": "A diversity-promoting objective function for neural conversation models",
            "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computa-",
            "year": 2016
        },
        {
            "authors": [
                "Potsawee Manakul",
                "Adian Liusie",
                "Mark J.F. Gales"
            ],
            "title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
            "year": 2023
        },
        {
            "authors": [
                "Gary Marcus."
            ],
            "title": "The next decade in AI: four steps towards robust artificial intelligence",
            "venue": "CoRR, abs/2002.06177.",
            "year": 2020
        },
        {
            "authors": [
                "Shamima Mithun",
                "Leila Kosseim",
                "Prasad Perera."
            ],
            "title": "Discrepancy between automatic and manual evaluation of summaries",
            "venue": "Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization@NACCL-HLT 2012,",
            "year": 2012
        },
        {
            "authors": [
                "Liang Pang",
                "Yanyan Lan",
                "Xueqi Cheng."
            ],
            "title": "Match-ignition: Plugging pagerank into transformer for long-form text matching",
            "venue": "CIKM \u201921: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event, Queensland,",
            "year": 2021
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Ohad Rubin",
                "Jonathan Herzig",
                "Jonathan Berant."
            ],
            "title": "Learning to retrieve prompts for in-context learning",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        },
        {
            "authors": [
                "Devendra Singh Sachan",
                "Siva Reddy",
                "William L. Hamilton",
                "Chris Dyer",
                "Dani Yogatama."
            ],
            "title": "End-toend training of multi-document reader and retriever for open-domain question answering",
            "venue": "Advances in Neural Information Processing Systems 34: An-",
            "year": 2021
        },
        {
            "authors": [
                "Weijia Shi",
                "Sewon Min",
                "Michihiro Yasunaga",
                "Minjoon Seo",
                "Rich James",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Wen-tau Yih."
            ],
            "title": "REPLUG: retrieval-augmented black-box language models",
            "venue": "CoRR, abs/2301.12652.",
            "year": 2023
        },
        {
            "authors": [
                "Haoqin Tu",
                "Zhongliang Yang",
                "Jinshuai Yang",
                "Yongfeng Huang."
            ],
            "title": "Adavae: Exploring adaptive gpt-2s in variational auto-encoders for language modeling",
            "venue": "arXiv preprint arXiv:2205.05862.",
            "year": 2022
        },
        {
            "authors": [
                "Tianming Wang",
                "Xiaojun Wan."
            ],
            "title": "T-CVAE: transformer-based conditioned variational autoencoder for story completion",
            "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, Au-",
            "year": 2019
        },
        {
            "authors": [
                "Shicheng Xu",
                "Liang Pang",
                "Huawei Shen",
                "Xueqi Cheng."
            ],
            "title": "BERM: training the balanced and extractable representation for matching to improve generalization ability of dense retrieval",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for",
            "year": 2023
        },
        {
            "authors": [
                "Zichao Yang",
                "Zhiting Hu",
                "Ruslan Salakhutdinov",
                "Taylor Berg-Kirkpatrick."
            ],
            "title": "Improved variational autoencoders for text modeling using dilated convolutions",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Syd-",
            "year": 2017
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with BERT",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-",
            "year": 2020
        },
        {
            "authors": [
                "Yaoming Zhu",
                "Sidi Lu",
                "Lei Zheng",
                "Jiaxian Guo",
                "Weinan Zhang",
                "Jun Wang",
                "Yong Yu."
            ],
            "title": "Texygen: A benchmarking platform for text generation models",
            "venue": "The 41st International ACM SIGIR Conference on",
            "year": 2018
        },
        {
            "authors": [
                "Yunchang Zhu",
                "Liang Pang",
                "Yanyan Lan",
                "Huawei Shen",
                "Xueqi Cheng."
            ],
            "title": "Adaptive information seeking for open-domain question answering",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Vir-",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Language models (LMs) have achieved state-ofthe-art performance on many NLP tasks (Zhu et al., 2021; Pang et al., 2021), which reveals that they store a large amount of world knowledge as implicit parameters. While this development is exciting, LMs still suffer from some problems (Li et al., 2022): 1) performance and model parameter size follow a power law relationship (Kaplan\n*Corresponding Author 1https://github.com/TrustedLLM/RegaVAE\net al., 2020), which results in model parameters having to grow exponentially in order to gain more world knowledge; 2) difficulty in adjusting for timesensitive knowledge (Lewis et al., 2020); 3) may produce \"fact hallucination\" problem (Guu et al., 2020; Marcus, 2020).\nRecently, the advent of retrieval-augmented text generation has emerged as a novel paradigm aimed at addressing these pertinent issues (Borgeaud et al., 2022; Li et al., 2022; Shi et al., 2023). Compared to generative-only models, this paradigm not only explicitly exploits similar texts to generate more fluent sentences but also leverages expertise to generate difficult responses. Nonetheless, we contend that there are two primary challenges associated with current retrieval-augmented language models. Firstly, not only current semantic information, but also future semantic information need to be considered during retrieval. Previous studies (Khandelwal et al., 2020; Guu et al., 2020; Lewis et al., 2020)\neither directly use the entire text as key and value parts at the same time, and then use cosine similarity (Xu et al., 2023), TF-IDF and other indicators to search, which leads to the value part is only similar to the source text (query), and does not necessarily serve the best for generator. Another way is to divide a piece of text into two parts, where the first part and the second part are regarded as current information and future information, such as RETRO (Borgeaud et al., 2022). However, RETRO adds future information to value part, but ignores the future information in query and key, which leads to the fact that candidate documents with high similarity do not necessarily contain future information that can help the generator. Secondly, explicitly aggregating retrieved documents and source texts is limited by the length of the model input and introduces too much noise. Implicit aggregation is inefficient in irregular embedding spaces, and retrieval vectors are not generalizable.\nTo address the above challenges, we design RegaVAE, a Retrieval-augmented language model based on gaussian mixture Variational AutoEncoder. Unlike previous methods that directly encode unlabeled corpora (Karpukhin et al., 2020; Lewis et al., 2020) or only adding future information to the value part (Borgeaud et al., 2022), as shown in Tab. 1, our model considers future information through a latent space, given an x, we decode it into a y using a conditional VAE, which ensures that the latent variables contain information from both source and target data. In addition, in order to implicitly aggregate the retrieved documents and source texts, we also use the probabilistic form of the retrieval generation paradigm to theoretically extend the prior Gaussian distribution to a Gaussian mixture distribution. This allows the latent space to satisfy continuity and uniformity, and the latent vector after aggregating retrieved documents and source text has better representation ability. Tab. 1 summarizes the differences between RegaVAE and existing representative methods. Overall, our contributions are as follows:\n\u2022 We propose a retrieval method that implicitly combines current and future information, which introduces future information into the query, key, and value parts at the same time, so that the higher the document similarity, the more helpful it is for the generator.\n\u2022 We integrate the VAE and retrieval generation probabilistic framework to efficiently aggre-\ngate retrieval information into the generation process. Furthermore, we derive an upper bound on the optimization of this framework.\n\u2022 Experiments have shown that RegaVAE is competitive in generating quality, generating diversity, and eliminating hallucinations."
        },
        {
            "heading": "2 Related Work",
            "text": "We classify related studies into two categories, explicit aggregation and implicit aggregation, according to the way the retrieved documents and source text are aggregated. Explicit aggregation refers to concatenating retrieved documents directly into source text to construct augmented input. Implicit aggregation refers to adding retrieved documents to the generator in the form of vectors or distributions.\nExplicit Aggregation Guu et al. (2020) proposed an end-to-end framework REALM that achieves state-of-the-art performance on three open-domain QA. A representative work is RAG (Lewis et al., 2020), which first uses DPR (Karpukhin et al., 2020) to retrieve relevant documents, and then links relevant documents with the source text for sequence-to-sequence generation. Different from RAG and REALM, Rubin et al. (2022) proposed EPR, which is a method for retrieving prompts and can improve the effect of prompts. Re2G (Glass et al., 2022) is an enhanced version of RAG, which improves the quality of retrieved documents by integrating multiple retrieval methods. Explicit aggregation is simple and effective, but it suffers from the limitation of the input length of the language model and cannot fully utilize the large number of retrieved documents. In addition, it is easy to introduce noise, making the model performance unstable. Unlike these methods, our model implicitly aggregates retrieved documents into the generation process.\nImplicit Aggregation FiD (Izacard and Grave, 2021) uses a DPR to retrieve candidate documents, and then splices and encodes the candidate documents with the source text, and inputs them into the generator in the form of vectors. EMDR2 (Sachan et al., 2021) is similar to FiD, and it provides an end-to-end framework to train both the retriever and the generator. However, the query, key and value parts of FiD and EMDR2 do not contain future information, which will cause the value part to be similar to the query part and not conducive to the generation of future tokens. RETRO (Borgeaud\net al., 2022) and KNN-LM (Khandelwal et al., 2020) set key and value parts as a piece of text, and added the continuation and the next token of this text in value part, respectively. However, they only calculate the similarity between the query and key while ignoring future information in value part, resulting in high similarity documents containing future information that may not necessarily help the generator. Our model sets both key and value parts as latent variables of a piece of text and its future continuation, and the query encoded by the VAE encoder also contains future information, so future information is also taken into account when calculating the similarity between query and key, making up for the shortcomings of previous studies."
        },
        {
            "heading": "3 Methodology",
            "text": "Most text generation tasks can be formulated as a mapping from a source text x to a target text y : y = f(x), while retrieval-augmented text generation can be further formulated as: y = f(x, r), where r is the relevant document retrieved based on x. Specifically, this approach generally encompasses the utilization of a retriever denoted as R and a generator denoted as G. The retriever R obtains r from the retrieval source S by the retrieval metric D and x. Then r and x are fed into G to obtain y through a predefined integration method I .\nCommonly used retrieval indexes D include cosine similarity, TF-IDF, etc. This paradigm can also be expressed in probabilistic form:\np(y|x) = \u2211\nr\u2208top-k(p(\u00b7|x))\np(y|x, r)p(r|x). (1)\nNext, the framework of RegaVAE is introduced, which consists of three steps. Firstly, in order to construct a compact space, we introduce the VAE structure. Since transformers based on VAE all suffer from the posterior collapse (Fu et al., 2019), we follow a previous study (Hu et al., 2022) which combines low-rank tensor products for latent variables and decoders (see \u00a7 3.1 and step 1 in Fig. 1). Secondly, to introduce retrieval information into the latent space, we first introduce how the retrieval library is constructed (see step 2 in Fig. 1), and then replace the prior Gaussian distribution in the original VAE with a Gaussian mixture distribution to derive RegaVAE (see step 3 in Fig. 1). This allows for deep aggregation of retrieved and input documents and simultaneously incorporates future information into query, key and value parts, which helps to generate more fluent sentences (see \u00a7 3.2) . Finally, to train RegaVAE, we derive an optimizable upper bound on the loss function for unclosed solutions (see \u00a7 3.3). Fig. 1 shows the whole framework diagram."
        },
        {
            "heading": "3.1 Introduce Retrieval Information into Latent Space",
            "text": "We consider using the VAE structure to make the space compact and continuous. As a kind of generative model, VAE estimates the intractable data distribution p(x) by deriving and maximizing its Evidence Lower BOund (ELBO) as:\nlog p(x) \u2265 LELBO = Eq\u03d5(z|x)[log p\u03b8(x|z)]\u2212KL(q\u03d5(z|x)||p(z)), (2)\nwhere z is the latent variable. p(z) and p(z|x) is the prior and posterior distribution of z, respectively. q\u03d5(z|x) and p\u03b8(x|z) represent Encoder and Decoder. \u03b8 and \u03d5 are corresponding parameters.\nDue to the power of the decoder, transformers based on VAE usually have the problem of posterior collapse. According to Hu et al. (2022), we use a low-rank tensor product in the l-th layer of the model:\nv\u0303 (l) i = ( r\u2211 j=1 W (l,j)v v (l) i ) \u25e6 ( r\u2211 j=1 W (l,j)z zl), (3)\nwhere zl and v(l) represent latent variable and hidden variable of l-th layer respectively. v(l)i represents the hidden vector of the i-th token in l-th layer. r is a hyper-parameter, and \u25e6 means element-wise multiplication. Wv and Wz are learnable parameters which are shared across all positions (i) but not shared with l-th layer.\nIn order not to introduce additional data, we use the training set as the data for training VAE. By optimizing ELBO, each sample is encoded into the latent space and then restored by the decoder to obtain a compact latent space."
        },
        {
            "heading": "3.2 Build the RegaVAE Model",
            "text": "Build Retrieval Database With a compact latent space, we use an encoder to encode x and r from S into the latent space. The latent variables of x and r are denoted by zx and zr, respectively. Then we store zr as key and value parts in the retrieval database. Given a query zx, we compute the inner product of it and zr to obtain the similarity.\nD(zx, zri ) = cos(z x, zri ), (4)\nwhere zri \u223c N(\u00b5i, \u03c32i ) represents the latent vector of the i-th retrieved sample in S. \u00b5i and \u03c32i are the corresponding mean and standard deviation, respectively. Since our framework is trained endto-end, the parameters of the encoder change with\neach training step, resulting in changes in the latent space. Considering that it is impractical to update the retrieval database in real-time, and previous work (Guu et al., 2020) has shown the practicality of updating the index intermittently during training, we follow this approach and update the index of retrieval database every fixed number of training steps.\nAggregate Retrieved Information Inspired by the retrieval-generated text generation paradigm, we assume y is influenced by latent variables zx and zr. To obtain the ELBO of RegaVAE, we first model log p(y) as:\nlog p(y) = log \u222b\u222b p(y, zr, zx)dzrdzx\n\u2265 \u222b\u222b log p(y, zr, zx)dzrdzx\n= \u222b\u222b log q(zr, zx|x) log p(y, z r, zx)\nlog q(zr, zx|x) dzrdzx\n= Eq(zr,zx|x) log[ p(y, zr, zx)\nq(zr, zx|x) ].\n(5) From the first step, the Jensen inequality can be used to transform to the second step, and then the expression of the desired form can be obtained. According to Bayes formula:\np(y, zr, zx) = p(y|zr, zx)p(zr, zx). (6)\nSubstituting Eq. 6 into Eq. 5:\nEq(zr,zx|x) log[ p(y, zr, zx)\nq(zr, zx|x) ]\n= Eq(zr,zx|x) log[ p(y|zr, zx)p(zr, zx)\nq(zr, zx|x) ]\n= Eq(zr,zx|y)[log p(y|zr, zx)] \u2212KL(q(zr, zx|x)||p(zr, zx)),\n(7)\nwhere KL stands for calculating the KL divergence between two distributions. Eq. 7 is the ELBO of RegaVAE. At this point, Eq. 7 and Eq. 2 have the same form, but the latent variable z is replaced by zx and zr. Since each zri follows a Gaussian distribution, we consider using a Gaussian mixture distribution to combine zx and zr. So q(zr, zx|x) can be expressed as:\nq(zr, zx|x) = w0q(zx|x) + n\u2211\ni=1\nwiq(z r i |x), (8)\nwhere n represents the number of retrieved documents.\nwi = softmax(D(z x, zri )), (9)\nwhere \u2211n\ni=0wi = 1 makes q(z r, zx|x) satisfy the\nrequirement of Gaussian mixture distribution. So far, we have obtained the theoretical framework for introducing retrieval information in latent space."
        },
        {
            "heading": "3.3 Training RegaVAE",
            "text": "We can optimize RegaVAE by optimizing Eq. 7. In the KL divergence term of Eq. 7, the closed-form solution cannot be obtained because the two distributions are mixed Gaussian distributions. Therefore, we continue to use previous research (Dilokthanakul et al., 2016), that is, to optimize its upper bound. First we assume two Gaussian mixture distributions as:\np = \u2211n\ni=1 \u03c0igi, p\u0302 = \u2211n i=1 \u03c0\u0302ig\u0302. (10)\nThe KL divergence between them can be expressed as:\nKL(p||p\u0302) = \u222b ( \u2211 i \u03c0igi) log \u2211 i \u03c0igi\u2211 i \u03c0\u0302ig\u0302i\n\u2264 \u222b \u2211\ni \u03c0igi log \u03c0igi \u03c0\u0302ig\u0302i\n= \u2211\ni \u03c0i log \u03c0i \u03c0\u0302i\n+ \u2211\ni \u03c0i\n\u222b gi log\ngi g\u0302i\n= KL(\u03c0||\u03c0\u0302) + \u2211\ni \u03c0iKL(gi||g\u0302i).\n(11)\nIn the variational distribution q(zr, zx|x), the trainable parameter is only wi and q(zx|x). And the prior distribution p(zr, zx) is defined as:\np(zr, zx) = w\u03020p(z x) + \u2211n i=1 w\u0302ip(z r i ), (12)\nwhere zx \u223c N(0, 1) and zri \u223c N(0, 1). So the upper bound for the KL term in Eq. 7 can become:\nKL(q(zr, zx|x)||p(zr, zx)) \u2264 \u2211n\ni=0 KL(wi||w\u0302i)\n+ KL(q(zx|x)||N(0, I)) + C, (13) where C is a constant that has nothing to do with model parameter updates. We do not update the retrieval library in real time, but regularly update it according to the number of training steps. In this setup, wi is constant, so Eq. 13 becomes:\nKL(q(zr, zx|x)||p(zr, zx)) \u2264 KL(q(zx|x)||N(0, 1)) + C.\n(14)\nSubstituting Eq 14 into Eq 7, we can get the final optimized loss function:\nL = Eq(zr,zx|y)[log p(y|zr, zx)] \u2212KL(q(zx|x)||N(0, 1)).\n(15)\nEq. 15 can be regarded as an optimizable upper bound of Eq. 8. When given a dataset, we first encode the source text to obtain a retrieval database. The top-k documents are then retrieved for each x separately. Then the corresponding latent variables zx and zr are aggregated in the form of Gaussian mixture distribution and then input into G to obtain the output. Finally, we use Eq. 15 to train RegaVAE."
        },
        {
            "heading": "4 Experiment",
            "text": "This section provides the experimental datasets, experimental settings, and experimental results."
        },
        {
            "heading": "4.1 Datasets",
            "text": "For experiments, we employ three datasets, namely Yelp (Yang et al., 2017), Yahoo (He et al., 2019) and WritingPrompts (WP) (Fan et al., 2018). As in previous studies (Hu et al., 2022), due to the limitation of computing resources, we adopt the methodology established in previous research and sample 100,000 data instances from the training set of Yelp and Yahoo for model training. This consistent approach ensures a fair and equitable basis for comparison across the evaluated models."
        },
        {
            "heading": "4.2 Metrics",
            "text": "Generation Quality In the context of the text generation task, we present the evaluation metrics of perplexity (PPL), Self-BLEU (Zhu et al., 2018), Dist2 (Li et al., 2016), and Activation Units (AU) (Burda et al., 2016). For the WritingPrompts, in addition to PPL, we also report the metrics of BLEU (Papineni et al., 2002), Rouge-1, Rouge-2, RougeL (Mithun et al., 2012), and BERTScore (Zhang et al., 2020).\nHallucination We use SelfCheckGPT (Manakul et al., 2023) to detect hallucinations produced by the model. There are four indicators in total, namely SBERT, SQA, San and S m n . The higher their value, the more likely the model is hallucinating."
        },
        {
            "heading": "4.3 Experiment Settings",
            "text": "We have chosen two distinct categories of models as our baselines. The first category comprises transformers based on VAE, and the second category consists of retrieval-generated models. These baselines provide us with a comprehensive framework for evaluating and contrasting different approaches.\nTransformers based on VAE For a comprehensive comparison, we choose Optimus (Li et al., 2020) and ADAVAE (Tu et al., 2022) as the baseline models, along with four distinct paradigms: Embed (Li et al., 2020), Memory (Fang et al., 2021), Softmax (Wang and Wan, 2019) and DELLA (Hu et al., 2022). Optimus is a large-scale model based on VAE that utilizes a pre-trained BERT model as its encoder and a pre-trained GPT2 model as its decoder. In order to ensure the fairness of the evaluation, RegaVAE uses the same pre-trained language model as Embed, Memory, Softmax and DELLA. This selection facilitates a rigorous and unbiased comparative analysis across these models.\nRetrieval-augmented Language Model According to the division method of related work, we select representative works from different categories of retrieval-augmented language models as baselines. Specifically, RAG, FiD, and RETRO represent models with explicit aggregation, implicit aggregation without future information, and implicit aggregation with only future information in value part, respectively.\nOur Model Consistent with prior research, we adopt the GPT2 model as the underlying backbone network for our experimentation. The dimension of\nthe hidden variable is set to 32, and KL annealing (Fu et al., 2019) is implemented to mitigate the issue of KL term disappearance. The learning rate is fixed at 5\u00d710\u22125 to ensure stable training. Our training procedure entails an initial 10 epoch training phase on the original DELLA model to establish a robust initial VAE space. Subsequently, we conduct approximately fifteen epochs of training on the RegaVAE model until it achieves convergence. To make the training process more efficient, we precomputed document embeddings for the training dataset and created a FAISS index (Johnson et al., 2021) for fast similarity searches. We use the bert_score library 2 to calculate the BERTScore for our models and baselines."
        },
        {
            "heading": "4.4 Automatic Evaluation",
            "text": "Text Generation Tab. 2 presents the results attained by RegaVAE model on text generation datasets. Compared to the three baseline models for retrieval augmentation, our model achieves substantial improvements in all metrics, and performs particularly well in generating quality metrics. The enhanced PPL, Self-BLEU, and Dist2 scores demonstrate that latent variables, which contain both source and target information, combined\n2https://github.com/Tiiiger/bert_score\nwith the extension to Gaussian mixture priors, effectively enhances the fluency and diversity of the generated text. This empirical validation corroborates the theoretical strength of our model.\nNotably, in comparison to the transformer-based VAE model, RegaVAE with retrieval demonstrates superior performance in terms of both generative diversity and quality. This enhancement can be attributed to the utilization of a Gaussian mixture distribution, which offers the ability to capture multimodal distributions more effectively than a single Gaussian distribution. Leveraging the source data, RegaVAE retrieves auxiliary latent variables that facilitate the generation of the target data, thereby yielding improved text generation outcomes. Furthermore, the significant improvement in the AU value indicates that the aggregation we employ positively contributes to the generative process of decoder. This alleviates the problem of collapsing at the rear of the model to a considerable extent.\nHallucination Evaluation We evaluate hallucinations of RegaVAE on the Yelp dataset. Specifically, we sample the text generated by the same latent variable three times, and then feed the sampling results into SelfCheckGPT to obtain evaluation scores. The results are shown in the Tab. 4. From the experimental results, it can be seen that the text generated by RegaVAE is the least hallucinatory compared with other models."
        },
        {
            "heading": "4.5 Human Evaluation",
            "text": "In addition to automated evaluation, we conducted a human evaluation to assess and compare the performance of baseline models against our proposed method. Five professionals with expertise in the domain were enlisted to participate in the manual evaluation process. Each evaluator was tasked with rating the attributes of fluency (Flu.), coherence (Coh.), diversity (Div.), and hallucination (Hal.) on a scale ranging from 1 to 5. A rating of 1 denoted very low performance, while a rating of 5 indicated very high performance. A total of 50 test samples were randomly selected and evaluated across different models. The final human evaluation result was obtained by averaging the scores provided by the evaluators.\nTab. 5 presents the outcomes of human evaluation conducted on the Yelp dataset. RegaVAE outperforms the baseline models in almost all dimensions, demonstrating superior performance in comparison. To further establish the correlation between human and automated evaluation results, we calculated the Pearson correlation coefficient and presented the corresponding values in Tab. 6. The results obtained from human evaluation align closely with those derived from partially automated evaluation metrics. For example, the correlation between the human evaluation metrics (Flu., Coh.) associated with PPL and PPL itself is nearly identical."
        },
        {
            "heading": "5 Analysis",
            "text": "To further analyze RegaVAE, we explore the impact of the number of retrieved neighbors, different model structures on the model performance. We also give a case to verify the model performance."
        },
        {
            "heading": "5.1 Number of Retrieved Neighbors",
            "text": "Fig .2 depicts the performance trends in relation to the number of retrieved neighbors. Notably, as the number of retrieved neighbors increases from 5 to 100, we observed a reduction in PPL by 0.64 on the Yelp dataset and 0.69 on the Yahoo dataset, and PPL on the WP dataset is reduced by 0.59. This upward trend proves that implicit aggregation methods can effectively filter noise compared to explicit aggregation methods, and moreover, aggregations using Gaussian mixture distributions are effective for retrieving documents and source texts."
        },
        {
            "heading": "5.2 Ablation Experiment",
            "text": "To evaluate the effectiveness of the model structure, we conducted ablation experiments involving retrieval and aggregation, as depicted in Tab. 7. When we excluded the VAE structure, there was a notable decline in the performance of RegaVAE. Interestingly, we observed that the model augmented with retrieval performed even worse than the model without retrieval when the VAE structure was absent. We speculate that the retrieved variables in this particular scenario reside in a space that fails to meet the requirements of uniformity and continuity. As a result, the model struggled to generate valid samples based on cosine similarity, introducing unwanted noise instead.\nCompared with other retrieval methods, it can be seen that the performance of traditional retrieval methods is obviously insufficient. This discrepancy can be attributed to our approach incorporating future information into key, value, and query parts simultaneously, thus taking future information into account in both retrieval and generation phases, further validating our motivation."
        },
        {
            "heading": "5.3 Case Study",
            "text": "We present a compelling example to examine the quality of RegaVAE-generated text and explore the integration of retrieval information into the generated content, as illustrated in Fig. 3.\nThrough our observations, we have noted that the text produced by RegaVAE demonstrates a remarkable ability to establish a coherent connection with the source text while being vivid and specific. Moreover, despite encoding only the retrieved document into the latent space and subsequently integrating it into the generation process, it is evident that RegaVAE-generated text effectively incorporates future information from the retrieved document."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we summarize two major challenges of existing retrieval-augmented language model methods, and propose RegaVAE to address them. We find that RegaVAE outperforms traditional retrieval generative models in terms of both generative quality and reduce hallucinations. In addition, ablation experiments and three analysis experiments verify the correctness of the model motivation. In future work, we will consider migrating RegaVAE to large language models.\nLimitations\nAt present, almost all large language models are pre-trained on large-scale corpus, and due to the limitation of computing resources, we cannot pretrain RegaVAE on large-scale corpus, which will lead to performance degradation.\nFurthermore, the model is not stable to train due to the posterior collapse problem. Even if we adopt a low-rank tensor product, this problem still cannot be completely solved.\nEthics Statement\nWe honor and support the EMNLP code of Ethics. This paper mainly studies the use of retrieval generation to eliminate the illusion in the language model and make the generated text more fluent. Our method can introduce canonical text to make language models more reliable. In addition, the data sets used in this article are all open source and do not involve any privacy or ethical issues."
        },
        {
            "heading": "Acknowledgement",
            "text": "This work was supported by the National Key R&D Program of China (2022YFB3103700, 2022YFB3103704), the National Natural Science Foundation of China (NSFC) under Grants No. 62276248, and the Youth Innovation Promotion Association CAS under Grants No. 2023111."
        }
    ],
    "title": "RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling",
    "year": 2023
}