{
    "abstractText": "Cross-lingual transfer learning is an important property of multilingual large language models (LLMs). But how do LLMs represent relationships between languages? Every language model has an input layer that maps tokens to vectors. This ubiquitous layer of language models is often overlooked. We find that similarities between these input embeddings are highly interpretable and that the geometry of these embeddings differs between model families. In one case (XLM-RoBERTa), embeddings encode language: tokens in different writing systems can be linearly separated with an average of 99.2% accuracy. Another family (mT5) represents cross-lingual semantic similarity: the 50 nearest neighbors for any token represent an average of 7.61 writing systems, and are frequently translations. This result is surprising given that there is no explicit parallel crosslingual training corpora and no explicit incentive for translations in pre-training objectives. Our research opens the door for investigations in 1) The effect of pre-training and model architectures on representations of languages and 2) The applications of cross-lingual representations embedded in language models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Andrea W Wen-Yi"
        },
        {
            "affiliations": [],
            "name": "David Mimno"
        }
    ],
    "id": "SP:f5c7500912d6429ea4e99a829959d41e86aa6f07",
    "references": [
        {
            "authors": [
                "Hanan Aldarmaki",
                "Mona Diab."
            ],
            "title": "Context-aware cross-lingual mapping",
            "venue": "Proceedings of the 2019",
            "year": 2019
        },
        {
            "authors": [
                "Waleed Ammar",
                "George Mulcaire",
                "Yulia Tsvetkov",
                "Guillaume Lample",
                "Chris Dyer",
                "Noah A Smith."
            ],
            "title": "Massively multilingual word embeddings",
            "venue": "arXiv preprint arXiv:1602.01925.",
            "year": 2016
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Gorka Labaka",
                "Eneko Agirre."
            ],
            "title": "Learning bilingual word embeddings with (almost) no bilingual data",
            "venue": "Proceedings of the 55th Annual",
            "year": 2017
        },
        {
            "authors": [
                "Mikel Artetxe",
                "Gorka Labaka",
                "Eneko Agirre."
            ],
            "title": "A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings",
            "venue": "Pro-",
            "year": 2018
        },
        {
            "authors": [
                "Xilun Chen",
                "Claire Cardie."
            ],
            "title": "Unsupervised multilingual word embeddings",
            "venue": "Proceedings of",
            "year": 2018
        },
        {
            "authors": [
                "Kevin Clark",
                "Urvashi Khandelwal",
                "Omer Levy",
                "Christopher D. Manning."
            ],
            "title": "What does BERT look at? an analysis of BERT\u2019s attention",
            "venue": "Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Pro-",
            "year": 2020
        },
        {
            "authors": [
                "Manaal Faruqui",
                "Chris Dyer."
            ],
            "title": "Improving vector space word representations using multilingual correlation",
            "venue": "Proceedings of the 14th Conference",
            "year": 2014
        },
        {
            "authors": [
                "Mor Geva",
                "Avi Caciularu",
                "Kevin Wang",
                "Yoav Goldberg."
            ],
            "title": "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space",
            "venue": "Proceedings of the 2022 Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Mor Geva",
                "Roei Schuster",
                "Jonathan Berant",
                "Omer Levy."
            ],
            "title": "Transformer feed-forward layers are keyvalue memories",
            "venue": "Proceedings of the 2021 Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Stephan Gouws",
                "Anders S\u00f8gaard."
            ],
            "title": "Simple taskspecific bilingual word embeddings",
            "venue": "Proceedings",
            "year": 2015
        },
        {
            "authors": [
                "Akshi Kumar",
                "Victor Hugo C. Albuquerque."
            ],
            "title": "Sentiment analysis using xlm-r transformer and zeroshot transfer learning on resource-poor indian language",
            "venue": "ACM Trans. Asian Low-Resour. Lang. Inf. Process., 20(5).",
            "year": 2021
        },
        {
            "authors": [
                "Guillaume Lample",
                "Alexis Conneau",
                "Marc\u2019Aurelio Ranzato",
                "Ludovic Denoyer",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Word translation without parallel data",
            "venue": "In International Conference on Learning Representations",
            "year": 2018
        },
        {
            "authors": [
                "Anne Lauscher",
                "Vinit Ravishankar",
                "Ivan Vuli\u0107",
                "Goran Glava\u0161."
            ],
            "title": "From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers",
            "venue": "Proceedings of the 2020",
            "year": 2020
        },
        {
            "authors": [
                "Angeliki Lazaridou",
                "Georgiana Dinu",
                "Marco Baroni."
            ],
            "title": "Hubness and pollution: Delving into cross-space mapping for zero-shot learning",
            "venue": "Pro-",
            "year": 2015
        },
        {
            "authors": [
                "Leland McInnes",
                "John Healy",
                "James Melville."
            ],
            "title": "Umap: Uniform manifold approximation and projection for dimension reduction",
            "venue": "arXiv preprint arXiv:1802.03426.",
            "year": 2018
        },
        {
            "authors": [
                "Antonio Valerio Miceli Barone."
            ],
            "title": "Towards crosslingual distributed representations without parallel text trained with adversarial autoencoders",
            "venue": "Proceedings of the 1st Workshop on Representation Learning for NLP, pages 121\u2013126, Berlin, Germany.",
            "year": 2016
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Quoc V Le",
                "Ilya Sutskever."
            ],
            "title": "Exploiting similarities among languages for machine translation",
            "venue": "arXiv preprint arXiv:1309.4168.",
            "year": 2013
        },
        {
            "authors": [
                "Khalil Mrini",
                "Franck Dernoncourt",
                "Quan Hung Tran",
                "Trung Bui",
                "Walter Chang",
                "Ndapa Nakashole."
            ],
            "title": "Rethinking self-attention: Towards interpretability in neural parsing",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Ruder",
                "Ivan Vuli\u0107",
                "Anders S\u00f8gaard."
            ],
            "title": "A survey of cross-lingual word embedding models",
            "venue": "Journal of Artificial Intelligence Research, 65:569\u2013 631.",
            "year": 2019
        },
        {
            "authors": [
                "Motoki Sato",
                "Jun Suzuki",
                "Hiroyuki Shindo",
                "Yuji Matsumoto."
            ],
            "title": "Interpretable adversarial perturbation in input embedding space for text",
            "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, pages",
            "year": 2018
        },
        {
            "authors": [
                "Sofia Serrano",
                "Noah A. Smith."
            ],
            "title": "Is attention interpretable? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2931\u20132951, Florence, Italy",
            "venue": "Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Genta Indra Winata",
                "Andrea Madotto",
                "Zhaojiang Lin",
                "Rosanne Liu",
                "Jason Yosinski",
                "Pascale Fung."
            ],
            "title": "Language models are few-shot multilingual learners",
            "venue": "Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 1\u201315, Punta Cana,",
            "year": 2021
        },
        {
            "authors": [
                "Shijie Wu",
                "Mark Dredze."
            ],
            "title": "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language",
            "year": 2019
        },
        {
            "authors": [
                "Yuemei Xu",
                "Han Cao",
                "Wanze Du",
                "Wenqing Wang."
            ],
            "title": "A survey of cross-lingual sentiment analysis: Methodologies, models and evaluations",
            "venue": "Data Science and Engineering, 7(3):279\u2013299.",
            "year": 2022
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings",
            "year": 2021
        },
        {
            "authors": [
                "Jinpeng Zhang",
                "Baijun Ji",
                "Nini Xiao",
                "Xiangyu Duan",
                "Min Zhang",
                "Yangbin Shi",
                "Weihua Luo."
            ],
            "title": "Combining static word embeddings and contextual representations for bilingual lexicon induction",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Meng Zhang",
                "Yang Liu",
                "Huanbo Luan",
                "Maosong Sun."
            ],
            "title": "Adversarial training for unsupervised bilingual lexicon induction",
            "venue": "Proceedings of the",
            "year": 2017
        },
        {
            "authors": [
                "Will Y. Zou",
                "Richard Socher",
                "Daniel Cer",
                "Christopher D. Manning."
            ],
            "title": "Bilingual word embeddings for phrase-based machine translation",
            "venue": "Proceed-",
            "year": 2013
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Multilingual large language models (LLMs) have the potential to support transfer learning between languages with little to no additional training data (Lauscher et al., 2020; Wu and Dredze, 2019; Conneau et al., 2020; Winata et al., 2021). But we have limited theory for how LLMs represent meanings across languages. This work describes a mechanism for cross-lingual transfer learning by measuring the properties of the input embedding vectors. While most of the interest in LLMs rightly focuses on their ability to produce contextualized output, in this study we focus specifically on the lowest network layer: the initial token embedding layer. This layer often comprises a large percentage of the\ntotal parameters in a model. It also serves as the connection between human-readable strings and the latent vector representations that initialize the remaining layers. As such, the initial token embedding layer is both geometrically expressive and readily interpretable. We explore the initial token embeddings of two highly multilingual model families, XLM-RoBERTa (XLM-R) (Conneau et al., 2020) and mT5 (Xue et al., 2021). We find that mT5 discovers a universal, cross-lingual semantic space that assigns words (or word fragments) with similar meanings to nearby vector positions.1\nPrevious work has shown that algorithms exist to train multilingual word embeddings without explicit parallel text or glossaries (Ammar et al., 2016; Chen and Cardie, 2018). What is novel about the current work is the discovery that certain highly multilingual LLMs contain such an embedding as an emergent property without any explicit instruction to do so.\nExplaining the factors that cause LLMs to find cross-lingual semantic embeddings would require pre-training experiments that are beyond the scope of this short paper. Describing these behaviors is, however, a necessary first step. At the same time, there is increasing attention on producing language technology for lower-resource languages (Kumar and Albuquerque, 2021), where the datahungry methods that have been successful for highresource languages may not be applicable. Creating predictive theories that explain the relationship between properties of pre-training data and representations learned by LLMs could lead us to build collections, architectures, and algorithms that most efficiently improve performance. This will be an extremely valuable step to enhance language technology for low-resource languages.\n1Code is available at: https://github.com/ andreawwenyi/hyperpolyglot"
        },
        {
            "heading": "2 Related work",
            "text": "Previous work has found interpretable patterns in feed-forward layers (Geva et al., 2021, 2022), selfattention (Mrini et al., 2020; Clark et al., 2019; Serrano and Smith, 2019), and input embeddings from the perspective of adversarial perturbation (Sato et al., 2018). In this work, we directly interpret the relative positions of the token embeddings through the lens of the vocabularies.\nPrior to the explosion of contextualized language models, there was substantial interest in crosslingual word embeddings (CLWE) (Ruder et al., 2019; Mikolov et al., 2013; Lazaridou et al., 2015). The goal is often to map monolingual word embeddings from different languages to the same shared space, so that words of the same meaning are close to each other. CLWE approaches involve some levels of supervised alignment (Faruqui and Dyer, 2014; Zou et al., 2013), seed dictionaries (Artetxe et al., 2017; Gouws and S\u00f8gaard, 2015) or adversarial training (Lample et al., 2018; Artetxe et al., 2018; Zhang et al., 2017; Miceli Barone, 2016). Contexualized embeddings from language models have also been used in combination with static word embeddings to improve alignments of crosslingual word vectors (Aldarmaki and Diab, 2019; Zhang et al., 2021). Contrary to our findings for the token embeddings of LLMs, it was not clear that aligning word vectors is possible without some level of supervision, or to more than two languages at a time. Previous results also showed that CLWE approaches are sensitive to language pairs, where languages with large semantic or structural differences usually failed, such as English-Japanese and Spanish-Chinese (Xu et al., 2022)."
        },
        {
            "heading": "3 Multilingual vocabularies",
            "text": "Sub-word tokenization and writing systems Initial embedding layer maps tokens to vectors. Most contemporary language models use sub-word tokenization schemes such as byte-pair encoding. These methods balance representational richness (providing distinct vectors for as many inputs as possible) with efficiency in limiting overall vocabulary size. While some methods produce tokens consisting of byte sequences that are not valid UTF8 characters, in practice almost all tokens are valid, displayable sequences of Unicode characters, and a large number are recognizable words 2.\nAs it is difficult to assign tokens to specific languages (e.g. war can be an English noun or a German verb), we use Unicode metadata to define categories of tokens. For example, a is LATIN SMALL LETTER A. Most characters are letters (L), but vocabularies also include punctuation (P), numbers (N), and symbols (S, which include emoji). Letters are also marked by writing system, such as LATIN, CYRILLIC, or BENGALI. We define the category of a character as either its writing system (for letters) or its Unicode class for non-letters. A token can mix characters of different categories, but they typically have a most frequent category: the string doesn\u2019t contains letters and punctuation, but is primarily LATIN letters. We define the category of a token based on its majority character category. As a result, doesn\u2019t is classified as LATIN.\nOverlap between model vocabularies While model families have distinct sets of vocabular-\n2We find 39.93% of mT5 and 47.78% of XLM-R vocabularies in multilingual dictionaries provided by Lample et al. (2018).\nies, they are often comparable. Current vocabulary sizes of monolingual models are often \u21e1 32,000 (BERT, T5, LLaMa), 50,000 (GPT2, GPTJ, Pythia), or 65,000 (falcon). Highly multilingual models have larger sizes around 250,000 (mT5, umT5, XLM-R, BLOOM).\nTo make strings comparable across tokenzation methods, we replace the space character in BPE tokenizations with Unicode LOWER ONE EIGHTH BLOCK, which looks like a long underscore, as it is more readable and compatible with T5-style SentencePiece vocabularies.\nFigure 2 shows the total overlap in vocabularies between six LLMs. We select mT5 and XLM-R as the most multilingual comparison due to their large vocabulary size, significant overlap, and large overlap in non-LATIN tokens. Of the models with a large vocabulary, BLOOM focuses more on African and South Asian languages and thus has a much smaller token overlap in CYRILLIC, HANGUL, HIRAGANA, and KATAKANA."
        },
        {
            "heading": "4 Results",
            "text": "Models encode languages differently. We make comparisons between the embeddings of XL-scale\nmodels from mT5 and XLM-R families. In addition to -XL models being potentially more expressive, XLM-R-XL and mT5-XL are also more comparable in parameter size (3.5B and 3.7B, respectively). Figure 1 shows UMAP (McInnes et al., 2018) projections of the embeddings from XLMR-XL and mT5-XL for each token in the shared vocabulary, colored by Unicode category. We find that XLM-R\u2019s representations encode languages \u2014 tokens of different categories form isolated clusters. Clusters of Unicode categories are also noticeable in mT5 but they are more overlapping. The exception is Symbols (S) and Numbers (N): they are scattered in XLM-R-XL, but clustered in mT5-XL.\nTo further show how embeddings encode languages, we use logistic regression to predict Unicode category from embeddings. For each category, we construct a balanced dataset and run 10-fold cross-validation. Embeddings from XLM-R-XL and mT5-XL both encode a high level of language information. Tokens with different Unicode category could be linearly separated with an average of 99.24% and 93.32% accuracy, respectively. Figure 3 shows accuracy for selected categories. XLM-RXL has significantly higher (near perfect) accuracy across the majority of categories than mT5-XL.\nEmbedding neighborhoods encode semantics across languages. We next zoom in to study the neighbors of individual tokens. Neighbors from mT5-XL are often direct translations, whereas XLM-R-XL finds tokens in the same language that are semantically or linguistically similar. Figure 5 shows that the 50 nearest neighbors for mT5-XL tokens have an average of 7.61 different Unicode categories, whereas XLM-R-XL has 1.64. Figure 4 shows examples of 20 nearest neighbors of selected tokens.\nThe neighboring categories of tokens in mT5 vary by the category of a token, as shown in Figure 6. We find interesting comparisons between two Japanese writing systems, KATAKANA and HIRAGANA. KATAKANA, a writing system often used for words of foreign origin, has more diverse neighbors (most often LATIN). HIRAGANA, used more for native Japanese words, has more neighbors in HIRAGANA and CJK (Kanji). We do not find evidence to suggest that tokens appearing more often in pre-training corpora have more diverse neighbors (Figure 9 in the Appendix).\nEmbedding geometries are similar across parameter scales. Finally, we consider whether initial token embeddings as a whole are similar across model families and parameter scales. We use two metrics to quantify similarity in geometric structures of embedding space.\nThe first metric measures the local similarity between vectors in two matrices. We first find the set of tokens shared between model families and create subset embedding matrices that only\ninclude those tokens. For each token in the shared vocabulary, we then find the 100 nearest neighbors within each matrix using cosine distance. Finally, we calculate the overlap between these neighbor sets from each pair of models. For example, on average, 60 of the 100 nearest neighbors of a token in XLM-R-XL will also be nearest neighbors of the same token in XLM-R-XXL. Figure 7 compares the average number of overlapping terms out of 100\nnearest neighbors for pairs of models, including BLOOM for comparison. Results are averaged over 45,000 shared tokens between mT5, XLM-R, and BLOOM. We find that XLM-R models have the largest similarity. mT5 are more varied, but still find 20\u201330 shared tokens on average. BLOOM is the most different: the 1.1B model shares only 5\u201310 tokens with other models, the 3B 15\u201320.\nThe second metric uses canonical angles, a linear algebraic measure of rotational alignment. While it is extremely unlikely that input embedding matrices will be comparable at the level of individual columns or entries, the matrices of two models may be identical up to rotation or permutation. We can measure the degree to which two matrices are rotations through the method of canonical angles. Given matrices A and B both with n rows, we calculate QARA = A and QBRB = B. Then we form the SVD U\u2303V T = QTAQB . The singular values \u2303 are the cosines of the angles of rotation. The single largest value is the cosine of the smallest angle, so it forms an upper bound on our ability to rotate one matrix to match the other. Figure 8 shows the first singular values for pairs of models restricted to rows for their shared vocabularies, including a random \u201cembedding\u201d matrix with size 512. We find that XLM-R models have a high rotational similarity to each other (0.99 out of 1.0), while mT5 models are more differentiated (0.95\u2013 0.97) but still highly similar. All models are significantly more similar to each other than to a random matrix (0.15\u20130.27)."
        },
        {
            "heading": "5 Conclusion",
            "text": "While input embeddings and sub-word tokenized vocabularies of LLMs may appear inscrutable, we find that they are in fact interpretable and meaningful. We observe significant patterns that differ by model families, including an emergent ability of mT5 to discover a shared semantic space that spans languages \u2014 accidentally achieving a goal that defied a decade of research on cross-lingual word embeddings.\nFuture directions include explaining factors that cause the different token embedding patterns we observe in XLM-R and mT5. This could include investigations in model architectures, pre-training procedures, and data-centric strategies such as the curation of pre-training corpora. Finding an efficient path to a more equitable language model performance will be valuable for enhancing language technology for low-resource languages. An interesting next study could explore the utility of combining low-resource languages with closely related higher-resource languages in pre-training corpora. Another future direction is to explore the potential applications of cross-lingual representations embedded in LLMs \u2014 What does it say about downstream applications? Could it be used to guide practitioners to select models that are more appropriate for their intended uses?"
        },
        {
            "heading": "6 Limitations",
            "text": "This work is descriptive rather than explanatory. We observe that there are patterns in the geometric structure of input embedding matrices in families of LLMs, but we are unable to identify why these\npatterns emerge and differ. There are many differences in model architectures, training methods, and pre-training corpora between LLM families. It is out of the scope of this work to determine what factors are causal. As we have limited ability to carry out pre-training experiments, we chose to focus on descriptive observations of existing LLMs."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank the anonymous reviewers for their valuable comments. We\u2019d also like to thank Inle Bush, Hyunju Kim, Omary Mzava, Daniel Mwesigwa, Cheng Perng Phoo, Top Piriyakulkij, and Ahkln Wong for their assistance in vocabulary translation."
        },
        {
            "heading": "B Frequency of tokens does not correlate with diversity of neighbors.",
            "text": "It could be that words with fewer occurrences in the training corpus would have more or less diversity in Unicode categories in their neighbor sets. We calculated an estimate of the frequency of mT5 SP tokens based on a sample from the mC4 dataset. We then took a stratified sample of tokens from the mT5 vocabulary from 10 frequency bands and calculated the mean number of distinct Unicode categories for their neighbors, see Figure 9. We\nfind no correlation between the frequency of terms and the diversity of their nearest neighbor sets."
        }
    ],
    "title": "Hyperpolyglot LLMs: Cross-Lingual Interpretability in Token Embeddings",
    "year": 2023
}