{
    "abstractText": "Deep neural networks have demonstrated their capacity in extracting features from speech inputs. However, these features may include non-linguistic speech factors such as timbre and speaker identity, which are not directly related to translation. In this paper, we propose a content-centric speech representation disentanglement learning framework for speech translation, CCSRD, which decomposes speech representations into content representations and non-linguistic representations via representation disentanglement learning. CCSRD consists of a content encoder that encodes linguistic content information from the speech input, a non-content encoder that models non-linguistic speech features, and a disentanglement module that learns disentangled representations with a cyclic reconstructor, feature reconstructor and speaker classifier trained in a multi-task learning way. Experiments on the MuST-C benchmark dataset demonstrate that CCSRD achieves an average improvement of +0.9 BLEU in two settings across five translation directions over the baseline, outperforming state-of-the-art endto-end speech translation models and cascaded models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaohu Zhao"
        },
        {
            "affiliations": [],
            "name": "Haoran Sun"
        },
        {
            "affiliations": [],
            "name": "Yikun Lei"
        },
        {
            "affiliations": [],
            "name": "Shaolin Zhu"
        },
        {
            "affiliations": [],
            "name": "Deyi Xiong"
        }
    ],
    "id": "SP:c8f4f42335d66b3ba2ec87665d66061058225770",
    "references": [
        {
            "authors": [
                "Jakob Abe\u00dfer",
                "Meinard M\u00fcller."
            ],
            "title": "Towards audio domain adaptation for acoustic scene classification using disentanglement learning",
            "venue": "arXiv preprint arXiv:2110.13586.",
            "year": 2021
        },
        {
            "authors": [
                "Alexei Baevski",
                "Yuhao Zhou",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:12449\u201312460",
            "year": 2020
        },
        {
            "authors": [
                "Alexandre B\u00e9rard",
                "Olivier Pietquin",
                "Christophe Servan",
                "Laurent Besacier."
            ],
            "title": "Listen and translate: A proof of concept for end-to-end speech-to-text translation",
            "venue": "NIPS workshop on End-to-end Learning for Speech and Audio Processing.",
            "year": 2016
        },
        {
            "authors": [
                "David Bertoin",
                "Emmanuel Rachelson."
            ],
            "title": "Disentanglement by cyclic reconstruction",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Chak Ho Chan",
                "Kaizhi Qian",
                "Yang Zhang",
                "Mark Hasegawa-Johnson."
            ],
            "title": "Speechsplit2",
            "venue": "0: Unsupervised speech disentanglement for voice conversion without tuning autoencoder bottlenecks. In ICASSP 2022-2022 IEEE International Conference on Acous-",
            "year": 2022
        },
        {
            "authors": [
                "David M Chan",
                "Shalini Ghosh."
            ],
            "title": "Contentcontext factorized representations for automated speech recognition",
            "venue": "Proceedings of the 23rd Annual Conference of the International Speech Communication Association.",
            "year": 2022
        },
        {
            "authors": [
                "Junkun Chen",
                "Mingbo Ma",
                "Renjie Zheng",
                "Liang Huang."
            ],
            "title": "Specrec: An alternative solution for improving end-to-end speech-to-text translation via spectrogram reconstruction",
            "venue": "Interspeech 2021, 22nd Annual Conference of the International Speech",
            "year": 2021
        },
        {
            "authors": [
                "Xi Chen",
                "Yan Duan",
                "Rein Houthooft",
                "John Schulman",
                "Ilya Sutskever",
                "Pieter Abbeel."
            ],
            "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
            "venue": "Advances in neural information processing systems, 29.",
            "year": 2016
        },
        {
            "authors": [
                "Pengyu Cheng",
                "Martin Renqiang Min",
                "Dinghan Shen",
                "Christopher Malon",
                "Yizhe Zhang",
                "Yitong Li",
                "Lawrence Carin."
            ],
            "title": "Improving disentangled text representation learning with information-theoretic guidance",
            "venue": "Proceedings of the 58th Annual Meet-",
            "year": 2020
        },
        {
            "authors": [
                "Mattia A Di Gangi",
                "Roldano Cattoni",
                "Luisa Bentivogli",
                "Matteo Negri",
                "Marco Turchi."
            ],
            "title": "Must-c: a multilingual speech translation corpus",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2019
        },
        {
            "authors": [
                "Qianqian Dong",
                "Mingxuan Wang",
                "Hao Zhou",
                "Shuang Xu",
                "Bo Xu",
                "Lei Li."
            ],
            "title": "Consecutive decoding for speech-to-text translation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12738\u201312748.",
            "year": 2021
        },
        {
            "authors": [
                "Yichao Du",
                "Zhirui Zhang",
                "Weizhi Wang",
                "Boxing Chen",
                "Jun Xie",
                "Tong Xu."
            ],
            "title": "Regularizing end-toend speech translation with triangular decomposition agreement",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10590\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Zongyang Du",
                "Berrak Sisman",
                "Kun Zhou",
                "Haizhou Li."
            ],
            "title": "Disentanglement of emotional style and speaker identity for expressive voice conversion",
            "venue": "Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association.",
            "year": 2022
        },
        {
            "authors": [
                "Qingkai Fang",
                "Rong Ye",
                "Lei Li",
                "Yang Feng",
                "Mingxuan Wang."
            ],
            "title": "STEMM: self-learning with speechtext manifold mixup for speech translation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, pages 7050\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Yaroslav Ganin",
                "Evgeniya Ustinova",
                "Hana Ajakan",
                "Pascal Germain",
                "Hugo Larochelle",
                "Fran\u00e7ois Laviolette",
                "Mario Marchand",
                "Victor Lempitsky."
            ],
            "title": "Domain-adversarial training of neural networks",
            "venue": "The journal of machine learning research, 17(1):2096\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Abel Gonzalez-Garcia",
                "Joost Van De Weijer",
                "Yoshua Bengio."
            ],
            "title": "Image-to-image translation for cross-domain disentanglement",
            "venue": "Advances in neural information processing systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "Hirofumi Inaguma",
                "Shun Kiyono",
                "Kevin Duh",
                "Shigeki Karita",
                "Nelson Enrique Yalta Soplin",
                "Tomoki Hayashi",
                "Shinji Watanabe."
            ],
            "title": "Espnet-st: Allin-one speech translation toolkit",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "Ye Jia",
                "Melvin Johnson",
                "Wolfgang Macherey",
                "Ron J Weiss",
                "Yuan Cao",
                "Chung-Cheng Chiu",
                "Naveen Ari",
                "Stella Laurenzo",
                "Yonghui Wu."
            ],
            "title": "Leveraging weakly supervised data to improve end-to-end speech-to-text translation",
            "venue": "ICASSP 2019-2019",
            "year": 2019
        },
        {
            "authors": [
                "Taku Kudo",
                "John Richardson."
            ],
            "title": "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2018
        },
        {
            "authors": [
                "Yoohwan Kwon",
                "Soo-Whan Chung",
                "Hong-Goo Kang."
            ],
            "title": "Intra-class variation reduction of speaker representation in disentanglement framework",
            "venue": "Interspeech 2020, 21st Annual Conference of the International Speech Communication Associa-",
            "year": 2020
        },
        {
            "authors": [
                "Tsz Kin Lam",
                "Shigehiko Schamoni",
                "Stefan Riezler."
            ],
            "title": "Sample, translate, recombine: Leveraging audio alignments for data augmentation in end-toend speech translation",
            "venue": "Proceedings of the 60th",
            "year": 2022
        },
        {
            "authors": [
                "Seunghun Lee",
                "Sunghyun Cho",
                "Sunghoon Im."
            ],
            "title": "Dranet: Disentangling representation and adaptation networks for unsupervised cross-domain adaptation",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15252\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Wei Li",
                "Can Gao",
                "Guocheng Niu",
                "Xinyan Xiao",
                "Hao Liu",
                "Jiachen Liu",
                "Hua Wu",
                "Haifeng Wang."
            ],
            "title": "Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning",
            "venue": "Proceedings of the 59th Annual Meeting of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Weiwei Lin",
                "Chenhang He",
                "Man-Wai Mak",
                "Youzhi Tu."
            ],
            "title": "Self-supervised neural factor analysis for disentangling utterance-level speech representations",
            "venue": "arXiv preprint arXiv:2305.08099.",
            "year": 2023
        },
        {
            "authors": [
                "Abdelrahman Mohamed",
                "Hung-yi Lee",
                "Lasse Borgholt",
                "Jakob D Havtorn",
                "Joakim Edin",
                "Christian Igel",
                "Katrin Kirchhoff",
                "Shang-Wen Li",
                "Karen Livescu",
                "Lars Maal\u00f8e"
            ],
            "title": "Self-supervised speech representation learning: A review",
            "venue": "IEEE Journal of Selected",
            "year": 2022
        },
        {
            "authors": [
                "Vassil Panayotov",
                "Guoguo Chen",
                "Daniel Povey",
                "Sanjeev Khudanpur."
            ],
            "title": "Librispeech: an asr corpus based on public domain audio books",
            "venue": "ICASSP 2015-2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages",
            "year": 2015
        },
        {
            "authors": [
                "Emanuel Parzen."
            ],
            "title": "On estimation of a probability density function and mode",
            "venue": "The annals of mathematical statistics, 33(3):1065\u20131076.",
            "year": 1962
        },
        {
            "authors": [
                "Juan Pino",
                "Qiantong Xu",
                "Xutai Ma",
                "Mohammad Javad Dousti",
                "Yun Tang."
            ],
            "title": "Self-training for endto-end speech translation",
            "venue": "Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, pages 1476\u20131480.",
            "year": 2020
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting bleu scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013 191.",
            "year": 2018
        },
        {
            "authors": [
                "Eduardo Hugo Sanchez",
                "Mathieu Serrurier",
                "Mathias Ortner."
            ],
            "title": "Learning disentangled representations via mutual information estimation",
            "venue": "European Conference on Computer Vision, pages 205\u2013 221. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Matthias Sperber",
                "Matthias Paulik."
            ],
            "title": "Speech translation and the end-to-end promise: taking stock of where we are",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7409\u20137421.",
            "year": 2020
        },
        {
            "authors": [
                "Yun Tang",
                "Hongyu Gong",
                "Ning Dong",
                "Changhan Wang",
                "Wei-Ning Hsu",
                "Jiatao Gu",
                "Alexei Baevski",
                "Xian Li",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "Unified speech-text pre-training for speech translation and recognition",
            "year": 2022
        },
        {
            "authors": [
                "Yun Tang",
                "Juan Pino",
                "Xian Li",
                "Changhan Wang",
                "Dmitriy Genzel."
            ],
            "title": "Improving speech translation by understanding and learning from the auxiliary text translation task",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Yun Tang",
                "Juan Pino",
                "Changhan Wang",
                "Xutai Ma",
                "Dmitriy Genzel."
            ],
            "title": "A general multi-task learning framework to leverage text data for speech to text tasks",
            "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Process-",
            "year": 2021
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton."
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research, 9(11).",
            "year": 2008
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Hari Krishna Vydana",
                "Martin Karafi\u00e1t",
                "Katerina Zmolikova",
                "Luk\u00e1\u0161 Burget",
                "Honza \u010cernock\u1ef3."
            ],
            "title": "Jointly trained transformers models for spoken language translation",
            "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal",
            "year": 2021
        },
        {
            "authors": [
                "Changhan Wang",
                "Yun Tang",
                "Xutai Ma",
                "Anne Wu",
                "Dmytro Okhonko",
                "Juan Pino."
            ],
            "title": "Fairseq s2t: Fast speech-to-text modeling with fairseq",
            "venue": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Chengyi Wang",
                "Yu Wu",
                "Shujie Liu",
                "Zhenglu Yang",
                "Ming Zhou."
            ],
            "title": "Bridging the gap between pretraining and fine-tuning for end-to-end speech translation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9161\u20139168.",
            "year": 2020
        },
        {
            "authors": [
                "Jie Wang",
                "Jingbei Li",
                "Xintao Zhao",
                "Zhiyong Wu",
                "Shiyin Kang",
                "Helen Meng."
            ],
            "title": "Adversarially learning disentangled speech representations for robust multifactor voice conversion",
            "venue": "pages 846\u2013850.",
            "year": 2021
        },
        {
            "authors": [
                "Yuying Xie",
                "Thomas Arildsen",
                "Zheng-Hua Tan."
            ],
            "title": "Disentangled speech representation learning based on factorized hierarchical variational autoencoder with self-supervised objective",
            "venue": "2021 IEEE 31st International Workshop on Machine Learning",
            "year": 2021
        },
        {
            "authors": [
                "Chen Xu",
                "Bojie Hu",
                "Yanyang Li",
                "Yuhao Zhang",
                "Qi Ju",
                "Tong Xiao",
                "Jingbo Zhu"
            ],
            "title": "Stacked acousticand-textual encoding: Integrating the pre-trained models into speech translation encoders",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Rong Ye",
                "Mingxuan Wang",
                "Lei Li."
            ],
            "title": "End-toend speech translation via cross-modal progressive training",
            "venue": "Proceedings of the 22rd Annual Conference of the International Speech Communication Association.",
            "year": 2021
        },
        {
            "authors": [
                "Rong Ye",
                "Mingxuan Wang",
                "Lei Li."
            ],
            "title": "Crossmodal contrastive learning for speech translation",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Biao Zhang",
                "Barry Haddow",
                "Rico Sennrich."
            ],
            "title": "Revisiting end-to-end speech-to-text translation from scratch",
            "venue": "International Conference on Machine Learning, pages 26193\u201326205. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Jing-Xuan Zhang",
                "Zhen-Hua Ling",
                "Li-Rong Dai."
            ],
            "title": "Non-parallel sequence-to-sequence voice conversion with disentangled linguistic and speaker representations",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:540\u2013552.",
            "year": 2019
        },
        {
            "authors": [
                "Jiawei Zhao",
                "Wei Luo",
                "Boxing Chen",
                "Andrew Gilman."
            ],
            "title": "Mutual-learning improves end-to-end speech translation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3989\u20133994.",
            "year": 2021
        },
        {
            "authors": [
                "Ye"
            ],
            "title": "2022), we used a beam size of 10 and a",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "End-to-end (E2E) speech-to-text translation (ST) aims to translate a source speech input into a target translation. Compared with traditional cascaded ST models, E2E ST models avoid the issue of error propagation and exhibit low inference latency. Recent works have made significant progress in E2E ST, which enables it to even outperform traditional cascaded systems on language pairs like En-De and En-Ru (Ye et al., 2022; Fang et al., 2022; Du et al., 2022a).\nDespite the remarkable progress, E2E ST is still confronted with challenges in learning desirable\n\u2217*Corresponding author\nspeech representations. Speech inputs to ST encompass not only content information that is essential for translation but also non-linguistic factors such as pitch, timbre, prosody, speaker identity and so on. As a result, the speech encoder encodes not only the content information but also a range of non-linguistic speech elements. Such non-linguistic factors may benefit translation, e.g., prosody as mentioned by (Sperber and Paulik, 2020). However, they may also introduce spurious correlations between speech inputs and target translations, undermining speech translation generalization.\nTo mitigate this issue, we propose to explore speech representation disentanglement learning in the context of speech translation, which aims to separate content from non-linguistic factors (e.g., prosody, timbre). Previous studies have consistently demonstrated the efficacy of representation disentanglement in improving model generalization (Chen et al., 2016; Sanchez et al., 2020; Chan and Ghosh, 2022; Mohamed et al., 2022). Given the highly complex nature of speech features and the modality gap between text and speech, we argue that representation disentanglement could enable ST to focus on content, reducing the negative influence from non-linguistic factors on speech translation modeling.\nSpecifically, we propose a Content-Centric Speech Representation Disentanglement learning framework, termed as CCSRD, for end-to-end speech translation. We re-function the original encoder as the content encoder that encodes linguistic content information contained in the speech input. We introduce an additional encoder, noncontent encoder, to encode non-linguistic speech features. The decomposition of the speech input into content and non-linguistic factors for the two encoders is completed by a disentanglement module. The disentanglement module is trained in a multi-task learning way, which leverages three\ntasks: a cyclic reconstruction task to reduce the mutual information between content representation and non-content representation, a feature reconstruction task to ensure the retention of speech information, and a speaker classifier task to guide the training of the non-content encoder. Additionally, we also employ a masking strategy to further improve disentanglement.\nIt is noteworthy that our method does not require transcription to achieve representation disentanglement. Therefore it can be used in scenarios that are short of transcription data or do not have transcription data at all. Our work is hence significantly different from most previous works that heavily depend on transcription for speech translation.\nAlthough disentangled speech representation learning is not new in the community of speech processing (Xie et al., 2021; Wang et al., 2021; Abe\u00dfer and M\u00fcller, 2021), to the best of our knowledge, this is the first attempt to learn disentangled speech representations for end-to-end speech translation. In a nutshell, our contributions are listed as follows.\n\u2022 We propose CCSRD for end-to-end ST, which learns speech representation disentanglement to separate content from non-linguistic features.\n\u2022 The proposed disentanglement module consists of cyclic reconstruction, feature reconstruction and speaker classifier. It does not require any transcription data for disentanglement learning.\n\u2022 We conduct extensive experiments on the MuST-C benchmark with five language pairs. CCSRD achieves an average improvement of +0.9 BLEU in a setting without using any transcription data and +0.9 BLEU in ST with the multi-task (MTL) setting using transcription data."
        },
        {
            "heading": "2 Related Work",
            "text": "End-to-End Speech Translation To avoid error propagation in cascaded ST and reduce inference latency, B\u00e9rard et al. (2016) and Weiss et al. (2017) propose end-to-end ST that directly translates speech in the source language into text in the target language, without relying on the intermediate transcriptions. However, due to the inherent complexity and variation of speech signals and the\nscarcity of high-quality E2E ST data, achieving satisfactory performance remains challenging. Over the years, a variety of approaches have been proposed to address these issues, such as pre-training (Wang et al., 2020b; Tang et al., 2021b; Dong et al., 2021), multi-task learning (Vydana et al., 2021; Ye et al., 2021; Tang et al., 2022), data augmentation (Jia et al., 2019; Lam et al., 2022), contrastive learning (Li et al., 2021; Ye et al., 2022) and knowledge distillation (Tang et al., 2021a; Zhao et al., 2021). Most of these approaches focus on using the transcription data in speech data triplets to perform MT/ASR tasks, pretrain model components and mitigate the modality gap between speech and text. Significantly different from them, we attempt to improve translation quality by efficiently exploring the speech representation disentanglement to learn content-centric speech representations for ST.\nRepresentation Disentanglement Representation disentanglement refers to a learning paradigm in which models represent input signals through multiple separated dimensions or embeddings. Therefore, it is always advantageous in obtaining representations that carry certain attributes or extract discriminative features. Reconstruction based training (Gonzalez-Garcia et al., 2018; Zhang et al., 2019; Bertoin and Rachelson, 2022) is widely adopted in disentanglement learning and used to obtain disentangled representations. The application of representation disentanglement is extensive, including speech (Chan and Ghosh, 2022; Chan et al., 2022), computer vision (Gonzalez-Garcia et al., 2018; Lee et al., 2021) and natural language precessing (Bao et al., 2019; Cheng et al., 2020). Since speech often contains multiple factors, disentangled representation learning provides a way to extract different representations for different tasks like voice conversion (Du et al., 2022b), automatic speech recognition (Chan and Ghosh, 2022) and speaker recognition (Kwon et al., 2020). Our approaches are partially motivated by these efforts but are significantly different from them in two aspects. First, we make the first step to use representation disentanglement during the training stage of E2E ST and propose CCSRD. Second, we focus on the quality of the content representation for ST, removing the additional encoder and disentanglement module during inference, while the previous works focus on the disentanglement of different speech factors for various speech tasks."
        },
        {
            "heading": "3 CCSRD",
            "text": "In this section, we first introduce the overall model architecture of CCSRD and subsequently elaborate disentanglement learning, training and inference of CCSRD."
        },
        {
            "heading": "3.1 Model Architecture",
            "text": "Our E2E ST model adopts the encoder-decoder ST backbone. As shown in Figure 1, it consists of five essential components: speech encoder, content encoder, non-content encoder, disentanglement module and translation decoder. Speech Encoder is to extract low-level features from speech signals. It contains Wav2vec2.0 (Baevski et al., 2020) and two additional convolutional layers, which are added to shrink the extracted speech features by a factor of 4. Content Encoder employs the same configuration as the original Transformer (Vaswani et al., 2017) encoder. The input of the content encoder is the output of the speech encoder for both ASR and ST tasks while the embeddings of the transcription are for MT task. The content encoder is trained to learn decomposed content representations. Non-content Encoder employs the same configurations as the content encoder. The input of the non-content encoder is the output of the speech\nencoder. The non-content encoder is trained to learn non-linguistic representations separated from content representations.\nTranslation Decoder employs the base configurations as the original Transformer (Vaswani et al., 2017) decoder, which is shared by ASR, MT and ST. It yields either speech transcription or target translation based on the output of the content encoder.\nDisentanglement Module is composed of four networks: content feature predictor network that predicts content representations from non-content features, and non-content feature predictor network that predicts non-content representations from content features, speaker classifier network that predicts the speaker IDs based on non-content representations and feature reconstruction network that predicts speech features from the speech encoder based on both content and non-content representations.\nThe speech encoder, content encoder and translation decoder are the same as those used by Ye et al. (2021). The non-content encoder and disentanglement module are additionally incorporated into CCSRD, which are removed during the inference stage.\nThe training data of ST usually contains speech-\ntranscription-translation triples, which can be denoted as D = {(s, x, y)}. With these training instances, E2E ST can be trained in two ways. Similar to standard neural machine translation, it can be trained in the way of being treated as a single ST task:\nLST = \u2212 \u2211\n(s,y)\u2208D\nlogP (y|s) (1)\nAs |D| is usually not large, E2E ST is often trained in a multi-task learning way (Ye et al., 2021). Additional ASR and MT tasks are incorporated into the training of ST with speech-transcription pairs {(s,x)} and transcription-translation pairs {(x,y)} to facilitate knowledge transfer:\nLASR = \u2212 \u2211\n(s,x)\u2208D\nlogP (x|s) (2)\nLMT = \u2212 \u2211\n(x,y)\u2208D\nlogP (y|x) (3)"
        },
        {
            "heading": "3.2 Speech Representation Disentanglement Learning",
            "text": "In the training stage of CCSRD, we attempt to decompose representations from the speech encoder into two distinct components: content representations that are essential for translation, and noncontent representations that encode non-linguistic speech factors. To achieve this, we propose three training strategies: (1) encouraging the content encoder to encode only linguistic content information, (2) encouraging the non-content encoder to encode non-linguistic speech factors other than linguistic content, (3) minimizing the mutual information between the content and the non-content representations. For the first training strategy, we mainly force the content encoder to learn content-centric representations to reduce ST loss. For the second and third strategies, we propose cyclic reconstruction, feature reconstruction, and speaker classifier tasks. Cyclic Reconstruction To effectively disentangle speech representations, we employ the cyclic reconstruction method proposed by Bertoin and Rachelson (2022) to reduce the mutual information between the content and non-content representations in an unsupervised learning setting. Specifically, after the extraction of content and non-content representations from the content and non-content encoder respectively, we stack two sub-networks: the content feature predictor \u03d5content and non-content\nfeature predictor \u03d5non\u2212content to cyclically reconstruct the content and non-content representations. Particularly, we train the content and non-content feature predictor with the following two reconstruction losses LCON and LNCON respectively:\nLCON = |D|\u2211 i=1 ||Hci \u2212 \u03d5content(Hsi )||22 (4)\nLNCON = |D|\u2211 i=1 ||Hsi \u2212 \u03d5non\u2212content(Hci )||22 (5) where Hci and H s i represent the representation for ith speech input from content and non-content encoder respectively. These two sub-networks are connected to the overall architecture through Gradient Reversal layers (GRLs) (Ganin et al., 2016). GRLs invert the gradient sign during the backward pass, thus pushing the parameters to maximize the losses. As such, the model is constrained to produce content and non-content representations that are hard to reconstruct each other, which means minimal information is shared between the two types of representations (as their distance is maximized by Eq. (4) and (5)). Feature Reconstruction We employ the reconstruction feature predictor network to predict the original speech features output by the speech encoder based on non-content representations learned by the non-content encoder and content representations learned by the content encoder. The training objective for this sub-network is defined as:\nLREC = |D|\u2211 i=1 ||Hi \u2212 \u03d5rec(Hci \u2295Hsi )||22 (6)\nwhere Hi represents the representation for ith speech input output from the speech encoder, \u03d5rec is the feature reconstruction network which tries to recover the speech representation from both content representation Hci and non-content representation Hsi . Speaker Classifier Minimizing the mutual information between the content and non-content representations may be not sufficient as the non-content encoder is not fully constrained and goal-oriented. To address this issue, we make full use of speaker ID information in the dataset. The speaker ID can be considered as a label for non-linguistic factors. We believe that the non-content encoder should fully model this information. Modeling nonlinguistic factors would benefit the decoupling of\nAlgorithm 1: CCSRD in the MTL setting Input :A batch of training set (s, x, y) Output :Loss\n1 while not converged do 2 get s\u2032 from s by applying the masking strategy; 3 get H for s\u2032 from the speech encoder; 4 get Hs and Hc for s\u2032 from the non-content and content encoder; 5 get LSRD in Eq. (9) from the\ndisentanglement module with the input H, Hs and Hc;\n6 do ST task with Hc and y, get LST; 7 do ASR task with Hc and x, get LASR; 8 get Hc for x from the content encoder; 9 do MT task with Hc and y, get LMT;\n10 get LMTL in Eq. (10); 11 end\nnon-linguistic information from content information. We train the speaker classifier network with the following objective:\nLSPK = \u2212 |D|\u2211 i=1 logP (spki | Hsi ) (7)\nwhere spki represents the speaker ID for the i th speech input."
        },
        {
            "heading": "3.3 Masking Strategy",
            "text": "Recent studies have demonstrated that the masking operation can significantly improve the robustness of models and benefit speech representation disentanglement in multiple speech tasks (Chan and Ghosh, 2022; Lin et al., 2023). As shown in Figure 1, during the training stage, we modify the input waveform s of the speech encoder to mask consecutive segments and obtain the masked waveform s\u2032. Then we take s\u2032 as the input to the model. Specifically, for each speech input, it is chosen to be masked with a probability p. The selected speech input is then masked for at least n spans, and each span contains at least m consecutive frames. In our experiments, we set p to 0.75, n to 2 and m to 3600."
        },
        {
            "heading": "3.4 Training and Inference",
            "text": "CCSRD can be trained either in the ST task setting or in the multi-task setting with both MT and ASR tasks. In the ST task setting, the overall training\nobjective is:\nL = LST + LSRD (8)\nwhere\nLSRD = LCON + LNCON + LREC + LSPK (9)\nIn the multi-task setting, the training process overall training objective is:\nLMTL = LST + LASR + LMT + LSRD (10)\nThe training process for the multi-task setting is demonstrated in Algorithm 1. During the inference stage, only the speech encoder, content encoder and translation decoder are used, which is consistent with previous works. Hence, our method does not introduce any additional inference latency compared with previous methods."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We conducted experiments on the widely-used MuST-C multilingual speech translation dataset (Di Gangi et al., 2019). We carried out experiments on English-to-German (DE), English-to-Spanish (ES), English-to-Russian (RU), English-to-French (Fr) and English-to-Italian (It). dev was used to develop and analyze our approaches, tst-COMMON was used for testing. See Appendix A for detailed dataset statistics."
        },
        {
            "heading": "4.2 Settings",
            "text": "Model Configurations We used Wav2vec2.0 in the speech encoder, which is only pretrained on audio data from Librispeech (Panayotov et al., 2015) without performing any downstream finetuning. The kernel, stride and hidden size of the two CNN layers stacked over Wav2vec2.0 were set to 5, 2 and 512. The used content encoder and decoder follow the base configuration of Transformer, with 6 layers, 512 hidden sizes and 8 attention heads. The non-content encoder follows the same configuration as the content encoder. For the network architecture of the disentanglement module in the experiments, both the content feature predictor network and non-content feature predictor network consist of three fully connected (FC) layers followed by ReLU activation and one FC layer followed by Tanh activation. The feature reconstruction network utilizes the same architecture as the feature predictor network, except for the different\ninput dimensions, since the input of the network is the concatenation of content and non-content representation. The classifier network comprises three FC layers with ReLU activation, an adaptive average pooling layer and a log softmax layer. Experiment Details We used the raw 16-bit 16 kHz mono-channel audio wave as speech input and removed utterances of which the duration is longer than 300K frames. For each translation direction, we used a unigram SentencePiece (Kudo and Richardson, 2018) model to learn a vocabulary on the text data from the dataset, which is the same as Ye et al. (2021, 2022)\u2019s setup. For evaluation, we computed case-sensitive detokenized BLEU using sacreBLEU (Post, 2018) on MuSTC tst-COMMON set. Appendix B contains more detailed settings. Baselines We compared our model with multiple strong E2E ST baselines including: (1) Fairseq ST (Wang et al., 2020a), (2) Self-training (Pino et al., 2020), (3) SpecRec (Chen et al., 2021), (4) Revisit ST (Zhang et al., 2022), (5) W2V2-Transformer (Fang et al., 2022), (6) XSTNet (Ye et al., 2021), (7) SATE (Xu et al., 2021), (8) Mutual-learning (Zhao et al., 2021), (9) STEMM (Fang et al., 2022) and (10) ConST (Ye et al., 2022). Additionally, we compared with a strong baseline \"ST baseline\" that uses the same neural architecture with W2V2Transformer (excluding the proposed non-content encoder and disentanglement module). We also compared against a strong \"MTL baseline\" that uses the same neural architecture as the ST baseline but is trained in the MTL setting, which is the same as XSTNet (Ye et al., 2021)."
        },
        {
            "heading": "4.3 Main Results",
            "text": "Comparison to End-to-End Baselines We compared our model with baselines for five language pairs on the MuST-C benchmark dataset. Results are shown in Table 1. In the ST setting that does not use transcriptions in the MUST-C data triples, our model achieves a substantial improvement of 0.9 BLEU over the ST baseline on average and outperforms the strongest baseline, W2V2-Transformer, in all translation directions. Since MTL models have achieved state-of-the-art results in recent studies, we also implemented the proposed CCSRD in the MTL setting that employs transcriptions to perform MT and ASR tasks. Our model achieves a 0.9 BLEU improvement over the MTL baseline, and even the strong model, ConST. These validate the effectiveness of our proposed approach in enhanc-\ning ST performance using speech representation disentanglement. Notably, even not being trained in the MTL setting, our model achieves the same BLEU score as the baseline model trained in the MTL setting, but with a reduced training time. Comparison to Cascaded Baselines We compared our method with several cascaded baselines. Espnet (Inaguma et al., 2020) and the model proposed by Ye et al. (2021) are two strong cascaded systems trained using MuST-C and external ASR and MT data. From Table 2, we find that as an end-toend model, our model outperforms these strong cascaded models even not using any external data."
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "To gain a deep understanding of the effect of components deployed in our proposed model, we conducted an ablation study by progressively removing the masking strategy, the cyclic reconstruction loss LCON and LNCON, and the speaker classifier loss LSPK and feature recontruction loss LREC. Results are shown in Table 3, which indicate that the masking strategy contributes to an average improvement of 0.3 BLEU. Besides, CCSRD without masking strategy can significantly enhance the translation performance, with an improvement of 0.6 BLUE achieved both in the En-De and En-It translation directions. Moreover, the cyclic reconstruction task plays an extremely important role in speech representation disentanglement, without which the per-\nformance drops substantially. Experimental results convincingly demonstrate the effectiveness of these approaches in improving ST performance."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Can CCSRD Learn Content-Centric Representations?",
            "text": "To empirically demonstrate the effectiveness of our model in learning content-centric representations, we plot the bivariate kernel density estimation (KDE) (Parzen, 1962) contour of speech and text dim-reduced representations to visualize their distributions Figure 2, where t-SNE (Van der Maaten and Hinton, 2008) is used to reduce the dimension of representations into 2D. Ideally, if the representations of speech have less non-linguistic information, their KDEs will be similar to the representations of text, resulting in the contour lines overlapping as much as possible. As illustrated in Figure 2, without CCSRD, the overlap between speech representation distribution and text representation distribution is small. In contrast, when CCSRD is applied, representations of different modalities become closer compared with those learned by the baseline. This suggests that speech representations contain more content information similar to that of text representations, which is beneficial to ST.\nWe also conducted experiments to analyze the disentanglement ability of CCSRD. We trained CCSRD and ST baseline on data with artificiallygenerated noise and compared their performance. Noise data were obtained by adding part of the information of another speech to the original speech input. Specifically, for a speech input si, we randomly selected another speech sj from the training set as noise, and add it to the si according to the preset weight of 0.15 to obtain a new noisy data: s \u2032 i = si + 0.15 \u2217 sj . Results of these experiments are shown in Table 4. We observe that the performance of CCSRD degrades slightly while the performance of the ST baseline drops substantially. It validates that our method is able to achieve successful disentanglement and that the content encoder is capable of learning high-quality content-centric representations.\nTo further compare the efficacy of our approach against the baseline, we visualize the crossattention matrices of the ST baseline and CCSRD in Figure 3. We obtain the attention matrix by averaging all heads of the last cross-attention layer. Our model confidently aligns target tokens to their corresponding speech frames respectively as in Figure 3, where the baseline model yields an incorrect translation with inappropriate attention weights."
        },
        {
            "heading": "5.2 Dose External MT Data Further Improve CCSRD ?",
            "text": "Many previous studies regard \u201cleveraging external MT data\u201d to be one of the advantages of their models and achieve better performance in the MTL setting. Therefore we further investigated CCSRD trained with external WMT16 En-De dataset. Results are shown in Table 5. We observe that CCSRD\ntrained with external MT data in the MTL setting achieves a further improvement of 2.1 BLUE while the MTL baseline obtains an improvement of 1.7 BLUE. We conjecture that with the external MT data for training the MT task in the MTL setting, CCSRD is able to learn more on content-to-targettranslation alignment and hence to generate better translations."
        },
        {
            "heading": "5.3 CCSRD in Low-Resource Scenarios",
            "text": "We extend the proposed CCSRD to ST in lowresource scenarios in the ST setting to investigate if it is still able to improve translation quality. We conducted experiments using different hours of speech data from the training dataset, simulating low-resource conditions. The results are shown in Figure 4. We reduced the ST data to 50 and 100 hours, corresponding to around 25K and 50K sentences. We observe that CCSRD is particularly helpful when the amount of speech data is small."
        },
        {
            "heading": "6 Conclusions",
            "text": "In this work, we have presented CCSRD, an E2E ST framework that explores speech representation disentanglement learning to capture content representations for ST. Experimental results validate the effectiveness of the proposed framework under both the ST task setting and the multi-task setting.\nIn-depth analyses demonstrate that CCSRD is capable of disentangling linguistic content from nonlinguistic speech factors. We would like to employ more speech factors to guide the training of the non-content encoder and explore the disentangled non-content representations for improving ST in the future.\nLimitations\nAlthough the proposed method facilitates ST to learn content-centric representation, and obtains significant improvements over previous methods, it still has some limitations: (1) We need labels of non-linguistic speech factors (e.g., speaker IDs, prosody labels) to guide the training of the noncontent encoder. (2) It is difficult to analyze the non-content representation since they may be related to many different speech factors. (3) Noncontent representations could also be used to improve ST, which we leave to our future work.\nEthics Statement\nThis work presents a framework and training strategies to help the model learn content-centric representations for speech translation. We evaluated our method on the widely-used standard benchmark dataset and did not introduce additional data that may cause ethics issues."
        },
        {
            "heading": "Acknowledgments",
            "text": "The present research was supported by the Natural Science Foundation of Xinjiang Uygur Autonomous Region (No. 2022D01D43) and the Key Research and Development Program of Yunnan Province (No. 202203AA080004). We would like to thank the anonymous reviewers for their insightful comments."
        },
        {
            "heading": "A Statistics of all datasets",
            "text": ""
        },
        {
            "heading": "B Experimental Details",
            "text": "Training and Implementation Details We implemented our model based on the fairseq toolkit and trained all models on 8 A6000 GPUs. During the training stage, we used Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.98, and learning rate = 1e\u22124 with warmup 25k steps during training. During the inference stage, we saved the checkpoints with the highest BLEU and averaged the last 10 checkpoints. For decoding, following previous studies (Ye et al., 2022), we used a beam size of 10 and a length penalty of 0.7 for German, 0.1 for Spanish, 0.5 for Italian and 0.4 for Russian. For the experiments in Table 5, we followed the previous works (Ye et al., 2021, 2022) and used the WMT16 En-De datasets as the external MT dataset, which contains 4.6M sentences. Baseline Model Details In Table 1, we compared our method with end-to-end baseline models under the setting of no transcription data being used (i.e., only training the ST task):\n\u2022 Fairseq ST (Wang et al., 2020a): a reimplemented model based on the Fairseq tooklit, which is trained with only the ST task data.\n\u2022 Self-training (Pino et al., 2020): a model trained with pseudo-labels.\n\u2022 SpecRec (Chen et al., 2021): a model trained with a spectrogram reconstruction technique.\n\u2022 Revisit ST (Zhang et al., 2022): a model that trained with several techniques like parameterized distance penalty and CTC-based regularization.\n\u2022 W2V2-Transformer (Fang et al., 2022): a model that has the same structure as our ST baseline.\nWe also compared our method against the following baseline models under the MTL setting using transcription data, (i.e., using the transcription data).:\n\u2022 XSTNet (Ye et al., 2021): a model that has the same structure as W2V2-Transformer but is adopted to the multi-task fine-tuning strategy.\n\u2022 STEMM (Fang et al., 2022): a model that bridges the modality representation gap by mixing up the speech representation sequences and text transcription embedding sequences.\n\u2022 ConST (Ye et al., 2022): a model that applies contrastive learning to bridge the modality gap between speech and transcriptions.\n\u2022 Mutual-learning (Zhao et al., 2021): a model that introduces a mutual-learning paradigm to iteratively learn and share the knowledge between MT and ST task.\n\u2022 SATE (Xu et al., 2021): a model that leverages an adapter to incorporate pre-trained ASR and MT models into E2E ST."
        },
        {
            "heading": "C Case Analysis",
            "text": "We present two translation examples yielded by CCSRD in comparison to those by the ST baseline model in Table 7. We observe that ST baseline cannot accurately translate some phrases, whereas CCSRD successfully conveys the meaning, and translates more accurately than ST baseline. This improvement might be due to the ability of CCSRD in capturing content representations."
        }
    ],
    "title": "CCSRD: Content-Centric Speech Representation Disentanglement Learning for End-to-End Speech Translation",
    "year": 2023
}