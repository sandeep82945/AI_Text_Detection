{
    "abstractText": "The task of Grammatical Error Correction (GEC) aims to automatically correct grammatical errors in natural texts. Almost all previous works treat annotated training data equally, but inherent discrepancies in data are neglected. In this paper, the inherent discrepancies are manifested in two aspects, namely, accuracy of data annotation and diversity of potential annotations. To this end, we propose MainGEC, which designs token-level and sentence-level training weights based on inherent discrepancies in accuracy and potential diversity of data annotation, respectively, and then conducts mixed-grained weighted training to improve the training effect for GEC. Empirical evaluation shows that whether in the Seq2Seq or Seq2Edit manner, MainGEC achieves consistent and significant performance improvements on two benchmark datasets, demonstrating the effectiveness and superiority of the mixedgrained weighted training. Further ablation experiments verify the effectiveness of designed weights of both granularities in MainGEC.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiahao Li"
        },
        {
            "affiliations": [],
            "name": "Quan Wang"
        },
        {
            "affiliations": [],
            "name": "Chiwei Zhu"
        },
        {
            "affiliations": [],
            "name": "Zhendong Mao"
        },
        {
            "affiliations": [],
            "name": "Yongdong Zhang"
        }
    ],
    "id": "SP:b7c09fb85ab960489b906d479aadb6e418f9b83a",
    "references": [
        {
            "authors": [
                "Abhijeet Awasthi",
                "Sunita Sarawagi",
                "Rasna Goyal",
                "Sabyasachi Ghosh",
                "Vihari Piratla."
            ],
            "title": "Parallel iterative edit models for local sequence transduction",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
            "year": 2019
        },
        {
            "authors": [
                "Christopher Bryant",
                "Mariano Felice",
                "\u00d8istein E. Andersen",
                "Ted Briscoe."
            ],
            "title": "The BEA-2019 shared task on grammatical error correction",
            "venue": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, BEA@ACL",
            "year": 2019
        },
        {
            "authors": [
                "Christopher Bryant",
                "Mariano Felice",
                "Ted Briscoe."
            ],
            "title": "Automatic annotation and evaluation of error types for grammatical error correction",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver,",
            "year": 2017
        },
        {
            "authors": [
                "Christopher Bryant",
                "Zheng Yuan",
                "Muhammad Reza Qorib",
                "Hannan Cao",
                "Hwee Tou Ng",
                "Ted Briscoe."
            ],
            "title": "Grammatical error correction: A survey of the state of the art",
            "venue": "CoRR, abs/2211.05166.",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Dahlmeier",
                "Hwee Tou Ng",
                "Siew Mei Wu."
            ],
            "title": "Building a large annotated corpus of learner english: The NUS corpus of learner english",
            "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,",
            "year": 2013
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Tao Ge",
                "Furu Wei",
                "Ming Zhou."
            ],
            "title": "Reaching human-level performance in automatic grammatical error correction: An empirical study",
            "venue": "CoRR, abs/1807.01270.",
            "year": 2018
        },
        {
            "authors": [
                "Masahiro Kaneko",
                "Masato Mita",
                "Shun Kiyono",
                "Jun Suzuki",
                "Kentaro Inui."
            ],
            "title": "Encoder-decoder models can benefit from pre-trained masked language models in grammatical error correction",
            "venue": "Proceedings of the 58th Annual Meeting of the Association",
            "year": 2020
        },
        {
            "authors": [
                "Shaopeng Lai",
                "Qingyu Zhou",
                "Jiali Zeng",
                "Zhongli Li",
                "Chao Li",
                "Yunbo Cao",
                "Jinsong Su."
            ],
            "title": "Typedriven multi-turn corrections for grammatical error correction",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland,",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jared Lichtarge",
                "Chris Alberti",
                "Shankar Kumar."
            ],
            "title": "Data weighted training strategies for grammatical error correction",
            "venue": "Trans. Assoc. Comput. Linguistics, 8:634\u2013646.",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Eric Malmi",
                "Sebastian Krause",
                "Sascha Rothe",
                "Daniil Mirylenka",
                "Aliaksei Severyn."
            ],
            "title": "Encode, tag, realize: High-precision text editing",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Hwee Tou Ng",
                "Siew Mei Wu",
                "Ted Briscoe",
                "Christian Hadiwinoto",
                "Raymond Hendy Susanto",
                "Christopher Bryant."
            ],
            "title": "The conll-2014 shared task on grammatical error correction",
            "venue": "Proceedings of the Eighteenth Conference on Computational Natural",
            "year": 2014
        },
        {
            "authors": [
                "Kostiantyn Omelianchuk",
                "Vitaliy Atrasevych",
                "Artem N. Chernodub",
                "Oleksandr Skurzhanskyi."
            ],
            "title": "Gector - grammatical error correction: Tag, not rewrite",
            "venue": "Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applica-",
            "year": 2020
        },
        {
            "authors": [
                "Sascha Rothe",
                "Jonathan Mallinson",
                "Eric Malmi",
                "Sebastian Krause",
                "Aliaksei Severyn."
            ],
            "title": "A simple recipe for multilingual grammatical error correction",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the",
            "year": 2021
        },
        {
            "authors": [
                "Felix Stahlberg",
                "Shankar Kumar."
            ],
            "title": "Synthetic data generation for grammatical error correction with tagged corruption models",
            "venue": "Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, BEA@EACL, Online, April 20,",
            "year": 2021
        },
        {
            "authors": [
                "Xin Sun",
                "Tao Ge",
                "Shuming Ma",
                "Jingjing Li",
                "Furu Wei",
                "Houfeng Wang."
            ],
            "title": "A unified strategy for multilingual grammatical error correction with pretrained cross-lingual language model",
            "venue": "Proceedings of the Thirty-First International Joint Confer-",
            "year": 2022
        },
        {
            "authors": [
                "Xin Sun",
                "Tao Ge",
                "Furu Wei",
                "Houfeng Wang."
            ],
            "title": "Instantaneous grammatical error correction with shallow aggressive decoding",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Xin Sun",
                "Houfeng Wang."
            ],
            "title": "Adjusting the precision-recall trade-off with align-and-predict decoding for grammatical error correction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Maksym Tarnavskyi",
                "Artem N. Chernodub",
                "Kostiantyn Omelianchuk."
            ],
            "title": "Ensembling and knowledge distilling of large sequence taggers for grammatical error correction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Peng Xia",
                "Yuechi Zhou",
                "Ziyan Zhang",
                "Zecheng Tang",
                "Juntao Li"
            ],
            "title": "Chinese grammatical error correction based on knowledge distillation",
            "year": 2022
        },
        {
            "authors": [
                "Helen Yannakoudakis",
                "Ted Briscoe",
                "Ben Medlock."
            ],
            "title": "A new dataset and method for automatically grading ESOL texts",
            "venue": "The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Con-",
            "year": 2011
        },
        {
            "authors": [
                "Zheng Yuan",
                "Ted Briscoe."
            ],
            "title": "Grammatical error correction using neural machine translation",
            "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2016
        },
        {
            "authors": [
                "Yue Zhang",
                "Zhenghua Li",
                "Zuyi Bao",
                "Jiacheng Li",
                "Bo Zhang",
                "Chen Li",
                "Fei Huang",
                "Min Zhang."
            ],
            "title": "Mucgec: a multi-reference multi-source evaluation dataset for chinese grammatical error correction",
            "venue": "Proceedings of the 2022 Conference of the",
            "year": 2022
        },
        {
            "authors": [
                "Yue Zhang",
                "Bo Zhang",
                "Zhenghua Li",
                "Zuyi Bao",
                "Chen Li",
                "Min Zhang."
            ],
            "title": "Syngec: Syntax-enhanced grammatical error correction with a tailored gecoriented parser",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The task of Grammatical Error Correction (GEC) aims to automatically correct grammatical errors in natural texts, which is extremely beneficial for language learners, such as children and non-native speakers (Bryant et al., 2022). The currently dominant neural GEC methods are categorized into two groups, i.e., Seq2Seq methods and Seq2Edit methods. Seq2Seq methods treat GEC as a monolingual translation task, regarding errorful sentences as the source language and error-free sentences as the target language (Yuan and Briscoe, 2016; Sun et al., 2021; Zhang et al., 2022b). Seq2Edit methods treat GEC as a sequence tagging task, which predicts a tagging sequence of edit operations to perform correction (Awasthi et al., 2019; Omelianchuk et al., 2020; Tarnavskyi et al., 2022).\n\u2217Corresponding author: Quan Wang.\nAccuracy of Data Annotation Sample 1: Their new house near the beach is very nice . Annotation: Your (\u2717) new house near the beach is very nice .\nSample 2: I read your email yesterday but I had n\u2019t had the time to reply yet . Annotation: I read your email yesterday but I have (\u2713) n\u2019t had the time to reply until now (\u2717) .\nSample 3: do you have best friend in your life ? Annotation: Do (\u2713) you have a best friend (\u2713) in your life ?\nDiversity of Potential Annotations Sample 4: Natural environment destroyed that is a people focus on frequently problem . Annotation: The natural environment is being destroyed . That is a problem people focus on frequently . Alternative: The natural environment is being destroyed ,\nwhich is a problem people focus on frequently .\nSample 5: Secondly there are not much variety of dessert mainly fruits and puddings . Annotation: Secondly , there is not a lot of variety in the desserts , mainly fruits and puddings . Alternative: Secondly , there are not many varieties of desserts , mainly fruits and puddings .\nSample 6: One of my favourite books are Diary of a Wimpy Kid . Annotation: One of my favourite books is Diary of a Wimpy Kid .\nTable 1: Instances from the BEA-19 (Bryant et al., 2019) training set to show the discrepancies in the annotated training data. Erroneous annotations are in red, correct annotations are in blue, and multiple potential annotations are in green.\nWhether in the Seq2Seq or Seq2Edit manner, almost all previous works treat annotated training data equally (Rothe et al., 2021; Tarnavskyi et al., 2022), that is, assigning the same training weight to each training sample and each token therein. However, inherent discrepancies in data are completely neglected, causing degradation of the training effect. Specifically, inherent discrepancies may be manifested in two aspects, i.e., accuracy of data annotation and diversity of potential annotations. The discrepancy in accuracy of data annotation refers to the uneven annotation quality, which is\ncaused by differences in the annotation ability of annotators and the difficulty of samples (Zhang et al., 2022a). For example, in Table 1, Sample 1 and Sample 2 contain annotation errors to varying degrees, while the annotation of Sample 3 is completely correct. The discrepancy in diversity of potential annotations refers to the different amounts of potential reasonable alternatives to annotation. Usually, it differs due to different sentence structures or synonymous phrases. For example, Sample 4 and 5 potentially have multiple reasonable annotations, while Sample 6 probably only has a single reasonable annotation. Due to the above data discrepancies, training data should be distinguished during the training process, by being assigned welldesigned weights.\nIn this paper, we propose MainGEC (i.e., Mixed-grained weighted training for GEC), which designs mixed-grained weights for training data based on inherent discrepancies therein to improve the training effect for GEC. First, we use a welltrained GEC model (called a teacher model) to quantify accuracy and potential diversity of data annotation. On the one hand, the accuracy of annotations is estimated by the generation probability of the teacher model for each target token, which represents the acceptance degree of the teacher model for the current annotation. Then, the quantified accuracy is converted into token-level training weights, as the accuracy of annotations may vary not only across samples but even across tokens in a single sample, e.g., sample 2 in Table 1. On the other hand, the diversity of potential annotations is estimated by the information entropy of output distribution of the teacher model for each training sample, which actually represents the uncertainty, i.e., diversity, of the target sentences that the teacher model is likely to generate. Then, the quantified potential diversity is converted into sentence-level training weights, considering that the potential annotations may involve the semantics and structures of the entire sentence. Finally, the token-level and sentence-level weigths constitute our mixedgrained weights for the training process.\nLichtarge et al. (2020) also considers to allocate training weights for samples. However, they only consider discrepancies in synthetic data and still treat human-annotated data equally, while the discrepancies we consider are across all data. Additionally, they only design sentence-level weighting, without token-level weighting considered in this pa-\nper. From another perspective, our method can be regarded as an \"alternative\" knowledge distillation method. Compared to Xia et al. (2022) applying general knowledge distillation on GEC, our method uses a teacher model to obtain mixed-grained training weights based on inherent discrepancies in data to guide the training process, rather than forcing the output distribution of the student model to be consistent with that of the teacher model.\nWe apply our mixed-grained weighted training to the mainstream Seq2Seq and Seq2Edit methods, and both of them achieve consistent and significant performance improvements on two benchmark datasets, verifying the superiority and generality of the method. In addition, we conduct ablation experiments, further verifying the effectiveness of the designed weights of both granularities. Besides, we conduct the comparative experiment with the general knowledge distillation method on GEC, verifying that our mixed-grained training weighting strategy outperforms the general knowledge distillation strategy.\nThe main contributions of this paper are summarized as follows: (1) We investigate two kinds of inherent discrepancies in data annotation of GEC for the first time, and propose MainGEC, which designs mixed-grained training weights based on the discrepancies above to improve the training effect. (2) The extensive empirical results show that MainGEC achieves consistent and significant performance improvements over the mainstream Seq2Seq and Seq2Edit methods on two benchmarks, proving the effectiveness and generality of our method for GEC."
        },
        {
            "heading": "2 Preliminary",
            "text": "This section presents the formulation of GEC task and currently mainstream Seq2Seq and Seq2Edit methods for GEC."
        },
        {
            "heading": "2.1 Problem Formulation",
            "text": "Grammatical Error Correction (GEC) is to correct grammatical errors in natural texts. Given an errorful sentence X = {x1, x2, \u00b7 \u00b7 \u00b7 , xm} with m tokens, a GEC system takes X as input, corrects grammatical errors therein, and outputs a corresponding error-free sentence Y = {y1, y2, \u00b7 \u00b7 \u00b7 , yn} with n tokens. In general, the target sentence Y often has substantial overlap with the source sentence X ."
        },
        {
            "heading": "2.2 Seq2Seq Methods",
            "text": "The Seq2Seq methods employ the encoder-decoder framework, where the encoder encodes the entire errorful sentence X into corresponding hidden states, and the decoder autoregressively generates each token in Y based on the hidden states and the previously generated tokens, as shown on the left in Figure 1.\nThe general objective function of the Seq2Seq methods is to minimize the negative log-likelihood loss:\nL(\u03b8) = \u2212 n\u2211\ni=1\nlog p(y\u0302i = yi|X,Y<i, \u03b8),\nwhere \u03b8 is learnable model parameters, y\u0302i is the i-th token predicted by the model, and Y<i = {y1, y2, \u00b7 \u00b7 \u00b7 , yi\u22121} denotes a set of tokens before the i-th token yi."
        },
        {
            "heading": "2.3 Seq2Edit Methods",
            "text": "Due to the substantial overlap between X and Y , autoregressive generation for the entire target Y is inefficient, and Seq2Edit methods is a good alternative. The Seq2Edit methods usually employ a sequence tagging model made up of a BERT-like encoder stacked with a simple classifier on the top, as shown on the right in Figure 1. At first, a predefined set of tags is required to denote edit operations. In general, this set of tags contains universal edits, (e.g. $KEEP for keeping the current token unchanged, $DELETE for deleting the current token, $VERB_FORM for conversion of verb forms, etc)1\n1Here we take GECToR\u2019s tags (Omelianchuk et al., 2020) for illustration.\nand token-dependent edits, (e.g. $APPEND_ei for appending a new token ei next to the current token, $REPLACE_ei for replacing the current token with another token ei). Considering the linear growth of tag vocab\u2019s size taken by token-dependent edits, usually, a moderate tag vocab\u2019s size is set to balance edit coverage and model size based on the frequency of edits. Then, the original sentence pair (X,Y ) is converted into a sentence-tags pair (X,T ) of equal length. Specifically, the target sentence Y is aligned to the source sentence X by minimizing the modified Levenshtein distance, and then converted to a tag sequence T = {t1, t2, \u00b7 \u00b7 \u00b7 , tm}. Refer to Omelianchuk et al. (2020) for more details.\nIn training, the general objective function of the Seq2Edit methods is to minimize the negative loglikelihood loss for the tag sequence:\nLs2e(\u03b8) = \u2212 m\u2211 i=1 log p(t\u0302i = ti|X, \u03b8),\nwhere t\u0302i is the i-th tag predicted by the model. During inference, Seq2Edit methods predict a tagging sequence T\u0302 at first, and then apply the edit operations in the source sentence X via post-processing to obtain the predicted result Y\u0302 ."
        },
        {
            "heading": "3 Our Approach",
            "text": "This section presents our approach, MainGEC which designs mixed-grained weights for training data based on inherent discrepancies therein to improve the training effect for GEC. Below, we first elaborate on how to quantify accuracy and potential diversity of data annotation at the token-level\nand sentence-level respectively, and convert quantified features to training weights of both granularities, correspondingly. Then, based on both-grained weights, the overall mixed-grained weighted training strategy is introduced. Figure 2 summarizes the overall architecture of MainGEC."
        },
        {
            "heading": "3.1 Token-Level Weights",
            "text": "Due to differences in the annotation ability of annotators and the difficulty of samples, there is a discrepancy in the accuracy of data annotation. Actually, this discrepancy exists not only across samples but even across tokens in a single sample. To this end, a well-trained GEC model is used to quantify the accuracy of data annotation for each token in all training samples, and then they are converted into token-level training weights.\nFor Seq2Seq Methods The source sentence X is fed into a well-trained Seq2Seq GEC model (called the teacher model), and the accuracy of the data annotation is estimated by the generation probability of the teacher model for each target token yi :\nAcc(yi) = p(y\u0302i = yi|X,Y<i, \u03b8T ),\nwhere i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n}, \u03b8T is parameters of the teacher model. Actually, this estimation implies the extend to which the teacher model agrees with the current annotation, which can be a measure of the accuracy. Then, quantified accuracy of data annotation for each target token can be directly regarded as the token-level training weight, as the higher accuracy of data annotation means the better annotation quality and thus a higher token-level training weight should be assigned for training. The\ntoken-level training weights for Seq2Seq methods is defined as:\nwtoken(yi) = Acc(yi).\nFor Seq2Edit Methods Similarly, the accuracy of the data annotation is estimated by the generation probability of a well-trained Seq2Edit teacher model for each target tag ti:\nAcc(ti) = p(t\u0302i = ti|X, \u03b8T ),\nwhere i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,m}. Correspondingly, the token-level training weights for each target tag is defined as:\nwtoken(ti) = Acc(ti)."
        },
        {
            "heading": "3.2 Sentence-Level Weigths",
            "text": "Due to different sentence structures or synonymous phrases, there can be multiple potential reasonable alternatives to the single target sentence Y of a training sample (X,Y ). Further, the amounts of potential reasonable alternatives may differ across all samples, which is referred to as the discrepancy in the diversity of potential annotations. Therefore, we quantify the diversity of potential annotations for each training sample by the same teacher model above, and convert them into sentence-level training weights.\nFor Seq2Seq Methods We feed the source sentence X into the teacher model to obtain the probability distribution of its prediction result. For this sample (X,Y ), the diversity of potential annotations is estimated by the information entropy of this distribution:\nDiv(X,Y ) = 1\nn n\u2211 i=1 H(y\u0302i|X,Y<i, \u03b8T ) log|V | ,\nwhere |V | is the vocab size and H() denotes the entropy of a random variable, with log|V | for normalization. Here, lower information entropy means that the teacher model produces a sparser and sharper probability distribution. This leads to the fact that fewer candidate target sentences are likely to be generated, i.e., there is less diversity of potential annotations therein. Further, this means the teacher model has more confidence for the target annotation, and a higher sentence-level training weight should be assigned during training. Therefore, a monotonically decreasing function and proper boundary processing are applied\nto the quantified diversity of potential annotations to obtain the sentence-level training weight for the sample (X,Y ):\nwsent(X,Y ) = Max[ log(Div(X,Y) + \u03f5)\nlog \u03f5 , \u03f5],\nwhere \u03f5 is a small positive quantity (e.g., e\u22129).\nFor Seq2Edit Methods Similarly, the diversity of potential annotations is estimated by the information entropy of output distribution of a Seq2Edit teacher model for a sample (X,T ):\nDiv(X,T ) = 1\nm m\u2211 i=1 H(t\u0302i|X, \u03b8T ) log|E| ,\nwhere |E| is the size of the pre-defined tag set. Correspondingly, the sentence-level training weight for the sample (X,T ) is defined as:\nwsent(X,T ) = Max[ log(Div(X,T) + \u03f5)\nlog \u03f5 , \u03f5]."
        },
        {
            "heading": "3.3 Mixed-Grained Weighted Training",
            "text": "The mixed-grained weighted training is to simply integrate both-grained weights into the training process. During training, the sentence-level weights determine the contribution of each sample to update the model parameters, while further tokenlevel weights are used to adjust the importance of each token/tag therein.\nFor Seq2Seq Methods We use the sentence-level and token-level weights as factors of the training loss for the samples and the tokens in them, respectively. The overall loss function of our mixedgrained weighted training is defined as:\nLw(\u03b8) = \u2212 \u2211\n(X,Y )\u2208D\nwsent(X,Y )\u2217\nn\u2211 i=1 wtoken(yi) \u2217 log p(y\u0302i = yi|X,Y<i, \u03b8),\nwhere D is all training corpus.\nFor Seq2Edit Methods Similarly, the loss function of our MainGEC for Seq2Edit methods is defined as:\nLw(\u03b8) = \u2212 \u2211\n(X,T )\u2208DT wsent(X,T )\u2217\nm\u2211 i=1 wtoken(ti) \u2217 log p(t\u0302i = ti|X, \u03b8),\nwhere DT is all training data after the tag transformation."
        },
        {
            "heading": "4 Experiments and Results",
            "text": "This section introduces our experiments and results on two benchmarks, i.e., CONLL-14 (Ng et al., 2014) and BEA-19 (Bryant et al., 2019). Then, we conduct ablation experiments on both-grained training weights and comparative experiments with the general knowledge distillation method. Finally, a case study is presented to visualize the weights in MainGEC."
        },
        {
            "heading": "4.1 Experimental Setups",
            "text": "Datasets and Evaluation Metrics As in Tarnavskyi et al. (2022), the training datasets we used consist of Troy-1BW (Tarnavskyi et al., 2022), CLang-82 (Rothe et al., 2021), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011), W&I+LOCNESS (Bryant et al., 2019). The statistics of the used datasets are shown in Table 2.\nFor evaluation, we consider two benchmarks, i.e., CONLL-14 and BEA-19. CONLL-14 test set is evaluated by official M2 scorer (Ng et al., 2014), while BEA-19 dev and test sets are evaluated by ERRANT (Bryant et al., 2017). Both evaluation metrics are precision, recall and F0.5.\nBaseline Methods We compare MainGEC against the following baseline methods. All these methods represent current state-of-the-art on GEC, in a Seq2Seq or Seq2Edit manner.\nSeq2Seq Methods\n\u2022 Lichtarge et al. (2020) introduces a sentencelevel training weighting strategy by scoring each sample based on delta-log perplexity, \u2206ppl, which represents the model\u2019s log per-\n2Here, CLang-8 is a clean version of Lang-8 used in Tarnavskyi et al. (2022).\nMethod Model Data Size Architecture CONLL-14 BEA-19\nP R F0.5 P R F0.5\nSeq2Seq\nLichtarge et al. (2020) 340M Transformer-big 69.4 43.9 62.1 67.6 62.5 66.5 Stahlberg and Kumar (2021) 540M Transformer-big 72.8 49.5 66.6 72.1 64.4 70.4 T5GEC (Rothe et al., 2021) 2.4M T5-large - - 66.1 - - 72.1 T5GEC (Rothe et al., 2021)\u2021 2.4M T5-xxl - - 68.8 - - 75.9\nSAD (Sun et al., 2021) 300M BART (12+2) - - 66.4 - - 72.9 BART (Zhang et al., 2022b) 2.4M BART 73.6 48.6 66.7 74 64.9 72.0 SynGEC (Zhang et al., 2022b) 2.4M BART + DepGCN 74.7 49.0 67.6 75.1 65.5 72.9\nBART (reimp)\u2020 2.4M BART 74.3 47.7 66.8 78.1 58.9 73.3 MainGEC (BART)\u2020 77.3 45.4 67.8 78.9 59.5 74.1\nSeq2Edit\nPIE (Awasthi et al., 2019) 1.2M BERT-large 66.1 43.0 59.7 - - - GECToR (Omelianchuk et al., 2020) 10.2M XLNET-base 77.5 40.1 65.3 79.2 53.9 72.4 TMTC (Lai et al., 2022) 10.2M XLNET-base 77.9 41.8 66.4 81.3 51.6 72.9 GECToR-L (Tarnavskyi et al., 2022) 3.6M RoBERTa-large 74.4 41.1 64.0 80.7 53.4 73.2\nLichtarge et al. (2020) (reimp) 3.6M RoBERTa-large 76.4 40.5 64.9 80.4 54.4 73.4\nGECToR-L (reimp) 3.6M RoBERTa-large 75.9 40.2 64.4 80.9 53.3 73.3MainGEC (GECToR-L) 78.9 39.4 65.7 82.7 53.8 74.5\n\u2022 SAD (Sun et al., 2021) employs an asymmetric Seq2Seq structure with a shallow decoder to accelerate training and inference efficiency of GEC.\n\u2022 BART (Zhang et al., 2022b) applies a multistage fine-tuning strategy on pre-trained language model BART.\n\u2022 SynGEC (Zhang et al., 2022b) extracts dependency syntactic information and incorporates it with output features of the origin encoder.\nSeq2Edit Methods\n\u2022 PIE (Awasthi et al., 2019) generates a tag sequence of edit operations and applys parallel decoding to accelerate inference.\nImplementation Details For the Seq2Seq implementation, BART-large (Lewis et al., 2020) is choosed as the model backbone. At first, we finetune BART with vanilla training as the teacher model with fairseq3 implementation. For a fair comparison with SynGEC (Zhang et al., 2022b), the training scheme here is to just fine-tune BART on the collection of all training sets excluding Troy1BW dataset, for just one stage. More training details are discussed in Appendix A.\nFor the Seq2Edit implementation, we choose GECToR-L based on RoBERTa (Liu et al., 2019) as the model backbone. The checkpoint released by GECToR-L is used for the teacher model4 to generate training weights of both granularities. We\n3https://github.com/pytorch/fairseq 4Please refer to Appendix B for effect of different teachers\non MainGEC.\nalso conduct 3-stage training as in GECToR-L. In Stage I, the model is pretrained on the Troy1BW dataset. Then, in Stage II, the model is fine-tuned on the collection of the CLang-8, NUCLE, FCE, and W&I+LOCNESS datasets, filtered out edit-free sentences. In Stage III, the model is fine-tuned on the W&I+LOCNESS dataset. All training hyperparameters used in MainGEC are set to their default values as in GECToR-L. Besides, we re-implement the most closely-related work, Lichtarge et al. (2020), based on GECToR-L for a more equitable comparison.\nAll checkpoints are selected by the loss on BEA19 (dev) and all experiments are conducted on 1 Tesla A800 with 80G memory."
        },
        {
            "heading": "4.2 Main Results",
            "text": "Table 3 presents the main results of Seq2Seq and Seq2Edit methods. We can see that whether in the Seq2Seq or Seq2Edit manner, MainGEC brings consistent performance improvements on both benchmarks, verifying the effectiveness of our method. Concretely, compared to vanilla training, our mixed-grained weighted training leads to 1.0/0.8 improvements in the Seq2Seq manner, and 1.3/1.2 improvements in the Seq2Edit manner. In addition, MainGEC outperforms all baselines on BEA-19 benchmark, with 1.2/1.3 improvements over previous SOTAs, while it also has a comparable performance on CONLL-14 benchmark. These results prove the superiority of our method."
        },
        {
            "heading": "4.3 Ablation Study",
            "text": "We also conduct ablation study on MainGEC to investigate the effects of both-grained training weights, in the Seq2Seq and Seq2Edit manners. Table 4 presents the ablation results. It is obviously observed that whether token-level or sentence-level training weights included in MainGEC, can bring a certain degree of improvement over the baseline. Moreover, the mixed-grained weighted training can provide more improvements on the basis of a single grained weighted training."
        },
        {
            "heading": "4.4 Exploration w.r.t Knowledge Distillation",
            "text": "As there is a \"teacher\" model used to obtain training weights in MainGEC, it is necessary to compare MainGEC with the general knowledge distillation method (Xia et al., 2022) for GEC, refered as KD. In KD, the probability distribution generated by the teacher model is regarded as a soft objective, which supervises the entire training process with the original groundtruth together. Here, we reimplement KD in the Seq2Edit manner, where the teacher model is the same as before and GECToR-L (RoBERTa-large) is choosed as the student model. The experimental result is presented in Table 5. As we can see, KD brings a significant improvement over the baseline, due to extra knowledge from the teacher model. More importantly, with the same teacher model, MainGEC outperforms KD with a considerable margin. This proves our our mixedgrained weighted training is superior to KD, forcing the output distribution of the student model to be consistent with that of the teacher model."
        },
        {
            "heading": "4.5 Case Study",
            "text": "Figure 3 shows the same cases as in Table 1 and their token-level or sentence-level weights obtained in MainGEC. The weights here are obtained in the Seq2Edit manner. As we can see, token-level and sentence-level weights in MainGEC indeed reflect the accuracy and potential diversity of data annotation respectively, to some extend. Specifically,\nfor those problematic annotation, MainGEC will assign a relatively low token-level weight, and vice versa. When there are multiple potential appropriate annotations for a single sample, only one objective contained in the training set will be assigned a relatively low sentence-level weight. For example, the sentence-level weights of Sample 4 and Sample 5 in Table 1 are relatively low due to multiple candidate sentence structures and synonymous phrases, respectively. This demonstrates that MainGEC is consistent with our motivation at first."
        },
        {
            "heading": "5 Related Works",
            "text": "GEC is a fundamental NLP task that has received wide attention over the past decades. Besides of the early statistical methods, the currently mainstream neural GEC methods are categorized into two groups, i.e., Seq2Seq methods and Seq2Edit methods, in general.\nSeq2Seq methods treat GEC as a monolingual translation task, regarding errorful sentences as the source language and error-free sentences as the target language (Yuan and Briscoe, 2016). Some works (Ge et al., 2018; Sun et al., 2022) generate considerable synthetic data based on the symmetry of the Seq2Seq\u2019s structure for data augmenta-\ntion. In addition, some works (Kaneko et al., 2020; Zhang et al., 2022b) feed additional features into the neural network to improve GEC, such as the BERT (Devlin et al., 2019) presentation or syntactic structure of the input sentence.\nSeq2Edit methods treat GEC as a sequence tagging task, which predicts a tagging sequence of edit operations to perform correction (Malmi et al., 2019). Parallel Iterative Edit (PIE) (Awasthi et al., 2019) and GECToR (Omelianchuk et al., 2020) define a set of tags representing the edit operations to be modelled by their system. Lai et al. (2022) investigates the characteristics of different types of errors in multi-turn correction based on GECToR. Tarnavskyi et al. (2022) applies multiple ensembling methods and knowledge distillation on the large version of the GECToR system."
        },
        {
            "heading": "6 Conclusion",
            "text": "This paper proposes MainGEC, which assigns mixed-grained weights to training data based on inherent discrepancies in data to improve the training effect for GEC. Our method uses a well-trained GEC model to quantify the accuracy and potential diversity of data annotation, and convert them into the mixed-grained weights for the training\nprocess. Whether in the Seq2Seq or Seq2Edit manner, MainGEC achieves consistent and significant performance improvements on two benchmark datasets, verifying the superiority and generality of the method. In addition, further ablation experiments and comparative experiments with the general knowledge distillation method provide more insights on both-grained training weights and the perspective of knowledge distillation.\nLimitations\nOur approach requires a well-trained model (called a teacher model) to obtain weights of two granularities before training. Therefore, compared to vanilla training, MainGEC has the additional preparation step to first acquire a teacher model (publicly released or trained by yourself) and then compute the weights by a forward propagation. In addition, the teacher model needs to be consistent with the weighted trained model in terms of type (Seq2Seq or Seq2Edit) and tokenizer."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank all the reviewers for their valuable advice to make this paper better. This research is supported by National Science Fund for Excellent Young Scholars under Grant 62222212 and the General Program of National Natural Science Foundation of China under Grant 62376033."
        },
        {
            "heading": "A Training Details",
            "text": "The hyper-parameters for MainGEC (BART) are listed in Table 6."
        },
        {
            "heading": "B Effect of Different Teachers",
            "text": "In MainGEC, a teacher is used to quantify training weights of both granularities, which is the main contribution of this work. To investigate effect of different teacher on MainGEC, we conduct comparative experiments under two settings: (1) Teachers of different model scales: we use GECTOR (RoBERTa-base) and GECTOR (RoBERTa-large) as the teacher respectively for weighted training of GECTOR (RoBERTa-base). (2) MainGEC with self-paced learning: we use MainGEC as a stronger teacher for a new round of weighted training, \u0131.e. iterative weighted training with MainGEC. The teacher used in the second round of training is the same model scale as the teacher used in the first round but performs better in GEC.\nTable 7 and Table 8 present the experiment results respectively. Experiment results show that no matter what teacher model you use, mixedgrained weights generated by them can bring improvement over the baseline, verifying effectiveness of MainGEC. Besides, this improvement is not sensitive to the choice of the teacher, either with different model sizes or with different performances in GEC."
        }
    ],
    "title": "Grammatical Error Correction via Mixed-Grained Weighted Training",
    "year": 2023
}