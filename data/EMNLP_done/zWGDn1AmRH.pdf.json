{
    "abstractText": "Text-to-SQL is the task that aims at translating natural language questions into SQL queries. Existing methods directly align the natural language with SQL Language and train one encoder-decoder-based model to fit all questions. However, they underestimate the inherent structural characteristics of SQL, as well as the gap between specific structure knowledge and general knowledge. This leads to structure errors in the generated SQL. To address the above challenges, we propose a retrievalargument framework, namely ReFSQL. It contains two parts, structure-enhanced retriever and the generator. Structure-enhanced retriever is designed to identify samples with comparable specific knowledge in an unsupervised way. Subsequently, we incorporate the retrieved samples\u2019 SQL into the input, enabling the model to acquire prior knowledge of similar SQL grammar. To further bridge the gap between specific and general knowledge, we present a mahalanobis contrastive learning method, which facilitates the transfer of the sample toward the specific knowledge distribution constructed by the retrieved samples. Experimental results on five datasets verify the effectiveness of our approach in improving the accuracy and robustness of Text-to-SQL generation. Our framework has achieved improved performance when combined with many other backbone models (including the 11B flan-T5) and also achieved state-of-the-art performance when compared to existing methods that employ the fine-tuning approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kun Zhang"
        },
        {
            "affiliations": [],
            "name": "Xiexiong Lin"
        },
        {
            "affiliations": [],
            "name": "Yuanzhuo Wang"
        },
        {
            "affiliations": [],
            "name": "Xin Zhang"
        },
        {
            "affiliations": [],
            "name": "Fei Sun"
        },
        {
            "affiliations": [],
            "name": "Jianhe Cen"
        },
        {
            "affiliations": [],
            "name": "Xuhui Jiang"
        },
        {
            "affiliations": [],
            "name": "Hexiang Tan"
        },
        {
            "affiliations": [],
            "name": "Huawei Shen"
        }
    ],
    "id": "SP:7763c66e505a3263867ec0488c04d56cb3985965",
    "references": [
        {
            "authors": [
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Jordan Hoffmann",
                "Trevor Cai",
                "Eliza Rutherford",
                "Katie Millican",
                "George Bm Van Den Driessche",
                "Jean-Baptiste Lespiau",
                "Bogdan Damoc",
                "Aidan Clark"
            ],
            "title": "Improving language models by retrieving from tril",
            "year": 2022
        },
        {
            "authors": [
                "Ruisheng Cao",
                "Lu Chen",
                "Zhi Chen",
                "Yanbin Zhao",
                "Su Zhu",
                "Kai Yu."
            ],
            "title": "Lgesql: line graph enhanced text-to-sql model with mixed local and nonlocal relations",
            "venue": "arXiv preprint arXiv:2106.01093.",
            "year": 2021
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton."
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "International conference on machine learning, pages 1597\u20131607. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Ting Chen",
                "Yizhou Sun",
                "Yue Shi",
                "Liangjie Hong."
            ],
            "title": "On sampling strategies for neural networkbased collaborative filtering",
            "venue": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 767\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Roy De Maesschalck",
                "Delphine Jouan-Rimbaud",
                "D\u00e9sir\u00e9 L Massart."
            ],
            "title": "The mahalanobis distance",
            "venue": "Chemometrics and intelligent laboratory systems, 50(1):1\u201318.",
            "year": 2000
        },
        {
            "authors": [
                "Xiang Deng",
                "Ahmed Hassan Awadallah",
                "Christopher Meek",
                "Oleksandr Polozov",
                "Huan Sun",
                "Matthew Richardson."
            ],
            "title": "Structure-grounded pretraining for text-to-sql",
            "venue": "arXiv preprint arXiv:2010.12773.",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Longxu Dou",
                "Yan Gao",
                "Mingyang Pan",
                "Dingzirui Wang",
                "Wanxiang Che",
                "Dechen Zhan",
                "Jian-Guang Lou."
            ],
            "title": "Unisar: A unified structure-aware autoregressive language model for text-to-sql",
            "venue": "arXiv preprint arXiv:2203.07781.",
            "year": 2022
        },
        {
            "authors": [
                "Yujian Gan",
                "Xinyun Chen",
                "Qiuping Huang",
                "Matthew Purver",
                "John R Woodward",
                "Jinxia Xie",
                "Pengsheng Huang."
            ],
            "title": "Towards robustness of textto-sql models against synonym substitution",
            "venue": "arXiv preprint arXiv:2106.01065.",
            "year": 2021
        },
        {
            "authors": [
                "Yujian Gan",
                "Xinyun Chen",
                "Matthew Purver."
            ],
            "title": "Exploring underexplored limitations of crossdomain text-to-sql generalization",
            "venue": "arXiv preprint arXiv:2109.05157.",
            "year": 2021
        },
        {
            "authors": [
                "Yujian Gan",
                "Xinyun Chen",
                "Jinxia Xie",
                "Matthew Purver",
                "John R Woodward",
                "John Drake",
                "Qiaofu Zhang."
            ],
            "title": "Natural sql: Making sql easier to infer from natural language specifications",
            "venue": "arXiv preprint arXiv:2109.05153.",
            "year": 2021
        },
        {
            "authors": [
                "Chia-Hsiang Kao",
                "Wei-Chen Chiu",
                "Pin-Yu Chen."
            ],
            "title": "Maml is a noisy contrastive learner in classification",
            "venue": "arXiv preprint arXiv:2106.15367.",
            "year": 2021
        },
        {
            "authors": [
                "Haoyang Li",
                "Jing Zhang",
                "Cuiping Li",
                "Hong Chen."
            ],
            "title": "Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql",
            "venue": "Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI).",
            "year": 2023
        },
        {
            "authors": [
                "Huayang Li",
                "Yixuan Su",
                "Deng Cai",
                "Yan Wang",
                "Lemao Liu."
            ],
            "title": "A survey on retrieval-augmented text generation",
            "venue": "arXiv preprint arXiv:2202.01110.",
            "year": 2022
        },
        {
            "authors": [
                "Mingzhe Li",
                "Xiexiong Lin",
                "Xiuying Chen",
                "Jinxiong Chang",
                "Qishen Zhang",
                "Feng Wang",
                "Taifeng Wang",
                "Zhongyi Liu",
                "Wei Chu",
                "Dongyan Zhao"
            ],
            "title": "2022b. Keywords and instances: A hierarchical contrastive learning framework unifying hybrid granularities",
            "year": 2022
        },
        {
            "authors": [
                "Xi Victoria Lin",
                "Richard Socher",
                "Caiming Xiong."
            ],
            "title": "Bridging textual and tabular data for crossdomain text-to-sql semantic parsing",
            "venue": "arXiv preprint arXiv:2012.12627.",
            "year": 2020
        },
        {
            "authors": [
                "Aiwei Liu",
                "Xuming Hu",
                "Lijie Wen",
                "Philip S Yu."
            ],
            "title": "A comprehensive evaluation of chatgpt\u2019s zero-shot text-to-sql capability",
            "venue": "arXiv preprint arXiv:2303.13547.",
            "year": 2023
        },
        {
            "authors": [
                "Ye Liu",
                "Semih Yavuz",
                "Rui Meng",
                "Dragomir Radev",
                "Caiming Xiong",
                "Yingbo Zhou."
            ],
            "title": "Uniparser: Unified semantic parser for question answering on knowledge base and database",
            "venue": "arXiv preprint arXiv:2211.05165.",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Long",
                "Wei Yin",
                "Thalaiyasingam Ajanthan",
                "Vu Nguyen",
                "Pulak Purkait",
                "Ravi Garg",
                "Alan Blair",
                "Chunhua Shen",
                "Anton van den Hengel."
            ],
            "title": "Retrieval augmented classification for long-tail visual recognition",
            "venue": "Proceedings of the IEEE/CVF",
            "year": 2022
        },
        {
            "authors": [
                "Jiexing Qi",
                "Jingyao Tang",
                "Ziwei He",
                "Xiangpeng Wan",
                "Chenghu Zhou",
                "Xinbing Wang",
                "Quanshi Zhang",
                "Zhouhan Lin."
            ],
            "title": "Rasat: Integrating relational structures into pretrained seq2seq model for text-tosql",
            "venue": "arXiv preprint arXiv:2205.06983.",
            "year": 2022
        },
        {
            "authors": [
                "Torsten Scholak",
                "Nathan Schucher",
                "Dzmitry Bahdanau."
            ],
            "title": "Picard: Parsing incrementally for constrained auto-regressive decoding from language models",
            "venue": "arXiv preprint arXiv:2109.05093.",
            "year": 2021
        },
        {
            "authors": [
                "Peter Shaw",
                "Ming-Wei Chang",
                "Panupong Pasupat",
                "Kristina Toutanova"
            ],
            "title": "Compositional generalization and natural language variation: Can a semantic parsing approach handle both? arXiv preprint arXiv:2010.12725",
            "year": 2020
        },
        {
            "authors": [
                "Chenglei Si",
                "Zhe Gan",
                "Zhengyuan Yang",
                "Shuohang Wang",
                "Jianfeng Wang",
                "Jordan Boyd-Graber",
                "Lijuan Wang."
            ],
            "title": "Prompting gpt-3 to be reliable",
            "venue": "arXiv preprint arXiv:2210.09150.",
            "year": 2022
        },
        {
            "authors": [
                "Kai Wang",
                "Weizhou Shen",
                "Yunyi Yang",
                "Xiaojun Quan",
                "Rui Wang."
            ],
            "title": "Relational graph attention network for aspect-based sentiment analysis",
            "venue": "arXiv preprint arXiv:2004.12362.",
            "year": 2020
        },
        {
            "authors": [
                "Fei Xiao",
                "Liang Pang",
                "Yanyan Lan",
                "Yan Wang",
                "Huawei Shen",
                "Xueqi Cheng."
            ],
            "title": "Transductive learning for unsupervised text style transfer",
            "venue": "arXiv preprint arXiv:2109.07812.",
            "year": 2021
        },
        {
            "authors": [
                "Tianbao Xie",
                "Chen Henry Wu",
                "Peng Shi",
                "Ruiqi Zhong",
                "Torsten Scholak",
                "Michihiro Yasunaga",
                "Chien-Sheng Wu",
                "Ming Zhong",
                "Pengcheng Yin",
                "Sida I Wang"
            ],
            "title": "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text",
            "year": 2022
        },
        {
            "authors": [
                "Peng Xu",
                "Dhruv Kumar",
                "Wei Yang",
                "Wenjie Zi",
                "Keyi Tang",
                "Chenyang Huang",
                "Jackie Chi Kit Cheung",
                "Simon JD Prince",
                "Yanshuai Cao."
            ],
            "title": "Optimizing deeper transformers on small datasets",
            "venue": "arXiv preprint arXiv:2012.15355.",
            "year": 2020
        },
        {
            "authors": [
                "Tao Yu",
                "Chien-Sheng Wu",
                "Xi Victoria Lin",
                "Bailin Wang",
                "Yi Chern Tan",
                "Xinyi Yang",
                "Dragomir Radev",
                "Richard Socher",
                "Caiming Xiong."
            ],
            "title": "Grappa: Grammar-augmented pre-training for table semantic parsing",
            "venue": "arXiv preprint arXiv:2009.13845.",
            "year": 2020
        },
        {
            "authors": [
                "Tao Yu",
                "Michihiro Yasunaga",
                "Kai Yang",
                "Rui Zhang",
                "Dongxu Wang",
                "Zifan Li",
                "Dragomir Radev."
            ],
            "title": "Syntaxsqlnet: Syntax tree networks for complex and cross-domaintext-to-sql task",
            "venue": "arXiv preprint arXiv:1810.05237.",
            "year": 2018
        },
        {
            "authors": [
                "Tao Yu",
                "Rui Zhang",
                "Kai Yang",
                "Michihiro Yasunaga",
                "Dongxu Wang",
                "Zifan Li",
                "James Ma",
                "Irene Li",
                "Qingning Yao",
                "Shanelle Roman"
            ],
            "title": "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql",
            "year": 2018
        },
        {
            "authors": [
                "Victor Zhong",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Seq2sql: Generating structured queries from natural language using reinforcement learning",
            "venue": "arXiv preprint arXiv:1709.00103.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Relational databases, which serve as ubiquitous components within data management systems, offer a means of storing heterogeneous data types, encompassing text, integer, float, and various other formats. However, the proficient utilization of managing databases by ordinary users remains a challenge, primarily due to their limited proficiency in\n*Corresponding author.\ntranslating their information needs into the structured query language (SQL), which serves as the standard database language. To facilitate the querying process for non-professional users, researchers have introduced the Text-to-SQL task, which seeks to automate the translation of users\u2019 natural language questions into SQL queries.\nRecently, with the rapid development of language models, many work (Qi et al., 2022; Li et al., 2023) have leveraged one encoder-decoder-based model to fit the entire training set, directly transformed SQL queries into a serialization structure and aligned them with natural languages. However, such methods can easily lead to structural errors in the generated SQL, e.g., missing \"JOIN IN\" operations, conducting incorrect comparison operations, etc. The observed phenomenon can be attributed to the underestimation of the inherent structural characteristics as well as the gap between specific knowledge and general knowledge.\nNatural languages exhibit inherent characteristics where the appearance of each word is highly influenced by the preceding or following sequence. In contrast, as shown in Example 4 in Figure 1, SQL language possesses unique inherent structural characteristics, i.e., heavily relies on the database schema and SQL grammar but lacks robust natural language attributes. Previous methods that directly\nalign the distributions of natural language and SQL query may fail to capture these inherent structural characteristics in SQL .\nIn the real world, SQL structures exhibit significant variations due to diverse structural characteristics, such as different database schemas and complex SQL grammars. As illustrated in Figure 1, Example 1 and Example 2 undergo drastic changes in their SQL representations when the underlying schema differs. The conventional approach in Textto-SQL is to train a one-size-fits-all model to fit the entire training set, acquiring the general knowledge, i.e., the distribution in the global space. However, such models struggle to acquire specific knowledge, i.e. the distribution of the local space, and adapt to the varied samples. The gap between specific and general knowledge significantly contributes to structural errors in the generated SQL. This discrepancy is particularly pronounced in cases with complex structures, where the general distribution learned by the model fails to adequately fit these cases unless a substantial amount of data, which can be prohibitively expensive in practice, is available.\nTo overcome these challenges, we draw inspiration from the process of human cognition. As depicted in Figure 1, humans can compose appropriate SQL queries by referring to similar samples that share common specific knowledge, including question semantics, schema structures, or SQL queries. In this paper, we propose a retrieval-augmented framework for Text-to-SQL generation. To identify samples with comparable specific knowledge, we design a structure-enhanced retriever that takes into account question semantics and schema structure. Subsequently, we incorporate the retrieved samples\u2019 SQL into the input, enabling our model to acquire prior knowledge of similar SQL grammar. To further enhance the model\u2019s ability to acquire specific knowledge, we employ a contrastive learning approach. As highlighted in (Kao et al., 2021), the utilization of contrastive learning offers a viable solution to mitigate the gaps arising from disparate distributions. The retrieved samples serve as building blocks for constructing the specific distribution that corresponds to the current sample. Through contrastive learning, we guide the samples toward the specific knowledge distribution. However, quantifying the distance from a sample to the distribution poses a challenge within contrastive learning. To address this, we introduce the\nMahalanobis distance as a measure of the distance between the sample and the specific distribution. By utilizing the Mahalanobis distance, we employ a contrastive loss function to facilitate the transfer of the sample toward the specific knowledge distribution.\nIn general, our main contributions are listed as follows:\n\u2022 We propose a retrieval-augmentation framework for Text-to-SQL generation, which can adapt to samples with various inherent SQL characteristics and bridge the gap between specific knowledge and general knowledge.\n\u2022 To further bridge the gap between specific and general knowledge, we present a mahalanobis contrastive learning method, which facilitates the transfer of the sample toward the specific knowledge distribution.\n\u2022 We verify the effectiveness of the proposed framework on five widely-used datasets and achieve state-of-the-art performance in methods that employ the fine-tuning approach."
        },
        {
            "heading": "2 Preliminaries",
            "text": "Given a natural language query Q and a database schema S = (T , C), the objective is to generate the corresponding SQL query Y . The natural language query Q is represented as a sequence of tokens Q = {qi}|Q|i=1, while the schema S consists of a collection of tables T = {ti}|T |i=1 along with their associated columns C = {Ci}|C|i=1. The content of the database S is denoted as V . Each table ti contains a set of columns represented as Ci = {cij}|Ci|j=1. Similarly, table names and column names are tokenized, such that a table name ti consists of |ti| tokens and the same applies to column names. In this study, the predicted SQL query is presented as a sequence of tokens, Y = {yi}|Y|i=1."
        },
        {
            "heading": "3 Methodology",
            "text": "In this section, we will describe the proposed framework ReFSQL, which comprises two main components: the structure-enhanced retriever and the generator. The structure-enhanced retriever finds similar samples based on questions and schemas and constructing prompts using their corresponding SQL queries. On the other hand, the generator aims to bridge the gap between specific and general\nknowledge. It achieves this by employing the Mahalanobis distance to guide the sample toward the distribution of similar samples. Figure 2 provides an overview of the ReFSQL framework."
        },
        {
            "heading": "3.1 Structure-enhanced Retriever",
            "text": "In this section, we present the design of a structureenhanced retriever aimed at identifying samples that exhibit a similar knowledge to the current sample. In Text-to-SQL tasks, the question and schema play crucial roles in generating accurate SQL queries. To enhance the representation of the question and schema, we leverage the questionSQL pairs and schema linking techniques. Thus, the structure-enhanced retriever consists of two modules: the linking-structure-enhanced schema retriever (LSES retriever) and the SQL-structureenhanced question retriever (SQLSE retriever). We provide a detailed description of these modules below."
        },
        {
            "heading": "3.1.1 SQL-Structure-enhanced Question Retriever",
            "text": "In this section, we present an unsupervised method to enhance question representation via SQL structure information.\nIn our dataset, each sample comprises a SQL query and a corresponding question. In order to acquire effective question representations, we generate contrastive samples based on the similarity between SQL queries. Inspired by the methodology introduced in (Yu et al., 2018a), we leverage the tree structure of SQL queries to quantify their similarity. Specifically, we employ a straightforward yet powerful method called Tree-Edit-Distancebased (TED) similarity to measure the similarity\nbetween two tree formats of SQL queries, denoted as tr1 and tr2. This similarity can be calculated using the following equation:\n\u03b4t = TED(tr1, tr2) (1)\nHere, \u03b4t represents the similarity between tr1 and tr2. To construct the positive set, we randomly select k samples with the highest similarity scores, while for the negative set, we also choose k samples with the lowest similarity scores.\nWe employ the pre-trained Bert model (Devlin et al., 2018) with fixed parameters to encode the question and obtain its representation. The encoding process is performed as follows:\nhqi = Bert(qi), (2)\nHere, hqi denotes the representation of the question qi.\nWe adopt the contrastive learning framework proposed in (Chen et al., 2020) and employ a cross-entropy objective with negative samples as described in (Chen et al., 2017). For each sample qi, we denote the positive sample as q+i and the negative sample as q\u2212i . The training objective for the\npair ( hqi , hq+i ) with k pairs is defined as follows:\nLsc=\u2212log\n( exp(Dcos ( hqi , hq+i )/\u03c4 )\u2211\nq\u2217j\u2208Q exp(Dcos ( hqi , hq\u2217i ) /\u03c4)\n) (3)\nHere, q\u2217j \u2208 Q represents the sample constructed from positive and negative samples. \u03c4 is a temperature hyperparameter, and Dcos (h1,h2) denotes the cosine similarity h \u22a4 1 h2\n|h1|\u00b7|h2| .\nAfter obtaining the learned question representations, we employ the cosine similarity function to rank the samples. Finally, we select the top m similar samples for each sample."
        },
        {
            "heading": "3.1.2 Linking-Structure-based Schema Retriever",
            "text": "After we obtain m samples by the previous Section 3.1.1, we utilize neural-based models to perform a reranking task with a focus on the schema structure.\nIn this section, we introduce the linkingstructure-based schema retriever. The construction of the interaction graph, which captures the diverse relations between questions and databases, is elaborated. Following that, we adopt an unsupervised learning approach to derive the representation of the interaction graph. By leveraging this graph representation, we retrieve the relevant samples from the dataset.\nInteraction Graph Construction We extract a diverse range of relations as triplets, establishing connections between tokens derived from both the question and the schema. These triplets are then seamlessly integrated to form a comprehensive graph representation. Our primary focus centers around two distinct types of relations: schema encoding and schema linking.\n\u2022 Schema Encoding. Schema encoding relations pertain to the interconnections among schema items, explicitly delineating the structural information encapsulated within a database schema. Notable instances of such relations encompass BELONGS-TO, which denotes the association between a column and its corresponding table, and FOREIGN-KEY, which establishes a linkage between the foreign key in one table and the primary key in another table. By comprehensively capturing these schema encoding relations, a more holistic understanding of the schema\u2019s internal structure can be attained.\n\u2022 Schema linking. Schema linking relations encompass the associations between schema and question items. In line with RAT-SQL (Wang et al., 2020), we leverage n-gram matches to signify instances wherein the question references specific schema items. However, the detection of these relations has proven to be a formidable task in prior investigations, primarily owing to the prevalent discrepancy between natural language expressions and the\nexplicit names assigned to schema components. Consequently, we employ a distinction between exact matches and partial matches to mitigate the deleterious impact of imprecise correspondences, thereby minimizing the noise engendered by imperfect matches.\nInteraction Graph Learning The interaction graph learning module consists of two trainable components: the graph encoder and the edge decoder. The graph encoder encodes the interaction graph and the edge predictor is used as a downstream task. As mentioned above, the interaction graph is a heterogeneous graph. To model the content and structure of the interaction graph, we leverage R-GCN to encode the interaction graph. We denote the interaction graph of ith sample as gi, then the graph embedding hgi is calculated as,\nhgi = R-GCN(gi) (4)\nThen we use graph embedding learned above to retrieve the most similar samples through the cosine similarity function.\nWe design a simple yet effective SQL prompt to make the language model learn the SQL character during encoding. The input of ith sample mainly contains three parts, the question Qi, the database schema S, involving Si =< T , C >, and the SQL prompt SPi. We follow (Qi et al., 2022) to serialize the inputs. Formally,\nSPi = the similar SQL was:sqli1 |sqli2 | \u00b7 \u00b7 \u00b7 , sqlij , Xi = Qi|Si|t1 : c11, \u00b7 \u00b7 \u00b7 , c1|T1| | t2 : c21, \u00b7 \u00b7 \u00b7 |SPi (5) where sqlij is denoted as the jth SQL of similar samples for the i sample. ti is the table name, cij is the j-th column name of the i-th table. We adopt the \"|\" symbol to delineate the boundaries between Q, S, different tables, and SQL prompts. We utilize the \":\" symbol to differentiate between the table name and its corresponding columns."
        },
        {
            "heading": "3.2 Generator",
            "text": "In this section, we will describe the generator, which consists of two components: the encoder and the decoder."
        },
        {
            "heading": "3.2.1 Encoder",
            "text": "In our framework, encoders can be replaced, so we take the pre-trained model T5 as an example. For the ith sample,\nhXi = T5-Encoder(Xi) (6)\nwhere hXi is denoted as the encoded state of Xi."
        },
        {
            "heading": "3.2.2 Decoder",
            "text": "As described in Section 1, there exists a gap between the specific knowledge and the general knowledge which disturbs the learning of samples. To further bridge this gap, we introduce contrastive learning to guide the sample representation toward the distribution of similar samples and farther away from the distribution of dissimilar samples.\nContrastive Sample Construction To optimize the efficacy of the contrastive learning approach, we propose a more refined strategy for constructing contrastive samples. Following the methodology outlined in Section 3.1.1, we identify the top m samples, denoted as set M . From this set, we select the top k samples after applying the linking-structure-enhanced retriever for constructing positive samples. Conversely, we construct a negative sample set from the remaining lowerranked samples in M . This process allows us to derive a distribution of similar semantic representations based on the positive samples, represented as f\u03d5(hx+) \u223c N(\u00b5+, \u03c32 + ). Simultaneously, we obtain a distribution of dissimilar semantic representations based on the negative samples, denoted as f\u03d5(hx\u2212) \u223c N(\u00b5\u2212, \u03c32 \u2212 ).\nMahalanobis Contrastive Learning To transfer the sample representation toward a distribution of similar semantic samples and distance it from a distribution of dissimilar samples, we propose the employment of the Mahalanobis contrastive mechanism (Li et al., 2022b). This mechanism aims to minimize the margin between the sample representation hxi and the similar semantic distribution f\u03d5(hx+) \u223c N(\u00b5+, (\u03c32)+), while simultaneously maximizing the margin between the sample representation hxi and the dissimilar semantic distribution f\u03d5(hx\u2212) \u223c N(\u00b5\u2212, (\u03c32)\u2212).\nThe contrastive loss, denoted as Lma, can be defined as follows:\nLMA=\u2212log  exp ( Dma ( f\u03d5(hx+j ),hxi ) /\u03c4 ) \u2211\nxj\u2208M exp\n( Dma ( f\u03d5(hxj ),hxi ) /\u03c4 )  (7)\nHere, x\u2217j \u2208 M represents the samples retrieved by the current sample xi, and Dma refers to the Mahalanobis distance (De Maesschalck et al., 2000)\nbetween the representation of the current sample xi and the distribution of retrieved samples f\u03d5(hxi) \u223c N(\u00b5, \u03c32). This Mahalanobis distance can be calculated as (hxi \u2212\u00b5)\u03c32I(hxi \u2212\u00b5), where I denotes the identity matrix. The hyperparameter \u03c4 controls the temperature of the contrastive loss."
        },
        {
            "heading": "3.2.3 Training Details",
            "text": "The loss in the decoding stage mainly consists of two parts, the MLE loss, and the contrastive loss. Specifically, the MLE loss, denoted by LCE , is computed as follows:\nLCE = \u2212 1\nN N\u2211 i=1 V\u2211 j=1 yi,j log y\u0302i,j , (8)\nHere, N is the number of samples in the training data, while V represents the vocabulary size. yi,j denotes the true label of the j-th word in the ith sample, and y\u0302i,j represents the corresponding predicted probability generated by the model.\nThus, the overall loss is computed as follows:\nL = LCE + LMA, (9)\n."
        },
        {
            "heading": "4 Experiment",
            "text": "In this section, we present a comprehensive evaluation of our proposed framework, ReFSQL, using five extensively utilized benchmark datasets. The obtained results serve as strong evidence to validate the effectiveness of our method."
        },
        {
            "heading": "4.1 Datasets and Preprocessing",
            "text": "We conduct experiments on several widely-used Text-to-SQL datasets. The details are shown below.\nSpider Spider (Yu et al., 2018b) is a challenging benchmark that poses significant demands on the cross-domain and multi-table Text-to-SQL task. The dataset consists of a training set with 7,000 samples, a dev set with 1,034 samples, and a concealed test set with 2,147 samples.\nSpider-DK, Spider-Syn, and Spider-Realistic To evaluate the robustness of our model, we employ a training strategy using the Spider training set while assessing its performance on three distinct evaluation sets: Spider-DK (Gan et al., 2021b), Spider-Syn (Gan et al., 2021a), and SpiderRealistic (Deng et al., 2020). The Spider-DK evaluation set consists of 535 samples and focuses on the integration of domain knowledge to rephrase\nthe questions. In contrast, Spider-Syn comprises 1034 samples where synonyms are used to replace schema-related words in the questions. Lastly, Spider-Realistic, comprising 508 samples, involves the removal of explicitly mentioned column names in the questions to simulate real-world scenarios with more ambiguous queries.\nWikiSQL (Zhong et al., 2017) is a typical table-based question answering dataset, which has 80654 hand-annotated examples of questions, SQL queries and the corresponding answers from execution."
        },
        {
            "heading": "4.2 Baseline Models and Evaluation Metrics",
            "text": "We compare the proposed model with several baseline methods, including the current state-of-the-art model over the two benchmark datasets.\n\u2022 T5: (Shaw et al., 2020) applies the pre-trained T5 to text-to-SQL task.\n\u2022 RATSQL: (Qi et al., 2022) improves the encoder by adding the relation-aware selfattention module.\n\u2022 RESDSQL: (Li et al., 2023) the schema is ranked and relevant schema items are injected into the encoder, while the decoder generates the skeleton first and then the SQL query.\n\u2022 PICARD: (Scholak et al., 2021) improves the decoder by constraining beam search to generate grammatically correct SQL queries.\n\u2022 RAT-SQL + GRAPPA (Yu et al., 2020) designs a schema item classification pre-training\ntask to adapt the seq-to-seq model to the structured input.\n\u2022 LGESQL (Cao et al., 2021) integrates relational features using a line graph, and introduces an auxiliary task, called graph pruning, to enhance the encoder\u2019s capability.\n\u2022 UNIFIEDSKG (Xie et al., 2022) unifies 21 structured knowledge grounding tasks into one single text-to-text model.\n\u2022 Uni-Parser (Liu et al., 2022) proposes a unified semantic parser method for question answering (QA) on both KB and DB.\n\u2022 UNISAR (Dou et al., 2022) proposes a unified structure-aware model to solve text-toSQL across various settings.\n\u2022 ChatGPT (Liu et al., 2023) explores using chatgpt to solve text-to-SQL tasks.\nTo assess the performance of the Text-to-SQL parser, we employ three evaluation metrics: Exactset-Match accuracy (EM), Execution accuracy (EX), and answer accuracy (F1) (Liu et al., 2022; Zhong et al., 2017). The EM metric determines whether the predicted SQL query can precisely match the gold SQL query by converting them into a specialized data structure. On the other hand, the EX metric compares the execution results of the predicted SQL query with the gold SQL query. In practice, we combine the EM and EX scores to evaluate the model and select the model with the best overall performance."
        },
        {
            "heading": "4.3 Results on Spider and WikiSQL",
            "text": "We implement comprehensive experiments to verify the effectiveness of our framework. The framework is integrated with the RESDSQL backend model, which utilizes fine-tuned techniques. The experimental results are presented in Table 1. Our ReFSQL framework, when applied to the RESDSQL backbone model, outperforms all baseline models on the two benchmark datasets. The approach achieves state-of-the-art performance in methods that employ the fine-tuning approach. Specifically, after adapting to our framework, the RESDSQL-based model improves the EM by 1.8 on Spider and 1.5 on WiKiSQL compared with the original model. In addition, our framework also has an improvement effect on other metrics."
        },
        {
            "heading": "4.4 Analyses on Different Models",
            "text": "As mentioned above, our framework can be used for different fine-tuning approaches. To further validate the performance of our framework, we have integrated our framework with some different models, the results are shown in Table 2.\nOur framework demonstrates flexibility and effectiveness, allowing for adaptation with numerous backbone models. Remarkably, our framework yields notable improvements when applied to small-size models like T5-small. This suggests that our framework can bridge the performance gap between small and large models, achieving comparable effects. Furthermore, we conducted experiments using a larger scale model, Flan-T5, and observed a substantial improvement of nearly 2%. These results indicate that our framework can\ndeliver impressive performance even on large language models. Additionally, we investigated the impact of model size. As illustrated in Table 2, our framework consistently showcases larger performance gaps when compared to its T5-3B counterpart. This observation aligns with previous findings in other fine-tuning tasks, highlighting the ability of larger pre-trained models to capture more knowledge effectively."
        },
        {
            "heading": "4.5 Ablation Study",
            "text": "We conducted a comprehensive ablation study on the development set of the Spider dataset to thoroughly examine the individual impact of each module within our framework.\n\u2022 Structure-enhanced retriever To assess the efficacy of the structure-enhanced retriever, we conducted an experiment. Instead, we relied solely on Bert for calculating the semantic similarity between questions. We selected samples with higher ranks as positive examples and randomly sampled other samples as negative examples within the batch. The results, as presented in Table 3, demonstrate a decrease in EM and EX scores when this module is removed. This outcome suggests that the retriever plays a crucial role in obtaining similar samples and facilitating the model\u2019s acquisition of specific knowledge during the text-to-SQL process.\n\u2022 Mahalanobis Contrastive Learning mechanism We further investigate the efficacy of the mahalanobis contrastive learning mechanism in our framework. The results, presented in Table 3, indicate that removing this contrastive learning mechanism results in a decrease in the EM and EX metrics. This finding suggests that the contrastive learning mechanism plays a crucial role in guiding the representation of samples toward a distribution that encompasses similar samples."
        },
        {
            "heading": "4.6 Robustness",
            "text": "To evaluate the robustness of our framework, we train our model on the training set of the Spider dataset and assess its performance on three challenging Spider variants: Spider-DK, SpiderSyn, and SpiderRealistic. The results, presented in Table 4, reveal a surprising and significant performance advantage of RESDSQL(FlanT5)+NatSQL+ReFSQL overall strong competitors across all three datasets. This finding suggests that our framework can also enhance the robustness of Text-to-SQL parsers."
        },
        {
            "heading": "5 Related Work",
            "text": ""
        },
        {
            "heading": "5.1 Text-to-SQL",
            "text": "In recent studies, significant advancements have been made in enhancing the performance of seq-toseq models for text-to-SQL tasks, focusing on both the encoding and decoding stages. (Qi et al., 2022) introduce relation-aware self-attention into the T5 encoder, enabling the capture of important structural information such as schema linking. (Scholak et al., 2021) proposes a beam search constraint during inference to ensure the generation of grammatically correct decoding results. (Li et al., 2023)enhance the encoder by incorporating the most relevant schema items into the input sequence, leading to improved performance."
        },
        {
            "heading": "5.2 Retrieval-augmented generation",
            "text": "Retrieval-augmented generation, which incorporates large language models with external retrieval modules, has achieved impressive performance in various tasks in recent years (Li et al., 2022a). One line of research focuses on enhancing language models with retrieval modules to provide additional knowledge (Si et al., 2022; Borgeaud et al., 2022). Another approach involves leveraging retrieval techniques to extract useful information from the training data. For example, (Long et al.,\n2022) fuse the base image encoder with relevant images retrieved from the training data to address the challenge of long-tail visual recognition. (Xiao et al., 2021) incorporate relevant sentences in the target style to improve the unsupervised style transfer model"
        },
        {
            "heading": "6 Conclusion",
            "text": "This paper presents a research focus on the Textto-SQL task, aiming to improve SQL generation through a retrieval-augmented framework called Ref-SQL. The main objective is to address the gap between specific knowledge and general knowledge. To obtain specific knowledge, a structureenhanced retriever is devised to identify similar samples based on question semantics and schema structure. Additionally, a contrastive learning approach is employed to facilitate the transfer of samples towards a similar semantic distribution, further mitigating the aforementioned gap. The effectiveness of RefSQL is evaluated on five widely-used benchmark datasets, where it surpasses all baseline models, verifying its superior performance."
        },
        {
            "heading": "7 Limitations",
            "text": "To train the retrieval model effectively, it is necessary to allocate sufficient computing resources. Additionally, our research focus is limited to the English language due to its availability of a wide range of analytical tools and resources, which surpass those available for other languages."
        },
        {
            "heading": "8 Acknowledgement",
            "text": "Thanks to reviewers for their helpful comments on this paper. This paper is funded by the the National Natural Science Foundation of China (No.62172393, U1836206 and U21B2046), Zhongyuanyingcai program-funded to central plains science and technology innovation leading\ntalent program (No.204200510002), AntGroup Research Intern Program and Major Public Welfare Project of Henan Province (No.201300311200)."
        },
        {
            "heading": "A Example Appendix",
            "text": "This is a section in the appendix."
        }
    ],
    "title": "ReFSQL: A Retrieval-Augmentation Framework for Text-to-SQL Generation",
    "year": 2023
}