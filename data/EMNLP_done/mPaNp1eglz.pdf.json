{
    "abstractText": "In law, lore, and everyday life, loopholes are commonplace. When people exploit a loophole, they understand the intended meaning or goal of another person, but choose to go with a different interpretation. Past and current AI research has shown that artificial intelligence engages in what seems superficially like the exploitation of loopholes, but this is likely anthropomorphization. It remains unclear to what extent current models, especially Large Language Models (LLMs), capture the pragmatic understanding required for engaging in loopholes. We examined the performance of LLMs on two metrics developed for studying loophole behavior in humans: evaluation (ratings of trouble, upset, and humor), and generation (coming up with new loopholes in a given context). We conducted a fine-grained comparison of state-of-the-art LLMs to humans, and find that while many of the models rate loophole behaviors as resulting in less trouble and upset than outright non-compliance (in line with humans), they struggle to recognize the humor in the creative exploitation of loopholes in the way that humans do. Furthermore, only two of the models, GPT-3.5 and 3, are capable of reliably generating loopholes of their own, with GPT-3.5 performing closest to the human baseline.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sonia K. Murthy"
        },
        {
            "affiliations": [],
            "name": "Kiera Parece"
        },
        {
            "affiliations": [],
            "name": "Sophie Bridgers"
        },
        {
            "affiliations": [],
            "name": "Peng Qian"
        },
        {
            "affiliations": [],
            "name": "Tomer Ullman"
        }
    ],
    "id": "SP:21f6149400583bc2364afe03a24bc2e9156152b8",
    "references": [
        {
            "authors": [
                "Dario Amodei",
                "Christopher Olah",
                "Jacob Steinhardt",
                "Paul Francis Christiano",
                "John Schulman",
                "Dandelion Man\u00e9."
            ],
            "title": "Concrete problems in ai safety",
            "venue": "ArXiv, abs/1606.06565.",
            "year": 2016
        },
        {
            "authors": [
                "S. Bridgers",
                "L. Schulz",
                "T. Ullman."
            ],
            "title": "Loopholes, a window into value alignment and the learning of meaning",
            "venue": "Proceedings of the Tenth Annual Conference of the Cognitive Science Society.",
            "year": 2021
        },
        {
            "authors": [
                "Sophie Bridgers",
                "Maya Taliaferro",
                "Kiera Parece",
                "Laura Schulz",
                "Tomer Ullman."
            ],
            "title": "Loopholes: A window into value alignment and the communication of meaning",
            "venue": "PsyArXiv.",
            "year": 2023
        },
        {
            "authors": [
                "Lambert Deckers",
                "Philip L. Kizer."
            ],
            "title": "Humor and the incongruity hypothesis",
            "venue": "The Journal of Psychology, 90:215\u2013218.",
            "year": 1975
        },
        {
            "authors": [
                "Daniel Fried",
                "Nicholas Tomlin",
                "Jennifer Hu",
                "Roma Patel",
                "Aida Nematzadeh"
            ],
            "title": "Pragmatics in language grounding: Phenomena, tasks, and modeling approaches",
            "year": 2022
        },
        {
            "authors": [
                "H. Paul Grice."
            ],
            "title": "Logic and conversation",
            "venue": "Syntax and Semantics, 3:41\u201358.",
            "year": 1975
        },
        {
            "authors": [
                "Takeshi Hashimoto",
                "Taraneh Mojaverian",
                "Heejung S. Kim."
            ],
            "title": "Culture, interpersonal stress, and psychological distress",
            "venue": "Journal of Cross-Cultural Psychology, 43:527 \u2013 532.",
            "year": 2012
        },
        {
            "authors": [
                "Jennifer Hu",
                "Sammy Floyd",
                "Olessia Jouravlev",
                "Evelina Fedorenko",
                "Edward Gibson."
            ],
            "title": "A finegrained comparison of pragmatic language understanding in humans and language models",
            "venue": "ArXiv, abs/2212.06801.",
            "year": 2022
        },
        {
            "authors": [
                "Sophie Jentzsch",
                "Kristian Kersting."
            ],
            "title": "Chatgpt is fun, but it is not funny! humor is still challenging large language models",
            "venue": "arXiv preprint arXiv:2306.04563.",
            "year": 2023
        },
        {
            "authors": [
                "Victoria Krakovna",
                "Jonathan Uesato",
                "Vladimir Mikulik",
                "Matthew Rahtz",
                "Tom Everitt",
                "Ramana Kumar",
                "Zac Kenton",
                "Jan Leike",
                "Shane Legg."
            ],
            "title": "Specification gaming: the flip side of ai ingenuity",
            "venue": "placeholder.",
            "year": 2020
        },
        {
            "authors": [
                "Matthew Le",
                "Y-Lan Boureau",
                "Maximilian Nickel."
            ],
            "title": "Revisiting the evaluation of theory of mind through question answering",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Kyle Mahowald",
                "Anna A Ivanova",
                "Idan A Blank",
                "Nancy Kanwisher",
                "Joshua B Tenenbaum",
                "Evelina Fedorenko."
            ],
            "title": "Dissociating language and thought in large language models: a cognitive perspective",
            "venue": "arXiv preprint arXiv:2301.06627.",
            "year": 2023
        },
        {
            "authors": [
                "Rod A. Martin",
                "Thomas Ford."
            ],
            "title": "The Psychology of Humor: An Integrative Approach",
            "venue": "Elsevier Academic Press.",
            "year": 2018
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Introducing chatgpt",
            "venue": "placeholder.",
            "year": 2022
        },
        {
            "authors": [
                "ter Welinder",
                "Paul Francis Christiano",
                "Jan Leike",
                "Ryan J. Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155",
            "year": 2022
        },
        {
            "authors": [
                "Eyal Peer",
                "Laura Brandimarte",
                "Sonam Samat",
                "Alessandro Acquisti."
            ],
            "title": "Beyond the turk: Alternative platforms for crowdsourcing behavioral research",
            "venue": "Journal of Experimental Social Psychology, 70:153\u2013163.",
            "year": 2017
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam M. Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "ArXiv, abs/1910.10683.",
            "year": 2019
        },
        {
            "authors": [
                "Laura Ruis",
                "Akbir Khan",
                "Stella Rose Biderman",
                "Sara Hooker",
                "Tim Rocktaschel",
                "Edward Grefenstette."
            ],
            "title": "Large language models are not zero-shot communicators",
            "venue": "ArXiv, abs/2210.14986.",
            "year": 2022
        },
        {
            "authors": [
                "Stuart J. Russell."
            ],
            "title": "Human-compatible artificial intelligence",
            "venue": "Human-Like Machine Intelligence.",
            "year": 2021
        },
        {
            "authors": [
                "Maarten Sap",
                "Ronan Le Bras",
                "Daniel Fried",
                "Yejin Choi."
            ],
            "title": "Neural theory-of-mind? on the limits of social intelligence in large lms",
            "venue": "ArXiv, abs/2210.13312.",
            "year": 2022
        },
        {
            "authors": [
                "Maarten Sap",
                "Ronan Le Bras",
                "Daniel Fried",
                "Yejin Choi."
            ],
            "title": "Neural theory-of-mind? on the limits of social intelligence in large LMs",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3762\u20133780, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Natalie Shapira",
                "Mosh Levy",
                "Seyed Hossein Alavi",
                "Xuhui Zhou",
                "Yejin Choi",
                "Yoav Goldberg",
                "Maarten Sap",
                "Vered Shwartz."
            ],
            "title": "Clever hans or neural theory of mind? stress testing social reasoning in large language models",
            "venue": "ArXiv, abs/2305.14763.",
            "year": 2023
        },
        {
            "authors": [
                "Aarohi Srivastava",
                "Abhinav Rastogi",
                "Abhishek Rao",
                "Abu Awal Md Shoeb",
                "Abubakar Abid",
                "Adam Fisch",
                "Adam R Brown",
                "Adam Santoro",
                "Aditya Gupta",
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating",
            "year": 2022
        },
        {
            "authors": [
                "Tomer David Ullman."
            ],
            "title": "Large language models fail on trivial alterations to theory-of-mind tasks",
            "venue": "ArXiv, abs/2302.08399.",
            "year": 2023
        },
        {
            "authors": [
                "Karthik Valmeekam",
                "Alberto Olmo",
                "Sarath Sreedharan",
                "Subbarao Kambhampati."
            ],
            "title": "Large language models still can\u2019t plan (a benchmark for llms on planning and reasoning about change)",
            "venue": "ArXiv, abs/2206.10498.",
            "year": 2022
        },
        {
            "authors": [
                "Sampat",
                "Savan Doshi",
                "Siddharth Deepak Mishra",
                "Sujan Reddy",
                "Sumanta Patro",
                "Tanay Dixit",
                "Xudong Shen",
                "Chitta Baral",
                "Yejin Choi",
                "Noah A. Smith",
                "Hanna Hajishirzi",
                "Daniel Khashabi"
            ],
            "title": "Supernaturalinstructions: Generalization via declarative",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz",
                "Jamie Brew."
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "venue": "ArXiv,",
            "year": 2019
        },
        {
            "authors": [
                "Ping Yu",
                "Tianlu Wang",
                "Olga Golovneva",
                "Badr AlKhamissi",
                "Siddharth Verma",
                "Zhijing Jin",
                "Gargi Ghosh",
                "Mona Diab",
                "Asli Celikyilmaz."
            ],
            "title": "ALERT: Adapt language models to reasoning tasks",
            "venue": "Proceedings of the 61st Annual Meeting of the",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "In law, lore, and everyday life, loopholes are commonplace. When people exploit a loophole, they understand the intended meaning or goal of another person, but choose to go with a different interpretation. Past and current AI research has shown that artificial intelligence engages in what seems superficially like the exploitation of loopholes, but this is likely anthropomorphization. It remains unclear to what extent current models, especially Large Language Models (LLMs), capture the pragmatic understanding required for engaging in loopholes. We examined the performance of LLMs on two metrics developed for studying loophole behavior in humans: evaluation (ratings of trouble, upset, and humor), and generation (coming up with new loopholes in a given context). We conducted a fine-grained comparison of state-of-the-art LLMs to humans, and find that while many of the models rate loophole behaviors as resulting in less trouble and upset than outright non-compliance (in line with humans), they struggle to recognize the humor in the creative exploitation of loopholes in the way that humans do. Furthermore, only two of the models, GPT-3.5 and 3, are capable of reliably generating loopholes of their own, with GPT-3.5 performing closest to the human baseline."
        },
        {
            "heading": "1 Introduction",
            "text": "Imagine a child poking at their beans, dreaming of dessert. Their exasperated father tells them, \u201cYou can\u2019t have dessert until you eat some beans.\u201d The child groans, but then lights up, eats two beans, and holds out their hand for a cookie. The father rolls his eyes and begins saving up for law school.\nThis commonplace example showcases the exploitation of loopholes: a person understands what is asked of them, but does not want to comply with the request, nor disobey it outright. In this grey\narea, they instead act on an unintended interpretation of the directive.\nThe underlying mechanics of loophole behavior are quite sophisticated, and require an understanding of pretense, pragmatics, planning, and value. Despite this cognitive complexity, everyday experience, as well as recent research, suggests that loophole-seeking is frequent, intuitive, and emerges in children as young as 5 years of age (Bridgers et al., 2021).\nLoopholes have been a source of amusement and headache in fable and history dating back centuries. But more recently, the behavior of agents that \u2018do what you ask, but not what you want\u2019 has become a source of concern for people who study machine intelligence, as well as policy makers interested in AI safety (Russell, 2021; Amodei et al., 2016). The problem is not restricted to a particular model or algorithm, and there are scores of examples of different kinds of systems gaming their task specifications to minimize a loss function, or achieve an objective in a way unintended by the people who specified it (Krakovna et al., 2020). Such machines are described as \u2018creative\u2019 or \u2018cheating\u2019 or \u2018genie-like\u2019, but it should be stressed that they are not engaging in loopholes in the sense that they recover the original goal or intent and choose to act on a different interpretation. Rather, such algorithms are exactly maximizing a given loss function or achieving a given goal. It is the human designer that realizes that the goal being achieved is not the one they intended. Complaining that such systems are cheating is like saying a bridge that fell down is lazy because it didn\u2019t want to stay up. Still, such failures are revealing of the human-side challenges of fully and accurately specifying one\u2019s goals and intentions, especially as models grow more complex and it becomes more difficult to evaluate their capabilities.\nDespite the concern with loopholes in AI and machine learning, and the existence of many examples\nof machines seemingly finding loopholes in a given task specification, to our knowledge there has not yet been an explicit evaluation of the comprehension and production of loopholes in state-of-the-art language models. Large Language Models (LLMs) form the backbone of a large and increasing set of AI applications, and they demonstrate increasingly impressive abilities across a wide range of domains (see, e.g., Srivastava et al., 2022).1 The present study of loophole behavior is especially relevant to interactions with LLMs, where conveying task specifications has become dependent on crafting natural language prompts. Since people are likely carrying over their priors from human communication to this interaction, it is important to understand the extent to which LLMs can calibrate the full spectrum of compliance to non-compliance in response to the kinds of ambiguous instructions that are used colloquially by people.\nTesting loophole behavior explicitly in LLMs is useful for at least three reasons: First, it helps us better understand the scope and limits of pragmatic reasoning abilities in LLMs. These models are taken by some researchers as models of human reasoning and language understanding, and a better understanding of the scope and limitations of LLMs in capturing loophole behavior can also help inform cognitive models of this behavior in humans (e.g., Mahowald et al., 2023). Given that there is an increased understanding that LLMs do well at formal linguistic competence, but not at pragmatic language use (Mahowald et al., 2023), then to the degree that LLMs succeed in such tasks, they can help isolate what aspects of loophole rea-\n1Given the pace of advances in LLMs, any more specific statement about their current state would likely be outdated by the time this paragraph is read.\nsoning may be \u201csolved\u201d without further specialized reasoning about value, pretense, or mental states. If they don\u2019t, then hypotheses about how this reasoning is carried out in humans can help build out scaffolds and structures to support this reasoning in LLMs. Second, as a phenomenon, loophole behavior subverts the usual cooperative assumptions that are at the heart of pragmatics (Grice, 1975): among humans, the loophole actor can pretend they were trying to be compliant by exploiting the ambiguity inherent in language and social interaction for their own ends (i.e., claiming they honestly misunderstood). So, this behavior provides an important test bed for potentially hostile machine abilities. Third, explicitly testing loophole behavior as a task in machines helps address AI safety concerns that have relied on indirect examination.\nIn this work, we compared the performance of several different LLMs to humans on two tasks designed to assess the understanding of loopholes 2. By loopholes, we mean a class of behaviors where people intentionally misunderstand a given request, favoring a less likely though still possible interpretation in the service of one\u2019s own goals. We used two different tasks to assess loophole behavior: evaluation and generation. In the evaluation task, models and human participants were given vignettes that describe the actions of different protagonists (compliance, non-compliance, or loophole) when presented with a directive from another person. Models and humans were asked to evaluate the protagonist\u2019s behavior in response to the directive on three metrics: how funny the behavior would be to the other person, how much trouble the protagonist will get in with the other person\n2Data and code available at https://github.com/ skmur/LLLMs\nfor their behavior, and how upset the other person would be about the behavior. The expectation is that loophole behavior will result in more trouble and upset than compliance, but less so than outright defiance. Further, the creative exploitation of loopholes is expected to introduce an element of humor that neither compliant nor non-compliant behaviors would. In the generation task, models and humans were presented with vignettes that, in addition to the directive, describe the intentions of the protagonist (always at odds with the intention of the directive), and asked to provide a loophole that the protagonist could exploit.\nWe assessed models of various sizes and training objectives on these tasks: Tk-Instruct (Wang et al., 2022), Flan-T5 (Chung et al., 2022), GPT-3 (Brown et al., 2020), InstructGPT (Ouyang et al., 2022), and GPT-3.5 (ChatGPT). On the evaluation task, we find that while many of these models succeed in differentiating the relative amounts of trouble a protagonist would get in for compliant, non-compliant, and loophole behaviors, fewer models are able to effectively reason about the upset that another person would experience as a result of these behavior types. Further, none of the models we test differentiate these behavior types on how funny they would be to another person, suggesting an inability to perform the more complex reasoning about social conventions and expectations that allows people to recognize the humor in the creative exploitation of loopholes. When it comes to loophole production, we find that GPT-3.5 is the only model that approaches the human baseline, with loopholes far outnumbering all other response categories. GPT-3 comes close, producing only slightly more loopholes than non-compliant actions, while the remaining models largely produce actions that range from non-compliance, to negotiation, to lying, but most often, incoherent or irrelevant responses."
        },
        {
            "heading": "2 Experimental Paradigm",
            "text": "We evaluate humans and models on two behavioral tasks: evaluation of the costs and rewards of engaging in compliant, non-compliant, or loophole behaviors, and generation of loopholes. Evaluation stimuli are taken from Bridgers et al. (2023), consisting of 36 scenarios that describe a directive for the protagonist.\nFor the evaluation task, social consequences of the protagonist\u2019s behaviors were measured through three metrics: (1) how much trouble the protagonist\nwould get into performing the behavior, (2) how upset the other person would be about the protagonist\u2019s behavior, and (3) how funny the other person would find the protagonist\u2019s behavior. These metrics reflect critical distinctions among compliant, loophole, and non-compliant behaviors, and children as young as five years are shown to differentiate loophole behaviors when reasoning about these consequences (Bridgers et al., 2021). In line with Bridgers et al. (2023), we posit that ratings of trouble and upset, as metrics of penalty, are essential to the generation of loopholes, as the costs associated with direct refusal to a speaker\u2019s request may trigger search about possible alternative actions that the agent could take to achieve their own goals. Rating of humor, on the other hand, can be thought of as a reward for an action that is recognized as clever or unexpected, and may help counteract the penalties of ultimately not complying. Taken together, these metrics help assess the social vs. egocentric components of loophole behavior: while trouble describes the direct consequences of one\u2019s own behavior, both humor and upset require higher-order reasoning about others\u2019 mental states.\nIn the generation task, we study the ability to generate reasonable actions that fall in the grey area between full compliance and outright noncompliance. This rounds out our assessment of loophole behavior because once the costs of noncompliance have been assessed and the decision to engage in loophole behavior has been made, one must be able to reliably calibrate the grey area between personally displeasing compliance and costly non-compliance and produce the alternative actions that fall within this space."
        },
        {
            "heading": "2.1 Scenarios",
            "text": "For both tasks, we used the same set of 36 scenarios from Bridgers et al. (2023). These scenarios were constructed based on real-world anecdotes of loophole behavior provided by US adults in a separate survey. They were constructed to represent a diverse range of loopholes, including scalar reasoning, polysemy, and the scope of generalization that needs to be made to identify the unintended interpretation of a directive.\nIn each scenario, a protagonist is given an instruction by another person who occupies an upward power relation to them (e.g. their mother, boss, landlord, etc.). This upward power relation was chosen as previous findings suggest that people\nare more likely to exploit loopholes (compared to outright defiance) when the cost of non-compliance is relatively high, such as in an upward relationship (Bridgers et al., 2023).\nIn the evaluation task, the scenario includes the relationship between the protagonist and the person that issues the directive, the uttered directive, as well as the protagonist\u2019s behavior in response to the directive: either compliance, non-compliance, or exploiting a loophole (see Figure 1, left). In the generation task, the scenario includes the same background information about the relationship and the directive uttered by another person, as well as the protagonist\u2019s intentions. Models and humans are prompted to come up with a loophole response (see Figure 1, right). A sample of these scenarios are shown in Table 1 in Appendix B."
        },
        {
            "heading": "2.2 Collecting Human Responses",
            "text": "For the evaluation task, we used data from Bridgers et al. (2023). They recruited 180 US adults with above a 95% approval rating online via Prolific (Peer et al., 2017) for the evaluation task. Participants (Mage: 35.06; 50% female; 70% White, 9% Hispanic or Latinx, 7% Black/ African American, 6% Mixed, 5% Asian, 3% Other) were U.S. residents fluent in English and from diverse regional and educational backgrounds. An additional 7 participants were recruited but excluded from analysis due to failing an attention check. The survey took approximately 17 minutes to complete, and compensation was $4.04. Participants saw 12 scenarios: 4 ending in compliance, 4 in loopholes, and 4 in non-compliance (see Appendix A for more information on the experimental setup).\nFor the generation task, we recruited 52 participants on Prolific (Mage: 32.8, range: 18 to 59 years, 54% female, 2% non-binary) with a 95% approval rating, who lived in the U.S., and were fluent in English. The survey took approximately 9 minutes to complete, and compensation was $2.38. Participants were majority White (67%; 4% Black, 10% Hispanic or Latinx, 11% Asian, 6% multi-racial) from diverse regional and educational backgrounds. An additional 8 participants were recruited but excluded from analysis due to failure to pass an attention check. Participants were presented with a definition of loophole behavior and were then shown three examples of protagonists engaging in loopholes. Following this training, participants were asked to generate loopholes for a random subset of\n12 scenarios out of the 36 scenarios we designed."
        },
        {
            "heading": "2.3 Models",
            "text": "We test a variety of models that have been fine-tuned to follow instructions and align with human feedback. Among these are the 3B and 11B parameter Tk-Instruct models (Wang et al., 2022) and three Flan-T5 models (base: 250M parameters; XL: 3B parameters; XXL: 11B parameters) (Chung et al., 2022). These models are all based on T5 (Raffel et al., 2019) and instruction-finetuned on a diverse collection of tasks (Wei et al., 2022). These models were accessed via Huggingface (Wolf et al., 2019). We also test three OpenAI model instances, including InstructGPT (davinci-instruct-beta), GPT-3 (text-davinci-003), and GPT-3.5 (gpt-3.5-turbo). InstructGPT (175B parameters) was trained using supervised fine-tuning on human demonstrations (SFT) (Ouyang et al., 2022). GPT-3 (text-davinci-003) was trained using reinforcement learning with reward models trained on comparisons by humans (PPO) (Brown et al., 2020). The GPT-3.5 model (gpt-3.5-turbo) was trained using Reinforcement Learning from Human Feedback (RLHF) on an initial model that was fine-tuned on a dialogue data supplement to the InstructGPT training data (OpenAI, 2022).3 These models were accessed via the OpenAI API. We chose models that were representative of state-of-art systems (at the time of writing) for zero-shot, instruction-based prompting, and that have been used to test pragmatic reasoning in related works (Hu et al., 2022; Yu et al., 2023; Ruis et al., 2022)."
        },
        {
            "heading": "2.3.1 Implementation Details",
            "text": "We designed our model assessments to be as faithful as possible to the human experiments conducted by Bridgers et al. (2023). In particular, our experimental setup makes use of zero-shot prompting for both the evaluation task and generation tasks, following previous works (Hu et al., 2022). This allows us to assess the abilities that emerge through the training of the models we test, and not as a result of in-context adaptation to the task. Note that for the evaluation task we simplified the format from fill-in-the-blank to question-answer (i.e.,\n3In previous experiments we additionally tested the largest GPT-2 model (XL, 1.5B parameters; Radford et al., 2019), as a standard autoregressive language model baseline. However, its performance was so poor on both tasks that we excluded it from further analysis.\nHow much trouble with the protagonist get in for what they did?).\nFor the evaluation task, whose design is a straightforward question-answer format, we used a temperature of 0 for all models so as not to introduce unnecessary noise in the generations through a higher temperature. Since the models\u2019 output at this temperature is deterministic, we elicit a single sample per scenario. For the generation task, where more diverse model outputs were desired, we use the default temperature settings for all models, but recognize that different settings of this parameter may influence performance. Here, we elicited 5 samples per model via beam search."
        },
        {
            "heading": "2.3.2 Prompt Structure",
            "text": "Each prompt consisted of three parts: the task instruction, the scenario prepended with the label \u201cScenario:\u201d, and label for the relevant query. We use the same instructions given to the human participants.\nFor the evaluation task, the four valid answer options for each metric (see Section 2.4) were appended to their respective instruction lines so that they read: \u201cHow much trouble will the protagonist get into for what they did? The answer options are: no trouble, a little bit of trouble, trouble, a lot of trouble.\u201d; \u201cHow upset is the other person about what the protagonist did? The answer options are: not upset, a little bit upset, upset, very upset.\u201d; and \u201cHow funny is the protagonist\u2019s behavior to the other person? The answer options are: not funny, funny, a little bit funny, very funny.\u201d. To account for the models\u2019 sensitivity to prompt, we report the average of two versions of the evaluation prompt: one in which the valid answer options were presented in increasing order, and another in which they were presented in decreasing order. The query labels for this task corresponded to the metric for that prompt: \u201cTrouble:\u201d, \u201cUpset:\u201d, and \u201cFunny:\u201d.\nFor the generation task, we presented the following condensed version of the instructions given to humans: \u201cWe are interested in your ability to create \u2019loopholes\u2019. Here\u2019s what we mean by \u2019loophole behavior\u2019: People understand the actual intended meaning of a request, command, or rule, but choose to interpret things differently. Please give an example of a loophole [protagonist\u2019s name] could use.\u201d As noted before, the scenario for this task included information about protagonist\u2019s intention that is omitted in the prompt for the evaluation experiments, and the query label preceding the models\u2019\ngenerations was \u201cLoophole:\u201d."
        },
        {
            "heading": "2.4 Assessment Protocol",
            "text": ""
        },
        {
            "heading": "2.4.1 Behavior Evaluation Task",
            "text": "In the evaluation task, both human and model responses were restricted to a 4-point scale describing the amount of trouble, upset, and humor the protagonist\u2019s behavior would result in, for the three behavior types we study: complying, not complying, or exploiting a loophole. These responses were coded as follows: \u201cno trouble\u201d/\u201cnot upset\u201d/\u201cnot funny\u201d (0), \u201ca little bit of trouble\u201d/\u201ca little bit upset\u201d/\u201ca little bit funny\u201d (1), \u201ctrouble\u201d/\u201cupset\u201d/\u201cfunny\u201d (2), \u201cor a lot of trouble\u201d/\u201cvery upset\u201d/\u201cvery funny\u201d (3). The models\u2019 natural language generations were restricted to the valid answer options by appending them to the relevant instruction line, automatically coded into the corresponding numerical response on the 4-point scale, and then verified by a human. Model outputs that did not adhere to one of four valid answer options for each metric were considered invalid, and not included in Figure 2. Nearly all of the invalid responses were empty strings (see Appendix, Figure 4)."
        },
        {
            "heading": "2.4.2 Loophole Generation Task",
            "text": "For the generation task, we categorized human and model responses into 5 response types, using the following criteria:\n1. Loophole: behavior that is consistent with a possible interpretation of the parent\u2019s request, but not with the intended interpretation.\n2. Compliance: behavior that is consistent with the intended meaning of the parent\u2019s request.\n3. Non-compliance: behavior that is inconsistent with any possible interpretation of the parent\u2019s request, an outright refusal or defiance.\n4. Unclear: relevant and coherent behavior that cannot be clearly identified as loophole, compliance, or non-compliance, often due to a meaningful semantic ambiguity.\n5. Other: behavior does not meet any of the criteria above, often because it is incoherent or irrelevant.\nWe manually annotated all human and model outputs. Model names were hidden during the annotation and all outputs were randomized and divided equally across two annotators. The annotators reviewed all generations together to reach consensus on any contended labels through discussion."
        },
        {
            "heading": "3 Results",
            "text": ""
        },
        {
            "heading": "3.1 Behavior Evaluation",
            "text": "How much trouble will the protagonist get in for their behavior? As expected, humans rate compliance as resulting in almost no trouble for the protagonist, while both loopholes and non-compliance result in moderate amounts of trouble\u2014though, notably, less so for loophole behavior. We find that GPT-3.5 comes closest to human baselines at differentiating compliant, non-compliant, and loophole behaviors for this metric (p < 0.001). GPT-3 comes close, significantly differentiating non-compliant and loophole behavior (p = 0.024), but assigns a higher rating of trouble to compliant behaviors. While most of the remaining models, except FlanT5-base, are able to assess the relative amounts of trouble for the different behavior types consistent with how humans assess these behaviors (p < 0.001, FlanT5-XL; p < 0.001, FlanT5XXL; p = 0.007, Tk-Instruct-11B; p = 0.032, InstructGPT), they all significantly overestimate the costs associated with compliant behaviors. Though Tk-Instruct-3B assigns appropriately low trouble\nratings to compliance, it also underestimates the trouble associated with both non-compliance and loopholes, relative to humans.\nHow upset is the other person about the protagonist\u2019s behavior? Similar to the trouble metric, humans rate compliance as resulting in almost no upset to the other person, and loophole behavior as resulting in less upset than noncompliance. GPT-3.5 once again comes closest to this baseline with loopholes resulting in less upset than non-compliance (p = 0.014). GPT3 (p = 0.002) and Tk-Instruct-3B (p = 0.017) also perform reasonably well on this metric, with loopholes resulting in significantly less upset than non-compliance, but once again, overestimate the costs associated with compliance compared to the human baseline. InstructGPT and the smallest FlanT5 model all entirely fail to differentiate the behavior types on this metric, while Tk-Instruct11B performs similarly, differentiating loopholes from non-compliance (p = 0.019), but not compliance from loopholes (p = 0.676). The remaining two models, FlanT5-XL and XXL almost get the relative amount of upset correct by rating compliance lower than loophole behavior, but then fail to significantly differentiate loopholes from noncompliance (p = 0.186, FlanT5-XL; p = 0.132, FlanT5-XXL).\nHow funny is the protagonist\u2019s behavior to the other person? Far fewer models effectively dif-\nferentiate the behavior types for the metric of humor. While humans generally produce low ratings for all behavior types on this metric, they do reliably recognize some humor in the creative exploitation of loopholes. FlanT5-XXL, GPT-3, and GPT-3.5 all appear to have higher ratings for humor for loopholes than non-compliance, however this difference is not significant for any of the models (p = 0.198, FlanT5-XXL; p = 0.240, GPT-3; p = 0.545, GPT-3.5). Meanwhile, the 3 billion parameter Tk-Instruct model rates all of the behaviors as not funny, which could either reflect a lack of understanding about expectations or could be interpreted as a reasonable real-world response to an adult exploiting a loophole. The remaining models all fail to differentiate the three behavior types on this metric, with InstructGPT seeming to perform worst of all and generating almost entirely empty responses for all the behavior types (see Figure 4 in Appendix for information about the proportion of empty generations)."
        },
        {
            "heading": "3.2 Loophole Generation",
            "text": "When it comes to loophole production, GPT-3.5 (loopholes = 59.9%) is the only model that approaches the human baseline (loopholes = 77.4%), with significantly more loopholes than all other response categories. GPT-3 comes close, with the majority of its generations being categorized as loophole, but still a high proportion of non-compliant and unclear actions. The two Tk-Instruct models\nfare slightly better than the best InstructGPT model, in that they produce the fewest number of incoherent/irrelevant generations. Still, the most of their remaining generations constitute non-compliance or involve lying or negotiation, but do not achieve the criteria of a loophole. All three of the FlanT5 models overwhelmingly generate incoherent or irrelevant responses (\u201cother\u201d), even though the two largest\u2014XL and XXL\u2014fared reasonably at aspects of the evaluation task, suggesting a separation between these abilities in these models. Table 2-5 in Appendix D showcase examples of generated loopholes from models and humans.\nTo better understand the shortcomings of the models on this task, we more closely examined the responses coded as \u201cunclear.\u201d Unlike the \u201cother\u201d category, these responses were relevant and coherent, but often confused compliance and noncompliance without actually achieving the criteria of a loophole. Among the models with a significant proportion of this response type, GPT-3, GPT-3.5, and Tk-Instruct-3B had higher percentages of unclear responses that described negotiation between the protagonist and the other person in attempt to delay or avoid compliance (overall rate 13% for GPT-3, 11% for GPT-3.5, 7% for TkInstruct-3B). The other two models, InstructGPT and Tk-Instruct-11B, had higher percentages of unclear responses that described lying or deceitful actions, such as hiding and pretending (overall rate 12% for Tk-Instruct-11B, 11% for GPT-3.5, 6% for InstructGPT)."
        },
        {
            "heading": "4 Discussion",
            "text": "The metrics that we use to assess models\u2019 ability to evaluate different behavior types\u2014trouble, upset, and funny\u2014can be thought of as representing two different aspects of costs and rewards. A more egocentric perspective is assessed by the trouble metric, where the models have to reason mostly about the direct consequences of an action. However, reasoning about the amount of upset those actions might cause to another person or how funny they might find such actions, requires higher-order, Theory of Mind reasoning. Further, among the latter two metrics, humor is thought to require especially complex reasoning about social conventions and expectations, with some theories suggesting that humor arises from a violation of our mental patterns and expectations (Deckers and Kizer, 1975). We find that the models\u2019 performance track on the eval-\nuation task with the graded social and pragmatic complexity of these metrics. While most models are able to reason about the direct consequences of a protagonist\u2019s actions through the amount of trouble they would get into, fewer are able to perform the higher-order reasoning needed to differentiate these behaviors on upset, while none managed to do so for humor. These findings corroborate with those of recent work that have studied humor in LLMs (Hu et al., 2022; Jentzsch and Kersting, 2023), suggesting that models continue to struggle to reason about complex social conventions and expectations. While the phenomena we study have been found to be sensitive to a variety of cultural and demographic factors (Martin and Ford, 2018; Hashimoto et al., 2012) and would benefit from further exploration with more culturally diverse populations, the models we test are known to have been trained on text data that reflects a similar distribution of cultural backgrounds as the population that participated in our behavioral studies. Thus, the models\u2019 failures may reflect a lack of a deeper cognitive understanding of the expectations, social norms, and incongruities needed to reason about the social consequences of loopholes and echo the growing skepticism about recent models\u2019 Theory of Mind abilities (e.g. Shapira et al., 2023; Sap et al., 2022b; Ullman, 2023).\nWe also note some effects of model size and training. At 250M parameters, FlanT5-base is significantly smaller than any other model we tested, and also performs the worst across both the evaluation and generation tasks. In addition to their smaller parameter size compared to the GPT models, all the T5-based models we tested also have smaller data input than the GPT models, with T5\u2019s C4 corpus comprising just 60% of GPT-3\u2019s training data. When comparing the performance of our two families of T5-based models, Tk-Instruct\u2019s fine-tuning on the larger and more diverse SUP-NATINST benchmark (compared to the FlanT5 task set) appears to translate to improvements on our task. Still, there appear to be interesting exceptions to the benefits of scaling on our task. For example, InstructGPT shows comparable performance to the two Tk-Instruct models on the generation task, despite being significantly larger than either (175B parameters vs. 3B and 11B). This could be because InstructGPT was specifically fine-tuned to improve alignment and better follow instructions (https://openai.\ncom/research/instruction-following), suggesting that instruction fine-tuning may not necessarily improve the flexibility of pragmatic reasoning.\nOur study adds to a growing body of work evaluating LLMs\u2019 understanding of various pragmatic phenomena (Le et al., 2019; Hu et al., 2022; Valmeekam et al., 2022; Sap et al., 2022a; Ruis et al., 2022; Fried et al., 2022). Some of these phenomena, like deceit (Hu et al., 2022), approach the spirit of loopholes by probing understanding of misaligned values. Similar to the evaluation of conversational implicature in Ruis et al. (2022), our evaluation task also probes understanding of intentions, while additionally testing the costs and values associated with agreeing or refusing to comply with them, analogous to recent social commonsense reasoning benchmarks (Sap et al., 2022a). Our generation task goes beyond any of these formats to probe models\u2019 ability to produce pragmatic behavior, as opposed to choosing between answer options. When given information detailing the misalignment between the goals of different agents (e.g. the other person wants X, the protagonist wants Y), we assess whether models are able to generate reasonable actions that fall in the grey area between full compliance and outright noncompliance."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we compared the performance of several different LLMs to humans on two tasks designed to assess loophole comprehension. In the evaluation task, we find that a number of models, of varying sizes, are able to differentiate compliant, non-compliant, and loophole behaviors based on how much trouble the protagonist would get in for their action. However, fewer models are able to effectively differentiate these behavior types when it comes to reasoning about others\u2019 emotional reactions in response to such behaviors. We find that only three of the models, GPT-3.5, GPT-3, and TkInstruct-3B reflect human baselines on the metric of how upset the other person would be about the protagonist\u2019s behavior, in that they significantly differentiated loopholes from non-compliance and rated compliance with low levels of upset, with GPT-3.5 performing best . For the final metric\u2014 how funny the protagonist\u2019s behavior would be to the other person\u2014no model was able to recognize the humor in the creative exploitation of loopholes\nin the way that humans do. When it comes to loophole generation, we find that only GPT-3.5 approaches the human baseline, with loopholes far outnumbering all other response categories. GPT3 comes close, with slightly more loopholes than non-compliant actions. In the future, we are interested in how loophole behavior can be used as a testbed for developing techniques that could equip smaller models with similar human-like pragmatic reasoning capacity.\nLimitations\nWith the exception of the 175B parameter OpenAI models that were accessible via their API, our own limited computational resources only allowed us to test models up to 11B parameters in size. Significant performance improvement on all tasks studied in this work may be achieved with increasing model sizes in the unstudied range beyond 11B parameters. Additionally, while we believe a low temperature setting is mostly appropriate for the question-answer format of the behavior evaluation task, the lack of an exhaustive parameter search for the more open-ended generation task may underestimate model capacity. However, we think that using the default temperature parameter in the text generation process still enables a fair and generalizable comparison across models.\nEthics Statement\nA primary ethical concern of the present work is its engagement with a complex reasoning ability in machines that might lead to problematic consequences: the ability to exploit loopholes. Our assessment of LLMs capacity for loophole generation gives us a baseline understanding of how these models might be exploited for malicious purposes so that users can be better prepared for their interactions with these systems, and so that model developers can preemptively build the necessary guardrails against such vulnerabilities."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank members of the Harvard Computation, Cognition, and Development Lab for their helpful comments and discussion, as well as three anonymous reviewers for their helpful feedback. This research is funded by a MIT Simons Center for the Social Brain Postdoctoral Fellowship (SB) and a NSF Science of Learning and Augmented Intelligence Grant 2118103 (TU)."
        },
        {
            "heading": "A Details of Human Behavioral Study",
            "text": "In Bridgers et al. (2023), human participants saw 36 scenarios with upward, equal, and downward power relations, but we filtered the data to only those trials involving upward relations. Thus, for each participant we have their trouble, upset, and humor ratings for a random subset of 12 scenarios: 4 ending in compliance, 4 in loopholes, and 4 in non-compliance. Participants indicated these ratings by using a drop-down menu to fill in the blank in three sentences: (1) The protagonist will get into {select amount of trouble} with the other person for what s/he did, (2) The other person is {select amount of upset} about what the protagonist did, and (3) The other person thinks what the protagonist did is {select amount of funny}."
        },
        {
            "heading": "B Example Scenarios",
            "text": "Table 1 list selected examples of 36 scenarios from Bridgers et al. (2023)."
        },
        {
            "heading": "C Additional Analyses of Model Outputs",
            "text": "Figure 4 visualizes the proportion of non-empty and empty responses among decoded generation from models in the behavioral evaluation task."
        },
        {
            "heading": "D Samples of Generated Loopholes",
            "text": "Table 2-5 list selected samples from models\u2019 and humans\u2019 responses that are categorized as loopholes."
        }
    ],
    "title": "Comparing the Evaluation and Production of Loophole Behavior in Humans and Large Language Models",
    "year": 2023
}