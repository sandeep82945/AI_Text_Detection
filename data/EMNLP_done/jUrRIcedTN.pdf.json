{
    "abstractText": "Metaphorical language, such as \u201cspending time together\u201d, projects meaning from a source domain (here, money) to a target domain (time). Thereby, it highlights certain aspects of the target domain, such as the effort behind the time investment. Highlighting aspects with metaphors (while hiding others) bridges the two domains and is the core of metaphorical meaning construction. For metaphor interpretation, linguistic theories stress that identifying the highlighted aspects is important for a better understanding of metaphors. However, metaphor research in NLP has not yet dealt with the phenomenon of highlighting. In this paper, we introduce the task of identifying the main aspect highlighted in a metaphorical sentence. Given the inherent interaction of source domains and highlighted aspects, we propose two multitask approaches a joint learning approach and a continual learning approach based on a finetuned contrastive learning model to jointly predict highlighted aspects and source domains. We further investigate whether (predicted) information about a source domain leads to better performance in predicting the highlighted aspects, and vice versa. Our experiments on an existing corpus suggest that, with the corresponding information, the performance to predict the other improves in terms of model accuracy in predicting highlighted aspects and source domains notably compared to the single-task baselines.",
    "authors": [
        {
            "affiliations": [],
            "name": "Meghdut Sengupta"
        },
        {
            "affiliations": [],
            "name": "Milad Alshomary"
        },
        {
            "affiliations": [],
            "name": "Ingrid Scharlau"
        },
        {
            "affiliations": [],
            "name": "Henning Wachsmuth"
        }
    ],
    "id": "SP:0a4b48d179bb52219ba61cbd88f177c6cac65afa",
    "references": [
        {
            "authors": [
                "Daniel G Andriessen."
            ],
            "title": "Stuff or love? how metaphors direct our efforts to manage knowledge in organisations",
            "venue": "Knowledge Management Research & Practice, 6(1):5\u201312.",
            "year": 2008
        },
        {
            "authors": [
                "Raymond W Gibbs"
            ],
            "title": "Categorization and metaphor understanding",
            "year": 1992
        },
        {
            "authors": [
                "Andrew Goatly."
            ],
            "title": "The language of metaphors",
            "venue": "Routledge.",
            "year": 1997
        },
        {
            "authors": [
                "Jonathan Gordon",
                "Jerry Hobbs",
                "Jonathan May",
                "Michael Mohler",
                "Fabrizio Morbini",
                "Bryan Rink",
                "Marc Tomlinson",
                "Suzanne Wertheim."
            ],
            "title": "A corpus of rich metaphor annotation",
            "venue": "Proceedings of the Third Workshop on Metaphor in NLP, pages 56\u201366, Den-",
            "year": 2015
        },
        {
            "authors": [
                "Raia Hadsell",
                "Dushyant Rao",
                "Andrei A Rusu",
                "Razvan Pascanu."
            ],
            "title": "Embracing change: Continual learning in deep neural networks",
            "venue": "Trends in cognitive sciences, 24(12):1028\u20131040.",
            "year": 2020
        },
        {
            "authors": [
                "Pengcheng He",
                "Xiaodong Liu",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "Deberta: Decoding-enhanced bert with disentangled attention",
            "venue": "arXiv preprint arXiv:2006.03654.",
            "year": 2020
        },
        {
            "authors": [
                "Matthew Henderson",
                "Rami Al-Rfou",
                "Brian Strope",
                "Yun hsuan Sung",
                "L\u00e1szl\u00f3 Luk\u00e1cs",
                "Ruiqi Guo",
                "Sanjiv Kumar",
                "Balint Miklos",
                "Ray Kurzweil."
            ],
            "title": "Efficient natural language response suggestion for smart reply",
            "venue": "ArXiv e-prints.",
            "year": 2017
        },
        {
            "authors": [
                "Janice Johnson",
                "Juan Pascual-Leone."
            ],
            "title": "Developmental levels of processing in metaphor interpretation",
            "venue": "Journal of Experimental Child Psychology, 48(1):1\u201331.",
            "year": 1989
        },
        {
            "authors": [
                "Zixuan Ke",
                "Bing Liu",
                "Nianzu Ma",
                "Hu Xu",
                "Lei Shu."
            ],
            "title": "Achieving forgetting prevention and knowledge transfer in continual learning",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 22443\u201322456. Curran Associates, Inc.",
            "year": 2021
        },
        {
            "authors": [
                "George Lakoff."
            ],
            "title": "Image metaphors",
            "venue": "Metaphor and Symbol, 2(3):219\u2013222.",
            "year": 1987
        },
        {
            "authors": [
                "George Lakoff",
                "Mark Johnson."
            ],
            "title": "Metaphors We Live By",
            "venue": "University of Chicago Press.",
            "year": 2003
        },
        {
            "authors": [
                "Hongsong Li",
                "Kenny Q. Zhu",
                "Haixun Wang."
            ],
            "title": "Data-driven metaphor recognition and explanation",
            "venue": "Transactions of the Association for Computational Linguistics, 1:379\u2013390.",
            "year": 2013
        },
        {
            "authors": [
                "Yucheng Li",
                "Shun Wang",
                "Chenghua Lin",
                "Frank Guerin",
                "Loic Barrault."
            ],
            "title": "FrameBERT: Conceptual metaphor detection with frame embedding learning",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Rui Mao",
                "Chenghua Lin",
                "Frank Guerin."
            ],
            "title": "Word embedding and wordnet based metaphor identification and interpretation",
            "venue": "Proceedings of the 56th annual meeting of the association for computational linguistics. Association for Computational Linguis-",
            "year": 2018
        },
        {
            "authors": [
                "Bruce Maxwell."
            ],
            "title": "teacher as professional\u2019as metaphor: What it highlights and what it hides",
            "venue": "Journal of Philosophy of Education, 49(1):86\u2013106.",
            "year": 2015
        },
        {
            "authors": [
                "George A Miller."
            ],
            "title": "Wordnet: a lexical database for english",
            "venue": "Communications of the ACM, 38(11):39\u201341.",
            "year": 1995
        },
        {
            "authors": [
                "Andrew Ed Ortony."
            ],
            "title": "Metaphor and thought",
            "venue": "Cambridge University Press.",
            "year": 1993
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "arXiv preprint arXiv:1908.10084.",
            "year": 2019
        },
        {
            "authors": [
                "Shani Robins",
                "Richard E Mayer."
            ],
            "title": "The metaphor framing effect: Metaphorical reasoning about text-based dilemmas",
            "venue": "Discourse Processes, 30(1):57\u201386.",
            "year": 2000
        },
        {
            "authors": [
                "Sebastian Ruder."
            ],
            "title": "An overview of multi-task learning in deep neural networks",
            "venue": "arXiv preprint arXiv:1706.05098.",
            "year": 2017
        },
        {
            "authors": [
                "Josef Ruppenhofer",
                "Michael Ellsworth",
                "Myriam Schwarzer-Petruck",
                "Christopher R Johnson",
                "Jan Scheffczyk."
            ],
            "title": "Framenet ii: Extended theory and practice",
            "venue": "Technical report, International Computer Science Institute.",
            "year": 2016
        },
        {
            "authors": [
                "Thomas Scialom",
                "Tuhin Chakrabarty",
                "Smaranda Muresan."
            ],
            "title": "Continual-t0: Progressively instructing 50+ tasks to language models without forgetting",
            "venue": "arXiv preprint arXiv:2205.12393.",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Scialom",
                "Tuhin Chakrabarty",
                "Smaranda Muresan."
            ],
            "title": "Fine-tuned language models are continual learners",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6107\u20136122, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Meghdut Sengupta",
                "Milad Alshomary",
                "Henning Wachsmuth."
            ],
            "title": "Back to the roots: Predicting the source domain of metaphors using contrastive learning",
            "venue": "Proceedings of the 3rd Workshop on Figurative Language Processing (FLP), pages 137\u2013142,",
            "year": 2022
        },
        {
            "authors": [
                "Ekaterina Shutova",
                "Lin Sun",
                "Anna Korhonen."
            ],
            "title": "Metaphor identification using verb and noun clustering",
            "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),",
            "year": 2010
        },
        {
            "authors": [
                "Ekaterina Shutova",
                "Simone Teufel",
                "Anna Korhonen."
            ],
            "title": "Statistical metaphor processing",
            "venue": "Computational Linguistics, 39(2):301\u2013353.",
            "year": 2013
        },
        {
            "authors": [
                "Ekaterina Shutova",
                "Tim Van de Cruys",
                "Anna Korhonen."
            ],
            "title": "Unsupervised metaphor paraphrasing using a vector space model",
            "venue": "Proceedings of COLING 2012: Posters, pages 1121\u20131130, Mumbai, India. The COLING 2012 Organizing Committee.",
            "year": 2012
        },
        {
            "authors": [
                "Gerard Steen."
            ],
            "title": "A method for linguistic metaphor identification: From MIP to MIPVU, volume 14",
            "venue": "John Benjamins Publishing.",
            "year": 2010
        },
        {
            "authors": [
                "Josef Stern."
            ],
            "title": "Metaphor in context",
            "venue": "mit Press.",
            "year": 2000
        },
        {
            "authors": [
                "Kevin Stowe",
                "Nils Beck",
                "Iryna Gurevych."
            ],
            "title": "Exploring metaphoric paraphrase generation",
            "venue": "Proceedings of the 25th Conference on Computational Natural Language Learning, pages 323\u2013336, Online. Association for Computational Linguistics.",
            "year": 2021
        },
        {
            "authors": [
                "Sam Witteveen",
                "Martin Andrews."
            ],
            "title": "Paraphrasing with large language models",
            "venue": "Proceedings of the 3rd Workshop on Neural Generation and Translation, pages 215\u2013220, Hong Kong. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Hans-Georg Wolf",
                "Frank Polzenhagen."
            ],
            "title": "Conceptual metaphor as ideological stylistic means: An exemplary analysis",
            "venue": "Dirven, R., Frank, R. & M. P\u00fctz. Cognitive Models in Language and Thought: Ideology, Metaphors and Meanings, pages 247\u2013275.",
            "year": 2003
        },
        {
            "authors": [
                "Rui Zhang",
                "Yangfeng Ji",
                "Yue Zhang",
                "Rebecca J. Passonneau."
            ],
            "title": "Contrastive data and learning for natural language processing",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Tianyu Zhao",
                "Junping Du",
                "Zhe Xu",
                "Ang Li",
                "Zeli Guan."
            ],
            "title": "Aspect-based sentiment analysis using local context focus mechanism with deberta",
            "venue": "arXiv preprint arXiv:2207.02424.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "A metaphor can be defined as a cross-domain conceptual mapping from a source domain to a target domain (Lakoff and Johnson, 2003). The abundance in which metaphors occur in everyday language (Gibbs, 1992; Ortony, 1993), such as \u201cwinning someone\u2019s heart\u201d or \u201ctax evasion\u201d, presents a need for metaphor interpretation and, to that end, the computational decoding of metaphors. Much like in other forms of figurative language, such\nmetaphorical sentence\nsource domain\nhighlighted aspect\n(a) Continual learning\n(b) Joint learning\ncontrastive learning encoder\ncontrastive learning encoder\nbest model\nfine-tuning phase 1\nfine-tuning phase 2\nas sarcasm, a central challenge with metaphors is that the implicit intended meaning differs from the meaning of the explicit metaphorical expression (Goatly, 1997; Allott and Textor, 2022). The context provided by the neighboring words in the sentence is an indicator of this implicit meaning (Stern, 2000). However, for better comprehending the meaning manifested by a metaphor in a given context, further levels of understanding are needed; for example, information about the source and target domain (Lakoff, 1987; Johnson and PascualLeone, 1989; Robins and Mayer, 2000). Consider the following metaphorical sentence:\n\u201cExcessive tax is killing American family business\u201d\nHere, the word \u201ckilling\u201d is used as a metaphor, since business as a concept cannot be killed by tax in physical realms. The intended meaning is manifested by establishing the mapping of two conceptual domains, drawing the meaning from the source domain (physical harm) and projecting the\nmeaning into the target domain (taxation). In this metaphorical context, the word killing is the literal representative of the metaphorical meaning construction. For simplicitly, we henceforth refer to this word as the literal metaphor.\nAccording to past research on metaphors (Lakoff and Johnson, 2003; Wolf and Polzenhagen, 2003; Andriessen, 2008; Maxwell, 2015), the core idea of a metaphor is to highlight certain aspects of its target domain while hiding others; both aspects have largely been disregarded so far in NLP. The more apparent and deliberate of these is highlighting. In the example above, a highlighted aspect may be threat, since the tax poses a threat in terms of the economic distress it can cause.\nOur research builds on the distinction between source and target domains in line with existing research on computational metaphor interpretation (Stowe et al., 2021). Beyond prior work, however, we argue based on aforementioned linguistic theories that it is also important to have an understanding of the aspects highlighted by a metaphor in a given context for better metaphor interpretation. To fill this gap, we provide the following contributions in the paper at hand:\n\u2022 We assess for the first time how and to what extent the aspects highlighted by a metaphor can be predicted computationally. In particular, we study the hypothesis that, by concurrently considering source domain and highlighted aspects and by effectively exchanging information between them, we can enhance their identification. In simpler terms, we investigate whether a joint modeling of source domains and highlighted aspects improves their predictability.\n\u2022 To implement our hypothesis, we develop two multitask contrastive learning approaches to the most highlighted aspect and the source domain in metaphorical sentences as illustrated in Figure 1: one using continual learning, the other using joint learning. We analyze whether, in this setup, involving the information of the highlighted aspects benefits the prediction performance of the model on the source domains, and vice versa.\nGiven the corpus of Gordon et al. (2015) with metaphorical sentences annotated for highlighted aspects and source domains, we evaluate different variations of our approaches against a single-task\nbaseline for both labels. Our results indicate that, in almost all cases, the continual learning approach outperforms a single-task setup, indicating that the combined information of source domains and highlighted aspects benefit the models to predict either of them. Our analysis of the results suggests that continual learning particularly learns to differentiate between single source domains and a composite source domains well."
        },
        {
            "heading": "2 Related Work",
            "text": "Highlighted and hidden aspects of metaphors contribute to the implicit intention conveyed by the metaphor in the given context. As we discuss in the following, different past research has utilized the source and the target domains in the computational analysis of metaphors. To the best of our knowledge, however, no one has investigated how aspects are highlighted by metaphors yet.\nShutova et al. (2013) defined an ideal metaphor processing system for NLP applications to consist of two components: metaphor detection and metaphor interpretation. In line with their work, Shutova et al. (2012) previously identified metaphors to then model metaphor interpretation as a paraphrasing task (Witteveen and Andrews, 2019). Similarly, Mao et al. (2018) used WordNet (Miller, 1995) to explore the contextual domains of metaphors to then successively identify and interpret metaphors as a use case for machine translation. The authors also design the interpretation step as a paraphrasing task.\nThe majority of computational research on metaphors in natural language has focused on metaphor identification so far, where the task is usually treated as a binary classification task: Given an input word or sentence, decide whether the meaning it represents is metaphorical or literal (Steen, 2010; Li et al., 2013). One of the pioneering works approached the identification with unsupervised spectral clustering techniques based on relevant parts of speech, such as nouns and verbs (Shutova et al., 2010). Using a seed phrase to learn similar metaphors associated with particular source domains, the authors adapt the conceptual mapping from one domain to the other. More recent approaches include that of Li et al. (2023) which employs FrameNet (Ruppenhofer et al., 2016) to identify metaphors where a RoBERTa-based pretrained language model (Liu et al., 2019) is finetuned to encode contextual cues of the concepts\nin the data from FrameNet for enhanced model performance. First they obtain a sentence representation with one encoder. Next, they employ a separate encoder to form FrameNet based representations of the concepts associated with the given literal metaphor. Finally they combine them with the sentence representations to obtain a joint representation, which is passed through a softmax layer for the final prediction. Unlike all these works, our research targets the broader interpretation of metaphors by exploiting components of metaphorical meaning construction, like source domains and highlighted aspects.\nBased on the founding work of Lakoff and Johnson (2003) previous work in metaphor interpretation has explored the source and target domains with the usage of FrameNet (Stowe et al., 2021) and in prediction of source domains (Sengupta et al., 2022). In the latter, they have proposed to predict source domains in given metaphorical sentences using contrastive learning (Zhang et al., 2022). We incorporate their contrastive learning idea in our approach and evaluate it on the same metaphor corpus (Gordon et al., 2015). However, not only is our main goal to predict highlighted aspects, but we also exploit the semantic connection between source domains and highlighted aspects using multitask learning."
        },
        {
            "heading": "3 Approach",
            "text": "In this section, we present how to predict both source domains and highlighted aspects jointly\nwith multitask learning. We start from our contrastive learning approach from previous work that we shortly summarize in the following subsection. Then, we propose two alternative multitask schemes for the prediction on this basis: continual learning and joint learning. A comparison of the two variants is shown in Figure 1."
        },
        {
            "heading": "3.1 Contrastive Learning for Metaphor Interpretation",
            "text": "As discussed in Section 2, Sengupta et al. (2022) demonstrated that modeling similarities between metaphorical sentences and their corresponding source domain through contrastive learning boosts the effectiveness of trained models on the source domain prediction task. The core idea is to learn an embedding space where the given input sentence representation and the representation of the most similar source domain is trained to be situated close to each other and the less likely source domain to be situated further apart.\nFor the work at hand, we extend their idea to both source domains and highlighted aspects. In particular, our hypothesis underlying the multitask approaches presented below is that the mutual semantic relations between source domains and highlighted aspects can be modeled in a the learned embedding space for their joint prediction. We illustrate this idea in Figure 2."
        },
        {
            "heading": "3.2 Continual Learning",
            "text": "Our first approach follows the idea of continual learning, illustrated in Figure 1(a). Research has shown that continual learning can be helpful in a variety of downstream tasks with regard to the performance improvement of machine learning models (Ke et al., 2021).\nFundamentally, continual learning is a machine learning technique where principally a model is trained on different tasks in a sequential order (Hadsell et al., 2020; Scialom et al., 2022b). This process directly preserves the information obtained in the first task and leverages that information in the next task where the model is finetuned (Scialom et al., 2022a).\nWe adapt this technique to train our models on the task of predicting the source domains and highlighted aspects sequentially. In particular, given a set of metaphorical sentences as input, we first finetune a Sentence-BERT (Reimers and Gurevych, 2019) with DeBERTa (He et al., 2020) as the encoder to learn the most similar source domains.\nNext, we take the best-performing model from this training phase on the validation set and continue fine-tuning it to predict the most similar highlighted aspects for the same set of sentences. Both training phases employ contrastive learning (as discussed in Section 3.1) by using the multiplenegatives ranking loss (Henderson et al., 2017) like Sengupta et al. (2022) where for a sentence representation a and a given correct highlighted aspect b for every positive pairs (ai, bi), for a negative pair ai for every highlighted aspect bj , j \u0338= i, if m = |A| = |B| and S is the similarity score computed from the sentence embeddings for the given pairs, the loss is computed as:\nL(a,b)\n= \u2212 1 m \u00b7 m\u2211 i=1 logPapprox(bi|ai)\n= \u2212 1 m \u00b7 m\u2211 i=1 ( S(ai, bi)\u2212 log m\u2211 j=1 eS(ai,bj) )\nThis loss function optimizes the embedding space such that the representation of the correct aspect highlighted and the representation of the sentence are positioned closer to each other while distancing the representation of the sentence from the incorrect concept representations as shown in figure 2. Similarly, we employ the same technique the other way round to first learn on highlighted aspects in order to then predict source domains."
        },
        {
            "heading": "3.3 Joint Learning",
            "text": "Our second approach follows the idea of joint learning, as illustrated in Figure 1(b). The approach employs a joint-learning technique where, at the core, it is trained via contrastive learning optimized with multiple-negatives ranking loss, as described in the previous subsection.\nWe adapt the multitask training procedure of the Sentence-BERT architecture of Reimers and\nGurevych (2019), where the training phase is optimized in a round-robin way. For a given metaphorical sentence, the loss is first computed for learning the correct source domain representation. Next, it is backpropagated to compute the loss for learning the representation of the highlighted aspect.\nHence, the training happens in an alternating fashion where information about the highlighted aspects and source domains is mutually exploited via a shared encoder with hard parameter sharing (Ruder, 2017). For comparability to the continual learning approach, we employ the same encoder to form the representations in this case as well."
        },
        {
            "heading": "4 Data",
            "text": "For our experiments, we employ the corpus of Gordon et al. (2015), which is to our knowledge the only metaphor corpus so far that is annotated for both source domains and highlighted aspects. In the following, we briefly summarize its characteristics and the dataset splits we use."
        },
        {
            "heading": "4.1 Corpus",
            "text": "The corpus of Gordon et al. (2015) consists of 1771 metaphorical sentences collected from press releases, news articles, weblog posts, online forum discussions, and social media. Each sentence is annotated for several concepts including the source domain, the target domain, and the literal metaphor. For each given metaphorical sentence, a highlighted aspect is annotated (referred to as schema slot in the data). For example, threat is an aspect highlighted by some sentences with the source domain addiction."
        },
        {
            "heading": "4.2 Datasets",
            "text": "We use the same dataset partitions as Sengupta et al. (2022). To obtain a clean experimental setting, the authors combined multiple source domains that were assigned to the same sentence into a single source domain (composite source domains) and removed duplicate sentences in the data to finally have 1429 instances. So for example, if a metaphorical sentence had multiple source domains such as competition, game, and war annotated, in their work the combination competition+game+war was treated as a single source domain, which was a different source domain from, for example, competition as hown in Table 2.\nIn the case of highlighted aspects, the only combination present is enemy/side. For consistency in the task design, we also treat it as a separate label. So, with a 70-30 train-test split, for each of our experiments, we have 1000 training samples, 128 validation samples, and 301 test samples.\nTable 1 shows the data distribution in the final corpus for our experiments. As shown in table 1 the experiment corpus has 1429 with 138 source domains and 78 highlighted aspects respectively - emphasizing the sparse label distribution for the downstream tasks of predicting source domains and highlighted aspects. This sparsity is intrinsic to metaphorical language due to its strong diversity."
        },
        {
            "heading": "5 Experiments",
            "text": "This section describes setup of the experiments we carried out on the data from Section 4 to study the effectiveness of the two proposed multitask learning approaches from Section 3. We present our basic experimental setup, before we give details on the single-task baseline that we compare to as well as on the two approaches.1\nTask Input A limitation of the approach of Sengupta et al. (2022) is that it appends the literal metaphor annotated in the corpus of Gordon et al. (2015) to the input metaphorical sentence (with a separator token <SEP> in between).\nIn a real-world setting this information may not be available. In our experiments, we predict source domains and highlighted aspects without the literal metaphor as input. However, to see the effectiveness of our approaches, we also report on the results\n1The source code of our experiments can be found here: https://github.com/webis-de/EMNLP-23\nwith the literal metaphor, with the corresponding single-task baseline setups.\nBaseline We compare our multitask approaches to a single-task approach, namely we use the contrastive learning approach of Sengupta et al. (2022) here, trained separately for each task.\nIn this setup, during training, the input sentence is first provided to the encoder in order to obtain the input sentence representation. Similarly, the corresponding label (highlighted aspect or source domain depending on the downstream task) is provided as an input to the same encoder to get the label representations - and hence the corresponding weights are shared.\nAfter that via contrasting learning, the input sentence representation is compared to all the incorrect label representations with paired cosine distance where the training procedure is optimized with multiple negatives ranking loss - which learns the embedding space of the representations such that the given input sentence and the correct label are situated in close proximity in the embedding space.\nAt inference, the model receives an input sentence in the encoder and forms the representation as stated before. Then it compares the sentence representation with all the labels present in the corpus for the corresponding downstream task, with paired cosine distance, and ranks all the labels in the order of their similarity. Finally, the top-ranked (most similar) label is chosen to be the prediction.\nFor our experiments with the metaphor appended to the input, we perform the training and inference similarly with the literal metaphor appended to the input sentence with a with a separator token <SEP> in both the cases.\nContrastive Learning Within the contrastive learning setting, we use DeBERTA (He et al., 2020), an enhanced encoder built on top of RoBERTa (Liu et al., 2019), which relies on a disentangled attention mechanism and has shown success in recent NLP research (Zhao et al., 2022). We employ it as the encoder for sentence-transformers in all evaluated model configurations.\nMultitask Learning To optimize the models of our multitask learning approaches, we performed a hyperparameter search over batch sizes from {4, 8}, learning rates in {2 \u00b7 10\u22125, 3 \u00b7 10\u22125, 4 \u00b7 10\u22125, 5 \u00b7 10\u22125}, and epochs in {4, 5, 6}.\nTo find the best checkpoint per experiment, we ran each model on each combination over the hy-\nperparameters for 20 iterations on the validation set. We then evaluated the optimal configuration in each case on the test set.2\nMetrics Given that the contrastive learning approach creates a ranking, we evaluate all models in terms of top-1, top-3 and top-5 accuracy.\nHere, top-1 accuracy means that only the highest-ranked output (that is, a highlighted aspect or a source domain, respectively) is chosen to be correct. In top-3 accuracy and top-5 accuracy, the output is seen as correct, if it is within the first three and first five ranks, respectively."
        },
        {
            "heading": "6 Results",
            "text": "The main results of our experiments are shown in Table 3. Overall, our continual learning approach largely outperforms both the single-task baseline and the joint learning approach in predicting both highlighted aspects and source domains. This suggests that the information of one of the concepts improves model performance on the other.\nThe limited performance of the joint learning approach may speak for that learning on two tasks concurrently confused the model in some cases rather than helping it to learn both tasks better. The following subsections discuss our results in detail."
        },
        {
            "heading": "6.1 Highlighted Aspects",
            "text": "We discuss the results separately for the two task variations: given only the metaphorical sentence as input, and additionally given the literal metaphor.\nWithout Literal Metaphor as Input Compared to the single-task baseline, the application of continual learning improves the top-1 accuracy by 1.1\n2Detailed hyperparameter configurations are provided in the appendix.\npoints from (0.524 to 0.535). While the top-3 accuracy is the same as the baseline, the top-5 accuracy surpasses it clearly (by 3.6 points), suggesting better overall representational capture in continual learning. A value of 86.7 means that it manages to put the right highlighted aspects into the top-5 in almost 7 out of 8 cases.\nWith Literal Metaphor as Input Given groundtruth information on the literal metaphor, the impact of multitask learning largely disappears (except for top-5 accuracy, where joint learning is strongest with 0.910). This suggests that the information about the literal metaphor suffices to tackle the task with the straightforward single-task model, while the multitask setting might increase complexity unnecessarily."
        },
        {
            "heading": "6.2 Source Domains",
            "text": "Again, we look at the two task variations one after the other.\nWithout Literal Metaphor as Input In the case of predicting source domains, the continual learning setup outperforms the single-task baseline with an overall top-1 accuracy increment by 3.4 points (0.522 vs. 0.488), while it does not seem to help in the case of top-3 and top-5 accuracies.\nWith Literal Metaphor as Input Finetuning our model to predict source domains in the setting where literal metaphors are added to the input improves the accuracy by 4.9 points, consistent to the case without the metaphor added to the input. The high gain over the single-task baseline indicates that the direct relationship of the metaphor with its source domain further benefits the learning capabilities of the model."
        },
        {
            "heading": "7 Analysis",
            "text": "To further investigate into our results we looked into the predictions of the two tasks for every experimental setup. In order to do that, we obtained the confusion matrices of the outcomes of our experiments (provided in the Appendix) and observed particularly which labels have an improvement resulting from our continual learning approach.\nWe also looked into the cases where both singletask baseline and continual learning fail to predict the correct outcome, to have a better idea where the approach can be improved further. We classify our analysis in two parts:\nImprovement Incorrect prediction by single-task but correct prediction by continual learning\nNo Gain Incorrect prediction by both single-task and continual learning"
        },
        {
            "heading": "7.1 Predicting Highlighted Aspects",
            "text": "In the following, we consolidate our main findings for highlighted aspects with selected examples.3\nImprovement We primarily observed that the continual learning approach performs better in detecting the aspects of threat and threatened, among others. For example, for the sentence \u201cTaxation destroys earnings and ability to save/invest...inflation destroys monetary wealth already owned.\u201d, the continual learning approach predicts the highlighted aspect correctly to be threat, unlike destruction potential predicted by the single-task baseline.\nContinual learning improves performance on the aspect barrier such as in the sentence \u201cThis would remove a mountain of taxation from the shoulders of labor.\u201d, where it correctly predicts the aspect highlighted to be a barrier instead of a scale.\nThese particular outcomes indicate that, with the knowledge of the source domain in this case, the continual learning procedure captures the broader implicit meaning better while the prediction of the single-task setup is possibly more influenced by the word destroy.\nNo Gain Both the single-task baseline and continual learning have room for improvements for the aspects of change and agent. For instance, for the sentence \u201cBut some pro-gun legislation, including the sweeping \u2018guns everywhere bill that was\n3Detailed outcomes of the experiments on the test set are provided with the code provided.\nsigned into law earlier this year by Georgia\u2019s Republican Gov. Nathan Deal, has advanced in recent months.\u201d, both the approaches predict the highlighted aspect to be agent instead of change.\nFurthermore, for the sentence \u201cThe reality is that firearm safety has not meaningfully advanced in the past century.\u201d, while the true label was agent, the single-task setup predicts change and the continual learning predicts movement, indicating a confusion regarding the three aspects."
        },
        {
            "heading": "7.2 Predicting Source Domains",
            "text": "Analogously to the previous subsection, we here present our findings on source domains.\nImprovement As stated in Section 4, we have composite source domains present in the corpus which are combinations of individual source domains that are present across the data.\nWe observe that continual learning can differentiate better between predicting single and composite source domains. An example is the source domain of struggle where for the sentence \u201cHow can local governments and civil-society organizations effectively fight poverty and promote social responsibility in countries as diverse as Canada, China and Ghana?\u201d, continual learning predicts the source domain of struggle correctly instead of struggle and war by the single-task baseline.\nThis holds true for other single and their equiv-\nalent composite source domains where in the sentence for example \u201cMy point was that governments have killed more people - the OP did not say the US government but the last time I looked the US Govt was in fact a \u2018government and \u2018governments have killed far more people then individuals not engaged in the service of \u2018government.\u201d the singletask incorrectly predicts crime and physical harm while continual learning predicts the correct source domain physical harm.\nNo Gain Overall, continual learning improves considerably over the single-task approach in predicting the source domains. However both the approaches systematically confused among source domains such as natural physical force and body of water, as in the sentence sweep out the old, then when the fresh rain of democracy came, a whole new country would spring up, like mushrooms. or among building or low location as in the sentence \u201cThis rate is an individual\u2019s tax floor.\u201d.\nIn the latter, it might be even challenging for humans to be certain, because the metaphor tax floor does indicate that the meaning originates from the concept such as low location."
        },
        {
            "heading": "7.3 Predicting Highlighted Aspects With Literal Metaphor as Input",
            "text": "One possibly unexpected outcome were the top1 accuracy results for predicting the highlighted aspects with the metaphor added to the input as shown in table 3. Upon inspection, we found out that the continual learning approach was biased towards the aspect movement as shown in Table 4.\nIn the sentence \u201cWe cannot allow Texas to go down the big government pathway\u201d, with the literal metaphor pathway appended to the input, the continual learning approach predicts the highlighted aspect to be movement instead of goal which is correctly predicted by the single-task baseline.\nSimilarly, in the sentence \u201cGun control advances in the Senate, Democrats thank MSM for propaganda April11, 2013\u201d, the continual learning setup predicts the highlighted aspect to be movement. It is highly likely that with the addition of the literal metaphor to the input, the continual learning approach fails to capture the broader meaning manifested by the sentence. On the contrary, without the addition of the literal metaphor to the input, in the similar experimental setting, continual learning approach predicts all the aspects correctly, supporting our aforementioned theory.\nAnother interesting example is where the continual learning approach predicts the highlighted aspect to be victim instead of criminal, as shown in Table 4. While theoretically, it can be argued that both of these aspects are highlighted by the metaphorical usage of murdering in the sentence, in the realm of the dataset, the single-task baseline predicts correctly. Given the direct relationship of a source domain to the metaphor, these outcomes intuitively make sense, because of the information regarding source domains already present in this case. However, further experiments are required to understand them better."
        },
        {
            "heading": "7.4 Relaxed Top-1 Accuracy",
            "text": "Our fine-grained analysis on the test data revealed that continual learning particularly improves in distinguishing single and composite source domains. However, they also showed that the single-task baseline, in most cases, predicted at least one of the concepts in a composite source domain correctly.\nTo further investigate the performance of our approach we evaluate our approaches to the baseline with a relaxed top-1 accuracy as shown in Table 5. Unlike our main evaluations, in this setting we consider a prediction to be correct if one of the n-combinations in the true label is correct. So if the correct composite source domain for a given metaphorical sentence was a combination of source domain A and source domain B, we consider it to be a correct prediction if the model predicts only the concept A as the source domain of this sentence.\nThis intuitively also makes sense, given these concepts are combined based on their semantic similarity (Gordon et al., 2015) which means that each of the concepts in a composite source domain is semantically equivalent to the composite source domain. For example, in the sentence \u201cAcross the globe, free markets and trade have helped defeat poverty, and taught men and women the habits of liberty\", the metaphor defeat is a meaning manifestation of the composite source domain Competition,\nGame, and War. In this case, we consider it to be a correct prediction if the model predicts War to be the source domain.\nIn the relaxed accuracy setting, continual learning improves top-1 accuracies for highlighted aspects and source domains. Joint learning is more effective than the baseline for source domains, suggesting that semantic similarity is captured well when considering highlighted aspects, despite the complexity of composite source domains.\nOverall, our analysis not only suggests that contrastive learning can also be applied to predict highlighted aspects, but also indicates the performance of continual learning in these two downstream task settings in the consistent performance improvement across the experiments."
        },
        {
            "heading": "8 Conclusion",
            "text": "The conceptual mapping from source to target domain in metaphorical meaning manifestation involves the highlighting of certain aspects of the target domain. In this study, we have examined the impact of source domain information on predicting highlighted aspects, and vice versa.\nTo accomplish this, we have proposed two multitask learning approaches within a contrastive learning setting: one utilizing continual learning, the other joint learning. We have evaluated the performance of our approaches in comparison to according single-task baselines for predicting either source domains or highlighted aspects.\nWe have found that continual learning enhances model performance for highlighted aspects and source domains, suggesting mutual improvement. Our fine-grained qualitative analysis further confirms the effectiveness of our approaches across various experimental setups. Moreover, in a more informal yet potentially more applicable assessment, our method outperforms the performance of the single-task baseline, demonstrating beneficial outcomes through joint learning.\nWe conclude that the aspects highlighted by metaphors can be predicted well in the majority of cases\u2014and even more so a small set of candidate aspects (as suggested by our top-5 accuracy results). We see this as a substantial step towards more comprehensive computational interpretation of metaphors. Following Lakoff and Johnson (2003), metaphors do not only highlight certain aspects of a target domain, they also hide others at the same time. Future work should thus pay more atten-\ntion to what is not put emphasis on by a metaphor, which will naturally bring up additional challenges regarding the interpretation of metaphors."
        },
        {
            "heading": "Acknowledgment",
            "text": "This work has been supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under project number TRR 318/1 2021 \u2013 438445824. We thank the anonymous reviewers for their helpful feedback.\nLimitations\nFirstly, one of the major limitations of our work is the nature of our downstream tasks, where from a theoretical standpoint, the number of highlighted aspects for a particular metaphor in a given context are unbounded - which means one cannot say with certainty, if a certain concept is the one and only correct aspect for a given metaphorical sentence While we have tackled this problem by treating the data as is and by incorporating a real-world setting for our experimental setup, it is also a limited real-world setting that we employ since we take the aspects annotated in the dataset to be the only possible aspects for the metaphorical sentences.\nSecondly, while our analysis reveals that our proposed continual learning approach improves model performance, without any level of explainibility it is difficult to say exactly to what degree the information from one task does help in the model performance of the other.\nFinally, to test the generalizability of our approach it is important to test our models on similar datasets. However, to the best of our knowledge, this is the only dataset which fits our task design.\nEthical Statement\nTo the best of our knowledge, we understand that there are no ethical concerns with our paper. We use relatively transparent approaches a publicly available dataset.4. To the best of our knowledge, it is unlikely that a potential harm is posed by either the data or our methods."
        },
        {
            "heading": "9 Appendix",
            "text": ""
        },
        {
            "heading": "9.1 Hyperparameter Configurations",
            "text": ""
        },
        {
            "heading": "9.2 Single-Task Setup",
            "text": "Highlighted Aspects Without the literal metaphor added to the input, we find the bestperforming checkpoint to result from a batch size of 8, a learning rate of 5 \u00b710\u22125, and 5 epochs. With the literal metaphor, 4 epochs are better while the others are hyperparameters identical.\nSource Domains Without metaphor added to the input, we find the optimized parameters for the best performing checkpoint to be a learning rate of 5 \u00b7 10\u22125, batch size of 8, and epochs of 6. With the metaphor added to the input we find the optimized parameters for the best performing checkpoint to be a learning rate of 3 \u00b7 10\u22125, batch size of 8, a epochs of 5."
        },
        {
            "heading": "9.3 Continual-Learning",
            "text": "Highlighted Aspects Without metaphor added to the input, in the continual learning approach, based on our hyperparameter search we find the optimized parameters for the best performing checkpoint to be a learning rate of 5 \u00b7 10\u22125, batch size of 8, and epochs of 6. With the metaphor added to the input we find the optimized parameters for the best performing checkpoint to be a learning rate of 5 \u00b7 10\u22125, batch size of 8, a epochs of 4.\nSource Domains Without metaphor added to the input, we find the optimized parameters for the best performing checkpoint to be a learning rate of 4 \u00b7 10\u22125, batch size of 8, and an epoch of 5. The encoder for the second phase of training is the best performing model from the single-setup of predicting source domains and hence it\u2019s optimized with the hyperparameters as mentioned above in section 9.2. With the metaphor added to the input we find the optimized parameters for the best performing checkpoint to be a learning rate of 4 \u00b7 10\u22125, batch size of 8, a epochs of 5."
        },
        {
            "heading": "9.4 Joint-Learning",
            "text": "Highlighted Aspects In the joint learning setup, we find the optimized parameters for the best performing checkpoint to be a learning rate of 5 \u00b710\u22125, batch size of 8, and epochs of 6. For the model to identify each of the tasks individually, we add a special token of <hghl> for every sentence for the highlighted aspects and <scm> for the source domains. At inference, we test the best performing model on the test set. With the metaphor added to the input we find the optimized parameters for the best performing checkpoint to be a learning rate of 5 \u00b7 10\u22125, batch size of 8, a epochs of 6.\nSource Domains The procedure to fine-tune for this task is the same as mentioned as above, while the only difference being at the inference time, where the instead of highlighted aspects we predict source domains. Without metaphor added to the input, we find the optimized parameters for the\nbest performing checkpoint to be a learning rate of 5 \u00b7 10\u22125, batch size of 8, and epochs of 6. With the metaphor added to the input we find the optimized parameters for the best performing checkpoint to be a learning rate of 5 \u00b7 10\u22125, batch size of 8, a epochs of 5."
        },
        {
            "heading": "9.5 Confusion Matrices",
            "text": "Next pages show the confusion matrices for each of the outcomes of the model performances on the test set."
        }
    ],
    "title": "Modeling Highlighting of Metaphors in Multitask Contrastive Learning Paradigms",
    "year": 2023
}