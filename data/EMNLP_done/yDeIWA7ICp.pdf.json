{
    "abstractText": "Open-domain dialog involves generating search queries that help obtain relevant knowledge for holding informative conversations. However, it can be challenging to determine what information to retrieve when the user is passive and does not express a clear need or request. To tackle this issue, we present a novel approach that focuses on generating internet search queries that are guided by social commonsense. Specifically, we leverage a commonsense dialog system to establish connections related to the conversation topic, which subsequently guides our query generation. Our proposed framework addresses passive user interactions by integrating topic tracking, commonsense response generation and instructiondriven query generation. Through extensive evaluations, we show that our approach1 overcomes limitations of existing query generation techniques that rely solely on explicit dialog information, and produces search queries that are more relevant, specific, and compelling, ultimately resulting in more engaging responses.",
    "authors": [
        {
            "affiliations": [],
            "name": "Revanth Gangi Reddy"
        },
        {
            "affiliations": [],
            "name": "Hao Bai"
        },
        {
            "affiliations": [],
            "name": "Wentao Yao"
        },
        {
            "affiliations": [],
            "name": "Sharath Chandra"
        },
        {
            "affiliations": [],
            "name": "Etagi Suresh"
        },
        {
            "affiliations": [],
            "name": "Heng Ji"
        },
        {
            "affiliations": [],
            "name": "ChengXiang Zhai"
        }
    ],
    "id": "SP:ee8b3765261fca3c5c60b713c8822e3144ac53ad",
    "references": [
        {
            "authors": [
                "Leonard Adolphs",
                "Kurt Shuster",
                "Jack Urbanek",
                "Arthur Szlam",
                "Jason Weston."
            ],
            "title": "Reason first, then respond: Modular generation for knowledge-infused dialogue",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Mingzhu Cai",
                "Siqi Bao",
                "Xin Tian",
                "Huang He",
                "Fan Wang",
                "Hua Wu."
            ],
            "title": "Query enhanced knowledgeintensive conversation via unsupervised joint modeling",
            "venue": "arXiv preprint arXiv:2212.09588.",
            "year": 2022
        },
        {
            "authors": [
                "Maximillian Chen",
                "Alexandros Papangelis",
                "Chenyang Tao",
                "Seokhwan Kim",
                "Andy Rosenbaum",
                "Yang Liu",
                "Zhou Yu",
                "Dilek Hakkani-Tur."
            ],
            "title": "Places: Prompting language models for social conversation synthesis",
            "venue": "arXiv preprint arXiv:2302.03269.",
            "year": 2023
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Jinlan Fu",
                "See-Kiong Ng",
                "Zhengbao Jiang",
                "Pengfei Liu."
            ],
            "title": "Gptscore: Evaluate as you desire",
            "venue": "arXiv preprint arXiv:2302.04166.",
            "year": 2023
        },
        {
            "authors": [
                "Fenfei Guo",
                "Angeliki Metallinou",
                "Chandra Khatri",
                "Anirudh Raju",
                "Anu Venkatesh",
                "Ashwin Ram."
            ],
            "title": "Topic-based evaluation for conversational bots",
            "venue": "arXiv preprint arXiv:1801.03622.",
            "year": 2018
        },
        {
            "authors": [
                "Amelia Hardy",
                "Ashwin Paranjape",
                "Christopher Manning."
            ],
            "title": "Effective social chatbot strategies for increasing user initiative",
            "venue": "Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 99\u2013110, Singapore and",
            "year": 2021
        },
        {
            "authors": [
                "Behnam Hedayatnia",
                "Di Jin",
                "Yang Liu",
                "Dilek Hakkani-T\u00fcr."
            ],
            "title": "A systematic evaluation of response selection for open domain dialogue",
            "venue": "SIGDIAL 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Chandra Khatri",
                "Rahul Goel",
                "Behnam Hedayatnia",
                "Angeliki Metallinou",
                "Raefer Gabriel",
                "Arindam Mandal."
            ],
            "title": "Contextual topic modeling for dialogue systems",
            "venue": "SLT 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Hyunwoo Kim",
                "Jack Hessel",
                "Liwei Jiang",
                "Ximing Lu",
                "Youngjae Yu",
                "Pei Zhou",
                "Ronan Le Bras",
                "Malihe Alikhani",
                "Gunhee Kim",
                "Maarten Sap"
            ],
            "title": "2022a. Soda: Million-scale dialogue distillation with social commonsense contextualization",
            "year": 2022
        },
        {
            "authors": [
                "Hyunwoo Kim",
                "Youngjae Yu",
                "Liwei Jiang",
                "Ximing Lu",
                "Daniel Khashabi",
                "Gunhee Kim",
                "Yejin Choi",
                "Maarten Sap."
            ],
            "title": "Prosocialdialog: A prosocial backbone for conversational agents",
            "venue": "arXiv preprint arXiv:2205.12688.",
            "year": 2022
        },
        {
            "authors": [
                "Mojtaba Komeili",
                "Kurt Shuster",
                "Jason Weston."
            ],
            "title": "Internet-augmented dialogue generation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8460\u20138478.",
            "year": 2022
        },
        {
            "authors": [
                "Tuan M. Lai",
                "Giuseppe Castellucci",
                "Saar Kuzi",
                "Heng Ji",
                "Oleg Rokhlenko."
            ],
            "title": "External knowledge acquisition for end-to-end document-oriented dialog systems",
            "venue": "Proc. The 17th Conference of the European Chapter of the Association for Computational",
            "year": 2023
        },
        {
            "authors": [
                "Tuan M Lai",
                "Giuseppe Castellucci",
                "Saar Kuzi",
                "Heng Ji",
                "Oleg Rokhlenko"
            ],
            "title": "External knowledge acquisition for end-to-end document-oriented dialog systems",
            "year": 2023
        },
        {
            "authors": [
                "Sha Li",
                "Mahdi Namazifar",
                "Di Jin",
                "Mohit Bansal",
                "Heng Ji",
                "Yang Liu",
                "Dilek Hakkani-Tur."
            ],
            "title": "Enhancing knowledge selection for grounded dialogues via document semantic graphs",
            "venue": "Proc. The 2022 Conference of the North American Chapter of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu."
            ],
            "title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
            "venue": "arXiv preprint arXiv:2303.16634.",
            "year": 2023
        },
        {
            "authors": [
                "Chris Moore"
            ],
            "title": "The development of commonsense psychology",
            "year": 2006
        },
        {
            "authors": [
                "Takayuki Nakata",
                "Shinichi Ando",
                "Akitoshi Okumura."
            ],
            "title": "Topic detection based on dialogue history",
            "venue": "COLING 2002: The 19th International Conference on Computational Linguistics.",
            "year": 2002
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Ashwin Ram",
                "Rohit Prasad",
                "Chandra Khatri",
                "Anu Venkatesh",
                "Raefer Gabriel",
                "Qing Liu",
                "Jeff Nunn",
                "Behnam Hedayatnia",
                "Ming Cheng",
                "Ashish Nagar"
            ],
            "title": "Conversational ai: The science behind the alexa prize",
            "venue": "arXiv preprint arXiv:1801.03604",
            "year": 2018
        },
        {
            "authors": [
                "Maarten Sap",
                "Ronan Le Bras",
                "Emily Allaway",
                "Chandra Bhagavatula",
                "Nicholas Lourie",
                "Hannah Rashkin",
                "Brendan Roof",
                "Noah A Smith",
                "Yejin Choi."
            ],
            "title": "Atomic: An atlas of machine commonsense for ifthen reasoning",
            "venue": "Proceedings of the AAAI con-",
            "year": 2019
        },
        {
            "authors": [
                "Kurt Shuster",
                "Mojtaba Komeili",
                "Leonard Adolphs",
                "Stephen Roller",
                "Arthur Szlam",
                "Jason Weston."
            ],
            "title": "Language models that seek for knowledge: Modular search & generation for dialogue and prompt completion",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Kurt Shuster",
                "Jing Xu",
                "Mojtaba Komeili",
                "Da Ju",
                "Eric Michael Smith",
                "Stephen Roller",
                "Megan Ung",
                "Moya Chen",
                "Kushal Arora",
                "Joshua Lane"
            ],
            "title": "Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage",
            "year": 2022
        },
        {
            "authors": [
                "Robyn Speer",
                "Joshua Chin",
                "Catherine Havasi"
            ],
            "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "arXiv preprint arXiv:2109.01652.",
            "year": 2021
        },
        {
            "authors": [
                "Jason D Williams",
                "Antoine Raux",
                "Deepak Ramachandran",
                "Alan W Black."
            ],
            "title": "The dialog state tracking challenge",
            "venue": "Proceedings of the SIGDIAL 2013 Conference, pages 404\u2013413.",
            "year": 2013
        },
        {
            "authors": [
                "Jing Xu",
                "Megan Ung",
                "Mojtaba Komeili",
                "Kushal Arora",
                "Y-Lan Boureau",
                "Jason Weston."
            ],
            "title": "Learning new skills after deployment: Improving open-domain internet-driven dialogue with human feedback",
            "venue": "arXiv preprint arXiv:2208.03270.",
            "year": 2022
        },
        {
            "authors": [
                "Hamed Zamani",
                "Johanne R Trippas",
                "Jeff Dalton",
                "Filip Radlinski."
            ],
            "title": "Conversational information seeking",
            "venue": "arXiv preprint arXiv:2201.08808.",
            "year": 2022
        },
        {
            "authors": [
                "Pei Zhou",
                "Karthik Gopalakrishnan",
                "Behnam Hedayatnia",
                "Seokhwan Kim",
                "Jay Pujara",
                "Xiang Ren",
                "Yang Liu",
                "Dilek Hakkani-Tur."
            ],
            "title": "Commonsensefocused dialogues for response generation: An empirical study",
            "venue": "Proceedings of the 22nd Annual",
            "year": 2021
        },
        {
            "authors": [
                "Pei Zhou",
                "Karthik Gopalakrishnan",
                "Behnam Hedayatnia",
                "Seokhwan Kim",
                "Jay Pujara",
                "Xiang Ren",
                "Yang Liu",
                "Dilek Hakkani-Tur"
            ],
            "title": "Think before you speak: Explicitly generating implicit commonsense",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Conversational systems have evolved to include personal assistants, task-oriented bots, and opendomain dialog agents for casual conversations. For these agents to maintain engaging and informative discussions, it is crucial to access external knowledge. Holding a knowledge-powered dialog (Dinan et al.; Komeili et al., 2022; Li et al., 2022; Lai et al., 2023a) typically involves the generation of a search query that can help gather the most relevant information to continue the conversation. While such queries are more obvious when the user explicitly asks for certain information, a.k.a. conversational information seeking (Zamani et al., 2022), it is unclear what information should be pursued when users are passive, disengaged, and do not provide\n1Code and models are available here: https://github.com/gangiswag/dialog-query-generation\nclear guidance for the conversation (Hardy et al., 2021). Nevertheless, in open-domain conversations, users can introduce any topic, and designing a comprehensive algorithm that can produce a relevant query in response to a random user topic poses a unique, complex challenge that has not been explored in prior research.\nTo tackle this challenge, we propose to integrate social commonsense reasoning for the generation of search queries in knowledge-powered conversations. Social commonsense (Moore, 2006) refers to the general knowledge about social situations and human behavior used to connect conversation topics and guide discussions. We thus hypothesize that by leveraging a deeper understanding of social commonsense and the unspoken cues that guide human conversation, chatbots can become more\nadept at navigating passive conversations. Concretely, we introduce a novel framework that uses a commonsense response as a latent directive for an instruction-following query generator. Our approach incorporates the use of topic tracking (\u00a72.1) to first identify the main point of discussion, followed by commonsense-based response generation that can associate concepts to the main topic to give a latent commonsense directive (\u00a72.2). Finally, we use instruction-driven query generation (\u00a72.3) to output a search query that adheres to the latent directive within the commonsense response.\nOur method overcomes the limitations of existing techniques (Shuster et al., 2022a,b; Cai et al., 2022; Lai et al., 2023b) that solely depend on explicit information present in the conversation to generate search queries. Such an approach is suboptimal in case of passive conversations, where the human isn\u2019t necessarily asking for any specific information. Figure 1 shows an example comparing our approach against a baseline query generation system (Shuster et al., 2022b). Our topic tracking identifies \u2018The Conjuring\u2019 as the subject, with the commonsense responder making the association movie \u2192 reviews to output a latent commonsense directive that refers to discuss movie reviews. This directive guides the search query generator to output a query for the movie reviews, the results of which lead to a more engaging bot response compared to the baseline, as can be seen in Figure 1."
        },
        {
            "heading": "2 A Novel Query Generation Framework",
            "text": "In this section, we present our framework for generating search queries by leveraging commonsense reasoning. Our approach consists of three main components: topic tracking to pinpoint the core subject, commonsense-based response generation that relates concepts with the primary topic and provides a latent commonsense directive, and instruction-driven query generation to produce a search query capable of retrieving relevant information that follows the commonsense directive. Figure 2 illustrates how these components are integrated, with each step described in detail below."
        },
        {
            "heading": "2.1 Fine-Grained Topic Tracking",
            "text": "Topic tracking (Nakata et al., 2002) aims to identify the primary subject of the discussion in free-form dialogs, and has been demonstrated (Guo et al., 2018) to improve the coherence of dialog systems. Unlike previous approaches (Khatri et al., 2018)\nthat track a fixed set of broad high-level topics (e.g., movies, sport), our objective is to detect unconstrained, finer-grained topics (such as movie/actor names or teams). For fine-grained topic tracking, we apply an instruction-tuned model (Chung et al., 2022) to identify the current topic from the dialog context. We utilize the prompt in Figure 2 and rely on instruction-tuned models with strong zero-shot abilities (Wei et al., 2021) due to lack of training data for such topic tracking in dialog2. An alternative topic tracking approach could follow Shuster et al. (2022a); Adolphs et al. (2022), extracting topics as relevant entities grounding the final response."
        },
        {
            "heading": "2.2 Commonsense-Based Directive",
            "text": "Social commonsense-based dialog systems (Kim et al., 2022a,b; Zhou et al., 2021) typically demonstrate a fundamental understanding of handling and responding to specific topics or situations. They involve using external commonsense knowledge graphs such as ConceptNet (Speer et al., 2017) or ATOMIC (Sap et al., 2019) to collect triples for response generation (Zhou et al., 2022), or distilling such knowledge into language models (LM) through large-scale pretraining (Kim et al., 2022a; Chen et al., 2023) for direct response generation. In this work, we adopt the latter approach, by using a pretrained LM to derive the commonsense directive in the form of a response.\nSpecifically, we use Cosmo (Kim et al., 2022a), which was trained on socially-grounded synthetic dialogues generated by prompting InstructGPT (Ouyang et al., 2022) using contextualized commonsense knowledge from ATOMIC. Cosmo takes\n2We note that topic tracking is related to open-domain dialog, and is different from state tracking (Williams et al., 2013) which is specific to task-oriented dialog.\na situation narrative and role instruction as input, and generates a response based on the dialog context. We also integrate the topic tracking output into the situation narrative definition, as illustrated in Figure 2. Subsequently, Cosmo\u2019s output serves as the latent commonsense directive to guide search query generation, which is discussed next."
        },
        {
            "heading": "2.3 Instruction-Driven Query Generation",
            "text": "Given the dialog context, conversation topic and a latent directive in the form of a commonsense response, we aim to generate a search query to obtain relevant information for continuing the conversation. We utilize an instruction-tuned model (Chung et al., 2022) for query generation, by prompting (see Figure 2) it to transform the commonsense response into a search query, while incorporating the fine-grained topic to enhance relevance and specificity. Essentially, the commonsense response embodies the bot\u2019s informational requirement, guiding it to obtain the mentioned information."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Setup",
            "text": "Dataset We use the Wizard of Internet (WoI) (Komeili et al., 2022) dataset for our experiments. WoI is a human-human dialog corpus for knowledge-powered conversations, with one of the speakers having internet access to gather information for generating responses.\nModels and Baselines The topic-tracker is based on Flan-T5 large (770M) (Chung et al., 2022) while the commonsense response generation uses the 3B version of Cosmo (Kim et al., 2022a). The query generator is also based on the Flan-T5 large model. We compare our query generation approach primarily against Blender Bot 3 (Shuster et al., 2022b), a state of the art open-domain conversational agent. We also compare against a version of our approach that does not incorporate the Cosmo response for query generation, termed Flan T5 w/o Cosmo.\nFinetuning with ChatGPT Annotations Our approach uses an instruction-tuned Flan T5 model in a zero-shot setting for topic tracking and query generation. To improve performance, we separately finetune the topic-tracker and query generator using ChatGPT annotations (the same prompt as Flan T5 is used to obtain silver labels from ChatGPT). To create finetuning data, we choose turns corresponding to internet search from the WoI training\nset, yielding 20k examples.\nInternet Search and Response Generation We obtain search results by scoring passages from the top-3 Bing Search pages using a reranker3. With our primary focus being query generation, we simply prompt ChatGPT to generate the response by incorporating the top search result given the dialog context. We also consider a \u201cno query\u201d baseline that corresponds to generating responses directly from ChatGPT (gpt-3.5-turbo-0301 version) without internet search. For both search query generation and final response generation, we employ the nucleus sampling method (Holtzman et al., 2019) with P set at 0.9 and a temperature of 0.7. We set max tokens to 40 and 100 for search query generation and final response generation respectively."
        },
        {
            "heading": "3.2 Evaluation",
            "text": "In our evaluation, we focus on WoI test set with dialog turns that had search queries annotated for generating responses, while specifically targeting \u201cpassive turns\u201d where users don\u2019t explicitly request information. Using an intent detection model (Khatri et al., 2018), we identify and remove turns related to information or opinion requests, and randomly selected 200 examples for human evaluation.\nHuman Evaluation We conducted a human study with four experienced NLP students to evaluate the quality of generated search queries and responses. Queries were assessed based on relevance, specificity, usefulness, and potential to maintain user engagement in the dialog. Responses were evaluated for engagement, coherence, and informativeness. Detailed guidelines are in the appendix.\nAutomatic Evaluation Recent studies, like GEVAL (Liu et al., 2023) and GPTScore (Fu et al., 2023), show that LLMs such as GPT-4 can effectively evaluate natural language generations and align well with human assessments. Therefore, we utilize GPT-4 for automatic evaluation, prompting4 it to provide an overall score (ranging from 1-10) for search queries and final responses. As seen in \u00a73.3, our human study results corroborate GPT-4 evaluations. Additionally, we use a ranker model (Hedayatnia et al., 2022) trained on the Alexa Prize Socialbot Grand Challenge (Johnston et al., 2023) response selection data (Ram et al., 2018) for response evaluation.\n3We use the ms-marco-MiniLM-L-6-v2 model. 4Detailed GPT-4 prompts are in the appendix."
        },
        {
            "heading": "3.3 Results",
            "text": "Quality of generated search query Table 1 (left) shows the results of human and automatic evaluation of search queries. Mainly, we notice that instruction-tuned models outperform Blender Bot 3 significantly, and using Cosmo\u2019s commonsense response as a directive for guiding query generation with Flan T5 shows consistent improvements. Lastly, substantial enhancements in query quality are observed upon fine-tuning the zero-shot system with ChatGPT annotations. By computing the Spearman correlation between automatic metrics (GPT-4) and overall score for human evaluations (average of four aspect ratings), we found a strong correlation (0.674) between the two measures.\nQuality of final responses Table 1 (right) shows results for evaluation of the generated responses. We see that directly generating a response from ChatGPT without internet search can still lead to a very coherent response, but is less engaging and very uninformative. Our proposed query generation framework leads to consistent improvements across all aspects of the final response, particularly with high engagement scores. Notably, boosting engagement, or the likelihood of continued humanbot interaction, is crucial in passive conversations."
        },
        {
            "heading": "3.4 Analysis",
            "text": "Instruction-Following Capability We study the impact of the query generator\u2019s instructionfollowing capability on utilizing the commonsense directive (i.e., cosmo output) to generate better queries. Using GPT-4 preference evaluation, we explore how increasing the query generator\u2019s size5\n5As per Chung et al. (2022), larger instruction-tuned models usually have better instruction-following capability.\ninfluences the quality of generated queries both with and without the commonsense directive. Figure 3 reveals that incorporating the commonsense directive (a) significantly enhances query quality as model size increases, and (b) leads to greater improvement for larger models (67.5% for XXL vs 53.5% for Large). Hence, a more robust instructiontuned model effectively leverages the commonsense directive in generating better queries.\nBenefit of Topic Tracking Within our framework, topic tracking serves to (a) maintain coherence between the generated query and the most recent discussion subject, and (b) assist in shaping Cosmo\u2019s situation narrative (refer to prompts in figure 2). Here, we study the benefit of topic tracking by removing it from the Cosmo and query generator inputs and evaluating the final query quality. Using GPT-4 for automatic preference evaluation, we compare queries generated with and without the\ntopic tracker. Table 2 shows results from the study, with GPT-4 finding queries generated by involving topic tracking better, particularly for relevance and specificity.\nError Categorization We examined 50 lowscoring examples6 from the human evaluation of search queries produced by the zero-shot approach. The main error categories were: (i) Incorrect Topic (31.4%) - topic tracker failed to identify the current discussion subject, (ii) Trivial Query (29.4%) - query was obvious or already answered in the conversation history, (iii) Query Instruction Mismatch (23.5%) - query generator misunderstands instructions, and outputs conversational questions instead, and (iv) other irrelevant queries (15.7%). After finetuning with ChatGPT annotations, 70.6% of the queries significantly improved, while the rest maintained a more or less similar quality level. The breakdown of examples that continue to score low after finetuning is as follows: 19% result from Incorrect Topic, 41% from Trivial Queries, 5% from Query Instruction Mismatch, and 35% from other unrelated queries. Notably, finetuning effectively reduces 67% of Trivial Query errors, 75% of Incorrect Topic errors, and over 90% of Query Instruction Mismatch errors. This suggests that finetuning enhances topic tracking capabilities (resulting in fewer Incorrect Topic errors) and ensures better adherence to search query generation instructions (leading to a decrease in Query Instruction Mismatch errors)."
        },
        {
            "heading": "4 Conclusion and Future Work",
            "text": "We introduce a novel framework that leverages commonsense to enhance the generation of search queries in internet-powered dialog. Our results show that incorporating a commonsense-based directive yields search queries with improved relevance, specificity and appeal, fostering user engagement in otherwise passive conversations. Future work will use more intricate social narratives by incorporating user preferences from past conversations, to align commonsense directives towards individual interests."
        },
        {
            "heading": "Acknowledgement",
            "text": "We would like to thank Amazon Inc. for providing a grant partially supporting the work of the team. We thank the Amazon Alexa Prize team for their\n6Qualitative examples are provided in the appendix\ngreat support, guidance, and help. We are also grateful to the anonymous reviewers, the Blender NLP group and CharmBana team members for their valuable feedback and comments.\nLimitations\nWe expect the following limitations for our approach:\n\u2022 Focusing only on turns involving search: Our methodology primarily targets the generation of search queries, hence we chose only those turns from the WoI dataset that contained annotated search queries. A more pragmatic approach would also require a search decision module to determine when it is essential to seek external information.\n\u2022 Assuming topic continuity in discussion: Our method assumes a continuous presence of a discussion topic. Nevertheless, situations like topic shifts can cause temporary absence of a topic, which our approach does not consider. For instance, when the human suggests, \"Let\u2019s discuss something else,\" there is no current topic for the discussion.\nEthical Considerations\nWe raise the following ethical concerns from leveraging internet search for open-domain dialog:\n\u2022 Toxicity of retrieved content: There is a need for a toxicity filter or a content moderation system to ensure that the retrieved content is safe, non-offensive, and free from any form of hate speech, discrimination, or harassment.\n\u2022 Privacy concerns: Using Internet search for dialog systems can raise privacy concerns, as users might be discussing or sharing personal or sensitive information in their conversations. It is crucial to implement proper data anonymization and encryption techniques to protect users\u2019 privacy .\n\u2022 Reliability and credibility of sources: Dialog systems must be cautious when referring to information from the Internet, as the content may not always be accurate or reliable. Verifying the credibility of sources and crossreferencing is essential to ensure that the information provided by the system is reliable and trustworthy."
        },
        {
            "heading": "A Experimental Setup",
            "text": "A.1 Wizard of Internet Dataset The Wizard of Internet (WoI) dataset (Komeili et al., 2022) comprises a vast collection of external knowledge-based conversations, allowing agents to leverage internet search to obtain relevant information. We utilize a training subset of WoI consisting of 6,029 conversations with 29,371 turns. Upon transforming the dataset, models are employed to sequentially generate three items for each bot utterance with human annotated search queries: the ChatGPT topic tracking output, commonsense directive from Cosmo, and the ChatGPT search query. From this, 20k instances are sampled to form the finetuning data.\nA.2 Models and Baselines Flan T5 (Chung et al., 2022) We employ the Flan T5 (Large, 770M) as the model for entity tracking and query generation. Given the context, we incorporate the prompt depicted in Figure 2 into the Flan T5 input using the entity and query created by ChatGPT as the ground truth for finetuning.\nCosmo (Kim et al., 2022a) We use the 3B variant of Cosmo model as our social commonsense-based response generator. The model has been trained on 1.5 million synthetically generated social dialogs from SODA (Kim et al., 2022a). Cosmo has been shown to be capable of managing conversations across diverse dialog situations.\nBlender Bot 3 (Shuster et al., 2022b) Blender Bot 3 (BB3) is a unified sequence-to-sequence language model designed for modular dialogue systems, which appends instruction tokens to the generated text. BB3 employs R2C2 (Shuster et al., 2022b) as the language model to create queries, consisting of a 2.7 billion-parameter Transformer architecture with 22 encoder and 22 decoder layers, pretrained on pushshift.io Reddit and RoBERTa + CC100en data (Shuster et al., 2022a). Furthermore, BB3 finetunes R2C2 on a variety of query datasets, such as the Wizard of Internet and Feedback on Interactive Talk & Search (FITS) (Xu et al., 2022).\nA.3 Qualitative Examples Here, we show some qualitative examples for the search queries generated by different approaches, and the corresponding responses generated by incorporating results obtained using the query. Table 3 shows an example for each error type.\nYou are given a dialog between a bot and a user:\nGenerate a single-sentence response from the bot that incorporates the most relevant part from the content below.\nContent: <Search Content> Bot:\nChatGPT Response Generation\nBot: ... User: ... ...... Bot: ... User: ...\nWe would like to request your evaluation of the performance of 5 systems, each of which generates a search query for obtaining relevant information to continue a conversation.\nYou will be given a short human-bot conversation (ending with a user utterance) and generated search query from each of the 5 systems, to gather information for the next bot response. You need to read the human-bot conversation and judge the overall quality of each of the search queries based on the relevance, specificity, usefulness and interestingness of the search query.\nHuman-Bot Conversation: <Context> System1 Query: <system 1> System2 Query: <system 2> ...... System5 Query: <system 5> For each of the system queries, you need to provide a single overall score in the range of 1-10, where a higher score indicates better overall performance. You should evaluate queries irrespective of how well formed they might be, as long as they have all the important terms within them. Note that interestingness is the highest bar for evaluation: A query that can provide interesting information for continuing a conversation is preferred, which such queries usually expected to be at least relevant, useful and somewhat specific. Please ensure that the order in which the queries were presented does not affect your judgment.\nEvaluation (scores in the range of 1-10 ONLY)\nSystem1: System2: ... System5:\nWe would like to request your evaluation of the performance of 2 systems, each of which generates a search query for obtaining relevant information to continue a conversation.\nYou will be given a short human-bot conversation (ending with a user utterance) and generated search query from each of the 2 systems, to gather information for the next bot response. You need to read the human-bot conversation and judge the overall quality of each of the search queries based on the relevance, specificity, usefulness and interestingness of the search query.\nHuman-Bot Conversation:\nSystem1 Query: <system 1> System2 Query: <system 2> You should evaluate queries irrespective of how well formed they might be, as long as they have all the important terms within them. Note that interestingness is the highest bar for evaluation: A query that can provide interesting information for continuing a conversation is preferred, which such queries usually expected to be at least relevant, useful and somewhat specific. Please ensure that the order in which the queries were presented does not affect your judgment.\nEvaluation - Which system query is better? (Just output system1 or system2)\nOutput:\nWe would like to request your evaluation of the performance of 2 systems, each of which generates a search query for obtaining relevant information to continue a conversation.\nYou will be given a short human-bot conversation (ending with a user utterance) and generated search query from each of the 2 systems, to gather information for the next bot response. You need to read the human-bot conversation and judge the overall quality of each of the search queries based on the relevance, specificity, usefulness and interestingness of the search query.\nHuman-Bot Conversation:\nBot1 Response: <Response 1> Bot2 Response: <Response 2> ...... Bot5 Response: <Response 5>\nFor each of the bot responses, you need to provide a single overall score in the range of 1-10, where a higher score indicates better overall performance. Please ensure that the order in which the responses were presented does not affect your judgment.\nEvaluation (scores in the range of 1-10 ONLY)\nBot1: Bot2: Bot3: Bot4: Bot5:\n<Context>\n<Context>\nQuery Scoring\nQuery Preference\nResponse Scoring\nGPT-4 Automatic Evaluation Prompt"
        }
    ],
    "title": "Social Commonsense-Guided Search Query Generation for Open-Domain Knowledge-Powered Conversations",
    "year": 2023
}