{
    "abstractText": "Prompt tuning, in which prompts are optimized to adapt large-scale pre-trained language models to downstream tasks instead of fine-tuning the full model parameters, has been shown to be particularly effective when the prompts are trained in the multi-task transfer learning setting. These methods generally involve individually training prompts for each source task and then aggregating them to provide the initialization of the prompt for the target task. However, this approach critically ignores the fact that some of the source tasks could be negatively or positively interfering with each other. We argue that when we extract knowledge from source tasks via training source prompts, we need to consider this correlation among source tasks for better transfer to target tasks. To this end, we propose a Bayesian approach where we work with the posterior distribution of prompts across source tasks. We obtain representative source prompts corresponding to the samples from the posterior utilizing Stein Variational Gradient Descent, which are then aggregated to constitute the initial target prompt. We show extensive experimental results on the standard benchmark NLP tasks, where our Bayesian multi-task transfer learning approach outperforms the state-of-the-art methods in many settings. Furthermore, our approach requires no auxiliary models other than the prompt itself, achieving high degree of parameter-efficiency.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Haeju Lee"
        },
        {
            "affiliations": [],
            "name": "Minchan Jeong"
        },
        {
            "affiliations": [],
            "name": "Se-Young Yun"
        },
        {
            "affiliations": [],
            "name": "Kee-Eung Kim"
        }
    ],
    "id": "SP:e11db5355ad474be642fa505e8530fc8bdac2dce",
    "references": [
        {
            "authors": [
                "Akari Asai",
                "Mohammadreza Salehi",
                "Matthew Peters",
                "Hannaneh Hajishirzi."
            ],
            "title": "ATTEMPT: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural",
            "year": 2022
        },
        {
            "authors": [
                "Jimmy Ba",
                "Murat A Erdogdu",
                "Marzyeh Ghassemi",
                "Shengyang Sun",
                "Taiji Suzuki",
                "Denny Wu",
                "Tianzong Zhang."
            ],
            "title": "Understanding the variance collapse of SVGD in high dimensions",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Elad Ben Zaken",
                "Yoav Goldberg",
                "Shauli Ravfogel."
            ],
            "title": "BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Charles Blundell",
                "Julien Cornebise",
                "Koray Kavukcuoglu",
                "Daan Wierstra."
            ],
            "title": "Weight uncertainty in neural network",
            "venue": "International conference on machine learning, pages 1613\u20131622. PMLR.",
            "year": 2015
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "I\u00f1igo LopezGazpio",
                "Lucia Specia."
            ],
            "title": "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
            "venue": "Proceedings of the 11th International Workshop on Semantic",
            "year": 2017
        },
        {
            "authors": [
                "Peng Chen",
                "Omar Ghattas."
            ],
            "title": "Projected stein variational gradient descent",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages 1947\u20131958. Curran Associates, Inc.",
            "year": 2020
        },
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova."
            ],
            "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "Proceedings of the 2019 Conference of the",
            "year": 2019
        },
        {
            "authors": [
                "Peter Clark",
                "Isaac Cowhey",
                "Oren Etzioni",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Carissa Schoenick",
                "Oyvind Tafjord."
            ],
            "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
            "venue": "arXiv preprint arXiv:1803.05457.",
            "year": 2018
        },
        {
            "authors": [
                "Marie-Catherine de Marneffe",
                "Mandy Simons",
                "Judith Tonhauser."
            ],
            "title": "The commitmentbank: Investigating projection in naturally occurring discourse",
            "venue": "Proceedings of Sinn und Bedeutung, 23(2):107\u2013124.",
            "year": 2019
        },
        {
            "authors": [
                "Dorottya Demszky",
                "Kelvin Guu",
                "Percy Liang."
            ],
            "title": "Transforming question answering datasets into natural language inference datasets",
            "venue": "arXiv preprint arXiv:1809.02922.",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "William B. Dolan",
                "Chris Brockett."
            ],
            "title": "Automatically constructing a corpus of sentential paraphrases",
            "venue": "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
            "year": 2005
        },
        {
            "authors": [
                "Matthew Dunn",
                "Levent Sagun",
                "Mike Higgins",
                "V Ugur Guney",
                "Volkan Cirik",
                "Kyunghyun Cho."
            ],
            "title": "Searchqa: A new q&a dataset augmented with context from a search engine",
            "venue": "arXiv preprint arXiv:1704.05179.",
            "year": 2017
        },
        {
            "authors": [
                "Adam Fisch",
                "Alon Talmor",
                "Robin Jia",
                "Minjoon Seo",
                "Eunsol Choi",
                "Danqi Chen."
            ],
            "title": "MRQA 2019 shared task: Evaluating generalization in reading comprehension",
            "venue": "Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 1\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Danilo Giampiccolo",
                "Bernardo Magnini",
                "Ido Dagan",
                "Bill Dolan."
            ],
            "title": "The third PASCAL recognizing textual entailment challenge",
            "venue": "Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1\u20139, Prague. Association for",
            "year": 2007
        },
        {
            "authors": [
                "Alex Graves."
            ],
            "title": "Practical variational inference for neural networks",
            "venue": "Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc.",
            "year": 2011
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for NLP",
            "venue": "Proceedings of the 36th International Conference",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large",
            "year": 2022
        },
        {
            "authors": [
                "Hamish Ivison",
                "Matthew Peters."
            ],
            "title": "Hyperdecoders: Instance-specific decoders for multitask NLP",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1715\u20131730, Abu Dhabi, United Arab Emirates. Asso-",
            "year": 2022
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "James Henderson",
                "Sebastian Ruder."
            ],
            "title": "Compacter: Efficient low-rank hypercomplex adapter layers",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 1022\u20131035. Curran Associates, Inc.",
            "year": 2021
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "Sebastian Ruder",
                "Mostafa Dehghani",
                "James Henderson."
            ],
            "title": "Parameterefficient multi-task fine-tuning for transformers via shared hypernetworks",
            "venue": "Proceedings of the 59th Annual Meeting of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Khashabi",
                "Snigdha Chaturvedi",
                "Michael Roth",
                "Shyam Upadhyay",
                "Dan Roth."
            ],
            "title": "Looking beyond the surface: A challenge set for reading comprehension over multiple sentences",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter",
            "year": 2018
        },
        {
            "authors": [
                "Tushar Khot",
                "Ashish Sabharwal",
                "Peter Clark."
            ],
            "title": "Scitail: A textual entailment dataset from science question answering",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 32(1).",
            "year": 2018
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics, 7:452\u2013466.",
            "year": 2019
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Domini-",
            "year": 2021
        },
        {
            "authors": [
                "Hector J. Levesque",
                "Ernest Davis",
                "Leora Morgenstern."
            ],
            "title": "The winograd schema challenge",
            "venue": "13th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2012, Proceedings of the International Conference",
            "year": 2012
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefixtuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural",
            "year": 2021
        },
        {
            "authors": [
                "Qiang Liu",
                "Dilin Wang."
            ],
            "title": "Stein variational gradient descent: A general purpose bayesian inference algorithm",
            "venue": "Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.",
            "year": 2016
        },
        {
            "authors": [
                "Tianle Liu",
                "Promit Ghosal",
                "Krishnakumar Balasubramanian",
                "Natesh S. Pillai"
            ],
            "title": "Towards understanding the dynamics of gaussian-stein variational gradient descent",
            "year": 2023
        },
        {
            "authors": [
                "Xing Liu",
                "Harrison Zhu",
                "Jean-Francois Ton",
                "George Wynne",
                "Andrew Duncan."
            ],
            "title": "Grassmann stein variational gradient descent",
            "venue": "Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume",
            "year": 2022
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "James Henderson",
                "Sebastian Ruder."
            ],
            "title": "Compacter: Efficient low-rank hypercomplex adapter layers",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2021
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Peter Clark",
                "Tushar Khot",
                "Ashish Sabharwal."
            ],
            "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Christian Naesseth",
                "Scott Linderman",
                "Rajesh Ranganath",
                "David Blei."
            ],
            "title": "Variational sequential monte carlo",
            "venue": "Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine",
            "year": 2018
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela."
            ],
            "title": "Adversarial NLI: A new benchmark for natural language understanding",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for",
            "year": 2020
        },
        {
            "authors": [
                "Mohammad Taher Pilehvar",
                "Jose Camacho-Collados"
            ],
            "title": "WiC: the word-in-context dataset",
            "year": 2019
        },
        {
            "authors": [
                "Edoardo M Ponti",
                "Alessandro Sordoni",
                "Yoshua Bengio",
                "Siva Reddy."
            ],
            "title": "Combining modular skills in multitask learning",
            "venue": "arXiv preprint arXiv:2202.13914.",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "SQuAD: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392,",
            "year": 2016
        },
        {
            "authors": [
                "Christian P. Robert",
                "George Casella."
            ],
            "title": "Monte Carlo Statistical Methods",
            "venue": "Springer New York.",
            "year": 2004
        },
        {
            "authors": [
                "Keisuke Sakaguchi",
                "Ronan Le Bras",
                "Chandra Bhagavatula",
                "Yejin Choi."
            ],
            "title": "Winogrande: An adversarial winograd schema challenge at scale",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8732\u20138740.",
            "year": 2020
        },
        {
            "authors": [
                "Adil Salim",
                "Lukang Sun",
                "Peter Richtarik."
            ],
            "title": "A convergence theory for SVGD in the population limit under talagrand\u2019s inequality t1",
            "venue": "Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning",
            "year": 2022
        },
        {
            "authors": [
                "Bernhard Sch\u00f6lkopf",
                "Alexander J. Smola."
            ],
            "title": "Learning with Kernels",
            "venue": "The MIT Press.",
            "year": 2018
        },
        {
            "authors": [
                "Ravid Shwartz-Ziv",
                "Micah Goldblum",
                "Hossein Souri",
                "Sanyam Kapoor",
                "Chen Zhu",
                "Yann LeCun",
                "Andrew Gordon Wilson"
            ],
            "title": "Pre-train your loss: Easy bayesian transfer learning with informative priors",
            "year": 2022
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on",
            "year": 2013
        },
        {
            "authors": [
                "Lukang Sun",
                "Avetik Karagulyan",
                "Peter Richtarik"
            ],
            "title": "Convergence of stein variational gradient descent under a weaker smoothness condition",
            "year": 2022
        },
        {
            "authors": [
                "Alon Talmor",
                "Jonathan Herzig",
                "Nicholas Lourie",
                "Jonathan Berant."
            ],
            "title": "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Adam Trischler",
                "Tong Wang",
                "Xingdi Yuan",
                "Justin Harris",
                "Alessandro Sordoni",
                "Philip Bachman",
                "Kaheer Suleman."
            ],
            "title": "NewsQA: A machine comprehension dataset",
            "venue": "Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 191\u2013200,",
            "year": 2017
        },
        {
            "authors": [
                "Tu Vu",
                "Brian Lester",
                "Noah Constant",
                "Rami AlRfou",
                "Daniel Cer"
            ],
            "title": "SPoT: Better frozen model adaptation through soft prompt transfer",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Advances in Neural Information",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "International Conference on Learning",
            "year": 2019
        },
        {
            "authors": [
                "Zhen Wang",
                "Rameswar Panda",
                "Leonid Karlinsky",
                "Rogerio Feris",
                "Huan Sun",
                "Yoon Kim."
            ],
            "title": "Multitask prompt tuning enables parameter-efficient transfer learning",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Alex Warstadt",
                "Amanpreet Singh",
                "Samuel R. Bowman."
            ],
            "title": "Neural network acceptability judgments",
            "venue": "Transactions of the Association for Computational Linguistics, 7:625\u2013641.",
            "year": 2019
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association",
            "year": 2018
        },
        {
            "authors": [
                "Zhilin Yang",
                "Peng Qi",
                "Saizheng Zhang",
                "Yoshua Bengio",
                "William Cohen",
                "Ruslan Salakhutdinov",
                "Christopher D. Manning."
            ],
            "title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
            "venue": "Proceedings of the 2018 Conference on",
            "year": 2018
        },
        {
            "authors": [
                "Jaesik Yoon",
                "Taesup Kim",
                "Ousmane Dia",
                "Sungwoong Kim",
                "Yoshua Bengio",
                "Sungjin Ahn."
            ],
            "title": "Bayesian model-agnostic meta-learning",
            "venue": "Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.",
            "year": 2018
        },
        {
            "authors": [
                "Jianyi Zhang",
                "Yang Zhao",
                "Changyou Chen."
            ],
            "title": "Variance reduction in stochastic particle-optimization sampling",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages",
            "year": 2020
        },
        {
            "authors": [
                "Sheng Zhang",
                "Xiaodong Liu",
                "Jingjing Liu",
                "Jianfeng Gao",
                "Kevin Duh",
                "Benjamin Van Durme."
            ],
            "title": "Record: Bridging the gap between human and machine commonsense reading comprehension",
            "venue": "arXiv preprint arXiv:1810.12885.",
            "year": 2018
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "Yuan Zhang",
                "Jason Baldridge",
                "Luheng He."
            ],
            "title": "PAWS: Paraphrase adversaries from word scrambling",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2019
        },
        {
            "authors": [
                "Qihuang Zhong",
                "Liang Ding",
                "Juhua Liu",
                "Bo Du",
                "Dacheng Tao."
            ],
            "title": "Panda: Prompt transfer meets knowledge distillation for efficient model adaptation",
            "venue": "arXiv preprint arXiv:2208.10160.",
            "year": 2022
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "Our utilization of SVGD over conventional variational inference (VI) methods is driven by multiple factors, each rooted in the limitations and attributes of standard VI approaches",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large-scale pre-trained language models (PLMs) have been recently fine-tuned for various NLP tasks (Devlin et al., 2019; Raffel et al., 2020a). Due to the computational challenges of training the extensive parameters in PLMs, there is a growing focus on methods that efficiently tune fewer parameters (Houlsby et al., 2019; Ben Zaken et al., 2022).\n*These authors contributed equally to this work 1Code: https://github.com/heyzude/BMTPT\nOne of the promising approaches is prompt tuning (PT, Lester et al. 2021), where a few adaptable vectors are added as prompts to the input of the downstream task (Lester et al., 2021; Li and Liang, 2021). PT freezes the PLM model parameters and limits the learning to prompts, yet it achieves impressive performance. However, it is still challenging to achieve the same level of performance as the full fine-tuning, as well as to mitigate sensitivity to initialization (Zhong et al., 2022).\nTo address these challenges, recent works (Wang et al., 2023; Asai et al., 2022; Vu et al., 2022) proposed to adopt multi-task transfer learning approach, where the prompt is trained from multiple source tasks to be applied to the target task. Specifically, they train real-valued vectors for prompts\n(i.e. soft prompts) on source tasks and use them as the initialization of prompt for the target task. However, it is unclear whether aggregating such individually-trained prompts provides a reliable initialization point and fully harnesses the benefits of multi-task transfer learning.\nIn this paper, we propose Bayesian Multi-Task Prompt Tuning (BMTPT) as a practical yet effective solution to this challenge. Unlike traditional methods of prompt tuning grounded in transfer learning, our approach engages with the posterior distribution of prompts across a multitude of source tasks. For the transference of knowledge gleaned from source tasks, we utilize the source prompts\u2019 posterior distribution as the prior for the designated target task. This Bayesian method of transfer learning augments the conventional transfer learning framework, which primarily learns the initialization point of the target prompt from the source tasks. Specifically, BMTPT employs Stein Variational Gradient Descent (SVGD, Liu and Wang 2016), a particle-based Variational Inference (VI) method, to approximate the source prompts\u2019 posterior distribution. Further elaboration on this method is provided in Section 2.2.\nWe validate our approach through experiments on 21 datasets across diverse NLP tasks and output formats. The experimental results demonstrate that BMTPT achieves comparable or superior performance to strong state-of-the-art parameter-efficient fine-tuning methods (Asai et al., 2022; Wang et al., 2023) as well as full fine-tuning, while utilizing a very small number of parameters and requiring no auxiliary models other than the prompt itself."
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 Transfer Learning for Prompt Tuning",
            "text": "Fine-tuning entire models for downstream NLP tasks, particularly with a Large Language Model (LLM), can be expensive in terms of training costs. Therefore, parameter-efficient tuning focuses on limiting the updates to a small set of parameters. Various approaches have been proposed, such as Adapter (Houlsby et al., 2019) and its variants (Karimi Mahabadi et al., 2021a; Hu et al., 2022) that involve inserting trainable layers, and BitFit (Ben Zaken et al., 2022) that only trains bias weights while keeping other weights intact.\nRecently, there has been growing interest in prompt tuning (PT). This approach involves updating only the \u2018soft prompt\u2019, a set of continuous\nvectors that are prepended to the input. We can formally describe PT as follows: consider an input sequence x, and a soft prompt \u03b8 \u2208 Rl\u00d7d with length l and dimension d, which matches the language model\u2019s (LM) embedding dimension. The soft prompt is prepended to the sequence x and then processed by the LM, resulting in the prediction of the target sequence y.\nOur work aligns closely with recent efforts to transfer soft prompts from source tasks in order to initialize prompts for target tasks. For instance, SPoT (Vu et al., 2022) retrieves a source task prompt based on similarity to initialize the target task prompt, while ATTEMPT (Asai et al., 2022) employs an attention mechanism to initialize the prompt for the target task using information from the source prompts2. The most recent method for prompt tuning transfer, MPT (Wang et al., 2023), decomposes source prompts into shared and taskspecific parts to reduce interference between tasks during aggregation.\nHowever, these strategies may not fully address the inherent heterogeneity within source task distributions. They can falter, especially when attempting to aggregate prompts that have been trained across various tasks. Particularly, these issues persist even when each source task\u2019s posterior distribution follows a Gaussian distribution. Further discussion on this subject can be found in Appendix A.\nHence, an integrated approach regarding source task distributions may prove advantageous if a representative knowledge set can be constituted for transfer to the target task. This paper takes a Bayesian approach to transferring prompts from source tasks to target tasks. Instead of learning prompts individually then aggregating them, we use the full posterior distribution of prompts across the source tasks. Since this is intractable, we approximate the posterior via sampling, and leverage these samples for training the prompt for the target task, which corresponds to setting the posterior as the prior of the target prompt."
        },
        {
            "heading": "2.2 Particle Based VI and SVGD",
            "text": "Variational Inference (VI) is a widely used approach in machine learning for distribution approximation, notable in Bayesian Neural Networks (BNNs) (Blundell et al., 2015; Graves, 2011). Despite its computational simplicity, it often restricts\n2Although ATTEMPT includes a randomly initialized target prompt in the attentional mixture, the argument in this section still applies.\nthe family of distributions, a limitation that is less present in methods like MCMC (Gilks et al., 1995; Doucet et al., 2001; Robert and Casella, 2004).\nParticle-based VI methods provide an alternative approach by drawing upon the strengths of both VI and MCMC. Unlike traditional VI methods, particle-based VI does not restrict itself to a specific family of distributions. This flexibility allows it to approximate a wider range of complex and diverse distributions (Liu and Wang, 2016; Zhang et al., 2020; Naesseth et al., 2018). However, its theoretical guarantees are not yet fully understood. Assumptions often made, such as the presence of infinite particles or adherence to simple distributions like Gaussian, may not hold in practical scenarios (Naesseth et al., 2018; Salim et al., 2022; Sun et al., 2022; Liu et al., 2023).\nStein Variational Gradient Descent (SVGD, Liu and Wang 2016) is a significant advancement in particle-based VI. SVGD applies a transformation to particles to make them more representative of the target distribution through an iterative process. Specifically, for let particles tend to position themselves as though they were samples drawn from the distribution p, The update rule of SVGD is described as follows:\n\u03b8i \u2190 \u03b8i + \u03b1\u03d5\u2217p(\u03b8i),where\u03d5\u2217p(\u03b8i) is\n1\nM M\u2211 j=1 k(\u03b8j ,\u03b8i)\u2207\u03b8jlog p(\u03b8j) +\u2207\u03b8jk(\u03b8j ,\u03b8i) ,\nwhere k(\u00b7, \u00b7) is the positive definite kernel function like RBF.\nDespite its merits, SVGD can face mode collapse (Chen and Ghattas, 2020; Liu et al., 2022). One workaround, Damped SVGD (Ba et al., 2022), mitigates this by adjusting the deterministic bias in the update rule, which is used in our work. For a more thorough mathematical explanation, kernel details, and information about damped SVGD, we direct readers to Appendix B."
        },
        {
            "heading": "3 Problem Setting",
            "text": "In this section, we formally introduce core elements, symbols, and problem statements that form the basis of our approach. We denote the trainable parameter of the soft prompt as \u03b8 \u2208 Rl\u00d7d, characterized by its length l and the dimension d of the Language Model (LM). For clarity, we use \u03b8S and \u03b8T to denote the soft prompts for source tasks and target task(s), respectively. This implies that the\nsoft prompt \u03b8 is prepended to the sequence x, prior to its processing by the LM. The underlying objective is to predict the target sequence y. We denote the dataset for the k-th source as DSk , and define DS = \u22c3K k=1DSk . The target task is represented as DT during task adaptation. Thus, the i-th instance in the DSk dataset will be represented as (xki ,yki ). Note that the log-likelihood log p(DSk |\u03b8\nS) for the k-th source task can be represented as follows:\nlog p(DSk |\u03b8S) = |DSk |\u2211 i=1 log pLM(y k i | [\u03b8S ;xki ]) .\nIn this formulation, pLM denotes the likelihood determined by the LM and the corresponding criterion.\nNext we state our Bayesian objective, aiming to optimize the target task prompt using the posterior of source prompts for a transfer learning scheme.\nProblem Statement. The objective is to maximize the posterior probability of the target prompt \u03b8T , as expressed by the following equation:\nargmax \u03b8T\np(DT |\u03b8T ) p(\u03b8T |DS) , (1)\nwhere p(DT |\u03b8T ) is the likelihood and p(\u03b8T |DS) is the prior that is learned from the source tasks in prior to the target task adaptation:\np(\u03b8T |DS) = \u222b \u03b8S p(\u03b8T |\u03b8S) p(\u03b8S |DS)d\u03b8S . (2)\nIn this context, the prior distribution p(\u03b8T | DS) serves as a guide for the target task adaptation. We model p(\u03b8T |\u03b8S) as the multivariate Gaussian with mean \u03b8S , since without any information on the target task, it is natural to have \u03b8T = \u03b8S .\nThis problem formulation provides a general framework subsuming conventional transfer learning method for prompt tuning. For example, we could approximate the above integral in Eq. (2) defining the prior on \u03b8T using a prompt trained from source tasks \u03b8S\u2217, i.e. p(\u03b8S |DS)=\u03b4\u03b8S\u2217(\u03b8\nS), which would be roughly equivalent to the conventional transfer learning setting where the source prompt serves as the initialization of the target prompt.\nAssuming an uninformative prior for the source prompt \u03b8S (e.g. uniform distribution) as well as independent selection of source tasks, the posterior distribution p(\u03b8S |DS) for source tasks is formulated as the product of the posteriors of each task.\nRemark. Assuming the uniform prior for \u03b8S and independent selection of source tasks, the global posterior p(\u03b8S | DS) is proportional to the product of posteriors:\np ( \u03b8S \u2223\u2223DS = \u22c3Kk=1DSk ) \u221d K\u220f k=1 p(\u03b8S | DSk ) ."
        },
        {
            "heading": "4 Approach",
            "text": "Instead of optimizing individual prompts for each source task in isolation, our method primarily revolves around learning the posterior distribution of source prompts across all source tasks. This approach assigns a larger probability mass to those prompts capable of addressing a greater number of source tasks, thereby potentially becoming more suitable candidate prompts for the target task as well. We implement this concept by using particles to approximate the posterior distribution. The following subsections provide a detailed explanation of this methodology."
        },
        {
            "heading": "4.1 Main Strategy",
            "text": "The optimization of the target task prompt is modeled as a MAP inference in Eq. (1) using p(\u03b8T |DS) as the prior. We approximate this with M particles {\u03b8Si }Mi=1 (each particle corresponds to a soft prompt) drawn from p( \u00b7 |DS) using SVGD:\np(\u03b8T |DS) = E [ p(\u03b8T |\u03b8S) \u2223\u2223\u2223\u03b8S\u223c p( \u00b7 |Ds)] Monte-Carlo\nSampling \u2243 1\nM M\u2211 i=1 p(\u03b8T |\u03b8Si ) . (3)\nFor task adaptation, i.e. obtaining the prompt for the target task, the objective Eq. (1) is achieved based on the approximation provided by Eq. (3):\nargmin \u03b8T\n\u2212 log p(DT |\u03b8T )\u2212 log p(\u03b8T | DS)\n\u2243 argmin \u03b8T\n\u2212 log p(DT |\u03b8T )\u2212 log 1 M M\u2211 i=1 p(\u03b8T |\u03b8Si )\n=: J ( \u03b8T ) (4)\nThe pseudo-code of our BMTPT algorithm is shown in Algorithm 1.\nFor practical purposes, we can minimize the second term of the objective J ( \u03b8T ) by applying Jensen\u2019s inequality, as demonstrated below:\n\u2212 log 1 M M\u2211 i=1 p(\u03b8T |\u03b8Si ) \u2264 \u2212 1 M M\u2211 i=1 log p(\u03b8T |\u03b8Si )\n= 1\n2\u03c32 \u2225\u2225\u2225\u03b8T \u2212 1 M M\u2211 i=1 \u03b8Si \u2225\u2225\u22252 + C (5)\nAlgorithm 1 Bayesian Multi-Task Prompt Tuning\nInput: DS ,DT : source tasks and target task \u03980 = {\u03b80,i}Mi=1 : initialized particle set\nSource Posterior Learning: for t\u2190 0 to T\u22121 do\n\u0398t+1 \u2190 \u0398t + \u03b1\u03d5\u2217p(\u00b7|DS)(\u0398t) (SVGD iteration; Section 2.2)\nend for Store \u03b8Si \u2190 \u03b8T,i for all i \u2208 [M ] Target Task Adaptation: J ( \u03b8T ) = \u2212 log p(DT |\u03b8T )\u2212 log 1\nM M\u2211 i=1 p(\u03b8T |\u03b8Si )\n\u03b8T \u2217 \u2190 argmin\u03b8T J ( \u03b8T ) Output: \u03b8T \u2217 : trained weight for the target task\nwhere \u03c3 and C are constants arising from the multivariate isotropic Gaussian assumption of p(\u03b8T |\u03b8S). Combining Eq. (4) and Eq. (5), the final loss for target adaptation is therefore:\nargmin \u03b8T\n[ \u2212 log p(DT |\u03b8T ) + 1\n2\u03c32 \u2225\u2225\u2225\u03b8T \u2212\u03b8\u0304S\u2225\u2225\u22252 ] (6)\nwhere \u03b8\u0304S = 1M \u2211M i=1\u03b8 S i . This objective suggests that, during target adaptation, we can initialize \u03b8T with the average value of the optimized particles \u03b8T \u2190 \u03b8\u0304S ."
        },
        {
            "heading": "4.2 Additional Strategies",
            "text": ""
        },
        {
            "heading": "4.2.1 Source Task Sampling",
            "text": "As transfer learning prepares for unknown arbitrary target tasks, usually it is considered preferable that various source tasks are learned. However, if the number of source tasks K increases, we have to calculate the training losses of all source tasks. Therefore it is necessary to alleviate the bottleneck coming from a large number of source tasks. To this end, we use an approximate posterior distribution instead of the true global posterior distribution. Specifically, during each source posterior learning iteration, we uniformly sample \u03ba tasks from the K source tasks (\u03ba < K) without replacement and constitute a batch with the data entries from that \u03ba tasks."
        },
        {
            "heading": "4.2.2 Composition of \u03b8T and Multi-target Task Adaptation",
            "text": "At the start of the target adaptation, we compose \u03b8T with element-wise multiplication of a full-rank matrix which is initialized with \u03b8\u0304S and a low-rank matrix whose elements are all 1, where both matrices are learnable and have the shape of (l, d). The low-rank matrix is made by abT where a = 1l and b = 1d and both a, b are trainable components. Importantly, during target adaptation, we adopt a two-speed learning rate scheme for the full-rank and low-rank matrices by setting a higher learning rate for the low-rank matrix (Ponti et al., 2022; Asai et al., 2022; Wang et al., 2023). This facilitates multi-target task adaptation, by employing multiple low-rank matrices to assign each low-rank matrix to each target task, while sharing the fullrank matrix among all target tasks. In doing so, the full-rank matrix captures the shared knowledge across tasks, while the respective low-rank matrices capture the task-specific knowledge (Wang et al., 2023). We also apply this scheme to single-target adaptation, as we empirically observed that the use of two-speed learning rate promotes faster performance convergence."
        },
        {
            "heading": "4.3 Training Process",
            "text": ""
        },
        {
            "heading": "4.3.1 Source Task Posterior Approximation",
            "text": "Unlike previous transfer learning methods in prompt tuning that individually train prompts for each source task, we approximate the global posterior distribution of source tasks, by employing M particles. Here, a particle corresponds to one instance of soft prompt. Each particle is initialized with randomly sampled tokens, following Lester et al. (2021). We pack a batch as the following: each particle \u03b8Si (which is an instantiation of soft prompt, and 1 \u2264 i \u2264 M ) is prepended to input texts from K source tasks, forming a batch of size M \u00b7 K. It is worth noting that the log p(\u00b7) in the SVGD update rule can be interpreted as the minus of the cross-entropy loss of the language model when given the input with the particle (prompt) prepended. Also, we employ a limited number of SVGD particles, usually M \u2264 10. We perform 100K SVGD updates to sample \u03b8S ."
        },
        {
            "heading": "4.3.2 Target Task Adaptation",
            "text": "With the initialized \u03b8T , we start target task adaptation. The loss for the adaptation process is Eq. (6), which is the combination of Maximum Likelihood\nEstimation (MLE) loss with respect to DT and minus of the average of log priors."
        },
        {
            "heading": "4.4 Efficiency of BMTPT",
            "text": "Recent prompt tuning transfer methods primarily focus on measuring the efficiency during target adaptation, overlooking the need to evaluate the efficiency of source task training phase, which is helpful for identifying potential bottlenecks. We highlight the efficiency of BMTPT in comparison to the most recent prompt tuning transfer methods, ATTEMPT (Asai et al., 2022) and MPT (Wang et al., 2023), in both source and target stage. It is noteworthy that both methods require additional neural networks beyond soft prompts during either source task training or target adaptation: MPT involves a teacher network that is of the same size as the LM backbone as it uses distillation during source task training, and ATTEMPT involves the training of an attention module during target adaptation.\nBMTPT, on the other hand, proves to be efficient in both the source posterior learning and target adaptation stages, when evaluated under criteria of computational and space complexity. The additional intricacies that BMTPT introduces, compared to vanilla prompt tuning, are the use of SVGD during source posterior learning and the computation of regularization terms derived from the prior during target adaptation (Eq. (6)). In terms of computational complexity, given that the SVGD step used in BMTPT primarily involves computing RBF kernel values among a limited number of particles, the computational cost is minimal. Likewise, the regularization calculation during target adaptation is also negligible. On the aspect of space complexity, BMTPT continues to exhibit efficiency. During source posterior learning, as BMTPT accompanies SVGD particles only, the memory space that BMTPT requires is occupied by the backbone LM parameters and the SVGD particles which are comprised of M \u00b7 l \u00b7 d trainable parameters. Since we employ a small number of particles, the memory consumption by SVGD particles is almost negligible. During target adaptation, as we compose one target task prompt with shared matrix (full-rank) and task-specific matrix (low-rank), BMTPT requires (l \u00b7 d)/N + (l+ d) trainable parameters per one target task, when we adapt on N target tasks. This makes BMTPT train only 0.035% parameters compared to full fine-tuning. For a detailed\nanalysis, we direct the reader to Appendix C."
        },
        {
            "heading": "4.5 Contrasts and Contributions",
            "text": ""
        },
        {
            "heading": "4.5.1 Constrast with Conventional Multi-Task Learning",
            "text": "Both BMTPT and traditional multi-task learning algorithms have a common point in that they utilize multi-source data. However, BMTPT uses multisource data to find a posterior distribution across the multi-source data and transfer the posterior to target domain, under the Bayesian perspective. Traditional multi-task learning methods, on the other hand, optimize network parameters with respect to MLE objectives in general."
        },
        {
            "heading": "4.5.2 Distinction from MPT",
            "text": "BMTPT constructs soft prompt using full-rank and low-rank matrices during the target adaptation stage, similar to MPT (Wang et al., 2023). However, the methodologies diverge in their application and intent. MPT separates shared (full-rank) and task-specific (low-rank) components during source training with the underlying intuition that discovering common patterns among various source tasks can promote efficient transfer. After source training, MPT re-uses the shared component and averages task-specific components to initialize full-rank and low-rank matrices, then applies element-wise multiplication of the full and low-rank matrices to form a target prompt. BMTPT, on the other hand, does not use the decomposition at source posterior learning. We employ SVGD particles which are instantiations of full-rank prompts, to learn the source posterior. Then, at the beginning of target adaptation, we prepare a full-rank matrix and lowrank matrix to form a target prompt. The full-rank matrix is initialized with the average of SVGD particles and the low-rank matrix is initialized with 1l\u00d7d (see Section 4.2.2). Notably, the intention behind this prompt decomposition is different from that of MPT; our aim is only to facilitate multi target task adaptation."
        },
        {
            "heading": "4.5.3 Unique Motivation behind BMTPT",
            "text": "At its core, BMTPT aims to focus on the essence of transfer learning by enhancing it via transferring a useful distribution as a prior for target adaptation. On the other hand, existing prompt transfer methods such as SPoT, ATTEMPT, and MPT tend to rely on the transferability between individual NLP tasks (e.g., SQuAD is more helpful for solving MRPC than SST-2). The efficacy of this unique\nmotivation is demonstrated by the experimental results in Section 6."
        },
        {
            "heading": "5 Experiment",
            "text": ""
        },
        {
            "heading": "5.1 Datasets and Tasks",
            "text": "As in previous works (Asai et al., 2022; Wang et al., 2023), We use a set of 6 extensive datasets as source tasks and assess the performance of our algorithm on a range of 21 distinct target tasks, encompassing entailment, paraphrase detection, sentiment analysis, question answering (QA), and commonsense reasoning.\nSource Tasks During source posterior learning, we use the following datasets from GLUE (Wang et al., 2019b), SuperGLUE (Wang et al., 2019a), and MRQA 2019 shared task (MRQA; Fisch et al. 2019), comprising over 100,000 annotations in total. Specifically, we utilize 6 source tasks, MNLI (Williams et al., 2018), QNLI (Demszky et al., 2018), QQP (Wang et al., 2019b) and SST-2 (Socher et al., 2013) from GLUE, SQuAD (Rajpurkar et al., 2016) from MRQA, and ReCoRD (Zhang et al., 2018) from SuperGLUE.\nTarget Tasks For target adaptation, we test our algorithm with 21 datasets from four benchmarks: MNLI, QQP, QNLI, SST-2, RTE (Giampiccolo et al., 2007), CoLA (Warstadt et al., 2019), STS-B (Cer et al., 2017) and MRPC (Dolan and Brockett, 2005) from GLUE; BoolQ (Clark et al., 2019), CB (de Marneffe et al., 2019), MultiRC (Khashabi et al., 2018), WiC (Pilehvar and Camacho-Collados, 2019) and WSC (Levesque et al., 2012) from SuperGLUE; Natural Questions (NQ; Kwiatkowski et al. 2019), HotpotQA (HQ; Yang et al. 2018), NewsQA (News; Trischler et al. 2017) and SearchQA (SQA; Dunn et al. 2017) from MRQA; WinoGrande (Sakaguchi et al., 2020), Yelp-2 (Zhang et al., 2015), SciTail (Khot et al., 2018) and PAWS-Wiki (Zhang et al., 2019) from the \"Others\" benchmark in Asai et al. (2022). We direct readers to Appendix D for the performance and analysis on MRQA and \"Others\" benchmarks."
        },
        {
            "heading": "5.2 Implementation Details and Baselines",
            "text": "Implementation Details Throughout the experiments, we use T5-base as the base LM for BMTPT and all of the baselines, and we use prompt of length 100. Unless specified differently, we\nemploy 5 particles for SVGD and use 6 source tasks as mentioned in Subsection 5.1, therefore forming a batch of size 30 (5\u00d7 6). Also we use \u03c3 =105 for target adaptation loss, denoted at Eq. (6). For two-speed learning rate, we set 0.3 as the full-rank matrix learning rate and 0.4 as low-rank matrix learning rate. We use a batch of size 32 during target adaptation. For multi-target task adaptation, we first form a batch of input texts from target tasks, using example-proportional\nmixing strategy (Raffel et al., 2020b), then prepend a corresponding target prompt to each input text in the batch. We ran all the experiments three times using different random seeds and provide the mean and standard deviations of the results. In cases where a dataset lacks a publicly available test split with annotations, we adopt either the original development set as our test set or perform a split within the original development set to create separate development and test sets, following\nMahabadi et al. (2021).\nBaselines We conduct a comprehensive comparison of BMTPT with various baseline methods, including full finetuning (FT), vanilla prompt tuning (PT) (Lester et al., 2021), existing prompt transfer methods such as SPoT (Vu et al., 2022), ATTEMPT (Asai et al., 2022) and MPT (Wang et al., 2023), as well as popular parameter-efficient approaches like Adapters (Houlsby et al., 2019) and BitFit (Ben Zaken et al., 2022). On GLUE, we additionally compare with several state-ofthe-art multi-task learning methods including HyperFormer (Karimi Mahabadi et al., 2021b) and HyperDecoder (Ivison and Peters, 2022), along with multi-task variants of FT and Adapters. Also, to compare our algorithm with conventional multi-task transfer learning, we implement and evaluate a vanilla multi-task transfer method that learns a single prompt upon the combined loss of source tasks and transfers it to the target task. We either directly quote reported numbers or utilize publicly available source code under the same backbone for a fair comparison, as outlined in the respective papers (Mahabadi et al., 2021; Karimi Mahabadi et al., 2021b; Asai et al., 2022; Wang et al., 2023)."
        },
        {
            "heading": "6 Results",
            "text": "In Section 6.1, we provide the main findings on GLUE and SuperGLUE benchmarks. For findings on MRQA and \"Others\" benchmarks, please refer to Appendix D. In Section 6.2, we further provide a set of analyses."
        },
        {
            "heading": "6.1 Main Results",
            "text": ""
        },
        {
            "heading": "6.1.1 GLUE and SuperGLUE",
            "text": "As shown in the top part of Table 1, BMTPT achieves new state-of-the-art results in parameterefficient fine-tuning for both GLUE and SuperGLUE, outperforming other prompt tuning transfer methods (Vu et al., 2022; Asai et al., 2022; Wang et al., 2023). Compared to vanilla PT (Lester et al., 2021), BMTPT demonstrates a relative improvement of 16.5% on GLUE and 16.8% on SuperGLUE. This highlights the advantages of transferring knowledge using Bayesian approach. It is worth mentioning that BMTPT outperforms the full fine-tuning baseline on both benchmarks, despite only tuning 0.035% of the parameters compared to full fine-tuning.\nThe results presented in the bottom part of Table 1 demonstrate the ability of BMTPT to effectively utilize multi-task knowledge during finetuning on a group of target tasks. This highlights that BMTPT can benefit from multi-target adaptation setting, by further reducing the number of trainable parameters.\nWe also compare the performance of BMTPT and vanilla multi-task transfer that is introduced in Section 5.2, in Table 1. Surprisingly, vanilla multitask transfer shows strong performance in GLUE and SuperGLUE tasks, outperforming competitive baselines. This result supports Section 2.1 which claims that previous methods (Vu et al., 2022; Asai et al., 2022; Wang et al., 2023) are not the optimal transfer technique. It is worth noting that BMTPT outperforms vanilla multi-task transfer. To understand this advantage, we may delve into the Bayesian perspective of BMTPT, which includes conventional transfer learning. While vanilla multitask transfer only learns an initialization point that contains relatively limited source task information (Shwartz-Ziv et al., 2022), BMTPT learns posterior from the source tasks and adopts it as prior during target adaptation, enabling a richer and more insightful adaptation process."
        },
        {
            "heading": "6.1.2 Few-Shot Experiments",
            "text": "We also present the results of the few-shot experiments on the GLUE and SuperGLUE datasets. For the 4-shot experiments, the learning rates were reduced to one-third of their original values to accommodate the decreased batch size relative to standard experiments. The performance figures for BMTPT are averaged over three runs, each initialized with a different random seed. These outcomes suggest that the prior used in target adaptation effectively positions the prompts to an optimal initial point for task adaptation in low-resource conditions."
        },
        {
            "heading": "6.2 Analyses",
            "text": ""
        },
        {
            "heading": "6.2.1 Model Scaling",
            "text": "We perform scaling experiments to analyze the performance of BMTPT as the size of the pretrained model increases. The result demonstrates that BMTPT can largely benefit from scaling LM to larger models. This aligns with the finding by (Lester et al., 2021), which suggests that prompt tuning is effective especially when applied to larger backbone LMs. Note that BMTPT achieves comparable performance to fully fine-tuned models even\nwith T5-base, meaning that BMTPT is effective across various model scales."
        },
        {
            "heading": "6.2.2 Effectiveness of Source Task Sampling",
            "text": "To evaluate the effectiveness of Source Task Sampling discussed in Section 4.2, we conducted experiments under two settings: (1) subsampling 3 tasks from a pool of 6 source tasks (refer to Section 5.1), to examine if Source Task Sampling can mitigate performance degradation at limited computation resource scenario, and (2) diversifying the source task set to include 12 tasks and subsampling 6 tasks from this expanded set, to investigate the potential benefits of Source Task Sampling with an expanded source task set. For the second setting, we expand the source task set with AGNews (Zhang et al., 2015), CommonsenseQA (Talmor et al., 2019), OpenBookQA (Mihaylov et al., 2018), ARC (Clark et al., 2018), adversarial NLI (Nie et al., 2020), and Winogrande (Sakaguchi et al., 2020).\nFrom Table 3, we can see that setting (1) shows minimal performance degradation compared to the case with 6 source tasks. This finding indicates the successful application of the Source Task Sampling technique in low computation resource scenarios. Also, setting (2) demonstrates slight performance enhancements, suggesting that Source Task Sampling can derive benefits from diversifying the source task set."
        },
        {
            "heading": "6.2.3 BMTPT Performance on Different Numbers of Particles",
            "text": "Since SVGD is a particle-based VI method, the number of particles employed may affect the performance of our method. Therefore we investigate the effect of the number of particles on target adaptation performance by comparing 5-particle BMTPT and 10-particle BMTPT (Table 3). We found that the 10-particle case does not yield better results than the 5-particle case. Because of the instability reported in the original SVGD paper (Liu and Wang, 2016) and a similar empirical finding from Yoon et al. (2018) we speculate that this absence of enhancement might be attributed to the inherent characteristics of SVGD, including its sensitivity to kernel function parameters."
        },
        {
            "heading": "6.2.4 Effect of Prior",
            "text": "To assess the impact of the prior term in Eq. (6), we conducted an ablation experiment by removing the prior term from the target adaptation loss. The ablated version of BMTPT exhibited poorer\nperformance, implying the efficacy of learning an informative source posterior and leveraging it during target adaptation to facilitate effective transfer learning."
        },
        {
            "heading": "7 Conclusion",
            "text": "We present Bayesian Multi-Task Prompt Tuning (BMTPT), a Bayesian approach for transferring soft prompt. Our method defines a posterior distribution over prompt on source tasks, and approximates the posterior using SVGD, then initializes the target prompt with aggregation of source prompts while regularizing the training of the target prompt using transferred posterior. Empirically we found this approach achieves comparable or superior performance over strong parameter-efficient fine-tuning baselines.\nLimitations While showing compelling experimental results with only the use of a soft prompt, BMTPT has its limitations. Primarily, appending the soft prompt to the input text leads to an extension in the overall input length, consequently increasing the memory footprint. This is a well-known issue in PT (Karimi Mahabadi et al., 2021a), and BMTPT is not immune to it. Furthermore, in BMTPT, since multiple particles are used and source task sentences are appended to each, the batch size grows to be a multiple of the number of particles. This could potentially increase memory demands during the source posterior learning. However, this can be mitigated by implementing Source Task Sampling or reducing the number of particles. Experiments determining the optimal number of particles have not been performed in our study, and future research could potentially explore this aspect to ascertain the most appropriate number of particles.\nAcknowledgements This work was supported by the \"Development of Efficient Fine-Tuning and Zero-Shot Generalization Methods\" project funded by KT (KT award B220002586), IITP grant funded by MSIT (No.2019-0-00075, AI Graduate School Program (KAIST); No.20200-00940, Foundations of Safe Reinforcement Learning and Its Applications to Natural Language Processing; No.2022-0-00311, Development of Goal-Oriented Reinforcement Learning Techniques for Contact-Rich Robotic Manipulation of Everyday Objects), Artificial intelligence\nindustrial convergence cluster development project funded by the Ministry of Science and ICT(MSIT, Korea) & Gwangju Metropolitan City, NRF of Korea (NRF2019R1A2C1087634), Field-oriented Technology Development Project for Customs Administration through NRF of Korea funded by the MSIT and Korea Customs Service (NRF2021M3I1A1097938), ETRI grant (22ZS1100, Core Technology Research for SelfImproving Integrated AI System), KAIST-NAVER Hypercreative AI Center."
        },
        {
            "heading": "A Analogy with Gaussians for the Aggregation of Prompts Trained on Diverse Tasks",
            "text": "Consider a scenario with K source tasks, each characterized by a posterior p(\u03b8|Dk) = N (\u00b5k,\u039b\u22121k ), where Dk represents the dataset of k-th task, and \u03b8 is a soft prompt. Under the uniform prior, maximizing the likelihood (MLE) is equivalent to MAP estimation, and would lead each source prompt trained on task k to the mode \u00b5k. By combining individual posteriors with assuming independent selection of tasks, we can construct the global posterior p(\u03b8|D \u2261 \u22c3K k=1Dk) \u221d \u220fK k=1 p(\u03b8|Dk)3. The goal of transfer learning is to maximize this posterior, anticipating that the overall knowledge captured from source tasks will lead to a good starting point for a target task. Note that the posterior, which is a product of Gaussian distributions, is a Gaussian distribution whose mean is \u00b5global=( \u2211K k=1\u039bk) \u22121(\u2211K k=1\u039bk\u00b5k ) . Since the mean is the mode of a Gaussian, \u00b5global would be a good candidate for the initialization point of the target prompt. However, unless the covariances differ only by a scaling factor, a weighted sum of the individual modes {\u00b5k}Kk=1 is unlikely to equal \u00b5global."
        },
        {
            "heading": "B Details for SVGD",
            "text": "B.1 Choice of SVGD Stein Variational Gradient Descent (SVGD) is a nonparametric variational inference technique that amalgamates the benefits of Markov Chain Monte Carlo (MCMC) and variational inference (Liu and Wang, 2016). Our utilization of SVGD over conventional variational inference (VI) methods is driven by multiple factors, each rooted in the limitations and attributes of standard VI approaches.\nThe target posterior distribution we aim to approximate is complex, potentially even multi-modal. Standard VI methods, constrained by a specific family of distributions, often fail to capture such intricate structures. Therefore, they can show an inherent bias toward particular tasks. In contrast, SVGD employs a particle-based approach to dynamically generate a more expansive class of approximating distributions. This capability allows SVGD to represent complex and multi-modal distributions with greater accuracy.\nFurthermore, traditional VI methods like Variational Autoencoders (VAE) are generator-based and necessitate sampling. In contrast, SVGD requires the log derivatives of the prior at each point, commonly referred to as the score function. Additionally, while most VI methods aim to minimize surrogates of KL divergence through optimization, SVGD employs a first-order update method with a competing mechanism between particle repulsion and gradient descent.\nB.2 Mathematical Explanation Whereas gradient descent guides particles towards the optimal direction of fastest objective decrease, SVGD identifies the optimal transformation to minimize the KL divergence between the current and target distributions.\nTo find the optimal direction in the unit ball B of the Reproducing Kernel Hilbert Space H, which is the closed linear span of {k(\u03b8, \u00b7) : \u03b8 \u2208 RD}, that minimizes the KL-divergence towards the target distribution p, SVGD uses the point transformation T[\u03b1\u03d5](\u03b8) = (I + \u03b1\u03d5)(\u03b8). We will use the same notation for probability density with probability measure \u00b5, if there is no confusion. Specifically, it finds \u03d5\u2217 that satisfies:\n\u03d5\u2217\u00b5,p \u2225\u03d5\u2217\u00b5,p\u2225H = argmax \u03d5\u2208B { \u2212 d d\u03b1 KL ( T[\u03b1\u03d5]#\u00b5 \u2225 p )\u2223\u2223\u2223 \u03b1=0 } ,\nwhere T#\u00b5(A) = \u00b5(T\u22121(A))4. The closed-form solution of the above is given by:\n\u03d5\u2217\u00b5,p = \u222b Rd [ \u2207 log p(\u03b8)k(\u03b8, \u00b7) +\u2207k(\u03b8, \u00b7) ] \u00b5(d\u03b8) .\nHere, log p(\u03b8) is the log-likelihood of p. The SVGD algorithm updates the distribution as follows:\n\u00b5t+1 = (I + \u03b1\u03d5 \u2217 \u00b5,p)#\u00b5t,\n3Under the uniform prior assumption, this relation can be derived from p(\u03b8|Dk) \u221d p(Dk|\u03b8) for each k. Please refer the Remark in the Section 3.\n4If random variable X follow distribution \u00b5, the distribution T#\u00b5 can be seen as the distribution of T(X).\nwhere \u03b1 is the step size. Discretized version of the above update rule for a finite set of particles {\u03b8i}Mi=1, SVGD iteratively transports the particles using the following update rule for m \u2208 [M ]:\n\u03b8i \u2190 \u03b8i + \u03b1\u03d5\u2217(\u03b8i),where\u03d5\u2217(\u03b8i) is 1\nM M\u2211 j=1 [\u2207\u03b8j log p(\u03b8j)k(\u03b8j ,\u03b8i) +\u2207\u03b8jk(\u03b8j ,\u03b8i)] .\nThe behavior inherent to SVGD is orchestrated by the two terms in the update, which define the key control mechanisms. Firstly, the first term entails the sharing of gradient information among particles, guiding their update trajectory. Additionally, the influence of neighboring particles is modulated by kernel distance weighting. The second term, \u2207\u03b8jk(\u03b8j ,\u03b8i), introduces a repelling force between the particles, preventing them from converging to a single mode.\nB.3 Detailed Explanation for RBF Kernel\nIn the execution of the Stein Variational Gradient Descent (SVGD) for our set of particles denoted as {\u03b8i}Mi=1, we adopted the Radial Basis Function (RBF) kernel, which is defined as follows:\nk(\u03b81,\u03b82) = exp\n( \u2212\u2225\u03b82 \u2212 \u03b81\u2225 2\nh\n) ,where h= ( Median { \u2225\u03b8j \u2212 \u03b8i\u22252 \u2223\u2223 i \u0338= j , i , j \u2208 [M ]} )2 log(M + 1) .\nIn this formulation, h is a parameter frequently adjusted according to the distances between particles. As part of our methodology, we adhere to the median heuristic, a strategy supported by previous studies (Sch\u00f6lkopf and Smola, 2018; Ba et al., 2022). This entails designating the bandwidth as the median of the set of mutual distances between particles.\nB.4 Damped SVGD\nThe variant of Stein Variational Gradient Descent (SVGD) we employ in this work is Damped SVGD, as delineated in the work by Ba et al. (2022). SVGD, in its typical implementation, is prone to variance collapse when applied in a finite regime with particles, rather than updating distributions directly. This necessitates an adaptation of the SVGD\u2019s update rule to ensure a proper approximation of the distribution with particles. The Damped SVGD specifically addresses this issue by moderating the influence of its own gradient descent term. This adjustment can be seen clearly in the update rule for \u03b8i in a configuration {\u03b8i}Mi=1. As compared to the standard update rule, the modification reads:\n\u03d5\u2217damped(\u03b8i) = 1\nM \u2211 j \u0338=i [\u2207\u03b8j log p(\u03b8j)k(\u03b8j ,\u03b8i) +\u2207\u03b8jk(\u03b8j ,\u03b8i)] + 1 M \u03bb \u00b7 \u2207\u03b8i log p(\u03b8i)k(\u03b8i,\u03b8i)\n= \u03d5\u2217(\u03b8)\u2212 (1\u2212 \u03bb) 1 M \u2207\u03b8i log p(\u03b8i)k(\u03b8i,\u03b8i) = \u03d5\u2217(\u03b8)\u2212 1\u2212 \u03bb M \u2207\u03b8i log p(\u03b8i) .\nIn the Damped SVGD paper, the parameter \u03bb can be chosen using one of two strategies: taking \u03bb as \u03bbmin = min ( 1, e\u22121(1 + Ml\u00b7d) ) for \"fully damped,\" or taking \u03bb as a value between \u03bbmin and 1 for \"intermediate.\" In\nour experiments, we use \"intermediate\" by consistently choosing the value min ( 1, e\u22121(5 + Ml\u00b7d) ) , taking both selections into account. In our standard setting, this yields \u03bb \u2248 0.368. This variant of SVGD improves upon the original by mitigating the issue of variance collapse in some degree."
        },
        {
            "heading": "C Computational Complexity Analysis of BMTPT",
            "text": "Our computational analysis verifies that BMTPT is computationally efficient for both source task posterior learning and task adaptation stages. Note that the additional computation necessary for BMTPT occurs after the prompt receives the back-propagated gradient information from the LLM.\nC.1 Definitions of Notations \u2022 M : Number of particles.\n\u2022 l: Length of the prompt.\n\u2022 d: Hidden dimension of LLM (Large Language Model).\n\u2022 dprompt = d\u00d7 l: Dimension of the prompt.\n\u2022 Tgrad: Number of operations for the gradient backpropagation through the backbone LLM.\n\u2022 K: RBF Kernel matrix with dimensions M \u00d7M (Ki,j = k(\u03b8i,\u03b8j)).\n\u2022 \u0398: Matrix of prompt parameters with dimensions M \u00d7 dprompt.\n\u2022 \u2207 logp: Gradient of log-probability for each particle.\n\u2022 \u03b1: Stepsize.\nC.2 Source Task Training In the source task training phase, we have a multi-particle formulation governed by SVGD with an RBF Kernel. The formulation involves various matrix and vector products, which we denote as\n\u2206\u0398 = \u03b1 ( K\u2207 logp+ 2\nh (diag(K1)\u2212K)\u0398\n) .\nThe computational complexity for BMTPT during this phase can be summarized as O(Tgrad) + M2 \u00b7 O(dprompt). This indicates that BMTPT requires additional M2 \u00b7 O(dprompt) calculations over the vanilla prompt tuning. However, since Tgrad is the dominating factor and M2 = 25 in our experiments, this increase is computationally acceptable. The average wall-clock time recorded during the training of the source task, based on 5 updates, is as follows: 0.42 seconds for the backward pass through the language model (LM) and 0.0035 seconds for Damped SVGD. We used a single GeForce RTX 3090 GPU for these computations.\nC.3 Task Adaptation Stage During the task adaptation stage, the additional computational complexity of BMTPT is mainly due to the upper bound of log prior term, which takes O(dprompt) computations. Therefore, the computational complexity for a single update is O(Tgrad) + O(dprompt). Here as well, the dominating factor is Tgrad. Similarly, we report the wall-clock time observed on our device during the target adaptation phase, specifically for the SuperGLUE-CB task with a batch size of 32. The forward pass through the LM takes an average of 0.16 seconds, while the forward pass for the prior term requires 0.00011 seconds. We used a single GeForce RTX 3090 GPU."
        },
        {
            "heading": "D Experiment on MRQA and \"Others\" Benchmark",
            "text": "For MRQA and several \"Others\" datasets, BMTPT remains competent among parameter-efficient baselines, showing the versatility of our approach outside GLUE and SuperGLUE."
        }
    ],
    "title": "Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning",
    "year": 2023
}