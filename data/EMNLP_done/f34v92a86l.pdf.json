{
    "abstractText": "Progress in neural grammatical error correction (GEC) is hindered by the lack of annotated training data. Sufficient amounts of highquality manually annotated data are not available, so recent research has relied on generating synthetic data, pretraining on it, and then finetuning on real datasets; performance gains have been achieved either by ensembling or by using huge pretrained models such as XXL-T5 as the backbone. In this work, we explore an orthogonal direction: how to use available data more efficiently. First, we propose auxiliary tasks that exploit the alignment between the original and corrected sentences, such as predicting a sequence of corrections. We formulate each task as a sequence-to-sequence problem and perform multi-task training. Second, we discover that the order of datasets used for training and even individual instances within a dataset may have important effects on the final performance, so we set out to find the best training schedule. Together, these two ideas lead to significant improvements, producing results that improve state of the art with much smaller models; in particular, we outperform the best models based on T5-XXL (11B parameters) with a BART-based model (400M parameters).",
    "authors": [
        {
            "affiliations": [],
            "name": "Andrey Bout"
        },
        {
            "affiliations": [],
            "name": "Huawei Noah\u2019s"
        },
        {
            "affiliations": [],
            "name": "Alexander Podolskiy"
        },
        {
            "affiliations": [],
            "name": "Sergey Nikolenko"
        },
        {
            "affiliations": [],
            "name": "Irina Piontkovskaya"
        }
    ],
    "id": "SP:8c3f3b67895daf3d2751d9abce7fef34462ffa70",
    "references": [
        {
            "authors": [
                "Abhijeet Awasthi",
                "Sunita Sarawagi",
                "Rasna Goyal",
                "Sabyasachi Ghosh",
                "Vihari Piratla."
            ],
            "title": "Parallel iterative edit models for local sequence transduction",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Abhijeet Awasthi",
                "Sunita Sarawagi",
                "Rasna Goyal",
                "Sabyasachi Ghosh",
                "Vihari Piratla."
            ],
            "title": "Parallel iterative edit models for local sequence transduction",
            "venue": "ArXiv, abs/1910.02893.",
            "year": 2019
        },
        {
            "authors": [
                "Chris Brockett",
                "William B. Dolan",
                "Michael Gamon."
            ],
            "title": "Correcting ESL errors using phrasal SMT techniques",
            "venue": "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational",
            "year": 2006
        },
        {
            "authors": [
                "Christopher Bryant",
                "Mariano Felice",
                "\u00d8istein E. Andersen",
                "Ted Briscoe."
            ],
            "title": "The BEA-2019 shared task on grammatical error correction",
            "venue": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52\u201375,",
            "year": 2019
        },
        {
            "authors": [
                "Christopher Bryant",
                "Mariano Felice",
                "\u00d8istein E. Andersen",
                "Ted Briscoe."
            ],
            "title": "The bea-2019 shared task on grammatical error correction",
            "venue": "BEA@ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Christopher Bryant",
                "Mariano Felice",
                "Ted Briscoe."
            ],
            "title": "Automatic annotation and evaluation of error types for grammatical error correction",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Christopher Bryant",
                "Hwee Tou Ng"
            ],
            "title": "How far are we from fully automatic high quality grammatical error correction",
            "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference",
            "year": 2015
        },
        {
            "authors": [
                "Yo Joong Choe",
                "Jiyeon Ham",
                "Kyubyong Park",
                "Yeoil Yoon."
            ],
            "title": "A neural grammatical error correction system built on better pre-training and sequential transfer learning",
            "venue": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building",
            "year": 2019
        },
        {
            "authors": [
                "Steven Coyne",
                "Keisuke Sakaguchi",
                "Diana Galvan-Sosa",
                "Michael Zock",
                "Kentaro Inui"
            ],
            "title": "Analyzing the performance of gpt-3.5 and gpt-4 in grammatical error correction",
            "year": 2023
        },
        {
            "authors": [
                "Daniel Dahlmeier",
                "Hwee Tou Ng."
            ],
            "title": "Better evaluation for grammatical error correction",
            "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2012
        },
        {
            "authors": [
                "Daniel Dahlmeier",
                "Hwee Tou Ng",
                "Siew Mei Wu."
            ],
            "title": "Building a large annotated corpus of learner English: The NUS corpus of learner English",
            "venue": "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages",
            "year": 2013
        },
        {
            "authors": [
                "Meiyuan Fang",
                "Kai Fu",
                "Jiping Wang",
                "Yang Liu",
                "Jin Huang",
                "Yitao Duan."
            ],
            "title": "A hybrid system for NLPTEA-2020 CGED shared task",
            "venue": "Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications, pages 67\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Tao Fang",
                "Shu Yang",
                "Kaixin Lan",
                "Derek F. Wong",
                "Jinpeng Hu",
                "Lidia S. Chao",
                "Yue Zhang"
            ],
            "title": "Is chatgpt a highly fluent grammatical error correction system? a comprehensive evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Jennifer Foster",
                "Oistein Andersen."
            ],
            "title": "GenERRate: Generating errors for use in grammatical error detection",
            "venue": "Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications, pages 82\u201390, Boulder, Colorado.",
            "year": 2009
        },
        {
            "authors": [
                "Roman Grundkiewicz",
                "Marcin Junczys-Dowmunt",
                "Kenneth Heafield."
            ],
            "title": "Neural grammatical error correction systems with unsupervised pre-training on synthetic data",
            "venue": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building",
            "year": 2019
        },
        {
            "authors": [
                "Jiatao Gu",
                "Changhan Wang",
                "Jake Zhao."
            ],
            "title": "Levenshtein transformer",
            "venue": "Neural Information Processing Systems.",
            "year": 2019
        },
        {
            "authors": [
                "Phu Mon Htut",
                "Joel Tetreault."
            ],
            "title": "The unbearable weight of generating artificial errors for grammatical error correction",
            "venue": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 478\u2013483, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Masahiro Kaneko",
                "Kengo Hotate",
                "Satoru Katsumata",
                "Mamoru Komachi."
            ],
            "title": "TMU transformer system using BERT for re-ranking at BEA 2019 grammatical error correction on restricted track",
            "venue": "Proceedings of the Fourteenth Workshop on Innovative",
            "year": 2019
        },
        {
            "authors": [
                "Satoru Katsumata",
                "Mamoru Komachi."
            ],
            "title": "Stronger baselines for grammatical error correction using a pretrained encoder-decoder model",
            "venue": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Shun Kiyono",
                "Jun Suzuki",
                "Masato Mita",
                "Tomoya Mizumoto",
                "Kentaro Inui."
            ],
            "title": "An empirical study of incorporating pseudo data into grammatical error correction",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
            "year": 2019
        },
        {
            "authors": [
                "Shaopeng Lai",
                "Qingyu Zhou",
                "Jiali Zeng",
                "Zhongli Li",
                "Chao Li",
                "Yunbo Cao",
                "Jinsong Su."
            ],
            "title": "Typedriven multi-turn corrections for grammatical error correction",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 3225\u20133236,",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jared Lichtarge",
                "Chris Alberti",
                "Shankar Kumar."
            ],
            "title": "Data weighted training strategies for grammatical error correction",
            "venue": "Transactions of the Association for Computational Linguistics, 8:634\u2013646.",
            "year": 2020
        },
        {
            "authors": [
                "Masato Mita",
                "Shun Kiyono",
                "Masahiro Kaneko",
                "Jun Suzuki",
                "Kentaro Inui."
            ],
            "title": "A self-refinement strategy for noise reduction in grammatical error correction",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 267\u2013280,",
            "year": 2020
        },
        {
            "authors": [
                "Jakub N\u00e1plava",
                "Milan Straka."
            ],
            "title": "Grammatical error correction in low-resource scenarios",
            "venue": "Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019), pages 346\u2013356, Hong Kong, China. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Hwee Tou Ng",
                "Siew Mei Wu",
                "Ted Briscoe",
                "Christian Hadiwinoto",
                "Raymond Hendy Susanto",
                "Christopher Bryant."
            ],
            "title": "The CoNLL-2014 shared task on grammatical error correction",
            "venue": "Proceedings of the Eighteenth Conference on Computational Natu-",
            "year": 2014
        },
        {
            "authors": [
                "Kostiantyn Omelianchuk",
                "Vitaliy Atrasevych",
                "Artem Chernodub",
                "Oleksandr Skurzhanskyi."
            ],
            "title": "GECToR \u2013 grammatical error correction: Tag, not rewrite",
            "venue": "Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational",
            "year": 2020
        },
        {
            "authors": [
                "Kostiantyn Omelianchuk",
                "Vipul Raheja",
                "Oleksandr Skurzhanskyi."
            ],
            "title": "Text Simplification by Tagging",
            "venue": "Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, pages 11\u201325, Online. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Muhammad Qorib",
                "Seung-Hoon Na",
                "Hwee Tou Ng."
            ],
            "title": "Frustratingly easy system combination for grammatical error correction",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Sascha Rothe",
                "Jonathan Mallinson",
                "Eric Malmi",
                "Sebastian Krause",
                "Aliaksei Severyn."
            ],
            "title": "A simple recipe for multilingual grammatical error correction",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th",
            "year": 2021
        },
        {
            "authors": [
                "Sascha Rothe",
                "Jonathan Mallinson",
                "Eric Malmi",
                "Sebastian Krause",
                "Aliaksei Severyn"
            ],
            "title": "A simple recipe for multilingual grammatical error correction",
            "year": 2022
        },
        {
            "authors": [
                "Felix Stahlberg",
                "Shankar Kumar."
            ],
            "title": "Synthetic data generation for grammatical error correction with tagged corruption models",
            "venue": "Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications, pages 37\u201347, Online.",
            "year": 2021
        },
        {
            "authors": [
                "Xin Sun",
                "Houfeng Wang."
            ],
            "title": "Adjusting the precision-recall trade-off with align-and-predict decoding for grammatical error correction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Pa-",
            "year": 2022
        },
        {
            "authors": [
                "Toshikazu Tajiri",
                "Mamoru Komachi",
                "Yuji Matsumoto."
            ],
            "title": "Tense and aspect error correction for ESL learners using global context",
            "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),",
            "year": 2012
        },
        {
            "authors": [
                "Maksym Tarnavskyi",
                "Artem Chernodub",
                "Kostiantyn Omelianchuk."
            ],
            "title": "Ensembling and knowledge distilling of large sequence taggers for grammatical error correction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Konstantin Yakovlev",
                "Alexander Podolskiy",
                "Andrey Bout",
                "Sergey Nikolenko",
                "Irina Piontkovskaya."
            ],
            "title": "GEC-DePenD: Non-autoregressive grammatical error correction with decoupled permutation and decoding",
            "venue": "Proceedings of the 61st Annual Meet-",
            "year": 2023
        },
        {
            "authors": [
                "Helen Yannakoudakis",
                "Ted Briscoe",
                "Ben Medlock."
            ],
            "title": "A new dataset and method for automatically grading ESOL texts",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages",
            "year": 2011
        },
        {
            "authors": [
                "Zheng Yuan",
                "Felix Stahlberg",
                "Marek Rei",
                "Bill Byrne",
                "Helen Yannakoudakis."
            ],
            "title": "Neural and FSTbased approaches to grammatical error correction",
            "venue": "Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applica-",
            "year": 2019
        },
        {
            "authors": [
                "Zheng Yuan",
                "Shiva Taslimipoor",
                "Christopher Davis",
                "Christopher Bryant."
            ],
            "title": "Multi-class grammatical error detection for correction: A tale of two systems",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2021
        },
        {
            "authors": [
                "Yi Zhang",
                "Tao Ge",
                "Furu Wei",
                "Ming Zhou",
                "Xu Sun."
            ],
            "title": "Sequence-to-sequence pre-training with data augmentation for sentence rewriting",
            "venue": "arXiv preprint arXiv:1909.06002.",
            "year": 2019
        },
        {
            "authors": [
                "Yue Zhang",
                "Bo Zhang",
                "Zhenghua Li",
                "Zuyi Bao",
                "Chen Li",
                "Min Zhang."
            ],
            "title": "SynGEC: Syntax-enhanced grammatical error correction with a tailored GECoriented parser",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "arXiv preprint",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Grammatical error correction (GEC) is an important problem setting with obvious applications that often require high-level understanding. Recent research has developed a common view of grammatical error correction (GEC) as monolingual text-to-text rewriting (N\u00e1plava and Straka, 2019; Grundkiewicz et al., 2019). GEC is often tackled with encoder-decoder architectures: the encoder receives an errorful sentence, and the decoder pro-\nduces its corrected version. In line with mainstream NLP developments, Transformer-based architectures have proven to be beneficial for GEC, leading to recent breakthroughs on multiple benchmarks (Rothe et al., 2021; Tarnavskyi et al., 2022; Sun and Wang, 2022).\nGEC is a challenging task that goes beyond simple spell-checking. The greatest difficulty comes from the complexity of grammar and punctuation rules, inconsistency in rules usage, contextual factors, and multiple correct answers. To solve GEC, a model should have a thorough understanding of grammar, punctuation, and syntax, and be aware of the ambiguity introduced by errors to correctly understand intended meaning.\nMost recent works approach these challenges by providing new complex architectures, using larger models, or ensembling multiple models, e.g. Yuan et al. (2021); Rothe et al. (2021); Tarnavskyi et al. (2022); Zhang et al. (2022). However, practicality demands faster and smaller models, so in this work, we examine another direction to improve GEC: modifying the training pipeline. Every step is important: (i) data preparation includes generating synthetic data for pretraining and preprocessing available datasets; (ii) pretraining is done on synthetic data and/or large-scale datasets with supervised pretext tasks such as language modeling; (iii) fine-tuning on downstream datasets is also far from straightforward; at the very least, one has to choose the order of training on available datasets, which may significantly impact the results. Due to the scarcity of annotated data for fine-tuning, we are concerned with using it as efficiently as possible and introduce two novel approaches for this.\nFirst, one can acquire additional information from parallel annotated corpora, e.g., an alignment\nbetween two sentences that can be used to derive operations that would transform one into the other. One can decompose the editing process into elementary edits: insertion, deletion, replacement, and change of word order; the latter can also be viewed as a sequence of deletions and insertions. This information is exploited by sequence tagging models that show good results on the GEC task, see Omelianchuk et al. (2020, 2021); Tarnavskyi et al. (2022). A drawback of this approach lies in the need to specify a vocabulary of available operations, use several decoding steps to correct complex errors, and in the hardness/inability to make complex rephrasing. We utilize additional information by converting it into auxiliary tasks and formulating them as sequence-to-sequence problems. As such, the model learns separate \u201cskills\u201d required for the successful correction of grammatical errors.\nSecond, GEC-specific datasets have different quality and sources, e.g., different language proficiency levels. Some datasets are collected from online platforms where users correct each other (they are noisier); in other datasets, sentences are corrected by teachers or annotators (these are cleaner). Some datasets consist of essays written by native speakers; others, of essays written by non-native students. Thus, the distribution of errors may severely differ. It is tempting to remove \u201cnoisy\u201d or out-of-distribution examples from training sets, but it seems that the model can learn even from such instances. We propose to use a training schedule for GEC datasets and show that it does matter how we order them. Also, we find that the order of sentences within the dataset matters as well; namely, we have found that placing sentences from the same block of text (e.g., the same essay) in the same batch is beneficial.\nOur primary contributions are as follows. First, we introduce a multi-task pretraining approach and a fine-tuning schema that together yield an improvement of up to 3% in F0.5 score compared to similarsized state of the art models1. Second, we show that our approach is able to outperform state of the art large models (T5-XL with 3B parameters and T5-XXL with 11B parameters) using a model with 400M parameters, reducing the computational load and ecological footprint. Third, our approach makes the model more robust, improving metrics on several datasets rather than trading them off\n1We plan to release the source code of our models upon acceptance.\nof each other (as usual). In what follows, Section 2 surveys related work, Section 3 introduces our approach, Section 4 shows evaluation results, an ablation study, and an analysis of our auxiliary tasks, Section 5 concludes the paper, and Section 6 discusses the limitations of our approach."
        },
        {
            "heading": "2 Related work",
            "text": "Neural approaches to grammatical error correction follow two main lines of research: (i) sequence tagging models and (ii) sequence-to-sequence models.\nSynthetic data. Training GEC models is difficult due to the natural lack of suitable training data and possible erroneous corrections, so synthetic data becomes a crucial part of any GEC pipeline (Choe et al., 2019; Stahlberg and Kumar, 2021; Htut and Tetreault, 2019). It had been used for GEC even before the deep learning era that required larger datasets (Foster and Andersen, 2009; Brockett et al., 2006). Synthetic data generators can mimic common typos and grammatical errors but usually cannot capture the target error distribution found in real-life evaluation sets or standard benchmarks. Methods for synthetic data generation include character perturbations, dictionary- or edit-distance-based replacements, shuffling word order, rule-based suffix transformations, and more (Grundkiewicz et al., 2019; Awasthi et al., 2019a; N\u00e1plava and Straka, 2019; Rothe et al., 2021). An empirical study of how to generate and use the synthetic data was done in Kiyono et al. (2019).\nAnother line of research upsamples training data in existing datasets. Mita et al. (2020) train a GEC model on a natural noisy dataset and then use its outputs for source sentences to construct a less noisy parallel dataset; Zhang et al. (2019)\nuse sentence rewriting approaches; Lichtarge et al. (2020) apply delta-log-perplexity scoring to rank sentences according to the difference in perplexity between two base model checkpoints and use higher-scoring sentences for final fine-tuning.\nMulti-stage fine-tuning. Due to data scarcity, training GEC models from scratch could be cumbersome. One of the options is to pre-train a model on some auxiliary task, e.g. Choe et al. (2019) proposed to initialize the GEC model with the pre-trained denoising autoencoder. Many GEC pipelines utilize pre-trained language models as backbone models for GEC; in particular, Rothe et al. (2021) used T5 (Raffel et al., 2020) while Katsumata and Komachi (2020) and Sun and Wang (2022) used BART (Lewis et al., 2020). Pretrained language models are also beneficial for reranking output hypotheses generated with beam search (Kaneko et al., 2019). Choe et al. (2019), Omelianchuk et al. (2020) and Tarnavskyi et al. (2022) decompose the training process of a GEC model into several stages: (i) pre-training on an errorful synthetic dataset; (ii) fine-tuning on natural high-quality datasets that combine both errorful and error-free sentences. Each stage requires its own tuning of hyperparameters such as the number of training steps and the learning rate.\nMulti-task learning. Several works aim to utilize additional information along with the standard parallel mapping. First, the grammatical error detection (GED) task can be extracted from GEC; Yuan et al. (2019) perform multi-task training with GED and GEC tasks and use GED features for reranking. A similar approach by Fang et al. (2020) trained a GED model separately and used it to filter edits. Yuan et al. (2021) separately pretrain a GED model and use its outputs as auxiliary inputs to fine-tune the encoder-decoder GEC model and rerank its outputs. Zhang et al. (2022) incorporate syntactic dependency information into the encoder.\nNon-autoregressive decoding. Another line of research introduces non-autoregressive decoding to speed up models. Awasthi et al. (2019b) predict language-specific edits to be applied to the output sequence. Iterative refinement is also possible. Gu et al. (2019) non-autoregressively refine an output sequence using language-agnostic insertions and deletions. Yakovlev et al. (2023) decompose the inference stage into permutation and decoding. First, a permutation network repositions the tokens of an input sequence with possible deletions and inser-\ntions. Second, the intermediate sequence is passed to a decoder network that iteratively fills in inserted placeholders.\nGPT-3.5 and GPT-4. The recent success of GPT-based models for a wide variety of tasks has led to several parallel works that compare how well these models fare in grammatical error correction. Fang et al. (2023) show that ChatGPT is still worse on GEC benchmarks than fine-tuned sequence-tosequence models, both in the zero-shot and fewshot scenarios. Coyne et al. (2023) provide the corresponding analysis for GPT-4, with even lower results, which means that specialized models are still relevant for GEC scenarios and validate our research."
        },
        {
            "heading": "3 Approach",
            "text": "In this section, we introduce our approach that uses multi-task learning and optimizes the training schedule for the sequence-to-sequence model architecture. In Section 3.1, we outline the model and hyperparameters used for training. In Section 3.2, we describe existing GEC datasets for training and evaluation, highlighting the specific properties of each. In Section 3.3, we specify the main and auxiliary tasks formulated as sequence-to-sequence mapping. Section 3.4 describes the training steps."
        },
        {
            "heading": "3.1 Model",
            "text": "We use a straightforward text-only approach while also keeping the model size limited. Namely, we formulate all training tasks as sequence-tosequence text rewriting problems and do not introduce any additional heads for edit prediction or other tasks. As the backbone, we use the BART (Lewis et al., 2020) model with a 12-layer encoder and decoder. We train the model in fp16 mode with learning rate 1e\u22125 and warmup ratio 7% with a linear scheduler. We fit 12 instances in a batch and use gradient accumulation with 4 steps to achieve a larger effective batch size. We did not perform a hyperparameter search except for the learning rate, which was chosen from [1e\u22125, 5e\u22125, 1e\u22126]. All training experiments were done on 8 NVIDIA Tesla V100 GPUs with 32GB of memory."
        },
        {
            "heading": "3.2 Training Data",
            "text": "For pretraining (Stage I), we use C4200M or PIE datasets. C4200M is a synthetic corpus based on clean sentences from the C4 dataset. This corpus was generated by a tagged corruption model to\nmeet the error distribution of BEA-dev (Bryant et al., 2019a); see Stahlberg and Kumar (2021) for details. PIE is a synthetic dataset of 9M parallel sentences generated by using rule-based grammatical errors such as deletion, insertion, and word replacement (Awasthi et al., 2019a).\nFor other training stages, we use the following datasets: (i) National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) that consists of essays written by undergraduate students on different topics and annotated by professional English instructors; (ii) Lang-8 Corpus of Learner English (Lang-8) (Tajiri et al., 2012) collected from the online language learning site Lang-8; this dataset is relatively \u201cnoisy\u201d as the users corrected themselves, and it comes with several annotations; (iii) First Certificate in English (FCE) (Yannakoudakis et al., 2011) with short texts written by English learners as answers to exam questions assessing the upper-intermediate level; this dataset is relatively clean but covers only a single group of English learners; (iv) Write & Improve + LOCNESS Corpus (W&I+L) (Bryant et al., 2019a); Write & Improve dataset consists of text blocks (essays, letters etc.) written by English learners and submitted to the W&I system; LOCNESS is composed of essays written by native English students, used only for evaluation; these datasets are the \u201ccleanest\u201d and have a wide distribution over different levels of English. Here and below, errorful sentences are those that contain at least one error. We use the BEA-2019 devel-\nopment set, i.e. W&I+L-dev, to choose the best model and report results on the CoNLL2014 test set (Ng et al., 2014) evaluated by the official M2 scorer (Dahlmeier and Ng, 2012) and the BEA2019 test set evaluated by ERRANT (Bryant et al., 2017). Table 1 summarizes dataset statistics and shows which training stages they are used for."
        },
        {
            "heading": "3.3 Multi-task learning",
            "text": "The standard approach is to view GEC as a sequence-to-sequence mapping from errorful sentences to corrected ones. Since the sentences are from the same language, one can align two sentences using an edit distance algorithm. The alignment can be transformed into a sequence of insert, delete, and replace operations that transform the errorful sentence into a correct one. We find the list of operations using ERRANT2 (Bryant et al., 2019b) and use them for auxiliary tasks.\nInspired by recent works on chain of thought prompting (Wei et al., 2023; Zhou et al., 2022), we construct several tasks and examine their influence on the model\u2019s performance. Each task, as illustrated in Fig. 1, is explicitly marked by a prefix written as \u27e8prefix\u27e9 and followed by an input string:\n(i) Correct: standard generation of a corrected sentence given the original, encoded as \u201c\u27e8correct\u27e9 source\u201d and trained to produce the target sentence (Fig. 1a);\n2We use version 2.3 of ERRANT for both evaluation and training set construction.\n(ii) Explain: generation of a sequence of explanations (atomic edits) given both original and corrected sentences; encoded as \u201c\u27e8explain\u27e9 Input: src \\n Target: tgt\u201d with the result in the format \u201cDelete smth \\n Insert smth...\u201d (Fig. 1b); if no correction is needed, the model should generate \u201cNo correction\u201d;\n(iii) Apply: application of edits to the original sentence to get the target, encoded as \u201c\u27e8apply\u27e9 Input: src \\n Do: Delete smth \\n Insert smth \\n Replace smth with smth\u201d; the result should be the correct target sentence (Fig. 1c);\n(iv) Edit: generation of the sequence of edits given the original sentence but not the target, encoded as \u201c\u27e8edit\u27e9 src\u201d with the target in the form \u201cDelete smth \\n Insert smth \\n Replace smth with smth\u201d (Fig. 1d); if no correction is needed, the model should generate \u201cNo correction\u201d.\nWe generate an auxiliary sequence-to-sequence pair for every task and for every pair from the original dataset."
        },
        {
            "heading": "3.4 Training Order",
            "text": "Tarnavskyi et al. (2022) and Omelianchuk et al. (2021) mention that training order is essential to obtain the best possible results. They separate the overall process into three stages: pretrain the model on synthetic data, fine-tune it on errorful sentences from the four GEC datasets: LANG-8, NUCLE, FCE, and W&I+L, and then fine-tune it on the clean GEC dataset W&I+L.\nIn this work, we suggest to modify this procedure and claim that the training process benefits from choosing a correct ordering of data. Our process is illustrated in Fig. 2: on Stage I, we similarly\npretrain our model on large-scale synthetic data. We consider two datasets that differ both in size and in generation approach: C4200M and PIE. This step adapts the model for the downstream task. On Stage II, we fine-tune on four GEC datasets but modify the above procedure. First, we use all sentences, not only errorful ones. Second, we use the datasets in a strict order: Lang-8, NUCLE, FCE, W&I+L, with no shuffling across datasets. Third, we do not shuffle samples inside the datasets either: each dataset is composed of coherent texts, so we thus preserve their original structure and place examples from the same texts together.\nFor a more complete comparison with previous works, we add Stage III where we additionally fine-tune the model on the W&I+L dataset. We note that this step helps to increase recall without a substantial decrease in precision, yielding modest improvements in F0.5. Note that in previous works, this step was mandatory as the target distribution is correlated with the W&I+L dataset. In contrast, we add this dataset as the last in our training order, which is a more natural approach; the suggested scheme also looks more suitable for real world tasks where there is no obvious way to split the data into different stages."
        },
        {
            "heading": "3.5 Re-weighted Sampling",
            "text": "The F0.5 metric is commonly used for the evaluation of GEC models. It puts a higher weight on the model\u2019s precision than recall. In our experiments, we see that the proposed training pipeline makes the model less aggressive in editing which improves performance. Another approach to making the model choose corrections that it is confident about is the Align-Pred method proposed by Sun and Wang (2022). It increases the probability of the next original token (obtained by aligning the original and currently generated sequences) during decoding. In order to show that our method is orthogonal to this, we apply Align-Pred to our best model. We introduce a modification to this method that goes beyond the original, applying temperature scaling before Align-Pred. This significantly improves Align-Pred (see Table 2)."
        },
        {
            "heading": "4 Evaluation and Analysis",
            "text": ""
        },
        {
            "heading": "4.1 Comparison with baselines",
            "text": "We compare the proposed model with several state of the art baselines. We used three variations of the tagging model GECToR: with the XLNet-L back-\nbone (Omelianchuk et al., 2020), with RoBERTa backbone and multi-stage training (Tarnavskyi et al., 2022), and with the XLNet-L backbone and multiturn corrections (Lai et al., 2022). For seq2seq baselines we consider the vanilla GEC Transformer trained on C4200M (Stahlberg and Kumar, 2021), T5-XL and T5-XXL models (Rothe et al., 2021) also trained on the C4 dataset, and a BART-based model trained on 300M synthetic data (Sun and Wang, 2022). All baselines have comparable or larger model sizes than ours and use the same PIE synthetic dataset or larger synthetic collections.\nTable 2 shows evaluation results for the CoNLL14 (Ng et al., 2014) and BEA-test (Bryant et al., 2019a) datasets. Our approach shows the best results for comparable model sizes with a significant margin and outperforms all models, including T5XL with 3B parameters and even T5-XXL which is 30x larger than our model (11B parameters) and trains on a larger synthetic dataset. We also present evaluation results of our approach with pretraining on the PIE dataset to compare with the models pretrained on it. Again, we see that our model outperforms more sophisticated methods such as (Lai et al., 2022)."
        },
        {
            "heading": "4.2 Influence of the Pretraining Dataset",
            "text": "In this section, we analyze the choice of the pretraining dataset, comparing two publicly available synthetic datasets: C4200M and PIE. They differ not only in size (see Table 1) but also in the type of generated errors that are model-based and rulebased respectively. Table 3 shows the performance of models pretrained in different setups. The model pretrained on C4200M has higher recall indicating that it covers more errors, while the model pretrained on PIE reaches higher precision. Note that almost all sentences from synthetic datasets contain errors, which means that the pretraining distribution differs a lot from the development set. Hence, to make a fair comparison we further fine-tune the models on GEC-specific datasets using the standard three-stage approach. Table 3 shows that the model pretrained on C4200M performs better in terms of precision, recall, and the F0.5 score."
        },
        {
            "heading": "4.3 Order of the Datasets",
            "text": "We also examine the influence of the ordering of training datasets and examples within a dataset. We are mainly concerned with multi-task training, but note that the training schedule also impacts the single-task pipeline (see Table 4). The model trained using a specific schedule achieves good results after the first stage of fine-tuning, while the third stage improves the model\u2019s recall and makes its results surpass the standard three-stage training.\nFor the multi-task approach, we use three tasks\u2014 Correct, Explain, and Apply\u2014and different dataset schedules. There are 16 possible orderings, but Lang-8 is a \u201cnoisy\u201d dataset so it is reasonable to place it early in the fine-tuning, while W&I+L is\nOrder Prec Rec F0.5 Order preserved within each dataset\nFCE \u2192 Lang-8 \u2192 NUCLE \u2192 W&I+L 68.72 44.98 62.15 NUCLE \u2192 Lang-8 \u2192 FCE \u2192 W&I+L 68.86 45.02 62.26 Lang-8 \u2192 FCE \u2192 NUCLE \u2192 W&I+L 68.52 45.32 62.16 Lang-8 \u2192 NUCLE \u2192 FCE \u2192 W&I+L 68.78 45.98 62.67 FCE \u2192 Lang-8 \u2192 W&I+L \u2192 NUCLE 77.41 13.70 40.11\nInstances shuffled within each dataset\nFCE \u2192 Lang-8 \u2192 NUCLE \u2192 W&I+L 66.41 48.47 61.84 Lang-8 \u2192 NUCLE \u2192 FCE \u2192 W&I+L 66.99 48.48 62.24\nInstances shuffled across datasets\nLang-8, NUCLE, FCE, W&I+L 48.21 41.20 46.63\nInstances shuffled within each dataset\nFCE \u2192 Lang-8 \u2192 NUCLE \u2192 W&I+L W&I+L 66.05 50.03 62.08 Lang-8 \u2192 NUCLE \u2192 FCE \u2192 W&I+L W&I+L 66.05 50.34 62.17\nInstances shuffled across datasets\nLang-8, NUCLE, FCE, W&I+L W&I+L 65.62 52.07 62.38\nTable 6: Ablation study of the dataset training order. The model is trained in 3-stages. Evaluation is done on the BEA-dev dataset.\n\u201ccleaner\u201d than others so we use it late in the training. Tables 5 and 6 shows the performance of models trained in different setups. It shows that, first, the order of datasets really matters: some cases outperform others by a significant margin. Second, shuffling instances within a dataset also reduces performance, which supports our hypothesis that sentences from the same block of text should be placed together while training. Third, the best\nperformance is achieved when W&I+L is the last dataset in the schedule, as evidenced by the drop in performance for the \u201cFCE \u2192 Lang-8 \u2192 W&I+L \u2192 NUCLE\u201d setup and improvement after further fine-tuning on the W&I+L dataset."
        },
        {
            "heading": "4.4 Multi-Task Pretraining",
            "text": "Table 4 compares standard fine-tuning with the proposed multi-task fine-tuning. We show the commonly used metrics\u2014precision, recall, and F0.5 score\u2014on three GEC datasets: CoNLL-14, BEAtest, and BEA-dev. In all cases, multi-task training leads to improved performance for the final task, especially for the BEA-dev dataset.\nTable 7 presents an ablation study related to the choice of auxiliary tasks; we fix the training schedule and use different task compositions. We make the following observations. First, the best combination of tasks is \u201cCorrect\u201d and \u201cApply\u201d, yielding the highest recall and high enough precision compared to other settings. Second, adding the edit prediction task (\u201cEdit\u201d) in combination with other tasks lowers the model\u2019s performance; below, we argue that this could be due to the complexity of the task that the model cannot effectively learn. On the other hand, while the \u201cEdit\u201d task does not improve performance, it could help to make the model interpretable without using external tools.\nAdditionally, we study the case where we replace\nthe GEC task with consecutive \u201cEdit\u201d and \u201cApply\u201d tasks, decomposing GEC into a chain-of-thought process: first the model says what is wrong and what actions should be taken and then it applies these actions to the original sentence to produce the final output. Table 7 shows that the model trained with only these two tasks severely underperforms, so \u201cclassical\u201d GEC is still necessary."
        },
        {
            "heading": "4.5 Auxiliary Tasks Analysis",
            "text": "Next, we examine the model\u2019s performance on auxiliary tasks. For the analysis of \u201cExplain\u201d and \u201cApply\u201d tasks, we use the model that had them both in pretraining; to analyze the \u201cEdit\u201d task, we use a model pretrained on all four tasks. We use the CoNLL-14 dataset for evaluation.\nThe \u201cExplain\u201d task urges the model to align to sentences and produce a correct sequence of edits to transform the source into the target. The exact match score of generated edits is 90% compared to the gold standard edits, and the F0.5 score is 91.55. Moreover, many \u201cerrors\u201d here come from the ambiguity of how to perform replacements or from merging/splitting corrections compared to the gold standard. For example, for the original sentence \u201cPeople can also know much information about the celebrity in Twitter and Facebook such as Obama , Bill Gates and get the first-hand study materials on it .\u201d an annotator suggested the following corrections: replace know much with find out a great deal of; delete the; replace celebrity with celebrities; replace in with from; replace , with and; delete the; replace it with these. The model correctly predicts all of them except one. It splits the first correction into two: replace know with find out and replace much with a great deal.\nThe model deals quite well with the \u201cApply\u201d task, getting 93.5% accuracy. Most errors correspond to doing a different operation (e.g., inserting a new word instead of replacing one word with another), applying edits in the wrong place (e.g., placing new tokens after a wrong word), and simply ignoring some of the edits, especially if the list of edits is long. It appears that some errors arise from ambiguous descriptions of edits\u2014in our approach we specify edits as, e.g., \u201cInsert the\u201d, and the model has to decide where to insert it by itself\u2014 so performance could be improved further. For an example, consider the following sentence: It is a concern that will be with us during our whole life , because we will never know when the \u201dpotential\nbomb\u2019 \u2019 will explode . Here, the correction \u201cdelete will\u201d targets the second will, but the model does not make changes to the original sentence, ignoring the correction. In this work, we restricted ourselves to simplistic prompts, and we leave an exploration of more detailed edit descriptions for further work.\nThe last auxiliary task, \u201cEdit\u201d, is the prediction of a list of edits. This task is hard for the model: the exact match score is only 23.5%, and the F0.5 score is 30.69. This low performance might be the reason why adding this task to the training pipeline decreases GEC performance. There are many cases where the model correctly applies an edit using a prediction prompt but omits it with the \u201cEdit\u201d prompt, which could indicate that the two tasks do not interact properly. It seems that the model struggles to predict a combination of connected or related corrections. For example, for the sentence \u201cThe people with albinism have sensitive skin and it needs regular treatment .\u201d, annotators gave the following edits: replace The people with People; replace it with this. But the model predicts the following: delete The; replace it with they. Thus, after correctly removing the it does not capitalize people, and while replacing it with they it does not replace needs with need.\nSince our model can be trained on both \u201cEdit\u201d and \u201cApply\u201d tasks, it is tempting to apply a chainof-thought procedure where we first predict the list of operations and then apply them to the original sentence. Applying such a procedure, we get the F0.5 score of 52.18 on the CoNLL-14 dataset (precision 55.85, recall 41.33), i.e., the most problematic metric is precision (see also Table 7). The main problem of this task is the ambiguity of the target sentence and edits needed to obtain it: edits can interact with each other, which is hard to account for given only the source sentence. This chain-of-thought approach appears unnecessarily challenging, while adding the target sentence in the \u201cExplain\u201d task makes the problem much easier and more helpful for pretraining.\nWe have also studied how tasks interact with each other. In the first experiment, we correct a sentence with the model and then ask it to explain the corrections. We compare the explanations with edits produced by the ERRANT tool, obtaining an exact match score of 95.9%, higher by 5.9% than for human-corrected sentences. This indicates some interaction between tasks as the model\u2019s corrections are more explainable by itself. Second, we\nchain three tasks: first the model corrects a sentence, then it explains the corrections, and finally it applies edits to the original sentence. Interestingly, the exact match between the corrected sentence on the first step and the sentence obtained after chaining is 97.02%, so the model is not always consistent across the tasks. Many errors come from the ambiguity of edits. It may be possible to use the discrepancy between corrected and generated sentences for filtering, either leaving the initial sentence intact or taking the intersection of the edits related to the two sentences. We find, however, that these approaches do not improve the final performance or even decrease it."
        },
        {
            "heading": "4.6 Automatic Human Evaluation",
            "text": "In this part, we compare the performance of our model with ChatGPT with chain-of-thoughts prompting, using the results by Fang et al. (2023) who showed that ChatGPT performs poorly on common benchmarks, far below state of the art finetuned models in the F0.5 metric, with high recall but very low precision. They show that the model often proposes corrections that are reasonable but far from minimal with respect to the number of required edits. To punish such over-corrections less, Bryant and Ng (2015) propose to use a test set where each erroneous sentence is annotated by several experts and averages the scores obtained in comparison to each annotator; this approach increases the scores for edits proposed by at least one annotator. Following Fang et al. (2023), we use the evaluation set from this paper consisting of CoNLL14 sentences with 10 human annotations each. We also compare with a human-level baseline which is measured as the average score of each human annotator with respect to others. Table 8 shows that even in this setup our model outperforms ChatGPT by a large margin; we compare with Fang et al. (2023) who use chain-of-thought prompting and report results in the zero-shot and few-shot (1, 3, and 5) settings. Interestingly, ChatGPT performance is slightly below the human level, while our model performs significantly better."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this work, we have introduced a new approach to training and fine-tuning sequence-to-sequence models for the grammatical error correction task based on multi-task pretraining and optimized training schedules. The proposed approach fares better\nthan previous state of the art models with a much smaller model size and outperforms models of comparable size by a wide margin on both CoNLL14 and BEA-test datasets; in particular, we have achieved results exceeding state of the art models based on T5-XXL (11B parameters) with a BARTbased model (400M parameters). Our multi-task approach encodes auxiliary tasks as text rewriting problems and does not require any changes in the model architecture. We believe that the proposed approach is of significant value for practical GEC pipelines and see several directions for possible improvements in further work, including modifications of text encodings for auxiliary tasks and adding new auxiliary tasks to the training process."
        },
        {
            "heading": "6 Limitations",
            "text": "Limitations of our study can serve as motivation for further work. First, we do not test our approach in the increasingly important multilingual setup; we believe that our ideas may contribute even more to multilingual models that need greater robustness. Second, the highest possible results in GEC are currently obtained by ensembles rather than individual models, including the recent state of the art approach by Qorib et al. (2022); while ensembling would make our reduced model size less important, it would be interesting to study how well our model can perform in such ensembles. Third, we consider only simple prompts and small models, and our results could be extended further."
        },
        {
            "heading": "Acknowledgements",
            "text": "The work of Sergey Nikolenko was done under support of the grant no. 075-15-2022-289 for the creation and development of Euler International Mathematical Institute."
        },
        {
            "heading": "A Error Type Distribution in Datasets",
            "text": "In order to analyze the difference between the GEC datasets, we compute the error type statistics using the ERRANT tool. Next, we compare each dataset with W&I+L-dev, see Figures 4-9. We see that W&I+L-train and W&I+L-dev have only a slight discrepancy in the proportions of each error type, with major differences coming from punctuation and spelling errors. Hence, we should expect that the distribution on W&I+L-test is close but not exactly the same as the articles that comprise the datasets were written by different people. Thus, extensive hyperparameter search on W&I+L-dev may lead to lower performance. The FCE dataset is also close to these datasets except for SPELL, NOUN, and VERB errors. These differences can be explained by the fact that essays were written by authors who had only begun learning English, and these error types are more common for them.\nComparing W&I+L-dev with NUCLE and Lang8 datasets, we note that for some error types the difference is striking. We highlight that the proportion of OTHER errors is high for both datasets. In the ERRANT toolkit, OTHER corresponds to errors that cannot be labeled by a predefined set of error types. After examining some instances with that error type, we have noticed that a large part of them is related to sentence rephrasing or rewriting, perhaps regrouping some parts of the sentence. Training on such examples might hurt the model\u2019s performance because it would becomes more aggressive in performing corrections, leading to much lower precision. Therefore, we mark those datasets as \u201cnoisy\u201d and others as \u201cclean\u201d.\nThe combination of all datasets\u2014FCE, W&I+Ltrain, Lang-8, and NUCLE\u2014does not improve the situation, as shown in Figure 9. Nevertheless, if we train only on FCE and W&I+L-train, we obtain poor performance. This indicates that not only the distribution of errors should be close but also the errors should be diverse in order to generalize well. These two factors reveal why we needed multi-stage fine-tuning.\nOur approach can be viewed as curriculum learning where we gradually train a model on \u201cless noisy\u201d data."
        },
        {
            "heading": "B Model comparison",
            "text": "Performance on the W&I+L-dev dataset of the 3- stage model and of the multi-task model with the improved training schedule seems to be marginal:\n62.62 versus 62.67. However, the models\u2019 behavior on W&I+L-dev differs. The 3-stage model has higher recall and the other has higher precision. As we have noted in the previous section, error distributions on parts of W&I+L datasets differ from each other since different people are prone to making different types of errors. Therefore, we expect that a model with higher precision that makes more accurate corrections would generalize better. Looking at the models\u2019 performance on W&I+L-test and CoNLL-14, we see that the gap between the models is indeed large.\nTo draw the distinction further, we compute the statistics of error types corrected and introduced by them with the ERRANT toolkit. Figure 10 presents the absolute number of corrected, generated, and non-corrected errors for every type. Again, we see that the 3-stage model is more aggressive. It corrects more errors of each type but also introduces more new errors. In Figures 11-13, we show the distribution of corrected, induced, and non-corrected errors by type. Again, we see that the models\u2019 behavior differs."
        }
    ],
    "title": "Efficient Grammatical Error Correction Via Multi-Task Training and Optimized Training Schedule",
    "year": 2023
}