{
    "abstractText": "Large, general purpose language models have demonstrated impressive performance across many different conversational domains. While multi-domain language models achieve low overall perplexity, their outputs are not guaranteed to stay within the domain of a given input prompt. This paper proposes domain privacy as a novel way to quantify how likely a conditional language model will leak across domains. We also develop policy functions based on token-level domain classification, and propose an efficient fine-tuning method to improve the trained model\u2019s domain privacy. Experiments on membership inference attacks show that our proposed method has comparable resiliency to methods adapted from recent literature on differentially private language models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Anmol Kabra"
        },
        {
            "affiliations": [],
            "name": "Ethan R. Elenberg"
        }
    ],
    "id": "SP:f5b0b367ec7d5336faddc20206a2a518f6a309e1",
    "references": [
        {
            "authors": [
                "Bosselut",
                "Emma Brunskill"
            ],
            "title": "On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258",
            "year": 2021
        },
        {
            "authors": [
                "Pawe\u0142 Budzianowski",
                "Ivan Vuli\u0107."
            ],
            "title": "Hello, it\u2019s gpt-2 \u2013 how can i help you? towards the use of pretrained language models for task-oriented dialogue systems",
            "venue": "EMNLP Workshop on Neural Generation and Translation.",
            "year": 2019
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Steve Chien",
                "Milad Nasr",
                "Shuang Song",
                "Andreas Terzis",
                "Florian Tramer."
            ],
            "title": "Membership inference attacks from first principles",
            "venue": "2022 IEEE Symposium on Security and Privacy (SP), pages 1897\u20131914. IEEE.",
            "year": 2022
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Florian Tram\u00e8r",
                "Eric Wallace",
                "Matthew Jagielski",
                "Ariel Herbert-Voss",
                "Katherine Lee",
                "Adam Roberts",
                "Tom Brown",
                "Dawn Song",
                "\u00dalfar Erlingsson",
                "Alina Oprea",
                "Colin Raffel."
            ],
            "title": "Extracting training data from large language models",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Cynthia Dwork",
                "Nitin Kohli",
                "Deirdre Mulligan"
            ],
            "title": "Differential privacy in practice: Expose your epsilons",
            "venue": "Journal of Privacy and Confidentiality,",
            "year": 2019
        },
        {
            "authors": [
                "Cynthia Dwork",
                "Aaron Roth."
            ],
            "title": "The algorithmic foundations of differential privacy",
            "venue": "Foundations and Trends\u00ae in Theoretical Computer Science, 9(3\u20134):211\u2013407.",
            "year": 2014
        },
        {
            "authors": [
                "Antonio Ginart",
                "Laurens van der Maaten",
                "James Zou",
                "Chuan Guo."
            ],
            "title": "Submix: Practical private prediction for large-scale language models",
            "venue": "arXiv preprint arXiv:2201.00971.",
            "year": 2021
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Mike Lewis",
                "Ari Holtzman",
                "Noah A. Smith",
                "Luke Zettlemoyer."
            ],
            "title": "Demix layers: Disentangling domains for modular language modeling",
            "venue": "NAACL, pages 2848\u20132859.",
            "year": 2022
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Ana Marasovi\u0107",
                "Swabha Swayamdipta",
                "Kyle Lo",
                "Iz Beltagy",
                "Doug Downey",
                "Noah A. Smith."
            ],
            "title": "Don\u2019t stop pretraining: Adapt language models to domains and tasks",
            "venue": "ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Ehsan Hosseini-Asl",
                "Bryan McCann",
                "Chien-Sheng Wu",
                "Semih Yavuz",
                "Richard Socher."
            ],
            "title": "A simple language model for task-oriented dialogue",
            "venue": "NeurIPS.",
            "year": 2020
        },
        {
            "authors": [
                "Jeremy Howard",
                "Sebastian Ruder."
            ],
            "title": "Universal language model fine-tuning for text classification",
            "venue": "ACL.",
            "year": 2018
        },
        {
            "authors": [
                "Shengyuan Hu",
                "Zhiwei Steven Wu",
                "Virginia Smith."
            ],
            "title": "Private multi-task learning: Formulation and applications to federated learning",
            "venue": "arXiv preprint arXiv:2108.12978.",
            "year": 2019
        },
        {
            "authors": [
                "Prateek Jain",
                "John Rush",
                "Adam Smith",
                "Shuang Song",
                "Abhradeep Guha Thakurta."
            ],
            "title": "Differentially private model personalization",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Xuechen Li",
                "Florian Tramer",
                "Percy Liang",
                "Tatsunori Hashimoto."
            ],
            "title": "Large language models can be strong differentially private learners",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Zhaojiang Lin",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Exploring versatile generative language model via parameter-efficient transfer learning",
            "venue": "Findings of EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Mironov."
            ],
            "title": "R\u00e9nyi differential privacy",
            "venue": "2017 IEEE 30th computer security foundations symposium (CSF), pages 263\u2013275. IEEE.",
            "year": 2017
        },
        {
            "authors": [
                "Haojie Pan",
                "Chengyu Wang",
                "Minghui Qiu",
                "Yichang Zhang",
                "Yaliang Li",
                "Jun Huang."
            ],
            "title": "Meta-kd: A meta knowledge distillation framework for language model compression across domains",
            "venue": "ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Denis Peskov",
                "Nancy Clarke",
                "Jason Krone",
                "Brigi Fodor",
                "Yi Zhang",
                "Adel Youssef",
                "Mona Diab."
            ],
            "title": "Multi-domain goal-oriented dialogues (MultiDoGO): Strategies toward curating and annotating large scale dialogue data",
            "venue": "Proceedings of the 2019 Confer-",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Weiyan Shi",
                "Si Chen",
                "Chiyuan Zhang",
                "Ruoxi Jia",
                "Zhou Yu."
            ],
            "title": "Just fine-tune twice: Selective differential privacy for large language models",
            "venue": "arXiv preprint arXiv:2204.07667.",
            "year": 2022
        },
        {
            "authors": [
                "Weiyan Shi",
                "Aiqi Cui",
                "Evan Li",
                "Ruoxi Jia",
                "Zhou Yu."
            ],
            "title": "Selective differential privacy for langauage modeling",
            "venue": "NAACL, pages 2848\u20132859.",
            "year": 2022
        },
        {
            "authors": [
                "Reza Shokri",
                "Marco Stronati",
                "Congzheng Song",
                "Vitaly Shmatikov."
            ],
            "title": "Membership inference attacks against machine learning models",
            "venue": "2017 IEEE symposium on security and privacy (SP), pages 3\u201318. IEEE.",
            "year": 2017
        },
        {
            "authors": [
                "Yunyi Yang",
                "Yunhao Li",
                "Xiaojun Quan."
            ],
            "title": "Ubar: Towards fully end-to-end task-oriented dialog systems with gpt-2",
            "venue": "AAAI.",
            "year": 2021
        },
        {
            "authors": [
                "Yunzhi Yao",
                "Shaohan Huang",
                "Wenhui Wang",
                "Li Dong",
                "Furu Wei."
            ],
            "title": "Adapt-and-distill: Developing small, fast and effective pretrained language models for domains",
            "venue": "Findings of ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Da Yu",
                "Saurabh Naik",
                "Arturs Backurs",
                "Sivakanth Gopi",
                "Huseyin A Inan",
                "Gautam Kamath",
                "Janardhan Kulkarni",
                "Yin Tat Lee",
                "Andre Manoel",
                "Lukas Wutschitz"
            ],
            "title": "Differentially private fine-tuning of language models",
            "year": 2022
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Felix Wu",
                "Arzoo Katiyar",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Revisiting few-sample bert fine-tuning",
            "venue": "ICLR.",
            "year": 2021
        },
        {
            "authors": [
                "quence",
                "Shi"
            ],
            "title": "2022b) introduce Selective Dif",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models have enabled significant progress in machine learning and NLP across a wide range of tasks and domains (Bommasani et al., 2021). They perform especially well in settings where little training data is available for new domains of interest. A popular approach in such settings is transfer learning: fine-tune a pretrained model on data from specialized domains (Howard and Ruder, 2018; Zhang et al., 2021; Yang et al., 2021; Budzianowski and Vulic\u0301, 2019; HosseiniAsl et al., 2020). Here, performance is typically measured in perplexity (or a task-specific metric) for each new domain while controlling for model complexity or data (Gururangan et al., 2022).\nWe introduce a novel definition of privacy for contextual language models to enforce that text prompts from one domain do not leak sensitive text of other domains. Practitioners train generative models on datasets often curated from diverse domains, e.g. news article categories or dialog tasks. Model users are often then interested in safe\n\u2217Work done at ASAPP Inc\ngeneration: models when prompted with text from one domain must not generate sensitive text from other domains. Safe generation is a key requirement for model providers who pool datasets from many contracted companies\u2014each company might require the model to not generate their sensitive text when prompted with text from other companies. We call such safe generation domain privacy. Let {d1, . . . , dN} be domains from which datasets are created. Let MD be a model trained on text dataset D. To verify if MD is domain private for domain di, we can prompt the model with contexts {ci} from domain di, and check if the generations contain sensitive text of domains dj for j \u0338= i.\nOur contributions are: we 1) define domain privacy as a new property for contextual language models, 2) propose fine-tuning algorithms that trade off domain privacy for model performance, and 3) conduct extensive experiments for text generation with multi-domain datasets. Domain privacy scales well with the number of domains, and allows for flexible definitions of domain-sensitive text. Our proposed fine-tuning algorithms utilize differentially-private training to attain domain privacy, while achieving good performance."
        },
        {
            "heading": "2 Related Work",
            "text": "Domain Adaptation Large pretrained language models have been shown to achieve good performance when fine-tuned on small datasets from new domains (Gururangan et al., 2020). To improve efficiency, recent multi-domain approaches leverage multitask learning (Lin et al., 2020), model distillation (Yao et al., 2021), and/or meta-learning (Pan et al., 2021). Hu et al. (2019) propose private metalearning for discriminative tasks; our work is the first for private multi-domain text generation.\nDifferentially Private Language Models Differential privacy is a powerful framework that provides rigorous guarantees on training data expo-\nsure to adversaries (Dwork and Roth, 2014). Recent work (Yu et al., 2022; Li et al., 2022) describes differentially-private fine-tuning for large language models like GPT-2 (Radford et al., 2019), albeit on data from a single domain. However, standard notions of differential privacy, including those for single-domain language models (Ginart et al., 2021; Shi et al., 2022b,a), are insufficient for multi-domain language modeling. Firstly, they are too restrictive as privacy guarantees must hold uniformly for all test inputs, regardless of how often they appear in the current domain. Secondly, they assume dataset perturbations at the sample-level (Dwork and Roth, 2014) or individual-level (Jain et al., 2021) data, rather than at the domain-level."
        },
        {
            "heading": "3 Preliminaries",
            "text": "We recall a few definitions first. See Appendix A for further details.\nLanguage modeling Given a text sequence of tokens \u03c4i = (t1, . . . , ti), an autoregressive language model estimates next token probability Pr[ti+1|\u03c4i]. The model is trained by minimizing cross-entropy between next ground-truth token and model\u2019s predictions. Finally, we can use the model to compute the perplexity (PPL) of a sequence \u03c4n.\nPrivacy An algorithm is differentially private if it is not too sensitive to the differentiating element in two neighboring inputs. In language modeling where the element is a text sequence, users often want to only control sensitivity on sequence\u2019s private tokens, e.g. phone numbers and proper nouns. Shi et al. (2022b) thus define Selective Differential Privacy using policy functions.\nA policy function F annotates a sequence \u03c4n with 0-1 labels; F (\u03c4n)i = 1 if the ith token is private and 0 if public. F then defines neighboring text sequence datasets: D\u2032 is called an F -neighbor of D, i.e. D\u2032 \u2208 NF (D), if they differ in exactly one text sequence on which F does not agree.\nDefinition 3.1 (Selective Differential Privacy) Given a policy function F , training algorithm A with range M is (F, \u03f5, \u03b4)-selective differential private if for all D \u2208 D, D\u2032 \u2208 NF (D), M \u2286 M,\nPr[A(D) \u2208 M ] \u2264 e\u03f5 \u00b7 Pr[A(D\u2032) \u2208 M ] + \u03b4.\nMembership Inference Attacks Differential privacy gives theoretical guarantees which may not be\napplicable in practice (Dwork et al., 2019). Empirically, we can verify models\u2019 privacy using membership inference attacks that check for training data leakage (Shokri et al., 2017). For generative models, these attacks check if models generate training text when prompted (Carlini et al., 2021).\nWe can measure the attacks\u2019 success rate and empirically compare the privacy of generative models. Likelihood Ratio (LiRa) membership inference attacks compare target models relative to a reference model (Carlini et al., 2022, 2021). LiRa attacks work as follows: (i) prompt a target model with contexts {ci} to generate text {xi}, (ii) rank {xi} by generation likelihood PPLtarget(xi|ci)/PPLref (xi|ci), and (iii) select xi with the highest ratios. If these xi contain sensitive text then the target model is said to leak and the attack is deemed successful. Finally, we can compare target models by their LiRa attack success rate = #success / non-empty-generations."
        },
        {
            "heading": "4 Domain Privacy",
            "text": "Consider two domains di and dj where i \u0338= j. The goal of domain privacy is to check how likely a model is to generate sensitive text from domain dj when prompted with text from domain di. To check if text contains private tokens of domain dj , we can use a policy function Fj . Since domains di and dj could have inherent overlap, e.g. politics and sports news overlapping due to geopolitics, we will use MDj as a reference model where Dj = D \\ dj is the dataset obtained by removing text of domain dj from D. The likelihood of MDj leaking sensitive text of dj serves as an upper bound for the target model leakage. Here D and Dj are neighbors at domain level w.r.t. Fj as they differ in one domain.\nDefinition 4.1 (Domain Privacy) Let C > 0 be a parameter. A model MD is C-domain-private for D, if for all i, j \u2208 [N ] where j \u0338= i, contexts {ci} from domain di,\nPr[MD(ci) \u2208 dj ] \u2264 C \u00b7 Pr [ MDj (ci) \u2208 dj ] .\nDomain privacy captures the need for safe generation: inter-domain private generation and intradomain public generation. It extends Selective Differential Privacy in three ways. Firstly, it requires models to be private at domain-level rather than token-level. Secondly, it allows models to generate sensitive text of di when prompted with {ci}\u2014only leaking text of other domains is restricted. Finally,\ndomain privacy uses LiRa membership inference attacks; Selective Differential Privacy lacks this. Hence, domain privacy can be empirically tested."
        },
        {
            "heading": "5 Methodology",
            "text": "Next we study domain privacy applied to the problem of generating dialog text."
        },
        {
            "heading": "5.1 Policy Functions",
            "text": "A policy function flags text considered sensitive for a domain, enabling us to check for domain privacy. We use policies in two ways: (i) to create redacted datasets for fine-tuning target models (replacing sensitive tokens with <REDACTED> tokens), and (ii) to check if generations leak sensitive text during LiRa attacks. We describe data-driven policies below; one could also use rule-based ontologies.\nThe Keyword Detection policy checks if any tokens in text \u03c4 are in a set of hand-picked keyword tokens Ki sensitive to domain di. Formally, F keywordi (\u03c4) = 1 if there exists token t \u2208 \u03c4 with t \u2208 Ki. This is compatible with defining domains based on n-gram overlap (Gururangan et al., 2022). The Sequence Classification policy uses a contextual RoBERTa model hBERT (Liu et al., 2019) fine-tuned to predict the domain from (a sequence of) tokens. We use a specified threshold z to set FBERTi (\u03c4) = 1 if there exists token sequence t\u2217 \u2286 \u03c4 such that Pr[hBERT (t\u2217) = di] > z."
        },
        {
            "heading": "5.2 Target Models",
            "text": "There has been much work to protect against membership inference attacks. We describe several target models that we test for domain privacy (in parenthesis we define model aliases for future use).\nLet D be the dataset and di be the domain being tested. As a baseline target, we use a model finetuned only on text from D \u2229 di (DOMAINi Only). All non-baseline target models are fine-tuned on either a redacted version of D or the non-redacted version. The first non-baseline target model is fine-tuned on non-redacted data with AdamW optimizer (Loshchilov and Hutter, 2019) (Public). The second is fine-tuned on redacted data instead (Pub+Redacted). Li et al. (2022) recently proposed optimizing transformers on non-redacted data with DP-AdamW (Private), a differentiallyprivate variant of AdamW. Shi et al. (2022a) optimize for Selective Differential Privacy with a \u201cJust Fine-tune Twice\u201d (JFT) procedure: fine-tune a model with AdamW on redacted data and use the\nweights to initialize a model, which is then finetuned with DP-AdamW on non-redacted data. Shi et al. (2022a) show that the model adapts to the linguistic style without generating sensitive tokens.\nWe adapt this two-stage process into a one-stage one: initially fine-tune on redacted data and gradually transition to non-redacted data (Redaction Schedule). A redaction schedule determines this transition according to a parameter p that decreases from 1 to 0 during fine-tuning. At every step during fine-tuning, with probability p we fine-tune with AdamW on redacted data, and with probability 1\u2212p we fine-tune with DP-AdamW on non-redacted data. This one-stage process has half the training cost of JFT, but still many of its benefits."
        },
        {
            "heading": "6 Experiments",
            "text": ""
        },
        {
            "heading": "6.1 Datasets",
            "text": "We use the MultiDoGo dataset (Peskov et al., 2019), which consists of task-oriented dialogs of useragent customer service simulation from 6 domains. We use the 3 largest domains: AIRLINE (air travel bookings), MEDIA (telecommunication and cable), and INSURANCE (policy modifications). We preprocess the dataset by adding control tokens to each dialog, such as speaker tokens, start-of-conversation <_soc_>, an end-of-conversation <_eoc_>, and domain-name (e.g. <AIRLINE>). Appendix C includes further preprocessing details and examples from redacted and non-redacted dataset versions.1"
        },
        {
            "heading": "6.2 Training target models",
            "text": "We create 60-20-20 train-validation-test splits for each domain, and coalesce similar splits. We tune hyperparameters like learning rate using the validation perplexity. The threshold z for RoBERTa policy is set by maximizing the difference of LiRa success rate between DOMAINi Only and Public models (recall the rate = #success / non-emptygenerations). To get the target models in Section 5.2, we fine-tune a pretrained GPT-2 checkpoint on data from all 3 domains. For the proposed Redaction Schedule fine-tuning procedure, we use the \u201cexpconcave\u201d schedule (see Appendix D)."
        },
        {
            "heading": "6.3 LiRa Attacks for MultiDoGo dataset",
            "text": "We conduct LiRa attacks on each target model to test for domain privacy\u2014we check if a model leaks sensitive text of domain dj when prompted with\n1Code is available at https://github.com/ asappresearch/domain-private-transformers\ncontexts from domain di, i \u0338= j. Here we focus on i = AIRLINE. Into each model, we feed 100 prompts from the AIRLINE domain and generate 10 different outputs for each prompt. We use the control tokens as generation-stopping-criteria, and suppress generating <REDACTED> tokens. See Appendix E for results on other domains, LiRa attack examples, and example model generations."
        },
        {
            "heading": "6.4 Results",
            "text": ""
        },
        {
            "heading": "AIRLINE, Keyword Redaction",
            "text": ""
        },
        {
            "heading": "AIRLINE, RoBERTA Redaction",
            "text": "We compare target models on LiRa success rate and test perplexity metrics. Figure 1 shows these two metrics for each target model. LiRa attacks are more successful w.r.t. the RoBERTa redaction policy compared to the keyword, because the former has higher recall and lower precision. Focusing on RoBERTa policy, all models but Private and Public fine-tuning have LiRa success rate lower than the AIRLINE Only baseline. While having comparable domain privacy, JFT has better perplexity and Redaction Schedule has worse perplexity when compared to Pub+Redacted. Domain leakage is generally more sensitive to learning rate for JFT, while perplexity is more sensitive to learning rate\nfor Redaction Schedule. We also test running each stage of JFT for half the number of steps, i.e. with total compute comparable to other models."
        },
        {
            "heading": "AIRLINE, Keyword Redaction",
            "text": ""
        },
        {
            "heading": "AIRLINE, RoBERTA Redaction",
            "text": "Figure 2 shows Renyi DP guarantee2 vs. test perplexity for each model. Public has no privacy guarantee (\u03f5 = \u221e), and (Pub+Redacted) has an ideal guarantee of \u03f5 = 0 as it is only finetuned on redacted data. We further see that for both keyword and RoBERTa redaction policies, Redaction Schedule models have privacy guarantees \u2248 35% better than JFT. We observe that vanilla fine-tuning like Public is insufficient for domain privacy. Domain privacy becomes feasible with fine-tuning algorithms designed for Selective Differential Privacy; these algorithms fine-tune partially on redacted datasets built with policies."
        },
        {
            "heading": "7 Conclusions",
            "text": "This paper compares multi-domain language models for dialog data on a new concept of domain privacy. We propose two policies for redacting domain-sensitive tokens, enabling recent\n2Renyi DP is commonly used to evaluate differential privacy of gradient-descent-based optimizers (see Appendix A).\ndifferentially-private training algorithms to be used for preserving domain privacy. Future research directions include studying the domain privacy properties of additional training strategies, and understanding the interplay between domain privacy and performance on downstream tasks."
        },
        {
            "heading": "8 Limitations",
            "text": "Sequence classification policies are more susceptible to data bias and systemic uncertainty than rule-based policies that are based on keywords or parts of speech. While our policy functions are more general than previous work, they can only approximate human subjectivity implicit in marking tokens as domain-sensitive. Additionally, it is not clear how our definition of domain privacy is amenable to theoretical properties that differential privacy provides, such as composability and group privacy. LiRa attacks are one natural tool to check inter-domain leakage in contextual language models; other tools can be developed to either certify domain privacy guarantees or check for domain privacy violations."
        },
        {
            "heading": "9 Ethics/Impact",
            "text": "Models that are not domain private pose a security risk in deployment due to inter-domain leakage. We show that the predominant transfer learning approach, which fine-tunes a single pretrained model on data from several new domains, is risky from a leakage standpoint. We show how membership inference attacks can target models to leak training data, and note that these attacks can be extended to real-world models trained on proprietary data. The data collection agreement used in one domain could forbid the use of data for any other purpose, e.g. generation for any other domain. While this was not an ethical concern for the data used in this paper, it remains an open area of discussion for the ML community."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank Ryan McDonald, Kilian Q. Weinberger, and the rest of the ASAPP Research team for their helpful discussions."
        },
        {
            "heading": "A Further Definitions",
            "text": "Language Modeling The perplexity (PPL) of a text sequence \u03c4n (w.r.t. an autoregressive language model) is defined as:\nPPL(\u03c4n) = exp ( \u2212 1 n n\u2211 i=1 log(Pr[ti+1|\u03c4i]) ) .\nPrivacy Let A : X \u2192 Y be a randomized algorithm. Two input sets X,X \u2032 \u2286 X are neighbors if they differ in exactly one element. Dwork and Roth (2014) define Differential Privacy for A as follows. Definition A.1 (Differential Privacy) Algorithm A : X \u2192 Y is (\u03f5, \u03b4)-differentially private if for all outputs Y \u2286 Y , neighboring sets X,X \u2032 \u2286 X ,\nPr[A(X) \u2208 Y ] \u2264 e\u03f5 \u00b7 Pr[A(X \u2032) \u2208 Y ] + \u03b4.\nTo hone in on the private tokens of a text sequence, Shi et al. (2022b) introduce Selective Differential Privacy, which uses policy functions to define neighboring datasets. Definition A.2 (Policy Function) A policy function F : T \u2192 {0, 1}n annotates tokens in a sequence \u03c4n \u2208 T as private or not. F (\u03c4n)i = 1 if the ith token is private and 0 if public.\nThus, two text sequence datasets D,D\u2032 are F - neighbors if they differ in only one text sequence on which F \u2019s annotations do not match.\nMironov (2017) show interchangeability between Renyi differential privacy and differential privacy, i.e. an algorithm satisfying (\u03b1, \u03f5)-Renyi differential privacy satisfies (\u03f5\u03b4, \u03b4)-differential privacy for any \u03b4 \u2208 (0, 1), and vice versa. Renyi differential privacy is defined as follows."
        },
        {
            "heading": "Definition A.3 (Renyi Differential Privacy)",
            "text": "Algorithm A : X \u2192 Y is said to have \u03f5-Renyi differential private of order \u03b1, if for all neighboring sets X,X \u2032 \u2286 X ,\nD\u03b1(A(X) || A(X \u2032)) \u2264 \u03f5\nwhere D\u03b1(P || Q) is the Renyi divergence (of order \u03b1 > 1) of two distributions P,Q over Y , defined as D\u03b1(P || Q) = 1\u03b1\u22121 logEy\u223cQ ( P (y) Q(y) )\u03b1 ."
        },
        {
            "heading": "B Computation",
            "text": "All language models were fine-tuned from a public GPT-2 small checkpoint with 124M parameters (Radford et al., 2019). Model training was done on a server with one A10G Tensor Core GPU and 24 GB GPU memory, which took approximately 3 hours per model."
        },
        {
            "heading": "C Data Preprocessing and Experimental setup",
            "text": "As mentioned earlier, we use dialogs from AIRLINE, MEDIA, and INSURANCE domains from the MultiDoGo dataset. These domains have \u224815k, \u224833k, and \u224814k dialogs respectively.\nWe preprocess dialog samples as follows. Consider a sample \u201cSYS: Hello, you are connected to LMT Airways! How may I help you? USR: Change my seat assignment SYS: . . . \u201d. We preprocess this dialog sample by adding start-of-conversation control token <_soc_>, end-of-conversation control token <_eoc_>, and domain-name control token <AIRLINE> before every utterance. A dialog then looks like \u201c<_soc_> <AIRLINE> SYS: Hello, you are connected to LMT Airways! How may I help you? <AIRLINE> USR: Change my seat assignment <AIRLINE> SYS: . . . <_eoc_>\u201d. For a dialog sample from MEDIA domain, we similarly add <MEDIA> control tokens.\nWe also create another set of datasets where we do not add the control domain tokens, and follow the same fine-tuning and LiRa attack procedure on these datasets. See Section E.3 for results on this ablation experiment.\nFinally, we concatenate all dialogs for a domain and chunk them by 1024 tokens, the maximum sequence length used during GPT-2 pretraining."
        },
        {
            "heading": "C.1 Redaction Policies",
            "text": "Table 1 shows example dialog turns for each dialog domain and redaction policy."
        },
        {
            "heading": "D Redaction Schedules",
            "text": "We experimented with the redaction schedules described in Figure 3. The two-stage process JFT fine-tunes on redacted data with AdamW optimizer, and then switches to non-redacted data with DPAdamW optimizer (Shi et al., 2022a). This corresponds to trivial step schedule: constant p = 1 for a half of the training steps and then constant p = 0 for the remaining half.\nThe linear redaction schedule is one approach that transitions smoothly between redacted and nonredacted data. The expconvex schedule decays exponentially fast, and is a convex function\u2014it transitions to non-redacted data after just a few training steps. We found that expconcave schedule outperformed the other schedules as it decayed exponentially slowly, causing the trainer to use redacted data for most of the initial training steps. This is in line with Shi et al. (2022a)\u2019s observation that fine-tuning on the new domain with a non-noisy optimizer like AdamW results in benign initialization. Our expconcave redaction schedule implements this idea in a one-stage fine-tuning process."
        },
        {
            "heading": "E Additional Results",
            "text": ""
        },
        {
            "heading": "E.1 LiRa Attack Outputs",
            "text": "Tables 2 and 3 show the results of LiRa membership inference attacks on the models in Section 6.4."
        },
        {
            "heading": "E.2 Additional Domains",
            "text": "Figures 4 through 7 show the results of domain leakage experiments when using prompts from the MEDIA and INSURANCE domains."
        },
        {
            "heading": "E.3 Use of Domain Tokens",
            "text": "Figures 8 and 9 show domain privacy tradeoffs when domain control tokens (<AIRLINE> etc.) are removed from all datasets and policy functions. As a general trend we get similar, if not slightly higher, LiRa\nsuccess rates when the domain control tokens are not present in the datasets."
        },
        {
            "heading": "E.4 Language Model Outputs",
            "text": ""
        }
    ],
    "title": "Domain Private Transformers for Multi-Domain Dialog Systems",
    "year": 2023
}