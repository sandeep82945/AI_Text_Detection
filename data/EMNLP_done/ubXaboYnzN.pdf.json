{
    "abstractText": "People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users\u2019 information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary. We introduce a new benchmark named QTSUMM for this task, which contains 7,111 human-annotated query-summary pairs over 2,934 tables covering diverse topics. We investigate a set of strong baselines on QTSUMM, including text generation, table-to-text generation, and large language models. Experimental results and manual analysis reveal that the new task presents significant challenges in table-totext generation for future research. Moreover, we propose a new approach named REFACTOR, to retrieve and reason over query-relevant information from tabular data to generate several natural language facts. Experimental results demonstrate that REFACTOR can bring improvements to baselines by concatenating the generated facts to the model input. Our data and code are publicly available at https: //github.com/yale-nlp/QTSumm.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yilun Zhao"
        },
        {
            "affiliations": [],
            "name": "Zhenting Qi"
        },
        {
            "affiliations": [],
            "name": "Linyong Nan"
        },
        {
            "affiliations": [],
            "name": "Boyu Mi"
        },
        {
            "affiliations": [],
            "name": "Yixin Liu"
        },
        {
            "affiliations": [],
            "name": "Weijin Zou"
        },
        {
            "affiliations": [],
            "name": "Simeng Han"
        },
        {
            "affiliations": [],
            "name": "Ruizhe Chen"
        },
        {
            "affiliations": [],
            "name": "Xiangru Tang"
        },
        {
            "affiliations": [],
            "name": "Yumo Xu"
        },
        {
            "affiliations": [],
            "name": "Dragomir Radev"
        },
        {
            "affiliations": [],
            "name": "Arman Cohan"
        }
    ],
    "id": "SP:6ced3eba34f64da1ff70ada406443ea2687da408",
    "references": [
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
            "venue": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-",
            "year": 2005
        },
        {
            "authors": [
                "Wenhu Chen",
                "Jianshu Chen",
                "Yu Su",
                "Zhiyu Chen",
                "William Yang Wang."
            ],
            "title": "Logical natural language generation from open-domain tables",
            "venue": "Pro8https://opensource.org/licenses/MIT 9https://creativecommons.org/licenses/",
            "year": 2020
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hongmin Wang",
                "Jianshu Chen",
                "Yunkai Zhang",
                "Hong Wang",
                "Shiyang Li",
                "Xiyou Zhou",
                "William Yang Wang."
            ],
            "title": "Tabfact : A large-scale dataset for table-based fact verification",
            "venue": "International Conference on Learning Representations",
            "year": 2020
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hanwen Zha",
                "Zhiyu Chen",
                "Wenhan Xiong",
                "Hong Wang",
                "William Wang."
            ],
            "title": "Hybridqa: A dataset of multi-hop question answering over tabular and textual data",
            "venue": "Findings of EMNLP 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Zhoujun Cheng",
                "Haoyu Dong",
                "Ran Jia",
                "Pengfei Wu",
                "Shi Han",
                "Fan Cheng",
                "Dongmei Zhang."
            ],
            "title": "FORTAP: Using formulas for numerical-reasoningaware table pretraining",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Zhoujun Cheng",
                "Haoyu Dong",
                "Zhiruo Wang",
                "Ran Jia",
                "Jiaqi Guo",
                "Yan Gao",
                "Shi Han",
                "Jian-Guang Lou",
                "Dongmei Zhang."
            ],
            "title": "HiTab: A hierarchical table dataset for question answering and natural language generation",
            "venue": "Proceedings of the 60th Annual Meet-",
            "year": 2022
        },
        {
            "authors": [
                "Hoa Trang Dang."
            ],
            "title": "DUC 2005: Evaluation of question-focused summarization systems",
            "venue": "Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 48\u201355, Sydney, Australia. Association for Computational Lin-",
            "year": 2006
        },
        {
            "authors": [
                "Haoyu Dong",
                "Zhoujun Cheng",
                "Xinyi He",
                "Mengyu Zhou",
                "Anda Zhou",
                "Fan Zhou",
                "Ao Liu",
                "Shi Han",
                "Dongmei Zhang."
            ],
            "title": "Table pre-training: A survey on model architectures, pre-training objectives, and downstream tasks",
            "venue": "Proceedings of the Thirty-First",
            "year": 2022
        },
        {
            "authors": [
                "Zhibin Gou",
                "Zhihong Shao",
                "Yeyun Gong",
                "Yelong Shen",
                "Yujiu Yang",
                "Nan Duan",
                "Weizhu Chen"
            ],
            "title": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Herzig",
                "Pawel Krzysztof Nowak",
                "Thomas M\u00fcller",
                "Francesco Piccinno",
                "Julian Eisenschlos."
            ],
            "title": "TaPas: Weakly supervised table parsing via pre-training",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Matthew F. Hurst"
            ],
            "title": "The interpretation of tables in texts",
            "year": 2000
        },
        {
            "authors": [
                "Mohit Iyyer",
                "Wen-tau Yih",
                "Ming-Wei Chang."
            ],
            "title": "Search-based neural structured learning for sequential question answering",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1821\u2013",
            "year": 2017
        },
        {
            "authors": [
                "Alon Jacovi",
                "Avi Caciularu",
                "Omer Goldman",
                "Yoav Goldberg"
            ],
            "title": "Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks",
            "year": 2023
        },
        {
            "authors": [
                "Dongfu Jiang",
                "Yishan Li",
                "Ge Zhang",
                "Wenhao Huang",
                "Bill Yuchen Lin",
                "Wenhu Chen"
            ],
            "title": "2023b. Tigerscore: Towards building explainable metric for all text generation",
            "year": 2023
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Yi Mao",
                "Pengcheng He",
                "Graham Neubig",
                "Weizhu Chen."
            ],
            "title": "OmniTab: Pretraining with natural and synthetic data for few-shot tablebased question answering",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of",
            "year": 2022
        },
        {
            "authors": [
                "Karen Kukich."
            ],
            "title": "Design of a knowledge-based report generator",
            "venue": "21st Annual Meeting of the Association for Computational Linguistics, pages 145\u2013 150, Cambridge, Massachusetts, USA. Association for Computational Linguistics.",
            "year": 1983
        },
        {
            "authors": [
                "Woosuk Kwon",
                "Zhuohan Li",
                "Siyuan Zhuang",
                "Ying Sheng",
                "Lianmin Zheng",
                "Cody Hao Yu",
                "Joseph E. Gonzalez",
                "Hao Zhang",
                "Ion Stoica."
            ],
            "title": "Efficient memory management for large language model serving with pagedattention",
            "venue": "Proceedings of the",
            "year": 2023
        },
        {
            "authors": [
                "R\u00e9mi Lebret",
                "David Grangier",
                "Michael Auli."
            ],
            "title": "Neural text generation from structured data with application to the biography domain",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203\u20131213, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin",
                "Eduard Hovy."
            ],
            "title": "Automatic evaluation of summaries using n-gram co-occurrence statistics",
            "venue": "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2003
        },
        {
            "authors": [
                "Ao Liu",
                "Haoyu Dong",
                "Naoaki Okazaki",
                "Shi Han",
                "Dongmei Zhang."
            ],
            "title": "PLOG: Table-to-logic pretraining for logical table-to-text generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5531\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Qian Liu",
                "Bei Chen",
                "Jiaqi Guo",
                "Morteza Ziyadi",
                "Zeqi Lin",
                "Weizhu Chen",
                "Jian-Guang Lou."
            ],
            "title": "TAPEX: Table pre-training via learning a neural SQL executor",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Yixin Liu",
                "Alexander R. Fabbri",
                "Pengfei Liu",
                "Yilun Zhao",
                "Linyong Nan",
                "Ruilin Han",
                "Simeng Han",
                "Shafiq Joty",
                "Chien-Sheng Wu",
                "Caiming Xiong",
                "Dragomir Radev"
            ],
            "title": "Revisiting the gold standard: Grounding summarization evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Yixin Liu",
                "Alexander R. Fabbri",
                "Yilun Zhao",
                "Pengfei Liu",
                "Shafiq Joty",
                "Chien-Sheng Wu",
                "Caiming Xiong",
                "Dragomir Radev"
            ],
            "title": "Towards interpretable and efficient automatic reference-based summarization evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Pan Lu",
                "Baolin Peng",
                "Hao Cheng",
                "Michel Galley",
                "KaiWei Chang",
                "Ying Nian Wu",
                "Song-Chun Zhu",
                "Jianfeng Gao"
            ],
            "title": "Chameleon: Plug-and-play compositional reasoning with large language models",
            "year": 2023
        },
        {
            "authors": [
                "Nafise Sadat Moosavi",
                "Andreas R\u00fcckl\u00e9",
                "Dan Roth",
                "Iryna Gurevych."
            ],
            "title": "Scigen: a dataset for reasoning-aware text generation from scientific tables",
            "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks",
            "year": 2021
        },
        {
            "authors": [
                "Xiong",
                "Dragomir Radev",
                "Dragomir Radev."
            ],
            "title": "FeTaQA: Free-form table question answering",
            "venue": "Transactions of the Association for Computational Linguistics, 10:35\u201349.",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "ArXiv, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Bhargavi Paranjape",
                "Scott Lundberg",
                "Sameer Singh",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer",
                "Marco Tulio Ribeiro"
            ],
            "title": "Art: Automatic multistep reasoning and tool-use for large language models",
            "year": 2023
        },
        {
            "authors": [
                "Ankur Parikh",
                "Xuezhi Wang",
                "Sebastian Gehrmann",
                "Manaal Faruqui",
                "Bhuwan Dhingra",
                "Diyi Yang",
                "Dipanjan Das."
            ],
            "title": "ToTTo: A controlled table-to-text generation dataset",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Panupong Pasupat",
                "Percy Liang."
            ],
            "title": "Compositional semantic parsing on semi-structured tables",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language",
            "year": 2015
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013 191, Belgium, Brussels. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Jay Pujara",
                "Pedro Szekely",
                "Huan Sun",
                "Muhao Chen."
            ],
            "title": "From tables to knowledge: Recent advances in table understanding",
            "venue": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, KDD \u201921, page 4060\u20134061, New York,",
            "year": 2021
        },
        {
            "authors": [
                "Shuofei Qiao",
                "Honghao Gui",
                "Huajun Chen",
                "Ningyu Zhang"
            ],
            "title": "Making language models better tool learners with execution feedback",
            "year": 2023
        },
        {
            "authors": [
                "Taylor",
                "Adina Williams",
                "Jian Xiang Kuan",
                "Puxin Xu",
                "Zhengxu Yan",
                "Iliyan Zarov",
                "Yuchen Zhang",
                "Angela Fan",
                "Melanie Kambadur",
                "Sharan Narang",
                "Aurelien Rodriguez",
                "Robert Stojnic",
                "Sergey Edunov",
                "Thomas Scialom"
            ],
            "title": "Llama 2: Open foundation",
            "year": 2023
        },
        {
            "authors": [
                "ing Xiong",
                "Lingpeng Kong",
                "Rui Zhang",
                "Noah A. Smith",
                "Luke Zettlemoyer",
                "Tao Yu"
            ],
            "title": "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models",
            "year": 2022
        },
        {
            "authors": [
                "Yiheng Xu",
                "Hongjin Su",
                "Chen Xing",
                "Boyu Mi",
                "Qian Liu",
                "Weijia Shi",
                "Binyuan Hui",
                "Fan Zhou",
                "Yitao Liu",
                "Tianbao Xie",
                "Zhoujun Cheng",
                "Siheng Zhao",
                "Lingpeng Kong",
                "Bailin Wang",
                "Caiming Xiong",
                "Tao Yu"
            ],
            "title": "Lemur: Harmonizing natural language",
            "year": 2023
        },
        {
            "authors": [
                "Yumo Xu",
                "Mirella Lapata."
            ],
            "title": "Coarse-to-fine query focused multi-document summarization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3632\u20133645, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Yumo Xu",
                "Mirella Lapata."
            ],
            "title": "Document summarization with latent queries",
            "venue": "Transactions of the Association for Computational Linguistics, 10:623\u2013 638.",
            "year": 2022
        },
        {
            "authors": [
                "Shiyue Zhang",
                "Mohit Bansal."
            ],
            "title": "Finding a balanced degree of automation for summary evaluation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6617\u20136632, Online and Punta Cana, Dominican Re-",
            "year": 2021
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q. Weinberger",
                "Yoav Artzi."
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Yilun Zhao",
                "Yunxiang Li",
                "Chenying Li",
                "Rui Zhang."
            ],
            "title": "MultiHiertt: Numerical reasoning over multi hierarchical tabular and textual data",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Yilun Zhao",
                "Boyu Mi",
                "Zhenting Qi",
                "Linyong Nan",
                "Minghao Guo",
                "Arman Cohan",
                "Dragomir Radev."
            ],
            "title": "OpenRT: An open-source framework for reasoning over tabular data",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Yilun Zhao",
                "Linyong Nan",
                "Zhenting Qi",
                "Rui Zhang",
                "Dragomir Radev."
            ],
            "title": "ReasTAP: Injecting table reasoning skills during pre-training via synthetic reasoning examples",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Yilun Zhao",
                "Zhenting Qi",
                "Linyong Nan",
                "Lorenzo Jaime Flores",
                "Dragomir Radev."
            ],
            "title": "LoFT: Enhancing faithfulness and diversity for table-to-text generation via logic form control",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the As-",
            "year": 2023
        },
        {
            "authors": [
                "Yilun Zhao",
                "Haowei Zhang",
                "Shengyun Si",
                "Linyong Nan",
                "Xiangru Tang",
                "Arman Cohan"
            ],
            "title": "2023c. Large language models are effective table-to-text generators, evaluators, and feedback providers",
            "year": 2023
        },
        {
            "authors": [
                "Yilun Zhao",
                "Chen Zhao",
                "Linyong Nan",
                "Zhenting Qi",
                "Wenlin Zhang",
                "Xiangru Tang",
                "Boyu Mi",
                "Dragomir Radev."
            ],
            "title": "RobuT: A systematic study of table QA robustness against human-annotated adversarial perturbations",
            "venue": "Proceedings of the 61st",
            "year": 2023
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric Xing"
            ],
            "title": "Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685",
            "year": 2023
        },
        {
            "authors": [
                "Ming Zhong",
                "Da Yin",
                "Tao Yu",
                "Ahmad Zaidi",
                "Mutethia Mutuma",
                "Rahul Jha",
                "Ahmed Hassan Awadallah",
                "Asli Celikyilmaz",
                "Yang Liu",
                "Xipeng Qiu",
                "Dragomir Radev."
            ],
            "title": "QMSum: A new benchmark for querybased multi-domain meeting summarization",
            "venue": "Pro-",
            "year": 2021
        },
        {
            "authors": [
                "Victor Zhong",
                "Caiming Xiong",
                "Richard Socher"
            ],
            "title": "Seq2SQL: Generating structured queries from natural language using reinforcement learning",
            "year": 2018
        },
        {
            "authors": [
                "Yijie Zhou",
                "Kejian Shi",
                "Wencai Zhang",
                "Yixin Liu",
                "Yilun Zhao",
                "Arman Cohan"
            ],
            "title": "Odsum: New benchmarks for open domain multi-document summarization",
            "year": 2023
        },
        {
            "authors": [
                "Fengbin Zhu",
                "Wenqiang Lei",
                "Youcheng Huang",
                "Chao Wang",
                "Shuo Zhang",
                "Jiancheng Lv",
                "Fuli Feng",
                "TatSeng Chua."
            ],
            "title": "TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance",
            "venue": "Proceedings of the 59th Annual",
            "year": 2021
        },
        {
            "authors": [
                "Paul Tracy"
            ],
            "title": "performed better than Mario Haberfeld, as evidenced in their results from the 2004 Centrix Financial Grand Prix of Denver",
            "year": 2004
        }
    ],
    "sections": [
        {
            "text": "People primarily consult tables to conduct data analysis or answer specific questions. Text generation systems that can provide accurate table summaries tailored to users\u2019 information needs can facilitate more efficient access to relevant data insights. Motivated by this, we define a new query-focused table summarization task, where text generation models have to perform human-like reasoning and analysis over the given table to generate a tailored summary. We introduce a new benchmark named QTSUMM for this task, which contains 7,111 human-annotated query-summary pairs over 2,934 tables covering diverse topics. We investigate a set of strong baselines on QTSUMM, including text generation, table-to-text generation, and large language models. Experimental results and manual analysis reveal that the new task presents significant challenges in table-totext generation for future research. Moreover, we propose a new approach named REFACTOR, to retrieve and reason over query-relevant information from tabular data to generate several natural language facts. Experimental results demonstrate that REFACTOR can bring improvements to baselines by concatenating the generated facts to the model input. Our data and code are publicly available at https: //github.com/yale-nlp/QTSumm."
        },
        {
            "heading": "1 Introduction",
            "text": "In the era of data-driven decision-making, tabular data plays a crucial role in facilitating data analysis, serving as a concise and structured representation of information (Kukich, 1983; Pasupat and Liang, 2015; Chen et al., 2020c; Zhu et al., 2021; Zhao et al., 2022a; Tang et al., 2023). People often consult tables to extract valuable insights and make informed decisions. For example, sales managers typically explore large tables with specific business questions to gain insights about clients and processes. Sports coaches will analyze performance\ntables containing various statistics to develop game strategies and make team adjustments. However, effectively accessing and comprehending the information contained within a large and complex table can be time-consuming for users (Hurst, 2000; Pasupat and Liang, 2015; Pujara et al., 2021; Nan et al., 2022a). Text generation systems that can accurately summarize a provided table according to users\u2019 information needs have the potential to greatly enhance data analysis and expedite the process of obtaining data insights.\nExisting work and datasets on table-to-text generation (Parikh et al., 2020; Chen et al., 2020a; Cheng et al., 2022b; Lebret et al., 2016; Moosavi et al., 2021; Suadaa et al., 2021) have mainly focused on converting tabular data into coherent statements, aiming to present the structured data in a humanreadable format. However, these approaches have overlooked the fundamental goal of addressing\nusers\u2019 information-seeking purposes. Table-to-text generation systems should adopt a more flexible and interactive approach that allows people to obtain a user-customized summary tailored to their information needs (Dang, 2006; Xu and Lapata, 2020; Zhong et al., 2021; Xu and Lapata, 2022; Zhou et al., 2023), as illustrated in Figure 1. While table question answering (QA) (Pasupat and Liang, 2015; Iyyer et al., 2017; Zhong et al., 2018; Chen et al., 2020c; Nan et al., 2022b) has made significant progress in answering fact-based questions, the primary focus of their approaches is on extracting relevant facts or entities from the table and composing short-form answers. Nevertheless, in real-world scenarios, users often have more complex and diverse information needs that extend beyond simple fact retrieval. They expect models to perform human-like reasoning and provide trustworthy explanations or analyses that accompany the extracted insights.\nWith comprehensive consideration of the realworld information needs of users when consulting tabular data, we propose a new task, query-focused table summarization. In this task, the model is required to generate a user-customized summary given the table and user query. To enable research in this area, we construct a human-annotated tableto-text generation dataset named QTSUMM1, that contains 7,111 query-summary pairs over 2,934 Wikipedia tables covering diverse topics. Table 1 compares QTSUMM with previous table-to-text generation datasets. To the best of our knowledge, QTSUMM is the first dataset that tackles tasks of generating user-customized table summaries based on real-world scenarios.\nWe provide a comprehensive evaluation of current state-of-the-art models, including text generation (Lewis et al., 2020; Raffel et al., 2020; Chung et al., 2022), table-to-text generation (Liu et al., 2022b; Zhao et al., 2022b; Jiang et al., 2022), and large language models (Touvron et al., 2023a,b; Zheng et al., 2023; Jiang et al., 2023a; Xu et al., 2023; OpenAI, 2023). Our results and analysis from different perspectives reveal that the existing models struggle in solving this new task, highlighting the challenges the models face when performing human-like reasoning and analysis to generate summary tailored to users\u2019 information needs.\n1We released the dataset at https://huggingface. co/datasets/yale-nlp/QTSumm using \u201cgated repositories\u201d to protect the data from automatic crawling (Jacovi et al., 2023).\nTo improve both text generation systems for QTSUMM, we propose REFACTOR. Given a user query, REFACTOR can retrieve and reason over query-relevant facts from the source table to generate multiple data insights in natural language sentences. Our results illustrate that directly concatenating the original input sequence with REFACTOR\u2019s generation can bring effective improvements to state-of-the-art baseline systems.\nWe conclude our main contributions as follows:\n\u2022 We propose a new query-focused table summarization task, and construct a large-scale benchmark, QTSUMM, comprising 7,111 querysummary pairs collected in real-world situations. Strict quality control measures are employed to ascertain the high quality of the dataset.\n\u2022 We conduct a systematic study of state-of-the-art models on QTSUMM, and illustrate that they are still far behind expert performance, motivating future research on this new table-to-text task.\n\u2022 We present REFACTOR for the efficient retrieval and reasoning of query-relevant facts from tables. It demonstrates significant enhancements pertaining to state-of-the-art text generation baselines."
        },
        {
            "heading": "2 Related Work",
            "text": "Table-to-Text Generation As illustrated in Table 1, existing work and datasets on table-to-text generation typically pose the problem as either a single-sentence generation task (Chen et al., 2020a; Parikh et al., 2020; Cheng et al., 2022b; Liu et al., 2022a), or a generic summarization task (Lebret et al., 2016; Moosavi et al., 2021; Suadaa et al., 2021). In the single-sentence generation task (Parikh et al., 2020; Chen et al., 2020a; Cheng et al., 2022b), the focus is on generating fluent and faithful descriptions using provided table regions as a control for text generation. Nevertheless, using table regions for controlling text generation does not align with real-world scenarios, where people refer to tabular data for information-seeking purposes. The generic table summarization tasks (Lebret et al., 2016; Moosavi et al., 2021; Suadaa et al., 2021) aim to create concise and informative summaries based on the content of a given domainspecific table (i.e., sports or scientific). In contrast, the tables in QTSUMM cover diverse topics. Furthermore, considering the numerous data points in the table, various users may be interested in different aspects for their own information-seeking\npurposes, making it challenging to create a generic summary that encompasses all the salient information within the table. Therefore, in this paper, we propose and investigate a new task setting related to query-focused summarization. FeTaQA (Nan et al., 2022b) is a table QA dataset that collects queries by rewriting ToTTo\u2019s (Parikh et al., 2020) statements into questions and uses the same statements as the answers. In comparison with FeTaQA, the queries in QTSUMM were annotated under realworld scenarios, making them more natural and better-reflecting users\u2019 actual information needs.\nReasoning Over Tabular Data Enhancing the table reasoning capabilities of models is essential for a variety of tasks related to tables, such as table question answering (Pasupat and Liang, 2015; Iyyer et al., 2017; Zhong et al., 2018; Zhao et al., 2023d), table fact verification (Chen et al., 2020b), and table-to-text generation (Chen et al., 2020a; Cheng et al., 2022b). One prevalent approach is pre-training models with table-text joint reasoning data (Herzig et al., 2020; Liu et al., 2022b; Zhao et al., 2022b; Liu et al., 2022a; Jiang et al., 2022; Dong et al., 2022; Cheng et al., 2022a; Xie et al., 2022). Nevertheless, these models generate text in an end-to-end manner, resulting in reduced explainability and difficulties in handling more complex reasoning, such as arithmetic calculation. Therefore, we propose REFACTOR, which can retrieve and generate query-relevant facts from tables as intermediate results for model input (Zhou et al., 2022; Zhao et al., 2023b), mitigating the implicit reasoning processes of text generation models.\nQuery-Focused Summarization Initially formulated as a document summarization task, QFS aims\nto generate summaries from documents that are tailored to specific user queries (Dang, 2006). Despite its potential real-world applications, QFS remains a challenging task due to the lack of large-scale training data. Existing works have attempted to address this issue by leveraging distant NLP resources, including question answering (Xu and Lapata, 2020) and paraphrase identification (Su et al., 2020), and generic summarization (Xu and Lapata, 2022; Zhou et al., 2023). Recently, Zhong et al. (2021) adopted QFS for meeting summarization and proposed a human-annotated benchmark over meeting transcripts. Similar to text, effectively accessing and comprehending the information contained within a large and complex table can be time-consuming for users, while QFS remains unexplored in tableto-text generation. In this work, we extend QFS to this new modality for more effective informationseeking and decision-making purposes."
        },
        {
            "heading": "3 Query-Focused Table Summarization",
            "text": ""
        },
        {
            "heading": "3.1 Problem Formulation",
            "text": "We formally define the proposed query-focused table summarization task as follows. The input is a user query Q, and a table T . The table T = W \u222a {Ti,j |i \u2264 RT , j \u2264 CT } has RT rows and CT columns, with W being the table title and Ti,j being the textual content in the (i, j)-th cell. The task objective of QTSUMM is to generate a paragraphlong textual summary Y = (y1, y2, . . . , yn) given the user query Q and source table T :\nY = argmax n\u220f\ni=1\nP (yi|y<i,Q,T ; \u03b8), (1)\nwhere \u03b8 denotes the parameters of a neural text generation model, and yi denotes the i-th tokens in the generated summary."
        },
        {
            "heading": "3.2 Data Collection Principles",
            "text": "At a high level, the goal of the data collection process is to obtain high-quality user queries and corresponding paragraph-long summaries grounded on the tabular data. We outline our key criteria for designing a benchmark to thoroughly evaluate the table-to-text summarization capabilities of models. To achieve this, we first design three principles for annotating a good query-summary pair:\n\u2022 Comprehensiveness: The tailored summary should provide enough details and analysis of the source table to respond to the user query, fulfilling user\u2019s information need.\n\u2022 Attributablity & Faithfulness: The query should be answerable using only information from the source table. The summary should be grounded on the source table, and not contain any unfaithful or nonsensical text.\n\u2022 Fluency: Both the user query and its corresponding table summary should be coherent and fluent."
        },
        {
            "heading": "3.3 QTSUMM Annotation Pipeline",
            "text": "To ensure that QTSUMM annotation fulfills the aforementioned principles, we carefully design an annotation pipeline consisting of following steps:\nSource Table Collection QTSUMM uses tables from LOGICNLG (Chen et al., 2020a) and TOTTO (Parikh et al., 2020) datasets as source tables, as these tables are crwaled from Wikipedia and covers diverse domains and topics. We filter out tables that are 1) too large or too small, 2) with only string-type columns, or 3) with hierarchical structures (e.g., containing more than one table header). Then we randomly sample 2,000 candidate tables from LOGICNLG and TOTTO, respectively, for the query-summary annotation.\nUser Query Annotation Given a table, the annotators are required to read its content, and determine whether the table is informative and intelligible to common web users. Then they were asked to come up with two or three queries, assuming they are users seeking certain information from the table. We require each query to be answerable using information only from the table. Moreover, as this work focuses on paragraph-long summaries\nas query responses, we avoid queries that can be answered in a short sentence (e.g., \u201cWhich country held the 2022 FIFA World Cup?\u201d).\nQuery-Focused Summary Annotation Given a table and user query, we ask another annotator to use only information from the source table to write a paragraph-long summary that satisfies the user\u2019s information need. We encourage annotators to produce sophisticated summaries that 1) contain as much information from the table as possible, and 2) involve more types of reasoning over multiple relevant table regions. To further encourage high quality annotations, we adopt the \"two channel collection\" design (Chen et al., 2020b), in which the annotators would be paid 60% more if their summaries are manually verified to exhibit adequate complexity. We also require the annotators to annotate the row indices of relevant table regions that are referenced in the written summary, allowing future researchers to quantify how well the summaries are grounded in the table in their work.\nMulti-Round Validation We conduct a multiround validation protocol to ensure that the annotated data fulfills the aforementioned annotation principles. We first assign query annotators to validate each summary against their corresponding queries, and fix the mistakes if there are any. Then we check 1) whether a query-summary pair contain adequate information and complex aggregation by examining the length of the summary, and 2) whether the information in summary is essential in responding to the user query. We manually revise pairs that do not meet the above standard."
        },
        {
            "heading": "3.4 Annotation Quality Control",
            "text": "Table 2 describes the basic statistics of QTSUMM. In addition to the multi-round validation, we carefully design several quality control approaches,\ncomprising expert annotation and numerous annotation de-biasing designs, to ensure the high quality of QTSUMM annotations.\nExpert Annotators To help improve the annotation process, five experts with professional experience in the text summarization tasks are invited to conduct the internal annotation. They are asked to provide feedback regarding the task instructions and the user experience of the annotation interface, based on which we iteratively modify the annotation guideline and interface design. In the stage of external annotation, we enroll 17 graduate students majoring in STEM fields (10 females, and 7 males). We do not use the crowd-source annotation platform such as Mechanical Turk as our preliminary study indicates that annotators on MTurk fail to annotate high-quality query-summary data. Before starting the official annotation process, each annotator is given a two-hour training session to learn the annotation requirements and interface.\nAnnotation De-biasing We observed several kinds of annotation bias during our internal annotation, and we proposed countermeasures as follows for annotation de-biasing:\nSource Table Diversity: During internal annotation, we found that many tables in LOGICNLG have similar content. For example, there are around 200 tables describing the results of football games, with identical table headers. To ensure the diversity of source tables, we keep only one table for each unique table header.\nQuery Diversity: When annotating queries, annotators may prefer simpler ones, resulting in low query diversity. Therefore, we frequently monitor the diversity of queries for each annotator. Annotators are also encouraged to craft queries that are either creative or require complex reasoning in summarization, resulting in a doubled payment to compensate them for the extra time.\nSupporting Fact Position: We found that annotators prefer to raise queries regarding the first few rows of each table. To deal with such bias regarding supporting fact positions, we randomly highlight certain rows for each table in the annotation interface. We require the annotators to write queries whose summaries should cover at least two rows of the highlighted regions.\nWe also report the human evaluation scores and inter-evaluator agreements over 200 sampled querysummary pairs. QTSUMM has a high annotation\nquality and inter-annotator agreement (Table 3)."
        },
        {
            "heading": "3.5 QTSUMM Evaluation",
            "text": "We develop a comprehensive approach for evaluating QTSumm, incorporating both automated and human evaluation. We adopt following popular automated evaluation metrics:\nBLEU (Papineni et al., 2002) computes the geometric average of the precision over output text\u2019s ngrams. We used SacreBLEU (Post, 2018) that produces comparable and reproducible BLEU scores.\nROUGE (Lin and Hovy, 2003) measures the word overlap between the candidate and reference summaries. We reported F1 score for ROUGE-L (longest common subsequences).\nMETEOR (Banerjee and Lavie, 2005) is based on a generalized concept of unigram matching between the machine-produced translation and human-produced reference translations.\nBERTScore (Zhang et al., 2020) computes the sim-\nilarity between the reference and generated summary using contextual word embeddings.\nTAPAS-Acc (Herzig et al., 2020; Liu et al., 2022a) is a reference-free metric that uses TAPAS (Herzig et al., 2020) fine-tuned on the TabFact dataset (Chen et al., 2020b) as the backbone to evaluate the faithfulness of generation.\nAutoACU (Liu et al., 2023a) is an interpretable and reference-based summarization evaluation system that exhibits better alignment with human judgements. The A2CU first extracts atomic content units (ACUs) from the generated summary and then evaluates them against reference. A3CU is an accelerated version of A2CU that directly computes the similarity between two text without extracting ACUs, but with the similar evaluation target. We use F1 score of A3CU for evaluation.\nFor human evaluation, the summaries from different models were evaluated by experts from three criteria (i.e., comprehensiveness, faithfulness, and fluency) that have been discussed in Section 3.2. Each summary was scored from 1 (worst) to 5 (best) for each criteria, with the final score averaged across different evaluators."
        },
        {
            "heading": "4 REFACTOR",
            "text": "QTSUMM requires models to perform human-like reasoning in generating summaries that provide comprehensive and precise analysis of the source table to fulfill the user\u2019s information need. However, existing end-to-end text generation models rely on error-prone implicit reasoning processes for generating text, leading to diminished explainability and challenges in addressing user queries that necessitate complex human-like reasoning (Zhou et al., 2022; Zhao et al., 2023b). To address this, we present REFACTOR, to retrieve and reason over query-relevant information from tabular data to generate several NL data insights (i.e., facts) as explicit reasoning results. As shown in Figure 3, the generated facts is concatenated to the model input to mitigate the implicit reasoning issues, enhancing the comprehensiveness and faithfulness of generated summary. We next discuss the implementation of REFACTOR."
        },
        {
            "heading": "4.1 Fact Generation",
            "text": "Given the user query and source table, REFACTOR will generate several candidate facts by executing various forms of human-like reasoning over the ta-\nble. Specifically, we define 6 types of table reasoning operations (e.g., numerical operation, counting, and conjunction) that are necessary for the QTSUMM task, as shown in Table 7 in the Appendix. For each reasoning operation, the fact generator (adopted from Zhao et al. (2022b)) takes a table and a query as input. It produces multiple facts based on the fact template. Each fact template includes several placeholders that need to be filled with information retrieved from the table. Specifically, column col and cell value val are indexed to specify the column and cell name, respectively. Some templates also regulate that the selected column and cell value must be date or number type. OPERATOR corresponds to operators that are instantiated according to the specific reasoning reasoning. And CONDITION:i can be 1) a cell value from the i-th column; or 2) a number/temporal comparison statement if the i-th column is date or number type. After substituting all the placeholders in the provided template, the fact generator will programmatically return the executed_results and form one fact. Once facts for a {table, query} pair are collected from different fact generators, we pass them to the Fact Ranking process."
        },
        {
            "heading": "4.2 Fact Ranking",
            "text": "Given the query and source table, each fact generator will be utilized to generate several queryrelevant facts, resulting in a large number of candidate facts in total. Therefore, we need to rank the generated facts to select the most relevant ones. We use the QA encoding model (Reimers and Gurevych, 2019) to obtain the embedding of the query and each generated fact. Then, we select the top-n generated facts with the highest cosine similarity to the query embedding. In practice, we\nassign n as max(\n\u221a rownum \u00d7 columnnum\n2 , 5),\nand ensure that the number of selected facts from each type of reasoning operation does not exceed 3. The selected facts, which are handy and readily available for end-to-end text generation systems, are then concatenated into the model input."
        },
        {
            "heading": "5 QTSUMM Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Baseline Systems",
            "text": "We evaluate the following three types of state-ofthe-art baseline systems2 on QTSUMM:"
        },
        {
            "heading": "5.1.1 Text Generation Models",
            "text": "BART (Lewis et al., 2020) is a pre-trained denoising autoencoder with transformer-based architecture and shows effectiveness in NLG tasks.\nT5 (Raffel et al., 2020) demonstrates effectiveness in NLG tasks by treating all NL problems as textto-text tasks during pre-training stage.\nFlan-T5 (Chung et al., 2022) enhances T5 by scaling instruction fine-tuning and demonstrates better human-like reasoning abilities than the T5."
        },
        {
            "heading": "5.1.2 Table-to-Text Generation Models",
            "text": "TAPEX (Liu et al., 2022b) continues pre-training the BART model by using a large-scale corpus of synthetic SQL query execution data. It shows better table understanding and reasoning abilities.\nReasTAP (Zhao et al., 2022b) enhances the table understanding and reasoning abilities of BART by pre-training on a synthetic Table QA corpus.\nOmniTab (Jiang et al., 2022) uses the same backbone as TAPEX, and is further pre-trained on collected natural and synthetic Table QA examples.\n2We released the model weights of evaluated fine-tuned models at HuggingFace (https://huggingface.co/ yale-nlp/{model_name}-finetuned-qtsumm)."
        },
        {
            "heading": "5.1.3 Large Language Models",
            "text": "Llama-23 (Touvron et al., 2023a,b) is an opensource large language model trained on large-scale and publicly available datasets.\nVicuna4 (Zheng et al., 2023) is tuned from Llama-1 with instruction-following data, exhibiting better instruction-following capabilities.\nMistral5 (Jiang et al., 2023a) is a 7\u2013billionparameter LLM that outperforms Llama-2-13B across most of popular evaluated benchmarks.\nLemur6 (Xu et al., 2023) is tuned from Llama-2 with instruction-following data, exhibiting competitive natural language and coding capabilities.\nGPT (Brown et al., 2020; OpenAI, 2023) is a powerful large language model which is capable of generating human-like text and performing a wide range of NLP tasks in a few-shot setting."
        },
        {
            "heading": "5.2 Experimental Setup",
            "text": "The specifics of input data serialization and LLM prompting examples are discussed in Appendix A. All experiments were conducted on an 8 NVIDIA RTX A6000 48GB cluster. We selected the large version for all fine-tuned baseline models, whose weights are publicly available at HuggingFace. For each fine-tuning experiment, we ran 15 epochs with a batch size of 128. The best fine-tuning checkpoints were selected according to the validation loss. The experiments for open-sourced LLMs were conducted using vLLM framework (Kwon et al., 2023). We used gpt-3.5-turbo-0613 for GPT-3.5 and gpt-4-0613 for GPT-4 via the OpenAI APIs7. For LLM hyperparameter settings, we set temperature as 1.0, Top P as 1.0, and maximum output length as 256."
        },
        {
            "heading": "5.3 Main Results",
            "text": "We draw following conclusions based on the automated and human evaluation results (Table 4 & 6).\nImportance of table structure understanding Table-to-text generation models achieve better performance than their corresponding text-generation\n3https://huggingface.co/meta-llama/ llama-2-{size}b-chat-hf\n4We only evaluate Vicuna (https://huggingface. co/lmsys/vicuna-33b-v1.3) under zero- and oneshot settings, as some examples under the two-shot setting might exceeds its maximum length limit.\n5mistralai/Mistral-7B-Instruct-v0.1 6https://huggingface.co/OpenLemur/\nlemur-70b-chat-v1 7https://openai.com/api/\nbackbones, demonstrating the importance of considering table structure for the QTSUMM task.\nImportance of reasoning and analysis Among text generation models, Flan-T5, which enhances T5 through scaled instruction fine-tuning, outperforms T5. Moreover, LLMs with improved reasoning capabilities (i.e., Llama-2-70B and GPT-4) also achieve better performance. These findings indicate the significance of reasoning and analytical skills in handling the QTSUMM task.\nMismatch between automated and human evaluation Despite receiving low scores in popular automated evaluation metrics such as BLEU and ROUGE, GPT-* exhibit better performance than state-of-the-art fine-tuned models in human evaluation. This finding underscores the need for future research to investigate the development of automated evaluation metrics for the QTSUMM task that better align with human judgments (Zhang and Bansal, 2021; Liu et al., 2023a; Jiang et al., 2023b).\nEffectiveness of REFACTOR As assessed by human evaluation, baseline systems employing REFACTOR typically yield better performance, especially in faithfulness-level. This suggests the efficacy of REFACTOR in enhancing the reasoning process in text generation."
        },
        {
            "heading": "5.4 Error Analysis",
            "text": "For a deeper understanding of the query-focused table summarization task on QTSUMM, we conduct an error analysis to illustrate existing challenges.\nWe identify four common mistakes that current text generation models are likely to make (i.e., hallucination, factual incorrectness, user intent misunderstanding, and repetition), providing detailed examples and explanations for each type of common mistake in Table 8 in the Appendix."
        },
        {
            "heading": "5.5 REFACTOR Analysis",
            "text": "We also undertake a human evaluation to examine the efficacy of REFACTOR in generating queryrelevant facts from tabular data. Specifically, we randomly sample 200 examples from QTSUMM validation set, and ask two human evaluators to evaluate each fact generated by REFACTOR, determining its relevance to the query. 56.4% generated facts (528 out of 937) are labeled as \u201crelevant\u201d, suggesting an adequate coverage of REFACTOR. To delve deeper into this, we also conduct a case study examining the failure cases, specifically those examples where less than two facts were annotated as \u201crelevant\u201d. We identified three kinds of common failure cases: (1) difficulty in parsing cell values via rule-based methods, (2) complex user query causes difficulty in ranking related facts, and (3) unsupported reasoning operations. We provide detailed examples and explanations in Table 5."
        },
        {
            "heading": "6 Conclusion",
            "text": "This paper defines a new query-focused table summarization task, and constructs a large-scale benchmark, QTSUMM. We investigate a set of strong baselines, including text generation, table-to-text generation, and large language models. Experimental results and manual analysis reveal that the new task presents significant challenges in tableto-text generation. Moreover, we propose a novel approach named REFACTOR, to retrieve and reason over query-relevant information from tables, improving the faithfulness of generated summary."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to dedicate this paper to the memory of Dr. Dragomir Radev. Dr. Radev\u2019s leadership, guidance, and expertise were instrumental in shaping the direction and quality of this project. We appreciate the efforts of all annotators in constructing QTSUMM and conducting human evaluation. We are grateful to the Google TRC program for their support. We would also like to thank the anonymous reviewers and action editors for constructive discussions and feedback.\nLimitations and Future Work\nThe baseline systems provided have a restricted maximum number of tokens they can accommodate (e.g., 1024 for all examined fine-tuned models), which prevents them from generating summaries for large tables that, when converted into a sequence, exceed the maximum number of tokens. To handle large tables (e.g., with more than 300 table cells), future work can apply neural models (Herzig et al., 2020; Liu et al., 2022b) to first filter out those query-irrelevant rows or columns.\nMoreover, this paper demonstrates the effectiveness of using intermediate results obtained from explicit reasoning operations to mitigate the implicit reasoning issues. However, the proposed REFACTOR utilizes template-based method to generate facts. Although such template-based approach can ensure the factual correctness of generated facts, as discussed in Section 5.5, it might not cover all crucial facts for some complex user query. We believe following directions warrant further exploration: (1) Complex query decomposition. Our case study reveals that the TAPEX-based fact ranking module struggles with comprehending complex questions. To address this, future research could investigate LLM chain-of-thought methods to break down complex questions into more understandable and actionable sub-questions. (2) Tool usage. The predefined and template-based execution modules in the REFACTOR fact generation phase have their limitations. Recent studies (Schick et al., 2023; Lu et al., 2023; Paranjape et al., 2023; Gou et al., 2023; Qiao et al., 2023) highlight the impressive abilities of LLMs in making and utilizing tools for problem-solving. It would be intriguing to explore if LLMs can produce executable programs from scratch to derive query-relevant insights. (3) Explainable automated evaluation. In Section 5.3, a discrepancy between automated and human evaluation results is observed. Such discrepancies are concerning, as developers might opt for suboptimal systems for real-world applications if they solely rely on automatic metrics for comparing and ranking different text generation systems. Therefore, a more reliable and explainable automated evaluation system is required (Zhang and Bansal, 2021; Liu et al., 2023a,b; Jiang et al., 2023b).\nEthical Consideration\nThe source tables in QTSUMM were collected from LOGICNLG (Chen et al., 2020a) and\nTOTTO (Parikh et al., 2020) datasets, which are publicly available under the MIT license8 and CC BY-SA 3.0 license9, respectively. They both permit us to compose, modify, publish, and distribute additional annotations upon the original dataset.\nFor the external annotation of QTSUMM, we hired 17 graduate students majoring in STEM majors. We regard 1) creating three queries for one table, and validating the corresponding summaries annotated by others, and 2) composing a queryfocused summary response as a unit task. And we paid around $1.5 for each unit task. For creative annotation rewards, we paid additional $0.5 for a query, and $1.5 for a summary. Averagely, an annotator can finish 7 unit tasks per hour after training and practicing. And the hourly rates are in the range of $9 and $13 based on the different working speed (above the local average wage of similar jobs). We recommended that annotators complete a maximum of 30 unit tasks per day in order to reduce pressure and maintain a comfortable pace. In total, the approximate working hours to annotate QTSUMM dataset was 1,400 hours. The whole annotation work lasted about 40 days."
        }
    ],
    "title": "QTSUMM: Query-Focused Summarization over Tabular Data",
    "year": 2023
}