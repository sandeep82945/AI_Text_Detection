{
    "abstractText": "ChatGPT shows remarkable capabilities for machine translation (MT). Several prior studies have shown that it achieves comparable results to commercial systems for high-resource languages, but lags behind in complex tasks, e.g., low-resource and distant-language-pairs translation. However, they usually adopt simple prompts which can not fully elicit the capability of ChatGPT. In this paper, we aim to further mine ChatGPT\u2019s translation ability by revisiting several aspects: temperature, task information, and domain information, and correspondingly propose an optimal temperature setting and two (simple but effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP). We show that: \u2776 The performance of ChatGPT depends largely on temperature, and a lower temperature usually can achieve better performance; \u2777 Emphasizing the task information can further improve ChatGPT\u2019s performance, particularly in complex MT tasks; \u2778 Introducing domain information can elicit ChatGPT\u2019s generalization ability and improve its performance in the specific domain; \u2779 ChatGPT tends to generate hallucinations for non-English-centric MT tasks, which can be partially addressed by our proposed prompts but still need to be highlighted for the MT/NLP community. We also explore the effects of advanced in-context learning strategies and find a (negative but interesting) observation: the powerful chain-ofthought prompt leads to word-by-word translation behavior, thus bringing significant translation degradation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Keqin Peng"
        },
        {
            "affiliations": [],
            "name": "Liang DingR"
        },
        {
            "affiliations": [],
            "name": "Qihuang Zhong"
        },
        {
            "affiliations": [],
            "name": "Li ShenR"
        },
        {
            "affiliations": [],
            "name": "Xuebo Liu"
        },
        {
            "affiliations": [],
            "name": "Min Zhang"
        },
        {
            "affiliations": [],
            "name": "Yuanxin Ouyang"
        },
        {
            "affiliations": [],
            "name": "Dacheng Tao"
        }
    ],
    "id": "SP:6a9eed7e610e22cf35df1d034be9232df7c53852",
    "references": [
        {
            "authors": [
                "Lo\u00efc Barrault",
                "Ondrej Bojar",
                "Marta R. Costa-juss\u00e0"
            ],
            "title": "Findings of the 2019 conference on machine translation (WMT19)",
            "venue": "WMT",
            "year": 2019
        },
        {
            "authors": [
                "Rachel Bawden",
                "Kevin Bretonnel Cohen",
                "Cristian Grozea"
            ],
            "title": "Findings of the WMT 2019 biomedical translation shared task: Evaluation for MEDLINE abstracts and biomedical terminologies",
            "venue": "WMT",
            "year": 2019
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "Tom B. Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared Kaplan"
            ],
            "title": "2020b. Language models are few-shot learners",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "Marta R Costa-juss\u00e0",
                "James Cross",
                "Onur \u00c7elebi",
                "Maha Elbayad",
                "Kenneth Heafield",
                "Kevin Heffernan",
                "Elahe Kalbassi",
                "Janice Lam",
                "Daniel Licht",
                "Jean Maillard"
            ],
            "title": "No language left behind: Scaling humancentered machine translation",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL.",
            "year": 2019
        },
        {
            "authors": [
                "Liang Ding",
                "Longyue Wang",
                "Xuebo Liu",
                "Derek F. Wong",
                "Dacheng Tao",
                "Zhaopeng Tu."
            ],
            "title": "Progressive multi-granularity training for non-autoregressive translation",
            "venue": "Findings of ACL.",
            "year": 2021
        },
        {
            "authors": [
                "Liang Ding",
                "Longyue Wang",
                "Dacheng Tao."
            ],
            "title": "Self-attention with cross-lingual position representation",
            "venue": "ACL.",
            "year": 2020
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Lei Li",
                "Zhifang Sui."
            ],
            "title": "A survey on in-context learning",
            "venue": "arXiv preprint.",
            "year": 2023
        },
        {
            "authors": [
                "Jinhua Du",
                "Andy Way"
            ],
            "title": "Pre-reordering for neural machine translation: Helpful or harmful",
            "venue": "Prague Bulletin of Mathematical Linguistics",
            "year": 2017
        },
        {
            "authors": [
                "Jiangtao Feng",
                "Lingpeng Kong",
                "Po-Sen Huang",
                "Chong Wang",
                "Da Huang",
                "Jiayuan Mao",
                "Kan Qiao",
                "Dengyong Zhou."
            ],
            "title": "Neural phrase-to-phrase machine translation",
            "venue": "arXiv preprint.",
            "year": 2018
        },
        {
            "authors": [
                "Markus Freitag",
                "Ricardo Rei",
                "Nitika Mathur"
            ],
            "title": "Results of WMT22 metrics shared task: Stop using BLEU \u2013 neural metrics are better and more robust",
            "venue": "WMT",
            "year": 2022
        },
        {
            "authors": [
                "Yuan Gao",
                "Ruili Wang",
                "Feng Hou."
            ],
            "title": "How to design translation prompts for chatgpt: An empirical study",
            "venue": "arXiv e-prints.",
            "year": 2023
        },
        {
            "authors": [
                "Naman Goyal",
                "Cynthia Gao",
                "Vishrav Chaudhary",
                "PengJen Chen"
            ],
            "title": "The flores-101 evaluation benchmark for low-resource and multilingual machine translation. TACL",
            "year": 2022
        },
        {
            "authors": [
                "Zhiwei He",
                "Tian Liang",
                "Wenxiang Jiao",
                "Zhuosheng Zhang",
                "Yujiu Yang",
                "Rui Wang",
                "Zhaopeng Tu",
                "Shuming Shi",
                "Xing Wang."
            ],
            "title": "Exploring humanlike translation strategy with large language models",
            "venue": "arXiv preprint.",
            "year": 2023
        },
        {
            "authors": [
                "Amr Hendy",
                "Mohamed Abdelrehim",
                "Amr Sharaf"
            ],
            "title": "How good are gpt models at machine translation? a comprehensive evaluation",
            "year": 2023
        },
        {
            "authors": [
                "Daphne Ippolito",
                "Reno Kriz",
                "Jo\u00e3o Sedoc",
                "Maria Kustikova",
                "Chris Callison-Burch."
            ],
            "title": "Comparison of diverse decoding methods from conditional language models",
            "venue": "ACL.",
            "year": 2019
        },
        {
            "authors": [
                "Wenxiang Jiao",
                "Wenxuan Wang",
                "Jen-tse Huang",
                "Xing Wang",
                "Zhaopeng Tu."
            ],
            "title": "Is chatgpt a good translator? a preliminary study",
            "venue": "arXiv preprint.",
            "year": 2023
        },
        {
            "authors": [
                "Ronald M Kaplan",
                "Klaus Netter",
                "Jurgen Wedekind",
                "Annie Zaenen."
            ],
            "title": "Translation by structural correspondences",
            "venue": "EACL.",
            "year": 1989
        },
        {
            "authors": [
                "Nagata",
                "Toshiaki Nakazawa",
                "Michal Nov\u00e1k",
                "Martin Popel",
                "Maja Popovi\u0107."
            ],
            "title": "Findings of the 2022 conference on machine translation (WMT22)",
            "venue": "WMT.",
            "year": 2022
        },
        {
            "authors": [
                "Tom Kocmi",
                "Christian Federmann."
            ],
            "title": "Large language models are state-of-the-art evaluators of translation quality",
            "venue": "arXiv preprint.",
            "year": 2023
        },
        {
            "authors": [
                "Philipp Koehn."
            ],
            "title": "Statistical machine translation",
            "venue": "Cambridge University Press.",
            "year": 2009
        },
        {
            "authors": [
                "Philipp Koehn",
                "Rebecca Knowles."
            ],
            "title": "Six challenges for neural machine translation",
            "venue": "WMT.",
            "year": 2017
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "NeurIPS.",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for GPT-3? In DeeLIO",
            "year": 2022
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Comput. Surv.",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint.",
            "year": 2019
        },
        {
            "authors": [
                "Hongyuan Lu",
                "Haoyang Huang",
                "Dongdong Zhang",
                "Haoran Yang",
                "Wai Lam",
                "Furu Wei."
            ],
            "title": "Chainof-dictionary prompting elicits translation in large language models",
            "venue": "arXiv preprint.",
            "year": 2023
        },
        {
            "authors": [
                "Qingyu Lu",
                "Baopu Qiu",
                "Liang Ding",
                "Liping Xie",
                "Dacheng Tao."
            ],
            "title": "Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt",
            "venue": "arXiv preprint.",
            "year": 2023
        },
        {
            "authors": [
                "Makoto Nagao."
            ],
            "title": "A framework of a mechanical translation between japanese and english by analogy principle",
            "venue": "Artificial and human intelligence.",
            "year": 1984
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L. Wainwright",
                "Pamela Mishkin",
                "Chong Zhang"
            ],
            "title": "2022a. Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "2022b. Training language models to follow instructions with human feedback. NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "ACL.",
            "year": 2002
        },
        {
            "authors": [
                "Keqin Peng",
                "Liang Ding",
                "Qihuang Zhong",
                "Yuanxin Ouyang",
                "Wenge Rong",
                "Zhang Xiong",
                "Dacheng Tao."
            ],
            "title": "Token-level self-evolution training for sequence-to-sequence learning",
            "venue": "ACL.",
            "year": 2023
        },
        {
            "authors": [
                "Maja Popovi\u0107."
            ],
            "title": "chrF: character n-gram F-score for automatic MT evaluation",
            "venue": "WMT.",
            "year": 2015
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "WMT.",
            "year": 2018
        },
        {
            "authors": [
                "Chengwei Qin",
                "Aston Zhang",
                "Zhuosheng Zhang",
                "Jiaao Chen",
                "Michihiro Yasunaga",
                "Diyi Yang"
            ],
            "title": "Is chatgpt a general-purpose natural language processing task solver",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "JMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Ricardo Rei",
                "Craig Stewart",
                "Ana C Farinha",
                "Alon Lavie."
            ],
            "title": "COMET: A neural framework for MT evaluation",
            "venue": "EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "David Vilar",
                "Markus Freitag",
                "Colin Cherry",
                "Jiaming Luo",
                "Viresh Ratnakar",
                "George Foster."
            ],
            "title": "Prompting palm for translation: Assessing strategies and performance",
            "venue": "arXiv preprint.",
            "year": 2022
        },
        {
            "authors": [
                "Jiaan Wang",
                "Yunlong Liang",
                "Fandong Meng",
                "Zhixu Li",
                "Jianfeng Qu",
                "Jie Zhou."
            ],
            "title": "Cross-lingual summarization via chatgpt",
            "venue": "arXiv preprint.",
            "year": 2023
        },
        {
            "authors": [
                "Qingyue Wang",
                "Liang Ding",
                "Yanan Cao",
                "Zhiliang Tian",
                "Shi Wang",
                "Dacheng Tao",
                "Li Guo."
            ],
            "title": "Recursively summarizing enables long-term dialogue memory in large language models",
            "venue": "arXiv preprint.",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "ICLR.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Yi Tay",
                "Rishi Bommasani",
                "Colin Raffel",
                "Barret Zoph",
                "Sebastian Borgeaud",
                "Dani Yogatama",
                "Maarten Bosma",
                "Denny Zhou",
                "Donald Metzler"
            ],
            "title": "Emergent abilities of large language models",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed H. Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint.",
            "year": 2022
        },
        {
            "authors": [
                "Changtong Zan",
                "Liang Ding",
                "Li Shen",
                "Yu Cao",
                "Weifeng Liu",
                "Dacheng Tao."
            ],
            "title": "Bridging cross-lingual gaps during leveraging the multilingual sequenceto-sequence pretraining for text generation",
            "venue": "arXiv preprint.",
            "year": 2022
        },
        {
            "authors": [
                "Changtong Zan",
                "Liang Ding",
                "Li Shen",
                "Yibin Lei",
                "Yibing Zhan",
                "Weifeng Liu",
                "Dacheng Tao."
            ],
            "title": "Unlikelihood tuning on negative samples amazingly improves zero-shot translation",
            "venue": "arXiv preprint.",
            "year": 2023
        },
        {
            "authors": [
                "Changtong Zan",
                "Keqin Peng",
                "Liang Ding",
                "Baopu Qiu",
                "Boan Liu",
                "Shwai He",
                "Qingyu Lu",
                "Zheng Zhang",
                "Chuang Liu",
                "Weifeng Liu",
                "Yibing Zhan",
                "Dacheng Tao."
            ],
            "title": "Vega-MT: The JD explore academy machine translation system for WMT22",
            "venue": "WMT.",
            "year": 2022
        },
        {
            "authors": [
                "Richard Zens",
                "Franz Josef Och",
                "Hermann Ney."
            ],
            "title": "Phrase-based statistical machine translation",
            "venue": "KI.",
            "year": 2002
        },
        {
            "authors": [
                "Wayne Xin Zhao",
                "Kun Zhou",
                "Junyi Li",
                "Tianyi Tang",
                "Xiaolei Wang",
                "Yupeng Hou",
                "Yingqian Min",
                "Beichen Zhang",
                "Junjie Zhang",
                "Zican Dong"
            ],
            "title": "A survey of large language models",
            "year": 2023
        },
        {
            "authors": [
                "Qihuang Zhong",
                "Liang Ding",
                "Juhua Liu",
                "Bo Du",
                "Dacheng Tao."
            ],
            "title": "Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert",
            "venue": "arXiv preprint.",
            "year": 2023
        },
        {
            "authors": [
                "Qihuang Zhong",
                "Liang Ding",
                "Yibing Zhan",
                "Yu Qiao",
                "Yonggang Wen",
                "Li Shen",
                "Juhua Liu",
                "Baosheng Yu",
                "Bo Du",
                "Yixin Chen"
            ],
            "title": "Toward efficient language model pretraining and downstream adaptation via self-evolution: A case study on superglue",
            "year": 2022
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba."
            ],
            "title": "Large language models are human-level prompt engineers",
            "venue": "arXiv preprint.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "task information, and domain information, and correspondingly propose an optimal temperature setting and two (simple but effective) prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP). We show that: \u2776 The performance of ChatGPT depends largely on temperature, and a lower temperature usually can achieve better performance; \u2777 Emphasizing the task information can further improve ChatGPT\u2019s performance, particularly in complex MT tasks; \u2778 Introducing domain information can elicit ChatGPT\u2019s generalization ability and improve its performance in the specific domain; \u2779 ChatGPT tends to generate hallucinations for non-English-centric MT tasks, which can be partially addressed by our proposed prompts but still need to be highlighted for the MT/NLP community. We also explore the effects of advanced in-context learning strategies and find a (negative but interesting) observation: the powerful chain-ofthought prompt leads to word-by-word translation behavior, thus bringing significant translation degradation."
        },
        {
            "heading": "1 Introduction",
            "text": "Recently, the emergence of ChatGPT1 has brought remarkable influence on natural language processing (NLP) tasks. ChatGPT is a large-scale language\n\u2217Work was done when Keqin was interning at JD Explore Academy.\n\u2020Corresponding Author. 1https://chat.openai.com\nmodel developed by OpenAI, based on InstructGPT (Ouyang et al., 2022a), that has been trained to follow instructions with human feedback. ChatGPT possesses diverse abilities of NLP, including question answering, dialogue generation, code debugging, generation evaluation, and so on (Qin et al., 2023; Zhong et al., 2023; Wang et al., 2023a; Kocmi and Federmann, 2023; Lu et al., 2023b; Wang et al., 2023b). We are particularly interested in how well ChatGPT can perform on the machine translation task.\nPrevious studies (Jiao et al., 2023; Hendy et al., 2023) on translation tasks have found that ChatGPT performs competitively with commercial translation products (e.g., Google Translate and Microsoft Translator) on high-resource languages, but has limited capabilities for low-resource and distant languages. However, they only adopt simple prompts and basic settings regardless of the significant influence of the prompts\u2019 quality (Zhou et al., 2022), which may limit ChatGPT\u2019s performance. In this paper, we aim to further elicit the capability of ChatGPT by revisiting the following three aspects and correspondingly propose an optimal temperature setting and two simple but effective prompts: Task-Specific Prompts (TSP) and Domain-Specific Prompts (DSP).\nTemperature. Temperature is an important parameter to ensure ChatGPT generates varied responses to human queries. Basically, decoding with higher temperatures displays greater linguistic variety, while the low one generates grammatically correct and deterministic text (Ippolito et al., 2019). However, for tasks with a high degree of certainty, such as machine translation, we argue, a diverse generation may impede its translation quality. We evaluate the performance of ChatGPT at different temperatures to verify its effect and find the optimal temperature setting for the following experiments.\nTask Information. ChatGPT is fine-tuned on high-quality chat datasets and thus essentially a conversational system that has a certain distance from the translation system, we argue that the task inconsistency will limit its translation ability to a certain degree. In response to this problem, we proposed Task-Specific Prompts (TSP) to further emphasize the task information to bridge the task gap, i.e., conversation and translation.\nDomain Information. Compared with traditional machine translation systems, ChatGPT can incorporate additional information, like human interactions, through the input prompts (Dong et al., 2023). We argue that such flexible interaction may alleviate some classical MT challenges, e.g., crossdomain generalization (Koehn and Knowles, 2017). We, therefore, propose Domain-Specific Prompts (DSP) to introduce the domain navigation information to elicit ChatGPT\u2019s generalization ability across different domains.\nThrough extensive experiments, we find that:\nChatGPT\u2019s performance largely depends on the temperatures, especially in difficult languages. Generally, setting a lower temperature can result in higher performance.\nEmphasizing the task information in prompts can further improve ChatGPT\u2019s performance, especially in complex tasks.\nIntroducing the correct domain information consistently improves ChatGPT\u2019s performance while wrong domain information leads to significant degradation in performance.\nWhen tackling the non-English-centric tasks (both the input and expected output are nonEnglish), ChatGPT may generate hallucinations, which should be paid more attention to by the MT/NLP community.\nFurthermore, we explore the effects of several advanced in-context learning strategies (Brown et al., 2020b). Specifically, we investigate ChatGPT\u2019s few-shot in-context learning (ICL) and chain-ofthought (CoT) (Wei et al., 2022c; Kojima et al., 2022) abilities on MT tasks. Experimental results show that few-shot ICL can further improve ChatGPT\u2019s performance, which is identical to the findings of Hendy et al. (2023), and we also find a negative but interesting observation: CoT leads to\nword-by-word translation behavior, thus bringing significant translation degradation. Also, we call for improving ICL and CoT for MT upon ChatGPT by incorporating the philosophy of example-based and statistical MT (Nagao, 1984; Koehn, 2009).\nThe remainder of this paper is designed as follows. We present the evaluation settings in Section 2. In Section 3, we revisit the performance of ChatGPT from three aspects (temperature, task, and domain information) and show the zero-shot translation performance of ChatGPT with our proposed advanced prompt recipes. Section 4 summarizes the few-shot in-context learning and chain-ofthought results. Section 6 presents conclusions."
        },
        {
            "heading": "2 Evaluation Setting",
            "text": "We provide a brief introduction of the evaluation setting, which mainly includes the used models, test set, and evaluation metrics.\nModels. We mainly compare ChatGPT2 with the commercial translation product Google Translator3, which supports translation in 133 languages. By default, the results in this paper come from the gpt3.5-turbo-0301 models, which power the ChatGPT.\nData. For multilingual translation and in-context learning, we evaluate the performance of the models on the Flores-200 (Goyal et al., 2022)4 test sets, which consists of 1012 sentences translated into 204 languages. To evaluate the effect of cross-domain translation, we adopt the test set of WMT19 Biomedical (Bawden et al., 2019), News Translation Task (Barrault et al., 2019) and WMT22 E-Commerce task (Kocmi et al., 2022). Table 1 lists the statistics of these test sets. We test all samples through OpenAI API.\nMetric. The translation metrics shared task (Freitag et al., 2022) recommends using neural network-\n2https://chat.openai.com/chat 3https://translate.google.com 4https://github.com/facebookresearch/flores\nbased metrics since they have demonstrated a high correlation with human evaluation and are resilient to domain shift. Hence, we adopt the mostly used COMET (Rei et al., 2020) as our primary metric and use the default parameters of \"cometcompare\" for significance test5. Specifically, we use the reference-based metric COMET-20 (wmt20COMET-da). Additionally, we also report BLEU scores (Papineni et al., 2002) and ChrF (Popovic\u0301, 2015) using SacreBLEU (Post, 2018) for completeness, but notably, we mainly analyze the performance in terms of model-based metric COMET."
        },
        {
            "heading": "3 Zero-Shot Translation",
            "text": "In this section, we explore the performance of ChatGPT from three aspects: TEMPERATURE, TASK INFORMATION, and DOMAIN INFORMATION, and correspondingly propose an optimal temperature setting and two simple and effective prompts to improve ChatGPT\u2019s performance."
        },
        {
            "heading": "3.1 The Effect of Temperature",
            "text": "ChatGPT is a chatting machine designed to provide fluent and diverse responses to a wide range of human requests. It is intuitive that the diversity of responses may hinder its performance on tasks with a high degree of certainty, such as machine translation, to some extent.\nTo investigate the influence of diversity, we compare the performance of ChatGPT in different temperature settings, including 0, 0.2, 0.4, 0.6, 0.8, and 1, across three translation directions: English\u21d2Romanian, English\u21d2Chinese, and English\u21d2German. The relationship between temperature and performance of ChatGPT is shown in Figure 1 and 2.\nResults. Figure 1 and 2 show that ChatGPT\u2019s performance largely depends on the value of temperatures, and as the temperature rises, there is\n5https://github.com/Unbabel/COMET\n98\n100\na clear degradation both in COMET and BLEU scores. Furthermore, it is noteworthy that ChatGPT\u2019s sensitivity to the temperature varies depending on the language pair: the impact of temperature is relatively small when translating to high-resource languages, e.g., German, while for complex languages, e.g., Chinese, it has a large degradation in performance (\u22124.3 COEMT points and \u22123.7 BLEU points for Chinese) when the temperature changes from 0 to 1. We speculate that the huge resource variance in training data leads to differences in the confidence of languages, which partially explains the different performances. In the following experiments, we adopt T = 0 as our default setting to make the most of ChatGPT and ensure the stability of generation to avoid a result of noise.\nSystem COMET BLEU ChrF COMET BLEU ChrF\nDE\u21d2EN EN\u21d2DE\nGoogle Translator 77.7 47.4 70.5 70.5 44.4 68.9 ChatGPT 77.2 43.5 69.4 69.3 40.6 67.1 ChatGPT + TSP 77.5\u2020 44.1 69.7 69.4 40.4 67.0\nthat our TSP method consistently boosts the performance of ChatGPT in most settings. Shadowed areas mean difficult English-centric translation tasks, Green areas mean non English-centric translation tasks. \u201c\u2020\u201d indicates a statistically significant difference from the ChatGPT baseline (p < 0.05)."
        },
        {
            "heading": "3.2 The Effect of Task Information",
            "text": "Previous studies (Jiao et al., 2023; Hendy et al., 2023) have shown that ChatGPT can achieve exceptional performance in conversational domain translation, which is attributed to its ability to generate more natural and diverse spoken language. However, given that ChatGPT is deliberately designed as a general task solver (Qin et al., 2023), when asking the ChatGPT to perform as a specific task engine, there will arise a task gap. This task inconsistency may limit ChatGPT\u2019s effectiveness in translation tasks other than the spoken domain.\nTo bridge the task gap and generate more translation-like sentences, we propose TaskSpecific Prompts (TSP) to emphasize the translation task information. Specifically, we prepend the sentence \"You are a machine translation system.\" to the best translation template in Jiao et al. (2023), and adopt it to query ChatGPT. The templates of prompts present in Table 2, and [TGT] represents the target languages of translation.\nWe have compared the performance of various\nmodels on four language pairs, covering eight distinct translation directions. These languages comprise 1) German, which is one of the most nonEnglish languages in the GPT training data, 2) Romanian, a less frequently encountered non-English language in the GPT training data, and 3) Chinese, a large-scale language with a script distinct from\nEnglish. We also adopt Chinese-Romanian as a non-English-centric use case. Table 3 lists the full results, where we list both English-centric and nonEnglish-centric language directions (marked with green ), and also, among English-centric directions, we highlight the difficult pairs (EN-ZH and EN-RO with shadow ) in terms of their resources and language distance."
        },
        {
            "heading": "3.2.1 English-Centric Language Pairs",
            "text": "We first consider the performance of ChatGPT in English-centric translation language pairs. Specifically, we conduct experiments in three language pairs: German\u21d4English (highresource), Romanian\u21d4English (low-resource), and Chinese\u21d4English (distant language).\nResults. Our results presented in Table 3 show that our TSP method achieves comparable results on COMET score compared to Google Translator and even outperforms it in some language pairs, e.g., English\u21d2Romanian (92.9 v.s. 91.6). We also observe that our TSP method consistently improves the performance of vanilla ChatGPT, especially when translating to low-resource or distant languages. Specifically, our TSP method brings +0.8 and +0.5 COMET score improvements in English\u21d2Chinese and English\u21d2Romanian, respectively, and +0.2 on average when translating to English. We speculate that the high-resource training data can help the model better understand the specific task from a few task-related navigations, thereby reducing the need for additional taskspecific information. Although our proposed TSP consistently improves the performance in terms of semantic metric, i.e., COMTE, notably, we have not consistently bridged the task gap in terms of lexical metrics (BLEU and ChrF), which is consistent with similar findings from Vilar et al. (2022) on PALM-540B model."
        },
        {
            "heading": "3.2.2 Non-English-Centric Language Pairs",
            "text": "We also evaluate the performance of ChatGPT in non-English-centric language pairs (since the pretraining process was dominated by the English tokens and the multilingual MT community argues it may harm the non-English-centric performance (Costa-juss\u00e0 et al., 2022; Zan et al., 2022a, 2023).). We have an important finding that,\nwhen tackling non-English-centric MT language pairs, ChatGPT tends to generate translation hallucinations, that is, some unrelated in-\nformation obeyed some patterns followed the translation, such as \"Translation may vary depending on context\", which will greatly affect the MT performance. We used a post-processing method to remove irrelevant information from the generated text. Specifically, we summarize some templates about irrelevant sentences and remove them from the generation texts. Some templates are shown in Table 4 and the number of post-processed sentences is presented in Figure 3.\nResults. Figure 3 shows that lower temperature can reduce the number of hallucinations (especially in distant languages, e.g., Chinese) and our TSP method can further reduce its number, which suggests that our method can help ChatGPT to better serve as a machine translation system. The full results on Romanian\u21d4Chinese lists are in Table 3. As seen, our TSP method can only slightly improve ChatGPT\u2019s performance, which could be due to the difficulty in both understanding and generating the language pairs. Meanwhile, our used post-editing approach could only roughly remove the hallucination patterns, the NLP/MT community should pay more attention to the potential hallucination when using ChatGPT to tackle the non-English text.\nThe subsequent experiments will use ChatGPT with TSP as the default setting."
        },
        {
            "heading": "3.3 The Effect of Domain Information",
            "text": "Compared with traditional machine translation systems, ChatGPT can incorporate additional information through the prompts to further improve its performance. While previous studies have shown that ChatGPT has great robust translation capabilities (Hendy et al., 2023), we believe that we can further enhance its performance by incorporating domain-specific guidance.\nTo this end, we propose Domain-Specific Prompts (DSP) that identify the domain informa-\ntion of translated sentences in prompts to facilitate ChatGPT\u2019s generalization. Specifically, we ask ChatGPT with the following prompts \"You are a machine translation system that translates sentences in the [DOM] domain\", as shown in Table 5. Here, [DOM] represents the correct domain of the translated sentence, while [FDOM] represents the wrong domain of that, which is used to verify whether the improvement comes from domain information. For example, for a biomedical sentence, [DOM] is biomedical, while [FDOM] can be any field except biomedical.\nWe evaluate our method on the WMT19 Bio and News datasets followed Jiao et al. (2023), which allows us to examine domain bias\u2019s impact. For example, the WMT19 Bio test set comprises Medline abstracts that require domain-specific knowledge, while the WMT19 News dataset features news-style texts that are significantly different from dialogues. To further prove the effectiveness of our method, we conduct our method on WMT22 English-Chinese E-Commerce test set, which is less likely to overlap with the GPT training data.\nResults. The results are listed in Table 6. Obviously, the original ChatGPT does not perform as well as Google Translator in both COMET and lexical metrics (e.g., BLEU). However, our DSP method can consistently improve the performance of ChatGPT in terms of COMET score and even outperforms Google Translator in two datasets (WMT19 Bio Chinese \u21d2 English and WMT19 News English \u21d2 Chinese). This finding indicates\nthat our method can further improve the generalization ability of ChatGPT and narrow the gap with one of the most advanced commercial systems \u2013 Google Translator. Nonetheless, our method\u2019s impact on BLEU is inconsistent, and it still lags significantly behind Google Translator\u2019s performance.\nTo verify that the observed improvement is indeed due to the introduction of the domain information, we deliberately provided incorrect domain information for each sentence, namely F-DSP, to attack the improvement brought by the DSP strategy. Specifically, We exchange domain information for the biomedical sentences and the news sentences. We expect that the wrong domain guidance (F-DSP) will under-perform the DSP, and even perform worse than the vanilla ChatGPT. The results of these experiments are shown in the last row of Table 6, which clearly shows a consistent degradation in COMET, proving that the domain information is the key to the success of our method.\nAll the above DSP and F-DSP results confirm the importance of domain-specific prompting guidance in using ChatGPT for MT tasks."
        },
        {
            "heading": "4 Few-shot Machine Translation",
            "text": "In this section, we simply explore the effects of advanced in-context learning (ICL) strategies, specifically, we investigate ChatGPT\u2019s few-shot ICL and Chain-of-Thought (CoT) abilities on MT tasks."
        },
        {
            "heading": "4.1 Few-Shot In-Context learning",
            "text": "In-context learning (Brown et al., 2020b) has shown its remarkable ability for many NLP tasks (Liu et al., 2023). To further explore the capabilities of the ChatGPT, we conduct experiments with different sample selection strategies. Specifically, we evaluate the performance of few-shot machine translation in the following three directions: English\u21d2Chinese, English\u21d2Romanian, and English\u21d2German in Flores-200. We conducted experiments with randomly and TopK (Liu et al., 2022) sampled demonstrations from development sets in the 1-shot and 3-shot settings.\nResults. Our results are listed in Table 7. As seen, in-context learning with random examples consistently improves the performance in both lexical metric (BLEU) and COMET score compared to the zero-shot approach, and increasing the number of shots can lead to further improvement, which is consistent with previous finding (Hendy et al., 2023). The advanced sample-selection strategy like\nTopK, which chooses test-sample similar examples as demonstrations, can further improve the performance, even outperform Google Translator in some language pairs, e.g., English\u21d2Romanian (94.0 v.s. 91.6) and English\u21d2Chinese (68.8 v.s. 68.5).\nWe encouragingly find that the advanced sampleselection strategy for in-context learning for MT tasks upon ChatGPT is extremely similar to the design philosophy of example-based machine translation (EBMT, Nagao, 1984), where the EBMT is often characterized by its use of a bilingual corpus as its main knowledge base, at run-time. It is worthy of designing better ICL strategies inspired by EBMT in future work."
        },
        {
            "heading": "4.2 Chain-of-Thought",
            "text": "Chain-of-Thought (CoT) prompting (Wei et al., 2022c) has been demonstrated to be effective in eliciting the reasoning ability of large language models. Previous studies have shown that CoT can improve the ChatGPT\u2019s performance in natural language understanding tasks (Zhong et al., 2023),\nbut its influence on machine translation tasks has hardly been investigated.\nTo investigate this further, we randomly select 20 samples from the test set and adopt the zero-shot CoT technique (Kojima et al., 2022) and the 1-shot CoT technique. Specifically, as shown in Table 8, for zero-shot CoT, we use the prompt \"Please provide the [TGT] translation for the following sentence step by step\" to extract step-by-step translation. We also add the sentence \u2018and then provide the complete sentence:\u2019 to the end of the prompting to ensure that ChatGPT can generate the complete translation. While for the 1-shot CoT, we provide the manual intermediate reasoning steps inspired by zero-shot CoT, as shown in Table 8. Here, [S] and [T] represent the corresponding source and target sentence in the demonstration, respectively, and [S_i] and [T_i] are the i-th matching tokens in the source and target sentence.\nResults. We conduct experiments in the following two translation directions: English\u21d2German\nand English\u21d2Chinese. The results are listed in Table 9, which shows that there is a significant degradation in COMET score with zero-shot CoT setting, especially in English\u21d2Chinese, which drops 8.8 COMET points. 1-shot CoT prompting can consistently outperform zero-shot CoT but still lags behind zero-shot prompting on COMET.\nWe looked in detail at the sentences generated by different prompts, presented in Table 10, and we have a negative but interesting observation: the CoT prompt leads to word-by-word translation behavior, which is the main reason for the significant translation degradation.\nFor more CoT variants designed with different principles inspired by the philosophy in statistical MT (Zens et al., 2002; Koehn, 2009) will be explored in the future. For example, word-by-word and then reordering the translation (Du and Way, 2017; Ding et al., 2020), phrase-to-phrase (Feng et al., 2018; Ding et al., 2021) and then reordering the translation, and structure-to-structure transla-\ntion (Kaplan et al., 1989)."
        },
        {
            "heading": "5 Related Work",
            "text": "Large Language Models. Large language models (LLMs) usually refer to language models with hundreds of billions of parameters, which are trained on massive text data (Zhao et al., 2023). LLMs usually can be classified into three groups based on model architectures: 1) encoder-only LLMs (Devlin et al., 2019; Liu et al., 2019; Zhong et al., 2022), usually used for NLU tasks; 2) decoder-only LLMs (Radford et al., 2019; Brown et al., 2020a), more suitable for NLG tasks; and 3) encoder-decoder LLMs (Raffel et al., 2020; Lewis et al., 2020; Zan et al., 2022b; Peng et al., 2023), which can achieve better performance on conditional text generation tasks.\nTraditionally, these PLMs can achieve remarkable performance in various natural language processing (NLP) tasks through fine-tuning on specific tasks. But with the scaling up and the development of LLMs (Brown et al., 2020a; Ouyang et al., 2022b), decoder-only LLMs exhibit remarkable zero-shot and few-shot abilities, denoted emergent abilities (Wei et al., 2022b), and achieve comparable results with other LLMs in NLU and conditional NLG tasks. Especially the emergency of ChatGPT, developed by OpenAI, takes LLMs a big step forward in both academia and industry. ChatGPT possesses diverse abilities of NLP and can generate human-like responses by instructiontuning (Wei et al., 2022a) and Reinforcement Learning from Human Feedback (RLHF) technique (Ouyang et al., 2022b).\nChatGPT for Machine Translation. The ability of ChatGPT has been widely studied in various domains (Qin et al., 2023; Zhong et al., 2023), but its ability on machine translation tasks has not been fully investigated. Jiao et al. (2023) and Hendy et al. (2023) first provided an evaluation on the performance of ChatGPT for machine translation, they found that ChatGPT can perform competitively with commercial translation products on high-resource European languages but lags behind significantly on low resource or distant languages. However, they usually adopt simple prompts and basic settings which cannot fully exploit the capabilities of ChatGPT, we first proposed that ChatGPT can achieve comparable results with proper settings and investigate how to make the most of ChatGPT for machine translation.\nSubsequent work follows our work to further explore the performance of ChatGPT, Gao et al. (2023) and Lu et al. (2023a) introduce new information (e.g., POS or multilingual dictionaries), He et al. (2023) proposed a CoT-like framework to generation human-like translation."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we investigate how to further mine ChatGPT\u2019s translation ability from three perspectives, namely temperature, task, and domain information, and correspondingly propose an optimal temperature setting and two simple but effective prompts. We empirically demonstrated that there is a high correlation between temperature and ChatGPT\u2019s performance, and a lower temperature usually can achieve better performance. Experimental results across various language pairs and domains proved the effectiveness of our proposed prompts. We further explore the effectiveness of advanced in-context learning strategies for ChatGPT, we find that the few-shot in-context learning method can consistently improve ChatGPT\u2019s performance, while conventional Chain-of-Thought (CoT) prompting will degrade its performance because of its word-by-word translation behavior.\nIn future work, besides the aforementioned explorations (EBMT-inspired prompts designing, statistical MT-inspired chain-of-thought designing), we would like to investigate how to further elicit the ability of ChatGPT by designing more effective prompts (e.g., design human-like CoT to navigate the LLMs, and better demonstration selection algorithms in few-shot ICL) and investigate the ability of ChatGPT for more MT settings (e.g., document translation).\nLimitations\nOur work has several potential limitations. First, we only propose some simple prompts that have not been carefully designed to investigate the capabilities of ChatGPT, which may not sufficiently elicit the power of ChatGPT. Second, we have not fully studied the performance of ChatGPT in fewshot scenarios, especially the effect of Chain-OfThought in machine translation. In future work, we would like to design different types of prompts to further improve ChatGPT\u2019s performance in machine translation and conduct more in-depth analyses and discussions.\nEthics Statement\nWe take ethical considerations very seriously and strictly adhere to the EMNLP Ethics Policy. This paper focuses on exploring the translation ability of ChatGPT on open-sourced machine translation datasets, not involving any ethics problem. Both the compared models and evaluation datasets used in this paper are publicly available and have been widely adopted by researchers. Therefore, we believe that this research will not pose ethical issues."
        }
    ],
    "title": "Towards Making the Most of ChatGPT for Machine Translation",
    "year": 2023
}