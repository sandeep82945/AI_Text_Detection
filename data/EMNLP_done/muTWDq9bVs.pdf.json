{
    "abstractText": "We propose Speculative Decoding (SpecDec), for the first time ever1, to formally study exploiting the idea of speculative execution to accelerate autoregressive (AR) decoding. Speculative Decoding has two innovations: SpecDrafter \u2013 an independent model specially optimized for efficient and accurate drafting \u2013 and Spec-Verification \u2013 a reliable method for verifying the drafted tokens efficiently in the decoding paradigm. Experimental results on various seq2seq tasks including machine translation and abstractive summarization show our approach can achieve around 5\u00d7 speedup for the popular Transformer architectures with comparable generation quality to beam search decoding, refreshing the impression that the draft-then-verify paradigm introduces only 1.4\u00d7\u223c2\u00d7 speedup. In addition to the remarkable speedup, we also demonstrate 3 additional advantages of SpecDec, revealing its practical value for accelerating generative models in real-world applications. Our models and codes are available at https://github.com/ hemingkx/SpecDec.",
    "authors": [
        {
            "affiliations": [],
            "name": "Heming Xia"
        },
        {
            "affiliations": [],
            "name": "Tao Ge"
        },
        {
            "affiliations": [],
            "name": "Peiyi Wang"
        },
        {
            "affiliations": [],
            "name": "Si-Qing Chen"
        },
        {
            "affiliations": [],
            "name": "Furu Wei"
        },
        {
            "affiliations": [],
            "name": "Zhifang Sui"
        }
    ],
    "id": "SP:85db9f5e4033c02ec6e3eecb311adab11cfaa750",
    "references": [
        {
            "authors": [
                "Yu Bao",
                "Shujian Huang",
                "Tong Xiao",
                "Dongqi Wang",
                "Xinyu Dai",
                "Jiajun Chen."
            ],
            "title": "Nonautoregressive translation by learning target categorical codes",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Charlie Chen",
                "Sebastian Borgeaud",
                "Geoffrey Irving",
                "Jean-Baptiste Lespiau",
                "Laurent Sifre",
                "John Jumper."
            ],
            "title": "Accelerating large language model decoding with speculative sampling",
            "venue": "CoRR, abs/2302.01318.",
            "year": 2023
        },
        {
            "authors": [
                "Mengyun Chen",
                "Tao Ge",
                "Xingxing Zhang",
                "Furu Wei",
                "Ming Zhou."
            ],
            "title": "Improving the efficiency of grammatical error correction with erroneous span detection and correction",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Tao Ge",
                "Si-Qing Chen",
                "Furu Wei."
            ],
            "title": "Edgeformer: A parameter-efficient transformer for ondevice seq2seq generation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi,",
            "year": 2022
        },
        {
            "authors": [
                "Tao Ge",
                "Heming Xia",
                "Xin Sun",
                "Si-Qing Chen",
                "Furu Wei."
            ],
            "title": "Lossless acceleration for seq2seq generation with aggressive decoding",
            "venue": "arXiv preprint arXiv:2205.10350.",
            "year": 2022
        },
        {
            "authors": [
                "Xinwei Geng",
                "Xiaocheng Feng",
                "Bing Qin."
            ],
            "title": "Learning to rewrite for non-autoregressive neural machine translation",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event",
            "year": 2021
        },
        {
            "authors": [
                "Marjan Ghazvininejad",
                "Vladimir Karpukhin",
                "Luke Zettlemoyer",
                "Omer Levy."
            ],
            "title": "Aligned cross entropy for non-autoregressive machine translation",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July",
            "year": 2020
        },
        {
            "authors": [
                "Marjan Ghazvininejad",
                "Omer Levy",
                "Yinhan Liu",
                "Luke Zettlemoyer."
            ],
            "title": "Mask-predict: Parallel decoding of conditional masked language models",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and",
            "year": 2019
        },
        {
            "authors": [
                "Jiatao Gu",
                "James Bradbury",
                "Caiming Xiong",
                "Victor O.K. Li",
                "Richard Socher."
            ],
            "title": "Non-autoregressive neural machine translation",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Jiatao Gu",
                "Xiang Kong."
            ],
            "title": "Fully nonautoregressive neural machine translation: Tricks of the trade",
            "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume",
            "year": 2021
        },
        {
            "authors": [
                "Karl Moritz Hermann",
                "Tom\u00e1s Kocisk\u00fd",
                "Edward Grefenstette",
                "Lasse Espeholt",
                "Will Kay",
                "Mustafa Suleyman",
                "Phil Blunsom."
            ],
            "title": "Teaching machines to read and comprehend",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2015
        },
        {
            "authors": [
                "Fei Huang",
                "Hao Zhou",
                "Yang Liu",
                "Hang Li",
                "Minlie Huang."
            ],
            "title": "Directed acyclic transformer for nonautoregressive machine translation",
            "venue": "International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, vol-",
            "year": 2022
        },
        {
            "authors": [
                "Jungo Kasai",
                "James Cross",
                "Marjan Ghazvininejad",
                "Jiatao Gu."
            ],
            "title": "Non-autoregressive machine translation with disentangled context transformer",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July",
            "year": 2020
        },
        {
            "authors": [
                "Jungo Kasai",
                "Nikolaos Pappas",
                "Hao Peng",
                "James Cross",
                "Noah A. Smith."
            ],
            "title": "Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual",
            "year": 2021
        },
        {
            "authors": [
                "Sehoon Kim",
                "Karttikeya Mangalam",
                "Jitendra Malik",
                "Michael W. Mahoney",
                "Amir Gholami",
                "Kurt Keutzer."
            ],
            "title": "Speculative decoding with big little decoder",
            "venue": "CoRR, abs/2302.07863.",
            "year": 2023
        },
        {
            "authors": [
                "Yoon Kim",
                "Alexander M Rush."
            ],
            "title": "Sequencelevel knowledge distillation",
            "venue": "arXiv preprint arXiv:1606.07947.",
            "year": 2016
        },
        {
            "authors": [
                "Jason Lee",
                "Elman Mansimov",
                "Kyunghyun Cho."
            ],
            "title": "Deterministic non-autoregressive neural sequence modeling by iterative refinement",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels,",
            "year": 2018
        },
        {
            "authors": [
                "Yaniv Leviathan",
                "Matan Kalman",
                "Yossi Matias."
            ],
            "title": "Fast inference from transformers via speculative decoding",
            "venue": "International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings",
            "year": 2023
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jindrich Libovick\u00fd",
                "Jindrich Helcl."
            ],
            "title": "Endto-end non-autoregressive neural machine translation with connectionist temporal classification",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels,",
            "year": 2018
        },
        {
            "authors": [
                "Eric Malmi",
                "Sebastian Krause",
                "Sascha Rothe",
                "Daniil Mirylenka",
                "Aliaksei Severyn."
            ],
            "title": "Encode, tag, realize: High-precision text editing",
            "venue": "arXiv preprint arXiv:1909.01187.",
            "year": 2019
        },
        {
            "authors": [
                "Xupeng Miao",
                "Gabriele Oliaro",
                "Zhihao Zhang",
                "Xinhao Cheng",
                "Zeyu Wang",
                "Rae Ying Yee Wong",
                "Zhuoming Chen",
                "Daiyaan Arfeen",
                "Reyna Abhyankar",
                "Zhihao Jia"
            ],
            "title": "Specinfer: Accelerating generative LLM serving with speculative inference and token",
            "year": 2023
        },
        {
            "authors": [
                "Kostiantyn Omelianchuk",
                "Vitaliy Atrasevych",
                "Artem Chernodub",
                "Oleksandr Skurzhanskyi."
            ],
            "title": "Gector\u2013grammatical error correction: tag, not rewrite",
            "venue": "arXiv preprint arXiv:2005.12592.",
            "year": 2020
        },
        {
            "authors": [
                "Myle Ott",
                "Sergey Edunov",
                "David Grangier",
                "Michael Auli."
            ],
            "title": "Scaling neural machine translation",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 - November 1, 2018, pages 1\u20139.",
            "year": 2018
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association",
            "year": 2002
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 - November 1, 2018, pages 186\u2013191. Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Lihua Qian",
                "Hao Zhou",
                "Yu Bao",
                "Mingxuan Wang",
                "Lin Qiu",
                "Weinan Zhang",
                "Yong Yu",
                "Lei Li."
            ],
            "title": "Glancing transformer for nonautoregressive neural machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the",
            "year": 2021
        },
        {
            "authors": [
                "Ricardo Rei",
                "Craig Stewart",
                "Ana C. Farinha",
                "Alon Lavie."
            ],
            "title": "COMET: A neural framework for MT evaluation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,",
            "year": 2020
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Mohammad Norouzi."
            ],
            "title": "Non-autoregressive machine translation with latent alignments",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP",
            "year": 2020
        },
        {
            "authors": [
                "Andrea Santilli",
                "Silvio Severino",
                "Emilian Postolache",
                "Valentino Maiorca",
                "Michele Mancusi",
                "Riccardo Marin",
                "Emanuele Rodol\u00e0."
            ],
            "title": "Accelerating transformer inference for translation via parallel decoding",
            "venue": "Proceedings of the 61st Annual Meeting",
            "year": 2023
        },
        {
            "authors": [
                "Robin M. Schmidt",
                "Telmo Pires",
                "Stephan Peitz",
                "Jonas L\u00f6\u00f6f."
            ],
            "title": "Non-autoregressive neural machine translation: A call for clarity",
            "venue": "CoRR, abs/2205.10577.",
            "year": 2022
        },
        {
            "authors": [
                "Rico Sennrich",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Neural machine translation of rare words with subword units",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin,",
            "year": 2016
        },
        {
            "authors": [
                "Chenze Shao",
                "Yang Feng."
            ],
            "title": "Non-monotonic latent alignments for ctc-based non-autoregressive machine translation",
            "venue": "CoRR, abs/2210.03953.",
            "year": 2022
        },
        {
            "authors": [
                "Kim M. Hazelwood"
            ],
            "title": "Sustainable AI: envi",
            "year": 2022
        },
        {
            "authors": [
                "Jun Zhang",
                "Jue Wang",
                "Huan Li",
                "Lidan Shou",
                "Ke Chen",
                "Gang Chen",
                "Sharad Mehrotra."
            ],
            "title": "Draft & verify: Lossless large language model acceleration via self-speculative decoding",
            "venue": "CoRR, abs/2309.08168.",
            "year": 2023
        },
        {
            "authors": [
                "Zaixiang Zheng",
                "Hao Zhou",
                "Shujian Huang",
                "Jiajun Chen",
                "Jingjing Xu",
                "Lei Li."
            ],
            "title": "Duplex sequence-to-sequence learning for reversible machine translation",
            "venue": "Advances in Neural Information Processing Systems 34: Annual",
            "year": 2021
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Kaifeng Lyu",
                "Ankit Singh Rawat",
                "Aditya Krishna Menon",
                "Afshin Rostamizadeh",
                "Sanjiv Kumar",
                "Jean-Fran\u00e7ois Kagy",
                "Rishabh Agarwal"
            ],
            "title": "Distillspec: Improving speculative decoding via knowledge distillation",
            "year": 2023
        },
        {
            "authors": [
                "Vaswani"
            ],
            "title": "2018), we also average model parameters from the last 10 checkpoints. B Memory Analysis B.1 Additional Memory Cost by SpecDec",
            "venue": "Drafter",
            "year": 2018
        },
        {
            "authors": [
                "Schmidt"
            ],
            "title": "2022) pointed out that inconsistencies in the use of tokenized BLEU lead to deviations of up to 1.8 BLEU points",
            "year": 2022
        },
        {
            "authors": [
                "Wu"
            ],
            "title": "2022) to compute the carbon emission of the two decoding strategies under the same device",
            "year": 2022
        },
        {
            "authors": [
                "Leviathan"
            ],
            "title": "2023)\u2019s speedup results are lower than 3\u00d7. The reasons for our approach significantly outperforming Leviathan et al",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "As the de facto method for text generation, AutoRegressive (AR) decoding is widely blamed for its poor inference efficiency due to its low level of parallelism, which fails to utilize the full potential of modern parallel computing devices like GPUs. This inefficiency not only leads to high deployment\n\u2217 This work was done during the author\u2019s internship at MSR Asia. Correspondence: Tao Ge (tage@microsoft.com)\n\u2020Co-first authors with equal contributions 1This work was initially announced in March 2022 (https://arxiv.org/abs/2203.16487) under the name Generalized Aggressive Decoding. It has been formally renamed Speculative Decoding in our submission to ICLR\u201923, which has been publicly available since September 2022 at https://openreview.net/pdf?id=H-VlwsYvVi. This marks the first time \"Speculative Decoding\", which explicitly studies the idea of speculative execution to accelerate Transformer inference, has been proposed.\ncosts but also limits the application of advanced AR models in real-time scenarios.\nIn this work, we study the draft-then-verify paradigm for accelerating seq2seq generation of an existing AR model2. As shown in Figure 1, the draft-then-verify paradigm first generates a number of drafted tokens efficiently and then verifies these tokens using the existing AR model in parallel to ensure the decoding result matches AR decoding. However, previous attempts in the \u201cdraftthen-verify\u201d paradigm such as Blockwise Decoding (Stern et al., 2018) and Aggressive Decoding (Sun et al., 2021) tend to lack in-depth investigation of this paradigm. Their modest speedup (i.e., 1.4\u00d7\u223c2.0\u00d7) or limitation to certain seq2seq tasks like Grammatical Error Correction (GEC) has caused this paradigm to be underestimated, resulting in it not receiving much attention and remaining dormant for years.\nTo fully exploit the draft-then-verify paradigm, we propose Speculative Decoding (SpecDec), drawing inspiration from speculative execution3 in com-\n2The existing AR model in this paper refers to the targeted Transformer using AR decoding that we want to accelerate.\n3Speculative execution is an optimization technique used in computer architecture where a system performs some task in advance to avoid delays that would have to be incurred by doing the task after it is known that it is required (https: //wikipedia.org/wiki/Speculative_execution).\nputer architecture, with two key innovations that improve drafting and verification processes respectively. For drafting, we derive two principles for designing the drafting model4: the Capability Principle and the Latency Principle. Following these two principles, we propose Spec-Drafter \u2013 a specialized independent model optimized in the draftthen-verify paradigm, which can accurately and efficiently fulfill the drafting task.\nFor verification, we propose an advanced method \u2013 Spec-Verification that relaxes the vanilla verification strategy. Spec-Verification allows the decoding results of SpecDec to be slightly different from AR greedy decoding, offering an opportunity to accept more drafted tokens without sacrificing generation quality and leading to higher decoding efficiency.\nWe conduct extensive experiments on various seq2seq generation tasks like machine translation and abstractive summarization. Results show our approach can achieve around 5\u00d7 speedup for the popular Transformer architectures with comparable generation quality to beam search decoding, largely outperforming previous draft-thenverify work (1.4\u00d7\u223c2.0\u00d7 speedup). Moreover, we demonstrate that SpecDec has several additional advantages that enhance its practicality for accelerating generative models in real-world applications.\nOur contributions can be summarized as follows:\n\u2022 We are the first work that explicitly exploits the idea of speculative execution to accelerate Transformer inference. Our proposed two key innovations \u2013 the independent Spec-Drafter and Spec-Verification strategy allow SpecDec to achieve over 5\u00d7 lossless speedup over autoregressive decoding in seq2seq tasks, refreshing the impression that the \u201cdraft-thenverify\u201d paradigm only has a limited 1.5\u00d7 \u223c 2\u00d7 acceleration potential.\n\u2022 We demonstrate 3 advantages of SpecDec with extensive empirical results in addition to its remarkable acceleration performance: better latency-throughput trade-off, easy adaptability for existing models and retaining the behavior of the original model, revealing its huge practical value and bringing the longdormant draft-then-verify paradigm back into the spotlight.\n4The drafting model is also called the drafter in this paper.\n2 Background: draft-then-verify decoding\nThe \u201cdraft-then-verify\u201d paradigm first drafts multiple tokens efficiently, as a speculation of AR decoding results; then, it verifies these tokens in parallel to ensure they match the AR decoding result, as illustrated in Figure 1. It is an implicit implementation of speculative execution in Transformer inference.\nDraft There are different approaches to drafting tokens, including model-based (Stern et al., 2018) and input-(context-) based5 methods (Sun et al., 2021; Yang et al., 2023). Take Blockwise Decoding \u2013 the most representative work attempting the draft-then-verify paradigm \u2013 as an example (illustrated in Figure 2(a)): it introduces additional k\u2212 1 feedforward network (FFN) heads on top of an existing AR model, enabling the model to predict the next k drafted tokens in parallel during inference.\nVerify The generated drafted tokens are fed into the original AR model and verified in parallel. Specifically, it finds the bifurcation position c, the largest index that ensures all previous c\u2212 1 drafted tokens and the corresponding AR decoded tokens are identical:\nc = argmax i I(y\u0303j+i \u0338= y\u0302j+i) i , 1 \u2264 i \u2264 k (1)\ny\u0302j+i = argmax y logP (y | y\u0302\u2264j , y\u0303j+1\u00b7\u00b7\u00b7j+i\u22121,x;\u03b8AR) (2)\nwhere I(\u00b7) is the indicator function, x is the source sentence, y\u0302\u2264j is the previously generated tokens6 and y\u0303j+i is the i-th drafted token. Drafted tokens after the position c are all discarded. The final decoded tokens in the current iteration are:\ny\u0302j+1\u00b7\u00b7\u00b7j+c = (y\u0303j+1\u00b7\u00b7\u00b7j+c\u22121, y\u0302j+c) (3)\nThe above draft and verification steps are iterated until the termination condition is met."
        },
        {
            "heading": "3 Speculative Decoding",
            "text": "To fully exploit speculative execution for Transformer inference, we propose Speculative Decoding (SpecDec) with two innovations \u2013 Spec-Drafter and Spec-Verification that substantially improve drafting (Section 3.1) and verification (Section 3.2) respectively.\n5This kind of method is usually limited to special tasks like GEC and retrieval-augmented generation. In this paper, we mainly focus on the model-based methods.\n6We use y\u0303 to denote drafted tokens, while we use y\u0302 to denote AR decoded/verified generation results."
        },
        {
            "heading": "3.1 Spec-Drafter",
            "text": ""
        },
        {
            "heading": "3.1.1 Design Principles",
            "text": "As a crucial ingredient in the draft-then-verify paradigm, the drafting process has a drastic impact on end-to-end acceleration performance. However, there are very limited explorations of the designing principles for the drafter by previous studies \u2013 most of them arbitrarily implement a drafter, which accounts for their undesirable acceleration results.\nTo understand the effect of drafting, we look into the overall latency in the draft-then-verify paradigm for one sample of the length L as follows:\nT = L\nTok. \u00d7 td\ufe38 \ufe37\ufe37 \ufe38\ntotal drafting latency\n+ L\nTok. \u00d7 tv\ufe38 \ufe37\ufe37 \ufe38\ntotal verification latency\n(4)\nwhere Tok. denotes the average number of drafted tokens accepted per iteration, td and tv are the time costs of drafting and verification7 each iteration respectively.\nAccording to Eq (4), Tok. is inversely proportional to the number of iterations, which is primarily influenced by drafting accuracy: A drafter that is more capable of drafting can attain greater Tok. values, consequently completing the decoding process in fewer iterations. This observation leads us to derive the first principle for designing the drafter:\nPrinciple I (Capability Principle): The drafter model should be seriously in-\n7We don\u2019t discuss tv in this paper because it is determined by the existing AR model and thus regarded constant.\nvested to guarantee its capability of accurate drafting.\nPrinciple I is the most crucial principle in determining the end-to-end speedup, as it directly influences the value of Tok. which affects both total drafting and verification latency. Surprisingly, little previous work adheres to this seemingly simple and straightforward principle maybe due to the concern of increasing the drafting latency. For instance, the drafter in Blockwise Decoding is not properly invested: Its drafter not only has limited parameters (FFN prediction heads), making it difficult to fit the challenging drafting task, but more importantly, it employs a shared attention mechanism that forces all drafted tokens to share a single set of attentions (only differentiating at the final prediction head), as shown in Figure 2(a). However, different target positions should attend different context tokens, as illustrated in Figure 3. Despite its computational efficiency, the shared attention mechanism in Blockwise decoding severely constrains the drafter\u2019s capability, resulting in low drafting accuracy and consequently leading to most drafted tokens being discarded.\nIn addition to the drafter\u2019s accuracy, its latency also impacts the end-to-end speedup result but from another perspective \u2013 by affecting the latency of each iteration (i.e., td in Eq (4)) \u2013 from which we derive Principle II for designing the drafter:\nSource Sentence Machen sich Hunderte Millionen von Autofahrern sorgen \u00fcber ihre Privatsph\u00e4re.\nPrinciple II (Latency Principle): The drafter should be fast at generating drafted tokens to minimize the latency overhead of each iteration.\nDesigning a fast drafter solely based on Principle II is not difficult, as done in most previous work. The real challenge lies in designing a lowlatency drafter without compromising its capability (Principle I), since it is difficult to achieve both low latency and high capability simultaneously."
        },
        {
            "heading": "3.1.2 Model Architecture",
            "text": "We propose Spec-Drafter, which adheres to both principles for accurate and fast drafting. To ensure the drafter is sufficiently capable of accurate drafting (Principle I), Spec-Drafter employs an independent encoder-decoder model architecture, which generates drafted tokens conditioned on the leftward context and source tokens in a mask-predict manner (Ghazvininejad et al., 2019), as illustrated in Figure 2(b). This independent model design facilitates Spec-Drafter to predict each drafted token using distinct attention queries, in contrast to Blockwise Decoding employing a shared attention query for predicting all drafted tokens (as illustrated in Figure 2). In this way, Spec-Drafter could better align with the AR model\u2019s behavior, thereby increasing the chances of its drafted tokens being accepted during verification, as shown in Figure 3.\nTo make Spec-Drafter fast (Principle II) without compromising its capability, we design its decoder to be lightweight by reducing the number of decoder layers and reallocating the freed-up budget to\nits encoder (by increasing its depth), which is motivated by the fact that the encoder is forwarded only once, while the decoder is frequently forwarded for iterative decoding. This encoder-favored modeling has been demonstrated by previous work to improve latency with little generation quality degradation (Kasai et al., 2021; Sun et al., 2021; Ge et al., 2022a). We find it also highly effective for the drafter in the draft-then-verify decoding paradigm."
        },
        {
            "heading": "3.1.3 Training and Inference",
            "text": "Formally, given the source sentence x and the randomly sampled prefix y\u2264p (0 \u2264 p < m) of the target sentence, Spec-Drafter appends k special \u201c[MASK]\u201d tokens to y\u2264p, and is trained to predict these masked tokens in parallel:\nLSpec-Drafter = p+k\u2211\ni=p+1\nlogP ( yi | yk\u2264p,x;\u03b8Spec-Drafter ) (5)\nyk\u2264p = (y1, \u00b7 \u00b7 \u00b7 , yp, [MASK], \u00b7 \u00b7 \u00b7 , [MASK]\ufe38 \ufe37\ufe37 \ufe38 \u00d7k ) (6)\nIn addition, we leverage the glancing strategy following Qian et al. (2021), which exploits curriculum learning during training to get better generation performance.\nDuring inference, Spec-Drafter appends k \u201c[MASK]\u201d tokens to the previously decoded tokens y\u0302\u2264j and simultaneously predict these masked tokens as a drafted block:\ny\u0303j+i = argmax y\nlogP ( y | y\u0302k\u2264j ,x;\u03b8Spec-Drafter ) (7)\nwhere i = 1, . . . , k."
        },
        {
            "heading": "3.2 Spec-Verification",
            "text": "As introduced in Section 2, the vanilla verification strategy of preliminary studies only accepts the drafted tokens that match the top-1 result of the AR model, which guarantees that the decoding results are identical to AR greedy decoding. However, the top-1 results are not necessarily better than the drafted tokens, especially when the paradigm is equipped with a high-quality drafter. Therefore, the strict verification criterion (i.e., top-1 matching) will result in many good drafted tokens being discarded just because they are different from the top-1 result of the AR model, which limits the speedup of the paradigm.\nTo make better use of the drafting results, we propose an advanced verification strategy named Spec-Verification, which is illustrated in Figure 4. Instead of the rigid matching requirement shown in Eq (2), Spec-Verification relaxes the criterion to trust the drafting results more, by only requiring the drafted tokens to fall in top-\u03b2 candidates with a tolerable (log-likelihood) score gap \u03c4 (away from the top-1 result). Formally, it will accept the i-th drafted token y\u0303j+i if all previous i\u2212 1 tokens are accepted, and Eq (8) and (9) are both true:\nlogP (y\u0303j+i|\u25b3;\u03b8AR) \u2265 logP (y\u0302(\u03b2)j+i|\u25b3;\u03b8AR), (8)\nlogP (y\u0302 (1) j+i|\u25b3;\u03b8AR)\u2212 logP (y\u0303j+i|\u25b3;\u03b8AR) \u2264 \u03c4, (9)\n\u25b3 = y\u0302\u2264j , y\u0303j+1\u00b7\u00b7\u00b7j+i\u22121,x, (10)\nwhere logP (y\u0302(\u03b2)j+i|\u25b3;\u03b8AR) is the top-\u03b2 ranked result\u2019s log-likelihood score by the AR model."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Datasets and Evaluation We mainly evaluate our approach on two standard machine translation benchmarks: WMT14 EN\u2194DE (4.5M pairs) and WMT16 EN\u2194RO (610K pairs). Following prior work (Ott et al., 2018), for WMT14 EN\u2194DE translation, we adopt newstest-13 as our validation set for finding the best hyperparameters, and test on newstest-14. For WMT16 EN\u2194RO translation, we use the dataset released by Lee et al. (2018), where newsdev2016 and newstest2016 are taken as validation and test sets. We use 32K Byte Pair Encoding (BPE) (Sennrich et al., 2016) subwords8\n8We use the same BPE tokenization and vocabulary as Ghazvininejad et al. (2019).\nas the joint source-target dictionary. We evaluate performance with BLEU (Papineni et al., 2002) for both language pairs9.\nFor inference efficiency, we report decoding speedup over beam search. Specifically, we test the inference speed by running the model with one sentence at a time (batch=1). We perform model inference with fairseq implementation10 using Pytorch 1.10.1 with 1 Nvidia Tesla P100-PCIe of 16GB GPU memory under CUDA 11.1.\nModel Configuration The primary target model we accelerate in our experiments is the Transformer-base model with a 6-layer encoder and a 6-layer decoder of 512/2048 embedding/FFN dimension, which can achieve state-of-the-art results on the benchmarks under comparable model size conditions. For the Spec-Drafter, we adopt a similar architecture to the AR model except with 12 encoder layers and 2 decoder layers to make sure it adheres to both the Capability and Latency principles. We apply sequence-level knowledge distillation (Kim and Rush, 2016) by the AR teacher to the Spec-Drafter to align its behavior with the AR model as much as possible. We include model training details in Appendix A. For the Spec-Verification, we find the hyperparameters \u03b2 and \u03c4 leading to the best generation quality on the validation set. Besides, we re-implement Blockwise Decoding11 using the same device and environment as ours to facilitate fair comparison."
        },
        {
            "heading": "4.2 Results",
            "text": "We present the performance and the acceleration effect of SpecDec to Transformer in Table 1. As reported in the previous work (Stern et al., 2018), Blockwise Decoding (k = 10) can only achieve 1.4\u00d7\u223c2\u00d7 speedup without affecting the generation results over the Transformer-base model. Further increasing the parallel capability of Blockwise Decoding (e.g., k = 25) will not introduce more speedup as its limited drafting accuracy prevents more drafted tokens from being accepted. In contrast, our SpecDec shows consistent performance improvement with increased parallel capabilities (k = 10 \u2192 k = 25), resulting in around\n9We also report sacreBLEU (Post, 2018) and COMET (Rei et al., 2020) scores in Appendix C.\n10https://github.com/pytorch/fairseq 11In the original paper of Blockwise Decoding, there is also a variation that allows the AR model to be fine-tuned for better drafting tokens. We don\u2019t discuss this variation because it severely affects the generation quality.\n4.6\u00d7\u223c5.5\u00d7 speedup across the translation benchmarks; moreover, it even achieves an improvement in generation quality (by the BLEU metric) compared with AR greedy decoding.\nSimilar results are also observed when accelerating the Transformer with a 12-layer encoder and 2-layer decoder \u2013 SpecDec can still achieve around 2.5\u00d7\u223c3.3\u00d7 speedup while Blockwise Decoding\u2019s acceleration effect becomes nearly negligible over the fast AR baseline."
        },
        {
            "heading": "4.3 Analysis",
            "text": "In this section, we conduct a comprehensive and thorough analysis, to demonstrate that the significant improvement of the SpecDec arises from both the Spec-Drafter (Section 4.3.1) and SpecVerification (Section 4.3.2)."
        },
        {
            "heading": "4.3.1 Drafting",
            "text": "According to Table 2, the Spec-Drafter significantly outperforms the head-based drafter (as used in Blockwise Decoding) in terms of both end-to-\nend generation quality and efficiency. To further investigate the Spec-Drafter, we conduct an ablation study on the principles it follows. Ablating the Capability Principle by reducing its size results in a drastic drop in end-to-end acceleration performance, as more iterations are needed to complete the decoding process, indicated by a lower Tok.. When we ablate the Latency Principle by using a balanced (6+6) encoder-decoder architecture for\nthe drafter, it also experiences a substantial decline in end-to-end acceleration performance due to increased latency in each iteration (reflected by a higher td).\nMoreover, we analyze the block size k\u2019s effect on the end-to-end acceleration performance of the paradigm in Table 3. In contrast to the blockwise decoding achieving its best acceleration performance at k = 10 as shown in Table 1, SpecDec achieves its best performance at k = 25 with 7.89 mean accepted tokens each iteration. Further increasing k has an adverse effect, because it will become very hard for the model to learn to draft too many tokens simultaneously given the model capacity, resulting in a drop of Tok.."
        },
        {
            "heading": "4.3.2 Verification",
            "text": "We study the effect of Spec-Verification on the development set (i.e., newstest-2013) of WMT14 EN\u2192DE in Table 4. Moderately increasing \u03c4 and \u03b2 in the Spec-Verification not only leads to an increase of mean accepted tokens (Tok.) and speed since AR verification becomes less strict but also improves the generation quality over greedy decoding. However, the generation quality may decrease if over-relaxed: the BLEU score will degrade from the peak of 26.97 to 26.58 when decoding with top-5 selection (i.e., \u03b2 = 5) and \u03c4 = 5.0. Based on the results in the development set, we select \u03b2 = 3, \u03c4 = 1.0 as our Spec-Verification hyperparameters."
        },
        {
            "heading": "4.4 Practical Value",
            "text": "In addition to the remarkable speedup results, we demonstrate SpecDec\u2019s additional advantages that\nenhance its practical value in the following three aspects:\nBetter latency-throughput trade-off SpecDec achieves inference acceleration by increasing the GPU computing parallelism. Although increasing the batch size can also increase the computing parallelism to improve throughput, it results in increased latency, which is not desirable in real-world application scenarios. Therefore, a smaller batch size is often employed during inference, but this in turn results in the underutilization of GPU computing resources, leading to the dilemma of low throughput for small batches and high latency for large batches, as illustrated by Figure 5. SpecDec effectively addresses this dilemma. Even by maintaining a small batch size, SpecDec can fully utilize the computing performance of the GPU, significantly improving both efficiency and throughput.\nEasily adaptable for existing models In many practical applications, generative models are often pretrained with massive data, which exhibits very high performance. Developing a faster model from scratch to replace the pretrained model is highly challenging and typically requires substantial computational costs to reiterate the pretraining process. Otherwise, the quality of the new models is very likely to be compromised despite the increased speed, as Table 5 shows. However, SpecDec can be easily adapted to accelerate existing pretrained models. Taking the BART-base (Lewis et al., 2020) model for the abstractive summarization task as an example, we can easily achieve 5.1\u00d7 speedup without compromising the generation quality only by initializing the Spec-Drafter with the BARTbase encoder and training it with the BART-base distilled summarization training set.\nRetaining the behavior of the original model As introduced in Section 1, one significant advantage of SpecDec is that it does not develop a new\nModels Rouge-1 Rouge-2 Rouge-L Speed\nAR BART-base (b = 5) 43.08 20.41 40.15 1.0\u00d7BART-base (b = 1) 43.00 20.28 39.96 1.1\u00d7\nresults on par with BART-base performance are highlighted in orange, while blue denotes performance degradation. Compared to prior NAR methods, SpecDec can be easily adapted to accelerate the BART model only by downstream task fine-tuning.\nfaster model to replace the existing model. Instead, it accelerates the existing model with minimal changes to its behavior. As shown in Table 6, the consistency (BLEU) of SpecDec\u2019s generated results with the original model exceeds 85%, while that of a newly built fast NAR model is only around 55%. The characteristic of maintaining the behavior of the original model makes SpecDec even more valuable in practical applications because transitioning from a well-tested and mature model to one with substantially different behavior is risky, requiring extensive recalibration and various offline evaluations and online feedback in practice."
        },
        {
            "heading": "5 Related Work",
            "text": "Speculative Decoding We have demonstrated since early 2022 (see our arXiv preprints in 2022) that our proposed methodology, which formally introduces an independent model as a drafter combined with an advanced verification strategy to fully exploit speculative execution, is promising and has potential to evolve into a de facto standard in the future for efficient and lossless de-\ncoding. Since this work was proposed, we are pleased to see an increasing number of following studies (Leviathan et al., 2023; Chen et al., 2023; Kim et al., 2023; Spector and Re, 2023; Zhang et al., 2023) acknowledge, explore and adopt this methodology to accelerate Transformer inference. Among them, Leviathan et al. (2023) use the same name as ours (i.e., Speculative Decoding), employing a small AR model as a drafter12 as well as advanced sampling algorithm. Chen et al. (2023) is similar to Leviathan et al. (2023) but it was the first to validate this methodology to accelerate a large language model (i.e., 70B Chinchilla) with a 4B drafter model, thus receiving the most attention. SpecInfer (Miao et al., 2023) proposed to utilize various boost-tuned small language models for joint drafting, to improve the speculation accuracy of the LLM\u2019s outputs. Besides, it introduces an advanced token tree verification strategy to verify all candidate token sequences in parallel. DistillSpec (Zhou et al., 2023) further investigated the efficacy of knowledge distillation in enhancing the alignment between the target model and the drafter in speculative decoding. In addition to employing additional models as drafters, there has also been some research that proposes various strategies to efficiently generate drafts from the LLM itself (Santilli et al., 2023; Zhang et al., 2023). All the following research strongly backs up the value of this original work.\nEarly Draft-then-verify attempts This work is a generalized version of our previously proposed (Input-guided) Aggressive Decoding13 (Sun et al.,\n12We provide a detailed comparison between Leviathan et al. (2023) and our work in Appendix G.\n13As our technical report (Ge et al., 2022b) in May 2022 discusses, the Input-guided Aggressive Decoding is indeed a special case of Speculative Decoding.\n2021) in Grammatical Error Correction (GEC), which assumes that the input is exactly the sentence to be generated in the future and then verifies the whole sentence in parallel. Blockwise Decoding (Stern et al., 2018) inserted k \u2212 1 feedforward heads on top of the Transformer decoder to generate k positions in parallel and used the original head to verify these outputs. However, both the above studies did not fully investigate the potential of this paradigm and thus failed to uncover its great value for efficient seq2seq generation: Sun et al. (2021) only works for tasks whose inputs and outputs are highly similar (e.g., GEC). Stern et al. (2018) overlooked the importance of drafting accuracy; as a result, their underinvested prediction heads severely limit the acceleration results. In contrast, we conduct thorough investigations and fully exploit speculative execution, refreshing the impression of its limited acceleration potential and revealing its real value in practice.\nNon-autoregressive Decoding There is also another line of work named Non-Autoregressive Decoding (NAR) (Gu et al., 2018), which decodes multiple tokens in parallel compared with conventional AR, thus showing remarkable superiority in inference efficiency. Recently, various attempts have been made to improve the performance of NAR models, including training with alignment-based objectives (Libovick\u00fd and Helcl, 2018; Ghazvininejad et al., 2020; Saharia et al., 2020; Gu and Kong, 2021; Shao and Feng, 2022), modeling dependencies between target tokens (Ghazvininejad et al., 2019; Shu et al., 2020; Qian et al., 2021; Bao et al., 2021) and designing various model architectures (Zheng et al., 2021; Huang et al., 2022). As discussed in Section 4.4, replacing a powerful pretrained model with NAR models in practice is challenging due to the substantial computational costs required to reiterate the pretraining process. Additionally, transitioning from a well-tested and mature model to a new NAR model with significantly different behavior poses risks in practical applications. In contrast, our proposed SpecDec can be conveniently adapted to speed up existing AR models including highperformance pretrained models like BART with little effort. Moreover, SpecDec minimally alters the behavior of existing models, showcasing its ability to preserve reliable generation performance in real-world practical applications."
        },
        {
            "heading": "6 Conclusion",
            "text": "We present Speculative Decoding (SpecDec), the first work to explicitly embrace the idea of speculative execution for seq2seq generation acceleration with a formal study and extensive discussion of both drafting and verification phases. Contrary to the common belief that an increase in model complexity tends to hamper inference speed, SpecDec\u2019s introduction of an appropriately invested auxiliary drafter model substantially speeds up Transformer inference, owing to higher computational parallelism introduced by speculative execution to better utilize computing resources.\nThe remarkable acceleration performance, combined with the advantages demonstrated in our experiments, clearly illustrates that SpecDec is a practical acceleration method for model deployment in real-world applications. We hope that our preliminary study could draw more attention to this promising decoding paradigm that may potentially evolve into a de facto standard for efficient Transformer decoding in the near future.\nLimitations\nCompared with conventional autoregressive decoding, SpecDec introduces an extra Spec-Drafter module for ensuring its drafting accuracy, which brings additional memory cost at test time. Therefore, SpecDec is particularly suitable for inference scenarios where GPU memory is abundant but there is an urgent need to improve latency \u2013 it provides a solution to trading the surplus GPU memory for speed improvements. As thoroughly discussed in Appendix B, such scenarios are very common in practice. Most importantly, memory is no longer the bottleneck for practical model deployment. With the emergence and maturity of various data/tensor/pipeline parallelism techniques, the addition of more GPUs can easily address memory issues, which is also why models continue to grow larger. In contrast, latency remains an inescapable bottleneck in model deployment that cannot be resolved merely by increasing the number of machines. Therefore, we believe the increased memory consumption may not severely affect its practical value."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank Fan Yang, Lingxiao Ma, and Lidong Zhou in Microsoft Research for their constructive\ncomments on this work. This paper is supported by the National Key Research and Development Program of China 2020AAA0106700 and NSFC project U19A2065."
        },
        {
            "heading": "A Hyperparameters",
            "text": "Hyper-parameters of training our proposed SpecDrafter are listed in Table 7. Following Vaswani et al. (2017) and Ott et al. (2018), we also average model parameters from the last 10 checkpoints."
        },
        {
            "heading": "B Memory Analysis",
            "text": "B.1 Additional Memory Cost by SpecDec\nThe peak memory footnote of SpecDec during inference mainly comes from three parts:\n\u2022 Static AR model\u2019s weights \u2022 Static Spec-Drafter\u2019s weights \u2022 Intermediate variables/results\nCompared with AR, the additional memory cost of SpecDec comes from the last two parts. While the static Spec-Drafter\u2019s weights account for the majority of the additional memory cost, the additional cost for storing intermediate variables is negligible because the Spec-Drafter and AR model decode alternatively during inference. Compared with AR, SpecDec\u2019s additional intermediate variables/results include:\n\u2022 The Spec-Drafter\u2019s last encoder layer\u2019s representation that will not be freed until decoding finishes, which is equal to B \u00b7S \u00b7d where B is the batch size, S is the sequence length and d is the dimension of the model. This part is actually negligible: for example, when B = 32, S = 128, d = 512, this part\u2019s memory cost is only 8MB (fp32) / 4MB (fp16).\n\u2022 The largest intermediate variables/results during inference:\n\u2013 For a short sequence (e.g., sentence-level inputs/outputs in MT tasks), the largest intermediate variable is the output tensor after the Spec-Drafter\u2019s/AR model\u2019s vocabulary projection layer \u2013 B \u00b7 |V | \u00b7 k where B is the batch size, |V | is the vocabulary size and k is the block size. Compared with the memory cost for storing the Spec-Drafter\u2019s weights, this part is usually smaller. Also B \u00b7 k tokens can be easily divided into small batches (e.g., \u2013softmax-batch in fairseq) for vocabulary projection to avoid massive memory cost in case B \u00b7 |V | \u00b7 k is large.\n\u2013 For a long sequence (e.g., paragraphlevel inputs/outputs in summarization tasks), the largest intermediate variable becomes the tensor for storing selfattention computation whose size increases quadratically with S (S is the sequence length). This variable accounts for the largest memory cost for storing intermediate results in both AR and SpecDec. Therefore, in this case, this part does not introduce additional memory cost compared with AR.\nTable 8 and Table 9 show the comparisons of peak GPU memory footprint14 (MB) between SpecDec and AR (during inference) on the above two scenarios (i.e., MT and summarization). The results are consistent with our analysis above:\nThe majority of the additional memory cost (i.e., \u2206Memory) is for storing the Spec-Drafter\u2019s weights and the additional memory cost is not very likely to significantly increase as the batch size or sequence length increases.\nOur experiments above pre-loaded both the SpecDrafter and AR model. In fact, it is also possible to load the static weights of the AR model and SpecDrafter in a lazy loading manner in the meantime of GPU computation to save memory as they run alternatively. However, it is usually unnecessary in practice, because for a seq2seq model deployed on modern GPUs for online service, it is latency rather than memory that is the performance bottleneck. See the next section for more discussion.\n14Tested with torch.cuda.max_memory_allocated()\nB.2 Memory Is Rarely the Bottleneck\nTo understand the performance bottleneck of online deployed seq2seq models, we test the latency and memory cost of T5-large15 (around 770M parameters) with fp16 on 1 Nvidia A40 GPU running greedy decoding in the machine translation and abstractive summarization task, and show results in Table 10 and 11.\nFor MT, T5-large\u2019s latency is over 1 second which is actually too long to be accepted because most MT engines in practice require the latency to be less than 100ms. However, its memory cost is\n15In practice, T5-large is rarely deployed for online service because it is too large and expensive to serve.\nonly less than 2GB \u2013 far below A40 GPU\u2019s memory capacity (i.e., 48GB16).\nFor abstractive summarization, even if the batch size increases to 32, its memory cost is still less than 50% utilization of 1 A40 GPU but its latency is already close up to 5 seconds that is too long for an online service in practice.\nTo sum up, we now understand latency is the bottleneck of seq2seq models for online deployment in most cases. Therefore, we do not think additional memory cost by SpecDec will undermine its practical value; instead, we think a significant lossless acceleration even at the cost of memory (i.e., time\u2013memory trade-off) is much more meaningful than the acceleration at the cost of quality, which should be the right path that we need to pay more attention to given much memory headroom on modern GPUs."
        },
        {
            "heading": "C SacreBLEU and COMET Scores",
            "text": "Despite tokenized BLEU scores, we also report SacreBLEU17 (Post, 2018) and COMET18 (Rei et al., 2020) scores in Table 12 and 13 to provide a reference for future research. SpecDec can also achieve performances on par with the AR model with the evaluation in sacreBLEU and COMET. Schmidt et al. (2022) pointed out that inconsistencies in the use of tokenized BLEU lead to deviations of up to 1.8 BLEU points. Therefore, we\n16It can also easily scale to 96GB or more with NVIDIA NVLink connection of multiple GPUs or multi-node connection.\n17https://github.com/mjpost/sacrebleu 18Obtained with wmt20-comet-da from version 1.1.0.\nrecommend that future research use sacreBLEU when comparing with our work."
        },
        {
            "heading": "D Speedup Distribution",
            "text": "Figure 6 presents SpecDec\u2019s speedup distribution of a single sentence on the WMT14 EN\u2192DE test set (which has 3,003 sentences in total), showing that most sentences are translated with a 3\u00d7\u223c7\u00d7 speedup compared to AR beam search, while some rare cases can even achieve over 10\u00d7\u223c11\u00d7 speedup."
        },
        {
            "heading": "E Discussions of Beam Search",
            "text": "For possible concerns that SpecDec may not apply beam search, we make three points here:\n1. As Kim and Rush (2016) mentioned, knowledge distillation largely decreases the performance gap of beam search and greedy decoding. In practice, greedy decoding can actually be comparable to beam search results after KD. 2. In practical online deployment, KD is almost used by default for enhancing the results for\nstudent models and greedy decoding is much more common than beam search because it is more cost-effective \u2013 it not only runs faster than beam search but also achieves decent performance with a student model trained through KD (as Point 1 addressed) 3. Beam search is also an approximate and heuristic solution, which is not a golden rule. In fact, Spec-Verification works in a similar way as beam search \u2013 it is also an approximate and heuristic solution by considering n-best and scores, which can be considered as an approximation of beam search. As shown in Table 1, it achieves comparable performance to beam search but much faster (4\u00d7 \u223c 6\u00d7)."
        },
        {
            "heading": "F Carbon Emission",
            "text": "SpecDec introduces extra computational overhead, which leads to an increase in GPU power consumption. However, it can substantially reduce GPU hours owing to its high efficiency. We compare GPU power consumption and GPU hours of autoregressive decoding (AR) and SpecDec for translating 3000 sentences in Table 15.\nWe follow a formula for Wu et al. (2022) to calculate the total energy consumption and carbon emitted: E = P \u00d7 t \u00d7 PUE, where we set PUE (Power Usage Effectiveness) at 1.1. CO2eq = 0.385g \u00d7 CO2eq/Wh \u00d7 E, where 0.385g*CO2eq/Wh is the carbon intensity factor that is set based on the US national average.\nAccording to Table 15, while SpecDec\u2019s GPU power consumption is 28% higher, its GPU hours are 540% shorter than autoregressive decoding. As a result, SpecDec\u2019s total energy consumption\npositions. The verification pieces after the bifurcation are annotated as strikethrough. The highlighted parts are translations of previous iterations. The hyperparameters are k = 10, top-3, \u03c4 = 1.0. The output pieces after the [EOS] token is omitted in the table.\nis 23.7% of autoregressive decoding, resulting in 4.2\u00d7 reduction of carbon emission.\nTherefore, SpecDec not only does not increase carbon emission but actually significantly reduces it, making it a more environmentally friendly option for inference."
        },
        {
            "heading": "G Comparison with Other Work",
            "text": "G.1 Speculative Decoding with an Autoregressive Drafter\nFollowing this work, some subsequent research (Leviathan et al., 2023; Chen et al., 2023) has also explored using AR models (e.g., smaller language models) as independent drafters to accelerate inference. However, for seq2seq generation, the acceleration effect of AR drafting is severely limited, resulting in an end-to-end speedup at 1.6\u00d7\u223c2.8\u00d7, which is far lower than our 4.9\u00d7 speedup.\nTable 14 illustrates the detailed comparison of our re-implemented Leviathan et al. (2023) and ours in accelerating T5-XXL (11B) for WMT14 EN-DE translation. We implemented our SpecDrafter with a 24-layer encoder and a 6-layer decoder for comparison. Its embedding/FFN dimension/#heads are 1024/4096/16. As shown in Table 14, our approach uses a 0.5B Spec-Drafter to achieve an almost 5\u00d7 speedup without qual-\nity degradation, while Leviathan et al. (2023)\u2019s speedup results are lower than 3\u00d7. The reasons for our approach significantly outperforming Leviathan et al. (2023) are twofold:\n\u2022 Our Spec-Drafter is specially learned to draft for accelerating the target model with the speculation execution idea: its draft results are better aligned with the target model\u2019s results.\n\u2022 Our approach adopts a fast deep-encodershallow-decoder architecture as well as a nonautoregressive approach to generate drafts, which is significantly more efficient than their autoregressive drafting method.\nG.2 Speculative Decoding in Special Cases\nOur previous paper (Sun et al., 2021) and technical reports (Ge et al., 2022b; Yang et al., 2023) present extensive empirical studies of Speculative Decoding in special cases (e.g., text editing or retrieval-augmented generation) and compare it with competitive approaches (Malmi et al., 2019; Omelianchuk et al., 2020; Chen et al., 2020) in those scenarios. We recommend interested readers to refer to our previous work for further insights into these specific applications."
        },
        {
            "heading": "H Case Study",
            "text": "In Table 16, we represent an example to illustrate how SpecDec generates translations. In the first iteration, the outputs of the Spec-Drafter are nonautoregressive with multi-modality problems like \"Angaben Angaben\". The verifier accepts tokens of \"Nach den\" and replaces the inappropriate translation \"Angaben\" with \"vorliegenden\". All the drafted tokens after the bifurcation position (i.e. marked as red tokens in Table 16) are all discarded. In the second iteration, Spec-Verification finds the bifurcation at the last position, thus all tokens before this position are accepted. After 3 iterations, the decoding is finished since the [EOS] token is found."
        }
    ],
    "title": "Speculative Decoding: Exploiting Speculative Execution for Accelerating Seq2seq Generation",
    "year": 2023
}