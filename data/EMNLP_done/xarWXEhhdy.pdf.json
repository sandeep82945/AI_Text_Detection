{
    "abstractText": "Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily overfit to few-shot training samples, thereby undermining generalizability. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they fail to data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel SelfsUpervised meta-Prompt learning framework with MEta-gradient Regularization for fewshot generalization (SUPMER). SUPMER leverages self-supervised meta-learning with a diverse set of well-designed meta-training tasks to learn a universal prompt initialization for efficient adaptation using only unlabeled data. Additionally, it jointly meta-learns a gradient regularization function to transform raw gradients into a domain-generalizable direction, thus alleviating the problem of overfitting. Extensive experiments show that SUPMER achieves better performance for different few-shot downstream tasks, and also exhibits a stronger domain generalization ability. The code for SUPMER will be available at https://github.com/beepkh/SUPMER.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kaihang Pan"
        },
        {
            "affiliations": [],
            "name": "Juncheng Li"
        },
        {
            "affiliations": [],
            "name": "Hongye Song"
        },
        {
            "affiliations": [],
            "name": "Jun Lin"
        },
        {
            "affiliations": [],
            "name": "Xiaozhong Liu"
        },
        {
            "affiliations": [],
            "name": "Siliang Tang"
        }
    ],
    "id": "SP:2715fcdec156790b1211fc974648a86066b67415",
    "references": [
        {
            "authors": [
                "Akari Asai",
                "Mohammadreza Salehi",
                "Matthew Peters",
                "Hannaneh Hajishirzi."
            ],
            "title": "ATTEMPT: Parameter-efficient multi-task tuning via attentional mixtures of soft prompts",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natu-",
            "year": 2022
        },
        {
            "authors": [
                "John Blitzer",
                "Mark Dredze",
                "Fernando Pereira."
            ],
            "title": "Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
            "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440\u2013447,",
            "year": 2007
        },
        {
            "authors": [
                "Nitay Calderon",
                "Eyal Ben-David",
                "Amir Feder",
                "Roi Reichart."
            ],
            "title": "DoCoGen: Domain counterfactual generation for low resource domain adaptation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2022
        },
        {
            "authors": [
                "Dong Chen",
                "Kaihang Pan",
                "Guoming Wang",
                "Yueting Zhuang",
                "Siliang Tang."
            ],
            "title": "Improving vision anomaly detection with the guidance of language modality",
            "venue": "arXiv preprint arXiv:2310.02821.",
            "year": 2023
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Marie-Catherine De Marneffe",
                "Mandy Simons",
                "Judith Tonhauser."
            ],
            "title": "The commitmentbank: Investigating projection in naturally occurring discourse",
            "venue": "proceedings of Sinn und Bedeutung.",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "William B. Dolan",
                "Chris Brockett."
            ],
            "title": "Automatically constructing a corpus of sentential paraphrases",
            "venue": "Proceedings of the Third International Workshop on Paraphrasing (IWP2005).",
            "year": 2005
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine."
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages",
            "year": 2017
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen."
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Alex Graves",
                "Greg Wayne",
                "Ivo Danihelka."
            ],
            "title": "Neural turing machines",
            "venue": "arXiv preprint arXiv:1410.5401.",
            "year": 2014
        },
        {
            "authors": [
                "Yuxian Gu",
                "Xu Han",
                "Zhiyuan Liu",
                "Minlie Huang."
            ],
            "title": "PPT: Pre-trained prompt tuning for few-shot learning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8410\u20138423, Dublin,",
            "year": 2022
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "A. Steven Younger",
                "Peter R. Conwell."
            ],
            "title": "Learning to learn using gradient descent",
            "venue": "Artificial Neural Networks \u2014 ICANN 2001, pages 87\u201394, Berlin, Heidelberg. Springer Berlin Heidelberg.",
            "year": 2001
        },
        {
            "authors": [
                "Yutai Hou",
                "Hongyuan Dong",
                "Xinghao Wang",
                "Bohan Li",
                "Wanxiang Che."
            ],
            "title": "MetaPrompting: Learning to learn better prompts",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 3251\u20133262, Gyeongju, Republic of",
            "year": 2022
        },
        {
            "authors": [
                "Minqing Hu",
                "Bing Liu."
            ],
            "title": "Mining and summarizing customer reviews",
            "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168\u2013177.",
            "year": 2004
        },
        {
            "authors": [
                "Yukun Huang",
                "Kun Qian",
                "Zhou Yu."
            ],
            "title": "Learning a better initialization for soft prompts via metalearning",
            "venue": "arXiv preprint arXiv:2205.12471.",
            "year": 2022
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "Luke Zettlemoyer",
                "James Henderson",
                "Lambert Mathias",
                "Marzieh Saeidi",
                "Veselin Stoyanov",
                "Majid Yazdani"
            ],
            "title": "Promptfree and efficient few-shot learning with language",
            "year": 2022
        },
        {
            "authors": [
                "Gregory Koch",
                "Richard Zemel",
                "Ruslan Salakhutdinov."
            ],
            "title": "Siamese neural networks for one-shot image recognition",
            "venue": "ICML Deep Learning Workshop, volume 2.",
            "year": 2015
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant."
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045\u20133059, Online and Punta Cana, Domini-",
            "year": 2021
        },
        {
            "authors": [
                "Lagunas",
                "Alexander Rush",
                "Thomas Wolf."
            ],
            "title": "Datasets: A community library for natural language processing",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 175\u2013184, Online",
            "year": 2021
        },
        {
            "authors": [
                "Juncheng Li",
                "Minghe Gao",
                "Longhui Wei",
                "Siliang Tang",
                "Wenqiao Zhang",
                "Mengze Li",
                "Wei Ji",
                "Qi Tian",
                "Tat-Seng Chua",
                "Yueting Zhuang."
            ],
            "title": "Gradient-regulated meta-prompt learning for generalizable vision-language models",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Juncheng Li",
                "Xin He",
                "Longhui Wei",
                "Long Qian",
                "Linchao Zhu",
                "Lingxi Xie",
                "Yueting Zhuang",
                "Qi Tian",
                "Siliang Tang."
            ],
            "title": "Fine-grained semantically aligned vision-language pre-training",
            "venue": "Advances in neural information processing systems, 35:7290\u20137303.",
            "year": 2022
        },
        {
            "authors": [
                "Juncheng Li",
                "Kaihang Pan",
                "Zhiqi Ge",
                "Minghe Gao",
                "Hanwang Zhang",
                "Wei Ji",
                "Wenqiao Zhang",
                "TatSeng Chua",
                "Siliang Tang",
                "Yueting Zhuang."
            ],
            "title": "Fine-tuning multimodal llms to follow zeroshot demonstrative instructions",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Juncheng Li",
                "Siliang Tang",
                "Linchao Zhu",
                "Haochen Shi",
                "Xuanwen Huang",
                "Fei Wu",
                "Yi Yang",
                "Yueting Zhuang."
            ],
            "title": "Adaptive hierarchical graph reasoning with semantic coherence for video-and-language inference",
            "venue": "Proceedings of the IEEE/CVF Interna-",
            "year": 2021
        },
        {
            "authors": [
                "Juncheng Li",
                "Junlin Xie",
                "Long Qian",
                "Linchao Zhu",
                "Siliang Tang",
                "Fei Wu",
                "Yi Yang",
                "Yueting Zhuang",
                "Xin Eric Wang."
            ],
            "title": "Compositional temporal grounding with structured variational crossgraph correspondence learning",
            "venue": "Proceedings of",
            "year": 2022
        },
        {
            "authors": [
                "Junyi Li",
                "Tianyi Tang",
                "Wayne Xin Zhao",
                "Jian-Yun Nie",
                "Ji-Rong Wen."
            ],
            "title": "Pretrained language models for text generation: A survey",
            "venue": "arXiv preprint arXiv:2201.05273.",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Computing Surveys, 55(9):1\u201335.",
            "year": 2023
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Weng Tam",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Nikhil Mishra",
                "Mostafa Rohaninejad",
                "Xi Chen",
                "Pieter Abbeel."
            ],
            "title": "A simple neural attentive metalearner",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Quang Nguyen"
            ],
            "title": "The airline review dataset",
            "year": 2015
        },
        {
            "authors": [
                "Alex Nichol",
                "Joshua Achiam",
                "John Schulman."
            ],
            "title": "On first-order meta-learning algorithms",
            "venue": "arXiv preprint arXiv:1803.02999.",
            "year": 2018
        },
        {
            "authors": [
                "Kaihang Pan",
                "Juncheng Li",
                "Hongye Song",
                "Hao Fei",
                "Wei Ji",
                "Shuo Zhang",
                "Jun Lin",
                "Xiaozhong Liu",
                "Siliang Tang."
            ],
            "title": "Controlretriever: Harnessing the power of instructions for controllable retrieval",
            "venue": "arXiv preprint arXiv:2308.10025.",
            "year": 2023
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee."
            ],
            "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
            "venue": "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 271\u2013278,",
            "year": 2004
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee"
            ],
            "title": "Seeing stars: Exploit",
            "year": 2005
        },
        {
            "authors": [
                "Jake Snell",
                "Kevin Swersky",
                "Richard Zemel."
            ],
            "title": "Prototypical networks for few-shot learning",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.",
            "year": 2017
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Tianxiang Sun",
                "Zhengfu He",
                "Qin Zhu",
                "Xipeng Qiu",
                "Xuanjing Huang."
            ],
            "title": "Multi-task pre-training of modular prompt for few-shot learning",
            "venue": "arXiv preprint arXiv:2210.07565.",
            "year": 2022
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Charles Blundell",
                "Timothy Lillicrap",
                "koray kavukcuoglu",
                "Daan Wierstra"
            ],
            "title": "Matching networks for one shot learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Ellen M. Voorhees",
                "Dawn M. Tice."
            ],
            "title": "Building a question answering test collection",
            "venue": "Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201900, page 200\u2013207, New York, NY,",
            "year": 2000
        },
        {
            "authors": [
                "Tu Vu",
                "Brian Lester",
                "Noah Constant",
                "Rami Al-Rfou",
                "Daniel Cer"
            ],
            "title": "SPoT: Better frozen model adaptation through soft prompt transfer",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Advances in Neural Information",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing",
            "year": 2018
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush"
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "year": 2020
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N. Dauphin",
                "David Lopez-Paz."
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Wenqiao Zhang",
                "Jiannan Guo",
                "Mengze Li",
                "Haochen Shi",
                "Shengyu Zhang",
                "Juncheng Li",
                "Siliang Tang",
                "Yueting Zhuang"
            ],
            "title": "2022a. Boss: Bottom-up crossmodal semantic composition with hybrid counterfactual training for robust content-based image retrieval",
            "year": 2022
        },
        {
            "authors": [
                "Wenqiao Zhang",
                "Haochen Shi",
                "Jiannan Guo",
                "Shengyu Zhang",
                "Qingpeng Cai",
                "Juncheng Li",
                "Sihui Luo",
                "Yueting Zhuang."
            ],
            "title": "Magic: Multimodal relational graph adversarial inference for diverse and unpaired text-based image captioning",
            "venue": "Proceed-",
            "year": 2022
        },
        {
            "authors": [
                "Wenqiao Zhang",
                "Siliang Tang",
                "Yanpeng Cao",
                "Shiliang Pu",
                "Fei Wu",
                "Yueting Zhuang."
            ],
            "title": "Frame augmented alternating attention network for video question answering",
            "venue": "IEEE Transactions on Multimedia, 22(4):1032\u20131041.",
            "year": 2019
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Jingkang Yang",
                "Chen Change Loy",
                "Ziwei Liu."
            ],
            "title": "Learning to prompt for visionlanguage models",
            "venue": "International Journal of Computer Vision, 130(9):2337\u20132348.",
            "year": 2022
        },
        {
            "authors": [
                "Yftah Ziser",
                "Roi Reichart."
            ],
            "title": "Pivot based language modeling for improved neural domain adaptation",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2018
        },
        {
            "authors": [
                "Gu"
            ],
            "title": "2022), we set two sentences next to each other as label 0, those from the same document but not adjacent as label 2, and those from different documents",
            "year": 2022
        },
        {
            "authors": [
                "Karimi Mahabadi"
            ],
            "title": "2022), we leverage the original validation sets of SST-2, CB, RTE, QNLI, WiC, MRPC, and QQP1 as substitutes for the unavailable test sets. And the validation sets",
            "year": 2022
        },
        {
            "authors": [
                "Gu"
            ],
            "title": "2022) and reset the pretrained language model to T5-base and Flan-T5XL. Specifically, for both PPT and Unified-PPT, we sample 10GB of unlabeled data from OpenWebText to construct pre-training tasks for each task",
            "year": 2022
        },
        {
            "authors": [
                "Huang"
            ],
            "title": "2022), We group all labeled data into 10 clusters through K-means. And we set the inner loop learning rate to 0.08, the outer loop learning rate",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent NLP accomplishments witnessed the rapid development of pre-trained language models (PLMs) (e.g., BERT Devlin et al., 2019; T5 Raffel et al., 2020; GPT3 Brown et al., 2020). Finetuning, which tunes the entire PLM parameters, has achieved outstanding performances in various NLP tasks. However, as the pre-trained model scale increases, tuning the entire set of parameters\n* Work done when interning at Alibaba DAMO Academy. \u2020 Corresponding Author.\nwould be sometimes unaffordable. More recently, prompt-based methods, which simply insert a piece of carefully designed text to the input (e.g., \u201cIt was \u27e8X\u27e9.\u201d) and predict target words (e.g., \u201cgreat\u201d or \u201cterrible\u201d) at the mask position with frozen PLMs, have demonstrated remarkable effectiveness. But it has been observed that the performance of promptbased methods is greatly affected by the design of prompts. In light of this, prompt tuning (PT Lester et al., 2021), as a parameter-efficient tuning method, is proposed to only prepend some additional learnable tokens called soft prompts to the input text, with all PLM parameters freezing.\nThough prompt tuning is an efficient and effective paradigm, Gu et al. (2022) shows it performs much worse than fine-tuning under few-shot settings. We argue that the performance is not satisfactory mainly due to two limitations: 1) The performance of PT is highly sensitive to the soft prompt initialization, especially for few-shot tasks. As shown in Figure 1 (a), different soft prompt initialization leads to significant performance variations. 2) Few-shot PT risks overfitting to some spurious correlations as soft prompts are tuned on limited training samples, thus undermining the generalizability of PLMs. As shown in Figure 1 (b), the performance of few-shot vanilla PT degrades significantly in the final training steps.\nRecent research mainly focused on the first limi-\ntation, leveraging pre-training or supervised metalearning for soft prompt initialization. A pretrained prompt tuning method (PPT) (Gu et al., 2022) is proposed from the beginning, which utilizes self-supervised tasks to pre-train soft prompts and then applies them in the few-shot scenario. However, without explicitly optimizing the fast adaptation ability of the model, PPT suffers from a train-test mismatch between the pre-training data and the downstream data. So it limits generalization to unseen few-shot tasks, especially when there is a significant disparity in task domains or formats. MetaPrompting (Hou et al., 2022), as another effort, seeks assistance from model-agnostic meta-learning (MAML Finn et al., 2017) for fast adaptation in few-shot settings. However, in each task, MetaPrompting requires plenty of labeled data within certain classes to perform supervised meta-learning for prompt initialization, which is often inaccessible in practical few-shot scenarios. And the learned initialization can only generalize to the remaining classes of the same task in a fewshot manner, exhibiting weak task transferability. Furthermore, all these existing works ignore the second limitation, i.e., the propensity for few-shot prompt tuning to lead to overfitting.\nTo address the shortcomings of existing works, we propose SUPMER, a Self-sUpervised metaPrompt learning framework with MEta-gradient Regularization. It leverages self-supervised metalearning to universally learn an efficient soft prompt initialization, also with a meta-gradient regularization function to mitigate overfitting. This comprehensive process only requires a one-time execution and enables seamless adaptation to different downstream few-shot tasks, while also facilitating faster convergence for downstream prompt tuning.\nSpecifically, to address the first limitation, we design a novel self-supervised meta-learning method for prompt initialization, which automatically generates a diverse set of meta-training tasks from large-scale unlabeled corpora and explicitly learns to fast adapt across these tasks. To ensure task diversity, we initially design a collection of anchor self-supervised meta-training tasks with different formats. And then a curriculum-based task augmentation method is further proposed to enrich the task distribution dynamically in terms of the current model capability.\nFor the second issue, we integrate a metagradient regularization function into meta-prompt\nlearning. As we simulate distribution shift through task augmentation, the meta-gradient regularization parameters are jointly optimized to align gradient directions across different distributions within our proposed meta-prompt learning paradigm. Consequently, in downstream tasks, these optimized parameters can be directly utilized to transform raw gradients over few-shot samples into a domaingeneralizable direction, preventing prompt tuning overfitting to some domain-specific correlations.\nOverall, our contributions are mainly three-fold: (1) We propose a novel self-supervised metaprompt learning framework to better initialize soft prompts, where only unlabeled pre-training data are used to construct different meta-training tasks with curriculum-based task augmentation for further task enrichment.\n(2) We incorporate a novel meta-gradient regularization function into our meta-prompt learning framework, which meta-learns to transform the raw gradient during few-shot learning into a domaingeneralizable direction, thus preventing prompt tuning overfitting to domain-specific correlations.\n(3) Comprehensive experiments on few-shot learning and domain generalization validate the superiority of our method, which even outperforms full-model tuning in few-shot learning. It also exhibits a stronger domain generalization ability."
        },
        {
            "heading": "2 Related Work",
            "text": "Soft Prompt Tuning. Soft prompt tuning is one of the most parameter-efficient tuning methods widely used in NLP (Liu et al., 2023) and visionlanguage tasks (Zhou et al., 2022; Li et al., 2023a), which only tunes a small number of (extra) parameters to attain strong performance. Specifically, it freezes the PLM parameters and prepends some trainable continuous embeddings (i.e., soft prompts) to the input sequence (Lester et al., 2021) or every layer of the pre-trained model (Li and Liang, 2021; Liu et al., 2022).\nTo efficiently train task-adaptive soft prompts in few-shot scenarios, some studies (Vu et al., 2022; Asai et al., 2022; Sun et al., 2022) employ task adaptation techniques, obtaining source prompts from source tasks in a supervised way and interpolating them into the target prompts. Other works focus on training improved prompt initializations. PPT (Gu et al., 2022) pre-trains the soft prompts with some self-supervised tasks on unlabeled corpora, but it doesn\u2019t explicitly optimize the fast adap-\ntation ability of the model. MetaPrompting(Hou et al., 2022) utilizes supervised meta-learning for soft prompt initialization, splitting each dataset into two sets with disjoint data classes. One split is used to initialize soft prompts while the other serves as the downstream task. In comparison, SUPMER differs from MetaPrompting in the following ways: 1) for each downstream task MetaPrompting focuses on a fixed supervised dataset to reinitialize soft prompts, whereas SUPMER can universally generalize to different unseen tasks with large-scale unlabeled corpora for initialization; 2) MetaPrompting doesn\u2019t freeze PLM parameters, while SUPMER only tunes the soft prompts as the general soft prompt tuning methods do.\nMeta-Learning. Meta-learning, also known as learning to learn, optimizes the ability to learn new tasks quickly and efficiently, utilizing experience from previously seen tasks. It can be classified into three types: metric-based methods (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017), model-based methods (Graves et al., 2014; Mishra et al., 2018; Qiao et al., 2018), and gradientbased methods (Hochreiter et al., 2001; Ravi and Larochelle, 2017; Nichol et al., 2018; Li et al., 2020). In this work, we focus on a gradient-based meta-learning algorithm (i.e., MAML Finn et al., 2017). Compared to typical meta-learning methods that rely on human-annotated meta-training tasks, we automatically generate abundant tasks in a selfsupervised way, also integrating a meta-gradient regularization function into MAML to steer gradi-\nents towards a domain-generalizable direction."
        },
        {
            "heading": "3 Method",
            "text": "In this section, we describe the whole framework of SUPMER (shown in Figure 2). With pre-defined preliminaries, we first introduce the way to construct anchor self-supervised meta tasks and the foundation of task augmentation to densify task distributions. Then we elaborate on the SUPMER model, including the meta-gradient regularization function. Finally, we upgrade the original task augmentation method into a curriculum-based one. Besides, we formalize all tasks in a text-to-text format following the T5 fashion (Raffel et al., 2020)."
        },
        {
            "heading": "3.1 Preliminaries",
            "text": "Prompt Tuning. In prompt tuning (Lester et al., 2021), given a training sample (xi, yi) from task D\u03c4 , we apply a prompt template P converting xi into a new sequence P (xi) and then concatenate a set of soft prompts \u03b8 to the beginning of P (xi). And verbalizer V plays a role in mapping yi to some corresponding label tokens V(yi) in the vocabulary of PLMs. So the objective of prompt tuning can be formulated as follows: argmin\n\u03b8 LD\u03c4 (\u03b8)\n= argmax \u03b8 \u2211 (xi,yi)\u2208D\u03c4 log p ( \u27e8X\u27e9 = V(yi)|[\u03b8;P (xi)]; \u03b8 ) (1) where \u03b8 denotes the soft prompt embedding (the only tunable parameters in prompt tuning). \u27e8X\u27e9 let PLMs predict target tokens at the masked positions and [\u00b7; \u00b7] is the concatenation operation.\nModel-Agnostic Meta-Learning. Assuming access to a task distribution p(T ), the goal of metalearning is to utilize tasks \u03c4i \u223c p(T ), referred to as meta-training tasks or meta tasks, to train a learning procedure that generalizes to unseen tasks from the distribution. Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) is a gradient-based bi-level optimization meta-learning method, which consists of an inner loop task-specific learning and outer loop fast adaptation across tasks.\nSpecifically, a task \u03c4 is composed of the support set Ds\u03c4 and the query set D q \u03c4 . In the inner loop of MAML, a model learns to adapt to a new task \u03c4i using its support set in the following way:\n\u03b8\u2032i = \u03b8 \u2212 \u03b11\u2207\u03b8LDs\u03c4i (\u03b8) (2)\nwhere \u03b11 is the inner loop learning rate and \u03b8 is the model\u2019s parameters. And the optimized parameters \u03b8\u2032i is then evaluated on the query set of task \u03c4i with the loss function LDq\u03c4i . In the outer loop, this loss across meta-training tasks is treated as the final training loss to update \u03b8:\n\u03b8 \u2190 \u03b8 \u2212 \u03b21\u2207\u03b8 \u2211\n\u03c4i\u223cp(T )\nLDq\u03c4i (\u03b8 \u2032 i) (3)\nwhere \u03b21 is the outer loop learning rate."
        },
        {
            "heading": "3.2 Constructing Anchor Meta Tasks",
            "text": "Supervised datasets with a large amount of labeled data are often unavailable in many NLP tasks. While unlabeled data is more easily accessible and generally covers broader semantic concepts. So we utilize the unlabeled data from a large corpus to create anchor self-supervised meta-training tasks.\nThe unlabeled data are first grouped into different clusters. We utilize PLMs to derive semantically meaningful embeddings for sentences in the corpus, and then apply unsupervised K-means to cluster these unlabeled sentences. Based on the results of K-means, we design three different formats of self-supervised meta-training tasks: sentencepair classification, multi-choice classification, and single-sentence classification.\nSpecifically, sentence-pair classification involves predicting whether two sentences are adjacent in the same document or from the same cluster after K-means clustering. Multi-choice classification identifies the correct sentence among several candidates, which is either adjacent to a query sentence or from its same cluster. And Singlesentence classification aims to associate each sentence with its correct cluster label, as determined\nby K-means. On this basis, for each task format, we distribute meta-training data into different tasks to construct anchor meta-training tasks with wellbalanced task distributions. We group samples with similar embeddings into the same task based on the results of K-means. And we give a more detailed description of anchor meta-training task construction in Appendix A.2."
        },
        {
            "heading": "3.3 Vanilla Task Augmentation",
            "text": "With a set of anchor meta-training tasks, in this section we first introduce the vanilla task augmentation to densify the task distribution. Extending the idea of mixup (Zhang et al., 2018), we augment the task set through task interpolation, which linearly combines features and corresponding labels of samples from the query set in different tasks. In \u00a73.5 we further upgrade the vanilla task augmentation method into a curriculum-based one, which dynamically controls the task interpolation in terms of the current model capability.\nSpecifically, for a task composed of the support set and the query set, we denote the hidden representations of the query set samples in task \u03c4k as Hq. Given an anchor task \u03c4i, first we randomly select another task \u03c4j . While retaining the support set of \u03c4i, we reconstruct its query set by interpolating on the hidden representations (Hqi ,H q j ) and corresponding labels (Y qi ,Y q j ) from the query sets in \u03c4i and \u03c4j , which can be accomplished using mixup:\nH\u0303qi = (1\u2212 \u03bb)H q i + \u03bbH q j , Y\u0303 q i = (1\u2212 \u03bb)Y q i + \u03bbY q j\n(4)\nwhere the mixing ratio \u03bb \u2208 [0, 1] is drawn from a Beta distribution Beta(\u03b1, \u03b1), and \u03b1 is a hyperparameter. The process of task augmentation not only enriches the task distribution, but also simulates the distribution shift between the support set and the query set within one task, as we only leverage interpolation between the query sets of different anchor meta-training tasks. And in \u00a73.4 we will show the effect of this distribution deviation."
        },
        {
            "heading": "3.4 Meta-Prompt Learning with Meta-Gradient Regularization",
            "text": "In this section we introduce the algorithm of our meta-prompt learning framework, which is a bi-level meta-learning paradigm learning a taskuniversal soft prompt initialization \u03b8 for efficient adaptation. And it jointly meta-learns a metagradient regularization function \u03c8\u03d5 that transforms raw gradients into a domain-generalizable direction to prevent prompt tuning from overfitting.\nSpecifically, considering that the inner loop update of MAML (i.e., Eq. (2)) over limited samples might overfit to some domain-specific correlations, we propose to learn a gradient regularization function \u03c8\u03d5(\u00b7), making a direct transformation to the raw gradients obtained from the support set Ds\u03c4i . The function first performs affine transformation h(\u00b7) (e.g., rotation) to modulate the raw gradients g, and then an update gate vector z is employed to combine g and h(g) into the final gradients:\n\u03c8\u03d5(g) = z \u00b7 h(g) + (1\u2212 z) \u00b7 g (5)\nObviously, the value of z can be used to control how much the transformed gradients h(g) contribute to the output of \u03c8\u03d5(g). We hope to determine this weight based on the input samples themselves, setting z = \u03c3(WH + b), where H is the hidden representations of input samples. Formally, now we transform Eq. (2) into:\n\u03b8\u2032i = \u03b8 \u2212 \u03b11\u03c8\u03d5(\u2207\u03b8LDs\u03c4i (\u03b8)) (6)\nAfter adapting the soft prompt embeddings to the support set Ds\u03c4i , in the outer loop we optimize the prompt initialization \u03b8 based on these adapted embeddings \u03b8\u2032 via Eq. (3). Besides, meta-gradient regularization parameters \u03d5 are also optimized using the same loss to learn a better gradient transformation, with \u03b22 as the learning rate:\n\u03d5\u2190 \u03d5\u2212 \u03b22\u2207\u03d5 \u2211\n\u03c4i\u223cp(T )\nLDq\u03c4i (\u03b8 \u2032 i) (7)\nOverall, the total meta-prompt learning obejective can be formulated as follows:\nargmin \u03b8,\u03d5 \u2211 \u03c4i\u223cp(T ) LDq\u03c4i ( \u03b8 \u2212 \u03b11\u03c8\u03d5(\u2207\u03b8LDs\u03c4i (\u03b8)) ) (8)\nDownstream Prompt Tuning. The above metaprompt learning framework only requires a onetime execution. The optimized prompt initialization \u03b8\u2217 and meta-gradient regularization parameters \u03d5\u2217 are then universal for different downstream tasks. During downstream prompt tuning, we fix \u03d5\u2217 and further adapt \u03b8\u2217 to testing tasks as Eq. (6).\nAnalysis of SUPMER. Here we give some analysis of how SUPMER could enhance generalizability, with more complete proof in Appendix A.1. Given that x = \u03b8\u2212 \u03b11\u03c8\u03d5(\u2207\u03b8LDs(\u03b8)) and x0 = \u03b8, focusing on a single meta-training task, we can apply a first-order Taylor expansion around the point\nx0 to reformulate Eq. (8) as:\n\u2235 LDq (x) = LDq (x0) + L\u2032Dq (x0)(x\u2212 x0) \u2234 argmin\n\u03b8,\u03d5 LDq\n( \u03b8 \u2212 \u03b11\u03c8\u03d5(\u2207\u03b8LDs(\u03b8)) ) =argmin\n\u03b8,\u03d5 LDq (\u03b8)\u2212 \u03b11\u2207\u03b8LDq (\u03b8) \u00b7 \u03c8\u03d5\n( \u2207\u03b8LDs(\u03b8) ) (9)\nBased on the aforementioned discussion, we can reach the following conclusions: (1) The update of \u03b8 minimizes the expected loss on the query set. (2) The optimization of both \u03b8 and \u03d5 maximizes the inner product between the regulated gradients from the support set and the gradients from the query set. The inner product of two vectors is larger if they are in a similar direction. Recalling that we simulate the distribution shift between the support set and the query set, the optimization of \u03b8 and \u03d5 tries to align the gradient directions across different distributions. To improve the alignment between the domain-specific gradients, the gradient regularization parameters \u03d5 are optimized to retain some domain-invariant information of metatraining data and then can be utilized to regulate raw gradients obtained from few-shot samples into a domain-generalizable direction in downstream prompt tuning, thus avoiding overfitting to some spurious correlations."
        },
        {
            "heading": "3.5 Curriculum-based Task Augmentation",
            "text": "In \u00a73.4 we show that SUPMER can help align the optimization direction across two distributions with deviation, which is simulated by performing task augmentation exclusively on the support sets. From Eq. (4) it is evident that the mixing ratio \u03bb of mixup controls the extent of the distribution deviation, with a larger \u03bb resulting in a more noticeable deviation. However, in the previously discussed method, \u03bb is sampled from a fixed Beta distribution. In this section, we propose a more flexible sampling approach, which upgrades the original task augmentation method into a curriculum-based one, gradually increasing the task difficulty and achieving a more reasonable distribution shift.\nThe curriculum-based task augmentation dynamically adjusts the parameters of the Beta distribution, from which we sample the mixing ratio \u03bb. Specifically, a batch of meta tasks is sampled in each training epoch. For each task, we can obtain gradients on the support set gsi and gradients on the query set gqi , along with their cosine similarity. We leverage the average cosine similarity sk\u22121 of all tasks in a batch during the last epoch to derive the\nmixing ratio \u03bbk for the current epoch k:\n\u03bbk = Beta(\u03b1, bk\u03b1)\nbk = m\n1+sk\u22121 2 \u2212 1\nm\u2212 1 ,\nwhere sk\u22121 = 1 |B| \u00b7 |B|\u2211 i=1 gsi \u00b7 gqi \u2225gsi \u2225 \u00b7 \u2225g q i \u2225\n(10)\nwhere m is the curve parameter. In this way, when our model is not capable of aligning the optimization directions across different distributions at the beginning, a smaller \u03bb is preferable to create a smaller distribution deviation. Then \u03bb tends to gradually increase as the model\u2019s capability improves, resulting in a larger distribution deviation and a corresponding increase in task difficulty.\nWe present the pseudo-codes of SUPMER in Appendix A.4."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "We evaluate our approach in two problem settings: 1) Few-shot learning with different NLP downstream tasks; 2) domain generalization.\nFew-shot Learning. We consider 6 downstream tasks with 12 datasets: 1) the sentiment analysis datasets SST-2, SST-5 (Socher et al., 2013), MR (Pang and Lee, 2005) and CR (Hu and Liu, 2004); 2) the subjectivity classification dataset SUBJ (Pang and Lee, 2004); 3) the question\nclassification dataset TREC (Voorhees and Tice, 2000); 4) the natural language inference datasets CB (De Marneffe et al., 2019) and RTE (Wang et al., 2019); 5) the question answering dataset QNLI (Rajpurkar et al., 2016); 6) the word sense disambiguation dataset WiC (Pilehvar and Camacho-Collados, 2019); 7) the paraphrase detection datasets MRPC (Dolan and Brockett, 2005) and QQP. Following Karimi Mahabadi et al. (2022), for each dataset we sample 16 instances per label from the original training set to form training and validation sets for few-shot learning.\nDomain Generalization. Then we design a more challenging problem about zero-shot domain generalization in the sentiment analysis task. Our experiments include 6 domains across 3 datasets: 1) the Amazon review dataset (Blitzer et al., 2007) containing reviews about Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K); 2) the airline review dataset (A) (Nguyen, 2015; Ziser and Reichart, 2018); 3) the restaurant (R) domain obtained from the Yelp dataset (Zhang et al., 2015).\nWe choose A as the source domain and the other five (B, D, E, K, R) constitute the target domains. On this basis, we sample 16 instances per label from the training set of the source domain to tune soft prompts. And then we directly use the soft prompts learned from the source domain to evaluate performance on the test set of each domain."
        },
        {
            "heading": "4.2 Experimental Details",
            "text": "Baselines. Our experiments are built on a smallerscale model, T5-base (Raffel et al., 2020), and then on a larger-scale instruction-tuned model, Flan-T5-XL (Chung et al., 2022). For both two backbone models, we use the following baselines: (1) prompt tuning methods with the same number of tunable parameters as SUPMER: vanilla prompt tuning (PT Lester et al., 2021), PPT (Gu et al., 2022), Unified-PPT (Gu et al., 2022), and MetaPT (Huang et al., 2022). (2) methods with more tunable parameters: Prefix-Tuning (Li and Liang, 2021), P-tuning-v2 (Liu et al., 2022), fullmodel tuning (FT). Furthermore, Given that FLANT5-XL was also designed with few-shot inference in mind, we additionally compare with two baseline methods on FLAN-T5-XL, i.e., zero-shot inference and few-shot inference, which directly employ Flan-T5-XL for downstream evaluation. We list the details of baselines in Appendix B.\nImplementation Details. We solve all downstream tasks in a text-to-text format and run each experiment with 5 different random seeds. For all prompt tuning methods, we follow Lester et al. (2021) to design soft prompts composed of 100 soft tokens, with tunable parameters far less than full-model tuning. For our SUPMER, following PPT (Gu et al., 2022) we sample 10GB data from OpenWebText (Gokaslan et al., 2019), a largescale unlabeled corpus, to construct self-supervised meta-training tasks. The meta-training stage only requires a one-time execution. In downstream prompt-tuning, we freeze the meta-gradient reg-\nularization parameters and the soft prompts are the only tunable parameters. We give more details of training hyper-parameters in Appendix C."
        },
        {
            "heading": "4.3 Main Result",
            "text": "Table 1 and Table 2 show the main results of fewshot learning and domain generalization. From the results, we have the following observations.\nFirst, in few-shot learning, SUPMER achieves better performance than all baselines on 10 of 12 datasets, whether using T5-base or Flan-T5-XL as the backbone. And the average accuracy of SUPMER over all datasets reaches 71.3% on T5-base, significantly outperforming other baselines (e.g., improving the performance by +1.3 points compared to FT). Notably, when utilizing the larger Flan-T5-XL as the backbone, SUPMER demonstrates even more substantial performance gains (e.g., improving the average performance by +2.5 points compared to FT), which indicates that our approach unlocks greater capabilities for stronger models that have undergone instruction-tuning with a higher number of parameters.\nSpecifically, SUPMER consistently outperforms all other prompt tuning methods with the same number of tunable parameters across all datasets. This indicates that our method offers soft prompts with better few-shot generalization ability. And it is noteworthy to highlight that SUPMER utilizes ex-\nactly the same unlabelled data as PPT and UnifiedPPT for soft prompt initialization. Yet it considerably outperforms these two baselines, demonstrating that the performance improvement is primarily attributable to our methodology rather than the meta-training data itself. Additionally, SUPMER outperforms baseline methods with more tunable parameters (e.g., full-model tuning) on the majority of datasets, achieving superior performance with fewer parameters.\nSecond, SUPMER is superior to all baselines in almost all domain-generalization setups. For example, compared to MetaPT which meta-trains soft prompts with a supervised sentiment analysis dataset, SUPMER exhibits average gains of 1.1% on T5-base and 1.4% on Flan-T5-XL. So it can be inferred that SUPMER shows stronger robustness to domain shifts, exhibiting better generalization to unseen tasks or domains.\nThird, for both few-shot learning and domain generalization on Flan-T5-XL, SUPMER demonstrates superior performance across almost all datasets and domains in contrast to few-shot inference. It provides further evidence that for LMs such as Flan-T5-XL with inherent few-shot inference capabilities, our approach can significantly enhance their abilities in a parameter-efficient tuning strategy, without providing any in-context examples during inference.\nFourth, SUPMER also results in lower variances on most datasets. Few-shot learning is often notorious for its instability. And in our method we keep few-shot prompt tuning more stable."
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "Analysis of Generalization. Figure 3 shows the performance trend for each method after different training steps on datasets CB and MRPC with T5base model. It illustrates that few-shot prompt tuning converges slowly with its performance typically showing an overall decline during the final training steps because they may easily result in overfitting. In comparison, SUPMER achieves faster, stronger, and more enduring few-shot generalization. It not\nonly accelerates the convergence to the optimal performance realizing fast adaptation, but also consistently maintains its optimal performance across prolonged training periods.\nEffect of Sample Size. We also discuss how the performance of SUPMER and other baselines varies when the number of training samples increases on SST-5 and SUBJ. As shown in Figure 4, with T5-base as the underlying PLM, when the number of training samples per label grows from 4 to 64, SUPMER is consistently better than other prompt tuning methods. And the performance gap between these methods is gradually reduced as the number of training data increases.\nSelf-Supervised v.s. Supervised. To illustrate that self-supervised meta-learning can better generalize to unseen tasks compared to supervised meta-learning, we also collect a set of labeled datasets (ensuring no overlap with downstream testing datasets) to formulate meta-training tasks for soft prompt initialization and conduct the experiments of few-shot learning on T5-base. The results are displayed in Table 3 (rows 1 and 2). As our collected labeled data contains lots of sentiment analysis datasets (e.g., Yelp5), SUPMER (only labeled) and SUPMER (only unlabeled) reveal proximity in their performance on sentiment analysis tasks (i.e., SST-2, SST-5, MR, CR). But in other tasks, using unlabeled data consistently achieves better results than utilizing only labeled data, also with a higher average accuracy over all datasets, which validates\nthe superiority of self-supervised meta-learning.\nEffect of integrating Labeled Data. To further explore the impact of integrating labeled data and substantiate the efficacy of SUPMER following this integration, we amalgamate the original unlabeled meta-training data with our collected labeled data mentioned above, with a mixing ratio of labeled to unlabeled as 1:2. The amalgamated data is employed for constructing meta-training tasks to meta-train SUPMER. Moreover, following PPT (Gu et al., 2022) and MetaPT (Huang et al., 2022), We also leverage pre-training and vanilla MAML to initialize soft prompts using the same amalgamated data. The experimental results of few-shot learning on T5-base are shown in Table 3 (rows 3-5). First, we can see that SUPMER (labeled+unlabeled) outperforms SUPMER (unlabeled) and SUPMER (labeled) as it allows us to harness the high-quality advantages of labeled data while also exploiting the broader semantic concepts encapsulated by unlabeled data. Second, After the integration of labeled data, SUPMER still consistently demonstrates significantly superior performance compared to baseline methods employing the same data for prompt initialization, which further underscores the effectiveness of SUPMER.\nEffect of Individual Components. We train the following ablation models. 1) only sp / mc / ss: we retain sentence-pair classification / multi-choice classification / single-sentence classification as the only anchor meta-training task format. 2) w/o ta: we entirely remove the task augmentation method. 3) w/o curriculum: we only retain the vanilla task augmentation without the curriculum-based idea. 4) w/o mgr: we remove the meta-gradient regularization function. All experiments follow the settings in \u00a74.1 and are conducted on T5-base. We report the average accuracy of few-shot learning and domain generalization in Table 4. More detailed results are in Appendix D.\nThe results of Row 1-3 indicate that considering diversified task formats during meta-training helps efficiently generalize to different tasks as downstream tasks often contain various task formats. Row 4 and Row 5 highlight that task augmentation plays an essential role in our framework, with curriculum-based augmentation further enriching the task distribution and realistically simulating the distribution shift. Moreover, Row 6 validates the superiority of meta-gradient regularization in\navoiding overfitting to some domain-specific correlations, thus achieving better performance."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we present SUPMER, a selfsupervised meta-prompt learning framework with meta-gradient regularization. With a diverse set of well-designed self-supervised meta-training tasks, SUPMER jointly meta-learns a universal prompt initialization and an effective gradient regularization function for efficient few-shot generalization. Extensive experiments on few-shot learning and domain generalization show that SUPMER outperforms other prompt methods and full-model tuning, achieving state-of-the-art performance.\nLimitations\nAlthough SUPMER performs superbly in a variety of problem scenarios, there still exist some limitations in our work: 1) We did not conduct any data filtering or cleaning operations to the meta-training data, which could potentially result in the inclusion of some biased content. 2) Our experiments are solely conducted on English tasks, and also do not involve some kinds of NLP tasks (e.g., language generation Li et al., 2022c) or vision-language tasks (Zhang et al., 2022b; Li et al., 2022b; Zhang et al., 2019; Li et al., 2021).\nTo address these limitations, in the future we plan to conduct further cleansing and filtering on the current meta-training data. Besides, we intend to evaluate the few-shot performance of our framework in the multilingual setting and also broaden the scope of tasks, including retrieval (Pan et al., 2023), language generation (Li et al., 2022c) and vision-language tasks (Li et al., 2023b; Chen et al., 2023; Li et al., 2022a; Zhang et al., 2022a). Furthermore, we hope our work could pave the way for future research on better leveraging parameterefficient methods under few-shot settings."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work has been supported in part by the Zhejiang NSF (LR21F020004), Key Research and Development Projects in Zhejiang Province (No. 2023C01030, 2023C01032), NSFC (No. 62272411), National Key Research and Development Program of China (2018AAA0101900), Ant Group and Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies."
        },
        {
            "heading": "A Additional Information for SUPMER",
            "text": "A.1 Complete Analysis of SUPMER\nIn this section, we provide a more comprehensive and complete analysis of SUPMER. We will show that during meta-training, the optimization of soft prompt embeddings \u03b8 and the meta-gradient regularization parameters \u03d5 tends to maximize the inner product of gradients obtained from the support set after regulation and gradients from the query set.\nSpecifically, to update the parameters \u03b8 and \u03d5, we should evaluate their gradients at first, denoting them as g\u03b8 and g\u03d5. Considering the original algorithm of MAML, each task consists of a support set and a query set. And only one step of gradient descent is applied in the inner-loop optimization. To make our statement more direct, we denote the loss function based on the support set and the query set as L0 and L1. In SUPMER, ignoring the regularized loss, only L1 is directly utilized to optimize \u03d5, while \u03b8 is optimized in a bi-level meta-optimization paradigm. Here we define the following terms related to \u03b8 similar to Nichol et al. (2018):\ng\u03b8i = \u2202Li(\u03b8i) \u2202\u03b8i\n(gradient obtained during SGD)\ng\u03b8i = \u2202Li(\u03b80) \u2202\u03b80\n(gradient at initial point)\nH \u03b8 i = \u22022Li(\u03b80) \u2202\u03b820\n(Hessian at initial point)\n\u03b81 = \u03b80 \u2212 \u03b11\u03c8\u03d5(g\u03b80) (gradient descent in the inner-loop) (11) For each definition i \u2208 {0, 1} and \u03c8\u03d5(\u00b7) is the meta-gradient regularization operation. \u03b80 denotes the initial soft prompt embeddings for each step, and \u03b81 denotes the embeddings after the inner-loop optimization. Obviously we have g\u03b80 = g \u03b8 0. Firstly we perform a Taylor series expansion to approximate the SGD gradients g\u03b81 obtained from the query set as follows:\ng\u03b81 = \u2202L1(\u03b81) \u2202\u03b81\n= \u2202L1(\u03b80) \u2202\u03b80 + \u22022L1(\u03b80) \u2202\u03b820 (\u03b81 \u2212 \u03b80) +O(||\u03b81 \u2212 \u03b80||2)\ufe38 \ufe37\ufe37 \ufe38 =O(\u03b121) = g\u03b81 \u2212 \u03b11H \u03b8 1\u03c8\u03d5(g \u03b8 0) +O(\u03b1 2 1)\n(12)\nThen we analysis the gradient descent operation in the inner-loop optimization based on the support set. Define U as the gradient descent and we have U(\u03b80) = \u03b80 \u2212 \u03b11\u03c8\u03d5(\u2202L0(\u03b80)\u2202\u03b80 ). So we can get\n\u2202U(\u03b80) \u2202\u03b80 and \u2202U(\u03b80)\u2202\u03d5 as follows:\n\u2202U(\u03b80)\n\u2202\u03b80 =\n\u2202\n\u2202\u03b80 (\u03b80 \u2212 \u03b11\u03c8\u03d5( \u2202L0 \u2202\u03b80 ))\n= I \u2212 \u03b11 \u2202\u03c8\u03d5(g\n\u03b8 0)\n\u2202g\u03b80 \u00b7 \u2202g\n\u03b8 0 \u2202\u03b80\n= I \u2212 \u03b11 \u2202\u03c8\u03d5(g\n\u03b8 0)\n\u2202g\u03b80 \u00b7H\u03b80\n(13)\n\u2202U(\u03b80)\n\u2202\u03d5 =\n\u2202\n\u2202\u03d5 (\u03b80 \u2212 \u03b11\u03c8\u03d5( \u2202L0 \u2202\u03b80 ))\n= \u2212\u03b11 \u2202\u03c8\u03d5(g\n\u03b8 0)\n\u2202\u03d5\n(14)\nSo based on Eq. (12, 13, 14), we can finally approximate the gradients g\u03b8 and g\u03d5 as:\ng\u03b8 = \u2202L1(\u03b81) \u2202\u03b80\n= \u2202L1(U(\u03b80))\n\u2202\u03b80\n= \u2202L1 \u2202\u03b81 \u00b7 \u2202U(\u03b80) \u2202\u03b80\n= g\u03b81 \u2212 \u03b11H \u03b8 1\u03c8\u03d5(g \u03b8 0)\u2212 \u03b11g \u03b8 1 \u00b7\n\u2202\u03c8\u03d5(g \u03b8 0)\n\u2202g\u03b80 \u00b7H\u03b80 +O(\u03b121)\n= g\u03b81 \u2212 \u03b11 \u2202\n\u2202\u03b80 (g\u03b81\u03c8\u03d5(g \u03b8 0)) +O(\u03b1 2 1)\ng\u03d5 = \u2202L1(\u03b81) \u2202\u03d5\n= \u2202L1(U(\u03b80)\n\u2202\u03d5\n= \u2202L1 \u2202\u03b81 \u00b7 \u2202U(\u03b80) \u2202\u03d5\n= \u2212\u03b11g\u03b81 \u2202\u03c8\u03d5(g\n\u03b8 0)\n\u2202\u03d5 +O(\u03b121)\n= \u2212\u03b11 \u2202\n\u2202\u03d5 (g\u03b81\u03c8\u03d5(g \u03b8 0)) +O(\u03b1 2 1)\n(15)\nThus, \u2212 \u2202\u2202\u03b80 (g \u03b8 1\u03c8\u03d5(g \u03b8 0)) and \u2212 \u2202\u2202\u03d5(g \u03b8 1\u03c8\u03d5(g \u03b8 0)) indicate the optimization direction, which increases the inner product between gradients from the query set and gradients from the support set after transformation. To further consolidate our analysis, we also track the normalized gradient inner product in the first 5000 steps during meta-training. As shown in Figure 5, it is clear that the normalized gradient inner product gradually increases during meta-training.\nOn this basis, since there exists distribution shift between the support set and the query set after task augmentation, our method aligns the gradient directions across different distributions, which helps enhance model generalization. In other words, the trainable parameters descend in a coordinated manner such that the input-output correspondence is as\nclose as possible across two distributions with deviation. Besides, the meta-gradient regularization parameters \u03d5 also retain some domain-invariant information of the meta-training data in the above process. Considering that \u03d5 is fixed in downstream tasks, \u03d5 can be applied to encourage the alignment between the domain-specific gradients and avoid prompt-tuning overfitting to some domain-specific correlations.\nA.2 Constructing Anchor Meta Tasks\nGiven a sentence x from unlabeled corpora, we can derive semantically meaningful sentence embedding H = fenc\u03b8 (x) with PLMs, e.g., T5 encoder. And we apply K-means to cluster these unlabeled sentences according to their embeddings:\nP, {\u00b5c} = arg min {Cc},{\u00b5c} K\u2211 c=1 \u2211 H\u2208Cc \u2225H \u2212 \u00b5c\u22252 (16)\nwhere \u00b5c indicates the learned centroid of cluster Cc and P indicates the partitions of all sentences. K-means clustering leads to more abundant formats and objectives of meta-training tasks. Based on the results of K-means, we design three formats of anchor self-supervised meta-training tasks: sentencepair classification, multi-choice classification, and single-text classification. Here we introduce each of them in detail.\nSentence-pair Classification. Sentence-pair classification takes a pair of sentences (x0, x1) as input, and x0 is the anchor sentence. We carry on next sentence prediction task and sentence similarity task in sentence-pair classification with the label list Y = [0, 1, 2]. For the former one, following Gu et al. (2022), we set two sentences next to each other as label 0, those from the same document but not adjacent as label 2, and those from different documents as label 1. And for\nAlgorithm 1 Meta-training Process of SUPMER 1: p(T ) : Distribution over anchor tasks 2: f\u03b8 : PLM with soft prompt embeddings \u03b8 3: \u03c8\u03d5 : Meta-gradient regularization 4: \u03b11, \u03b21, \u03b22 : Learning rate 5: TA : Task augmentation in Algorithm 2\n6: s\u2190 \u22121 7: Randomly initialize \u03b8, \u03d5 8: while not done do 9: Sample a batch of task {\u03c4i}ni=1 from p(T )\n10: {\u03c4i}ni=1 = TA({\u03c4i}ni=1, p(T ), s) 11: for all \u03c4i = {Ds\u03c4i ,D q \u03c4i} do 12: Evaluate\u2207\u03b8LDs\u03c4i (f\u03b8) with D s \u03c4i 13: Evaluate\u2207\u03b8LDq\u03c4i (f\u03b8) with D q \u03c4i 14: Transform \u2207\u03b8LDs\u03c4i (f\u03b8) via \u03c8\u03d5(\u00b7) 15: si = \u2207\u03b8LDq\u03c4i (f\u03b8)\u00b7\u03c8\u03d5(\u2207\u03b8LDs\u03c4i (f\u03b8))\n\u2225\u2207\u03b8LDq\u03c4i (f\u03b8)\u2225\u00b7\u2225\u03c8\u03d5(\u2207\u03b8LDs\u03c4i (f\u03b8))\u2225\n16: \u03b8\u2032i = \u03b8 \u2212 \u03b11\u03c8\u03d5(\u2207\u03b8LDs\u03c4i (f\u03b8)) 17: end for 18: s\u2190 \u2211 i si/ \u2211 i 1\n19: \u03b8 \u2190 \u03b8 \u2212 \u03b21\u2207\u03b8 \u2211\n\u03c4i LDq\u03c4i (f\u03b8\u2032i) 20: \u03d5\u2190 \u03d5\u2212 \u03b22\u2207\u03d5 (\u2211\n\u03c4i LDq\u03c4i (f\u03b8\u2032i) + Lreg ) 21: end while 22: return \u03b8, \u03d5\nsentence similarity task, we set two sentences coming from the same cluster as label 0, and those from different clusters as label 1. In this way, the prompt template and verbalizer are designed as:\nP = \u201cs1 \u27e8X\u27e9 .s2\u201d V = {0\u2192 yes, 1\u2192 no, 2\u2192 maybe}\n(17)\nMulti-choice Classification. Multi-choice classification takes an anchor sentence x0 as the query and we should find the correct one in several answer candidates. Here we also set two different tasks. The first one aims to select the sentence next to s0 and the second one aims to select the sentence which belongs to the same cluster as s0. In each task we will set four candidates, and only one of them is correct. We design the prompt template and verbalizer as follows:\nP = \u201cs0? A.s1 \u00b7 \u00b7 \u00b7D.s4. Answer: \u27e8X\u27e9 \u201d V = {0\u2192 A, 1\u2192 B, 2\u2192 C, 3\u2192 D}\n(18)\nSingle-Sentence Classification. Through Kmeans clustering, each sentence is associated with a cluster label ri in {0, 1}K where ric = 1 if c = k and yic = 0 if c \u0338= k. Here k represents the cluster\nto which the sentence belongs. We simply use ri as the pseudo label for meta-training and construct 4-way classification tasks. As for the designing of the verbalizer, we transform the single-sentence classification into the format of multi-choice classification. We insert the centroid of cluster \u00b5c into the template and use it to represent the corresponding cluster. So that we have:\nP = \u201cs0? A. \u27e8\u00b5c1\u27e9 \u00b7 \u00b7 \u00b7D. \u27e8\u00b5c4\u27e9 . Answer: \u27e8X\u27e9 \u201d V = {0\u2192 A, 1\u2192 B, 2\u2192 C, 3\u2192 D} (19)\nOn this basis, for each task format, we separate all data into different tasks to construct anchor meta-training tasks with good task distributions. Through K-means, sentences with similar embeddings are clustered into the same group. So in sentence-pair classification and multi-choice classification, we group samples whose anchor sentence comes from the same cluster into the same metatraining task. And in single-sentence classification, for each meta-training task, we randomly select N clusters as N classes and then sample k sentences for each cluster to construct a N -way k-shot classification task (N = 4). In this way, we completely construct all anchor meta-training tasks.\nA.3 Additional Loss to Train Meta-Gradient Regularization Parameters\nIn the meta-training stage, we optimize the metagradient regularization parameters \u03d5 via Eq. (7), utilizing the same loss which optimizes the soft prompt embeddings. Here we introduce a regularized loss to attach some additional restrictions when updating the meta-gradient regularization parameters. Notably, a higher value of bk in Eq. (10) indicates a higher probability of a larger distribution deviation between the support set and the query set. Furthermore, in Eq. (5) we also tend to increase z to achieve a more pronounced gradient transformation with a more noticeable distribution deviation. From this perspective, z has a similar monotonicity with bk, and they both range between 0 and 1. Thus we further add a regularized loss Lreg = \u2225z \u2212 bk\u22252 to constrain the value of z and finally modify Eq. (7) into:\n\u03d5\u2190 \u03d5\u2212 \u03b22\u2207\u03d5 ( \u2211 \u03c4i\u223cp(T ) LDq\u03c4i (f\u03b8\u2032i) + \u03bbLreg )\n(20)\nA.4 Pseudo-Codes of SUPMER We show the pseudo-codes for the meta-training process of SUPMER in Alg. 1. And the process of curriculum-based task augmentation is described in Alg. 2.\nAlgorithm 2 TA : Curriculum-based Task Augmentation\n1: {\u03c4i}ni=1 : A batch of anchor tasks 2: p(T ) : Distribution over anchor tasks 3: s \u2208 [\u22121, 1] : Avg cos-sim between gradients 4: \u03b1, m : hyper-parameters\n5: s\u2190 (1 + s)/2 6: b\u2190 (ms \u2212 1)/(m\u2212 1) 7: for all \u03c4i = {Ds\u03c4i ,D q \u03c4i} do 8: Sample task \u03c4j = {Ds\u03c4j ,D q \u03c4j} from p(T ) 9: Draw \u03bb from Beta(\u03b1, b\u03b1) // Dq\u03c4i = (H q i ,Y q i ),D q \u03c4j = (H q j ,Y q j )\n// H: the hidden representations of samples 10: H\u0303qi = (1\u2212 \u03bb)H q i + \u03bbH q j 11: Y\u0303 qi = (1\u2212 \u03bb)Y q i + \u03bbY q j 12: Dq\u03c4i \u2190 (H\u0303 q i , Y\u0303 q i ) 13: end for 14: return {\u03c4i}ni=1"
        },
        {
            "heading": "B Dataset & Baseline Details",
            "text": "Few-shot Learning. We conduct experiments of few-shot learning on 6 different downstream English tasks with 12 datasets. Since some of the test sets of the datasets are not publicly available, following Karimi Mahabadi et al. (2022), we leverage the original validation sets of SST-2, CB, RTE, QNLI, WiC, MRPC, and QQP1 as substitutes for the unavailable test sets. And the validation sets for few-shot learning are sampled from the original training set, ensuring no overlap with our designated test sets. Besides, we download the datasets of SST-2, SST-5, MR, CR, and SUBJ from Gao et al. (2021). And the rest of the datasets are obtained from the HuggingFace Datasets library (Lhoest et al., 2021). CB, RTE, BoolQ, and Wic are from SuperGLUE Benchmark (Wang et al., 2019), while QNLI, MRPC, and QQP are from GLUE Benchmark (Wang et al., 2018) with Creative Commons license (CC BY 4.0). We give the statistics of all these datasets in Table 5.\nDomain Generalization. Similar to Calderon et al. (2022), We evaluate on the sentiment analysis task including 6 different domains: Airlines (A), Books (B), DVDs (D), Electronics (E), Kitchen appliances (K), and Restaurants (R). Each domain has totally 2,000 manually labeled data of binary categories for testing, including 1000 positive and\n1https://quoradata.quora.com/\n1000 negative. We choose A as the source domain and the other five (B, D, E, K, R) constitute the target domains. We sample 16 instances per label from the training set of the source domain for prompt tuning and then evaluate on the test sets of all 6 domains.\nBaselines. We first compare with baseline methods with the same number of parameters as SUPMER. These methods utilize prompt tuning (Lester et al., 2021) to handle downstream tasks, with the key distinction lying in the initialization of the soft prompts. Vallina prompt tuning (PT Lester et al., 2021) directly tunes the soft prompts in the downstream task, which are randomly initialized from a normal distribution. PPT (Gu et al., 2022) pretrains soft prompts in a self-supervised way with 3 formats of pre-training tasks: sentence-pair classification, multiple-choice classification and singletext classification. Unified-PPT (Gu et al., 2022) formulate all these three formats into a unified task form. MetaPT (Huang et al., 2022) using a supervised sentiment analysis dataset Yelp5 as the meta-training data and directly leveraging MAML to initialize soft prompts.\nTo further demonstrate the effectiveness of our method, we also consider baseline methods with more tunable parameters, including PrefixTuning (Li and Liang, 2021) and P-tuning-v2 (Liu et al., 2022), which add prompts at each layer of PLM. We also compare with full-model tuning (FT) that fine-tunes all parameters of the PLM.\nGiven that FLAN-T5-XL was also designed with few-shot inference in mind, we newly compare with two baseline methods on FLAN-T5-XL: zero-\nshot inference and few-shot inference. For both of them, we directly employ Flan-T5-XL for downstream evaluation, coupled with carefully designed task instructions for each dataset. Furthermore, in few-shot inference, we also provide an appropriate number of few-shot examples to form a demonstration context."
        },
        {
            "heading": "C Training Details",
            "text": "We apply the T5 base model (Raffel et al., 2020) (220M parameters) and Flan-T5-XL model (Chung et al., 2022) (3B parameters) as the underlying PLM, and use the HuggingFace Pytorch implementation (Wolf et al., 2020). We run experiments with 8 GeForce RTX 3090 24G GPUs. And the metatraining process of SUPMER takes about 140 GPU hours. Next we will describe the details of training hyper-parameters in the case of leveraging T5-base as the PLM.\nC.1 Training Hyper-parameters for Downstream Tasks\nIn our experiments, we leverage full-model tuning and prompt tuning to solve downstream tasks, including few-shot learning and domain generalization. In few-shot learning, following some prior work (Schick and Sch\u00fctze, 2021; Karimi Mahabadi et al., 2022), we set the maximum sequence length of each example to 256 for CR, SUBJ, CB, RTE and WiC, and 128 for other datasets. While in domain generalization, the maximum sequence length of each example is set to 256.\nWe run each experiment 5 times on the random\nseed [10, 20, 30, 40, 50] and report the average accuracy as well as the standard deviation. For both full-model tuning and prompt tuning, We implement AdamW as the optimizer. We use a batch size of 32 and train the model for 200 epochs, meanwhile evaluating the model every 10 steps. And we report the results for hyper-parameters performing the best on the validation set for each task.\nBesides, for full-model tuning, all parameters of PLM are fine-tuned without adding soft prompts. We use the learning rate of [1e-5, 2e-5, 3e-5] and choose the one obtaining the highest validation performance. Moreover, to fine-tune the Flan-T5XL model, we use ZeRO (Rajbhandari et al., 2020) stage-2 provided in DeepSpeed (Rasley et al., 2020) to reduce GPU memory usage.\nFor prompt tuning, we freeze all PLM parameters and only tune soft prompts composed of 100 soft tokens. As a result, the tunable parameters of prompt tuning are only 77K with T5-base and 205K with Flan-T5-XL, updating around 3000 and 15000 times fewer parameters on T5-base and Flan-T5-Xl, respectively, compared to full-model tuning. And we find that prompt tuning requires a much larger learning rate than full-model tuning. We search for the learning rate in [1e-1, 2e-1, 3e-1] and also choose the model with the best performance on the\nvalidation set.\nC.2 Training Hyper-parameters for Prompt Initialization\nPre-training for prompt initialization. Gu et al. (2022) proposes two frameworks for unsupervised prompt pre-training, named PPT and Unified PPT. PPT designs three formats of unsupervised pre-training tasks (sentence-pair classification, multiple-choice classification and single-text classification), and Unified-PPT further formulate them into a unified task form. We implement PPT and Unified-PPT following the hyper-parameters provided in Gu et al. (2022) and reset the pretrained language model to T5-base and Flan-T5XL. Specifically, for both PPT and Unified-PPT, we sample 10GB of unlabeled data from OpenWebText to construct pre-training tasks for each task format. And 5% data are split for validation. We apply the \u201cinverse square root\u201d learning rate scheduler with no warm-up steps and set the learning rate as 0.1. We set the batch size to 256 with the max sequence length as 512, and train soft prompts for at most 200,000 steps. We evaluate the performance on the validation set every 2,000 steps and choose prompts with the lowest validation loss.\nMeta-training for prompt initialization. In our SUPMER framework, we sample 10GB of unlabeled data from OpenWebText to construct selfsupervised meta-training tasks. We split 5% data to construct tasks for validation. And for each task format, we first set the number of clusters to 250. We sample 4 meta-training tasks in a batch, and train the prompt embeddings \u03b8 and the meta-gradient regularization parameters \u03d5 for at most 100,000 steps. We also evaluate the performance on the validation set every 2,000 steps, choosing \u03b8 and \u03d5 with the lowest validation loss for downstream tasks. Table 6 lists all training hyper-parameters for SUPMER. It is worth noting that for most hyperparameters in Table 6, we just set a default value by experience without tuning them. we tune the hyperparameters which are also tuned in other baselines (e.g., learning rate), ensuring all methods have the same number of tunable hyper-parameters in our experiments.\nMoreover, to illustrate the superiority of self-supervised meta-learning, we also imitate MetaPT(Huang et al., 2022) to initialize soft prompts via supervised meta-learning. MetaPT uses a supervised sentiment analysis dataset Yelp5 as the meta-training data, which has 650,000 training samples only covering the domain of restaurants. Following Huang et al. (2022), We group all labeled data into 10 clusters through K-means. And we set the inner loop learning rate to 0.08, the outer loop learning rate to 0.025 with the early stop patience as 6. Other hyper-parameters are consistent with those in SUPMER."
        },
        {
            "heading": "D Full Results of Ablation Study",
            "text": "In this section, we first give detailed experimental results of the ablation study to illustrate the effect of individual components. We evaluate each ablation model over all 12 datasets of few-shot learning and all 6 domains of domain generalization, with\nT5-base as the underlying PLM. We run each experiment 5 times on the random seed [10, 20, 30, 40, 50] and report the average performances as well as the standard deviation. The detailed results of few-shot learning and domain generalization are shown in Table 7 and Table 8. We can see each component is critical in our framework.\nBesides, in \u00a74.4, to explore the superiority of self-supervised meta-learning and the impact of integrating additional labeled data for soft prompt initialization, we conduct experiments of few-shot learning on T5-base, considering different data and methods for soft prompt initialization. We also carry out experiments of domain generation leveraging different data with various prompt initialization methods, with the results presented in Table 9. From Table 3 and 9, it is evident that selfsupervised meta-learning utilizing unlabeled data exhibits enhanced adaptability to unseen tasks in comparison to its supervised counterparts. And amalgamating both labeled and unlabeled data for the construction of meta-training tasks emerges as a more advantageous strategy. When it comes to employing both labeled and unlabeled data for prompt initialization, SUPMER continues to showcase markedly superior results in contrast to baseline methods in the realms of both few-shot learning and domain generalization."
        }
    ],
    "title": "Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization",
    "year": 2023
}