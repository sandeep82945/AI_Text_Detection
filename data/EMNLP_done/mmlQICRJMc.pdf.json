{
    "abstractText": "Recent work has found that few-shot sentence classification based on pre-trained Sentence Encoders (SEs) is efficient, robust, and effective. In this work, we investigate strategies for domain-specialization in the context of fewshot sentence classification with SEs. We first establish that unsupervised Domain-Adaptive Pre-Training (DAPT) of a base Pre-trained Language Model (PLM) (i.e., not an SE) substantially improves the accuracy of few-shot sentence classification by up to 8.4 points. However, applying DAPT on SEs, on the one hand, disrupts the effects of their (general-domain) Sentence Embedding Pre-Training (SEPT). On the other hand, applying general-domain SEPT on top of a domain-adapted base PLM (i.e., after DAPT) is effective but inefficient, since the computationally expensive SEPT needs to be executed on top of a DAPT-ed PLM of each domain. As a solution, we propose AdaSent, which decouples SEPT from DAPT by training a SEPT adapter on the base PLM. The adapter can be inserted into DAPT-ed PLMs from any domain. We demonstrate AdaSent\u2019s effectiveness in extensive experiments on 17 different few-shot sentence classification datasets. AdaSent matches or surpasses the performance of full SEPT on DAPT-ed PLM, while substantially reducing the training costs. The code for AdaSent is available1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yongxin Huang"
        },
        {
            "affiliations": [],
            "name": "Kexin Wang"
        },
        {
            "affiliations": [],
            "name": "Sourav Dutta"
        },
        {
            "affiliations": [],
            "name": "Raj Nath Patel"
        },
        {
            "affiliations": [],
            "name": "Goran Glava\u0161"
        },
        {
            "affiliations": [],
            "name": "Iryna Gurevych"
        }
    ],
    "id": "SP:90ec82f0c21796e495943cd1660b7f54897f2c70",
    "references": [
        {
            "authors": [
                "Samuel R. Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D. Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2015
        },
        {
            "authors": [
                "Nikolaos Aletras"
            ],
            "title": "LexGLUE: A benchmark",
            "year": 2022
        },
        {
            "authors": [
                "Kristina Toutanova"
            ],
            "title": "2019. BERT: Pre-training",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen."
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Do-",
            "year": 2021
        },
        {
            "authors": [
                "Harsha Gurulingappa",
                "Abdul Mateen Rajput",
                "Angus Roberts",
                "Juliane Fluck",
                "Martin Hofmann-Apitius",
                "Luca Toldo"
            ],
            "title": "Development of a benchmark corpus to support the automatic extraction of drugrelated adverse effects from medical case reports",
            "year": 2012
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Ana Marasovi\u0107",
                "Swabha Swayamdipta",
                "Kyle Lo",
                "Iz Beltagy",
                "Doug Downey",
                "Noah A. Smith."
            ],
            "title": "Don\u2019t stop pretraining: Adapt language models to domains and tasks",
            "venue": "Proceedings of the 58th Annual Meeting of the",
            "year": 2020
        },
        {
            "authors": [
                "Xiaochuang Han",
                "Jacob Eisenstein."
            ],
            "title": "Unsupervised domain adaptation of contextualized embeddings for sequence labeling",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
            "year": 2019
        },
        {
            "authors": [
                "Junxian He",
                "Chunting Zhou",
                "Xuezhe Ma",
                "Taylor BergKirkpatrick",
                "Graham Neubig."
            ],
            "title": "Towards a unified view of parameter-efficient transfer learning",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-",
            "year": 2022
        },
        {
            "authors": [
                "Matthew Henderson",
                "Rami Al-Rfou",
                "Brian Strope",
                "Yun hsuan Sung",
                "Laszlo Lukacs",
                "Ruiqi Guo",
                "Sanjiv Kumar",
                "Balint Miklos",
                "Ray Kurzweil."
            ],
            "title": "Efficient natural language response suggestion for smart reply",
            "venue": "ArXiv, abs/1705.00652.",
            "year": 2017
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly."
            ],
            "title": "Parameter-efficient transfer learning for NLP",
            "venue": "In",
            "year": 2019
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen."
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "Luke Zettlemoyer",
                "James Henderson",
                "Lambert Mathias",
                "Marzieh Saeidi",
                "Veselin Stoyanov",
                "Majid Yazdani."
            ],
            "title": "Promptfree and efficient few-shot learning with language models",
            "venue": "Proceedings of the 60th Annual Meet-",
            "year": 2022
        },
        {
            "authors": [
                "Gregory Koch",
                "Richard Zemel",
                "Ruslan Salakhutdinov"
            ],
            "title": "Siamese neural networks for one-shot image recognition",
            "venue": "In ICML deep learning workshop,",
            "year": 2015
        },
        {
            "authors": [
                "Ananya Kumar",
                "Aditi Raghunathan",
                "Robbie Matthew Jones",
                "Tengyu Ma",
                "Percy Liang."
            ],
            "title": "Finetuning can distort pretrained features and underperform out-of-distribution",
            "venue": "The Tenth International Conference on Learning Representations, ICLR 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang."
            ],
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "venue": "Bioinformatics, 36(4):1234\u20131240.",
            "year": 2019
        },
        {
            "authors": [
                "Haoran Li",
                "Abhinav Arora",
                "Shuohui Chen",
                "Anchit Gupta",
                "Sonal Gupta",
                "Yashar Mehdad."
            ],
            "title": "MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Shiyang Li",
                "Semih Yavuz",
                "Wenhu Chen",
                "Xifeng Yan."
            ],
            "title": "Task-adaptive pre-training and self-training are complementary for natural language understanding",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1006\u20131015,",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang."
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Liu",
                "Samuel Yang."
            ],
            "title": "Masked autoencoders as the unified learners for pre-trained sentence representation",
            "venue": "ArXiv, abs/2208.00231.",
            "year": 2022
        },
        {
            "authors": [
                "Haokun Liu",
                "Derek Tam",
                "Mohammed Muqeeth",
                "Jay Mohta",
                "Tenghao Huang",
                "Mohit Bansal",
                "Colin A Raffel."
            ],
            "title": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Andrew L. Maas",
                "Raymond E. Daly",
                "Peter T. Pham",
                "Dan Huang",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human",
            "year": 2011
        },
        {
            "authors": [
                "Pekka Malo",
                "Ankur Sinha",
                "Pekka J. Korhonen",
                "Jyrki Wallenius",
                "Pyry Takala."
            ],
            "title": "Good debt or bad debt: Detecting semantic orientations in economic texts",
            "venue": "J. Assoc. Inf. Sci. Technol., 65(4):782\u2013796.",
            "year": 2014
        },
        {
            "authors": [
                "Julian McAuley",
                "Jure Leskovec."
            ],
            "title": "Hidden factors and hidden topics: Understanding rating dimensions with review text",
            "venue": "Proceedings of the 7th ACM Conference on Recommender Systems, RecSys \u201913, page 165\u2013172, New York, NY, USA. Association",
            "year": 2013
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Nouamane Tazi",
                "Loic Magne",
                "Nils Reimers."
            ],
            "title": "MTEB: Massive text embedding benchmark",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2014\u20132037, Dubrovnik,",
            "year": 2023
        },
        {
            "authors": [
                "James O\u2019Neill",
                "Polina Rozenshtein",
                "Ryuichi Kiryo",
                "Motoko Kubota",
                "Danushka Bollegala"
            ],
            "title": "I wish I would have loved this one, but I didn\u2019t \u2013 a multilingual dataset for counterfactual detection in product review",
            "year": 2021
        },
        {
            "authors": [
                "Raj Nath Patel",
                "Edward Burgin",
                "Haytham Assem",
                "Sourav Dutta."
            ],
            "title": "Efficient multi-lingual sentence classification framework with sentence meta encoders",
            "venue": "2021 IEEE International Conference on Big Data (Big Data), pages 1889\u20131899.",
            "year": 2021
        },
        {
            "authors": [
                "Ethan Perez",
                "Douwe Kiela",
                "Kyunghyun Cho."
            ],
            "title": "True few-shot learning with language models",
            "venue": "Advances in Neural Information Processing Systems 34:",
            "year": 2021
        },
        {
            "authors": [
                "Christian S. Perone",
                "Roberto Silveira",
                "Thomas S. Paula."
            ],
            "title": "Evaluation of sentence embeddings in downstream and linguistic probing tasks",
            "venue": "ArXiv, abs/1806.06259.",
            "year": 2018
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Iryna Gurevych",
                "Sebastian Ruder."
            ],
            "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Guangyuan Piao."
            ],
            "title": "Scholarly text classification with sentence BERT and entity embeddings",
            "venue": "Trends and applications in knowledge discovery and data mining, pages 79\u201387, Cham. Springer International Publishing.",
            "year": 2021
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Vin Sachidananda",
                "Jason Kessler",
                "Yi-An Lai."
            ],
            "title": "Efficient domain adaptation of language models via adaptive tokenization",
            "venue": "Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing, pages 155\u2013165, Virtual. Association for",
            "year": 2021
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "venue": "ArXiv, abs/1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Elvis Saravia",
                "Hsien-Chi Toby Liu",
                "Yen-Hao Huang",
                "Junlin Wu",
                "Yi-Shin Chen."
            ],
            "title": "CARER: Contextualized affect representations for emotion recognition",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze."
            ],
            "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Hongjin Su",
                "Weijia Shi",
                "Jungo Kasai",
                "Yizhong Wang",
                "Yushi Hu",
                "Mari Ostendorf",
                "Wen tau Yih",
                "Noah A. Smith",
                "Luke Zettlemoyer",
                "Tao Yu."
            ],
            "title": "One embedder, any task: Instruction-finetuned text embeddings",
            "venue": "ArXiv, abs/2212.09741.",
            "year": 2023
        },
        {
            "authors": [
                "Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all",
            "year": 2017
        },
        {
            "authors": [
                "ong",
                "Quoc V. Le"
            ],
            "title": "Unsupervised data aug",
            "year": 2020
        },
        {
            "authors": [
                "Jingtao Zhan",
                "Qingyao Ai",
                "Yiqun Liu",
                "Jiaxin Mao",
                "Xiaohui Xie",
                "Min Zhang",
                "Shaoping Ma."
            ],
            "title": "Disentangled modeling of domain and relevance for adaptable dense retrieval",
            "venue": "CoRR, abs/2208.05753.",
            "year": 2022
        },
        {
            "authors": [
                "Lei Zhao",
                "Cheng Yao."
            ],
            "title": "EICO: Improving few-shot text classification via explicit and implicit consistency regularization",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 3582\u20133587, Dublin, Ireland. Association for",
            "year": 2022
        },
        {
            "authors": [
                "Yukun Zhu",
                "Ryan Kiros",
                "Richard S. Zemel",
                "Ruslan Salakhutdinov",
                "Raquel Urtasun",
                "Antonio Torralba",
                "Sanja Fidler."
            ],
            "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
            "venue": "2015 IEEE Interna-",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Few-shot learning aims at training an effective model with a few labeled examples, reducing the cost of developing models for new domains and tasks. In recent work, SetFit (Tunstall et al., 2022) achieves strong performance in few-shot classification by contrastively fine-tuning (Koch et al., 2015)\n1https://github.com/UKPLab/AdaSent\n09.05.2023 Computer Science Department | UKP Lab \u2013 Iryna Gurevych | Yongxin Huang 15\nSEPT on paraphrase data in general domain\nPLM\nDAPT on task-specific unlabeled data\nSetFit on few-shot labeled data\nPLM Adapter\nPLM Adapter\nFigure 1: Training diagram of AdaSent. Trainable parameters are marked in green. After Domain-Adaptive Pre-training (DAPT) on the Pre-Trained Language Model (PLM) and Sentence-Embedding Pre-Training (SEPT) with an adapter, the two parts are assembled together to perform SetFit for few-shot classification.\npre-trained sentence embeddings. Being promptfree and effective on relative small models, SetFit is much more efficient than popular promptbased methods including In-Context Learning (ICL, Brown et al., 2020) and Pattern Exploit Training (PET, Schick and Sch\u00fctze, 2021), which require careful prompt engineering and large model size.\nDespite its success, SetFit fine-tunes a sentence encoder with only a few labeled samples without leveraging unlabeled data from the target-task domain, which are easy to obtain. It is well-known that Domain-Adaptive Pre-Training (DAPT)2 on a vanilla PLM with unlabeled in-domain data can significantly improve its downstream performance (Han and Eisenstein, 2019; Gururangan et al., 2020). However, it is ineffective to apply\n2By DAPT we refer to the TAPT (Task-Adaptive PreTraining) in Gururangan et al. (2020). We do not strictly differentiate between domain and task in the present work.\nDAPT on sentence encoders, i.e. vanilla PLMs that have undergone Sentence Embedding Pre-Training (SEPT, Reimers and Gurevych, 2019) in general domain, as DAPT messes up the effects of SEPT and disrupts the model\u2019s ability to semantically accurately embed sentences. Though DAPT before SEPT is effective in contrast (Wang et al., 2021), it is computationally inefficient as the generaldomain SEPT has to be done all over again on every domain-adapted PLM if we have more than one domain.\nTo create a domain-specialized sentence encoder for few-shot sentence classification both efficiently and effectively, we propose AdaSent, which combines DAPT and SEPT in a modular fashion. Specifically, it stores the sentence-specialization abilities \u2013 obtained via a single SEPT procedure in the general domain \u2013 into an adapter. This sentenceencoding adapter is trained once regardless of the number of domains, and can be plugged into domain-adapted PLMs from various domains to make them domain-specialized sentence encoders, on which SetFit is carried out to do downstream classification training (Figure 1). Our experiments show that AdaSent can match or surpass the inefficient \"full SEPT after DAPT\" approach\u2019s performance on 17 sentence classification tasks from various domains. The contribution of AdaSent is two-fold:\n\u2022 AdaSent significantly improves SetFit, the previous state-of-the-art few-shot classification approach, by leveraging unlabeled taskspecific data through DAPT.\n\u2022 AdaSent resolves the conflict between DAPT and SEPT and the efficiency issue of the sequential execution of both training procedures, by combining them in a modular fashion without sacrificing the performance."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Text Classification with Sentence Embeddings",
            "text": "Transformer-based (Vaswani et al., 2017) Pretrained Language Models (PLMs) (Devlin et al., 2019; Liu et al., 2019; Sanh et al., 2019) can be fine-tuned to build sentence embedding models (Reimers and Gurevych, 2019). Since the original goal of training sentence embeddings is to better model the sentence similarity for applications such as dense retrieval and sentence cluster-\ning (Reimers and Gurevych, 2019), their usage is less explored in text classification. Though frozen sentence embeddings can directly serve as input features in text classification (Perone et al., 2018; Piao, 2021), the performance is limited compared to standard full fine-tuning of PLMs (Kumar et al., 2022). To compensate this performance loss, Patel et al. (2021) concatenate encodings from various Sentence Transformers to form semantically richer sentence representations, achieving results comparable to standard fine-tuning, but at the cost of slower inference. More recently, SetFit (Tunstall et al., 2022) significantly improves the few-shot classification by contrastively fine-tuning a pretrained sentence-embedding model before training a classification head. Despite efficiently utilizing the limited labeled samples, SetFit does not leverage the abundant in-domain unlabeled data that can provide more domain knowledge for the task."
        },
        {
            "heading": "2.2 Few-Shot Text Classification",
            "text": "Large language models can perform few-shot classification through ICL with task-specific prompts consisting of a few labeled examples (Brown et al., 2020). Though it avoids any gradient update, ICL relies on large model sizes for good performance, which makes inference costly. Prompt-based finetuning, on the other hand, can work with smaller models (Schick and Sch\u00fctze, 2021; Tam et al., 2021; Gao et al., 2021a). Parameter Efficient FineTuning (PEFT) can further reduce the training cost by fine-tuning a much smaller module in a frozen PLM (Houlsby et al., 2019; Li and Liang, 2021; Hu et al., 2022; Karimi Mahabadi et al., 2022; He et al., 2022; Liu et al., 2022). As an alternative way to employ task instructions, Su et al. (2023) train domain- and task-aware text embeddings by prepending instructions to the input text. In contrast to these methods, SetFit and our approach not only require a smaller model size, but also eliminate the need for prompts or instructions, which can introduce large variance and should be carefully designed (Perez et al., 2021)."
        },
        {
            "heading": "2.3 Domain Adaptation of Language Models",
            "text": "One typical way for creating domain-specific language models is pre-training through Masked Language Modelling on in-domain corpora, either continuously (Gururangan et al., 2020) or from-scratch (Lee et al., 2019). An alternative is adapting the tokenizer to accommodate domain-specific vocabulary (Sachidananda et al., 2021; Yao et al., 2021).\nFor sentence embedding models specifically, domain adaptation is usually done through unsupervised training with novel objectives (Wang et al., 2021; Liu and Yang, 2022) or in-domain data generation (Wang et al., 2022), mainly for the similarity or relevance estimation tasks. However, supervised sentence embedding training with general-domain data (SEPT) is always required after the unsupervised domain-specific training phase (DAPT) to achieve optimal performance (Wang et al., 2021). Our proposed method is inspired by the idea of disentangling domain adaptation and the downstream relevance estimation task via PEFT in Zhan et al. (2022). In the present study, we show that PEFT can also be used to decouple DAPT and SEPT for few-shot classification tasks."
        },
        {
            "heading": "2.4 Semi-Supervised Text Classification",
            "text": "Unsupervised data can be incorporated in various ways to improve few-shot classification. While the DAPT approaches in subsection 2.3 allow the model to learn domain-specific features in a taskagnostic way, other semi-supervised methods typically propagate task information from labeled data to unlabeled data through pseudo labeling. The pseudo-labeled data are either used for self-training (Schick and Sch\u00fctze, 2021) or consistency training (Xie et al., 2020). All these approaches can also be combined to enable more efficient use of unlabeled data (Li et al., 2021b; Chen et al., 2021; Zhao and Yao, 2022). In our experiments, we found that simple self-training using the same data for DAPT can further improve the performance of AdaSent."
        },
        {
            "heading": "3 Background",
            "text": ""
        },
        {
            "heading": "3.1 SetFit",
            "text": "SetFit (Tunstall et al., 2022) is a two-step training procedure based on pre-trained sentenceembedding Transformer models for few-shot sentence classification. In the sentence-embedding fine-tuning step, positive and negative sentence pairs are generated from few-shot labeled sentences as follows: Pairs consisting of sentences from the same class are labeled positively with a score of 1 and pairs of sentences from different classes are assigned a negative score of 0. These generated pairs are used to fine-tune the sentence-embedding model with the Cosine Similarity Loss:\nLcosine = \u2225y \u2212 cos_sim(u, v)\u22252 ,\nwhere u, v \u2208 RD are the D-dimensional sentence embeddings of two sentences respectively and y \u2208 {0, 1} is the pair label. This aims to push instances of the same classes closer together in the representation space and those from different classes further apart, thereby clustering sentences according to their class labels to provide a clearer decision boundary for the classifier training later. In the second step, the Transformer is frozen to embed the original few-shot sentences. These sentence embeddings are used as input features to train a simple Logistic Regression (Cox, 1958) classification head."
        },
        {
            "heading": "3.2 Sentence Embedding Pre-Training (SEPT)",
            "text": "As will be shown in subsection 6.2, the success of SetFit heavily relies on SEPT. This is because the averaged word representations or the [CLS] representation from a PLM cannot capture the sentence semantics well without further training with sentence-level objectives (Reimers and Gurevych, 2019). The purpose of sentence-embedding pretraining is to train universal semantic representations that can be fine-tuned for different downstream tasks, e.g. in SetFit. Unlike SetFit, sentences with similar meaning are brought closer together in SEPT, while those with dissimilar meanings are pushed apart. Sentence pairs for this kind of contrastive training are typically obtained from Natural Language Inference (NLI, Bowman et al., 2015; Williams et al., 2018) or paraphrase datasets in the general domain. Sentence pairs labeled as \"entailment\" or \"paraphrase\" in the original datasets are used as positive pairs, i.e. sentences with similar meaning, in SEPT. The MultipleNegative Ranking Loss (MNRL, Henderson et al., 2017) with in-batch negatives is usually applied for training:\nLMNRL = \u2212 1\nK K\u2211 i=1 log ecos_sim(xi,yi)\u2211K j=1 e cos_sim(xi,yj) ,\nwhere {(xi, yi)}Ki=1 are a batch of K positive sentence pairs."
        },
        {
            "heading": "3.3 Domain-Adapted Sentence Embeddings",
            "text": "The definition of sentence similarity varies from domain to domain, but labeled data for SEPT are usually expensive to obtain in specialized domains. Wang et al. (2021) found that domain-adapted sentence embedding models can be trained following a two-stage recipe: first doing unsupervised DAPT\n09.05.2023 Computer Science Department | UKP Lab \u2013 Iryna Gurevych | Yongxin Huang\n(e.g. MLM) on the domain-specific corpus, then applying supervised SEPT in the general domain (Figure 2 (1)). With this training order, if we want to train models for various domains, the same second stage has to be repeated for every domain, although it does not involve any domain-specific data. Such computational overhead cannot be avoided by simply reversing the order of the two training stages (Figure 2 (4)), since it has been shown in previous work that DAPT after the generic sentence embedding training has a negative impact on the downstream performance (Wang et al., 2021)."
        },
        {
            "heading": "4 Method",
            "text": "As illustrated in Figure 1, our method for few-shot classification with domain-adapted sentence embeddings consists of three parts of training: (1) DAPT on the base PLM with task-specific unlabeled data, (2) SEPT on an adapter module with labeled sentence pairs from the general domain and (3) SetFit on the whole architecture (i.e. both the PLM and the adapter) with few-shot labeled data.\nIn the first part, specifically, we continue to train a base PLM like DistilRoBERTa on unlabeled target task data with the MLM loss to learn domainspecific language knowledge. In another separate procedure, SEPT is done by tuning an adapter on a frozen base Transformer (the same PLM as in DAPT) without any domain adaptation. Once the domain-independent sentence encoding adapter is trained, it can be easily inserted into different DAPT models, ready for the few-shot classification task learning via SetFit in the third part.\nCompared to the previous approach described in subsection 3.3, AdaSent is more efficient for three\nreasons. Most significantly, our SEPT adapter is trained only once and shared across various downstream classification tasks, avoiding the overhead of repeating SEPT on new DAPT-ed models. Moreover, AdaSent allows for the independent execution of DAPT and SEPT, eliminating the need for sequential training. Therefore, they can be run concurrently in parallel to save training time. Lastly, training an adapter instead of the full model in SEPT reduces the number of trainable parameters.\nGiven the extensive number of experiments in this study, we use a mixture of three datasets for SEPT, dubbed NLI+SC+SE, consisting SNLI (Bowman et al., 2015) + MultiNLI (Williams et al., 2018), Sentence Compression (Filippova and Altun, 2013) and StackExchange duplicate questions, for the sake of simplicity. This is a much smaller subset of the 1 billion sentence pairs on which the popular off-the-shelf sentence embedding models5 are pre-trained. We found that these three SEPT datasets transfer the best for the downstream clas-\n3https://huggingface.co/sentence-transformers/ all-distilroberta-v1\n4https://huggingface.co/sentence-transformers/ paraphrase-distilroberta-base-v2\n5https://huggingface.co/sentence-transformers\nsification tasks6, and are adequate to train a model that performs on par with or even better than offthe-shelf models as shown in Table 1."
        },
        {
            "heading": "5 Experimental Setup",
            "text": ""
        },
        {
            "heading": "5.1 Models",
            "text": "We experiment with three baselines and five types of domain-adapted sentence embedding models. All of these models serve as the sentence encoder in the SetFit for the few-shot classification tasks. The baselines are: (1) Base, the base PLM without any DAPT or SEPT; (2) SEPT, with only SEPT on the base PLM, which is also the default encoder in the original SetFit work; (3) DAPT, a domain-adapted PLM, i.e. the Base model continuously pre-trained on the in-domain corpus without SEPT. We also experiment with five variations of domain-adapted sentence embedding models, which differ in the way SEPT and DAPT are combined (Figure 2). In detail, they are: (1) DAPT\u2192SEPT, created through DAPT followed by SEPT on the full Transformer parameters without adapter; (2) DAPT+SEPTada is our AdaSent model; (3) DAPT\u2192SEPTada differs from AdaSent in the training of the SEPT adapter, which is trained on the DAPT model instead of the base PLM; (4) SEPT\u2192DAPT reverses the training order of (1), namely doing DAPT after SEPT; (5) SEPT\u2192DAPTada trains a DAPT adapter on a frozen SEPT model. It requires the shortest training time, since it avoids any update of the Transformer parameters."
        },
        {
            "heading": "5.2 Training Details",
            "text": "We use DistilRoBERTa as the base PLM in our main experiments. Additional results on DistilBERT are reported in the Appendix D. We set the maximum sequence length to 512. We do not tune the hyperparameters and keep them the same for all downstream tasks. If not stated otherwise, the default setting in the used libraries (cf. Appendix A) is applied. For DAPT with MLM in the main experiments, we train for a fixed number of 2344 steps7 with a batch size of 256. When using PEFT methods for DAPT, we keep the same batch size and number of steps, but with a larger learning rate of 1e\u2212 4. For SEPT, we train with a batch size of 64 for 1 epoch; the learning rates are 2e-5 and 1e-4 for\n6See Appendix B for results of individual SEPT datasets. 7This corresponds to 3 epochs on the largest training set in\nour evaluation datasets.\nfull and parameter-efficient training, respectively. For parameter-efficient training, a parallel adapter (He et al., 2022) is used by default. We also provide results of three other different PEFT methods: bottleneck adapter (Houlsby et al., 2019; Pfeiffer et al., 2020), LoRA (Hu et al., 2022) and prefix-tuning (Li and Liang, 2021).\nIn a separate experiment (subsection 6.1), we compare, on models DAPT, DAPT\u2192SEPT and SEPT\u2192DAPT, three objectives for DAPT: MLM, TSDAE (Wang et al., 2021) and SimCSE (Gao et al., 2021b). The latter two are designed for unsupervised sentence embedding learning, representing two mainstream training objectives for this task: denoising autoencoding and contrastive learning, respectively. For all three objectives, we train on the unlabeled dataset for 3 epochs. The batch sizes are 8, 8, 64 and the learning rates are 5e-5, 3e-5 and 1e-2, respectively. We only use NLI data in SEPT here for simplicity. The same setting is applied for the experiment in subsection 6.5.\nFor each downstream classification task, we do SetFit on all the models with 8-shot labeled data per class for 1 epoch. The default classification head in SetFit is Logistic Regression."
        },
        {
            "heading": "5.3 Evaluation",
            "text": "We evaluate the models on 17 classification tasks, an overview of which is provided in Table 2. These include 11 datasets from the MTEB (Massive Text Embedding Benchmark, Muennighoff et al., 2023). For datasets that contain multilingual data, we only use the English subset in this work. Since most of the MTEB tasks are from the general domain, we add another six tasks for domain-specific cases, including Adverse Drug Events Binary Classification (Gurulingappa et al., 2012) and PubMed RCT (Dernoncourt and Lee, 2017) from the biomedical domain, LEDGAR (Tuggener et al., 2020; Chalkidis et al., 2022) from the legal domain, as well as Financial PhraseBank (Malo et al., 2014), Twitter Financial News Sentiment8 and Twitter Financial News Topic9 from the financial domain.\nFor each task, we sample 8 shots per class from the training set as the labeled data for SetFit and treat the whole original training set as the unlabeled data for DAPT. We run SetFit five times with different random seeds, which correspond to five\n8https://huggingface.co/datasets/zeroshot/ twitter-financial-news-sentiment\n9https://huggingface.co/datasets/zeroshot/ twitter-financial-news-topic\ndifferent sets of few-shot samples. We report the average accuracy on the test set of each dataset over the five runs."
        },
        {
            "heading": "6 Results",
            "text": ""
        },
        {
            "heading": "6.1 Training Order and DAPT Objectives",
            "text": "In our first experiment, we compare two training orders: SEPT\u2192DAPT and DAPT\u2192SEPT, and three DAPT objectives: MLM, TSDAE and SimCSE. The results are shown in Figure 3.\nRegarding the training order, DAPT\u2192SEPT outperforms SEPT\u2192DAPT for all three DAPT objectives. DAPT can enhance the SEPT baseline only when it is performed prior to SEPT, but this setting has the efficiency issue described in subsection 3.3. On the other hand, DAPT has a negative impact on an already pre-trained sentence encoder, because\nit may distort the sentence representation space shaped by SEPT. These findings on our classification tasks are consistent with those on the retrieval tasks in Wang et al. (2021).\nWith the DAPT\u2192SEPT order, MLM achieves the best result among three DAPT objectives, improving the SEPT baseline by around 3 points on average. Although TSDAE has been shown to have a clear advantage in tasks like re-ranking and paraphrase identification (Wang et al., 2021), it turns out to be suboptimal for sentence classification. On the contrary, MLM performs worse than TSDAE and SimCSE when there is no SEPT. We suppose that sentence classification with SetFit requires a good representation of both token- and sentencelevel semantics, which are learned through MLM and SEPT respectively in the DAPTMLM\u2192SEPT setting. In other settings, either supervised sentence embedding training is absent (only DAPT), or token representation learning is missing (both TSDAE and SimCSE are for sentence representation learning)."
        },
        {
            "heading": "6.2 Combination of DAPT and SEPT",
            "text": "In this subsection, we present the results of our main experiments on various combination strategies for DAPT and SEPT. The results on the MTEB datasets are reported in Table 3, and those for the domain-specific datasets are in Table 4. AdaSent achieves the best result on 10 out of 17 tasks, outperforming the not domain-adapted SEPT model\nby 3.9 on average on the MTEB datasets, and more prominently, by 4.7 on the datasets in Table 4 with a larger domain shift from the pre-training data. The improvement is statistically significant on 8 datasets, with a significance level of 0.05. Our following analysis will focus on Table 3, while similar trends can be observed in Table 4.\nSEPT is crucial to the final accuracy of classification methods based on sentence embeddings like SetFit, though this is not explicitly mentioned in the original SetFit paper (Tunstall et al., 2022). SEPT improves both the Base model (R3 vs. R1) and the DAPT model (R4 vs. R2) by 7.3 and 6.2 points on average, respectively.\nBy adding a DAPT stage before SEPT, the classification accuracy can be significantly increased by up to 6.7 points (on AMI) and 2.7 points on average (R4 vs. R3). However, as we discussed in subsection 3.3, executing the same SEPT procedure on every DAPT model results in computational inefficiency. As a more efficient alternative, our AdaSent avoids repeating SEPT by sharing a SEPT adapter\nacross different downstream tasks, while obtaining comparable results without statistically significant difference (R7 vs. R4), except for the AMS dataset, where Adasent is even significantly better than DAPT\u2192SEPT. The comparable performance of DAPT\u2192SEPTada and AdaSent (R6 vs. R7) proves the viability of decoupling DAPT and SEPT: The SEPT adapter does not have to be trained on a specific DAPT model. Instead of doing SEPT on adapter, we also tried with DAPT on adapter (SEPT\u2192DAPTada), which should be the most efficient method as explained in subsection 5.1. Disappointingly, it can barely improve over SEPT (R8 vs. R3) and is much worse than AdaSent (R8 vs. R7). The reason could be that this setting suffers from the same problem as SEPT\u2192DAPT, as the DAPT phase, despite on an adapter, is still conducted after SEPT."
        },
        {
            "heading": "6.3 Comparison of PEFT Methods",
            "text": "We experimented with four different PEFT methods for both SEPT and DAPT (Figure 4). When applied to SEPT in AdaSent, parallel adapter works best on the majority of the datasets (Ta-\nble 13) and on average. Prefix-tuning is significantly worse than the other three methods. This might be due to the fact that the data in our SEPT dataset NLI+SC+SE come from three different tasks, whose properties cannot be compressed into a single prefix. When applied to DAPT in the SEPT\u2192DAPTPEFT setting, their performance exhibits variability across different datasets (Table 13), but none of the four PEFT methods in this setting can beat the AdaSent variants due to the critical drawback of the setting as discussed at the end of subsection 6.2."
        },
        {
            "heading": "6.4 Tunable Parameters in SetFit",
            "text": "We tune various subsets of parameters in the SetFit stage of AdaSent and compare the results in Table 5. We found that only updating the adapter parameters is not sufficient. However, tuning only the Transformer backbone leads to almost the same results as tuning all parameters (i.e. Transformer + adapter). This indicates that with only few-shot labeled data, SetFit must at least update the Transformer parameters to achieve good performance, and cannot work well on an adapter as in the case of SEPT, where much more supervised data are available."
        },
        {
            "heading": "6.5 Explaining the Success of AdaSent",
            "text": "The success of AdaSent relies on the fact that a SEPT adapter trained on a base PLM can be unproblematically inserted into any domain-adapted version of the same PLM. This might be because in both original pre-training and domain-adaptive pre-training, the PLM parameters are consistently tuned with the MLM objective. This implies that the adapter can generalize to work together with PLM parameters trained on different types of data, from general-language data (e.g. BookCorpus, Zhu\net al., 2015) to domain-specific data, as long as the same MLM objective is used. To verify this idea, we replace the MLM objective with TSDAE in both AdaSent and DAPT\u2192SEPT. As shown in Table 6, using TSDAE instead of MLM in the DAPT stage of AdaSent leads to a substantial decrease of 2.4 points in the classification accuracy, while the performance drop in DAPT\u2192SEPT is relatively marginal (0.6 on average). This supports our hypothesis that the adapter can only generalize to collaborate with PLM parameters that are domain-adapted with the same objective as in the pre-training."
        },
        {
            "heading": "6.6 Combining DAPT and Self-Training",
            "text": "Besides DAPT, another major way to utilize the unlabeled data is self-training, which has been shown to be complementary to DAPT (Li et al., 2021b). To integrate self-training into SetFit, we first encode the unlabeled data with the sentence encoder (in our case a DAPT Transformer + SEPT adapter) trained with few-shot labeled data in the contrastive fine-tuning phase. When training the classification head, we iteratively pseudo-label the encoded unlabeled sentences and train with both the pseudolabeled and the gold-labeled data10. In Table 7, we show that self-training can further improve both SEPT and AdaSent\u2019s accuracy by 1.0 and 0.9 on average, respectively. These two close improvements reveal that the benefit of self-training is orthogonal to that of AdaSent/DAPT. We leave more complex\n10The training details are available in Appendix G.\napproaches of combining AdaSent and self-training for future work."
        },
        {
            "heading": "7 Training Cost",
            "text": "Table 8 gives an overview of the training cost for DAPT\u2192SEPT and AdaSent in our experiments. We use a Tesla V100 GPU for training. We leave out IMDB and LED as they have too long sequences (cf. Table 2), thus cannot represent the majority of our tasks.\nWith AdaSent, SEPT is trained once for 0.17h and the SEPT adapter can be shared across tasks. In contrast, DAPT\u2192SEPT costs 0.27 hours additionally for every task due to its repeated SEPT. In our experiments, we use relatively small-sized data for SEPT. However, the SEPT cost can increase dramatically if much larger training data are used. For example, SEPT on the combination of all datasets in Table 10 for 1 epoch can take 4 hours, resulting in 15 \u00d7 4 hours for DAPT\u2192SEPT for 15 tasks. For DAPT, we can see that 1000 steps are already sufficient for a substantial improvement in accuracy. In this case, AdaSent takes 4.59 hours for the training on 15 tasks in total, while DAPT\u2192SEPT takes 8.47 hours (\u00d71.85)."
        },
        {
            "heading": "8 Conclusion",
            "text": "We introduce an efficient method to obtain domainadapted sentence embeddings for few-shot classification. We found that SetFit, the previous state-of-the-art approach, can be significantly improved by introducing a simple Domain-Adaptive Pre-Training (DAPT) stage before its SentenceEmbedding Pre-Training (SEPT). However, this DAPT\u2192SEPT approach requires the same SEPT procedure to be done on each DAPT-ed PLM for every domain, resulting in computational inefficiency. We propose a novel approach, AdaSent, to address this issue by storing the SEPT knowledge in an adapter that is trained on an unadapted PLM and insertable into any DAPT-ed PLM. AdaSent matches or surpasses the performance of DAPT\u2192SEPT, while significantly reducing the training cost of SEPT. We attribute the success of AdaSent to the generalization ability of the SEPT adapter to work with PLM parameters trained on data from different domains with a consistent MLM objective.\nLimitations\nSince our method is based on SetFit, it inherits some of its limitations. It is, for example, not appli-\ncable for sentence pair classification like NLI. In addition, the advantage of SetFit is not significant in classification tasks with too many classes. Moreover, as our method is based on sentence embeddings, its application is limited to sentence classification, unlike other few-shot classification methods that can also handle token-level classification tasks like NER and POS tagging.\nAnother limitation is associated with the fact that the SEPT adapter in our method can only be inserted into domain-adapted language models with the same unmodified tokenizer and vocabulary as the original base PLM. For DAPT-ed models with a domain-specific tokenizer or vocabulary, we suppose the adapter trained on the original PLM will not be compatible anymore.\nEthics Statement\nOur experiments use publicly available datasets and benchmarks for training and evaluation, which are commonly used in the field of NLP. No personal information or sensitive data are involved in our work. Existing biases in the datasets or pre-trained models can still be relevant concerns, since we do not specifically focus on mitigating them in the current work."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work has been funded by HUAWEI Technologies (Ireland) Co., Ltd. and by the German Federal Ministry of Education and Research (BMBF) under the promotional reference 13N15897 (MISRIK)."
        },
        {
            "heading": "B Experiment with SEPT Datasets",
            "text": "We experiment with different SEPT datasets11 to check their transferability to downstream tasks Table 10. On average, AllNLI, SentenceCompression and StackexchangeDuplicateQuestions are the top three datasets. The similarity between the SEPT data and the downstream data seems to have an influence on the performance. For example, QArelated data (YahooAnswersTitleAnswer, StackexchangeDuplicateQuestions and YahooAnswersQuestionAnswer) are especially beneficial for the classification tasks involving user utterances in dialogues (BANK, AMI, AMS, MI, MD). Given this observation, one might want to search for the optimal SEPT datasets depending on certain types of classification tasks. Our adapter-based method enables efficient SEPT, which helps to ease the data selection."
        },
        {
            "heading": "C DAPT Objectives and Training Order",
            "text": "Results on individual datasets are listed in Table 11."
        },
        {
            "heading": "D Results on DistilBERT",
            "text": "We report the results on DistilBERT in Table 12. Similar to DistilRoBERTa, DAPT with MLM\n11See https://www.sbert.net/examples/training/ paraphrases/README.html for information of the datasets.\n(DAPT\u2192SEPT and DAPT+SEPTada) improves the performance of SEPT by around 3 points on average. Replacing full SEPT with SEPT adapter causes a slight drop of around 0.5 in the classification accuracy. Interestingly, without any supervised sentence embedding pre-training, DAPT itself can outperform SEPT on some datasets (AC, ADE, LED)."
        },
        {
            "heading": "E PEFT results",
            "text": "Results on individual datasets when using different PEFT methods as discussed in subsection 6.3 in our AdaSent method (DAPT+SEPTPEFT) and SEPT\u2192DAPTPEFT are shown in Table 13."
        },
        {
            "heading": "F Evaluation Datasets",
            "text": "Table 14 provides examples from each evaluation dataset."
        },
        {
            "heading": "G Self-Training Setting",
            "text": "In the SetFit phase, we contrastively fine-tune the sentence embedding model with the few-shot data as before (subsection 3.1), but replace the normal Logistic Regression fitting with self-training on both labeled and unlabeled data. For this, we use the SelfTrainingClassifier from scikit-learn12 with 10 iterations and a threshold of 0.9. At each iteration, the classifier predicts the label of the unlabeled data. The pseudo-labeled data with a confidence score higher than the threshold are used to augment the training data in the next iteration.\n12https://scikit-learn.org/stable/ modules/generated/sklearn.semi_supervised. SelfTrainingClassifier.html"
        }
    ],
    "title": "AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification",
    "year": 2023
}