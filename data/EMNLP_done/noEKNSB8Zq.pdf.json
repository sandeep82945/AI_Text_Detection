{
    "abstractText": "Large Language Models (LLMs) have recently emerged as an effective tool to assist individuals in writing various types of content, including professional documents such as recommendation letters. Though bringing convenience, this application also introduces unprecedented fairness concerns. Model-generated reference letters might be directly used by users in professional scenarios. If underlying biases exist in these model-constructed letters, using them without scrutinization could lead to direct societal harms, such as sabotaging application success rates for female applicants. In light of this pressing issue, it is imminent and necessary to comprehensively study fairness issues and associated harms in this real-world use case. In this paper, we critically examine gender biases in LLM-generated reference letters. Drawing inspiration from social science findings, we design evaluation methods to manifest biases through 2 dimensions: (1) biases in language style and (2) biases in lexical content. We further investigate the extent of bias propagation by analyzing the hallucination bias of models, a term that we define to be bias exacerbation in model-hallucinated contents. Through benchmarking evaluation on 2 popular LLMsChatGPT and Alpaca, we reveal significant gender biases in LLM-generated recommendation letters. Our findings not only warn against using LLMs for this application without scrutinization, but also illuminate the importance of thoroughly studying hidden biases and harms in LLM-generated professional documents.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yixin Wan"
        },
        {
            "affiliations": [],
            "name": "George Pu"
        },
        {
            "affiliations": [],
            "name": "Jiao Sun"
        },
        {
            "affiliations": [],
            "name": "Aparna Garimella"
        },
        {
            "affiliations": [],
            "name": "Kai-Wei Chang"
        },
        {
            "affiliations": [],
            "name": "Nanyun Peng"
        }
    ],
    "id": "SP:6a3d5d52be2472fb9e49a0e6ab0437a210d870a6",
    "references": [
        {
            "authors": [
                "Razvan Azamfirei",
                "Sapna R Kudchadkar",
                "James Fackler."
            ],
            "title": "Large language models and the perils of their hallucinations",
            "venue": "Critical Care, 27(1):1\u20132.",
            "year": 2023
        },
        {
            "authors": [
                "Solon Barocas",
                "Kate Crawford",
                "Aaron Shapiro",
                "Hanna Wallach."
            ],
            "title": "The problem with bias: From allocative to representational harms in machine learning",
            "venue": "Proceedings of the 9th Annual Conference of the Special Interest Group for Computing, Informa-",
            "year": 2017
        },
        {
            "authors": [
                "Emily M. Bender",
                "Timnit Gebru",
                "Angelina"
            ],
            "title": "McMillanMajor, and Shmargaret Shmitchell",
            "venue": "In Proceedings of the 2021 ACM Conference on Fairness, Accountability,",
            "year": 2021
        },
        {
            "authors": [
                "Su Lin Blodgett",
                "Solon Barocas",
                "Hal Daum\u00e9 III",
                "Hanna Wallach."
            ],
            "title": "Language (technology) is power: A critical survey of \u201cbias\u201d in NLP",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Tolga Bolukbasi",
                "Kai-Wei Chang",
                "James Zou",
                "Venkatesh Saligrama",
                "Adam Kalai."
            ],
            "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
            "venue": "Conference on Neural Information Processing Systems.",
            "year": 2016
        },
        {
            "authors": [
                "Shikha Bordia",
                "Samuel R. Bowman."
            ],
            "title": "Identifying and reducing gender bias in word-level language models",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop,",
            "year": 2019
        },
        {
            "authors": [
                "Samuel R. Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D. Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2015
        },
        {
            "authors": [
                "Yang Trista Cao",
                "Yada Pruksachatkun",
                "Kai-Wei Chang",
                "Rahul Gupta",
                "Varun Kumar",
                "Jwala Dhamala",
                "Aram Galstyan."
            ],
            "title": "On the intrinsic and extrinsic fairness evaluation metrics for contextualized language representations",
            "venue": "Proceedings of the 60th",
            "year": 2022
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Kate Crawford."
            ],
            "title": "The trouble with bias",
            "venue": "Conference on Neural Information Processing Systems, invited speaker.",
            "year": 2017
        },
        {
            "authors": [
                "Melissa Cugno."
            ],
            "title": "Talk Like a Man: How Resume Writing Can Impact Managerial Hiring Decisions for Women",
            "venue": "Ph.D. thesis. Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works; Last",
            "year": 2020
        },
        {
            "authors": [
                "Maria De-Arteaga",
                "Alexey Romanov",
                "Hanna Wallach",
                "Jennifer Chayes",
                "Christian Borgs",
                "Alexandra Chouldechova",
                "Sahin Geyik",
                "Krishnaram Kenthapadi",
                "Adam Tauman Kalai"
            ],
            "title": "Bias in bios: A case study of semantic representation bias in a high-stakes",
            "year": 2019
        },
        {
            "authors": [
                "Sunipa Dev",
                "Emily Sheng",
                "Jieyu Zhao",
                "Aubrie Amstutz",
                "Jiao Sun",
                "Yu Hou",
                "Mattie Sanseverino",
                "Jiin Kim",
                "Akihiro Nishi",
                "Nanyun Peng",
                "Kai-Wei Chang."
            ],
            "title": "On measures of biases and harms in NLP",
            "venue": "Findings of the Association for Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Jwala Dhamala",
                "Tony Sun",
                "Varun Kumar",
                "Satyapriya Krishna",
                "Yada Pruksachatkun",
                "Kai-Wei Chang",
                "Rahul Gupta."
            ],
            "title": "Bold: Dataset and metrics for measuring biases in open-ended language generation",
            "venue": "FAccT.",
            "year": 2021
        },
        {
            "authors": [
                "Emily Dinan",
                "Angela Fan",
                "Adina Williams",
                "Jack Urbanek",
                "Douwe Kiela",
                "Jason Weston."
            ],
            "title": "Queens are powerful too: Mitigating gender bias in dialogue generation",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language",
            "year": 2020
        },
        {
            "authors": [
                "Lucas Dixon",
                "John Li",
                "Jeffrey Sorensen",
                "Nithum Thain",
                "Lucy Vasserman."
            ],
            "title": "Measuring and mitigating unintended bias in text classification",
            "venue": "New York, NY, USA. Association for Computing Machinery.",
            "year": 2018
        },
        {
            "authors": [
                "Kuheli Dutt",
                "Danielle L. Pfaff",
                "Ariel Finch Bernstein",
                "Joseph Solomon Dillard",
                "Caryn J. Block."
            ],
            "title": "Gender differences in recommendation letters for postdoctoral fellowships in geoscience",
            "venue": "Nature Geoscience, 9:805\u2013808.",
            "year": 2016
        },
        {
            "authors": [
                "Umang Gupta",
                "Jwala Dhamala",
                "Varun Kumar",
                "Apurv Verma",
                "Yada Pruksachatkun",
                "Satyapriya Krishna",
                "Rahul Gupta",
                "Kai-Wei Chang",
                "Greg Ver Steeg",
                "Aram Galstyan"
            ],
            "title": "Mitigating gender bias in distilled language models via counterfactual role rever",
            "year": 2022
        },
        {
            "authors": [
                "Alejandro Hallo-Carrasco",
                "Benjamin F Gruenbaum",
                "Shaun E Gruenbaum."
            ],
            "title": "Heat and moisture exchanger occlusion leading to sudden increased airway pressure: A case report using chatgpt as a personal writing assistant",
            "venue": "Cureus, 15(4).",
            "year": 2023
        },
        {
            "authors": [
                "Matthew Honnibal",
                "Ines Montani."
            ],
            "title": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing",
            "venue": "To appear.",
            "year": 2017
        },
        {
            "authors": [
                "Ziwei Ji",
                "Nayeon Lee",
                "Rita Frieske",
                "Tiezheng Yu",
                "Dan Su",
                "Yan Xu",
                "Etsuko Ishii",
                "Ye Jin Bang",
                "Andrea Madotto",
                "Pascale Fung."
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Computing Surveys, 55(12):1\u201338.",
            "year": 2023
        },
        {
            "authors": [
                "Shawn Khan",
                "Abirami Kirubarajan",
                "Tahmina Shamsheri",
                "Adam Clayton",
                "Geeta Mehta."
            ],
            "title": "Gender bias in reference letters for residency and academic medicine: a systematic review",
            "venue": "Postgraduate Medical Journal.",
            "year": 2021
        },
        {
            "authors": [
                "Wojciech Kryscinski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher."
            ],
            "title": "Evaluating the factual consistency of abstractive text summarization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Philippe Laban",
                "Tobias Schnabel",
                "Paul N Bennett",
                "Marti A Hearst."
            ],
            "title": "Summac: Re-visiting nlibased models for inconsistency detection in summarization",
            "venue": "Transactions of the Association for Computational Linguistics, 10:163\u2013177.",
            "year": 2022
        },
        {
            "authors": [
                "Alisa Liu",
                "Maarten Sap",
                "Ximing Lu",
                "Swabha Swayamdipta",
                "Chandra Bhagavatula",
                "Noah A. Smith",
                "Yejin Choi."
            ],
            "title": "DExperts: Decoding-time controlled text generation with experts and anti-experts",
            "venue": "Proceedings of the 59th Annual Meeting of the",
            "year": 2021
        },
        {
            "authors": [
                "Ou Lydia Liu",
                "Jennifer Minsky",
                "Guangming Ling",
                "Patrick Kyllonen"
            ],
            "title": "Using the standardized letters of recommendation in selectionresults",
            "year": 2009
        },
        {
            "authors": [
                "Juan Madera",
                "Mikki Hebl",
                "Heather Dial",
                "Randi Martin",
                "Virginia Valian."
            ],
            "title": "Raising doubt in letters of recommendation for academia: Gender differences and their impact",
            "venue": "Journal of Business and Psychology, 34.",
            "year": 2019
        },
        {
            "authors": [
                "Juan Madera",
                "Mikki Hebl",
                "Randi Martin."
            ],
            "title": "Gender and letters of recommendation for academia: Agentic and communal differences",
            "venue": "The Journal of applied psychology, 94:1591\u20139.",
            "year": 2009
        },
        {
            "authors": [
                "Joshua Maynez",
                "Shashi Narayan",
                "Bernd Bohnet",
                "Ryan McDonald."
            ],
            "title": "On faithfulness and factuality in abstractive summarization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906\u20131919, On-",
            "year": 2020
        },
        {
            "authors": [
                "Niels M\u00fcndler",
                "Jingxuan He",
                "Slobodan Jenko",
                "Martin Vechev."
            ],
            "title": "Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation",
            "venue": "arXiv preprint arXiv:2305.15852.",
            "year": 2023
        },
        {
            "authors": [
                "Yixin Nie",
                "Adina Williams",
                "Emily Dinan",
                "Mohit Bansal",
                "Jason Weston",
                "Douwe Kiela."
            ],
            "title": "Adversarial nli: A new benchmark for natural language understanding",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. As-",
            "year": 2020
        },
        {
            "authors": [
                "Anaelia Ovalle",
                "Palash Goyal",
                "Jwala Dhamala",
                "Zachary Jaggers",
                "Kai-Wei Chang",
                "Aram Galstyan",
                "Richard Zemel",
                "Rahul Gupta"
            ],
            "title": "i\u2019m fully who i am\u201d: Towards centering transgender and non-binary voices to measure biases in open language generation",
            "year": 2023
        },
        {
            "authors": [
                "Anaelia Ovalle",
                "Arjun Subramonian",
                "Vagrant Gautam",
                "Gilbert Gee",
                "Kai-Wei Chang"
            ],
            "title": "Factoring the matrix of domination: A critical review and reimagination of intersectionality in ai fairness",
            "year": 2023
        },
        {
            "authors": [
                "Marcelo O.R. Prates",
                "Pedro H.C. Avelar",
                "L. Lamb."
            ],
            "title": "Assessing gender bias in machine translation: a case study with google translate",
            "venue": "Neural Computing and Applications, 32:6363\u20136381.",
            "year": 2018
        },
        {
            "authors": [
                "Sudha Rao",
                "Joel Tetreault."
            ],
            "title": "Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer",
            "venue": "Proceedings of the 2018 Conference of the North",
            "year": 2018
        },
        {
            "authors": [
                "Malik Sallam."
            ],
            "title": "Chatgpt utility in healthcare education, research, and practice: Systematic review on the promising perspectives and valid concerns",
            "venue": "Healthcare, 11(6).",
            "year": 2023
        },
        {
            "authors": [
                "Emily Sheng",
                "Kai-Wei Chang",
                "Prem Natarajan",
                "Nanyun Peng."
            ],
            "title": "Towards Controllable Biases in Language Generation",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3239\u20133254, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Emily Sheng",
                "Kai-Wei Chang",
                "Prem Natarajan",
                "Nanyun Peng."
            ],
            "title": "nice try, kiddo\u201d: Investigating ad hominems in dialogue responses",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "Emily Sheng",
                "Kai-Wei Chang",
                "Prem Natarajan",
                "Nanyun Peng."
            ],
            "title": "Societal biases in language generation: Progress and challenges",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Emily Sheng",
                "Kai-Wei Chang",
                "Premkumar Natarajan",
                "Nanyun Peng."
            ],
            "title": "The woman worked as a babysitter: On biases in language generation",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empiri-",
            "year": 2013
        },
        {
            "authors": [
                "Chris Stokel-Walker."
            ],
            "title": "Chatgpt listed as author on research papers: Many scientists disapprove",
            "venue": "Nature, 613(7945):620\u2013621.",
            "year": 2023
        },
        {
            "authors": [
                "Fan-Keng Sun",
                "Cheng-I Lai."
            ],
            "title": "Conditioned natural language generation using only unconditioned language model: An exploration",
            "venue": "ArXiv, abs/2011.07347.",
            "year": 2020
        },
        {
            "authors": [
                "Jiao Sun",
                "Nanyun Peng"
            ],
            "title": "Men are elected, women are married: Events gender bias on Wikipedia",
            "year": 2021
        },
        {
            "authors": [
                "Magdalena Szumilas."
            ],
            "title": "Explaining odds ratios",
            "venue": "Journal of the Canadian Academy of Child and Adolescent Psychiatry = Journal de l\u2019Academie canadienne de psychiatrie de l\u2019enfant et de l\u2019adolescent, 19 3:227\u20139.",
            "year": 2010
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "James Thorne",
                "Andreas Vlachos",
                "Christos Christodoulopoulos",
                "Arpit Mittal."
            ],
            "title": "FEVER: a large-scale dataset for fact extraction and VERification",
            "venue": "NAACL-HLT.",
            "year": 2018
        },
        {
            "authors": [
                "Frances Trix",
                "Carolyn E. Psenka."
            ],
            "title": "Exploring the color of glass: Letters of recommendation for female and male medical faculty",
            "venue": "Discourse & Society, 14:191 \u2013 220.",
            "year": 2003
        },
        {
            "authors": [
                "Angelina Wang",
                "Vikram V. Ramaswamy",
                "Olga Russakovsky."
            ],
            "title": "Towards intersectionality in machine learning: Including more identities, handling underrepresentation, and performing evaluation",
            "venue": "Proceedings of the 2022 ACM Conference on Fairness,",
            "year": 2022
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Jieyu Zhao",
                "Tianlu Wang",
                "Mark Yatskar",
                "Vicente Ordonez",
                "Kai-Wei Chang."
            ],
            "title": "Men also like shopping: Reducing gender bias amplification using corpus-level constraints",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Lan-",
            "year": 2017
        },
        {
            "authors": [
                "Jieyu Zhao",
                "Tianlu Wang",
                "Mark Yatskar",
                "Vicente Ordonez",
                "Kai-Wei Chang."
            ],
            "title": "Gender bias in coreference resolution: Evaluation and debiasing methods",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "LLMs have emerged as helpful tools to facilitate the generation of coherent long texts, enabling various use cases of document generation (Sallam, 2023; Osmanovic-Thunstr\u00f6m et al., 2023; StokelWalker, 2023; Hallo-Carrasco et al., 2023). Recently, there has been a growing trend to use LLMs\nin the creation of professional documents, including recommendation letters. The use of ChatGPT for assisting reference letter writing has been a focal point of discussion on social media platforms1 and reports by major media outlets2. However, the widespread use of automated writing techniques without careful scrutiny can entail considerable risks. Recent studies have shown that Natural Language Generation (NLG) models are gender biased (Sheng et al., 2019, 2020; Dinan et al., 2020; Sheng et al., 2021a; Bender et al., 2021) and therefore pose a risk to harm minorities when used in sensitive applications (Sheng et al., 2021b; Ovalle et al., 2023a; Prates et al., 2018). Such biases might also infiltrate the application of automated reference letter generation and cause substantial societal harm, as research in social sciences (Madera et al., 2009; Khan et al., 2021) unveiled how biases in professional documents lead to diminished career opportunities for gender minority groups. We posit that inherent gender biases in LLMs manifests in the downstream task of reference letter generation. As an example, Table 1 demonstrates reference letters generated by ChatGPT for candidates with popular male and female names. The model manifests the stereotype of men being agentic (e.g., natural leader) and women being communal (e.g., well-liked member).\nIn this paper, we systematically investigate gender biases present in reference letters generated by LLMs under two scenarios: (1) Context-Less Generation (CLG), where the model is prompted to produce a letter based solely on simple descriptions of the candidate, and (2) Context-Based Generation (CBG), in which the model is also given the candidate\u2019s personal information and experience in the prompt. CLG reveals inherent biases towards sim-\n1See, for example, the discussion on Reddit https:// shorturl.at/eqsV6\n2For example, see the article published in the Atlantic https://shorturl.at/fINW3.\nple gender-associated descriptors, whereas CBG simulates how users typically utilize LLMs to facilitate letter writing. Inspired by social science literature, we investigate 3 aspects of biases in LLM-generated reference letters: (1) bias in lexical content, (2) bias in language style, and (3) hallucination bias. We construct the first comprehensive testbed with metrics and prompt datasets for identifying and quantifying biases in the generated letters. Furthermore, we use the proposed framework to evaluate and unveil significant gender biases in recommendation letters generated by two recently developed LLMs: ChatGPT (OpenAI, 2022) and Alpaca (Taori et al., 2023).\nOur findings emphasize a haunting reality: the current state of LLMs is far from being mature when it comes to generating professional documents. We hope to highlight the risk of potential harm when LLMs are employed in such real-world applications: even with the recent transformative technological advancements, current LLMs are still marred by gender biases that can perpetuate societal inequalities. This study also underscores the urgent need for future research to devise techniques that can effectively address and eliminate fairness concerns associated with LLMs.3"
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Social Biases in NLP",
            "text": "Social biases in NLP models have been an important field of research. Prior works have defined two major types of harms and biases in NLP models: allocational harms and representational harms\n3Code and data are available at: https://github.com/ uclanlp/biases-llm-reference-letters\n(Blodgett et al., 2020; Barocas et al., 2017; Crawford, 2017). Researchers have studied methods to evaluate and mitigate the two types of biases in Natural Language Understanding (NLU) (Bolukbasi et al., 2016; Dev et al., 2022; Dixon et al., 2018; Bordia and Bowman, 2019; Zhao et al., 2017, 2018; Sun and Peng, 2021) and Natural Language Generation (NLG) tasks (Sheng et al., 2019, 2021b; Dinan et al., 2020; Sheng et al., 2021a).\nAmong previous works, Sun and Peng (2021) proposed to use the Odds Ratio (OR) (Szumilas, 2010) as a metric to measure gender biases in items with large frequency differences or highest saliency for females and males. Sheng et al. (2019) measured biases in NLG model generations conditioned on certain contexts of interest. Dhamala et al. (2021) extended the pipeline to use real prompts extracted from Wikipedia. Several approaches (Sheng et al., 2020; Gupta et al., 2022; Liu et al., 2021; Cao et al., 2022) studied how to control NLG models for reducing biases. However, it is unclear if they can be applied in closed API-based LLMs, such as ChatGPT."
        },
        {
            "heading": "2.2 Biases in Professional Documents",
            "text": "Recent studies in NLP fairness (Wang et al., 2022; Ovalle et al., 2023b) point out that some AI fairness works fail to discuss the source of biases investigated, and suggest to consider both social and technical aspects of AI systems. Inspired by this, we ground bias definitions and metrics in our work on related social science research. Previous works in social science (Cugno, 2020; Madera et al., 2009; Khan et al., 2021; Liu et al., 2009; Madera et al., 2019) have revealed the existence and dan-\ngers of gender biases in the language styles of professional documents. Such biases might lead to harmful gender differences in application success rate (Madera et al., 2009; Khan et al., 2021). For instance, Madera et al. (2009) observed that biases in gendered language in letters of recommendation result in a higher residency match rate for male applicants. These findings further emphasize the need to study gender biases in LLM-generated professional documents. We categorize major findings in previous literature into 3 types of gender biases in language styles of professional documents: biases in language professionalism, biases in language excellency, and biases in language agency.\nBias in language professionalism states that male candidates are considered more \u201cprofessional\u201d than females. For instance, Trix and Psenka (2003) revealed the gender schema where women are seen as less capable and less professional than men. Khan et al. (2021) also observed more mentions of personal life in letters for female candidates. Gender biases in this dimension will lead to biased information on candidates\u2019 professionalism, therefore resulting in unfair hiring evaluation.\nBias in language excellency states that male candidates are described using more \u201cexcellent\u201d language than female candidates in professional documents (Trix and Psenka, 2003; Madera et al., 2009, 2019). For instance, Dutt et al. (2016) points out that female applicants are only half as likely than male applicants to receive \u201cexcellent\u201d letters. Naturally, gender biases in the level of excellency of language styles will lead to a biased perception of a candidate\u2019s abilities and achievements, creating inequality in hiring evaluation.\nBias in language agency states that women are more likely to be described using communal adjectives in professional documents, such as delightful and compassionate, while men are more likely to be described using \u201cagentic\u201d adjectives, such as leader or exceptional (Madera et al., 2009, 2019; Khan et al., 2021). Agentic characteristics include speaking assertively, influencing others, and initiating tasks. Communal characteristics include concerning with the welfare of others, helping others, accepting others\u2019 direction, and maintaining relationships (Madera et al., 2009). Since agentic language is generally perceived as being more hirable than communal language style (Madera et al., 2009, 2019; Khan et al., 2021), bias in language agency might further lead to biases in hiring decisions."
        },
        {
            "heading": "2.3 Hallucination Detection",
            "text": "Understanding and detecting hallucinations in LLMs have become an important problem (M\u00fcndler et al., 2023; Ji et al., 2023; Azamfirei et al., 2023). Previous works on hallucination detection proposed three main types of approaches: Information Extraction-based, Question Answering (QA)-based and Natural Language Inference (NLI)based approaches. Our study utilizes the NLIbased approach (Kryscinski et al., 2020; Maynez et al., 2020; Laban et al., 2022), which uses the original input as context to determine the entailment with the model-generated text. To do this, prior works have proposed document-level NLI and sentence-level NLI approaches. Document-level NLI (Maynez et al., 2020; Laban et al., 2022) investigates entailment between full input and generation text. Sentence-level NLI (Laban et al., 2022) chunks original and generated texts into sentences and determines entailment between each pair. However, little is known about whether models will propagate or amplify biases in their hallucinated outputs."
        },
        {
            "heading": "3 Methods",
            "text": ""
        },
        {
            "heading": "3.1 Task Formulation",
            "text": "We consider two different settings for reference letter generation tasks. (1) Context-Less Generation (CLG): prompting the model to generate a letter based on minimal information, and (2) ContextBased Generation (CBG): guiding the model to generate a letter by providing contextual information, such as a personal biography. The CLG setting better isolates biases influenced by input information and acts as a lens to examine underlying biases in models. The CBG setting aligns more closely with the application scenarios: it simulates a user scenario where the user would write a short description of themselves and ask the model to generate a recommendation letter accordingly."
        },
        {
            "heading": "3.2 Bias Definitions",
            "text": "We categorize gender biases in LLM-generated professional documents into two types: Biases in Lexical Content, and Biases in Language Style."
        },
        {
            "heading": "3.2.1 Biases in Lexical Content",
            "text": "Biases in lexical content can be manifested by harmful differences in the most salient components of LLM-generated professional documents. In this work, we measure biases in lexical context through\nevaluating biases in word choices. We define biases in word choices to be the salient frequency differences between wordings in male and female documents. We further dissect our analysis into biases in nouns and biases in adjectives. Odds Ratio Inspired by previous work (Sun and Peng, 2021), we propose to use Odds Ratio (OR) (Szumilas, 2010) for qualitative analysis on biases in word choices. Taking analysis on adjectives as an example. Let am = {am1 , am2 , ...amM} and af = {af1 , a f 2 , ...a f F } be the set of all adjectives in male documents and female documents, respectively. For an adjective an, we first count its occurrences in male documents Em(an) and in female documents Ef (an). Then, we can calculate OR for adjective an to be its odds of existing in the male adjectives list divided by the odds of existing in the female adjectives list:\nEm(an)\u2211i ami \u0338=an\ni\u2208{1,...,M} Em(ami )\n/ Ef (an)\u2211i\nafi \u0338=an i\u2208{1,...,F}\nEm(afi ) .\nLarger OR means that an adjective is more likely to exist, or more salient, in male letters than female letters. We then sort adjectives by their OR in descending order, and extract the top and last adjectives, which are the most salient adjectives for males and for females respectively."
        },
        {
            "heading": "3.2.2 Biases in Language Style",
            "text": "We define biases in language style as significant stylistic differences between LLM-generated documents for different gender groups. For instance, we can say that bias in language style exists if the language in model-generated documents for males is significantly more positive or more formal than that for females. Given two sets of model-generated documents for males Dm = {dm,1, dm,2, ...} and females Df = {df,1, df,2, ...}, we can measure the extent that a given text conforms to a certain language style l by a scoring function Sl(\u00b7). Then, we can measure biases in language style through t-testing on language style differences between Dm and Df . Biases in language style blang can therefore be mathematically formulate as:\nblang = \u00b5(Sl(dm))\u2212 \u00b5(Sl(df ))\u221a std(Sl(dm))2\n|Dm| + std(Sl(df ))2 |Df |\n, (1)\nwhere \u00b5(\u00b7) and std(\u00b7) represents sample mean and standard deviation. Due to the nature of blang as a\nt-test value, a small value of blang that is lower than the significance threshold indicates the existence of bias. Following the bias aspects in social science that are discussed in Section 2.2, we establish 3 aspects to measure biases in language style: (1) Language Formality, (2) Language Positivity, and (3) Language Agency.\nBiases in Language Formality Our method uses language formality as a proxy to reflect the level of language professionalism. We define biases in Language Formality to be statistically significant differences in the percentage of formal sentences in male and female-generated documents. Specifically, we conduct statistical t-tests on the percentage of formal sentences in documents generated for each gender and report the significance of the difference in formality levels.\nBiases in Language Positivity Our method uses positive sentiment in language as a proxy to reflect the level of excellency in language. We define biases in Language Positivity to be statistically significant differences in the percentage of sentences with positive sentiments in generated documents for males and females. Similar to analysis for biases in language formality, we use statistical ttesting to construct the quantitative metric.\nBiases in Language Agency We propose and study Language Agency as a novel metric for bias evaluation in LLM-generated professional documents. Although widely observed and analyzed in social science literature (Cugno, 2020; Madera et al., 2009; Khan et al., 2021), biases in language agency have not been defined, discussed or analyzed in the NLP community. We define biases in language agency to be statistically significant differences in the percentage of agentic sentences in generated documents for males and females, and again report the significance of biases using t-testing."
        },
        {
            "heading": "3.3 Hallucination Bias",
            "text": "In addition to directly analyzing gender biases in model-generated reference letters, we propose to separately study biases in model-hallucinated information for CBG task. Specifically, we want to find out if LLMs tend to hallucinate biased information in their generations, other than factual information provided from the original context. We define Hallucination Bias to be the harmful propagation or amplification of bias levels in model hallucinations.\nHallucination Detection Inspired by previous works (Maynez et al., 2020; Laban et al., 2022), we propose and utilize Context-Sentence NLI as a framework for Hallucination Detection. The intuition behind this method is that the source knowledge reference should entail the entirety of any generated information in faithful and hallucinationfree generations. Specifically, given a context C and a corresponding model generated document D, we first split D into sentences {S1, S2, . . . , Sn} as hypotheses. We use the entirety of C as the premise and establish premise-hypothesis pairs: {(C, S1), (C, S2), . . . , (C, Sn)} Then, we use an NLI model to determine the entailment between each premise-hypothesis pair. Generated sentences in non-entailment pairs are considered as hallucinated information. The detected hallucinated information is then used for hallucination bias evaluation. A visualization of the hallucination detection pipeline is demonstrated in Figure 1.\nHallucination Bias Evaluation In order to measure gender bias propagation and amplification in model hallucinations, we utilize the same 3 quantitative metrics as evaluation of Biases in Language Style: Language Formality, Language Positivity, and Language Agency. Since our goal is to investigate if information in model hallucinations demonstrates the same level or a higher level of gender biases, we conduct statistical t-testing to reveal significant harmful differences in language styles between only the hallucinated content and the full generated document. Taking language formality as an example, we conduct a t-test on the percentage of formal sentences in the detected hallucinated contents and the full generated document, respectively. For male documents, bias propagation exists if the hallucinated information does not demonstrate significant differences in levels of formality, positivity, or agency. Bias amplification exists if the hallucinated information demonstrates significantly higher levels of formality, positivity,\nor agency than the full document. Similarly, for female documents, bias propagation exists if hallucination is not significantly different in levels of formality, positivity, or agency. Bias amplification exists if hallucinated information is significantly lower in its levels of formality, positivity, or agency than the full document."
        },
        {
            "heading": "4 Experiments",
            "text": "We conduct bias evaluation experiments on two tasks: Context-Less Generation and Context-Based Generation. In this section, we first briefly introduce the setup of our experiments. Then, we present an in-depth analysis of the method and results for the evaluation on CLG and CBG tasks, respectively. Since CBG\u2019s formulation is closer to real-world use cases of reference letter generation, we place our research focus on CBG task, while conducting a preliminary exploration on CLG biases."
        },
        {
            "heading": "4.1 Experiment Setup",
            "text": "Model Choices Since experiments on CLG act as a preliminary exploration, we only use ChatGPT as the model for evaluation. To choose the best models for experiments CBG task, we investigate the generation qualities of four LLMs: ChatGPT (OpenAI, 2022), Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), and StableLM (AI, 2023). While ChatGPT can always produce reasonable reference letter generations, other LLMs sometimes fail to do so, outputting unrelated content. In order to only evaluate valid reference letter generations, we define and calculate the generation success rate of LLMs using criteria-based filtering. Details on generation success rate calculation and behavior analysis can be found in Appendix B. After evaluating LLMs\u2019 generation success rates on the task, we choose to conduct further experiments using only ChatGPT and Alpaca for letter generations."
        },
        {
            "heading": "4.2 Context-Less Generation",
            "text": "Analysis on CLG evaluates biases in model generations when given minimal context information, and acts as a lens to interpret underlying biases in models\u2019 learned distribution."
        },
        {
            "heading": "4.2.1 Generation",
            "text": "Prompting (Brown et al., 2020; Sun and Lai, 2020) steers pre-trained language models with taskspecific instructions to generate task outputs without task fine-tuning. In our experiments, we de-\nsign simple descriptor-based prompts for CLG analysis. We have attached the full list of descriptors in Appendix C.1, which shows the three axes (name/gender, age, and occupation) and corresponding specific descriptors (e.g. Joseph, 20, student) that we iterate through to query model generations. We then formulate the prompt by filling descriptors of each axis in a prompt template, which we have attached in Appendix C.2. Using these descriptors, we generated a total of 120 CLGbased reference letters. Hyperparameter settings for generation can be found in Appendix A."
        },
        {
            "heading": "4.2.2 Evaluation: Biases in Lexical Content",
            "text": "Since only 120 letters were generated for preliminary CLG analysis, running statistics analysis on biases in lexical content or word choices might lack significance as we calculate OR for one word at a time. To mitigate this issue, we calculate OR for words belonging to gender-stereotypical traits, instead of for single words. Specifically, we implement the traits as 9 lexicon categories: Ability, Standout, Leadership, Masculine, Feminine, Agentic, Communal, Professional, and Personal. Full lists of the lexicon categories can be found in Appendix F.5. An OR score that is greater than 1 indicates higher odds for the trait to appear in generated letters for males, whereas an OR score that is below 1 indicates the opposite."
        },
        {
            "heading": "4.2.3 Result",
            "text": "Table 2 shows experiment results for biases in lexical content analysis on CLG task, which reveals significant and harmful associations between gender and gender-stereotypical traits. Most malestereotypical traits -- Ability, Standout, Leadership, Masculine, and Agentic -- have higher odds of appearing in generated letters for males. Femalestereotypical traits -- Feminine, Communal, and\nPersonal -- also demonstrate the same trend to have higher odds of appearing in female letters. Evaluation results on CLG unveil significant underlying gender biases in ChatGPT, driving the model to generate reference letters with harmful genderstereotypical traits."
        },
        {
            "heading": "4.3 Context-Based Generation",
            "text": "Analysis on CBG evaluates biases in model generations when provided with certain context information. For instance, a user can input personal information such as a biography and prompt the model to generate a full letter."
        },
        {
            "heading": "4.3.1 Data Preprocessing",
            "text": "We utilize personal biographies as context information for CBG task. Specifically, we further preprocess and use WikiBias (Sun and Peng, 2021), a personal biography dataset with scraped demographic and biographic information from Wikipedia. Our data augmentation pipeline aims at producing an anonymized and gender-balanced biography datasest as context information for reference letter generation to prevent pre-existing biases. Details on preprocessing implementations can be found in Appendix F.1. We denote the biography dataset after preprocessing as WikiBias-Aug, statistics of which can be found in Appendix D."
        },
        {
            "heading": "4.3.2 Generation",
            "text": "Prompt Design Similar to CLG experiments, we use prompting to obtain LLM-generated professional documents. Different from CLG, CBG provides the model with more context information in the form of personal biographies in the input. Specifically, we use biographies in the preprocessed WikiBias-Aug dataset as contextual information. Templates used to prompt different LLMs are attached in Appendix C.3. Generation hyperparameter settings can be found in Appendix A.\nGenerating Reference Letters We verbalize biographies in the WikiBias-Aug dataset with the designed prompt templates and query LLMs with the combined information. Upon filtering out unsuccessful generations with the criterion defined in Section 4.1, we get 6, 028 generations for ChatGPT and 4, 228 successful generations for Alpaca."
        },
        {
            "heading": "4.3.3 Evaluation: Biases in Lexical Content",
            "text": "Given our aim to investigate biases in nouns and adjectives as lexical content, we first extract words\nof the two lexical categories in professional documents. To do this, we use the Spacy Python library (Honnibal and Montani, 2017) to match and extract all nouns and adjectives in the generated documents for males and females. After collecting words in documents, we create a noun dictionary and an adjective dictionary for each gender to further apply the odds ratio analysis."
        },
        {
            "heading": "4.3.4 Evaluation: Biases in Language Style",
            "text": "In accordance with the definitions of the three types of gender biases in the language style of LLM-generated documents in Section 3.2.2, we implement three corresponding metrics for evaluation. Biases in Language Formality For evaluation of biases in language formality, we first classify the formality of each sentence in generated letters, and calculate the percentage of formal sentences in each generated document. To do so, we apply an off-the-shelf language formality classifier from the Transformers Library that is fine-tuned on Grammarly\u2019s Yahoo Answers Formality Corpus (GYAFC) (Rao and Tetreault, 2018). We then conduct statistical t-tests on formality percentages in male and female documents to report significance levels. Biases in Language Positivity Similarly, for evaluation of biases in language positivity, we calculate and conduct t-tests on the percentage of positive sentences in each generated document for males and females. To do so, we apply an off-theshelf language sentiment analysis classifier from\nthe Transformers Library that was fine-tuned on the SST-2 dataset (Socher et al., 2013). Language Agency Classifier Along similar lines, for evaluation of biases in language agency, we conduct t-tests on the percentage of agentic sentences in each generated document for males and females. Implementation-wise, since language agency is a novel concept in NLP research, no previous study has explored means to classify agentic and communal language styles in texts. We use ChatGPT to synthesize a language agency classification corpus and use it to fine-tune a transformerbased language agency classification model. Details of the dataset synthesis and classifier training process can be found in Appendix F.2."
        },
        {
            "heading": "4.3.5 Result",
            "text": "Biases in Lexical Content Table 3 shows results for biases in lexical content on ChatGPT and Alpaca. Specifically, we show the top 10 salient adjectives and nouns for each gender. We first observe that both ChatGPT and Alpaca tend to use gender-stereotypical words in the generated letter (e.g. \u201crespectful\u201d for males and \u201cwarm\u201d for females). To produce more interpretable results, we run WEAT score analysis with two sets of genderstereotypical traits: i) male and female popular names (WEAT (MF)) and ii) career and familyrelated words (WEAT (CF)), full word lists of which can be found in Appendix F.3. WEAT takes two lists of words (one for male and one for female) and verifies whether they have a smaller embedding distance with female-stereotypical traits or\nmale-stereotypical traits. A positive WEAT score indicates a correlation between female words and female-stereotypical traits, and vice versa. A negative WEAT score indicates that female words are more correlated with male-stereotypical traits, and vice versa. To target words that potentially demonstrate gender stereotypes, we identify and highlight words that could be categorized within the nine lexicon categories in Table 2, and run WEAT test on these identified words. WEAT score result reveals that the most salient words in male and female documents are significantly associated with genderstereotypical lexicon.\nBiases in Language Style Table 4 shows results for biases in language style on ChatGPT and Alpaca. T-testing results reveal gender biases in the language styles of documents generated for both models, showing that male documents are significantly higher than female documents in all three aspects: language formality, positivity, and agency. Interestingly, our experiment results align well with social science findings on biases in language professionalism, language excellency, and language agency for human-written reference letters.\nTo unravel biases in model-generated letters in a more intuitive way, we manually select a few snippets from ChatGPT\u2019s generations that showcase biases in language agency. Each pair of grouped texts in Table 5 is sampled from the 2 generated letters for male and female candidates with the same original biography information. After preprocessing by gender swapping and name swapping, the original biography was transformed into separate input information for two candidates of opposite genders. We observe that even when provided with the exact same career-related information despite name and gender, ChatGPT still generates reference letters\nwith significantly biased levels of language agency for male and female candidates. When describing female candidates, ChatGPT uses communal phrases such as \u201cgreat to work with\u201d, \u201ccommunicates well\u201d, and \u201ckind\u201d. On the contrary, the model tends to describe male candidates as being more agentic, using narratives such as \u201ca standout in the industry\u201d and \u201ca true original\u201d."
        },
        {
            "heading": "4.4 Hallucination Bias",
            "text": ""
        },
        {
            "heading": "4.4.1 Hallucination Detection",
            "text": "We use the proposed Context-Sentence NLI framework for hallucination detection. Specifically, we implement an off-the-shelf RoBERTa-Large-based NLI model from the Transformers Library that was fine-tuned on a combination of four NLI datasets: SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), FEVER-NLI (Thorne et al., 2018), and ANLI (R1, R2, R3) (Nie et al., 2020). We then identify bias exacerbation in model hallucination along the same three dimensions as in Section 4.3.4, through t-testing on the percentage of formal, positive, and agentic sentences in the hallucinated content compared to the full generated letter."
        },
        {
            "heading": "4.4.2 Result",
            "text": "As shown in Table 6, both ChatGPT and Alpaca demonstrate significant hallucination biases in language style. Specifically, ChatGPT hallucinations are significantly more formal and more positive for male candidates, whereas significantly less agentic for female candidates. Alpaca hallucinations\nare significantly more positive for male candidates, whereas significantly less formal and agentic for females. This reveals significant gender bias propagation and amplification in LLM hallucinations, pointing to the need to further study this harm.\nTo further unveil hallucination biases in a straightforward way, we also manually select snippets from hallucinated parts in ChatGPT\u2019s generations. Each pair of grouped texts in Table 7 is selected from two generated letters for male and female candidates given the same original biography information. Hallucinations in the female reference letters use communal language, describing the candidate as having an \u201ceasygoing nature\u201d, and \u201cis a joy to work with\u201d. Hallucinations in the male reference letters, in contrast, use evidently agentic descriptions of the candidate, such as \u201cnatural talent\u201d, with direct mentioning of \u201cprofessionalism\u201d."
        },
        {
            "heading": "5 Conclusion and Discussion",
            "text": "Given our findings that gender biases do exist in LLM-generated reference letters, there are many avenues for future work. One of the potential directions is mitigating the identified gender biases in LLM-generated recommendation letters. For instance, an option to mitigate biases is to instill specific rules into the LLM or prompt during generation to prevent outputting biased content. Another direction is to explore broader areas of our problem statement, such as more professional document\ncategories, demographics, and genders, with more language style or lexical content analyses. Lastly, reducing and understanding the biases with hallucinated content and LLM hallucinations is an interesting direction to explore.\nThe emergence of LLMs such as ChatGPT has brought about novel real-world applications such as reference letter generation. However, fairness issues might arise when users directly use LLMgenerated professional documents in professional scenarios. Our study benchmarks and critically analyzes gender bias in LLM-assisted reference letter generation. Specifically, we define and evaluate biases in both Context-Less Generation and Context-Based Generation scenarios. We observe that when given insufficient context, LLMs default to generating content based on gender stereotypes. Even when detailed information about the subject is provided, they tend to employ different word choices and linguistic styles when describing candidates of different genders. What\u2019s more, we find out that LLMs are propagating and even amplifying harmful gender biases in their hallucinations.\nWe conclude that AI-assisted writing should be employed judiciously to prevent reinforcing gender stereotypes and causing harm to individuals. Furthermore, we wish to stress the importance of building a comprehensive policy of using LLM in real-world scenarios. We also call for further research on detecting and mitigating fairness issues in LLM-generated professional documents, since understanding the underlying biases and ways of reducing them is crucial for minimizing potential harms of future research on LLMs."
        },
        {
            "heading": "Limitations",
            "text": "We identify some limitations of our study. First, due to the limited amount of datasets and previous literature on minority groups and additional backgrounds, our study was only able to consider the binary gender when analyzing biases. We do stress, however, the importance of further extending our study to fairness issues for other gender minority groups as future works. In addition, our study primarily focuses on reference letters to narrow the scope of analysis. We recognize that there\u2019s a large space of professional documents now possible due to the emergence of LLMs, such as resumes, peer evaluations, and so on, and encourage future researchers to explore fairness issues in other categories of professional documents. Additionally, due to cost and compute constraints, we were only able to experiment with the ChatGPT API and 3 other open-source LLMs. Future work can build upon our investigative tools and extend the analysis to more gender and demographic backgrounds, professional document types, and LLMs. We believe in the importance of highlighting the harms of using LLMs for these applications and that these tools act as great writing assistants or first drafts of a document but should be used with caution as biases and harms are evident."
        },
        {
            "heading": "Ethics Statement",
            "text": "The experiments in this study incorporate LLMs that were pre-trained on a wide range of text from the internet and have been shown to learn or amplify biases from this data. In our study, we seek to further explore the ethical considerations of using LLMs within professional documents through the representative task of reference letter generation. Although we were only able to analyze a subset of the representative user base of LLMs, our study uncover noticeable harms and areas of concern when using these LLMs for real-world scenarios. We hope that our study adds an additional layer of caution when using LLMs for generating professional documents, and promotes the equitable and inclusive advancement of these intelligent systems."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank UCLA-NLP+ members and anonymous reviewers for their invaluable feedback. The work is supported in part by CISCO, NSF 2331966. KC was supported as a Sloan Fellow."
        },
        {
            "heading": "A Generation Hyperparameter Settings",
            "text": "We use the default parameters of ChatGPT with OpenAI\u2019s chat completion API, which are \"GPT3.5-Turbo\" with temperature, top_p, and n set to 1 and no stop token. For Alpaca, Vicuna, and StableLM, we configure the maximum number of new tokens to be 512, repetition penalty to be 1.5, temperature to be 0.1, top p to be 0.75, and number of beams to be 2. All configuration hyper-parameters are selected through parameter tuning experiments to ensure the best generation performance of each model."
        },
        {
            "heading": "B Generation Success Rate Analysis",
            "text": "During reference letter generation, we observe that i) ChatGPT can always produce reasonable reference letters, and ii) other LLMs that we investigate sometimes fail to do so. In the following section, we will first briefly show typical examples of generation failure. Then, we will provide our definition and criteria for successful generations. Finally, we compare Alpaca, Vicuna, and StableLM in terms of their generation success rate, and argue that Alpaca significantly outperforms the other two models investigated in the reference letter generation task."
        },
        {
            "heading": "B.1 Failure Analysis",
            "text": "Table 8 presents the three types of unsuccessful generations of LLMs: empty content, repetitive content, and task divergence."
        },
        {
            "heading": "B.2 Successful Generation",
            "text": "Taking into consideration the failure types of LLM generations, we define a success generation to be nonempty, non-repetitive, and task-following (i.e. generating a recommendation letter instead of other types of text). Therefore, we establish 3 criteria as a vanilla way to implement rule-based unsuccessful generation detection. Specifically, we keep generations that are: i) non-empty, ii) do not contain long continuous strings, and iii) contain the word \u201crecommend\u201d."
        },
        {
            "heading": "B.3 Generation Success Rate",
            "text": "We calculate and report the generation success rate of LLMs in Table 9. Overall, Alpaca achieves a significantly higher generation success rate than the other LLMs. Therefore, we chose to conduct further evaluation experiments only with generated letters ChatGPT and Alpaca."
        },
        {
            "heading": "C Prompt Design",
            "text": ""
        },
        {
            "heading": "C.1 Descriptors for CLG task",
            "text": "Table 10 shows the descriptors that we consider when generating CLG reference letters, narrowing down our generation space to a feasible amount."
        },
        {
            "heading": "C.2 Prompts for CLG Task",
            "text": "Table 11 shows the prompts that we use to query the generation of reference letters for the CLG task."
        },
        {
            "heading": "C.3 Prompts for CBG Task",
            "text": "Table 12 shows the prompts that we use to query the generation of reference letters for CBG task."
        },
        {
            "heading": "D Dataset Statistics: WikiBias-Aug",
            "text": "Table 13 shows statistics of the pre-processed WikiBias-Aug dataset."
        },
        {
            "heading": "E Sample Reference Letter Generations",
            "text": ""
        },
        {
            "heading": "E.1 ChatGPT",
            "text": "Context-Less Generation Please see Table 14 for an example of a generated reference letter by ChatGPT under CLG scenario. Context-Based Generation Please see Table 15 for an example of a generated reference letter by ChatGPT under CBG scenario."
        },
        {
            "heading": "E.2 Alpaca",
            "text": "Context-Based Generation Please see Table 16 for an example of a generated reference letter by Alpaca under CBG scenario."
        },
        {
            "heading": "F Experiment Details",
            "text": ""
        },
        {
            "heading": "F.1 Preprocessing Pipeline",
            "text": "Evaluation of CBG-based professional document generation requires a dataset with gender-balanced and anonymized contexts to avoid i) pre-existing gender biases and ii) potential model hallucinations triggered by real demographic information, such as names. To this end, we propose and use a data\npreprocessing pipeline to produce an anonymized and gender-balanced personal biography dataset as context information in CBG-based reference letter generation, which we denote as WikiBias-Aug. In our work, the preprocessing pipeline was built to augment the WikiBias dataset (Sun and Peng, 2021), a personal biography dataset with scraped demographic information as well as biographic information from Wikipedia. However, the proposed pipeline can also be extended to augmentation on other biography datasets. Due to the inclusion of only binary gender in the WikiBias dataset, our study is also limited to studying biases within the two genders. More details will be discussed in the Limitation section. In this study, each biography entry of the original WikiBias dataset consists of the personal life and career life sections in the Wikipedia description of the person. In order to utilize personal biographies as contexts in our CBG-based evaluation pipeline, we need to construct a more gender-balanced dataset with a certain level of anonymization. In addition, considering LLMs\u2019 input tokens limit, we would need to design methods to control the overall length of the biographies in each entry. Figure 2 provides an illustration of the preprocessing pipeline. We first iterate through all demographic information in the WikiBias dataset to stack all the 1) female first names, 2) male first names, as well as 3) all last names regardless of gender. Since we have the gender information of the person described in each biography, we use it as the ground truth to categorize names of each gender, without introducing noises in gender-stereotypical names. For each entry of the WikiBias dataset, we first randomly select 2 paragraphs from the personal and career life sections in the biography. Next, we make heuristics-based changes to the sampled biography to output a number of male biographies and a number of female biographies. For constructing the male biography, we randomly select a male first name and a last name from the according stacks, and replace all name mentions in the original biography with the new male name. If the original biography describes a female, we also make sure to flip all gendered pronouns (e.g. her, she, hers) in the sentence to male pronouns. Similarly, for constructing the female biography, we randomly select a female first name and a last name and replace all name mentions in the original biography with the new female name. We also flip the gendered\npronouns if the original biography is describing a male."
        },
        {
            "heading": "F.2 Building a Language Agency Classifier",
            "text": "Dataset Construction Given that no prior research in the NLP community has covered a classifier to detect agentic vs communal, we opted to create our classifier and dataset. For this approach, we use ChatGPT to synthetically generate an evenly distributed dataset of 400 unique biographies per category. The initial biography is sampled from the Bias in Bios dataset (De-Arteaga et al., 2019), which is sourced from online biographies in the Common Crawl corpus. The dataset also includes metadata across several occupations and gender indicators. We prompt ChatGPT to rephrase this initial biography into two versions: one leaning towards agentic language style (e.g. leadership) and another leaning towards communal language style. To ensure reliability, consistency, and quality of generation, we additionally condition ChatGPT\u2019s outputs on specific definitions of agentic and communal language in social science literature. The full prompt used to generate the language agency classification dataset is shown in Table 17. Eventually, we synthesized a dataset of around 600 samples. To validate ChatGPT\u2019s generation quality, we invited 2 expert annotators to conduct a human evaluation of a held-out test set of 60 samples (10% of our 600 generations) from the generated dataset. Specifically, each expert is asked to manually label the test set. The mean expert-dataset agreement score using Cohen\u2019s Kappa is 0.864 and the inter-researcher agreement score using Cohen\u2019s Kappa between the two experts is 0.862. Fleiss\u2019s Kappa agreement score between the two expert annotators and the dataset labels is 0.863. All agreement scores demonstrate good levels of inter-rater and rater-dataset alignment, proving the satisfactory quality of the synthesized agency classification dataset.\nTraining Details Given this synthetic dataset of\naround 600 samples, we build a BERT classifier given an 80/10/10 train/dev/test split. We performed a hyperparameter search and ended up with a learning rate of 2e-5, training epochs of 10, and a batch size of 16. After training and saving the bestperforming checkpoints on the validation samples, the final trained classifier achieves an accuracy of 96.0%, with a precision of 92.0% and a recall of 100.00%. The synthesized dataset and the checkpoint of the final classifier will be released."
        },
        {
            "heading": "F.3 Word Lists For WEAT Test",
            "text": "Table 18 demonstrates Gendered word lists used for WEAT testing."
        },
        {
            "heading": "F.4 Trained Classifier Statistics",
            "text": "In our experiments, we use several classifiers as a proxy to investigate biases in language style across language formality, sentiment, and agency. In Table 19, we hereby provide full details of the precision, recall, and F1 score metrics for all three classifiers. The \u201cLanguage Agency\u201d dataset refers to the language agency classification dataset that we synthesized in this work."
        },
        {
            "heading": "F.5 Full List of Lexicon Categories",
            "text": "Table 20 demonstrates the full lists of the nine lexicon categories investigated."
        }
    ],
    "title": "\u201cKelly is a Warm Person, Joseph is a Role Model\u201d: Gender Biases in LLM-Generated Reference Letters",
    "year": 2023
}