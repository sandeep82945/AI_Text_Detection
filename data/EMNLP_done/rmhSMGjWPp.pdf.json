{
    "abstractText": "Data collection from manual labeling provides domain-specific and task-aligned supervision for data-driven approaches, and a critical mass of well-annotated resources is required to achieve reasonable performance in natural language processing tasks. However, manual annotations are often challenging to scale up in terms of time and budget, especially when domain knowledge, capturing subtle semantic features, and reasoning steps are needed. In this paper, we investigate the efficacy of leveraging large language models on automated labeling for computational stance detection. We empirically observe that while large language models show strong potential as an alternative to human annotators, their sensitivity to taskspecific instructions and their intrinsic biases pose intriguing yet unique challenges in machine annotation. We introduce a multi-label and multi-target sampling strategy to optimize the annotation quality. Experimental results on the benchmark stance detection corpora show that our method can significantly improve performance and learning efficacy.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhengyuan Liu"
        },
        {
            "affiliations": [],
            "name": "Hai Leong Chieu"
        },
        {
            "affiliations": [],
            "name": "Nancy F. Chen"
        }
    ],
    "id": "SP:f57513fbf747645da634b64fab55a7a618ae84ba",
    "references": [
        {
            "authors": [
                "Emily Allaway",
                "Kathleen Mckeown."
            ],
            "title": "Zeroshot stance detection: A dataset and model using generalized topic representations",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8913\u20138931.",
            "year": 2020
        },
        {
            "authors": [
                "Isabelle Augenstein",
                "Tim Rockt\u00e4schel",
                "Andreas Vlachos",
                "Kalina Bontcheva."
            ],
            "title": "Stance detection with bidirectional conditional encoding",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 876\u2013885. As-",
            "year": 2016
        },
        {
            "authors": [
                "Ji",
                "Tiezheng Yu",
                "Willy Chung"
            ],
            "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Innocent Chiluwa",
                "Presley Ifukor."
            ],
            "title": "war against our children\u2019: Stance and evaluation in# bringbackourgirls campaign discourse on twitter and facebook",
            "venue": "Discourse & Society, 26(3):267\u2013296.",
            "year": 2015
        },
        {
            "authors": [
                "Christopher Clark",
                "Mark Yatskar",
                "Luke Zettlemoyer."
            ],
            "title": "Don\u2019t take the easy way out: Ensemble based methods for avoiding known dataset biases",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Jiachen Du",
                "Ruifeng Xu",
                "Yulan He",
                "Lin Gui."
            ],
            "title": "Stance classification with target-specific neural attention networks",
            "venue": "International Joint Conferences on Artificial Intelligence.",
            "year": 2017
        },
        {
            "authors": [
                "John W Du Bois."
            ],
            "title": "The stance triangle",
            "venue": "Stancetaking in discourse: Subjectivity, evaluation, interaction, 164(3):139\u2013182.",
            "year": 2007
        },
        {
            "authors": [
                "Yu Fei",
                "Yifan Hou",
                "Zeming Chen",
                "Antoine Bosselut."
            ],
            "title": "Mitigating label biases for in-context learning",
            "venue": "arXiv preprint arXiv:2305.19148.",
            "year": 2023
        },
        {
            "authors": [
                "Shalmoli Ghosh",
                "Prajwal Singhania",
                "Siddharth Singh",
                "Koustav Rudra",
                "Saptarshi Ghosh."
            ],
            "title": "Stance detection in web and social media: a comparative study",
            "venue": "International Conference of the CrossLanguage Evaluation Forum for European Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Kyle Glandt",
                "Sarthak Khanal",
                "Yingjie Li",
                "Doina Caragea",
                "Cornelia Caragea."
            ],
            "title": "Stance detection in COVID-19 tweets",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Pentti Haddington"
            ],
            "title": "Stance taking in news interviews",
            "venue": "SKY Journal of Linguistics, 17:101\u2013142.",
            "year": 2004
        },
        {
            "authors": [
                "Kazi Saidul Hasan",
                "Vincent Ng."
            ],
            "title": "Stance classification of ideological debates: Data, models, features, and constraints",
            "venue": "Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 1348\u20131356, Nagoya, Japan. Asian",
            "year": 2013
        },
        {
            "authors": [
                "Xingwei He",
                "Zhenghao Lin",
                "Yeyun Gong",
                "A Jin",
                "Hang Zhang",
                "Chen Lin",
                "Jian Jiao",
                "Siu Ming Yiu",
                "Nan Duan",
                "Weizhu Chen"
            ],
            "title": "Annollm: Making large language models to be better crowdsourced annotators",
            "venue": "arXiv preprint arXiv:2303.16854",
            "year": 2023
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "Yonatan Belinkov",
                "James Henderson."
            ],
            "title": "End-to-end bias mitigation by modelling biases in corpora",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8706\u20138716, Online. Asso-",
            "year": 2020
        },
        {
            "authors": [
                "Ayush Kaushal",
                "Avirup Saha",
                "Niloy Ganguly."
            ],
            "title": "twt\u2013wt: A dataset to assert the role of target entities for detecting stance of tweets",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "Dilek K\u00fc\u00e7\u00fck",
                "Fazli Can."
            ],
            "title": "Stance detection: A survey",
            "venue": "ACM Computing Surveys (CSUR), 53(1).",
            "year": 2020
        },
        {
            "authors": [
                "Yingjie Li",
                "Cornelia Caragea."
            ],
            "title": "Target-aware data augmentation for stance detection",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.",
            "year": 2021
        },
        {
            "authors": [
                "Yingjie Li",
                "Tiberiu Sosea",
                "Aditya Sawant",
                "Ajith Jayaraman Nair",
                "Diana Inkpen",
                "Cornelia Caragea."
            ],
            "title": "P-stance: A large dataset for stance detection in political domain",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zhengyuan Liu",
                "Yong Keong Yap",
                "Hai Leong Chieu",
                "Nancy Chen."
            ],
            "title": "Guiding computational stance detection with expanded stance triangle framework",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2023
        },
        {
            "authors": [
                "Saif Mohammad",
                "Svetlana Kiritchenko",
                "Parinaz Sobhani",
                "Xiaodan Zhu",
                "Colin Cherry."
            ],
            "title": "SemEval-2016 task 6: Detecting stance in tweets",
            "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 31\u201341.",
            "year": 2016
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Chengwei Qin",
                "Aston Zhang",
                "Zhuosheng Zhang",
                "Jiaao Chen",
                "Michihiro Yasunaga",
                "Diyi Yang"
            ],
            "title": "Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476",
            "year": 2023
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in Neural Information Processing Systems, pages 6000\u20136010.",
            "year": 2017
        },
        {
            "authors": [
                "Prashanth Vijayaraghavan",
                "Ivan Sysoev",
                "Soroush Vosoughi",
                "Deb Roy."
            ],
            "title": "Deepstance at semeval-2016 task 6: Detecting stance in tweets using character and word-level cnns",
            "venue": "arXiv preprint arXiv:1606.05694.",
            "year": 2016
        },
        {
            "authors": [
                "Bin Wang",
                "Zhengyuan Liu",
                "Xin Huang",
                "Fangkai Jiao",
                "Yang Ding",
                "Ai Ti Aw",
                "Nancy F Chen."
            ],
            "title": "Seaeval for multilingual foundation models: From cross-lingual alignment to cultural reasoning",
            "venue": "arXiv preprint arXiv:2309.04766.",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jianhua Yuan",
                "Yanyan Zhao",
                "Yanyue Lu",
                "Bing Qin."
            ],
            "title": "Ssr: Utilizing simplified stance reasoning process for robust stance detection",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, pages 6846\u20136858.",
            "year": 2022
        },
        {
            "authors": [
                "Guido Zarrella",
                "Amy Marsh."
            ],
            "title": "Mitre at semeval2016 task 6: Transfer learning for stance detection",
            "venue": "arXiv preprint arXiv:1606.03784.",
            "year": 2016
        },
        {
            "authors": [
                "Chenye Zhao",
                "Yingjie Li",
                "Cornelia Caragea."
            ],
            "title": "C-stance: A large dataset for chinese zero-shot stance detection",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13369\u201313385.",
            "year": 2023
        },
        {
            "authors": [
                "Yiwei Zhou",
                "Alexandra I Cristea",
                "Lei Shi."
            ],
            "title": "Connecting targets to tweets: Semantic attentionbased model for target-specific stance detection",
            "venue": "International Conference on Web Information Systems Engineering, pages 18\u201332. Springer.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Stance detection is one of the fundamental social computing tasks in natural language processing, which aims to predict the attitude toward specified targets from a piece of text. It is commonly formulated as a target-based classification problem (K\u00fc\u00e7\u00fck and Can, 2020): given the input text (e.g., online user-generated content) and a specified target, stance detection models are used to predict a categorical label (e.g., Favor, Against, None) (Mohammad et al., 2016; Li et al., 2021). Upon social networking platforms\u2019 growing impact on our lives, stance detection is crucial for various downstream tasks such as fact verification and rumor detection, with wide applications including analyzing user feedback and political opinions (Chiluwa and Ifukor, 2015; Ghosh et al., 2019).\nData-driven approaches largely benefit from the available human-annotated corpora, which provide domain-specific and task-aligned supervision. Developing state-of-the-art models via in-domain finetuning usually requires certain well-annotated training data, such as machine translation and text summarization. However, manual annotations are challenging to scale up in terms of time and budget, especially when domain knowledge, capturing subtle semantic features, and reasoning steps are needed. For instance, for computational stance detection, there are some corpora with target-aware annotation (Mohammad et al., 2016; Allaway and Mckeown, 2020; Li et al., 2021; Glandt et al., 2021), but they are limited in sample size, target diversity, and domain coverage. Learning from insufficient data leads to low generalization on out-of-domain samples and unseen targets (Allaway and Mckeown, 2020), and models are prone to over-fit on superficial and biased features (Kaushal et al., 2021).\nRecently, how the versatile large language models (LLMs) would contribute to data annotation raised emerging research interest (He et al., 2023). Machine annotation is also one approach to knowledge distillation from large and general models (Hinton et al., 2015), and student models can be lightweight and cost-effective to deploy. In this paper, we leverage and evaluate LLMs on automated\nlabeling for computational stance detection. We empirically find that the general-purpose models can provide reasonable stance labeling, and show the potential as an alternative to human annotators. However, there are substantial variances from multiple aspects, and models exhibit biases on target description and label arrangement (\u00a73). Such issues pose challenges regarding the quality and reliability of machine annotation. To optimize the annotation framework, we introduce the multi-label inference and multi-target sampling strategy (\u00a74). Evaluation results on five benchmark stance detection corpora show that our method can significantly improve the performance and learning efficacy (\u00a75)."
        },
        {
            "heading": "2 Related Work",
            "text": "Computational stance detection aims to automatically predict polarities given the text and a stance target. The model design for this task has been shifted from the traditional statistical methods such as support vector machines (Hasan and Ng, 2013), to neural approaches such as recurrent neural networks (Zarrella and Marsh, 2016), convolutional neural networks (Vijayaraghavan et al., 2016; Augenstein et al., 2016; Du et al., 2017; Zhou et al., 2017), and Transformer (Vaswani et al., 2017). To facilitate the development of data-driven approaches, human-annotated datasets for stance detection are constructed for model training and evaluation (Mohammad et al., 2016; Allaway and Mckeown, 2020; Li et al., 2021; Glandt et al., 2021). While the Transformer-based models, especially pre-trained language backbones (Devlin et al., 2019; Liu et al., 2019), boost the performance on stance detection benchmarks (Ghosh et al., 2019; Li and Caragea, 2021), due to the limited training data and low labeling diversity, these supervised models showed degraded performance on unseen-target and out-of-domain evaluation (Kaushal et al., 2021). To address this problem, recent work introduced de-biasing methods (Clark et al., 2019; Karimi Mahabadi et al., 2020), multitask learning (Yuan et al., 2022), and the linguisticsinspired augmentation (Liu et al., 2023)."
        },
        {
            "heading": "3 Machine Annotation for Computational Stance Detection",
            "text": "Based on large-scale pre-training and instruction tuning, language models are capable of solving various downstream tasks in a zero-shot manner (Ouyang et al., 2022). In many NLP benchmarks,\nthey outperform supervised state-of-the-art models, and even achieve comparable performance to human annotators (Bang et al., 2023; Qin et al., 2023). In this section, we investigate the feasibility and reliability of machine annotation for computational stance detection. To compare automated outputs with human annotation, our experiments are conducted on a well-annotated tweet stance detection corpus: SemEval-16 Task-6 A (Mohammad et al., 2016), which serves as a benchmark in many previous works. For extensive comparison, we select three general-purpose LLMs: Alpaca-13B (Taori et al., 2023), Vicuna-13B (Chiang et al., 2023), and GPT-3.5-turbo-0301)."
        },
        {
            "heading": "3.1 Vanilla Machine Annotation Setup",
            "text": "The overview of the machine annotation process is shown in Figure 1. Since stance detection is commonly formulated as a target-based classification task, the text sample and specified target are two elementary components of model input (Allaway and Mckeown, 2020; Liu et al., 2023). To leverage LLMs for automated stance labeling, we formulate task-specific instructions as the prompt (e.g., \u201cClassify the tweet\u2019s stance on the target into Favor, Against, None.\u201d), and adjust their format accordingly to different prompt styles (see details in Appendix Table 4). Since generative system outputs are not in a pre-defined label space, we adopt keyword matching and label re-mapping, and obtain the final model prediction. Moreover, to assess model performance under different settings, we do multiple inference rounds by changing prompts from three aspects, as shown in Table 1."
        },
        {
            "heading": "3.2 Vanilla Annotation Result & Analysis",
            "text": "Following previous work (Mohammad et al., 2016; Allaway and Mckeown, 2020), we use the classification metric F1 score to quantitatively measure the model performance. As shown in Figure 2, the general-purpose LLMs can provide reasonable zero-shot inference results, demonstrating the feasibility of machine annotation. In particular, the GPT-3.5-turbo is comparable to human annotators. Not surprisingly, it significantly outperforms Vicuna-13B and Alpaca-13B, since it has a much larger parameter size and is further optimized via human-reinforced instruction tuning. On the other hand, we observe that models do not achieve consistent performance across different prompt settings (see details in Table 1), and the annotation accuracy is prone to fluctuate from multiple aspects:\n(1) Model predictions are sensitive to taskspecific instructions. The nature of human instruction results in diverse prompts for one specific task (Chiang et al., 2023), and there is no gold standard. To simulate this scenario, we paraphrase the instruction for stance detection and keep the prompts semantically identical. We then compare model generations by feeding rephrased instruction prompts. As shown in Figure 2, all models show\na certain variance of predicted labels under three different instructions. (2) Model predictions are sensitive to the target description. As stance objects can be in the form of entity, concept, or claim, we then assess the models\u2019 robustness on target variants. Here we select \u201catheism\u201d as one example, and use three relevant terms \u201cno belief in gods\u201d, \u201creligion\u201d, and \u201cbelieve in god\u201d for comparison. In particular, the stance labels of \u201catheism\u201d and \u201creligion\u201d/\u201cbelieve in god\u201d are reversed. For instance, if the original label of one text on \u201catheism\u201d is Favor, then its label of \u201creligion\u201d/\u201cbelieve in god\u201d is Against. As shown in Figure 3, models perform much better on the target \u201creligion\u201d than the original one, and their results are different across the four variants. (3) Model predictions are sensitive to the label arrangement. Current LLMs still suffer from exposure biases, such as positional bias in summarization, and majority label bias in classification (Fei et al., 2023; Wang et al., 2023). To evaluate the intrinsic bias on label arrangement, we keep the Instruction Prompt A fixed, but re-order the stance labels (i.e., Favor, Against, None). As shown in Figure 4, the selected LLMs perform differently given the re-ordered labels, and we observed that prediction of the label None is more affected."
        },
        {
            "heading": "4 Improving Machine Annotation via Multi-label and Multi-target Sampling",
            "text": "The aforementioned labeling variances pose a question mark on the reliability of machine annotation, and the noisy data cannot be readily used to train dedicated parameter-efficient models. Previous work proved that capturing target referral information and simulating human reasoning steps are beneficial for stance detection (Yuan et al., 2022; Liu et al., 2023). Therefore, we introduce two methods to improve the machine annotation quality. In this section, we evaluate the Vicuna-13B, as it is an open model and reaches a balance between performance and computational complexity.\n(1) Enhanced Multi-label Instruction One challenge of computational stance detection comes from the ubiquitous indirect referral of stance objects (Haddington et al., 2004). Social network users express their subjective attitude with brevity and variety: they often do not directly mention the final target, but mention its related entities, events, or concepts. Inspired by chain-of-thought prompting (Wei et al., 2022) and human behavior in stancetaking (Du Bois, 2007; Liu et al., 2023), to encourage models to explicitly utilize the referral information, we decompose the inference process to two steps. More specifically, as shown in Figure 6, before the stance label prediction, we add one step to describe the context-object relation (i.e., implicit or explicit mention). With the enhanced two-hop instruction, Vicuna-13B becomes comparable to the GPT-3.5-turbo (see Figure 5).\n(2) Diverse Sampling with Multi-target Prediction The SemEval-16 Task-6 A corpus only covers five stance targets (e.g., politicians, feminism, climate change, atheism), and each sample is associated with one target with a human-annotated stance label. To reduce superficial target-related patterns and biases from single target labeling, following previous work (Liu et al., 2023), we apply an adversarial multi-target sampling for machine annotation. First, an off-the-shelf constituency parser is used to extract all noun phrases from the text.1 We then use these phrases as targets for stance labeling; if one phrase is in a contrary stance label to the original target, we include it as an additional sample. This adversarial multi-target sampling helps models to condition their prediction more on the given target, as well as learn some correlation between explicit and implicit objects. In our experiment, we obtain 1.2k multi-target samples, where the original corpus size is 2914."
        },
        {
            "heading": "5 Leveraging Machine-annotated Data for Model Training",
            "text": "Based on our improvements in Section 4, to compare the learning efficacy from machine-annotated and human-annotated data, we conduct experiments by training a supervised model. Experimental Setting As light-weight language backbones are more applicable to practical use cases, we select RoBERTa (Liu et al., 2019) which provides state-of-the-art performance in many pre-\n1https://spacy.io/universe/project/self-attentive-parser\nvious works on stance detection (Liu et al., 2023; Zhao et al., 2023). We train the model on the SemEval-16 Task-6 A corpus, and collect five representative benchmarks to build the in-domain, out-of-domain, and cross-target evaluation settings (K\u00fc\u00e7\u00fck and Can, 2020), including SemEval-16 Task-6 A and Task-6 B (Mohammad et al., 2016), PStance (Li et al., 2021), VAST (Allaway and Mckeown, 2020), and Tweet-COVID (Glandt et al., 2021). Model input is formulated as \u201c<s> {target} </s> <s> {context} </s>\u201d, and the final hidden representation of the first token \u201c</s>\u201d is fed to a linear layer and softmax function to compute the output probabilities. Experimental details are shown in Appendix Table 3 and Table 4. We use the 3-class prediction scheme, as the None label is necessary for practical use cases. Following previous work (Mohammad et al., 2016; Allaway and Mckeown, 2020), we adopt the macro-averaged F1, Precision, and Recall as evaluation metrics.\nResults and Analysis Since we train the model on a single-domain corpus, testing it on out-ofdomain samples and various unseen targets poses a challenging task. As shown in Table 2, compared with training on human-annotated data, applying machine annotation upon the vanilla instruction in Section 3 results in lower scores, especially on the SemEval-16 Task-6 A; F1, precision, and recall scores are all affected. When applying our proposed methods, we observe that: (1) The multi-label instruction can improve indomain performance. Our proposed two-hop instruction provides training data with higher quality, particularly in the in-domain evaluation (17.6% rel-\native gain); this is consistent with the improved annotation accuracy shown in Figure 5. (2) The multi-target sampling can improve outof-domain and cross-target performance. The single-target data under vanilla and enhanced twohop instruction only enable the model to achieve reasonable results on in-domain samples and existing targets. In contrast, the multi-target sampling brings substantial and consistent improvement on out-of-domain and unseen targets, where relative gains on SemEval-16 Task-6B and VAST are 19.8% and 61.2%, and it approaches 83% of supervised training on human-annotated datasets. This demonstrates that the model learns more general and domain-invariant features. We also report 2-class macro-averaged scores (i.e., Favor, Against), where the None label is only discarded during inference (see Appendix Table 5), and our methods bring performance elevation at all fronts."
        },
        {
            "heading": "6 Conclusions",
            "text": "In this work, we investigated the potential and challenges of leveraging large language models on automated labeling for computational stance detection. Quantitative analyses demonstrated the feasibility, but their predictions are sensitive to taskspecific instructions, and LLMs inevitably present exposure biases on target description and label arrangement. We improved machine annotation by adopting a multi-label and multi-target sampling strategy. Experimental results on several stance detection benchmarks showed the effectiveness of our proposed methods. This work can shed light on further extensions of machine annotation."
        },
        {
            "heading": "Limitations",
            "text": "All samples used in this work are in English, thus to apply the model to other languages, the result might be limited by the multilingual capability of language backbones. Moreover, we are aware that it remains an open problem to mitigate biases in human stancetaking. Of course, current models and laboratory experiments are always limited in this or similar ways. We do not foresee any unethical uses of our proposed methods or their underlying tools, but hope that it will contribute to reducing incorrect system outputs."
        },
        {
            "heading": "Ethics and Impact Statement",
            "text": "We acknowledge that all of the co-authors of this work are aware of the provided ACL Code of Ethics and honor the code of conduct. All data used in this work are collected from existing published NLP studies. Following previous work, the annotated corpora are only for academic research purposes and should not be used outside of academic research contexts. Our proposed framework and methodology in general do not create a direct societal consequence and are intended to be used to prevent data-driven models from over-fitting domaindependent and potentially-biased features."
        },
        {
            "heading": "Acknowledgments",
            "text": "This research was supported by funding from the Institute for Infocomm Research (I2R), A*STAR, Singapore, and DSO National Laboratories, Singapore. We thank Yong Keong Yap and Wen Haw Chong for the research discussions. We also thank the anonymous reviewers for their precious feedback to help improve and extend this piece of work."
        },
        {
            "heading": "Supervised Training Experimental Configuration",
            "text": ""
        },
        {
            "heading": "Dataset Processing Details",
            "text": ""
        },
        {
            "heading": "Environment Details",
            "text": ""
        },
        {
            "heading": "Machine Annotation Experimental Configuration",
            "text": ""
        }
    ],
    "title": "Multi-label and Multi-target Sampling of Machine Annotation for Computational Stance Detection",
    "year": 2023
}