{
    "abstractText": "Visual storytelling aims to generate compelling narratives from image sequences. Existing models often focus on enhancing the representation of the image sequence, e.g., with external knowledge sources or advanced graph structures. Despite recent progress, the stories are often repetitive, illogical, and lacking in detail. To mitigate these issues, we present a novel framework which integrates visual representations with pretrained language models and planning. Our model translates the image sequence into a visual prefix, a sequence of continuous embeddings which language models can interpret. It also leverages a sequence of question-answer pairs as a blueprint plan for selecting salient visual concepts and determining how they should be assembled into a narrative. Automatic and human evaluation on the VIST benchmark (Huang et al., 2016) demonstrates that blueprint-based models generate stories that are more coherent, interesting, and natural compared to competitive baselines and state-of-the-art systems.",
    "authors": [
        {
            "affiliations": [],
            "name": "Danyang Liu"
        },
        {
            "affiliations": [],
            "name": "Mirella Lapata"
        },
        {
            "affiliations": [],
            "name": "Frank Keller"
        }
    ],
    "id": "SP:70f0511310b27f98f7c7c3c6da359e93e20f0784",
    "references": [
        {
            "authors": [
                "Jean-Baptiste Alayrac",
                "Jeff Donahue",
                "Pauline Luc",
                "Antoine Miech",
                "Iain Barr",
                "Yana Hasson",
                "Karel Lenc",
                "Arthur Mensch",
                "Katherine Millican",
                "Malcolm Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning",
            "venue": "Advances in Neural",
            "year": 2022
        },
        {
            "authors": [
                "Chris Alberti",
                "Daniel Andor",
                "Emily Pitler",
                "Jacob Devlin",
                "Michael Collins."
            ],
            "title": "Synthetic QA corpora generation with roundtrip consistency",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6168\u20136173, Flo-",
            "year": 2019
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Hong Chen",
                "Yifei Huang",
                "Hiroya Takamura",
                "Hideki Nakayama."
            ],
            "title": "Commonsense knowledge aware concept selection for diverse and informative visual storytelling",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 999\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Kordula De Kuthy",
                "Madeeswaran Kannan",
                "Haemanth Santhi Ponnusamy",
                "Detmar Meurers."
            ],
            "title": "Towards automatically generating questions under discussion to link information and discourse structure",
            "venue": "Proceedings of the 28th International Conference",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Deutsch",
                "Tania Bedrax-Weiss",
                "Dan Roth."
            ],
            "title": "Towards question-answering as an automatic metric for evaluating the content quality of a summary",
            "venue": "Transactions of the Association for Computational Linguistics, 9:774\u2013789.",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Deutsch",
                "Dan Roth."
            ],
            "title": "Incorporating question answering-based signals into abstractive summarization via salient span selection",
            "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "Vladimir Dobrovolskii."
            ],
            "title": "Word-level coreference resolution",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7670\u20137675, Online and Punta Cana, Dominican Republic. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Joshua Eisenberg",
                "Mark Finlayson."
            ],
            "title": "A simpler and more generalizable story detector using verb and character features",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2708\u20132715, Copenhagen,",
            "year": 2017
        },
        {
            "authors": [
                "Alexander Richard Fabbri",
                "Chien-Sheng Wu",
                "Wenhao Liu",
                "Caiming Xiong."
            ],
            "title": "Qafacteval: Improved qa-based factual consistency evaluation for summarization",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Associ-",
            "year": 2022
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Strategies for structuring story generation",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2650\u2013 2660.",
            "year": 2019
        },
        {
            "authors": [
                "Seraphina Goldfarb-Tarrant",
                "Tuhin Chakrabarty",
                "Ralph Weischedel",
                "Nanyun Peng."
            ],
            "title": "Content planning for neural story generation with aristotelian rescoring",
            "venue": "arXiv preprint arXiv:2009.09870.",
            "year": 2020
        },
        {
            "authors": [
                "Seraphina Goldfarb-Tarrant",
                "Tuhin Chakrabarty",
                "Ralph Weischedel",
                "Nanyun Peng."
            ],
            "title": "Content planning for neural story generation with aristotelian rescoring",
            "venue": "Proceedings of the 2020 Conference on",
            "year": 2020
        },
        {
            "authors": [
                "Diana Gonzalez-Rico",
                "Gibran Fuentes-Pineda."
            ],
            "title": "Contextualize, show and tell: A neural visual storyteller",
            "venue": "arXiv preprint arXiv:1806.00738.",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013 778.",
            "year": 2016
        },
        {
            "authors": [
                "Xudong Hong",
                "Rakshith Shetty",
                "Asad Sayeed",
                "Khushboo Mehra",
                "Vera Demberg",
                "Bernt Schiele."
            ],
            "title": "Diverse and relevant visual storytelling with scene graph embeddings",
            "venue": "Proceedings of the 24th Conference on Computational Natural Language Learn-",
            "year": 2020
        },
        {
            "authors": [
                "Chao-Chun Hsu",
                "Zi-Yuan Chen",
                "Chi-Yang Hsu",
                "ChihChia Li",
                "Tzu-Yuan Lin",
                "Ting-Hao Huang",
                "LunWei Ku."
            ],
            "title": "Knowledge-enriched visual storytelling",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7952\u20137960.",
            "year": 2020
        },
        {
            "authors": [
                "Chi-Yang Hsu",
                "Yun-Wei Chu",
                "Vincent Chen",
                "KuanChieh Lo",
                "Chacha Chen",
                "Ting-Hao Huang",
                "LunWei Ku."
            ],
            "title": "Learning to rank visual stories from human ranking data",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Chi-yang Hsu",
                "Yun-Wei Chu",
                "Ting-Hao Huang",
                "Lun-Wei Ku."
            ],
            "title": "Plot and rework: Modeling storylines for visual storytelling",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4443\u20134453.",
            "year": 2021
        },
        {
            "authors": [
                "Junjie Hu",
                "Yu Cheng",
                "Zhe Gan",
                "Jingjing Liu",
                "Jianfeng Gao",
                "Graham Neubig."
            ],
            "title": "What makes a good story? designing composite rewards for visual storytelling",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7969\u20137976.",
            "year": 2020
        },
        {
            "authors": [
                "Shaohan Huang",
                "Li Dong",
                "Wenhui Wang",
                "Yaru Hao",
                "Saksham Singhal",
                "Shuming Ma",
                "Tengchao Lv",
                "Lei Cui",
                "Owais Khan Mohammed",
                "Qiang Liu"
            ],
            "title": "Language is not all you need: Aligning perception with language models",
            "year": 2023
        },
        {
            "authors": [
                "Taehyeong Kim",
                "Min-Oh Heo",
                "Seonil Son",
                "KyoungWha Park",
                "Byoung-Tak Zhang."
            ],
            "title": "Glac net: Glocal attention cascading networks for multiimage cued story generation",
            "venue": "arXiv preprint arXiv:1805.10973.",
            "year": 2018
        },
        {
            "authors": [
                "Staffan Larsson."
            ],
            "title": "Issue-based dialogue management",
            "venue": "Citeseer.",
            "year": 2002
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Danyang Liu",
                "Juntao Li",
                "Meng-Hsuan Yu",
                "Ziming Huang",
                "Gongshen Liu",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "A character-centric neural model for automated story generation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,",
            "year": 2020
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee"
            ],
            "title": "Visual instruction tuning",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zhengyuan Liu",
                "Nancy Chen."
            ],
            "title": "Controllable neural dialogue summarization with personal named entity planning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 92\u2013106, Online and Punta Cana,",
            "year": 2021
        },
        {
            "authors": [
                "Cewu Lu",
                "Ranjay Krishna",
                "Michael Bernstein",
                "Li Fei-Fei."
            ],
            "title": "Visual relationship detection with language priors",
            "venue": "European conference on computer vision, pages 852\u2013869. Springer.",
            "year": 2016
        },
        {
            "authors": [
                "Yatri Modi",
                "Natalie Parde."
            ],
            "title": "The steep road to happily ever after: an analysis of current visual storytelling models",
            "venue": "Proceedings of the Second Workshop on Shortcomings in Vision and Language, pages 47\u201357.",
            "year": 2019
        },
        {
            "authors": [
                "Ron Mokady",
                "Amir Hertz",
                "Amit H Bermano."
            ],
            "title": "Clipcap: Clip prefix for image captioning",
            "venue": "arXiv preprint arXiv:2111.09734.",
            "year": 2021
        },
        {
            "authors": [
                "Amit Moryossef",
                "Yoav Goldberg",
                "Ido Dagan."
            ],
            "title": "Step-by-step: Separating planning from realization in neural data-to-text generation",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2019
        },
        {
            "authors": [
                "Shashi Narayan",
                "Joshua Maynez",
                "Reinald Kim Amplayo",
                "Kuzman Ganchev",
                "Annie Louis",
                "Fantine Huot",
                "Dipanjan Das",
                "Mirella Lapata."
            ],
            "title": "Conditional generation with a question-answering blueprint",
            "venue": "arXiv preprint arXiv:2207.00397.",
            "year": 2022
        },
        {
            "authors": [
                "Shashi Narayan",
                "Yao Zhao",
                "Joshua Maynez",
                "Gon\u00e7alo Sim\u00f5es",
                "Vitaly Nikolaev",
                "Ryan McDonald."
            ],
            "title": "Planning with learned entity prompts for abstractive summarization",
            "venue": "Transactions of the Association for Computational Linguistics, 9:1475\u20131492.",
            "year": 2021
        },
        {
            "authors": [
                "Krishna Pillutla",
                "Swabha Swayamdipta",
                "Rowan Zellers",
                "John Thickstun",
                "Sean Welleck",
                "Yejin Choi",
                "Zaid Harchaoui."
            ],
            "title": "Mauve: Measuring the gap between neural text and human text using divergence frontiers",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Ratish Puduppully",
                "Yao Fu",
                "Mirella Lapata."
            ],
            "title": "Data-to-text generation with variational sequential planning",
            "venue": "Transactions of the Association for Computational Linguistics, 10:697\u2013715.",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Know what you don\u2019t know: Unanswerable questions for squad",
            "venue": "arXiv preprint arXiv:1806.03822.",
            "year": 2018
        },
        {
            "authors": [
                "Hannah Rashkin",
                "Asli Celikyilmaz",
                "Yejin Choi",
                "Jianfeng Gao."
            ],
            "title": "PlotMachines: Outlineconditioned generation with dynamic plot state tracking",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Craige Roberts."
            ],
            "title": "Information structure: Towards an integrated formal theory of pragmatics",
            "venue": "Semantics and pragmatics, 5:6\u20131.",
            "year": 2012
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jon Shlens",
                "Zbigniew Wojna."
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818\u20132826.",
            "year": 2016
        },
        {
            "authors": [
                "Tom Trabasso",
                "Paul Van den Broek",
                "So Young Suh."
            ],
            "title": "Logical necessity and transitivity of causal relations in stories",
            "venue": "Discourse processes, 12(1):1\u201325.",
            "year": 1989
        },
        {
            "authors": [
                "Maria Tsimpoukelli",
                "Jacob L Menick",
                "Serkan Cabi",
                "SM Eslami",
                "Oriol Vinyals",
                "Felix Hill."
            ],
            "title": "Multimodal few-shot learning with frozen language models",
            "venue": "Advances in Neural Information Processing Systems, 34:200\u2013212.",
            "year": 2021
        },
        {
            "authors": [
                "Eileen Wang",
                "Caren Han",
                "Josiah Poon."
            ],
            "title": "Rovist: Learning robust metrics for visual storytelling",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 2691\u20132702.",
            "year": 2022
        },
        {
            "authors": [
                "Ruize Wang",
                "Zhongyu Wei",
                "Piji Li",
                "Qi Zhang",
                "Xuanjing Huang."
            ],
            "title": "Storytelling from an image stream using scene graphs",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9185\u20139192.",
            "year": 2020
        },
        {
            "authors": [
                "Xin Wang",
                "Wenhu Chen",
                "Yuan-Fang Wang",
                "William Yang Wang."
            ],
            "title": "No metrics are perfect: Adversarial reward learning for visual storytelling",
            "venue": "arXiv preprint arXiv:1804.09160.",
            "year": 2018
        },
        {
            "authors": [
                "Chunpu Xu",
                "Min Yang",
                "Chengming Li",
                "Ying Shen",
                "Xiang Ao",
                "Ruifeng Xu."
            ],
            "title": "Imagine, reason and write: Visual storytelling with graph knowledge and relational reasoning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol-",
            "year": 2021
        },
        {
            "authors": [
                "Jingjing Xu",
                "Xuancheng Ren",
                "Yi Zhang",
                "Qi Zeng",
                "Xiaoyan Cai",
                "Xu Sun."
            ],
            "title": "A skeleton-based model for promoting coherence among sentences in narrative story generation",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural",
            "year": 2018
        },
        {
            "authors": [
                "Kevin Yang",
                "Nanyun Peng",
                "Yuandong Tian",
                "Dan Klein."
            ],
            "title": "Re3: Generating longer stories with recursive reprompting and revision",
            "venue": "arXiv preprint arXiv:2210.06774.",
            "year": 2022
        },
        {
            "authors": [
                "Pengcheng Yang",
                "Fuli Luo",
                "Peng Chen",
                "Lei Li",
                "Zhiyi Yin",
                "Xiaodong He",
                "Xu Sun."
            ],
            "title": "Knowledgeable storyteller: A commonsense-driven generative model for visual storytelling",
            "venue": "IJCAI, pages 5356\u2013 5362.",
            "year": 2019
        },
        {
            "authors": [
                "Lili Yao",
                "Nanyun Peng",
                "Ralph Weischedel",
                "Kevin Knight",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Planand-write: Towards better automatic storytelling",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7378\u20137385.",
            "year": 2019
        },
        {
            "authors": [
                "Xiaohua Zhai",
                "Xiao Wang",
                "Basil Mustafa",
                "Andreas Steiner",
                "Daniel Keysers",
                "Alexander Kolesnikov",
                "Lucas Beyer."
            ],
            "title": "Lit: Zero-shot transfer with locked-image text tuning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Visual storytelling involves narrating an engaging and logically coherent story based on a sequence of images (see the example in Figure 1). The task lies at the intersection of natural language processing and computer vision and has recently attracted increasing interest from both communities (Wang et al., 2022; Hsu et al., 2021; Xu et al., 2021; Chen et al., 2021; Hsu et al., 2020; Wang et al., 2020; Huang et al., 2016). Visual storytelling differs from image captioning, which typically focuses on generating descriptive text, e.g., by identifying and depicting objects within an image. It requires a deeper understanding of how images and the events they illustrate relate to each other in order to create a convincing narrative.\nVisual storytelling is commonly modeled as a two-stage process. The image sequence is first\nencoded into a representation which typically includes image embeddings and detected objects. Subsequently, a decoder generates a story token by token based on the encoding of the image sequence. Recent work has mainly focused on enhancing the first stage of the generation process e.g., by leveraging external knowledge sources (Hsu et al., 2021; Chen et al., 2021; Hsu et al., 2020; Yang et al., 2019). Advanced representations for image sequences have also been explored, such as scene graphs (Hong et al., 2020) and story graphs (Hsu et al., 2021). Despite recent progress, these methods struggle to produce meaningful narratives, are prone to hallucination and repetition, often generate vague sentences, and have difficulty identifying salient visual concepts.\nWe attribute the lack of story quality to at least two reasons. Previous work on text-based genera-\ntion has demonstrated that planning can improve story coherence, allowing to control the trajectory of events, the characters described and their actions (Yao et al., 2019; Xu et al., 2018; Rashkin et al., 2020; Goldfarb-Tarrant et al., 2020a; Fan et al., 2019; Yang et al., 2022). However, planning has not been considered in the context of visual storytelling, existing models adopt black-box architectures which are not particularly controllable or interpretable. Another limitation concerns the nature of current models which are essentially trained from scratch, and as a result have limited language modelling and generalization capabilities (they only see multimodal training samples; see top of Figure 1). Although pretrained language models (Raffel et al., 2020; Lewis et al., 2020; Brown et al., 2020) have been widely adopted for generalpurpose story generation, their potential for visual storytelling remains unexplored.\nIn this work we propose an approach to visual storytelling which integrates pretrained language models with visual representations and incorporates an intermediate planning step before generating the full story. Our encoder translates the image sequence into a visual prefix, a sequence of continuous embeddings which language models can interpret. Following Narayan et al. (2022), we represent plans as a sequence of question-answer pairs, called blueprints, which serve as a proxy for content selection (i.e., what to say) and planning (i.e., in what order). Blueprints are loosely related to the Question-under-Discussion (QUD) theory of discourse (Larsson, 2002; Roberts, 2012; De Kuthy et al., 2020), which posits that text structure can be analyzed by identifying implicit questions raised and answered by subsequent spans of text. We augment visual storytelling training data with story blueprints (see Figure 1), which we obtain automatically thanks to state-of-the-art question generation technology.\nWe fine-tune pretrained language models to generate blueprints from image sequences and the stories based on them. We showcase two types of storytelling models, which vary in how the planning mechanism is implemented. A top-down model generates the blueprint first and then continues to generate the corresponding story in one go, whereas an integrated model interleaves planning with text generation rather than determining a plan in advance; generation is iteratively conditioned on the image input, the blueprint and the story gener-\nated so far. Experiments on the VIST benchmark (Huang et al., 2016) show that blueprint-based models generate more coherent, interesting, and humanlike stories compared to the state of the art and large language models (LLMs) like GPT-3.5, according to automatic and human evaluation."
        },
        {
            "heading": "2 Related Work",
            "text": "Visual Storytelling Huang et al. (2016) introduced visual storytelling as a vehicle for developing AI tools with human-like understanding of grounded event structure and linguistic abilities that go beyond descriptive language. While earlier work (Gonzalez-Rico and Fuentes-Pineda, 2018; Kim et al., 2018) employed simple encoderdecoder architectures (using CNNs to extract visual features and RNNs to generate text), more recent methods (Xu et al., 2021; Chen et al., 2021; Hsu et al., 2020; Yang et al., 2019) leverage external resources (e.g., ConceptNet) as a way of instilling commonsense reasoning skills. Sometimes, scene graphs are also used to model relations between objects (Lu et al., 2016; Hong et al., 2020; Wang et al., 2020). To our knowledge, none of these approaches make use of plan-based decoding. Hsu et al. (2021) construct a graph representing the image sequence (based on training data and external resources) and identify the highest scoring path as the best storyline encapsulated therein. The storyline can be viewed as a form of planning, however, on the encoder side.\nMost existing approaches (Xu et al., 2021; Hsu et al., 2020; Wang et al., 2020; Yang et al., 2019) train Transformer models from scratch, with the exception of Chen et al. (2021), who employ a vanilla BART model as a baseline without task-specific adaptation. In contrast, our work leverages the language modeling and generalization capabilities of pretrained language models for visual storytelling.\nPlanning and Generation In the domain of automatic story generation, planning has been effective at capturing the content and structure of stories. The generation process is often decomposed into two stages, namely planning an outline and then elaborating on it, e.g., by filling in specific details of a story. Plans have been represented as a sequence of event or phrase keywords (Yao et al., 2019; Xu et al., 2018; Rashkin et al., 2020), character actions (Liu et al., 2020), plot structures (Goldfarb-Tarrant et al., 2020a), and more elaborate descriptions including details about the setting\nof the story, its characters, and main plot points (Yang et al., 2022).\nThe idea of having a separate planning stage has also been explored for other text generation tasks including summarization (Narayan et al., 2021; Liu and Chen, 2021) and data-to-text generation (Moryossef et al., 2019; Puduppully et al., 2022). Our work is closest to Narayan et al. (2022) who propose the use of question-answer pairs as intermediate plans for summarization. However, their approach is designed for descriptive text. Our work extends their framework to a multimodal setting, where the input consists of image sequences, and the output are narratives characterized by more abstract and figurative language."
        },
        {
            "heading": "3 Blueprint-based Visual Storytelling",
            "text": "Let I represent a sequence of k images, denoted as {v1, v2..., vk}. Given this input, our goal is to generate a blueprint plan B (i.e., an ordered set of question-answer pairs) and a story S based on it. Most generation datasets do not include blueprint annotations, and visual storytelling is no exception. We first describe how we automatically obtain {Ii, Bi, Si}Ni=1 training data samples (Section 3.1), and then introduce our story generation models (Section 3.2)."
        },
        {
            "heading": "3.1 Blueprint Annotation",
            "text": "Let {Ii, Si}Ni=1 denote a dataset consisting of pairs of image sequences and their corresponding stories. We automatically create blueprint Bi based on story Si using state-of-the-art question generators (Romero, 2021; Raffel et al., 2020), coupled with a filtering procedure to remove repetitions and ill-formed questions.\nThe generation of question-answer pairs involves two steps, namely answer extraction and question generation. In the context of storytelling, capturing key events is crucial for a compelling narrative. While noun phrases and named entities are commonly recognized as significant content units in other tasks such as summarization (Narayan et al., 2022; Deutsch and Roth, 2023), verb phrases also play a vital role in conveying story dynamics, actions, and relationships (Trabasso et al., 1989; Eisenberg and Finlayson, 2017; Liu et al., 2020). Therefore, in addition to named entities and noun phrases, we also extract verb phrases as answer candidates using the spaCy library.\nWe then generate questions for answer candi-\ndates with a T5 model (Raffel et al., 2020; Romero, 2021) fine-tuned on the SQuAD reading comprehension dataset (Rajpurkar et al., 2018). The answer and the story are provided as context to predict the corresponding question. We decontextualize stories by replacing pronouns with their corresponding head mentions, using a state-of-the-art coreference resolution model (Dobrovolskii, 2021).\nQuestion-answer pairs are subsequently filtered to eliminate noise which is unavoidable due to the automatic preprocessing steps mentioned above. We thus remove any question-answer pairs where the answer is already present in the question. We also employ a round-trip consistency check (Alberti et al., 2019) which discards questions if they yield answers different from those used to generate them."
        },
        {
            "heading": "3.2 Blueprint Models",
            "text": "Our approach leverages the generation capabilities of pre-trained sequence-to-sequence models. As our backbone model, we employ BART-base (Lewis et al., 2020) which has been fine-tuned for text generation. We adapt this model to our visual storytelling task in two ways. Aside from enabling the generation of blueprints, we convert the image sequence to a visual prefix which the pretrained language model can interpret. The pretrained language model is prompted with this prefix to generate the blueprint, and eventually the story.\nVisual Prefix Construction Our model needs to grasp what the image sequence is about, e.g., the depicted objects, actions, and their associations. Drawing inspiration from recent advances in vision and language research (Mokady et al., 2021; Tsimpoukelli et al., 2021; Alayrac et al., 2022; Zhai et al., 2022; Liu et al., 2023; Huang et al., 2023), we translate the input sequence of images into a\nsequence of continuous embeddings, aka a visual prefix (see Figure 2).\nFollowing previous work (Wang et al., 2018; Xu et al., 2021; Chen et al., 2021), we use ResNet-152 (He et al., 2016) to extract visual features from images. We next employ a lightweight linear mapping network that consists of a series of feedforward layers, denoted as F\u03d5, to map image features to k visual clues:\np1, . . . , pk = F\u03d5 (ResNet (v1, v2, . . . , vk)) (1)\nwhere k is the image sequence length, and each visual clue pi has the same dimensionality as a token embedding of the pretrained language model.\nTo further instil world knowledge in our visual prefix, we employ a concept detector. The latter identifies specific objects within images, but also actions, scenes, and attributes. For each image vi, we retain the K concepts {c1i , c2i , ..., cKi } with the highest confidence score:\nc1i , . . . , c K i = Concept (ResNet (vi)) (2)\nConcepts for each image are then concatenated with a \u27e8SEP\u27e9 token and serve as input to the embedding layer of the pretrained model. The visual clues and concept embeddings are concatenated to form the visual prefix V . The image encoder and the concept detector remain frozen during the training phase. Only the parameters in the mapping network F\u03d5 are updated (see Figure 2).\nTop-down Planning This model takes image sequence I as input and generates blueprint B and story S in one go. More precisely, during decoding, the model first generates the blueprint, which then serves as a prompt guiding subsequent story generation. Our training objective maximizes the\nlog-likelihood of the joint distribution:\nmax \u03b8,\u03d5 N\u2211 i=1 log p\u03b8,\u03d5 (Bi,Si | Ii) (3)\nwhere (B ,S ) refers to the concatenation of blueprint B and story S . \u03b8 represents the parameters of the pretrained language model, \u03d5 are the parameters of the mapping visual network F\u03d5, and N denotes the size of the dataset.\nWe introduce special tokens Story: and Plan: preceding the story and blueprint, respectively. In experiments, our blueprints consist of answerquestion pairs {a1, q1, . . . , am, qm} (rather than question-answer pairs). We place the answer before its question to encourage the model to zoom in on salient visual concepts depicted in the image sequence. This ordering is intuitive for our storytelling task: We first decide on what the story is about and then elaborate on key concepts. Incidentally, Narayan et al. (2022) also find that generating the answer before the question performs better for their summarization tasks. Finally, the model is trained with the standard maximum likelihood objective to generate the joint target.\nIterative Planning This model employs an incremental generation strategy to create the story. Rather than generating in one step a global blueprint and the story, planning and generation are interleaved. At each time step, the iterative model considers the image sequence and context from previous steps, including the blueprint and story generated so far. We gradually construct the blueprint and its corresponding story sentence-bysentence; our planning is informed by generation and vice versa, which we argue should be mutually beneficial (they are conditioned on each other).\nLet S = {s1, s2, . . . , sk}, denote a target story and B = {b1, b2, . . . , bk} its blueprint where\nsi represents the i-th sentence in the story, and bi its associated blueprint. Each bi consists of answerquestion pairs, denoted as {ai1, qi1, . . . , ail(i), q i l(i)}, where l(i) is the number of pairs in the i-th blueprint. The training objective for the iterative model is defined as follows:\nmax \u03b8,\u03d5 N\u2211 j=1 k\u2211 i=1 log p\u03b8,\u03d5 (bi+1, si+1|b1:i, s1:i, Ij) (4)\nwhere b1:i and s1:i refer to the blueprint and sentences generated so far, from step 1 to step i.\nAt each time step i, the encoder takes image sequence I as input, and the decoder takes the context (i.e., blueprint and sentences generated so far {b1, b2, . . . , bi; s1, s2, . . . , si}) as a prompt to predict the next blueprint bi+1 and sentence si+1. Therefore, the iterative model is trained on samples {I , (b1:i, s1:i), bi+1, si+1}. We prefix (b1:i, s1:i), bi+1, and si+1 with Context:, Plan:, and Next Sentence, respectively. To handle the first time step, we introduce special token \u27e8START \u27e9 as context to predict b1 and s1. We also use \u27e8END\u27e9 to indicate the completion of an iteration (see Figure 3 for an illustration). It is important to note that (b1:i, s1:i) are masked out when computing the loss because they serve as prompts to the decoder. We want to avoid the model repeatedly predicting and overly optimizing the blueprints and sentences that appear at the beginning of the output."
        },
        {
            "heading": "4 Experimental Setting",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "We performed experiments on the widely used VIST dataset (Huang et al., 2016), which contains 10,117 Flickr albums and 210,819 unique photos. Each training sample consists of k=5 images and a corresponding story of k=5 sentences. As described in Section 3.1, we augment each story with an automatically generated blueprint."
        },
        {
            "heading": "4.2 Implementation Details",
            "text": "Our models are built on top of BART-base (Lewis et al., 2020) and finetuned with a learning rate of 3e-5, batch size of 64, and warm-up ratio of 0.05. We select the best checkpoint on the validation set using a QA-based metric which quantifies the extent to which the output story follows its blueprint (see Section 4.4). During inference, we employ beam search (size 5). For our visual prefix, we employed the Clarifai Concept Detector which was trained on a dataset containing 9,098 concepts and 20 million images (multiple concepts are assigned to each image), and is integrated with the InceptionV2 architecture (Szegedy et al., 2016) ."
        },
        {
            "heading": "4.3 Comparison Systems",
            "text": "We compared our models against several baselines and state-of-the-art visual storytelling models. These included a vanilla BART-base model with the same encoder and visual prefix as ours but no planning (VP-BART; it generates the story directly in an autoregressive manner without the blueprint). KG-Story (Hsu et al., 2020) predicts a set of words representative of the image sequence, enriches them using external knowledge graphs, and generates stories based on the enriched word set. PR-VIST (Hsu et al., 2021) is a state-of-the-art model which constructs a graph representing the relations between elements in the image sequence, identifies the best storyline captured therein, and proceeds to generate a story based on it. The process of constructing the story graph can be viewed as a form of planning. Along similar lines, Chen et al. (2021) build a common sense knowledge graph capturing concepts in the image sequence, and use MCSM, a Maximal Clique Selection Module to identify which ones to write a story about. They use BART-large to generate the story based on selected concepts (and image features).\nWe also compared against an LLM that generates stories via prompting. We provide GPT-3.5 with a visual prefix, namely the concepts identified in the image sequence, and a prompt which explains how to create the blueprint and generate the story together with examples (in-context learning). Details on the prompt can be found in Appendix A."
        },
        {
            "heading": "4.4 Automatic Evaluation",
            "text": "We evaluated our stories using BLEU, ROUGE, METEOR, and CIDER, mainly to compare to previous work. Several studies (Hsu et al., 2022, 2021,\n2020; Hu et al., 2020; Yang et al., 2019; Modi and Parde, 2019) have demonstrated the inadequacy of lexical matching metrics: they correlate poorly with human judgments, and not do effectively measure the semantic similarity to human-written stories or the lexical richness of the generated stories.\nWe further employ story-specific metrics to assess story quality aspects such as diversity, fluency, naturalness, and grounding. Specifically, we use two types of trigram repetition metrics (Yao et al., 2019; Goldfarb-Tarrant et al., 2020b). Intra-story repetition is a fluency metric, it measures the proportion of trigrams repeated within a story. Interstory repetition examines trigram repetition across stories. This metric evaluates diversity, high intrastory repetition suggests that the model tends to generate the same story even when conditioned on different image sequences. We also use MAUVE (Pillutla et al., 2021) to measure the naturalness of the generated stories. MAUVE is a recently introduced automatic metric for open-ended generation which has high correlation with human judgements. It computes the similarity of the distribution of human-written text and machine-generated text.\nTo quantify the extent to which the generated story is grounded, i.e., whether it accurately represents the content of the image sequence, we measure concept precision and recall. Precision measures the number of words in the generated story that align with the detected concept set, while recall assesses the number of words in the detected concept set that are present in the generated story.\nFinally, for our own models we also evaluate whether the generated stories are faithful to their blueprint. Drawing inspiration from recent studies on summary evaluation (Deutsch et al., 2021; Fabbri et al., 2022), we measure how well the generated story answers questions from the predicted blueprint. We utilize a RoBERTa-based (Liu et al., 2019) QA model finetuned on the SQuAD dataset."
        },
        {
            "heading": "5 Results",
            "text": "Our results are summarized in Table 2. The first block presents the performance of state-ofthe-art storytelling systems. The second block presents variants of our approach: a vanilla BART model, enhanced with a visual prefix (VP), and two blueprint models which vary in the way plans are generated, i.e., in a top-down fashion or iteratively. The third block contains GPT-3.5 models with (+BP) and without blueprints.\nPretrained Language Models Produce Better Stories We observe that models based on pretrained language models (i.e., our models and MCSM, outperform models trained from scratch (i.e., KG-Story and PR-VIST) in terms of trigramrepetition scores and MAUVE. This indicates that we can maintain strong language modeling capabilities while enabling pretrained language models to process visual signals effectively.\nThe Visual Prefix is an Effective Interface between Image and Text MCSM is the only existing model that utilizes a pretrained language model for visual storytelling. However, our baseline model (VP-BART) demonstrates superior performance in most story-specific metrics. Remarkably, this is achieved using a smaller pretrained model (BART-base, 140M parameters); MCSM is built on top of BART-large (400M parameters). This highlights the effectiveness of our visual prefix, indicating it successfully translates the image sequence into a space that BART can understand.\nBlueprint Models are Most Grounded Our models outperform comparison systems in terms of concept grounding. This confirms that an intermediate planning step allows the model to effectively select salient concepts based on the visual prefix. The top-down model in particular achieves the highest concept grounding recall, it stays close to the image sequence, accurately describing the information conveyed therein. The higher lexical matching scores further support this observation. The iterative blueprint model achieves the best concept grounding precision (excluding GPT-3.5 models) which in turn suggests that the stories generated by this model exhibit a stronger grounding to the images with fewer hallucinations.\nThe Iterative Model Generates Most Natural and Faithful Stories Despite not achieving the highest scores in lexical matching metrics, the iterative blueprint model stands out in terms of MAUVE evaluation. Compared to other models, it generates more natural stories, closer to those written by humans. This finding suggests that humans might employ a similar iterative planning strategy, at least for the short stories considered here; they construct a narrative gradually rather than a global plan which they subsequently convert into a story.\nWith regard to faithfulness, we observe that both blueprint models achieve scores higher than 40%, indicating effective translation of blueprints into\nstories. Notably, the iterative model performs best in terms of faithfulness, which suggests that translating the entire global blueprint into a story is more challenging, whereas breaking down planning into individual steps is more effective. To get an idea of the upper bound performance for blueprint models, we ran the top-down model with silver standard blueprints extracted from the human-written stories (see row +BP (gold) in Table 2). As can be seen, the MAUVE score jumps to 52.24, edging closer to human-written stories (their MAUVE score is 69.6). This further supports our hypothesis that our model successfully leverages the blueprints and retains the information captured in them.\nGPT-3.5 Struggles with Blueprints We also compared our approach to GPT-3.5, which we adapted to our task with in-context learning. A GPT-3.5 model enhanced with blueprints performs well at concept grounding, i.e., it generates stories which talk about what is depicted in the image. However, these stories are neither human-like (see the very low MAUVE score) nor faithful to the intermediate blueprints (in fact they are 10% less faithful compared to our iterative model). This suggests that GPT-3.5 tends to ignore the plan, despite being explicitly prompted with blueprints."
        },
        {
            "heading": "6 Human Evaluation",
            "text": "We conducted a judgment elicitation study to further evaluate the stories generated by our models. Specifically, we compared the best performing blueprint model (iterative) and three other systems: (a) PR-VIST, which represents the current planning-based state of the art; (b) VP-BART, our proposed model without blueprints; and (c) ground truth stories written by humans. Raters were shown an image sequence, alongside two stories and were\nasked to provide pairwise references along the following dimensions: Relevance, Fluency, Coherence, Interestingness, and Overall. The full instructions are given in Appendix A. We recruited nine native English speakers and elicited preferences for 100 stories (three judgments per story).\nOur human evaluation results are summarized in Table 3. The iterative blueprint model outperforms PR-VIST across metrics. Our participants perceive VP-BART stories as marginally more fluent and coherent compared to those created by the iterative model (even though they prefer iterative stories overall). This discrepancy is likely due to the generation process introduced by the iterative model which requires the decoder to produce a mix of questions, answers, and corresponding sentences, deviating from the traditional BART pretraining pattern. This added complexity might result in minor grammatical errors and pose challenges for coherence, given that story generation is broken down into separate steps instead of being a continuous process. Nonetheless, the coherence scores are fairly close.\nThe blueprint model excels in terms of interestingness and grounding, indicating its effectiveness in creating engaging and memorable stories. Our model\u2019s superior grounding performance aligns with our hypothesis that blueprints serve not only as a planning strategy but also as a visual concept selector. This is due to the way blueprints are structured (as answer-question pairs), which explicitly forces the model to first identify salient visual concepts and then generate questions based on them.\nFigure 4 shows example stories created by the models used in our human evaluation study and GPT-3.5+BP. The story generated by the iterative model is coherent, rich in detail, and fluent. VP-\nBART generates a grounded and accurate story without hallucination and semantic errors. However, it is a relatively plain narrative, offering limited detail about the market or the experience of the characters. Compared to GPT-3.5+BP, the iterative model\u2019s story follows the image sequence more closely, mentioning details like an array of different fruits and various types of pepper, which significantly enhances storytelling."
        },
        {
            "heading": "7 Controllable Generation",
            "text": "In this section we showcase how the blueprint plan allows us to control the content and length of model output without additional mechanisms or training.\nFor example, in cases where the generated story contains entities which do not appear in the image sequence, it is possible to refine the story generation process, mitigating hallucinations. Specif-\nically, we apply a filtering step which removes non-grounded entities (and corresponding QA pair) from the blueprint before generating the story. We consider as non-grounded any blueprint entity which is not included in the output of the concept detector (see Section 3.2).\nFigure 5 shows how this refinement approach can be used to adjust the model\u2019s output. In the first example, we observe that the story generated with a refined blueprint effectively avoids hallucinations (highlighted in blue) and is overall more faithful. However, it is important to note that imagination plays a crucial role in crafting an engaging story, especially when the image sequence provides limited information. Therefore, employing the refinement method may result in shorter and less detailed stories, as illustrated in the second example. While the refined blueprint successfully eliminates all hal-\nlucinated entities, the resulting story appears plain and lacks depth. Our blueprint method seems to strike the right balance between accurate and captivating story generation, prioritizing faithfulness to the image sequence and creativity in storytelling.\nMost visual storytelling systems generate 5-sentence stories, following the predefined story structure of the VIST dataset (Huang et al., 2016). Nevertheless, our iterative blueprint model can flexibly modulate the length of the story by controlling the number of iterative steps, thereby overcoming the conventional sentence limitation. Figure 6 presents stories generated by this model with a maximum of 10 iterations. Despite the increased length, the stories maintain coherence and are engaging."
        },
        {
            "heading": "8 Conclusion",
            "text": "In this work, we have introduced a novel approach to visual storytelling which integrates visual representations with pretrained language models and a blueprint-based planning method for story gen-\nWe took a car out onto the street that day. Here we are looking at the car that the winner of the race was in. The car with the winner in blue was very close throughout the whole race. It was neck and neck as the one in blue held the lead throughout the entire race. Finally, we crossed the lap and the winner pulled in for first place at the landing line. We were very close to winning the race and finishing in the final place line. It was a very very, very close finish!\nLast night, the band played an awesome concert. The guitarist had an incredible performance. The keyboard player was incredible as well. The guitarist was superb. He played his guitar very well. The next song he sang was very good. They had such a good performance, and I'm glad they went. The audience loved them so much. The night was over when the band wrote and put together their music. It was a fantastic way to finish the night.\nLong Story Generation\nFigure 6: Stories generated within 10 iterations.\neration. Blueprint models leverage a sequence of question-answer pairs as intermediate plans, enabling better selection of salient concepts from the image sequence and guiding the construction of the final narrative. Specifically, we have showcased two model variants: a top-down model which relies on a global plan, and an iterative model, which interleaves planning with sentence generation. Our experiments have shown that blueprint models excel in concept grounding and their ability to create human-like stories. Additionally, they are controllable: Blueprints can be made shorter or longer and their details can be refined (e.g., by emphasising specific entities or characters), thus enabling human-in-the-loop and personalized storytelling. We showcase examples of controllability in Section 7. In the future, we would like to explore visual storytelling with grounded characters and entities, as well as tackle the generation of more complex narratives, such as long-form stories.\nAcknowledgments The authors gratefully acknowledge the support of the UK Engineering and Physical Sciences Research Council (grant EP/W002876/1). Liu was supported by the UKRI Centre for Doctoral Training in Natural Language Processing, funded by the UKRI (grant EP/S022481/1) and the University of Edinburgh.\nLimitations\nWhile our proposed model demonstrates effective story generation, it has certain limitations. Firstly, the grounding relation between the visual concepts and the corresponding text may not always be clear, leading to potential ambiguity in the generated stories. Furthermore, the model can sometimes suffer from hallucinations due to falsely detected visual concepts.\nIt is worth noting that our model was built on top of BART-base (Lewis et al., 2020). It would be beneficial to investigate the performance of larger models, as they could potentially enhance the quality of the planning component and overall storytelling capability.\nEthics Statement\nLarge Language Models This paper uses large pretrained language models, which have been shown to be subject to a variety of biases, to occasionally generate toxic language, and to hallucinate content. Model output used for the human evaluation study (Section 6) was screened by the authors for harmful content.\nExperimental Participants The departmental ethics panel judged our human evaluation study to be exempt from ethical approval, as all participants were employees of the University of X, and as such were protected by employment law. Participants were paid at the standard hourly rate for tutors and demonstrators at the university."
        },
        {
            "heading": "A GPT-3.5 Experimental Setting",
            "text": "We designed the following prompts for GPT-3.5:\nPrompt (w/o Blueprint): I want you to act as a visual storyteller by creating a story based on a given sequence of images. For each image, I\u2019ll provide the key concepts. The concepts from different images will be separated by <SEP>. \\n Concepts: ..., \\n Story: ..., ..., \\n Concepts: ..., \\n Story:\nPrompt (w/ Blueprint): I\u2019d like you to act as a visual storyteller by creating a story based on a given sequence of images. For each image, I\u2019ll provide the key concepts separated by <SEP>. Your task is to first generate a series of question-answer pairs for each image as part of the planning process, and then use these pairs to create the final story. \\n Concepts: ..., \\n Plan: ..., \\n Story: ..., ..., \\n Concepts: ..., \\n Plan: ..., \\n Story:\nIn-context learning examples in the prompts are not shown for brevity. To make the best of incontext learning, we employed max-shot learning, while adhering to the token limit of 4,096."
        },
        {
            "heading": "B Human Evaluation Study",
            "text": "As mentioned in Section 6 we conducted a judgment elicitation study to further evaluate the stories generated by our models. Specifically, we compared the best performing blueprint model (iterative) and three other systems: (a) PR-VIST, which represents the current planning-based state of the art; (b) VP-BART, our proposed model without blueprints; and (c) ground truth stories written by humans. Raters were shown an image sequence, alongside two stories and were asked to provide pairwise preferences along various dimensions of story quality. We describe below our evaluation procedure and reproduce the instructions given to our raters.\nB.1 Evaluation Procedure We initially conducted a pilot study, based on which we devised our instructions. Subsequently, 100 image sequences were randomly selected from the test set, leading to a total of 300 pairwise comparisons. We employed the expertise of 9 native speakers who triple-annotated the 300 pairwise comparisons, resulting in a total of 900 judgments.\nB.2 Experimental Instructions Human raters were asked to compare and evaluate stories generated by different systems using pair-\nwise judgments. The evaluation focused on the following dimensions of story quality: Relevance, Fluency, Coherence, Interestingness, and an Overall judgment. We provide their definitions below.\nRelevance Relevance captures whether the sentences in the stories relate to the input images. For each sentence, if the sentence accurately describes the content of a specific image (i.e., not imagining something that does not exist in the image), it will be marked as relevance. Otherwise, if the sentence does not correspond to an image or is too vague, it will be not marked as relevance.\nFluency Fluency evaluates the grammatical correctness of the text. A story is fluent if it has few or no grammatical errors and is easy to understand.\nCoherence Coherence assesses whether the story makes sense. A coherent story flows well, the sentences are related, and logically connected. In contrast, an incoherent story would be more or less incomprehensible, without any logical connection between its sentences.\nInterestingness This evaluates whether the story contains unique, possibly unexpected elements. For example, a memorable storyline. Below are examples of a dull story and an interesting story.\nOverall Taking all the aforementioned criteria into consideration, the annotators selected their preferred story for the given set of five images.\nB.3 Annotation Interface We designed an annotation interface using Python Flask. Figure 7 shows a screenshot of the interface."
        }
    ],
    "title": "Visual Storytelling with Question-Answer Plans",
    "year": 2023
}